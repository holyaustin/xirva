[{"id": "2106.00043", "submitter": "Matthew Baas", "authors": "Matthew Baas, Herman Kamper", "title": "StarGAN-ZSVC: Towards Zero-Shot Voice Conversion in Low-Resource\n  Contexts", "comments": "16 pages, 3 figures. Published in Springer Communications in Computer\n  and Information Science, Artificial Intelligence Research (SACAIR 2021), vol.\n  1342, pp. 69-84, 2020", "journal-ref": "In: Springer Communications in Computer and Information Science,\n  Artificial Intelligence Research (SACAIR 2021), vol. 1342, pp. 69-84, 2020", "doi": "10.1007/978-3-030-66151-9_5", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Voice conversion is the task of converting a spoken utterance from a source\nspeaker so that it appears to be said by a different target speaker while\nretaining the linguistic content of the utterance. Recent advances have led to\nmajor improvements in the quality of voice conversion systems. However, to be\nuseful in a wider range of contexts, voice conversion systems would need to be\n(i) trainable without access to parallel data, (ii) work in a zero-shot setting\nwhere both the source and target speakers are unseen during training, and (iii)\nrun in real time or faster. Recent techniques fulfil one or two of these\nrequirements, but not all three. This paper extends recent voice conversion\nmodels based on generative adversarial networks (GANs), to satisfy all three of\nthese conditions. We specifically extend the recent StarGAN-VC model by\nconditioning it on a speaker embedding (from a potentially unseen speaker).\nThis allows the model to be used in a zero-shot setting, and we therefore call\nit StarGAN-ZSVC. We compare StarGAN-ZSVC against other voice conversion\ntechniques in a low-resource setting using a small 9-minute training set.\nCompared to AutoVC -- another recent neural zero-shot approach -- we observe\nthat StarGAN-ZSVC gives small improvements in the zero-shot setting, showing\nthat real-time zero-shot voice conversion is possible even for a model trained\non very little data. Further work is required to see whether scaling up\nStarGAN-ZSVC will also improve zero-shot voice conversion quality in\nhigh-resource contexts.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 18:21:28 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Baas", "Matthew", ""], ["Kamper", "Herman", ""]]}, {"id": "2106.00052", "submitter": "Nikolay Mikhaylovskiy", "authors": "Roman Bedyakin, Nikolay Mikhaylovskiy", "title": "Low-Resource Spoken Language Identification Using Self-Attentive Pooling\n  and Deep 1D Time-Channel Separable Convolutions", "comments": "Accepted to Dialog2021. arXiv admin note: text overlap with\n  arXiv:2104.11985", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This memo describes NTR/TSU winning submission for Low Resource ASR challenge\nat Dialog2021 conference, language identification track.\n  Spoken Language Identification (LID) is an important step in a multilingual\nAutomated Speech Recognition (ASR) system pipeline. Traditionally, the ASR task\nrequires large volumes of labeled data that are unattainable for most of the\nworld's languages, including most of the languages of Russia. In this memo, we\nshow that a convolutional neural network with a Self-Attentive Pooling layer\nshows promising results in low-resource setting for the language identification\ntask and set up a SOTA for the Low Resource ASR challenge dataset.\n  Additionally, we compare the structure of confusion matrices for this and\nsignificantly more diverse VoxForge dataset and state and substantiate the\nhypothesis that whenever the dataset is diverse enough so that the other\nclassification factors, like gender, age etc. are well-averaged, the confusion\nmatrix for LID system bears the language similarity measure.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 18:35:27 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bedyakin", "Roman", ""], ["Mikhaylovskiy", "Nikolay", ""]]}, {"id": "2106.00055", "submitter": "Thomas Bott", "authors": "Thomas Bott, Dominik Schlechtweg and Sabine Schulte im Walde", "title": "More than just Frequency? Demasking Unsupervised Hypernymy Prediction\n  Methods", "comments": "ACL Findings, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a comparison of unsupervised methods of hypernymy\nprediction (i.e., to predict which word in a pair of words such as fish-cod is\nthe hypernym and which the hyponym). Most importantly, we demonstrate across\ndatasets for English and for German that the predictions of three methods\n(WeedsPrec, invCL, SLQS Row) strongly overlap and are highly correlated with\nfrequency-based predictions. In contrast, the second-order method SLQS shows an\noverall lower accuracy but makes correct predictions where the others go wrong.\nOur study once more confirms the general need to check the frequency bias of a\ncomputational method in order to identify frequency-(un)related effects.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 18:41:39 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bott", "Thomas", ""], ["Schlechtweg", "Dominik", ""], ["Walde", "Sabine Schulte im", ""]]}, {"id": "2106.00085", "submitter": "Clara Meister", "authors": "Clara Meister, Ryan Cotterell", "title": "Language Model Evaluation Beyond Perplexity", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternate approach to quantifying how well language models\nlearn natural language: we ask how well they match the statistical tendencies\nof natural language. To answer this question, we analyze whether text generated\nfrom language models exhibits the statistical tendencies present in the\nhuman-generated text on which they were trained. We provide a framework--paired\nwith significance tests--for evaluating the fit of language models to these\ntrends. We find that neural language models appear to learn only a subset of\nthe tendencies considered, but align much more closely with empirical trends\nthan proposed theoretical distributions (when present). Further, the fit to\ndifferent distributions is highly-dependent on both model architecture and\ngeneration strategy. As concrete examples, text generated under the nucleus\nsampling scheme adheres more closely to the type--token relationship of natural\nlanguage than text produced using standard ancestral sampling; text from LSTMs\nreflects the natural language distributions over length, stopwords, and symbols\nsurprisingly well.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 20:13:44 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:41:11 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Meister", "Clara", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.00104", "submitter": "Yumo Xu", "authors": "Yumo Xu and Mirella Lapata", "title": "Text Summarization with Latent Queries", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large-scale datasets has driven the development of neural\nmodels that create summaries from single documents, for generic purposes. When\nusing a summarization system, users often have specific intents with various\nlanguage realizations, which, depending on the information need, can range from\na single keyword to a long narrative composed of multiple questions. Existing\nsummarization systems, however, often either fail to support or act robustly on\nthis query focused summarization task. We introduce LaQSum, the first unified\ntext summarization system that learns Latent Queries from documents for\nabstractive summarization with any existing query forms. Under a deep\ngenerative framework, our system jointly optimizes a latent query model and a\nconditional language model, allowing users to plug-and-play queries of any type\nat test time. Despite learning from only generic summarization data and\nrequiring no further optimization for downstream summarization tasks, our\nsystem robustly outperforms strong comparison systems across summarization\nbenchmarks with different query types, document settings, and target domains.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 21:14:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Xu", "Yumo", ""], ["Lapata", "Mirella", ""]]}, {"id": "2106.00130", "submitter": "Rui Meng", "authors": "Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong\n  Wang, Daqing He", "title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for\n  Long Scientific Documents", "comments": "Accepted at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Faceted summarization provides briefings of a document from different\nperspectives. Readers can quickly comprehend the main points of a long document\nwith the help of a structured outline. However, little research has been\nconducted on this subject, partially due to the lack of large-scale faceted\nsummarization datasets. In this study, we present FacetSum, a faceted\nsummarization benchmark built on Emerald journal articles, covering a diverse\nrange of domains. Different from traditional document-summary pairs, FacetSum\nprovides multiple summaries, each targeted at specific sections of a long\ndocument, including the purpose, method, findings, and value. Analyses and\nempirical results on our dataset reveal the importance of bringing structure\ninto summaries. We believe FacetSum will spur further advances in summarization\nresearch and foster the development of NLP systems that can leverage the\nstructured information in both long texts and summaries.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 22:58:38 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 01:59:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Meng", "Rui", ""], ["Thaker", "Khushboo", ""], ["Zhang", "Lei", ""], ["Dong", "Yue", ""], ["Yuan", "Xingdi", ""], ["Wang", "Tong", ""], ["He", "Daqing", ""]]}, {"id": "2106.00139", "submitter": "Jiaming Shen", "authors": "Jiaming Shen, Jialu Liu, Tianqi Liu, Cong Yu, Jiawei Han", "title": "Training ELECTRA Augmented with Multi-word Selection", "comments": "Accepted in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained text encoders such as BERT and its variants have recently\nachieved state-of-the-art performances on many NLP tasks. While being\neffective, these pre-training methods typically demand massive computation\nresources. To accelerate pre-training, ELECTRA trains a discriminator that\npredicts whether each input token is replaced by a generator. However, this new\ntask, as a binary classification, is less semantically informative. In this\nstudy, we present a new text encoder pre-training method that improves ELECTRA\nbased on multi-task learning. Specifically, we train the discriminator to\nsimultaneously detect replaced tokens and select original tokens from candidate\nsets. We further develop two techniques to effectively combine all pre-training\ntasks: (1) using attention-based networks for task-specific heads, and (2)\nsharing bottom layers of the generator and the discriminator. Extensive\nexperiments on GLUE and SQuAD datasets demonstrate both the effectiveness and\nthe efficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:19:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Shen", "Jiaming", ""], ["Liu", "Jialu", ""], ["Liu", "Tianqi", ""], ["Yu", "Cong", ""], ["Han", "Jiawei", ""]]}, {"id": "2106.00143", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov", "title": "An Exploratory Analysis of Multilingual Word-Level Quality Estimation\n  with Cross-Lingual Transformers", "comments": "Accepted to appear at the ACL-IJCNLP 2021 Main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most studies on word-level Quality Estimation (QE) of machine translation\nfocus on language-specific models. The obvious disadvantages of these\napproaches are the need for labelled data for each language pair and the high\ncost required to maintain several language-specific models. To overcome these\nproblems, we explore different approaches to multilingual, word-level QE. We\nshow that these QE models perform on par with the current language-specific\nmodels. In the cases of zero-shot and few-shot QE, we demonstrate that it is\npossible to accurately predict word-level quality for any given new language\npair from models trained on other language pairs. Our findings suggest that the\nword-level QE models based on powerful pre-trained transformers that we propose\nin this paper generalise well across languages, making them more useful in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:21:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Orasan", "Constantin", ""], ["Mitkov", "Ruslan", ""]]}, {"id": "2106.00145", "submitter": "Tedo Vrbanec", "authors": "Tedo Vrbanec and Ana Mestrovic", "title": "Corpus-Based Paraphrase Detection Experiments and Review", "comments": "25 pages, 7 figures, 4 tables", "journal-ref": "In Information (Switzerland) (Vol. 11, Issue 5, p. 241). 2020,\n  MDPI AG", "doi": "10.3390/INFO11050241", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Paraphrase detection is important for a number of applications, including\nplagiarism detection, authorship attribution, question answering, text\nsummarization, text mining in general, etc. In this paper, we give a\nperformance overview of various types of corpus-based models, especially deep\nlearning (DL) models, with the task of paraphrase detection. We report the\nresults of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO,\nand USE) evaluated on three different public available corpora: Microsoft\nResearch Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase\nCorpus 2011. Through a great number of experiments, we decided on the most\nappropriate approaches for text pre-processing: hyper-parameters, sub-model\nselection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and\nsemantic similarity/paraphrase detection threshold. Our findings and those of\nother researchers who have used deep learning models show that DL models are\nvery competitive with traditional state-of-the-art approaches and have\npotential that should be further developed.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:29:24 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Vrbanec", "Tedo", ""], ["Mestrovic", "Ana", ""]]}, {"id": "2106.00149", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang", "title": "HiddenCut: Simple Data Augmentation for Natural Language Understanding\n  with Better Generalization", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-tuning large pre-trained models with task-specific data has achieved\ngreat success in NLP. However, it has been demonstrated that the majority of\ninformation within the self-attention networks is redundant and not utilized\neffectively during the fine-tuning stage. This leads to inferior results when\ngeneralizing the obtained models to out-of-domain distributions. To this end,\nwe propose a simple yet effective data augmentation technique, HiddenCut, to\nbetter regularize the model and encourage it to learn more generalizable\nfeatures. Specifically, contiguous spans within the hidden space are\ndynamically and strategically dropped during training. Experiments show that\nour HiddenCut method outperforms the state-of-the-art augmentation methods on\nthe GLUE benchmark, and consistently exhibits superior generalization\nperformances on out-of-distribution and challenging counterexamples. We have\npublicly released our code at https://github.com/GT-SALT/HiddenCut.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:57:43 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Chen", "Jiaao", ""], ["Shen", "Dinghan", ""], ["Chen", "Weizhu", ""], ["Yang", "Diyi", ""]]}, {"id": "2106.00162", "submitter": "Weixin Liang", "authors": "Weixin Liang, Kai-Hui Liang, Zhou Yu", "title": "HERALD: An Annotation Efficient Method to Detect User Disengagement in\n  Social Conversations", "comments": "ACL 2021. Code & data available at\n  https://github.com/Weixin-Liang/HERALD/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain dialog systems have a user-centric goal: to provide humans with\nan engaging conversation experience. User engagement is one of the most\nimportant metrics for evaluating open-domain dialog systems, and could also be\nused as real-time feedback to benefit dialog policy learning. Existing work on\ndetecting user disengagement typically requires hand-labeling many dialog\nsamples. We propose HERALD, an efficient annotation framework that reframes the\ntraining data annotation process as a denoising problem. Specifically, instead\nof manually labeling training samples, we first use a set of labeling\nheuristics to label training samples automatically. We then denoise the weakly\nlabeled data using the Shapley algorithm. Finally, we use the denoised data to\ntrain a user engagement detector. Our experiments show that HERALD improves\nannotation efficiency significantly and achieves 86% user disengagement\ndetection accuracy in two dialog corpora.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:09:55 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 06:15:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liang", "Weixin", ""], ["Liang", "Kai-Hui", ""], ["Yu", "Zhou", ""]]}, {"id": "2106.00169", "submitter": "Adithya Renduchintala", "authors": "Adithya Renduchintala, Denise Diaz, Kenneth Heafield, Xian Li, Mona\n  Diab", "title": "Gender Bias Amplification During Speed-Quality Optimization in Neural\n  Machine Translation", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is bias amplified when neural machine translation (NMT) models are optimized\nfor speed and evaluated on generic test sets using BLEU? We investigate\narchitectures and techniques commonly used to speed up decoding in\nTransformer-based models, such as greedy search, quantization, average\nattention networks (AANs) and shallow decoder models and show their effect on\ngendered noun translation. We construct a new gender bias test set, SimpleGEN,\nbased on gendered noun phrases in which there is a single, unambiguous, correct\nanswer. While we find minimal overall BLEU degradation as we apply speed\noptimizations, we observe that gendered noun translation performance degrades\nat a much faster rate.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:32:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Renduchintala", "Adithya", ""], ["Diaz", "Denise", ""], ["Heafield", "Kenneth", ""], ["Li", "Xian", ""], ["Diab", "Mona", ""]]}, {"id": "2106.00181", "submitter": "Ziyang Luo", "authors": "Meichun Jiao, Ziyang Luo", "title": "Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese\n  Adjectives", "comments": "Accepted at the 3rd Workshop on Gender Bias in Natural Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender bias in word embeddings gradually becomes a vivid research field in\nrecent years. Most studies in this field aim at measurement and debiasing\nmethods with English as the target language. This paper investigates gender\nbias in static word embeddings from a unique perspective, Chinese adjectives.\nBy training word representations with different models, the gender bias behind\nthe vectors of adjectives is assessed. Through a comparison between the\nproduced results and a human-scored data set, we demonstrate how gender bias\nencoded in word embeddings differentiates from people's attitudes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:12:45 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Jiao", "Meichun", ""], ["Luo", "Ziyang", ""]]}, {"id": "2106.00188", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi,\n  Aniruddha Kembhavi, Ali Farhadi, Yejin Choi", "title": "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D\n  World", "comments": "ACL 2021 camera ready, project page at\n  https://rowanzellers.com/piglet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PIGLeT: a model that learns physical commonsense knowledge through\ninteraction, and then uses this knowledge to ground language. We factorize\nPIGLeT into a physical dynamics model, and a separate language model. Our\ndynamics model learns not just what objects are but also what they do: glass\ncups break when thrown, plastic ones don't. We then use it as the interface to\nour language model, giving us a unified model of linguistic form and grounded\nmeaning. PIGLeT can read a sentence, simulate neurally what might happen next,\nand then communicate that result through a literal symbolic representation, or\nnatural language.\n  Experimental results show that our model effectively learns world dynamics,\nalong with how to communicate them. It is able to correctly forecast \"what\nhappens next\" given an English sentence over 80% of the time, outperforming a\n100x larger, text-to-text approach by over 10%. Likewise, its natural language\nsummaries of physical interactions are also judged by humans as more accurate\nthan LM alternatives. We present comprehensive analysis showing room for future\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:32:12 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zellers", "Rowan", ""], ["Holtzman", "Ari", ""], ["Peters", "Matthew", ""], ["Mottaghi", "Roozbeh", ""], ["Kembhavi", "Aniruddha", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "2106.00197", "submitter": "Xingshan Zeng", "authors": "Xingshan Zeng, Liangyou Li and Qun Liu", "title": "Multilingual Speech Translation with Unified Transformer: Huawei Noah's\n  Ark Lab at IWSLT 2021", "comments": "IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the system submitted to the IWSLT 2021 Multilingual\nSpeech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified\ntransformer architecture for our MultiST model, so that the data from different\nmodalities (i.e., speech and text) and different tasks (i.e., Speech\nRecognition, Machine Translation, and Speech Translation) can be exploited to\nenhance the model's ability. Specifically, speech and text inputs are firstly\nfed to different feature extractors to extract acoustic and textual features,\nrespectively. Then, these features are processed by a shared encoder--decoder\narchitecture. We apply several training techniques to improve the performance,\nincluding multi-task learning, task-level curriculum learning, data\naugmentation, etc. Our final system achieves significantly better results than\nbilingual baselines on supervised language pairs and yields reasonable results\non zero-shot language pairs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:50:49 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 03:11:01 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zeng", "Xingshan", ""], ["Li", "Liangyou", ""], ["Liu", "Qun", ""]]}, {"id": "2106.00200", "submitter": "Haitian Sun", "authors": "Haitian Sun, William W. Cohen, Ruslan Salakhutdinov", "title": "End-to-End Multihop Retrieval for Compositional Question Answering over\n  Long Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering complex questions from long documents requires aggregating multiple\npieces of evidence and then predicting the answers. In this paper, we propose a\nmulti-hop retrieval method, DocHopper, to answer compositional questions over\nlong documents. At each step, DocHopper retrieves a paragraph or sentence\nembedding from the document, mixes the retrieved result with the query, and\nupdates the query for the next step. In contrast to many other retrieval-based\nmethods (e.g., RAG or REALM) the query is not augmented with a token sequence:\ninstead, it is augmented by \"numerically\" combining it with another neural\nrepresentation. This means that model is end-to-end differentiable. We\ndemonstrate that utilizing document structure in this was can largely improve\nquestion-answering and retrieval performance on long documents. We experimented\nwith DocHopper on three different QA tasks that require reading long documents\nto answer compositional questions: discourse entailment reasoning, factual QA\nwith table and text, and information seeking QA from academic papers. DocHopper\noutperforms all baseline models and achieves state-of-the-art results on all\ndatasets. Additionally, DocHopper is efficient at inference time, being 3~10\ntimes faster than the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 03:13:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Sun", "Haitian", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2106.00210", "submitter": "Zonghai Yao", "authors": "Zonghai Yao and Hong Yu", "title": "Improving Formality Style Transfer with Context-Aware Rule Injection", "comments": "ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models pre-trained on large-scale regular text corpora often do not work well\nfor user-generated data where the language styles differ significantly from the\nmainstream text. Here we present Context-Aware Rule Injection (CARI), an\ninnovative method for formality style transfer (FST). CARI injects multiple\nrules into an end-to-end BERT-based encoder and decoder model. It learns to\nselect optimal rules based on context. The intrinsic evaluation showed that\nCARI achieved the new highest performance on the FST benchmark dataset. Our\nextrinsic evaluation showed that CARI can greatly improve the regular\npre-trained models' performance on several tweet sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 03:59:07 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yao", "Zonghai", ""], ["Yu", "Hong", ""]]}, {"id": "2106.00218", "submitter": "Yucheng Wang", "authors": "Yucheng Wang, Bowen Yu, Hongsong Zhu, Tingwen Liu, Nan Yu and Limin\n  Sun", "title": "Discontinuous Named Entity Recognition as Maximal Clique Discovery", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Named entity recognition (NER) remains challenging when entity mentions can\nbe discontinuous. Existing methods break the recognition process into several\nsequential steps. In training, they predict conditioned on the golden\nintermediate results, while at inference relying on the model output of the\nprevious steps, which introduces exposure bias. To solve this problem, we first\nconstruct a segment graph for each sentence, in which each node denotes a\nsegment (a continuous entity on its own, or a part of discontinuous entities),\nand an edge links two nodes that belong to the same entity. The nodes and edges\ncan be generated respectively in one stage with a grid tagging scheme and\nlearned jointly using a novel architecture named Mac. Then discontinuous NER\ncan be reformulated as a non-parametric process of discovering maximal cliques\nin the graph and concatenating the spans in each clique. Experiments on three\nbenchmarks show that our method outperforms the state-of-the-art (SOTA)\nresults, with up to 3.5 percentage points improvement on F1, and achieves 5x\nspeedup over the SOTA model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:13:39 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Yucheng", ""], ["Yu", "Bowen", ""], ["Zhu", "Hongsong", ""], ["Liu", "Tingwen", ""], ["Yu", "Nan", ""], ["Sun", "Limin", ""]]}, {"id": "2106.00219", "submitter": "Shweta Yadav", "authors": "Shweta Yadav, Deepak Gupta, Asma Ben Abacha and Dina Demner-Fushman", "title": "Question-aware Transformer Models for Consumer Health Question\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Searching for health information online is becoming customary for more and\nmore consumers every day, which makes the need for efficient and reliable\nquestion answering systems more pressing. An important contributor to the\nsuccess rates of these systems is their ability to fully understand the\nconsumers' questions. However, these questions are frequently longer than\nneeded and mention peripheral information that is not useful in finding\nrelevant answers. Question summarization is one of the potential solutions to\nsimplifying long and complex consumer questions before attempting to find an\nanswer. In this paper, we study the task of abstractive summarization for\nreal-world consumer health questions. We develop an abstractive question\nsummarization model that leverages the semantic interpretation of a question\nvia recognition of medical entities, which enables the generation of\ninformative summaries. Towards this, we propose multiple Cloze tasks (i.e. the\ntask of filing missing words in a given context) to identify the key medical\nentities that enforce the model to have better coverage in question-focus\nrecognition. Additionally, we infuse the decoder inputs with question-type\ninformation to generate question-type driven summaries. When evaluated on the\nMeQSum benchmark corpus, our framework outperformed the state-of-the-art method\nby 10.2 ROUGE-L points. We also conducted a manual evaluation to assess the\ncorrectness of the generated summaries.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:21:31 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yadav", "Shweta", ""], ["Gupta", "Deepak", ""], ["Abacha", "Asma Ben", ""], ["Demner-Fushman", "Dina", ""]]}, {"id": "2106.00237", "submitter": "Nicolas Zampieri", "authors": "Nicolas Zampieri, Irina Illina and Dominique Fohr", "title": "Improving Automatic Hate Speech Detection with Multiword Expression\n  Features", "comments": "In Proceedings of NLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of automatically detecting hate speech in social media is gaining\nmore and more attention. Given the enormous volume of content posted daily,\nhuman monitoring of hate speech is unfeasible. In this work, we propose new\nword-level features for automatic hate speech detection (HSD): multiword\nexpressions (MWEs). MWEs are lexical units greater than a word that have\nidiomatic and compositional meanings. We propose to integrate MWE features in a\ndeep neural network-based HSD framework. Our baseline HSD system relies on\nUniversal Sentence Encoder (USE). To incorporate MWE features, we create a\nthree-branch deep neural network: one branch for USE, one for MWE categories,\nand one for MWE embeddings. We conduct experiments on two hate speech tweet\ncorpora with different MWE categories and with two types of MWE embeddings,\nword2vec and BERT. Our experiments demonstrate that the proposed HSD system\nwith MWE features significantly outperforms the baseline system in terms of\nmacro-F1.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:30:29 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zampieri", "Nicolas", ""], ["Illina", "Irina", ""], ["Fohr", "Dominique", ""]]}, {"id": "2106.00240", "submitter": "Kshitij Gupta", "authors": "Kshitij Gupta, Devansh Gautam, Radhika Mamidi", "title": "Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and\n  Images using Textual and Multimodal Ensemble", "comments": "7 pages, accepted at SemEval-2021 co-located with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memes are one of the most popular types of content used to spread information\nonline. They can influence a large number of people through rhetorical and\npsychological techniques. The task, Detection of Persuasion Techniques in Texts\nand Images, is to detect these persuasive techniques in memes. It consists of\nthree subtasks: (A) Multi-label classification using textual content, (B)\nMulti-label classification and span identification using textual content, and\n(C) Multi-label classification using visual and textual content. In this paper,\nwe propose a transfer learning approach to fine-tune BERT-based models in\ndifferent modalities. We also explore the effectiveness of ensembles of models\ntrained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1\nin the corresponding subtasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:41:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gupta", "Kshitij", ""], ["Gautam", "Devansh", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2106.00241", "submitter": "Shining Liang", "authors": "Shining Liang, Ming Gong, Jian Pei, Linjun Shou, Wanli Zuo, Xianglin\n  Zuo, Daxin Jiang", "title": "Reinforced Iterative Knowledge Distillation for Cross-Lingual Named\n  Entity Recognition", "comments": "KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Named entity recognition (NER) is a fundamental component in many\napplications, such as Web Search and Voice Assistants. Although deep neural\nnetworks greatly improve the performance of NER, due to the requirement of\nlarge amounts of training data, deep neural networks can hardly scale out to\nmany languages in an industry setting. To tackle this challenge, cross-lingual\nNER transfers knowledge from a rich-resource language to languages with low\nresources through pre-trained multilingual language models. Instead of using\ntraining data in target languages, cross-lingual NER has to rely on only\ntraining data in source languages, and optionally adds the translated training\ndata derived from source languages. However, the existing cross-lingual NER\nmethods do not make good use of rich unlabeled data in target languages, which\nis relatively easy to collect in industry applications. To address the\nopportunities and challenges, in this paper we describe our novel practice in\nMicrosoft to leverage such large amounts of unlabeled data in target languages\nin real production settings. To effectively extract weak supervision signals\nfrom the unlabeled data, we develop a novel approach based on the ideas of\nsemi-supervised learning and reinforcement learning. The empirical study on\nthree benchmark data sets verifies that our approach establishes the new\nstate-of-the-art performance with clear edges. Now, the NER techniques reported\nin this paper are on their way to become a fundamental component for Web\nranking, Entity Pane, Answers Triggering, and Question Answering in the\nMicrosoft Bing search engine. Moreover, our techniques will also serve as part\nof the Spoken Language Understanding module for a commercial voice assistant.\nWe plan to open source the code of the prototype framework after deployment.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:46:22 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liang", "Shining", ""], ["Gong", "Ming", ""], ["Pei", "Jian", ""], ["Shou", "Linjun", ""], ["Zuo", "Wanli", ""], ["Zuo", "Xianglin", ""], ["Jiang", "Daxin", ""]]}, {"id": "2106.00245", "submitter": "Linjie Li", "authors": "Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu", "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With large-scale pre-training, the past two years have witnessed significant\nperformance boost on the Visual Question Answering (VQA) task. Though rapid\nprogresses have been made, it remains unclear whether these state-of-the-art\n(SOTA) VQA models are robust when encountering test examples in the wild. To\nstudy this, we introduce Adversarial VQA, a new large-scale VQA benchmark,\ncollected iteratively via an adversarial human-and-model-in-the-loop procedure.\nThrough this new benchmark, we present several interesting findings. (i)\nSurprisingly, during dataset collection, we find that non-expert annotators can\nsuccessfully attack SOTA VQA models with relative ease. (ii) We test a variety\nof SOTA VQA models on our new dataset to highlight their fragility, and find\nthat both large-scale pre-trained models and adversarial training methods can\nonly achieve far lower performance than what they can achieve on the standard\nVQA v2 dataset. (iii) When considered as data augmentation, our dataset can be\nused to improve the performance on other robust VQA benchmarks. (iv) We present\na detailed analysis of the dataset, providing valuable insights on the\nchallenges it brings to the community. We hope Adversarial VQA can serve as a\nvaluable benchmark that will be used by future work to test the robustness of\nits developed VQA models. Our dataset is publicly available at\nhttps://adversarialvqa. github.io/.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:54:41 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Linjie", ""], ["Lei", "Jie", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""]]}, {"id": "2106.00248", "submitter": "Devansh Gautam", "authors": "Devansh Gautam, Kshitij Gupta, Manish Shrivastava", "title": "Volta at SemEval-2021 Task 9: Statement Verification and Evidence\n  Finding with Tables using TAPAS and Transfer Learning", "comments": "9 pages, accepted at SemEval-2021 co-located with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables are widely used in various kinds of documents to present information\nconcisely. Understanding tables is a challenging problem that requires an\nunderstanding of language and table structure, along with numerical and logical\nreasoning. In this paper, we present our systems to solve Task 9 of\nSemEval-2021: Statement Verification and Evidence Finding with Tables\n(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a\nstatement, predicting whether the table supports the statement and (B)\nPredicting which cells in the table provide evidence for/against the statement.\nWe fine-tune TAPAS (a model which extends BERT's architecture to capture\ntabular structure) for both the subtasks as it has shown state-of-the-art\nperformance in various table understanding tasks. In subtask A, we evaluate how\ntransfer learning and standardizing tables to have a single header row improves\nTAPAS' performance. In subtask B, we evaluate how different fine-tuning\nstrategies can improve TAPAS' performance. Our systems achieve an F1 score of\n67.34 in subtask A three-way classification, 72.89 in subtask A two-way\nclassification, and 62.95 in subtask B.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:06:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 15:37:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Gautam", "Devansh", ""], ["Gupta", "Kshitij", ""], ["Shrivastava", "Manish", ""]]}, {"id": "2106.00250", "submitter": "Kshitij Gupta", "authors": "Kshitij Gupta, Devansh Gautam, Radhika Mamidi", "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags", "comments": "7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:19:29 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:26:04 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 10:44:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gupta", "Kshitij", ""], ["Gautam", "Devansh", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2106.00257", "submitter": "Yu Wang", "authors": "Yu Wang, Hongxia Jin", "title": "A Coarse to Fine Question Answering System based on Reinforcement\n  Learning", "comments": "9 pages, original work published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a coarse to fine question answering (CFQA) system\nbased on reinforcement learning which can efficiently processes documents with\ndifferent lengths by choosing appropriate actions. The system is designed using\nan actor-critic based deep reinforcement learning model to achieve multi-step\nquestion answering. Compared to previous QA models targeting on datasets mainly\ncontaining either short or long documents, our multi-step coarse to fine model\ntakes the merits from multiple system modules, which can handle both short and\nlong documents. The system hence obtains a much better accuracy and faster\ntrainings speed compared to the current state-of-the-art models. We test our\nmodel on four QA datasets, WIKEREADING, WIKIREADING LONG, CNN and SQuAD, and\ndemonstrate 1.3$\\%$-1.7$\\%$ accuracy improvements with 1.5x-3.4x training\nspeed-ups in comparison to the baselines using state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:41:48 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Yu", ""], ["Jin", "Hongxia", ""]]}, {"id": "2106.00261", "submitter": "Jiang Hui", "authors": "Hui Jiang, Chulun Zhou, Fandong Meng, Biao Zhang, Jie Zhou, Degen\n  Huang, Qingqiang Wu, Jinsong Su", "title": "Exploring Dynamic Selection of Branch Expansion Orders for Code\n  Generation", "comments": "Accepted by ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the great potential in facilitating software development, code\ngeneration has attracted increasing attention recently. Generally, dominant\nmodels are Seq2Tree models, which convert the input natural language\ndescription into a sequence of tree-construction actions corresponding to the\npre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal\norder may not be suitable for handling all multi-branch nodes. In this paper,\nwe propose to equip the Seq2Tree model with a context-based Branch Selector,\nwhich is able to dynamically determine optimal expansion orders of branches for\nmulti-branch nodes. Particularly, since the selection of expansion orders is a\nnon-differentiable multi-step operation, we optimize the selector through\nreinforcement learning, and formulate the reward function as the difference of\nmodel losses obtained through different expansion orders. Experimental results\nand in-depth analysis on several commonly-used datasets demonstrate the\neffectiveness and generality of our approach. We have released our code at\nhttps://github.com/DeepLearnXMU/CG-RL.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:52:41 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Jiang", "Hui", ""], ["Zhou", "Chulun", ""], ["Meng", "Fandong", ""], ["Zhang", "Biao", ""], ["Zhou", "Jie", ""], ["Huang", "Degen", ""], ["Wu", "Qingqiang", ""], ["Su", "Jinsong", ""]]}, {"id": "2106.00291", "submitter": "Yinpei Dai", "authors": "Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si,\n  Xiaodan Zhu", "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for\n  Multi-Domain Dialog State Tracking", "comments": "7 pages, 2 figures, accepted to ACL21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing dialog state tracking (DST) models are trained with dialog data in a\nrandom order, neglecting rich structural information in a dataset. In this\npaper, we propose to use curriculum learning (CL) to better leverage both the\ncurriculum structure and schema structure for task-oriented dialogs.\nSpecifically, we propose a model-agnostic framework called Schema-aware\nCurriculum Learning for Dialog State Tracking (SaCLog), which consists of a\npreview module that pre-trains a DST model with schema information, a\ncurriculum module that optimizes the model with CL, and a review module that\naugments mispredicted data to reinforce the CL training. We show that our\nproposed approach improves DST performance over both a transformer-based and\nRNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art\nresults on WOZ2.0 and MultiWOZ2.1.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 07:52:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Dai", "Yinpei", ""], ["Li", "Hangyu", ""], ["Li", "Yongbin", ""], ["Sun", "Jian", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2106.00316", "submitter": "Zhenghao Wu", "authors": "Zhongyi Yu, Zhenghao Wu, Hao Zheng, Zhe XuanYuan, Jefferson Fong,\n  Weifeng Su", "title": "LenAtten: An Effective Length Controlling Unit For Text Summarization", "comments": "8 pages, accepted at Findings of ACL 2021 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed length summarization aims at generating summaries with a preset number\nof words or characters. Most recent researches incorporate length information\nwith word embeddings as the input to the recurrent decoding unit, causing a\ncompromise between length controllability and summary quality. In this work, we\npresent an effective length controlling unit Length Attention (LenAtten) to\nbreak this trade-off. Experimental results show that LenAtten not only brings\nimprovements in length controllability and ROGUE scores but also has great\ngeneralization ability. In the task of generating a summary with the target\nlength, our model is 732 times better than the best-performing length\ncontrollable summarizer in length controllability on the CNN/Daily Mail\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:45:41 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yu", "Zhongyi", ""], ["Wu", "Zhenghao", ""], ["Zheng", "Hao", ""], ["XuanYuan", "Zhe", ""], ["Fong", "Jefferson", ""], ["Su", "Weifeng", ""]]}, {"id": "2106.00320", "submitter": "Yongfeng Huang", "authors": "Yongfeng Huang, Yujun Chen, Yulun Du, Zhilin Yang", "title": "Distribution Matching for Rationalization", "comments": "Accepted by AAAI2021", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of rationalization aims to extract pieces of input text as\nrationales to justify neural network predictions on text classification tasks.\nBy definition, rationales represent key text pieces used for prediction and\nthus should have similar classification feature distribution compared to the\noriginal input text. However, previous methods mainly focused on maximizing the\nmutual information between rationales and labels while neglecting the\nrelationship between rationales and input text. To address this issue, we\npropose a novel rationalization method that matches the distributions of\nrationales and input text in both the feature space and output space.\nEmpirically, the proposed distribution matching approach consistently\noutperforms previous methods by a large margin. Our data and code are\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:49:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Huang", "Yongfeng", ""], ["Chen", "Yujun", ""], ["Du", "Yulun", ""], ["Yang", "Zhilin", ""]]}, {"id": "2106.00334", "submitter": "Chen Gong", "authors": "Chen Gong, Saihao Huang, Houquan Zhou, Zhenghua Li, Min Zhang, Zhefeng\n  Wang, Baoxing Huai, Nicholas Jing Yuan", "title": "An In-depth Study on Internal Structure of Chinese Words", "comments": "Accepted by ACL-IJCNLP 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike English letters, Chinese characters have rich and specific meanings.\nUsually, the meaning of a word can be derived from its constituent characters\nin some way. Several previous works on syntactic parsing propose to annotate\nshallow word-internal structures for better utilizing character-level\ninformation. This work proposes to model the deep internal structures of\nChinese words as dependency trees with 11 labels for distinguishing syntactic\nrelationships. First, based on newly compiled annotation guidelines, we\nmanually annotate a word-internal structure treebank (WIST) consisting of over\n30K multi-char words from Chinese Penn Treebank. To guarantee quality, each\nword is independently annotated by two annotators and inconsistencies are\nhandled by a third senior annotator. Second, we present detailed and\ninteresting analysis on WIST to reveal insights on Chinese word formation.\nThird, we propose word-internal structure parsing as a new task, and conduct\nbenchmark experiments using a competitive dependency parser. Finally, we\npresent two simple ways to encode word-internal structures, leading to\npromising gains on the sentence-level syntactic parsing task.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 09:09:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gong", "Chen", ""], ["Huang", "Saihao", ""], ["Zhou", "Houquan", ""], ["Li", "Zhenghua", ""], ["Zhang", "Min", ""], ["Wang", "Zhefeng", ""], ["Huai", "Baoxing", ""], ["Yuan", "Nicholas Jing", ""]]}, {"id": "2106.00352", "submitter": "Mark Anderson", "authors": "Mark Anderson and Anders S{\\o}gaard and Carlos G\\'omez Rodr\\'iguez", "title": "Replicating and Extending \"Because Their Treebanks Leak\": Graph\n  Isomorphism, Covariants, and Parser Performance", "comments": "To appear in the Proceedings of the 59th Annual Meeting of the\n  Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  S{\\o}gaard (2020) obtained results suggesting the fraction of trees occurring\nin the test data isomorphic to trees in the training set accounts for a\nnon-trivial variation in parser performance. Similar to other statistical\nanalyses in NLP, the results were based on evaluating linear regressions.\nHowever, the study had methodological issues and was undertaken using a small\nsample size leading to unreliable results. We present a replication study in\nwhich we also bin sentences by length and find that only a small subset of\nsentences vary in performance with respect to graph isomorphism. Further, the\ncorrelation observed between parser performance and graph isomorphism in the\nwild disappears when controlling for covariants. However, in a controlled\nexperiment, where covariants are kept fixed, we do observe a strong\ncorrelation. We suggest that conclusions drawn from statistical analyses like\nthis need to be tempered and that controlled experiments can complement them by\nmore readily teasing factors apart.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:00:46 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 07:18:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Anderson", "Mark", ""], ["S\u00f8gaard", "Anders", ""], ["Rodr\u00edguez", "Carlos G\u00f3mez", ""]]}, {"id": "2106.00400", "submitter": "Chenglei Si", "authors": "Chenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao Qi, Xiaozhi Wang,\n  Zhiyuan Liu, Maosong Sun", "title": "SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language\n  Model Pretraining", "comments": "Work in progress. Feedback is welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional tokenization methods for Chinese pretrained language models\n(PLMs) treat each character as an indivisible token (Devlin et al., 2019),\nwhich ignores the characteristics of the Chinese writing system. In this work,\nwe comprehensively study the influences of three main factors on the Chinese\ntokenization for PLM: pronunciation, glyph (i.e., shape), and word boundary.\nCorrespondingly, we propose three kinds of tokenizers: 1) SHUOWEN (meaning Talk\nWord), the pronunciation-based tokenizers; 2) JIEZI (meaning Solve Character),\nthe glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with\nChinese word segmentation. To empirically compare the effectiveness of studied\ntokenizers, we pretrain BERT-style language models with them and evaluate the\nmodels on various downstream NLU tasks. We find that SHUOWEN and JIEZI\ntokenizers can generally outperform conventional single-character tokenizers,\nwhile Chinese word segmentation shows no benefit as a preprocessing step.\nMoreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly\nbetter robustness in handling noisy texts. The code and pretrained models will\nbe publicly released to facilitate linguistically informed Chinese NLP.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:20:02 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Si", "Chenglei", ""], ["Zhang", "Zhengyan", ""], ["Chen", "Yingfa", ""], ["Qi", "Fanchao", ""], ["Wang", "Xiaozhi", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2106.00410", "submitter": "Genta Indra Winata", "authors": "Genta Indra Winata, Holy Lovenia, Etsuko Ishii, Farhad Bin Siddique,\n  Yongsheng Yang, Pascale Fung", "title": "Nora: The Well-Being Coach", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current pandemic has forced people globally to remain in isolation and\npractice social distancing, which creates the need for a system to combat the\nresulting loneliness and negative emotions. In this paper we propose Nora, a\nvirtual coaching platform designed to utilize natural language understanding in\nits dialogue system and suggest other recommendations based on user\ninteractions. It is intended to provide assistance and companionship to people\nundergoing self-quarantine or work-from-home routines. Nora helps users gauge\ntheir well-being by detecting and recording the user's emotion, sentiment, and\nstress. Nora also recommends various workout, meditation, or yoga exercises to\nusers in support of developing a healthy daily routine. In addition, we provide\na social community inside Nora, where users can connect and share their\nexperiences with others undergoing a similar isolation procedure. Nora can be\naccessed from anywhere via a web link and has support for both English and\nMandarin.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:42:07 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Winata", "Genta Indra", ""], ["Lovenia", "Holy", ""], ["Ishii", "Etsuko", ""], ["Siddique", "Farhad Bin", ""], ["Yang", "Yongsheng", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.00420", "submitter": "Yi Xu", "authors": "Yi Xu, Hai Zhao", "title": "Dialogue-oriented Pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (PrLM) has been shown powerful in enhancing a\nbroad range of downstream tasks including various dialogue related ones.\nHowever, PrLMs are usually trained on general plain text with common language\nmodel (LM) training objectives, which cannot sufficiently capture dialogue\nexclusive features due to the limitation of such training setting, so that\nthere is an immediate need to fill the gap between a specific dialogue task and\nthe LM task. As it is unlikely to collect huge dialogue data for\ndialogue-oriented pre-training, in this paper, we propose three strategies to\nsimulate the conversation features on general plain text. Our proposed method\ndiffers from existing post-training methods that it may yield a general-purpose\nPrLM and does not individualize to any detailed task while keeping the\ncapability of learning dialogue related features including speaker awareness,\ncontinuity and consistency. The resulted Dialog-PrLM is fine-tuned on three\npublic multi-turn dialogue datasets and helps achieve significant and\nconsistent improvement over the plain PrLMs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 12:02:46 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Xu", "Yi", ""], ["Zhao", "Hai", ""]]}, {"id": "2106.00451", "submitter": "Fan Huang", "authors": "Fan Huang", "title": "Highlight Timestamp Detection Model for Comedy Videos via Multimodal\n  Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the videos on the Internet are prevailing. The precise and in-depth\nunderstanding of the videos is a difficult but valuable problem for both\nplatforms and researchers. The existing video understand models do well in\nobject recognition tasks but currently still cannot understand the abstract and\ncontextual features like highlight humor frames in comedy videos. The current\nindustrial works are also mainly focused on the basic category classification\ntask based on the appearances of objects. The feature detection methods for the\nabstract category remains blank. A data structure that includes the information\nof video frames, audio spectrum and texts provide a new direction to explore.\nThe multimodal models are proposed to make this in-depth video understanding\nmission possible. In this paper, we analyze the difficulties in abstract\nunderstanding of videos and propose a multimodal structure to obtain\nstate-of-the-art performance in this field. Then we select several benchmarks\nfor multimodal video understanding and apply the most suitable model to find\nthe best performance. At last, we evaluate the overall spotlights and drawbacks\nof the models and methods in this paper and point out the possible directions\nfor further improvements.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:39:19 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Huang", "Fan", ""]]}, {"id": "2106.00459", "submitter": "Kuldeep Singh", "authors": "Abhishek Nadgeri, Anson Bastos, Kuldeep Singh, Isaiah Onando Mulang',\n  Johannes Hoffart, Saeedeh Shekarpour, Vijay Saraswat", "title": "KGPool: Dynamic Knowledge Graph Context Selection for Relation\n  Extraction", "comments": "ACL 2021 (findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel method for relation extraction (RE) from a single\nsentence, mapping the sentence and two given entities to a canonical fact in a\nknowledge graph (KG). Especially in this presumed sentential RE setting, the\ncontext of a single sentence is often sparse. This paper introduces the KGPool\nmethod to address this sparsity, dynamically expanding the context with\nadditional facts from the KG. It learns the representation of these facts\n(entity alias, entity descriptions, etc.) using neural methods, supplementing\nthe sentential context. Unlike existing methods that statically use all\nexpanded facts, KGPool conditions this expansion on the sentence. We study the\nefficacy of KGPool by evaluating it with different neural models and KGs\n(Wikidata and NYT Freebase). Our experimental evaluation on standard datasets\nshows that by feeding the KGPool representation into a Graph Neural Network,\nthe overall method is significantly more accurate than state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:12:24 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 20:48:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Nadgeri", "Abhishek", ""], ["Bastos", "Anson", ""], ["Singh", "Kuldeep", ""], ["Mulang'", "Isaiah Onando", ""], ["Hoffart", "Johannes", ""], ["Shekarpour", "Saeedeh", ""], ["Saraswat", "Vijay", ""]]}, {"id": "2106.00473", "submitter": "Matthew Shardlow", "authors": "Matthew Shardlow, Richard Evans, Gustavo Henrique Paetzold, Marcos\n  Zampieri", "title": "SemEval-2021 Task 1: Lexical Complexity Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results and main findings of SemEval-2021 Task 1 -\nLexical Complexity Prediction. We provided participants with an augmented\nversion of the CompLex Corpus (Shardlow et al 2020). CompLex is an English\nmulti-domain corpus in which words and multi-word expressions (MWEs) were\nannotated with respect to their complexity using a five point Likert scale.\nSemEval-2021 Task 1 featured two Sub-tasks: Sub-task 1 focused on single words\nand Sub-task 2 focused on MWEs. The competition attracted 198 teams in total,\nof which 54 teams submitted official runs on the test data to Sub-task 1 and 37\nto Sub-task 2.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:22:36 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Shardlow", "Matthew", ""], ["Evans", "Richard", ""], ["Paetzold", "Gustavo Henrique", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2106.00479", "submitter": "Syrine Krichene", "authors": "Syrine Krichene, Thomas M\\\"uller and Julian Martin Eisenschlos", "title": "DoT: An efficient Double Transformer for NLP tasks with tables", "comments": "11 pages, 4 figures, to be published in Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based approaches have been successfully used to obtain\nstate-of-the-art accuracy on natural language processing (NLP) tasks with\nsemi-structured tables. These model architectures are typically deep, resulting\nin slow training and inference, especially for long inputs. To improve\nefficiency while maintaining a high accuracy, we propose a new architecture,\nDoT, a double transformer model, that decomposes the problem into two\nsub-tasks: A shallow pruning transformer that selects the top-K tokens,\nfollowed by a deep task-specific transformer that takes as input those K\ntokens. Additionally, we modify the task-specific attention to incorporate the\npruning scores. The two transformers are jointly trained by optimizing the\ntask-specific loss. We run experiments on three benchmarks, including\nentailment and question-answering. We show that for a small drop of accuracy,\nDoT improves training and inference time by at least 50%. We also show that the\npruning transformer effectively selects relevant tokens enabling the end-to-end\nmodel to maintain similar accuracy as slower baseline models. Finally, we\nanalyse the pruning and give some insight into its impact on the task model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:33:53 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Krichene", "Syrine", ""], ["M\u00fcller", "Thomas", ""], ["Eisenschlos", "Julian Martin", ""]]}, {"id": "2106.00507", "submitter": "Zheng Ye", "authors": "Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, Xiaodan Liang", "title": "Towards Quantifiable Dialogue Coherence Evaluation", "comments": "Long paper; ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic dialogue coherence evaluation has attracted increasing attention\nand is crucial for developing promising dialogue systems. However, existing\nmetrics have two major limitations: (a) they are mostly trained in a simplified\ntwo-level setting (coherent vs. incoherent), while humans give Likert-type\nmulti-level coherence scores, dubbed as \"quantifiable\"; (b) their predicted\ncoherence scores cannot align with the actual human rating standards due to the\nabsence of human guidance during training. To address these limitations, we\npropose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel\nframework aiming to train a quantifiable dialogue coherence metric that can\nreflect the actual human rating standards. Specifically, QuantiDCE includes two\ntraining stages, Multi-Level Ranking (MLR) pre-training and Knowledge\nDistillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is\nproposed for enabling the model to learn the coarse judgement of coherence\ndegrees. Then, during KD fine-tuning, the pretrained model is further finetuned\nto learn the actual human rating standards with only very few human-annotated\ndata. To advocate the generalizability even with limited fine-tuning data, a\nnovel KD regularization is introduced to retain the knowledge learned at the\npre-training stage. Experimental results show that the model trained by\nQuantiDCE presents stronger correlations with human judgements than the other\nstate-of-the-art metrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:11:17 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 14:30:12 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ye", "Zheng", ""], ["Lu", "Liucun", ""], ["Huang", "Lishan", ""], ["Lin", "Liang", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2106.00510", "submitter": "Deepanway Ghosal", "authors": "Deepanway Ghosal and Pengfei Hong and Siqi Shen and Navonil Majumder\n  and Rada Mihalcea and Soujanya Poria", "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning", "comments": "SIGDIAL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:14:46 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 02:47:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ghosal", "Deepanway", ""], ["Hong", "Pengfei", ""], ["Shen", "Siqi", ""], ["Majumder", "Navonil", ""], ["Mihalcea", "Rada", ""], ["Poria", "Soujanya", ""]]}, {"id": "2106.00590", "submitter": "Jialu Liu", "authors": "Jialu Liu, Tianqi Liu, Cong Yu", "title": "NewsEmbed: Modeling News through Pre-trained Document Representations", "comments": "Accepted in SIGKDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467392", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effectively modeling text-rich fresh content such as news articles at\ndocument-level is a challenging problem. To ensure a content-based model\ngeneralize well to a broad range of applications, it is critical to have a\ntraining dataset that is large beyond the scale of human labels while achieving\ndesired quality. In this work, we address those two challenges by proposing a\nnovel approach to mine semantically-relevant fresh documents, and their topic\nlabels, with little human supervision. Meanwhile, we design a multitask model\ncalled NewsEmbed that alternatively trains a contrastive learning with a\nmulti-label classification to derive a universal document encoder. We show that\nthe proposed approach can provide billions of high quality organic training\nexamples and can be naturally extended to multilingual setting where texts in\ndifferent languages are encoded in the same semantic space. We experimentally\ndemonstrate NewsEmbed's competitive performance across multiple natural\nlanguage understanding tasks, both supervised and unsupervised.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:59:40 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 03:00:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Jialu", ""], ["Liu", "Tianqi", ""], ["Yu", "Cong", ""]]}, {"id": "2106.00641", "submitter": "Jinlan Fu", "authors": "Jinlan Fu, Xuanjing Huang, Pengfei Liu", "title": "SpanNER: Named Entity Re-/Recognition as Span Prediction", "comments": "Accepted by ACL 2021 (Main track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen the paradigm shift of Named Entity Recognition (NER)\nsystems from sequence labeling to span prediction. Despite its preliminary\neffectiveness, the span prediction model's architectural bias has not been\nfully understood. In this paper, we first investigate the strengths and\nweaknesses when the span prediction model is used for named entity recognition\ncompared with the sequence labeling framework and how to further improve it,\nwhich motivates us to make complementary advantages of systems based on\ndifferent paradigms. We then reveal that span prediction, simultaneously, can\nserve as a system combiner to re-recognize named entities from different\nsystems' outputs. We experimentally implement 154 systems on 11 datasets,\ncovering three languages, comprehensive results show the effectiveness of span\nprediction models that both serve as base NER systems and system combiners. We\nmake all code and datasets available: \\url{https://github.com/neulab/spanner},\nas well as an online system demo: \\url{http://spanner.sh}. Our model also has\nbeen deployed into the ExplainaBoard platform, which allows users to flexibly\nperform a system combination of top-scoring systems in an interactive way:\n\\url{http://explainaboard.nlpedia.ai/leaderboard/task-ner/}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:11:42 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 04:54:23 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Fu", "Jinlan", ""], ["Huang", "Xuanjing", ""], ["Liu", "Pengfei", ""]]}, {"id": "2106.00665", "submitter": "Joshua Myszewski", "authors": "Joshua J Myszewski, Emily Klossowski, Patrick Meyer, Kristin Bevil,\n  Lisa Klesius, Kristopher M Schroeder", "title": "Validating GAN-BioBERT: A Methodology For Assessing Reporting Trends In\n  Clinical Trials", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past decade, there has been much discussion about the issue of biased\nreporting in clinical research. Despite this attention, there have been limited\ntools developed for the systematic assessment of qualitative statements made in\nclinical research, with most studies assessing qualitative statements relying\non the use of manual expert raters, which limits their size. Also, previous\nattempts to develop larger scale tools, such as those using natural language\nprocessing, were limited by both their accuracy and the number of categories\nused for the classification of their findings. With these limitations in mind,\nthis study's goal was to develop a classification algorithm that was both\nsuitably accurate and finely grained to be applied on a large scale for\nassessing the qualitative sentiment expressed in clinical trial abstracts.\nAdditionally, this study seeks to compare the performance of the proposed\nalgorithm, GAN-BioBERT, to previous studies as well as to expert manual rating\nof clinical trial abstracts. This study develops a three-class sentiment\nclassification algorithm for clinical trial abstracts using a semi-supervised\nnatural language process model based on the Bidirectional Encoder\nRepresentation from Transformers (BERT) model, from a series of clinical trial\nabstracts annotated by a group of experts in academic medicine. Results: The\nuse of this algorithm was found to have a classification accuracy of 91.3%,\nwith a macro F1-Score of 0.92, which is a significant improvement in accuracy\nwhen compared to previous methods and expert ratings, while also making the\nsentiment classification finer grained than previous studies. The proposed\nalgorithm, GAN-BioBERT, is a suitable classification model for the large-scale\nassessment of qualitative statements in clinical trial literature, providing an\naccurate, reproducible tool for the large-scale study of clinical publication\ntrends.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:51:54 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Myszewski", "Joshua J", ""], ["Klossowski", "Emily", ""], ["Meyer", "Patrick", ""], ["Bevil", "Kristin", ""], ["Klesius", "Lisa", ""], ["Schroeder", "Kristopher M", ""]]}, {"id": "2106.00676", "submitter": "Zejiang Shen", "authors": "Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S. Weld,\n  Doug Downey", "title": "Incorporating Visual Layout Structures for Scientific Text\n  Classification", "comments": "13 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifying the core textual components of a scientific paper-title, author,\nbody text, etc.-is a critical first step in automated scientific document\nunderstanding. Previous work has shown how using elementary layout information,\ni.e., each token's 2D position on the page, leads to more accurate\nclassification. We introduce new methods for incorporating VIsual LAyout (VILA)\nstructures, e.g., the grouping of page texts into text lines or text blocks,\ninto language models to further improve performance. We show that the I-VILA\napproach, which simply adds special tokens denoting the boundaries of layout\nstructures into model inputs, can lead to 1.9% Macro F1 improvements for token\nclassification. Moreover, we design a hierarchical model, H-VILA, that encodes\nthe text based on layout structures and record an up-to 47% inference time\nreduction with less than 1.5% Macro F1 loss for the text classification models.\nExperiments are conducted on a newly curated evaluation suite, S2-VLUE, with a\nnovel metric measuring classification uniformity within visual groups and a new\ndataset of gold annotations covering papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code will be available at\nhttps://github.com/allenai/VILA.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:59:00 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:35:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shen", "Zejiang", ""], ["Lo", "Kyle", ""], ["Wang", "Lucy Lu", ""], ["Kuehl", "Bailey", ""], ["Weld", "Daniel S.", ""], ["Downey", "Doug", ""]]}, {"id": "2106.00737", "submitter": "Belinda Z. Li", "authors": "Belinda Z. Li, Maxwell Nye, Jacob Andreas", "title": "Implicit Representations of Meaning in Neural Language Models", "comments": "15 pages, 6 figures; accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does the effectiveness of neural language models derive entirely from\naccurate modeling of surface word co-occurrence statistics, or do these models\nrepresent and reason about the world they describe? In BART and T5 transformer\nlanguage models, we identify contextual word representations that function as\nmodels of entities and situations as they evolve throughout a discourse. These\nneural representations have functional similarities to linguistic models of\ndynamic semantics: they support a linear readout of each entity's current\nproperties and relations, and can be manipulated with predictable effects on\nlanguage generation. Our results indicate that prediction in pretrained neural\nlanguage models is supported, at least in part, by dynamic representations of\nmeaning and implicit simulation of entity state, and that this behavior can be\nlearned with only text as training data. Code and data are available at\nhttps://github.com/belindal/state-probes .\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:23:20 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Li", "Belinda Z.", ""], ["Nye", "Maxwell", ""], ["Andreas", "Jacob", ""]]}, {"id": "2106.00742", "submitter": "Md Saroar Jahan", "authors": "Md Saroar Jahan, Mourad Oussalah", "title": "A systematic review of Hate Speech automatic detection using Natural\n  Language Processing", "comments": "34 pages, 22 Figure, 14 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the multiplication of social media platforms, which offer anonymity,\neasy access and online community formation, and online debate, the issue of\nhate speech detection and tracking becomes a growing challenge to society,\nindividual, policy-makers and researchers. Despite efforts for leveraging\nautomatic techniques for automatic detection and monitoring, their performances\nare still far from satisfactory, which constantly calls for future research on\nthe issue. This paper provides a systematic review of literature in this field,\nwith a focus on natural language processing and deep learning technologies,\nhighlighting the terminology, processing pipeline, core methods employed, with\na focal point on deep learning architecture. From a methodological perspective,\nwe adopt PRISMA guideline of systematic review of the last 10 years literature\nfrom ACM Digital Library and Google Scholar. In the sequel, existing surveys,\nlimitations, and future research directions are extensively discussed.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 21:48:14 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jahan", "Md Saroar", ""], ["Oussalah", "Mourad", ""]]}, {"id": "2106.00745", "submitter": "Ofek Rafaeli", "authors": "Ofek Rafaeli, Omri Abend, Leshem Choshen, Dmitry Nikolaev", "title": "Part of Speech and Universal Dependency effects on English Arabic\n  Machine Translation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this research paper, I will elaborate on a method to evaluate machine\ntranslation models based on their performance on underlying syntactical\nphenomena between English and Arabic languages. This method is especially\nimportant as such \"neural\" and \"machine learning\" are hard to fine-tune and\nchange. Thus, finding a way to evaluate them easily and diversely would greatly\nhelp the task of bettering them.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:48:23 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 11:24:28 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Rafaeli", "Ofek", ""], ["Abend", "Omri", ""], ["Choshen", "Leshem", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "2106.00749", "submitter": "Ran Zmigrod", "authors": "Ran Zmigrod, Tim Vieira, Ryan Cotterell", "title": "Higher-order Derivatives of Weighted Finite-state Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weighted finite-state machines are a fundamental building block of NLP\nsystems. They have withstood the test of time -- from their early use in noisy\nchannel models in the 1990s up to modern-day neurally parameterized conditional\nrandom fields. This work examines the computation of higher-order derivatives\nwith respect to the normalization constant for weighted finite-state machines.\nWe provide a general algorithm for evaluating derivatives of all orders, which\nhas not been previously described in the literature. In the case of\nsecond-order derivatives, our scheme runs in the optimal $\\mathcal{O}(A^2 N^4)$\ntime where $A$ is the alphabet size and $N$ is the number of states. Our\nalgorithm is significantly faster than prior algorithms. Additionally, our\napproach leads to a significantly faster algorithm for computing second-order\nexpectations, such as covariance matrices and gradients of first-order\nexpectations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:51:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zmigrod", "Ran", ""], ["Vieira", "Tim", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.00780", "submitter": "Ran Zmigrod", "authors": "Ran Zmigrod, Tim Vieira, Ryan Cotterell", "title": "On Finding the $K$-best Non-projective Dependency Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The connection between the maximum spanning tree in a directed graph and the\nbest dependency tree of a sentence has been exploited by the NLP community.\nHowever, for many dependency parsing schemes, an important detail of this\napproach is that the spanning tree must have exactly one edge emanating from\nthe root. While work has been done to efficiently solve this problem for\nfinding the one-best dependency tree, no research has attempted to extend this\nsolution to finding the $K$-best dependency trees. This is arguably a more\nimportant extension as a larger proportion of decoded trees will not be subject\nto the root constraint of dependency trees. Indeed, we show that the rate of\nroot constraint violations increases by an average of $13$ times when decoding\nwith $K\\!=\\!50$ as opposed to $K\\!=\\!1$. In this paper, we provide a\nsimplification of the $K$-best spanning tree algorithm of Camerini et al.\n(1980). Our simplification allows us to obtain a constant time speed-up over\nthe original algorithm. Furthermore, we present a novel extension of the\nalgorithm for decoding the $K$-best dependency trees of a graph which are\nsubject to a root constraint.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:23:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zmigrod", "Ran", ""], ["Vieira", "Tim", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.00786", "submitter": "Peter Hase", "authors": "Peter Hase, Harry Xie, Mohit Bansal", "title": "Search Methods for Sufficient, Socially-Aligned Feature Importance\n  Explanations with In-Distribution Counterfactuals", "comments": "26 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature importance (FI) estimates are a popular form of explanation, and they\nare commonly created and evaluated by computing the change in model confidence\ncaused by removing certain input features at test time. For example, in the\nstandard Sufficiency metric, only the top-k most important tokens are kept. In\nthis paper, we study several under-explored dimensions of FI-based\nexplanations, providing conceptual and empirical improvements for this form of\nexplanation. First, we advance a new argument for why it can be problematic to\nremove features from an input when creating or evaluating explanations: the\nfact that these counterfactual inputs are out-of-distribution (OOD) to models\nimplies that the resulting explanations are socially misaligned. The crux of\nthe problem is that the model prior and random weight initialization influence\nthe explanations (and explanation metrics) in unintended ways. To resolve this\nissue, we propose a simple alteration to the model training process, which\nresults in more socially aligned explanations and metrics. Second, we compare\namong five approaches for removing features from model inputs. We find that\nsome methods produce more OOD counterfactuals than others, and we make\nrecommendations for selecting a feature-replacement function. Finally, we\nintroduce four search-based methods for identifying FI explanations and compare\nthem to strong baselines, including LIME, Integrated Gradients, and random\nsearch. On experiments with six diverse text classification datasets, we find\nthat the only method that consistently outperforms random search is a Parallel\nLocal Search that we introduce. Improvements over the second-best method are as\nlarge as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All\nsupporting code is publicly available at\nhttps://github.com/peterbhase/ExplanationSearch.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:36:48 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hase", "Peter", ""], ["Xie", "Harry", ""], ["Bansal", "Mohit", ""]]}, {"id": "2106.00791", "submitter": "Xinyu Hua", "authors": "Xinyu Hua, Ashwin Sreevatsa, and Lu Wang", "title": "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text\n  Generation", "comments": "Accepted at ACL 2021. Project page:\n  https://xinyuhua.github.io/Resources/acl21/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of long-form opinion text generation, which faces at least\ntwo distinct challenges. First, existing neural generation models fall short of\ncoherence, thus requiring efficient content planning. Second, diverse types of\ninformation are needed to guide the generator to cover both subjective and\nobjective content. To this end, we propose DYPLOC, a generation framework that\nconducts dynamic planning of content while generating the output based on a\nnovel design of mixed language models. To enrich the generation with diverse\ncontent, we further propose to use large pre-trained models to predict relevant\nconcepts and to generate claims. We experiment with two challenging tasks on\nnewly collected datasets: (1) argument generation with Reddit ChangeMyView, and\n(2) writing articles using New York Times' Opinion section. Automatic\nevaluation shows that our model significantly outperforms competitive\ncomparisons. Human judges further confirm that our generations are more\ncoherent with richer content.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:56:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hua", "Xinyu", ""], ["Sreevatsa", "Ashwin", ""], ["Wang", "Lu", ""]]}, {"id": "2106.00793", "submitter": "Zhengbao Jiang", "authors": "Zhengbao Jiang, Jialong Han, Bunyamin Sisman, Xin Luna Dong", "title": "CoRI: Collective Relation Integration with Data Augmentation for Open\n  Information Extraction", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating extracted knowledge from the Web to knowledge graphs (KGs) can\nfacilitate tasks like question answering. We study relation integration that\naims to align free-text relations in subject-relation-object extractions to\nrelations in a target KG. To address the challenge that free-text relations are\nambiguous, previous methods exploit neighbor entities and relations for\nadditional context. However, the predictions are made independently, which can\nbe mutually inconsistent. We propose a two-stage Collective Relation\nIntegration (CoRI) model, where the first stage independently makes candidate\npredictions, and the second stage employs a collective model that accesses all\ncandidate predictions to make globally coherent predictions. We further improve\nthe collective model with augmented data from the portion of the target KG that\nis otherwise unused. Experiment results on two datasets show that CoRI can\nsignificantly outperform the baselines, improving AUC from .677 to .748 and\nfrom .716 to .780, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:01:43 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jiang", "Zhengbao", ""], ["Han", "Jialong", ""], ["Sisman", "Bunyamin", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2106.00794", "submitter": "Nikita Nangia", "authors": "Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara\n  Vania, Samuel R. Bowman", "title": "What Ingredients Make for an Effective Crowdsourcing Protocol for\n  Difficult NLU Data Collection Tasks?", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is widely used to create data for common natural language\nunderstanding tasks. Despite the importance of these datasets for measuring and\nrefining model understanding of language, there has been little focus on the\ncrowdsourcing methods used for collecting the datasets. In this paper, we\ncompare the efficacy of interventions that have been proposed in prior work as\nways of improving data quality. We use multiple-choice question answering as a\ntestbed and run a randomized trial by assigning crowdworkers to write questions\nunder one of four different data collection protocols. We find that asking\nworkers to write explanations for their examples is an ineffective stand-alone\nstrategy for boosting NLU example difficulty. However, we find that training\ncrowdworkers, and then using an iterative process of collecting data, sending\nfeedback, and qualifying workers based on expert judgments is an effective\nmeans of collecting challenging data. But using crowdsourced, instead of expert\njudgments, to qualify workers and send feedback does not prove to be effective.\nWe observe that the data from the iterative protocol with expert assessments is\nmore challenging by several measures. Notably, the human--model gap on the\nunanimous agreement portion of this data is, on average, twice as large as the\ngap for the baseline protocol data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:05:52 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nangia", "Nikita", ""], ["Sugawara", "Saku", ""], ["Trivedi", "Harsh", ""], ["Warstadt", "Alex", ""], ["Vania", "Clara", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2106.00829", "submitter": "Alexander Fabbri", "authors": "Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li,\n  Yashar Mehdad, Dragomir Radev", "title": "ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive\n  Summarization with Argument Mining", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While online conversations can cover a vast amount of information in many\ndifferent formats, abstractive text summarization has primarily focused on\nmodeling solely news articles. This research gap is due, in part, to the lack\nof standardized datasets for summarizing online discussions. To address this\ngap, we design annotation protocols motivated by an\nissues--viewpoints--assertions framework to crowdsource four new datasets on\ndiverse online conversation forms of news comments, discussion forums,\ncommunity question answering forums, and email threads. We benchmark\nstate-of-the-art models on our datasets and analyze characteristics associated\nwith the data. To create a comprehensive benchmark, we also evaluate these\nmodels on widely-used conversation summarization datasets to establish strong\nbaselines in this domain. Furthermore, we incorporate argument mining through\ngraph construction to directly model the issues, viewpoints, and assertions\npresent in a conversation and filter noisy input, showing comparable or\nimproved results according to automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 22:17:13 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fabbri", "Alexander R.", ""], ["Rahman", "Faiaz", ""], ["Rizvi", "Imad", ""], ["Wang", "Borui", ""], ["Li", "Haoran", ""], ["Mehdad", "Yashar", ""], ["Radev", "Dragomir", ""]]}, {"id": "2106.00840", "submitter": "Clara Vania", "authors": "Clara Vania, Phu Mon Htut, William Huang, Dhara Mungra, Richard\n  Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, Samuel R. Bowman", "title": "Comparing Test Sets with Item Response Theory", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen numerous NLP datasets introduced to evaluate the\nperformance of fine-tuned models on natural language understanding tasks.\nRecent results from large pretrained models, though, show that many of these\ndatasets are largely saturated and unlikely to be able to detect further\nprogress. What kind of datasets are still effective at discriminating among\nstrong models, and what kind of datasets should we expect to be able to detect\nfuture improvements? To measure this uniformly across datasets, we draw on Item\nResponse Theory and evaluate 29 datasets using predictions from 18 pretrained\nTransformer models on individual test examples. We find that Quoref, HellaSwag,\nand MC-TACO are best suited for distinguishing among state-of-the-art models,\nwhile SNLI, MNLI, and CommitmentBank seem to be saturated for current strong\nmodels. We also observe span selection task format, which is used for QA\ndatasets like QAMR or SQuAD2.0, is effective in differentiating between strong\nand weak models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 22:33:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vania", "Clara", ""], ["Htut", "Phu Mon", ""], ["Huang", "William", ""], ["Mungra", "Dhara", ""], ["Pang", "Richard Yuanzhe", ""], ["Phang", "Jason", ""], ["Liu", "Haokun", ""], ["Cho", "Kyunghyun", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2106.00851", "submitter": "Louis Castricato", "authors": "Louis Castricato, Stephen Fitz, Won Young Shin", "title": "Parameter-Efficient Neural Question Answering Models via Graph-Enriched\n  Document Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the computational footprint of modern NLP systems grows, it becomes\nincreasingly important to arrive at more efficient models. We show that by\nemploying graph convolutional document representation, we can arrive at a\nquestion answering system that performs comparably to, and in some cases\nexceeds the SOTA solutions, while using less than 5\\% of their resources in\nterms of trainable parameters. As it currently stands, a major issue in\napplying GCNs to NLP is document representation. In this paper, we show that a\nGCN enriched document representation greatly improves the results seen in\nHotPotQA, even when using a trivial topology. Our model (gQA), performs\nadmirably when compared to the current SOTA, and requires little to no\npreprocessing. In Shao et al. 2020, the authors suggest that graph networks are\nnot necessary for good performance in multi-hop QA. In this paper, we suggest\nthat large language models are not necessary for good performance by showing a\nna\\\"{i}ve implementation of a GCN performs comparably to SoTA models based on\npretrained language models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:24:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Castricato", "Louis", ""], ["Fitz", "Stephen", ""], ["Shin", "Won Young", ""]]}, {"id": "2106.00853", "submitter": "Ashkan Kazemi", "authors": "Ashkan Kazemi, Kiran Garimella, Devin Gaffney and Scott A. Hale", "title": "Claim Matching Beyond English to Scale Global Fact-Checking", "comments": "to appear in ACL 2021 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual fact-checking does not scale well to serve the needs of the internet.\nThis issue is further compounded in non-English contexts. In this paper, we\ndiscuss claim matching as a possible solution to scale fact-checking. We define\nclaim matching as the task of identifying pairs of textual messages containing\nclaims that can be served with one fact-check. We construct a novel dataset of\nWhatsApp tipline and public group messages alongside fact-checked claims that\nare first annotated for containing \"claim-like statements\" and then matched\nwith potentially similar items and annotated for claim matching. Our dataset\ncontains content in high-resource (English, Hindi) and lower-resource (Bengali,\nMalayalam, Tamil) languages. We train our own embedding model using knowledge\ndistillation and a high-quality \"teacher\" model in order to address the\nimbalance in embedding quality between the low- and high-resource languages in\nour dataset. We provide evaluations on the performance of our solution and\ncompare with baselines and existing state-of-the-art multilingual embedding\nmodels, namely LASER and LaBSE. We demonstrate that our performance exceeds\nLASER and LaBSE in all settings. We release our annotated datasets, codebooks,\nand trained embedding model to allow for further research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:28:05 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kazemi", "Ashkan", ""], ["Garimella", "Kiran", ""], ["Gaffney", "Devin", ""], ["Hale", "Scott A.", ""]]}, {"id": "2106.00872", "submitter": "Divyansh Kaushik", "authors": "Divyansh Kaushik, Douwe Kiela, Zachary C. Lipton, Wen-tau Yih", "title": "On the Efficacy of Adversarial Data Collection for Question Answering:\n  Results from a Large-Scale Randomized Study", "comments": "Accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adversarial data collection (ADC), a human workforce interacts with a\nmodel in real time, attempting to produce examples that elicit incorrect\npredictions. Researchers hope that models trained on these more challenging\ndatasets will rely less on superficial patterns, and thus be less brittle.\nHowever, despite ADC's intuitive appeal, it remains unclear when training on\nadversarial datasets produces more robust models. In this paper, we conduct a\nlarge-scale controlled study focused on question answering, assigning workers\nat random to compose questions either (i) adversarially (with a model in the\nloop); or (ii) in the standard fashion (without a model). Across a variety of\nmodels and datasets, we find that models trained on adversarial data usually\nperform better on other adversarial datasets but worse on a diverse collection\nof out-of-domain evaluation sets. Finally, we provide a qualitative analysis of\nadversarial (vs standard) data, identifying key differences and offering\nguidance for future research.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 00:48:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kaushik", "Divyansh", ""], ["Kiela", "Douwe", ""], ["Lipton", "Zachary C.", ""], ["Yih", "Wen-tau", ""]]}, {"id": "2106.00874", "submitter": "Munazza Zaib", "authors": "Munazza Zaib and Wei Emma Zhang and Quan Z. Sheng and Adnan Mahmood\n  and Yang Zhang", "title": "Conversational Question Answering: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) systems provide a way of querying the information\navailable in various formats including, but not limited to, unstructured and\nstructured data in natural languages. It constitutes a considerable part of\nconversational artificial intelligence (AI) which has led to the introduction\nof a special research topic on Conversational Question Answering (CQA), wherein\na system is required to understand the given context and then engages in\nmulti-turn QA to satisfy the user's information needs. Whilst the focus of most\nof the existing research work is subjected to single-turn QA, the field of\nmulti-turn QA has recently grasped attention and prominence owing to the\navailability of large-scale, multi-turn QA datasets and the development of\npre-trained language models. With a good amount of models and research papers\nadding to the literature every year recently, there is a dire need of arranging\nand presenting the related work in a unified manner to streamline future\nresearch. This survey, therefore, is an effort to present a comprehensive\nreview of the state-of-the-art research trends of CQA primarily based on\nreviewed papers from 2016-2021. Our findings show that there has been a trend\nshift from single-turn to multi-turn QA which empowers the field of\nConversational AI from different perspectives. This survey is intended to\nprovide an epitome for the research community with the hope of laying a strong\nfoundation for the field of CQA.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:06:34 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:02:38 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zaib", "Munazza", ""], ["Zhang", "Wei Emma", ""], ["Sheng", "Quan Z.", ""], ["Mahmood", "Adnan", ""], ["Zhang", "Yang", ""]]}, {"id": "2106.00877", "submitter": "S\\'ilvia Casacuberta", "authors": "S\\'ilvia Casacuberta, Karina Halevy, Dami\\'an E. Blasi", "title": "Evaluating Word Embeddings with Categorical Modularity", "comments": "Accepted to Findings of ACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce categorical modularity, a novel low-resource intrinsic metric to\nevaluate word embedding quality. Categorical modularity is a graph modularity\nmetric based on the $k$-nearest neighbor graph constructed with embedding\nvectors of words from a fixed set of semantic categories, in which the goal is\nto measure the proportion of words that have nearest neighbors within the same\ncategories. We use a core set of 500 words belonging to 59 neurobiologically\nmotivated semantic categories in 29 languages and analyze three word embedding\nmodels per language (FastText, MUSE, and subs2vec). We find moderate to strong\npositive correlations between categorical modularity and performance on the\nmonolingual tasks of sentiment analysis and word similarity calculation and on\nthe cross-lingual task of bilingual lexicon induction both to and from English.\nOverall, we suggest that categorical modularity provides non-trivial predictive\ninformation about downstream task performance, with breakdowns of correlations\nby model suggesting some meta-predictive properties about semantic information\nloss as well.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:29:11 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Casacuberta", "S\u00edlvia", ""], ["Halevy", "Karina", ""], ["Blasi", "Dami\u00e1n E.", ""]]}, {"id": "2106.00882", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi", "title": "Efficient Passage Retrieval with Hashing for Open-domain Question\n  Answering", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:34:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yamada", "Ikuya", ""], ["Asai", "Akari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2106.00887", "submitter": "Zanbo Wang", "authors": "Zanbo Wang, Wei Wei, Xianling Mao, Shanshan Feng, Pan Zhou, Zhiyong He\n  and Sheng Jiang", "title": "Exploiting Global Contextual Information for Document-level Named Entity\n  Recognition", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing named entity recognition (NER) approaches are based on sequence\nlabeling models, which focus on capturing the local context dependencies.\nHowever, the way of taking one sentence as input prevents the modeling of\nnon-sequential global context, which is useful especially when local context\ninformation is limited or ambiguous. To this end, we propose a model called\nGlobal Context enhanced Document-level NER (GCDoc) to leverage global\ncontextual information from two levels, i.e., both word and sentence. At\nword-level, a document graph is constructed to model a wider range of\ndependencies between words, then obtain an enriched contextual representation\nfor each word via graph neural networks (GNN). To avoid the interference of\nnoise information, we further propose two strategies. First we apply the\nepistemic uncertainty theory to find out tokens whose representations are less\nreliable, thereby helping prune the document graph. Then a selective auxiliary\nclassifier is proposed to effectively learn the weight of edges in document\ngraph and reduce the importance of noisy neighbour nodes. At sentence-level,\nfor appropriately modeling wider context beyond single sentence, we employ a\ncross-sentence module which encodes adjacent sentences and fuses it with the\ncurrent sentence representation via attention and gating mechanisms. Extensive\nexperiments on two benchmark NER datasets (CoNLL 2003 and Ontonotes 5.0 English\ndataset) demonstrate the effectiveness of our proposed model. Our model reaches\nF1 score of 92.22 (93.40 with BERT) on CoNLL 2003 dataset and 88.32 (90.49 with\nBERT) on Ontonotes 5.0 dataset, achieving new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:52:07 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Zanbo", ""], ["Wei", "Wei", ""], ["Mao", "Xianling", ""], ["Feng", "Shanshan", ""], ["Zhou", "Pan", ""], ["He", "Zhiyong", ""], ["Jiang", "Sheng", ""]]}, {"id": "2106.00891", "submitter": "Zhiwen Tang", "authors": "Zhiwen Tang, Hrishikesh Kulkarni, Grace Hui Yang", "title": "High-Quality Diversification for Task-Oriented Dialogue Systems", "comments": "Accepted by ACL-IJCNLP 2021 (Findings of ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many task-oriented dialogue systems use deep reinforcement learning (DRL) to\nlearn policies that respond to the user appropriately and complete the tasks\nsuccessfully. Training DRL agents with diverse dialogue trajectories prepare\nthem well for rare user requests and unseen situations. One effective\ndiversification method is to let the agent interact with a diverse set of\nlearned user models. However, trajectories created by these artificial user\nmodels may contain generation errors, which can quickly propagate into the\nagent's policy. It is thus important to control the quality of the\ndiversification and resist the noise. In this paper, we propose a novel\ndialogue diversification method for task-oriented dialogue systems trained in\nsimulators. Our method, Intermittent Short Extension Ensemble (I-SEE),\nconstrains the intensity to interact with an ensemble of diverse user models\nand effectively controls the quality of the diversification. Evaluations on the\nMultiwoz dataset show that I-SEE successfully boosts the performance of several\nstate-of-the-art DRL dialogue agents.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:10:07 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 01:45:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tang", "Zhiwen", ""], ["Kulkarni", "Hrishikesh", ""], ["Yang", "Grace Hui", ""]]}, {"id": "2106.00893", "submitter": "Kaden Griffith", "authors": "Kaden Griffith and Jugal Kalita", "title": "Solving Arithmetic Word Problems with Transformers and Preprocessing of\n  Problem Text", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.00871", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines the use of Transformer networks trained to translate math\nword problems to equivalent arithmetic expressions in infix, prefix, and\npostfix notations. We compare results produced by many neural configurations\nand find that most configurations outperform previously reported approaches on\nthree of four datasets with significant increases in accuracy of over 20\npercentage points. The best neural approaches boost accuracy by 30% when\ncompared to the previous state-of-the-art on some datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:12:45 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Griffith", "Kaden", ""], ["Kalita", "Jugal", ""]]}, {"id": "2106.00903", "submitter": "Liang Ding", "authors": "Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao and\n  Zhaopeng Tu", "title": "Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in\n  Non-Autoregressive Translation", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Knowledge distillation (KD) is commonly used to construct synthetic data for\ntraining non-autoregressive translation (NAT) models. However, there exists a\ndiscrepancy on low-frequency words between the distilled and the original data,\nleading to more errors on predicting low-frequency words. To alleviate the\nproblem, we directly expose the raw data into NAT by leveraging pretraining. By\nanalyzing directed alignments, we found that KD makes low-frequency source\nwords aligned with targets more deterministically but fails to align sufficient\nlow-frequency words from target to source. Accordingly, we propose reverse KD\nto rejuvenate more alignments for low-frequency target words. To make the most\nof authentic and synthetic data, we combine these complementary approaches as a\nnew training strategy for further boosting NAT performance. We conduct\nexperiments on five translation benchmarks over two advanced architectures.\nResults demonstrate that the proposed approach can significantly and\nuniversally improve translation quality by reducing translation errors on\nlow-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU\npoints on the WMT14 English-German and WMT16 Romanian-English datasets,\nrespectively. Our code, data, and trained models are available at\n\\url{https://github.com/longyuewangdcu/RLFW-NAT}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:41:40 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ding", "Liang", ""], ["Wang", "Longyue", ""], ["Liu", "Xuebo", ""], ["Wong", "Derek F.", ""], ["Tao", "Dacheng", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2106.00920", "submitter": "Rishabh Joshi", "authors": "Rishabh Joshi, Vidhisha Balachandran, Shikhar Vashishth, Alan Black,\n  Yulia Tsvetkov", "title": "DialoGraph: Incorporating Interpretable Strategy-Graph Networks into\n  Negotiation Dialogues", "comments": "Accepted at ICLR 2021; https://openreview.net/forum?id=kDnal_bbb-E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To successfully negotiate a deal, it is not enough to communicate fluently:\npragmatic planning of persuasive negotiation strategies is essential. While\nmodern dialogue agents excel at generating fluent sentences, they still lack\npragmatic grounding and cannot reason strategically. We present DialoGraph, a\nnegotiation system that incorporates pragmatic strategies in a negotiation\ndialogue using graph neural networks. DialoGraph explicitly incorporates\ndependencies between sequences of strategies to enable improved and\ninterpretable prediction of next optimal strategies, given the dialogue\ncontext. Our graph-based method outperforms prior state-of-the-art negotiation\nmodels both in the accuracy of strategy/dialogue act prediction and in the\nquality of downstream dialogue response generation. We qualitatively show\nfurther benefits of learned strategy-graphs in providing explicit associations\nbetween effective negotiation strategies over the course of the dialogue,\nleading to interpretable and strategic dialogues.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 03:34:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Joshi", "Rishabh", ""], ["Balachandran", "Vidhisha", ""], ["Vashishth", "Shikhar", ""], ["Black", "Alan", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2106.00933", "submitter": "Yilun Zhu", "authors": "Yilun Zhu, Sameer Pradhan, Amir Zeldes", "title": "OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12\n  More Genres", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SOTA coreference resolution produces increasingly impressive scores on the\nOntoNotes benchmark. However lack of comparable data following the same scheme\nfor more genres makes it difficult to evaluate generalizability to open domain\ndata. This paper provides a dataset and comprehensive evaluation showing that\nthe latest neural LM based end-to-end systems degrade very substantially out of\ndomain. We make an OntoNotes-like coreference dataset called OntoGUM publicly\navailable, converted from GUM, an English corpus covering 12 genres, using\ndeterministic rules, which we evaluate. Thanks to the rich syntactic and\ndiscourse annotations in GUM, we are able to create the largest human-annotated\ncoreference corpus following the OntoNotes guidelines, and the first to be\nevaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation\nacross 12 genres shows nearly 15-20% degradation for both deterministic and\ndeep learning systems, indicating a lack of generalizability or covert\noverfitting in existing coreference resolution models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:42:51 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:39:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhu", "Yilun", ""], ["Pradhan", "Sameer", ""], ["Zeldes", "Amir", ""]]}, {"id": "2106.00934", "submitter": "Nada Almarwani", "authors": "Nada Almarwani and Mona Diab", "title": "Discrete Cosine Transform as Universal Sentence Encoder", "comments": "to be published in ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern sentence encoders are used to generate dense vector representations\nthat capture the underlying linguistic characteristics for a sequence of words,\nincluding phrases, sentences, or paragraphs. These kinds of representations are\nideal for training a classifier for an end task such as sentiment analysis,\nquestion answering and text classification. Different models have been proposed\nto efficiently generate general purpose sentence representations to be used in\npretraining protocols. While averaging is the most commonly used efficient\nsentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an\nalternative that captures the underlying syntactic characteristics of a given\ntext without compromising practical efficiency compared to averaging. However,\nas with most other sentence encoders, the DCT sentence encoder was only\nevaluated in English. To this end, we utilize DCT encoder to generate universal\nsentence representation for different languages such as German, French, Spanish\nand Russian. The experimental results clearly show the superior effectiveness\nof DCT encoding in which consistent performance improvements are achieved over\nstrong baselines on multiple standardized datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:43:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Almarwani", "Nada", ""], ["Diab", "Mona", ""]]}, {"id": "2106.00941", "submitter": "Wenxiang Jiao", "authors": "Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R. Lyu,\n  Irwin King", "title": "Self-Training Sampling with Monolingual Data Uncertainty for Neural\n  Machine Translation", "comments": "ACL 2021 main conference, long paper, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-training has proven effective for improving NMT performance by\naugmenting model training with synthetic parallel data. The common practice is\nto construct synthetic data based on a randomly sampled subset of large-scale\nmonolingual data, which we empirically show is sub-optimal. In this work, we\npropose to improve the sampling procedure by selecting the most informative\nmonolingual sentences to complement the parallel data. To this end, we compute\nthe uncertainty of monolingual sentences using the bilingual dictionary\nextracted from the parallel data. Intuitively, monolingual sentences with lower\nuncertainty generally correspond to easy-to-translate patterns which may not\nprovide additional gains. Accordingly, we design an uncertainty-based sampling\nstrategy to efficiently exploit the monolingual data for self-training, in\nwhich monolingual sentences with higher uncertainty would be sampled with\nhigher probability. Experimental results on large-scale WMT\nEnglish$\\Rightarrow$German and English$\\Rightarrow$Chinese datasets demonstrate\nthe effectiveness of the proposed approach. Extensive analyses suggest that\nemphasizing the learning on uncertain monolingual sentences by our approach\ndoes improve the translation quality of high-uncertainty sentences and also\nbenefits the prediction of low-frequency words at the target side.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:01:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jiao", "Wenxiang", ""], ["Wang", "Xing", ""], ["Tu", "Zhaopeng", ""], ["Shi", "Shuming", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2106.00948", "submitter": "Keyang Xu", "authors": "Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng and Caiming Xiong", "title": "Unsupervised Out-of-Domain Detection via Pre-trained Transformers", "comments": "Accepted by ACL 2021. Code is available at\n  https://github.com/rivercold/BERT-unsupervised-OOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployed real-world machine learning applications are often subject to\nuncontrolled and even potentially malicious inputs. Those out-of-domain inputs\ncan lead to unpredictable outputs and sometimes catastrophic safety issues.\nPrior studies on out-of-domain detection require in-domain task labels and are\nlimited to supervised classification scenarios. Our work tackles the problem of\ndetecting out-of-domain samples with only unsupervised in-domain data. We\nutilize the latent representations of pre-trained transformers and propose a\nsimple yet effective method to transform features across all layers to\nconstruct out-of-domain detectors efficiently. Two domain-specific fine-tuning\napproaches are further proposed to boost detection accuracy. Our empirical\nevaluations of related methods on two datasets validate that our method greatly\nimproves out-of-domain detection ability in a more general scenario.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:21:25 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Xu", "Keyang", ""], ["Ren", "Tongzheng", ""], ["Zhang", "Shikun", ""], ["Feng", "Yihao", ""], ["Xiong", "Caiming", ""]]}, {"id": "2106.00950", "submitter": "Canasai Kruengkrai", "authors": "Canasai Kruengkrai, Junichi Yamagishi, Xin Wang", "title": "A Multi-Level Attention Model for Evidence-Based Fact Checking", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence-based fact checking aims to verify the truthfulness of a claim\nagainst evidence extracted from textual sources. Learning a representation that\neffectively captures relations between a claim and evidence can be challenging.\nRecent state-of-the-art approaches have developed increasingly sophisticated\nmodels based on graph structures. We present a simple model that can be trained\non sequence structures. Our model enables inter-sentence attentions at\ndifferent levels and can benefit from joint training. Results on a large-scale\ndataset for Fact Extraction and VERification (FEVER) show that our model\noutperforms the graph-based approaches and yields 1.09% and 1.42% improvements\nin label accuracy and FEVER score, respectively, over the best published model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:40:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kruengkrai", "Canasai", ""], ["Yamagishi", "Junichi", ""], ["Wang", "Xin", ""]]}, {"id": "2106.00954", "submitter": "Zhe Liu", "authors": "Zhe Liu, Yufan Guo, Jalal Mahmud", "title": "When and Why does a Model Fail? A Human-in-the-loop Error Detection\n  Framework for Sentiment Analysis", "comments": "NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep neural networks have been widely employed and proven effective\nin sentiment analysis tasks, it remains challenging for model developers to\nassess their models for erroneous predictions that might exist prior to\ndeployment. Once deployed, emergent errors can be hard to identify in\nprediction run-time and impossible to trace back to their sources. To address\nsuch gaps, in this paper we propose an error detection framework for sentiment\nanalysis based on explainable features. We perform global-level feature\nvalidation with human-in-the-loop assessment, followed by an integration of\nglobal and local-level feature contribution analysis. Experimental results show\nthat, given limited human-in-the-loop intervention, our method is able to\nidentify erroneous model predictions on unseen data with high precision.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:45:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Zhe", ""], ["Guo", "Yufan", ""], ["Mahmud", "Jalal", ""]]}, {"id": "2106.00955", "submitter": "Luca Soldaini", "authors": "Chao-Chun Hsu, Eric Lind, Luca Soldaini, Alessandro Moschitti", "title": "Answer Generation for Retrieval-based Question Answering Systems", "comments": "Short paper, Accepted at Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in transformer-based models have greatly improved the\nability of Question Answering (QA) systems to provide correct answers; in\nparticular, answer sentence selection (AS2) models, core components of\nretrieval-based systems, have achieved impressive results. While generally\neffective, these models fail to provide a satisfying answer when all retrieved\ncandidates are of poor quality, even if they contain correct information. In\nAS2, models are trained to select the best answer sentence among a set of\ncandidates retrieved for a given question. In this work, we propose to generate\nanswers from a set of AS2 top candidates. Rather than selecting the best\ncandidate, we train a sequence to sequence transformer model to generate an\nanswer from a candidate set. Our tests on three English AS2 datasets show\nimprovement up to 32 absolute points in accuracy over the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:45:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hsu", "Chao-Chun", ""], ["Lind", "Eric", ""], ["Soldaini", "Luca", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2106.00957", "submitter": "Junwei Bao Doctor", "authors": "Yu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguang Cui, Youzheng Wu, and\n  Xiaodong He", "title": "RevCore: Review-augmented Conversational Recommendation", "comments": "Accepted by ACL-Findings 2021. 13 pages, 3 figures, and 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing conversational recommendation (CR) systems usually suffer from\ninsufficient item information when conducted on short dialogue history and\nunfamiliar items. Incorporating external information (e.g., reviews) is a\npotential solution to alleviate this problem. Given that reviews often provide\na rich and detailed user experience on different interests, they are potential\nideal resources for providing high-quality recommendations within an\ninformative conversation. In this paper, we design a novel end-to-end\nframework, namely, Review-augmented Conversational Recommender (RevCore), where\nreviews are seamlessly incorporated to enrich item information and assist in\ngenerating both coherent and informative responses. In detail, we extract\nsentiment-consistent reviews, perform review-enriched and entity-based\nrecommendations for item suggestions, as well as use a review-attentive\nencoder-decoder for response generation. Experimental results demonstrate the\nsuperiority of our approach in yielding better performance on both\nrecommendation and conversation responding.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:46:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lu", "Yu", ""], ["Bao", "Junwei", ""], ["Song", "Yan", ""], ["Ma", "Zichen", ""], ["Cui", "Shuguang", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""]]}, {"id": "2106.00969", "submitter": "Shikhar Singh", "authors": "Shikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-Lin Wu,\n  Xuezhe Ma, Nanyun Peng", "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary\n  Sentences", "comments": "In Proceedings of Findings of the Association for Computational\n  Linguistics: ACL 2021 (ACL-Findings). Contains 16 pages, 14 figures and 11\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning is intuitive for humans but has been a long-term\nchallenge for artificial intelligence (AI). Recent advancements in pretrained\nlanguage models have shown promising results on several commonsense benchmark\ndatasets. However, the reliability and comprehensiveness of these benchmarks\ntowards assessing model's commonsense reasoning ability remains unclear. To\nthis end, we introduce a new commonsense reasoning benchmark dataset comprising\nnatural language true/false statements, with each sample paired with its\ncomplementary counterpart, resulting in 4k sentence pairs. We propose a\npairwise accuracy metric to reliably measure an agent's ability to perform\ncommonsense reasoning over a given situation. The dataset is crowdsourced and\nenhanced with an adversarial model-in-the-loop setup to incentivize challenging\nsamples. To facilitate a systematic analysis of commonsense capabilities, we\ndesign our dataset along the dimensions of knowledge domains, reasoning\nscenarios and numeracy. Experimental results demonstrate that our strongest\nbaseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and\n~51% pairwise accuracy, well below human performance (~95% for both metrics).\nThe dataset is available at https://github.com/PlusLabNLP/Com2Sense.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 06:31:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Singh", "Shikhar", ""], ["Wen", "Nuan", ""], ["Hou", "Yu", ""], ["Alipoormolabashi", "Pegah", ""], ["Wu", "Te-Lin", ""], ["Ma", "Xuezhe", ""], ["Peng", "Nanyun", ""]]}, {"id": "2106.00976", "submitter": "Xin Liu", "authors": "Xin Liu, Jiefu Ou, Yangqiu Song, Xin Jiang", "title": "Exploring Discourse Structures for Argument Impact Classification", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relations among arguments reveal logical structures of a debate\nconversation. However, no prior work has explicitly studied how the sequence of\ndiscourse relations influence a claim's impact. This paper empirically shows\nthat the discourse relations between two arguments along the context path are\nessential factors for identifying the persuasive power of an argument. We\nfurther propose DisCOC to inject and fuse the sentence-level structural\ndiscourse information with contextualized features derived from large-scale\nlanguage models. Experimental results and extensive analysis show that the\nattention and gate mechanisms that explicitly model contexts and texts can\nindeed help the argument impact classification task defined by Durmus et al.\n(2019), and discourse structures among the context path of the claim to be\nclassified can further boost the performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 06:49:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Xin", ""], ["Ou", "Jiefu", ""], ["Song", "Yangqiu", ""], ["Jiang", "Xin", ""]]}, {"id": "2106.00984", "submitter": "Guoxian Yu", "authors": "Yunfeng Zhao, Guoxian Yu, Lei Liu, Zhongmin Yan, Lizhen Cui and\n  Carlotta Domeniconi", "title": "Few-Shot Partial-Label Learning", "comments": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial-label learning (PLL) generally focuses on inducing a noise-tolerant\nmulti-class classifier by training on overly-annotated samples, each of which\nis annotated with a set of labels, but only one is the valid label. A basic\npromise of existing PLL solutions is that there are sufficient partial-label\n(PL) samples for training. However, it is more common than not to have just few\nPL samples at hand when dealing with new tasks. Furthermore, existing few-shot\nlearning algorithms assume precise labels of the support set; as such,\nirrelevant labels may seriously mislead the meta-learner and thus lead to a\ncompromised performance. How to enable PLL under a few-shot learning setting is\nan important problem, but not yet well studied. In this paper, we introduce an\napproach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance\nmetric learning by an embedding network and rectifying prototypes on the tasks\npreviously encountered. Next, it calculates the prototype of each class of a\nnew task in the embedding network. An unseen example can then be classified via\nits distance to each prototype. Experimental results on widely-used few-shot\ndatasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a\nsuperior performance than the state-of-the-art methods across different\nsettings, and it needs fewer samples for quickly adapting to new tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 07:03:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhao", "Yunfeng", ""], ["Yu", "Guoxian", ""], ["Liu", "Lei", ""], ["Yan", "Zhongmin", ""], ["Cui", "Lizhen", ""], ["Domeniconi", "Carlotta", ""]]}, {"id": "2106.01006", "submitter": "Liang Qiu", "authors": "Liang Qiu, Yuan Liang, Yizhou Zhao, Pan Lu, Baolin Peng, Zhou Yu, Ying\n  Nian Wu, Song-Chun Zhu", "title": "SocAoG: Incremental Graph Parsing for Social Relation Inference in\n  Dialogues", "comments": "Long paper (oral) accepted by ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring social relations from dialogues is vital for building emotionally\nintelligent robots to interpret human language better and act accordingly. We\nmodel the social network as an And-or Graph, named SocAoG, for the consistency\nof relations among a group and leveraging attributes as inference cues.\nMoreover, we formulate a sequential structure prediction task, and propose an\n$\\alpha$-$\\beta$-$\\gamma$ strategy to incrementally parse SocAoG for the\ndynamic inference upon any incoming utterance: (i) an $\\alpha$ process\npredicting attributes and relations conditioned on the semantics of dialogues,\n(ii) a $\\beta$ process updating the social relations based on related\nattributes, and (iii) a $\\gamma$ process updating individual's attributes based\non interpersonal social relations. Empirical results on DialogRE and MovieGraph\nshow that our model infers social relations more accurately than the\nstate-of-the-art methods. Moreover, the ablation study shows the three\nprocesses complement each other, and the case study demonstrates the dynamic\nrelational inference.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 08:07:42 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:45:14 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Qiu", "Liang", ""], ["Liang", "Yuan", ""], ["Zhao", "Yizhou", ""], ["Lu", "Pan", ""], ["Peng", "Baolin", ""], ["Yu", "Zhou", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2106.01023", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang", "title": "One Teacher is Enough? Pre-trained Language Model Distillation from\n  Multiple Teachers", "comments": "Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models (PLMs) achieve great success in NLP. However,\ntheir huge model sizes hinder their applications in many practical systems.\nKnowledge distillation is a popular technique to compress PLMs, which learns a\nsmall student model from a large teacher PLM. However, the knowledge learned\nfrom a single teacher may be limited and even biased, resulting in low-quality\nstudent model. In this paper, we propose a multi-teacher knowledge distillation\nframework named MT-BERT for pre-trained language model compression, which can\ntrain high-quality student model from multiple teacher PLMs. In MT-BERT we\ndesign a multi-teacher co-finetuning method to jointly finetune multiple\nteacher PLMs in downstream tasks with shared pooling and prediction layers to\nalign their output space for better collaborative teaching. In addition, we\npropose a multi-teacher hidden loss and a multi-teacher distillation loss to\ntransfer the useful knowledge in both hidden states and soft labels from\nmultiple teacher PLMs to the student model. Experiments on three benchmark\ndatasets validate the effectiveness of MT-BERT in compressing PLMs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 08:42:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.01024", "submitter": "Yuxuan Lai", "authors": "Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, and Dongyan Zhao", "title": "Why Machine Reading Comprehension Models Learn Shortcuts?", "comments": "13 pages, 8 figures, ACL 2021 (findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies report that many machine reading comprehension (MRC) models\ncan perform closely to or even better than humans on benchmark datasets.\nHowever, existing works indicate that many MRC models may learn shortcuts to\noutwit these benchmarks, but the performance is unsatisfactory in real-world\napplications. In this work, we attempt to explore, instead of the expected\ncomprehension skills, why these models learn the shortcuts. Based on the\nobservation that a large portion of questions in current datasets have shortcut\nsolutions, we argue that larger proportion of shortcut questions in training\ndata make models rely on shortcut tricks excessively. To investigate this\nhypothesis, we carefully design two synthetic datasets with annotations that\nindicate whether a question can be answered using shortcut solutions. We\nfurther propose two new methods to quantitatively analyze the learning\ndifficulty regarding shortcut and challenging questions, and revealing the\ninherent learning mechanism behind the different performance between the two\nkinds of questions. A thorough empirical analysis shows that MRC models tend to\nlearn shortcut questions earlier than challenging questions, and the high\nproportions of shortcut questions in training sets hinder models from exploring\nthe sophisticated reasoning skills in the later stage of training.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 08:43:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lai", "Yuxuan", ""], ["Zhang", "Chen", ""], ["Feng", "Yansong", ""], ["Huang", "Quzhe", ""], ["Zhao", "Dongyan", ""]]}, {"id": "2106.01033", "submitter": "Kunwoo Park", "authors": "Kunwoo Park, Zhufeng Pan, and Jungseock Joo", "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment\n  Extraction in News Text", "comments": "Published in Findings of ACL 2021 (Long paper). The manuscript is\n  slightly revised after the camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:02:14 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 07:32:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Park", "Kunwoo", ""], ["Pan", "Zhufeng", ""], ["Joo", "Jungseock", ""]]}, {"id": "2106.01040", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang", "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and\n  Effective Long Document Modeling", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Transformer is important for text modeling. However, it has difficulty in\nhandling long documents due to the quadratic complexity with input text length.\nIn order to handle this problem, we propose a hierarchical interactive\nTransformer (Hi-Transformer) for efficient and effective long document\nmodeling. Hi-Transformer models documents in a hierarchical way, i.e., first\nlearns sentence representations and then learns document representations. It\ncan effectively reduce the complexity and meanwhile capture global document\ncontext in the modeling of each sentence. More specifically, we first use a\nsentence Transformer to learn the representations of each sentence. Then we use\na document Transformer to model the global document context from these sentence\nrepresentations. Next, we use another sentence Transformer to enhance sentence\nmodeling using the global document context. Finally, we use hierarchical\npooling method to obtain document embedding. Extensive experiments on three\nbenchmark datasets validate the efficiency and effectiveness of Hi-Transformer\nin long document modeling.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:30:29 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 07:16:46 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.01044", "submitter": "Jennifer C. White", "authors": "Jennifer C. White and Ryan Cotterell", "title": "Examining the Inductive Bias of Neural Language Models with Artificial\n  Languages", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since language models are used to model a wide variety of languages, it is\nnatural to ask whether the neural architectures used for the task have\ninductive biases towards modeling particular types of languages. Investigation\nof these biases has proved complicated due to the many variables that appear in\nthe experimental setup. Languages vary in many typological dimensions, and it\nis difficult to single out one or two to investigate without the others acting\nas confounders. We propose a novel method for investigating the inductive\nbiases of language models using artificial languages. These languages are\nconstructed to allow us to create parallel corpora across languages that differ\nonly in the typological feature being investigated, such as word order. We then\nuse them to train and test language models. This constitutes a fully controlled\ncausal framework, and demonstrates how grammar engineering can serve as a\nuseful tool for analyzing neural models. Using this method, we find that\ncommonly used neural architectures exhibit different inductive biases: LSTMs\ndisplay little preference with respect to word ordering, while transformers\ndisplay a clear preference for some orderings over others. Further, we find\nthat neither the inductive bias of the LSTM nor that of the transformer appears\nto reflect any tendencies that we see in attested natural languages.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:34:32 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["White", "Jennifer C.", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.01045", "submitter": "Marco Gaido", "authors": "Luisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto\n  Martinelli, Matteo Negri, Marco Turchi", "title": "Cascade versus Direct Speech Translation: Do the Differences Still Make\n  a Difference?", "comments": "Accepted at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Five years after the first published proofs of concept, direct approaches to\nspeech translation (ST) are now competing with traditional cascade solutions.\nIn light of this steady progress, can we claim that the performance gap between\nthe two is closed? Starting from this question, we present a systematic\ncomparison between state-of-the-art systems representative of the two\nparadigms. Focusing on three language directions\n(English-German/Italian/Spanish), we conduct automatic and manual evaluations,\nexploiting high-quality professional post-edits and annotations. Our\nmulti-faceted analysis on one of the few publicly available ST benchmarks\nattests for the first time that: i) the gap between the two paradigms is now\nclosed, and ii) the subtle differences observed in their behavior are not\nsufficient for humans neither to distinguish them nor to prefer one over the\nother.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:37:37 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bentivogli", "Luisa", ""], ["Cettolo", "Mauro", ""], ["Gaido", "Marco", ""], ["Karakanta", "Alina", ""], ["Martinelli", "Alberto", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2106.01051", "submitter": "Rahul Aralikatte", "authors": "Edoardo Maria Ponti, Rahul Aralikatte, Disha Shrivastava, Siva Reddy,\n  Anders S{\\o}gaard", "title": "Minimax and Neyman-Pearson Meta-Learning for Outlier Languages", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-agnostic meta-learning (MAML) has been recently put forth as a strategy\nto learn resource-poor languages in a sample-efficient fashion. Nevertheless,\nthe properties of these languages are often not well represented by those\navailable during training. Hence, we argue that the i.i.d. assumption ingrained\nin MAML makes it ill-suited for cross-lingual NLP. In fact, under a\ndecision-theoretic framework, MAML can be interpreted as minimising the\nexpected risk across training languages (with a uniform prior), which is known\nas Bayes criterion. To increase its robustness to outlier languages, we create\ntwo variants of MAML based on alternative criteria: Minimax MAML reduces the\nmaximum risk across languages, while Neyman-Pearson MAML constrains the risk in\neach language to a maximum threshold. Both criteria constitute fully\ndifferentiable two-player games. In light of this, we propose a new adaptive\noptimiser solving for a local approximation to their Nash equilibrium. We\nevaluate both model variants on two popular NLP tasks, part-of-speech tagging\nand question answering. We report gains for their average and minimum\nperformance across low-resource languages in zero- and few-shot settings,\ncompared to joint multi-source transfer and vanilla MAML.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:53:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ponti", "Edoardo Maria", ""], ["Aralikatte", "Rahul", ""], ["Shrivastava", "Disha", ""], ["Reddy", "Siva", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2106.01060", "submitter": "Yova Kementchedjhieva", "authors": "Yova Kementchedjhieva, Mark Anderson and Anders S{\\o}gaard", "title": "John praised Mary because he? Implicit Causality Bias and Its\n  Interaction with Explicit Cues in LMs", "comments": "To appear at Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some interpersonal verbs can implicitly attribute causality to either their\nsubject or their object and are therefore said to carry an implicit causality\n(IC) bias. Through this bias, causal links can be inferred from a narrative,\naiding language comprehension. We investigate whether pre-trained language\nmodels (PLMs) encode IC bias and use it at inference time. We find that to be\nthe case, albeit to different degrees, for three distinct PLM architectures.\nHowever, causes do not always need to be implicit -- when a cause is explicitly\nstated in a subordinate clause, an incongruent IC bias associated with the verb\nin the main clause leads to a delay in human processing. We hypothesize that\nthe temporary challenge humans face in integrating the two contradicting\nsignals, one from the lexical semantics of the verb, one from the\nsentence-level semantics, would be reflected in higher error rates for models\non tasks dependent on causal links. The results of our study lend support to\nthis hypothesis, suggesting that PLMs tend to prioritize lexical patterns over\nhigher-order signals.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:26:07 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kementchedjhieva", "Yova", ""], ["Anderson", "Mark", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2106.01064", "submitter": "Shahbaz Syed", "authors": "Shahbaz Syed, Khalid Al-Khatib, Milad Alshomary, Henning Wachsmuth,\n  and Martin Potthast", "title": "Generating Informative Conclusions for Argumentative Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of an argumentative text is to support a certain conclusion. Yet,\nthey are often omitted, expecting readers to infer them rather. While\nappropriate when reading an individual text, this rhetorical device limits\naccessibility when browsing many texts (e.g., on a search engine or on social\nmedia). In these scenarios, an explicit conclusion makes for a good candidate\nsummary of an argumentative text. This is especially true if the conclusion is\ninformative, emphasizing specific concepts from the text. With this paper we\nintroduce the task of generating informative conclusions: First,\nWebis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of\nargumentative texts and their conclusions. Second, two paradigms for conclusion\ngeneration are investigated; one extractive, the other abstractive in nature.\nThe latter exploits argumentative knowledge that augment the data via control\ncodes and finetuning the BART model on several subsets of the corpus. Third,\ninsights are provided into the suitability of our corpus for the task, the\ndifferences between the two generation paradigms, the trade-off between\ninformativeness and conciseness, and the impact of encoding argumentative\nknowledge. The corpus, code, and the trained models are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:35:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Syed", "Shahbaz", ""], ["Al-Khatib", "Khalid", ""], ["Alshomary", "Milad", ""], ["Wachsmuth", "Henning", ""], ["Potthast", "Martin", ""]]}, {"id": "2106.01065", "submitter": "Yujian Gan", "authors": "Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R.\n  Woodward, Jinxia Xie, Pengsheng Huang", "title": "Towards Robustness of Text-to-SQL Models against Synonym Substitution", "comments": "To appear in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant progress in studying neural networks to\ntranslate text descriptions into SQL queries. Despite achieving good\nperformance on some public benchmarks, existing text-to-SQL models typically\nrely on the lexical matching between words in natural language (NL) questions\nand tokens in table schemas, which may render the models vulnerable to attacks\nthat break the schema linking mechanism. In this work, we investigate the\nrobustness of text-to-SQL models to synonym substitution. In particular, we\nintroduce Spider-Syn, a human-curated dataset based on the Spider benchmark for\ntext-to-SQL translation. NL questions in Spider-Syn are modified from Spider,\nby replacing their schema-related words with manually selected synonyms that\nreflect real-world question paraphrases. We observe that the accuracy\ndramatically drops by eliminating such explicit correspondence between NL\nquestions and table schemas, even if the synonyms are not adversarially\nselected to conduct worst-case adversarial attacks. Finally, we present two\ncategories of approaches to improve the model robustness. The first category of\napproaches utilizes additional synonym annotations for table schemas by\nmodifying the model input, while the second category is based on adversarial\ntraining. We demonstrate that both categories of approaches significantly\noutperform their counterparts without the defense, and the first category of\napproaches are more effective.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:36:23 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 10:02:51 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gan", "Yujian", ""], ["Chen", "Xinyun", ""], ["Huang", "Qiuping", ""], ["Purver", "Matthew", ""], ["Woodward", "John R.", ""], ["Xie", "Jinxia", ""], ["Huang", "Pengsheng", ""]]}, {"id": "2106.01071", "submitter": "Lixing Zhu", "authors": "Lixing Zhu and Gabriele Pergola and Lin Gui and Deyu Zhou and Yulan He", "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion detection in dialogues is challenging as it often requires the\nidentification of thematic topics underlying a conversation, the relevant\ncommonsense knowledge, and the intricate transition patterns between the\naffective states. In this paper, we propose a Topic-Driven Knowledge-Aware\nTransformer to handle the challenges above. We firstly design a topic-augmented\nlanguage model (LM) with an additional layer specialized for topic detection.\nThe topic-augmented LM is then combined with commonsense statements derived\nfrom a knowledge base based on the dialogue contextual information. Finally, a\ntransformer-based encoder-decoder architecture fuses the topical and\ncommonsense information, and performs the emotion label sequence prediction.\nThe model has been experimented on four datasets in dialogue emotion detection,\ndemonstrating its superiority empirically over the existing state-of-the-art\napproaches. Quantitative and qualitative results show that the model can\ndiscover topics which help in distinguishing emotion categories.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:57:44 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhu", "Lixing", ""], ["Pergola", "Gabriele", ""], ["Gui", "Lin", ""], ["Zhou", "Deyu", ""], ["He", "Yulan", ""]]}, {"id": "2106.01072", "submitter": "James Thorne", "authors": "James Thorne, Andreas Vlachos", "title": "Evidence-based Factual Error Correction", "comments": "Uploaded as a new paper in error. Please see the replacement of arxiv\n  paper 2012.15788v2 for this version: arXiv:2012.15788", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the task of factual error correction: performing edits\nto a claim so that the generated rewrite is better supported by evidence. This\nextends the well-studied task of fact verification by providing a mechanism to\ncorrect written texts that are refuted or only partially supported by evidence.\nWe demonstrate that it is feasible to train factual error correction systems\nfrom existing fact checking datasets which only contain labeled claims\naccompanied by evidence, but not the correction. We achieve this by employing a\ntwo-stage distant supervision approach that incorporates evidence into masked\nclaims when generating corrections. Our approach, based on the T5 transformer\nand using retrieved evidence, achieved better results than existing work which\nused a pointer copy network and gold evidence, producing accurate factual error\ncorrections for 5x more instances in human evaluation and a .125 increase in\nSARI score. The evaluation is conducted on a dataset of 65,000 instances based\non a recent fact verification shared task and we release it to enable further\nwork on the task.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:00:17 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 10:29:43 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Thorne", "James", ""], ["Vlachos", "Andreas", ""]]}, {"id": "2106.01074", "submitter": "James Thorne", "authors": "James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,\n  Sebastian Riedel, Alon Halevy", "title": "Database Reasoning Over Text", "comments": "To appear at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have shown impressive performance gains in answering queries\nfrom natural language text. However, existing works are unable to support\ndatabase queries, such as \"List/Count all female athletes who were born in 20th\ncentury\", which require reasoning over sets of relevant facts with operations\nsuch as join, filtering and aggregation. We show that while state-of-the-art\ntransformer models perform very well for small databases, they exhibit\nlimitations in processing noisy data, numerical operations, and queries that\naggregate facts. We propose a modular architecture to answer these\ndatabase-style queries over multiple spans from text and aggregating these at\nscale. We evaluate the architecture using WikiNLDB, a novel dataset for\nexploring such queries. Our architecture scales to databases containing\nthousands of facts whereas contemporary models are limited by how many facts\ncan be encoded. In direct comparison on small databases, our approach increases\noverall answer accuracy from 85% to 90%. On larger databases, our approach\nretains its accuracy whereas transformer baselines could not encode the\ncontext.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:09:40 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Thorne", "James", ""], ["Yazdani", "Majid", ""], ["Saeidi", "Marzieh", ""], ["Silvestri", "Fabrizio", ""], ["Riedel", "Sebastian", ""], ["Halevy", "Alon", ""]]}, {"id": "2106.01077", "submitter": "Hitomi Yanaka", "authors": "Hitomi Yanaka, Koji Mineshima, Kentaro Inui", "title": "SyGNS: A Systematic Generalization Testbed Based on Natural Language\n  Semantics", "comments": "Findings (long paper) of ACL-IJCNLP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have achieved great success in\nsemantically challenging NLP tasks, yet it remains unclear whether DNN models\ncan capture compositional meanings, those aspects of meaning that have been\nlong studied in formal semantics. To investigate this issue, we propose a\nSystematic Generalization testbed based on Natural language Semantics (SyGNS),\nwhose challenge is to map natural language sentences to multiple forms of\nscoped meaning representations, designed to account for various semantic\nphenomena. Using SyGNS, we test whether neural networks can systematically\nparse sentences involving novel combinations of logical expressions such as\nquantifiers and negation. Experiments show that Transformer and GRU models can\ngeneralize to unseen combinations of quantifiers, negations, and modifiers that\nare similar to given training instances in form, but not to the others. We also\nfind that the generalization performance to unseen combinations is better when\nthe form of meaning representations is simpler. The data and code for SyGNS are\npublicly available at https://github.com/verypluming/SyGNS.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:24:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yanaka", "Hitomi", ""], ["Mineshima", "Koji", ""], ["Inui", "Kentaro", ""]]}, {"id": "2106.01087", "submitter": "Clara Meister", "authors": "Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell", "title": "Is Sparse Attention more Interpretable?", "comments": "ACL 2021", "journal-ref": "Proceedings of ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse attention has been claimed to increase model interpretability under\nthe assumption that it highlights influential inputs. Yet the attention\ndistribution is typically over representations internal to the model rather\nthan the inputs themselves, suggesting this assumption may not have merit. We\nbuild on the recent work exploring the interpretability of attention; we design\na set of experiments to help us understand how sparsity affects our ability to\nuse attention as an explainability tool. On three text classification tasks, we\nverify that only a weak relationship between inputs and co-indexed intermediate\nrepresentations exists -- under sparse attention and otherwise. Further, we do\nnot find any plausible mappings from sparse attention distributions to a sparse\nset of influential inputs through other avenues. Rather, we observe in this\nsetting that inducing sparsity may make it less plausible that attention can be\nused as a tool for understanding model behavior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:42:56 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:39:49 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Meister", "Clara", ""], ["Lazov", "Stefan", ""], ["Augenstein", "Isabelle", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.01091", "submitter": "Janna De Boer", "authors": "Joppe Wouts, Janna de Boer, Alban Voppel, Sanne Brederoo, Sander van\n  Splunter and Iris Sommer", "title": "belabBERT: a Dutch RoBERTa-based language model applied to psychiatric\n  classification", "comments": "arXiv admin note: substantial text overlap with arXiv:2008.01543", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language processing (NLP) is becoming an important means for\nautomatic recognition of human traits and states, such as intoxication,\npresence of psychiatric disorders, presence of airway disorders and states of\nstress. Such applications have the potential to be an important pillar for\nonline help lines, and may gradually be introduced into eHealth modules.\nHowever, NLP is language specific and for languages such as Dutch, NLP models\nare scarce. As a result, recent Dutch NLP models have a low capture of long\nrange semantic dependencies over sentences. To overcome this, here we present\nbelabBERT, a new Dutch language model extending the RoBERTa architecture.\nbelabBERT is trained on a large Dutch corpus (+32 GB) of web crawled texts. We\napplied belabBERT to the classification of psychiatric illnesses. First, we\nevaluated the strength of text-based classification using belabBERT, and\ncompared the results to the existing RobBERT model. Then, we compared the\nperformance of belabBERT to audio classification for psychiatric disorders.\nFinally, a brief exploration was performed, extending the framework to a hybrid\ntext- and audio-based classification. Our results show that belabBERT\noutperformed the current best text classification network for Dutch, RobBERT.\nbelabBERT also outperformed classification based on audio alone.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:50:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wouts", "Joppe", ""], ["de Boer", "Janna", ""], ["Voppel", "Alban", ""], ["Brederoo", "Sanne", ""], ["van Splunter", "Sander", ""], ["Sommer", "Iris", ""]]}, {"id": "2106.01093", "submitter": "Ruisheng Cao", "authors": "Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu and Kai Yu", "title": "LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and\n  Non-Local Relations", "comments": "15 pages, 8 figures, accepted to ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to tackle the challenging heterogeneous graph encoding problem\nin the text-to-SQL task. Previous methods are typically node-centric and merely\nutilize different weight matrices to parameterize edge types, which 1) ignore\nthe rich semantics embedded in the topological structure of edges, and 2) fail\nto distinguish local and non-local relations for each node. To this end, we\npropose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying\nrelational features without constructing meta-paths. By virtue of the line\ngraph, messages propagate more efficiently through not only connections between\nnodes, but also the topology of directed edges. Furthermore, both local and\nnon-local relations are integrated distinctively during the graph iteration. We\nalso design an auxiliary task called graph pruning to improve the\ndiscriminative capability of the encoder. Our framework achieves\nstate-of-the-art results (62.8% with Glove, 72.0% with Electra) on the\ncross-domain text-to-SQL benchmark Spider at the time of writing.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:53:35 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 14:48:01 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 02:49:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Cao", "Ruisheng", ""], ["Chen", "Lu", ""], ["Chen", "Zhi", ""], ["Zhao", "Yanbin", ""], ["Zhu", "Su", ""], ["Yu", "Kai", ""]]}, {"id": "2106.01097", "submitter": "Sidharth Pancholi", "authors": "Sarojadevi Palani, Prabhu Rajagopal, Sidharth Pancholi", "title": "T-BERT -- Model for Sentiment Analysis of Micro-blogs Integrating Topic\n  Model and BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis (SA) has become an extensive research area in recent years\nimpacting diverse fields including ecommerce, consumer business, and politics,\ndriven by increasing adoption and usage of social media platforms. It is\nchallenging to extract topics and sentiments from unsupervised short texts\nemerging in such contexts, as they may contain figurative words, strident data,\nand co-existence of many possible meanings for a single word or phrase, all\ncontributing to obtaining incorrect topics. Most prior research is based on a\nspecific theme/rhetoric/focused-content on a clean dataset. In the work\nreported here, the effectiveness of BERT(Bidirectional Encoder Representations\nfrom Transformers) in sentiment classification tasks from a raw live dataset\ntaken from a popular microblogging platform is demonstrated. A novel T-BERT\nframework is proposed to show the enhanced performance obtainable by combining\nlatent topics with contextual BERT embeddings. Numerical experiments were\nconducted on an ensemble with about 42000 datasets using NimbleBox.ai platform\nwith a hardware configuration consisting of Nvidia Tesla K80(CUDA), 4 core CPU,\n15GB RAM running on an isolated Google Cloud Platform instance. The empirical\nresults show that the model improves in performance while adding topics to BERT\nand an accuracy rate of 90.81% on sentiment classification using BERT with the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:01:47 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Palani", "Sarojadevi", ""], ["Rajagopal", "Prabhu", ""], ["Pancholi", "Sidharth", ""]]}, {"id": "2106.01105", "submitter": "Sebastin Santy", "authors": "Sebastin Santy, Anku Rani, Monojit Choudhury", "title": "Use of Formal Ethical Reviews in NLP Literature: Historical Trends and\n  Current Practices", "comments": "Accepted at ACL 2021 Findings (7 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethical aspects of research in language technologies have received much\nattention recently. It is a standard practice to get a study involving human\nsubjects reviewed and approved by a professional ethics committee/board of the\ninstitution. How commonly do we see mention of ethical approvals in NLP\nresearch? What types of research or aspects of studies are usually subject to\nsuch reviews? With the rising concerns and discourse around the ethics of NLP,\ndo we also observe a rise in formal ethical reviews of NLP studies? And, if so,\nwould this imply that there is a heightened awareness of ethical issues that\nwas previously lacking? We aim to address these questions by conducting a\ndetailed quantitative and qualitative analysis of the ACL Anthology, as well as\ncomparing the trends in our field to those of other related disciplines, such\nas cognitive science, machine learning, data mining, and systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:12:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Santy", "Sebastin", ""], ["Rani", "Anku", ""], ["Choudhury", "Monojit", ""]]}, {"id": "2106.01112", "submitter": "Chen Zhang", "authors": "Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas\n  Friedrichs, Grandee Lee, Haizhou Li", "title": "DynaEval: Unifying Turn and Dialogue Level Evaluation", "comments": "ACL-IJCNLP 2021 (Main conference, Long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dialogue is essentially a multi-turn interaction among interlocutors.\nEffective evaluation metrics should reflect the dynamics of such interaction.\nExisting automatic metrics are focused very much on the turn-level quality,\nwhile ignoring such dynamics. To this end, we propose DynaEval, a unified\nautomatic evaluation framework which is not only capable of performing\nturn-level evaluation, but also holistically considers the quality of the\nentire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted\nto model a dialogue in totality, where the graph nodes denote each individual\nutterance and the edges represent the dependency between pairs of utterances. A\ncontrastive loss is then applied to distinguish well-formed dialogues from\ncarefully constructed negative samples. Experiments show that DynaEval\nsignificantly outperforms the state-of-the-art dialogue coherence model, and\ncorrelates strongly with human judgements across multiple dialogue evaluation\naspects at both turn and dialogue level.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:23:18 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:21:35 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 04:42:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Chen", ""], ["Chen", "Yiming", ""], ["D'Haro", "Luis Fernando", ""], ["Zhang", "Yan", ""], ["Friedrichs", "Thomas", ""], ["Lee", "Grandee", ""], ["Li", "Haizhou", ""]]}, {"id": "2106.01144", "submitter": "Chujie Zheng", "authors": "Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou\n  Yu, Yong Jiang, Minlie Huang", "title": "Towards Emotional Support Dialog Systems", "comments": "Accepted to ACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional support is a crucial ability for many conversation scenarios,\nincluding social interactions, mental health support, and customer service\nchats. Following reasonable procedures and using various support skills can\nhelp to effectively provide support. However, due to the lack of a\nwell-designed task and corpora of effective emotional support conversations,\nresearch on building emotional support into dialog systems remains untouched.\nIn this paper, we define the Emotional Support Conversation (ESC) task and\npropose an ESC Framework, which is grounded on the Helping Skills Theory. We\nconstruct an Emotion Support Conversation dataset (ESConv) with rich annotation\n(especially support strategy) in a help-seeker and supporter mode. To ensure a\ncorpus of high-quality conversations that provide examples of effective\nemotional support, we take extensive effort to design training tutorials for\nsupporters and several mechanisms for quality control during data collection.\nFinally, we evaluate state-of-the-art dialog models with respect to the ability\nto provide emotional support. Our results show the importance of support\nstrategies in providing effective emotional support and the utility of ESConv\nin training more emotional support systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:30:43 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Siyang", ""], ["Zheng", "Chujie", ""], ["Demasi", "Orianna", ""], ["Sabour", "Sahand", ""], ["Li", "Yu", ""], ["Yu", "Zhou", ""], ["Jiang", "Yong", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.01167", "submitter": "Ishani Mondal", "authors": "Ishani Mondal, Yufang Hou and Charles Jochim", "title": "End-to-End NLP Knowledge Graph Construction", "comments": "Accepted in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)\nfrom scientific papers. We focus on extracting four types of relations:\nevaluatedOn between tasks and datasets, evaluatedBy between tasks and\nevaluation metrics, as well as coreferent and related relations between the\nsame type of entities. For instance, F1-score is coreferent with F-measure. We\nintroduce novel methods for each of these relation types and apply our final\nframework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a\nlarge-scale KG, which can facilitate automatically constructing scientific\nleaderboards for the NLP community. The results of our experiments indicate\nthat the resulting KG contains high-quality information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:03:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mondal", "Ishani", ""], ["Hou", "Yufang", ""], ["Jochim", "Charles", ""]]}, {"id": "2106.01170", "submitter": "Anthony Rios", "authors": "Paras Bhatt and Anthony Rios", "title": "Detecting Bot-Generated Text by Characterizing Linguistic Accommodation\n  in Human-Bot Interactions", "comments": "13 pages, to be published in Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language generation models' democratization benefits many domains, from\nanswering health-related questions to enhancing education by providing\nAI-driven tutoring services. However, language generation models'\ndemocratization also makes it easier to generate human-like text at-scale for\nnefarious activities, from spreading misinformation to targeting specific\ngroups with hate speech. Thus, it is essential to understand how people\ninteract with bots and develop methods to detect bot-generated text. This paper\nshows that bot-generated text detection methods are more robust across datasets\nand models if we use information about how people respond to it rather than\nusing the bot's text directly. We also analyze linguistic alignment, providing\ninsight into differences between human-human and human-bot conversations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:10:28 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bhatt", "Paras", ""], ["Rios", "Anthony", ""]]}, {"id": "2106.01183", "submitter": "Sara Rajaee", "authors": "Sara Rajaee and Mohammad Taher Pilehvar", "title": "A Cluster-based Approach for Improving Isotropy in Contextual Embedding\n  Space", "comments": "To appear in ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation degeneration problem in Contextual Word Representations\n(CWRs) hurts the expressiveness of the embedding space by forming an\nanisotropic cone where even unrelated words have excessively positive\ncorrelations. Existing techniques for tackling this issue require a learning\nprocess to re-train models with additional objectives and mostly employ a\nglobal assessment to study isotropy. Our quantitative analysis over isotropy\nshows that a local assessment could be more accurate due to the clustered\nstructure of CWRs. Based on this observation, we propose a local cluster-based\nmethod to address the degeneration issue in contextual embedding spaces. We\nshow that in clusters including punctuations and stop words, local dominant\ndirections encode structural information, removing which can improve CWRs\nperformance on semantic tasks. Moreover, we find that tense information in verb\nrepresentations dominates sense semantics. We show that removing dominant\ndirections of verb representations can transform the space to better suit\nsemantic applications. Our experiments demonstrate that the proposed\ncluster-based method can mitigate the degeneration problem on multiple tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:26:37 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rajaee", "Sara", ""], ["Pilehvar", "Mohammad Taher", ""]]}, {"id": "2106.01186", "submitter": "Dvir Ginzburg", "authors": "Dvir Ginzburg and Itzik Malkiel and Oren Barkan and Avi Caciularu and\n  Noam Koenigstein", "title": "Self-Supervised Document Similarity Ranking via Contextualized Language\n  Models and Hierarchical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel model for the problem of ranking a collection of documents\naccording to their semantic similarity to a source (query) document. While the\nproblem of document-to-document similarity ranking has been studied, most\nmodern methods are limited to relatively short documents or rely on the\nexistence of \"ground-truth\" similarity labels. Yet, in most common real-world\ncases, similarity ranking is an unsupervised problem as similarity labels are\nunavailable. Moreover, an ideal model should not be restricted by documents'\nlength. Hence, we introduce SDR, a self-supervised method for document\nsimilarity that can be applied to documents of arbitrary length. Importantly,\nSDR can be effectively applied to extremely long documents, exceeding the 4,096\nmaximal token limits of Longformer. Extensive evaluations on large document\ndatasets show that SDR significantly outperforms its alternatives across all\nmetrics. To accelerate future research on unlabeled long document similarity\nranking, and as an additional contribution to the community, we herein publish\ntwo human-annotated test sets of long documents similarity evaluation. The SDR\ncode and datasets are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:29:35 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ginzburg", "Dvir", ""], ["Malkiel", "Itzik", ""], ["Barkan", "Oren", ""], ["Caciularu", "Avi", ""], ["Koenigstein", "Noam", ""]]}, {"id": "2106.01191", "submitter": "Jiasheng Si", "authors": "Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, Yulan He", "title": "Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact\n  Verification", "comments": "Accepted by ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact verification is a challenging task that requires simultaneously\nreasoning and aggregating over multiple retrieved pieces of evidence to\nevaluate the truthfulness of a claim. Existing approaches typically (i) explore\nthe semantic interaction between the claim and evidence at different\ngranularity levels but fail to capture their topical consistency during the\nreasoning process, which we believe is crucial for verification; (ii) aggregate\nmultiple pieces of evidence equally without considering their implicit stances\nto the claim, thereby introducing spurious information. To alleviate the above\nissues, we propose a novel topic-aware evidence reasoning and stance-aware\naggregation model for more accurate fact verification, with the following four\nkey properties: 1) checking topical consistency between the claim and evidence;\n2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring\nsemantic similarity between the global topic information and the semantic\nrepresentation of evidence; 4) aggregating evidence based on their implicit\nstances to the claim. Extensive experiments conducted on the two benchmark\ndatasets demonstrate the superiority of the proposed model over several\nstate-of-the-art approaches for fact verification. The source code can be\nobtained from https://github.com/jasenchn/TARSA.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:33:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Si", "Jiasheng", ""], ["Zhou", "Deyu", ""], ["Li", "Tongzhe", ""], ["Shi", "Xingyu", ""], ["He", "Yulan", ""]]}, {"id": "2106.01195", "submitter": "Tuhin Chakrabarty Mr", "authors": "Tuhin Chakrabarty, Debanjan Ghosh, Adam Poliak, Smaranda Muresan", "title": "Figurative Language in Recognizing Textual Entailment", "comments": "ACL 2021 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a collection of recognizing textual entailment (RTE) datasets\nfocused on figurative language. We leverage five existing datasets annotated\nfor a variety of figurative language -- simile, metaphor, and irony -- and\nframe them into over 12,500 RTE examples.We evaluate how well state-of-the-art\nmodels trained on popular RTE datasets capture different aspects of figurative\nlanguage. Our results and analyses indicate that these models might not\nsufficiently capture figurative language, struggling to perform pragmatic\ninference and reasoning about world knowledge. Ultimately, our datasets provide\na challenging testbed for evaluating RTE models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:37:32 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:20:10 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Chakrabarty", "Tuhin", ""], ["Ghosh", "Debanjan", ""], ["Poliak", "Adam", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2106.01199", "submitter": "Qingqing Cao", "authors": "Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian,\n  Niranjan Balasubramanian", "title": "IrEne: Interpretable Energy Prediction for Transformers", "comments": "ACL 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Existing software-based energy measurements of NLP models are not accurate\nbecause they do not consider the complex interactions between energy\nconsumption and model execution. We present IrEne, an interpretable and\nextensible energy prediction system that accurately predicts the inference\nenergy consumption of a wide range of Transformer-based NLP models. IrEne\nconstructs a model tree graph that breaks down the NLP model into modules that\nare further broken down into low-level machine learning (ML) primitives. IrEne\npredicts the inference energy consumption of the ML primitives as a function of\ngeneralizable features and fine-grained runtime resource usage. IrEne then\naggregates these low-level predictions recursively to predict the energy of\neach module and finally of the entire model. Experiments across multiple\nTransformer models show IrEne predicts inference energy consumption of\ntransformer models with an error of under 7% compared to the ground truth. In\ncontrast, existing energy models see an error of over 50%. We also show how\nIrEne can be used to conduct energy bottleneck analysis and to easily evaluate\nthe energy impact of different architectural choices. We release the code and\ndata at https://github.com/StonyBrookNLP/irene.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:43:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Cao", "Qingqing", ""], ["Lal", "Yash Kumar", ""], ["Trivedi", "Harsh", ""], ["Balasubramanian", "Aruna", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2106.01207", "submitter": "Forrest Davis", "authors": "Forrest Davis and Marten van Schijndel", "title": "Uncovering Constraint-Based Behavior in Neural Models via Targeted\n  Fine-Tuning", "comments": "Proceedings of 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of literature has focused on detailing the linguistic\nknowledge embedded in large, pretrained language models. Existing work has\nshown that non-linguistic biases in models can drive model behavior away from\nlinguistic generalizations. We hypothesized that competing linguistic processes\nwithin a language, rather than just non-linguistic model biases, could obscure\nunderlying linguistic knowledge. We tested this claim by exploring a single\nphenomenon in four languages: English, Chinese, Spanish, and Italian. While\nhuman behavior has been found to be similar across languages, we find\ncross-linguistic variation in model behavior. We show that competing processes\nin a language act as constraints on model behavior and demonstrate that\ntargeted fine-tuning can re-weight the learned constraints, uncovering\notherwise dormant linguistic knowledge in models. Our results suggest that\nmodels need to learn both the linguistic constraints in a language and their\nrelative ranking, with mismatches in either producing non-human-like behavior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:52:11 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Davis", "Forrest", ""], ["van Schijndel", "Marten", ""]]}, {"id": "2106.01210", "submitter": "Arie Cattan", "authors": "Arie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar Joshi, Ido Dagan", "title": "Cross-document Coreference Resolution over Predicted Mentions", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coreference resolution has been mostly investigated within a single document\nscope, showing impressive progress in recent years based on end-to-end models.\nHowever, the more challenging task of cross-document (CD) coreference\nresolution remained relatively under-explored, with the few recent models\napplied only to gold mentions. Here, we introduce the first end-to-end model\nfor CD coreference resolution from raw text, which extends the prominent model\nfor within-document coreference to the CD setting. Our model achieves\ncompetitive results for event and entity coreference resolution on gold\nmentions. More importantly, we set first baseline results, on the standard ECB+\ndataset, for CD coreference resolution over predicted mentions. Further, our\nmodel is simpler and more efficient than recent CD coreference resolution\nsystems, while not using any external resources.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:56:28 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Cattan", "Arie", ""], ["Eirew", "Alon", ""], ["Stanovsky", "Gabriel", ""], ["Joshi", "Mandar", ""], ["Dagan", "Ido", ""]]}, {"id": "2106.01221", "submitter": "Xiang Yue", "authors": "Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li, Huan Sun and Sherman\n  S. M. Chow", "title": "Differential Privacy for Text Analytics via Natural Text Sanitization", "comments": "ACL-ICJNLP'21 Findings; The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texts convey sophisticated knowledge. However, texts also convey sensitive\ninformation. Despite the success of general-purpose language models and\ndomain-specific mechanisms with differential privacy (DP), existing text\nsanitization mechanisms still provide low utility, as cursed by the\nhigh-dimensional text representation. The companion issue of utilizing\nsanitized texts for downstream analytics is also under-explored. This paper\ntakes a direct approach to text sanitization. Our insight is to consider both\nsensitivity and similarity via our new local DP notion. The sanitized texts\nalso contribute to our sanitization-aware pretraining and fine-tuning, enabling\nprivacy-preserving natural language processing over the BERT language model\nwith promising utility. Surprisingly, the high utility does not boost up the\nsuccess rate of inference attacks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:15:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yue", "Xiang", ""], ["Du", "Minxin", ""], ["Wang", "Tianhao", ""], ["Li", "Yaliang", ""], ["Sun", "Huan", ""], ["Chow", "Sherman S. M.", ""]]}, {"id": "2106.01223", "submitter": "Hang Yan", "authors": "Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang and Xipeng Qiu", "title": "A Unified Generative Framework for Various NER Subtasks", "comments": "Accepted in the main conference of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is the task of identifying spans that\nrepresent entities in sentences. Whether the entity spans are nested or\ndiscontinuous, the NER task can be categorized into the flat NER, nested NER,\nand discontinuous NER subtasks. These subtasks have been mainly solved by the\ntoken-level sequence labelling or span-level classification. However, these\nsolutions can hardly tackle the three kinds of NER subtasks concurrently. To\nthat end, we propose to formulate the NER subtasks as an entity span sequence\ngeneration task, which can be solved by a unified sequence-to-sequence\n(Seq2Seq) framework. Based on our unified framework, we can leverage the\npre-trained Seq2Seq model to solve all three kinds of NER subtasks without the\nspecial design of the tagging schema or ways to enumerate spans. We exploit\nthree types of entity representations to linearize entities into a sequence.\nOur proposed framework is easy-to-implement and achieves state-of-the-art\n(SoTA) or near SoTA performance on eight English NER datasets, including two\nflat NER datasets, three nested NER datasets, and three discontinuous NER\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:19:23 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yan", "Hang", ""], ["Gui", "Tao", ""], ["Dai", "Junqi", ""], ["Guo", "Qipeng", ""], ["Zhang", "Zheng", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2106.01227", "submitter": "Jayadev Billa", "authors": "Jayadev Billa", "title": "Improving low-resource ASR performance with untranscribed out-of-domain\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised training (SST) is a common approach to leverage\nuntranscribed/unlabeled speech data to improve automatic speech recognition\nperformance in low-resource languages. However, if the available unlabeled\nspeech is mismatched to the target domain, SST is not as effective, and in many\ncases performs worse than the original system. In this paper, we address the\nissue of low-resource ASR when only untranscribed out-of-domain speech data is\nreadily available in the target language. Specifically, we look to improve\nperformance on conversational/telephony speech (target domain) using web\nresources, in particular YouTube data, which more closely resembles\nnews/topical broadcast data. Leveraging SST, we show that while in some cases\nsimply pooling the out-of-domain data with the training data lowers word error\nrate (WER), in all cases, we see improvements if we train first with the\nout-of-domain data and then fine-tune the resulting model with the original\ntraining data. Using 2000 hours of speed perturbed YouTube audio in each target\nlanguage, with semi-supervised transcripts, we show improvements on multiple\nlanguages/data sets, of up to 16.3% relative improvement in WER over the\nbaseline systems and up to 7.4% relative improvement in WER over a system that\nsimply pools the out-of-domain data with the training data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:23:34 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Billa", "Jayadev", ""]]}, {"id": "2106.01228", "submitter": "Kevin Stowe", "authors": "Kevin Stowe, Tuhin Chakrabarty, Nanyun Peng, Smaranda Muresan, Iryna\n  Gurevych", "title": "Metaphor Generation with Conceptual Mappings", "comments": "13 pages, 3 figures, to be published in the Joint Conference of the\n  59th Annual Meeting of the Association for Computational Linguistics and the\n  11th International Joint Conference on Natural Language Processing\n  (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating metaphors is a difficult task as it requires understanding nuanced\nrelationships between abstract concepts. In this paper, we aim to generate a\nmetaphoric sentence given a literal expression by replacing relevant verbs.\nGuided by conceptual metaphor theory, we propose to control the generation\nprocess by encoding conceptual mappings between cognitive domains to generate\nmeaningful metaphoric expressions. To achieve this, we develop two methods: 1)\nusing FrameNet-based embeddings to learn mappings between domains and applying\nthem at the lexical level (CM-Lex), and 2) deriving source/target pairs to\ntrain a controlled seq-to-seq generation model (CM-BART). We assess our methods\nthrough automatic and human evaluation for basic metaphoricity and conceptual\nmetaphor presence. We show that the unsupervised CM-Lex model is competitive\nwith recent deep learning metaphor generation systems, and CM-BART outperforms\nall other models both in automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:27:05 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Stowe", "Kevin", ""], ["Chakrabarty", "Tuhin", ""], ["Peng", "Nanyun", ""], ["Muresan", "Smaranda", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2106.01229", "submitter": "Tatsuki Kuribayashi", "authors": "Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki\n  Asahara, Kentaro Inui", "title": "Lower Perplexity is Not Always Human-Like", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In computational psycholinguistics, various language models have been\nevaluated against human reading behavior (e.g., eye movement) to build\nhuman-like computational models. However, most previous efforts have focused\nalmost exclusively on English, despite the recent trend towards linguistic\nuniversal within the general community. In order to fill the gap, this paper\ninvestigates whether the established results in computational psycholinguistics\ncan be generalized across languages. Specifically, we re-examine an established\ngeneralization -- the lower perplexity a language model has, the more\nhuman-like the language model is -- in Japanese with typologically different\nstructures from English. Our experiments demonstrate that this established\ngeneralization exhibits a surprising lack of universality; namely, lower\nperplexity is not always human-like. Moreover, this discrepancy between English\nand Japanese is further explored from the perspective of (non-)uniform\ninformation density. Overall, our results suggest that a cross-lingual\nevaluation will be necessary to construct human-like computational models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:27:29 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kuribayashi", "Tatsuki", ""], ["Oseki", "Yohei", ""], ["Ito", "Takumi", ""], ["Yoshida", "Ryo", ""], ["Asahara", "Masayuki", ""], ["Inui", "Kentaro", ""]]}, {"id": "2106.01251", "submitter": "Vishal Vinod", "authors": "Vishal Vinod, Susmit Agrawal, Vipul Gaurav, Pallavi R, Savita\n  Choudhary", "title": "Multilingual Medical Question Answering and Information Retrieval for\n  Rural Health Intelligence Access", "comments": null, "journal-ref": "ICLR 2021 Workshop", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In rural regions of several developing countries, access to quality\nhealthcare, medical infrastructure, and professional diagnosis is largely\nunavailable. Many of these regions are gradually gaining access to internet\ninfrastructure, although not with a strong enough connection to allow for\nsustained communication with a medical practitioner. Several deaths resulting\nfrom this lack of medical access, absence of patient's previous health records,\nand the unavailability of information in indigenous languages can be easily\nprevented. In this paper, we describe an approach leveraging the phenomenal\nprogress in Machine Learning and NLP (Natural Language Processing) techniques\nto design a model that is low-resource, multilingual, and a preliminary\nfirst-point-of-contact medical assistant. Our contribution includes defining\nthe NLP pipeline required for named-entity-recognition, language-agnostic\nsentence embedding, natural language translation, information retrieval,\nquestion answering, and generative pre-training for final query processing. We\nobtain promising results for this pipeline and preliminary results for EHR\n(Electronic Health Record) analysis with text summarization for medical\npractitioners to peruse for their diagnosis. Through this NLP pipeline, we aim\nto provide preliminary medical information to the user and do not claim to\nsupplant diagnosis from qualified medical practitioners. Using the input from\nsubject matter experts, we have compiled a large corpus to pre-train and\nfine-tune our BioBERT based NLP model for the specific tasks. We expect recent\nadvances in NLP architectures, several of which are efficient and\nprivacy-preserving models, to further the impact of our solution and improve on\nindividual task performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:05:24 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vinod", "Vishal", ""], ["Agrawal", "Susmit", ""], ["Gaurav", "Vipul", ""], ["R", "Pallavi", ""], ["Choudhary", "Savita", ""]]}, {"id": "2106.01263", "submitter": "Chiyu Song", "authors": "Chiyu Song, Hongliang He, Huachuan Qiu, Haofei Yu, Zhenzhong Lan", "title": "Global-Selector: A New Benchmark Dataset and Model Architecture for\n  Multi-turn Response Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential component of dialogue systems, multi-turn response selection\naims to pick out the optimal response among a set of candidates to improve the\ndialogue fluency. In this paper, we investigate three problems of current\nresponse selection approaches, especially for generation-based conversational\nagents: (i) Existing approaches are often formulated as a sentence scoring\nproblem, which does not consider relationships between responses. (ii) Existing\nmodels tend to select undesirable candidates that have large overlaps with the\ndialogue history. (iii) Negative instances in training are mainly constructed\nby random sampling from the corpus, whereas generated candidates in practice\ntypically have a closer distribution. To address the above problems, we create\na new dataset called ConvAI2+ and propose a new response selector called\nGlobal-Selector. Experimental results show that Global-Selector trained on\nConvAI2+ have noticeable improvements in both accuracy and inference speed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:14:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Song", "Chiyu", ""], ["He", "Hongliang", ""], ["Qiu", "Huachuan", ""], ["Yu", "Haofei", ""], ["Lan", "Zhenzhong", ""]]}, {"id": "2106.01269", "submitter": "Soujanya Poria", "authors": "Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria, Eduard Hovy", "title": "More Identifiable yet Equally Performant Transformers for Text\n  Classification", "comments": "ACL 2021", "journal-ref": "ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Interpretability is an important aspect of the trustworthiness of a model's\npredictions. Transformer's predictions are widely explained by the attention\nweights, i.e., a probability distribution generated at its self-attention unit\n(head). Current empirical studies provide shreds of evidence that attention\nweights are not explanations by proving that they are not unique. A recent\nstudy showed theoretical justifications to this observation by proving the\nnon-identifiability of attention weights. For a given input to a head and its\noutput, if the attention weights generated in it are unique, we call the\nweights identifiable. In this work, we provide deeper theoretical analysis and\nempirical observations on the identifiability of attention weights. Ignored in\nthe previous works, we find the attention weights are more identifiable than we\ncurrently perceive by uncovering the hidden role of the key vector. However,\nthe weights are still prone to be non-unique attentions that make them unfit\nfor interpretation. To tackle this issue, we provide a variant of the encoder\nlayer that decouples the relationship between key and value vector and provides\nidentifiable weights up to the desired length of the input. We prove the\napplicability of such variations by providing empirical justifications on\nvaried text classification tasks. The implementations are available at\nhttps://github.com/declare-lab/identifiable-transformers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:21:38 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bhardwaj", "Rishabh", ""], ["Majumder", "Navonil", ""], ["Poria", "Soujanya", ""], ["Hovy", "Eduard", ""]]}, {"id": "2106.01317", "submitter": "Yichen Jiang", "authors": "Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha\n  Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, Jianfeng\n  Gao", "title": "Enriching Transformers with Structured Tensor-Product Representations\n  for Abstractive Summarization", "comments": "NAACL 2021 (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization, the task of generating a concise summary of input\ndocuments, requires: (1) reasoning over the source document to determine the\nsalient pieces of information scattered across the long document, and (2)\ncomposing a cohesive text by reconstructing these salient facts into a shorter\nsummary that faithfully reflects the complex relations connecting these facts.\nIn this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019), an architecture\nthat enriches the original Transformer (Vaswani et al., 2017) with the\nexplicitly compositional Tensor Product Representation (TPR), for the task of\nabstractive summarization. The key feature of our model is a structural bias\nthat we introduce by encoding two separate representations for each token to\nrepresent the syntactic structure (with role vectors) and semantic content\n(with filler vectors) separately. The model then binds the role and filler\nvectors into the TPR as the layer output. We argue that the structured\nintermediate representations enable the model to take better control of the\ncontents (salient facts) and structures (the syntax that connects the facts)\nwhen generating the summary. Empirically, we show that our TP-TRANSFORMER\noutperforms the Transformer and the original TP-TRANSFORMER significantly on\nseveral abstractive summarization datasets based on both automatic and human\nevaluations. On several syntactic and semantic probing tasks, we demonstrate\nthe emergent structural information in the role vectors and improved syntactic\ninterpretability in the TPR layer outputs. Code and models are available at\nhttps://github.com/jiangycTarheel/TPT-Summ.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:32:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jiang", "Yichen", ""], ["Celikyilmaz", "Asli", ""], ["Smolensky", "Paul", ""], ["Soulos", "Paul", ""], ["Rao", "Sudha", ""], ["Palangi", "Hamid", ""], ["Fernandez", "Roland", ""], ["Smith", "Caitlin", ""], ["Bansal", "Mohit", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2106.01335", "submitter": "Tianchu Ji", "authors": "Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H. Andrew\n  Schwartz, Niranjan Balasubramanian", "title": "On the Distribution, Sparsity, and Inference-time Quantization of\n  Attention Values in Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much information do NLP tasks really need from a transformer's attention\nmechanism at application-time (inference)? From recent work, we know that there\nis sparsity in transformers and that the floating-points within its computation\ncan be discretized to fewer values with minimal loss to task accuracies.\nHowever, this requires retraining or even creating entirely new models, both of\nwhich can be expensive and carbon-emitting. Focused on optimizations that do\nnot require training, we systematically study the full range of typical\nattention values necessary. This informs the design of an inference-time\nquantization technique using both pruning and log-scaled mapping which produces\nonly a few (e.g. $2^3$) unique values. Over the tasks of question answering and\nsentiment analysis, we find nearly 80% of attention values can be pruned to\nzeros with minimal ($< 1.0\\%$) relative loss in accuracy. We use this pruning\ntechnique in conjunction with quantizing the attention values to only a 3-bit\nformat, without retraining, resulting in only a 0.8% accuracy reduction on\nquestion answering with fine-tuned RoBERTa.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:45:47 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ji", "Tianchu", ""], ["Jain", "Shraddhan", ""], ["Ferdman", "Michael", ""], ["Milder", "Peter", ""], ["Schwartz", "H. Andrew", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2106.01354", "submitter": "Swarnadeep Saha", "authors": "Swarnadeep Saha, Prateek Yadav, Mohit Bansal", "title": "multiPRover: Generating Multiple Proofs for Improved Interpretability in\n  Rule Reasoning", "comments": "NAACL 2021 (16 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on a type of linguistic formal reasoning where the goal is to reason\nover explicit knowledge in the form of natural language facts and rules (Clark\net al., 2020). A recent work, named PRover (Saha et al., 2020), performs such\nreasoning by answering a question and also generating a proof graph that\nexplains the answer. However, compositional reasoning is not always unique and\nthere may be multiple ways of reaching the correct answer. Thus, in our work,\nwe address a new and challenging problem of generating multiple proof graphs\nfor reasoning over natural language rule-bases. Each proof provides a different\nrationale for the answer, thereby improving the interpretability of such\nreasoning systems. In order to jointly learn from all proof graphs and exploit\nthe correlations between multiple proofs for a question, we pose this task as a\nset generation problem over structured output spaces where each proof is\nrepresented as a directed graph. We propose two variants of a proof-set\ngeneration model, multiPRover. Our first model, Multilabel-multiPRover,\ngenerates a set of proofs via multi-label classification and implicit\nconditioning between the proofs; while the second model, Iterative-multiPRover,\ngenerates proofs iteratively by explicitly conditioning on the previously\ngenerated proofs. Experiments on multiple synthetic, zero-shot, and\nhuman-paraphrased datasets reveal that both multiPRover models significantly\noutperform PRover on datasets containing multiple gold proofs.\nIterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios\nwhere all examples have single correct proofs. It also generalizes better to\nquestions requiring higher depths of reasoning where multiple proofs are more\nfrequent. Our code and models are publicly available at\nhttps://github.com/swarnaHub/multiPRover\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:58:35 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Saha", "Swarnadeep", ""], ["Yadav", "Prateek", ""], ["Bansal", "Mohit", ""]]}, {"id": "2106.01415", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Kazuhiro Kobayashi, Yu-Huai Peng, Ching-Feng Liu, Yu\n  Tsao, Hsin-Min Wang, Tomoki Toda", "title": "A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker\n  Identity in Dysarthric Voice Conversion", "comments": "Accepted to Interspeech 2021. 5 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new paradigm for maintaining speaker identity in dysarthric\nvoice conversion (DVC). The poor quality of dysarthric speech can be greatly\nimproved by statistical VC, but as the normal speech utterances of a dysarthria\npatient are nearly impossible to collect, previous work failed to recover the\nindividuality of the patient. In light of this, we suggest a novel, two-stage\napproach for DVC, which is highly flexible in that no normal speech of the\npatient is required. First, a powerful parallel sequence-to-sequence model\nconverts the input dysarthric speech into a normal speech of a reference\nspeaker as an intermediate product, and a nonparallel, frame-wise VC model\nrealized with a variational autoencoder then converts the speaker identity of\nthe reference speech back to that of the patient while assumed to be capable of\npreserving the enhanced quality. We investigate several design options.\nExperimental evaluation results demonstrate the potential of our approach to\nimproving the quality of the dysarthric speech while maintaining the speaker\nidentity.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 18:41:03 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Kobayashi", "Kazuhiro", ""], ["Peng", "Yu-Huai", ""], ["Liu", "Ching-Feng", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""], ["Toda", "Tomoki", ""]]}, {"id": "2106.01424", "submitter": "Marcella Cornia", "authors": "Marco Cagrandi, Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi,\n  Rita Cucchiara", "title": "Learning to Select: A Fully Attentive Approach for Novel Object\n  Captioning", "comments": "ICMR 2021", "journal-ref": null, "doi": "10.1145/3460426.3463587", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models have lately shown impressive results when applied to\nstandard datasets. Switching to real-life scenarios, however, constitutes a\nchallenge due to the larger variety of visual concepts which are not covered in\nexisting training sets. For this reason, novel object captioning (NOC) has\nrecently emerged as a paradigm to test captioning models on objects which are\nunseen during the training phase. In this paper, we present a novel approach\nfor NOC that learns to select the most relevant objects of an image, regardless\nof their adherence to the training set, and to constrain the generative process\nof a language model accordingly. Our architecture is fully-attentive and\nend-to-end trainable, also when incorporating constraints. We perform\nexperiments on the held-out COCO dataset, where we demonstrate improvements\nover the state of the art, both in terms of adaptability to novel objects and\ncaption quality.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:11:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cagrandi", "Marco", ""], ["Cornia", "Marcella", ""], ["Stefanini", "Matteo", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2106.01444", "submitter": "Joshua Feinglass", "authors": "Joshua Feinglass and Yezhou Yang", "title": "SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption\n  Evaluation via Typicality Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The open-ended nature of visual captioning makes it a challenging area for\nevaluation. The majority of proposed models rely on specialized training to\nimprove human-correlation, resulting in limited adoption, generalizability, and\nexplainabilty. We introduce \"typicality\", a new formulation of evaluation\nrooted in information theory, which is uniquely suited for problems lacking a\ndefinite ground truth. Typicality serves as our framework to develop a novel\nsemantic comparison, SPARCS, as well as referenceless fluency evaluation\nmetrics. Over the course of our analysis, two separate dimensions of fluency\nnaturally emerge: style, captured by metric SPURTS, and grammar, captured in\nthe form of grammatical outlier penalties. Through extensive experiments and\nablation studies on benchmark datasets, we show how these decomposed dimensions\nof semantics and fluency provide greater system-level insight into captioner\ndifferences. Our proposed metrics along with their combination, SMURF, achieve\nstate-of-the-art correlation with human judgment when compared with other\nrule-based evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:58:20 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Feinglass", "Joshua", ""], ["Yang", "Yezhou", ""]]}, {"id": "2106.01451", "submitter": "Richard Diehl Martinez", "authors": "Richard Diehl Martinez, Scott Novotney, Ivan Bulyko, Ariya Rastrow,\n  Andreas Stolcke, Ankur Gandhe", "title": "Attention-based Contextual Language Model Adaptation for Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Language modeling (LM) for automatic speech recognition (ASR) does not\nusually incorporate utterance level contextual information. For some domains\nlike voice assistants, however, additional context, such as the time at which\nan utterance was spoken, provides a rich input signal. We introduce an\nattention mechanism for training neural speech recognition language models on\nboth text and non-linguistic contextual data. When applied to a large\nde-identified dataset of utterances collected by a popular voice assistant\nplatform, our method reduces perplexity by 7.0% relative over a standard LM\nthat does not incorporate contextual information. When evaluated on utterances\nextracted from the long tail of the dataset, our method improves perplexity by\n9.0% relative over a standard LM and by over 2.8% relative when compared to a\nstate-of-the-art model for contextual LM.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:19:57 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Martinez", "Richard Diehl", ""], ["Novotney", "Scott", ""], ["Bulyko", "Ivan", ""], ["Rastrow", "Ariya", ""], ["Stolcke", "Andreas", ""], ["Gandhe", "Ankur", ""]]}, {"id": "2106.01452", "submitter": "Steffen Eger", "authors": "Yannik Keller, Jan Mackensen, Steffen Eger", "title": "BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively\n  Inspired Orthographic Adversarial Attacks", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks expose important blind spots of deep learning systems.\nWhile word- and sentence-level attack scenarios mostly deal with finding\nsemantic paraphrases of the input that fool NLP models, character-level attacks\ntypically insert typos into the input stream. It is commonly thought that these\nare easier to defend via spelling correction modules. In this work, we show\nthat both a standard spellchecker and the approach of Pruthi et al. (2019),\nwhich trains to defend against insertions, deletions and swaps, perform poorly\non the character-level benchmark recently proposed in Eger and Benz (2020)\nwhich includes more challenging attacks such as visual and phonetic\nperturbations and missing word segmentations. In contrast, we show that an\nuntrained iterative approach which combines context-independent character-level\ninformation with context-dependent information from BERT's masked language\nmodeling can perform on par with human crowd-workers from Amazon Mechanical\nTurk (AMT) supervised via 3-shot learning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:21:03 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Keller", "Yannik", ""], ["Mackensen", "Jan", ""], ["Eger", "Steffen", ""]]}, {"id": "2106.01463", "submitter": "Hang Le", "authors": "Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent\n  Besacier", "title": "Lightweight Adapter Tuning for Multilingual Speech Translation", "comments": "Accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapter modules were recently introduced as an efficient alternative to\nfine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters\nof a model and injecting lightweight modules between layers, resulting in the\naddition of only a small number of task-specific trainable parameters. While\nadapter tuning was investigated for multilingual neural machine translation,\nthis paper proposes a comprehensive analysis of adapters for multilingual\nspeech translation (ST). Starting from different pre-trained models (a\nmultilingual ST trained on parallel data or a multilingual BART (mBART) trained\non non-parallel multilingual data), we show that adapters can be used to: (a)\nefficiently specialize ST to specific language pairs with a low extra cost in\nterms of parameters, and (b) transfer from an automatic speech recognition\n(ASR) task and an mBART pre-trained model to a multilingual ST task.\nExperiments show that adapter tuning offer competitive results to full\nfine-tuning, while being much more parameter-efficient.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:51:42 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:30:17 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Le", "Hang", ""], ["Pino", "Juan", ""], ["Wang", "Changhan", ""], ["Gu", "Jiatao", ""], ["Schwab", "Didier", ""], ["Besacier", "Laurent", ""]]}, {"id": "2106.01465", "submitter": "Jieyu Zhao", "authors": "Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and\n  Kai-Wei Chang", "title": "Ethical-Advice Taker: Do Language Models Understand Natural Language\n  Interventions?", "comments": "9 pages, Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to use natural language to intervene in a model's behavior and\nalter its prediction in a desired way? We investigate the effectiveness of\nnatural language interventions for reading-comprehension systems, studying this\nin the context of social stereotypes. Specifically, we propose a new language\nunderstanding task, Linguistic Ethical Interventions (LEI), where the goal is\nto amend a question-answering (QA) model's unethical behavior by communicating\ncontext-specific principles of ethics and equity to it. To this end, we build\nupon recent methods for quantifying a system's social stereotypes, augmenting\nthem with different kinds of ethical interventions and the desired model\nbehavior under such interventions. Our zero-shot evaluation finds that even\ntoday's powerful neural language models are extremely poor ethical-advice\ntakers, that is, they respond surprisingly little to ethical interventions even\nthough these interventions are stated as simple sentences. Few-shot learning\nimproves model behavior but remains far from the desired outcome, especially\nwhen evaluated for various types of generalization. Our new task thus poses a\nnovel language understanding challenge for the community.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:57:58 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhao", "Jieyu", ""], ["Khashabi", "Daniel", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2106.01478", "submitter": "Fajri Koto", "authors": "Fajri Koto and Jey Han Lau and Timothy Baldwin", "title": "Evaluating the Efficacy of Summarization Evaluation across Languages", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While automatic summarization evaluation methods developed for English are\nroutinely applied to other languages, this is the first attempt to\nsystematically quantify their panlinguistic efficacy. We take a summarization\ncorpus for eight different languages, and manually annotate generated summaries\nfor focus (precision) and coverage (recall). Based on this, we evaluate 19\nsummarization evaluation metrics, and find that using multilingual BERT within\nBERTScore performs well across all languages, at a level above that for\nEnglish.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:28:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Koto", "Fajri", ""], ["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""]]}, {"id": "2106.01481", "submitter": "Anne Marie Stupinski", "authors": "Anne Marie Stupinski, Thayer Alshaabi, Michael V. Arnold, Jane Lydia\n  Adams, Joshua R. Minot, Matthew Price, Peter Sheridan Dodds, Christopher M.\n  Danforth", "title": "Quantifying language changes surrounding mental health on Twitter", "comments": "12 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health challenges are thought to afflict around 10% of the global\npopulation each year, with many going untreated due to stigma and limited\naccess to services. Here, we explore trends in words and phrases related to\nmental health through a collection of 1- , 2-, and 3-grams parsed from a data\nstream of roughly 10% of all English tweets since 2012. We examine temporal\ndynamics of mental health language, finding that the popularity of the phrase\n'mental health' increased by nearly two orders of magnitude between 2012 and\n2018. We observe that mentions of 'mental health' spike annually and reliably\ndue to mental health awareness campaigns, as well as unpredictably in response\nto mass shootings, celebrities dying by suicide, and popular fictional stories\nportraying suicide. We find that the level of positivity of messages containing\n'mental health', while stable through the growth period, has declined recently.\nFinally, we use the ratio of original tweets to retweets to quantify the\nfraction of appearances of mental health language due to social amplification.\nSince 2015, mentions of mental health have become increasingly due to retweets,\nsuggesting that stigma associated with discussion of mental health on Twitter\nhas diminished with time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:35:53 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Stupinski", "Anne Marie", ""], ["Alshaabi", "Thayer", ""], ["Arnold", "Michael V.", ""], ["Adams", "Jane Lydia", ""], ["Minot", "Joshua R.", ""], ["Price", "Matthew", ""], ["Dodds", "Peter Sheridan", ""], ["Danforth", "Christopher M.", ""]]}, {"id": "2106.01491", "submitter": "Christine Herlihy", "authors": "Christine Herlihy and Rachel Rudinger", "title": "MedNLI Is Not Immune: Natural Language Inference Artifacts in the\n  Clinical Domain", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdworker-constructed natural language inference (NLI) datasets have been\nfound to contain statistical artifacts associated with the annotation process\nthat allow hypothesis-only classifiers to achieve better-than-random\nperformance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).\nWe investigate whether MedNLI, a physician-annotated dataset with premises\nextracted from clinical notes, contains such artifacts (Romanov and Shivade,\n2018). We find that entailed hypotheses contain generic versions of specific\nconcepts in the premise, as well as modifiers related to responsiveness,\nduration, and probability. Neutral hypotheses feature conditions and behaviors\nthat co-occur with, or cause, the condition(s) in the premise. Contradiction\nhypotheses feature explicit negation of the premise and implicit negation via\nassertion of good health. Adversarial filtering demonstrates that performance\ndegrades when evaluated on the difficult subset. We provide partition\ninformation and recommendations for alternative dataset construction strategies\nfor knowledge-intensive domains.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:12:39 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Herlihy", "Christine", ""], ["Rudinger", "Rachel", ""]]}, {"id": "2106.01494", "submitter": "Shujian Zhang", "authors": "Shujian Zhang, Chengyue Gong, Eunsol Choi", "title": "Knowing More About Questions Can Help: Improving Calibration in Question\n  Answering", "comments": "ACL 2021 (finding)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study calibration in question answering, estimating whether model\ncorrectly predicts answer for each question. Unlike prior work which mainly\nrely on the model's confidence score, our calibrator incorporates information\nabout the input example (e.g., question and the evidence context). Together\nwith data augmentation via back translation, our simple approach achieves 5-10%\ngains in calibration accuracy on reading comprehension benchmarks. Furthermore,\nwe present the first calibration study in the open retrieval setting, comparing\nthe calibration accuracy of retrieval-based span prediction models and answer\ngeneration models. Here again, our approach shows consistent gains over\ncalibrators relying on the model confidence. Our simple and efficient\ncalibrator can be easily adapted to many tasks and model architectures, showing\nrobust gains in all settings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:22:52 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Shujian", ""], ["Gong", "Chengyue", ""], ["Choi", "Eunsol", ""]]}, {"id": "2106.01518", "submitter": "Jiacheng Xu", "authors": "Jiacheng Xu and Greg Durrett", "title": "Dissecting Generation Modes for Abstractive Summarization Models via\n  Ablation and Attribution", "comments": "ACL 2021; 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the prominence of neural abstractive summarization models, we know\nlittle about how they actually form summaries and how to understand where their\ndecisions come from. We propose a two-step method to interpret summarization\nmodel decisions. We first analyze the model's behavior by ablating the full\nmodel to categorize each decoder decision into one of several generation modes:\nroughly, is the model behaving like a language model, is it relying heavily on\nthe input, or is it somewhere in between? After isolating decisions that do\ndepend on the input, we explore interpreting these decisions using several\ndifferent attribution methods. We compare these techniques based on their\nability to select content and reconstruct the model's predicted token from\nperturbations of the input, thus revealing whether highlighted attributions are\ntruly important for the generation of the next token. While this machinery can\nbe broadly useful even beyond summarization, we specifically demonstrate its\ncapability to identify phrases the summarization model has memorized and\ndetermine where in the training pipeline this memorization happened, as well as\nstudy complex generation phenomena like sentence fusion on a per-instance\nbasis.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 00:54:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xu", "Jiacheng", ""], ["Durrett", "Greg", ""]]}, {"id": "2106.01526", "submitter": "George Boateng", "authors": "George Boateng, Peter Hilpert, Guy Bodenmann, Mona Neysari, Tobias\n  Kowatsch", "title": "\"You made me feel this way\": Investigating Partners' Influence in\n  Predicting Emotions in Couples' Conflict Interactions using Speech Data", "comments": "5 pages, Under review at ICMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  How romantic partners interact with each other during a conflict influences\nhow they feel at the end of the interaction and is predictive of whether the\npartners stay together in the long term. Hence understanding the emotions of\neach partner is important. Yet current approaches that are used include\nself-reports which are burdensome and hence limit the frequency of this data\ncollection. Automatic emotion prediction could address this challenge. Insights\nfrom psychology research indicate that partners' behaviors influence each\nother's emotions in conflict interaction and hence, the behavior of both\npartners could be considered to better predict each partner's emotion. However,\nit is yet to be investigated how doing so compares to only using each partner's\nown behavior in terms of emotion prediction performance. In this work, we used\nBERT to extract linguistic features (i.e., what partners said) and openSMILE to\nextract paralinguistic features (i.e., how they said it) from a data set of 368\nGerman-speaking Swiss couples (N = 736 individuals) which were videotaped\nduring an 8-minutes conflict interaction in the laboratory. Based on those\nfeatures, we trained machine learning models to predict if partners feel\npositive or negative after the conflict interaction. Our results show that\nincluding the behavior of the other partner improves the prediction\nperformance. Furthermore, for men, considering how their female partners spoke\nis most important and for women considering what their male partner said is\nmost important in getting better prediction performance. This work is a step\ntowards automatically recognizing each partners' emotion based on the behavior\nof both, which would enable a better understanding of couples in research,\ntherapy, and the real world.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:15:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Boateng", "George", ""], ["Hilpert", "Peter", ""], ["Bodenmann", "Guy", ""], ["Neysari", "Mona", ""], ["Kowatsch", "Tobias", ""]]}, {"id": "2106.01536", "submitter": "George Boateng", "authors": "Jacopo Biggiogera, George Boateng, Peter Hilpert, Matthew Vowels, Guy\n  Bodenmann, Mona Neysari, Fridtjof Nussbeck, Tobias Kowatsch", "title": "BERT meets LIWC: Exploring State-of-the-Art Language Models for\n  Predicting Communication Behavior in Couples' Conflict Interactions", "comments": "6 pages. Under review at ICMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many processes in psychology are complex, such as dyadic interactions between\ntwo interacting partners (e.g. patient-therapist, intimate relationship\npartners). Nevertheless, many basic questions about interactions are difficult\nto investigate because dyadic processes can be within a person and between\npartners, they are based on multimodal aspects of behavior and unfold rapidly.\nCurrent analyses are mainly based on the behavioral coding method, whereby\nhuman coders annotate behavior based on a coding schema. But coding is\nlabor-intensive, expensive, slow, focuses on few modalities. Current approaches\nin psychology use LIWC for analyzing couples' interactions. However, advances\nin natural language processing such as BERT could enable the development of\nsystems to potentially automate behavioral coding, which in turn could\nsubstantially improve psychological research. In this work, we train machine\nlearning models to automatically predict positive and negative communication\nbehavioral codes of 368 German-speaking Swiss couples during an 8-minute\nconflict interaction on a fine-grained scale (10-seconds sequences) using\nlinguistic features and paralinguistic features derived with openSMILE. Our\nresults show that both simpler TF-IDF features as well as more complex BERT\nfeatures performed better than LIWC, and that adding paralinguistic features\ndid not improve the performance. These results suggest it might be time to\nconsider modern alternatives to LIWC, the de facto linguistic features in\npsychology, for prediction tasks in couples research. This work is a further\nstep towards the automated coding of couples' behavior which could enhance\ncouple research and therapy, and be utilized for other dyadic interactions as\nwell.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:37:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Biggiogera", "Jacopo", ""], ["Boateng", "George", ""], ["Hilpert", "Peter", ""], ["Vowels", "Matthew", ""], ["Bodenmann", "Guy", ""], ["Neysari", "Mona", ""], ["Nussbeck", "Fridtjof", ""], ["Kowatsch", "Tobias", ""]]}, {"id": "2106.01540", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao\n  Ma, Luke Zettlemoyer", "title": "Luna: Linear Unified Nested Attention", "comments": "Preprint. 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quadratic computational and memory complexities of the Transformer's\nattention mechanism have limited its scalability for modeling long sequences.\nIn this paper, we propose Luna, a linear unified nested attention mechanism\nthat approximates softmax attention with two nested linear attention functions,\nyielding only linear (as opposed to quadratic) time and space complexity.\nSpecifically, with the first attention function, Luna packs the input sequence\ninto a sequence of fixed length. Then, the packed sequence is unpacked using\nthe second attention function. As compared to a more traditional attention\nmechanism, Luna introduces an additional sequence with a fixed length as input\nand an additional corresponding output, which allows Luna to perform attention\noperation linearly, while also storing adequate contextual information. We\nperform extensive evaluations on three benchmarks of sequence modeling tasks:\nlong-context sequence modeling, neural machine translation and masked language\nmodeling for large-scale pretraining. Competitive or even better experimental\nresults demonstrate both the effectiveness and efficiency of Luna compared to a\nvariety\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:47:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ma", "Xuezhe", ""], ["Kong", "Xiang", ""], ["Wang", "Sinong", ""], ["Zhou", "Chunting", ""], ["May", "Jonathan", ""], ["Ma", "Hao", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2106.01541", "submitter": "Jia-Chen Gu", "authors": "Jia-Chen Gu, Chongyang Tao, Zhen-Hua Ling, Can Xu, Xiubo Geng, Daxin\n  Jiang", "title": "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation\n  Understanding", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, various neural models for multi-party conversation (MPC) have\nachieved impressive improvements on a variety of tasks such as addressee\nrecognition, speaker identification and response prediction. However, these\nexisting methods on MPC usually represent interlocutors and utterances\nindividually and ignore the inherent complicated structure in MPC which may\nprovide crucial interlocutor and utterance semantics and would enhance the\nconversation understanding process. To this end, we present MPC-BERT, a\npre-trained model for MPC understanding that considers learning who says what\nto whom in a unified model with several elaborated self-supervised tasks.\nParticularly, these tasks can be generally categorized into (1) interlocutor\nstructure modeling including reply-to utterance recognition, identical speaker\nsearching and pointer consistency distinction, and (2) utterance semantics\nmodeling including masked shared utterance restoration and shared node\ndetection. We evaluate MPC-BERT on three downstream tasks including addressee\nrecognition, speaker identification and response selection. Experimental\nresults show that MPC-BERT outperforms previous methods by large margins and\nachieves new state-of-the-art performance on all three downstream tasks at two\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:49:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gu", "Jia-Chen", ""], ["Tao", "Chongyang", ""], ["Ling", "Zhen-Hua", ""], ["Xu", "Can", ""], ["Geng", "Xiubo", ""], ["Jiang", "Daxin", ""]]}, {"id": "2106.01555", "submitter": "Jekaterina Novikova Dr.", "authors": "Aparna Balagopalan, Jekaterina Novikova", "title": "Comparing Acoustic-based Approaches for Alzheimer's Disease Detection", "comments": "Accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the performance and generalizability of three\napproaches for AD detection from speech on the recent ADReSSo challenge\ndataset: 1) using conventional acoustic features 2) using novel pre-trained\nacoustic embeddings 3) combining acoustic features and embeddings. We find that\nwhile feature-based approaches have a higher precision, classification\napproaches relying on the combination of embeddings and features prove to have\na higher, and more balanced performance across multiple metrics of performance.\nOur best model, using such a combined approach, outperforms the acoustic\nbaseline in the challenge by 2.8\\%.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:44:40 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Balagopalan", "Aparna", ""], ["Novikova", "Jekaterina", ""]]}, {"id": "2106.01559", "submitter": "Zhuoren Jiang", "authors": "Fubang Zhao, Zhuoren Jiang, Yangyang Kang, Changlong Sun, Xiaozhong\n  Liu", "title": "Adjacency List Oriented Relational Fact Extraction via Adaptive\n  Multi-task Learning", "comments": "13 pages, 3 figures, accepted by findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relational fact extraction aims to extract semantic triplets from\nunstructured text. In this work, we show that all of the relational fact\nextraction models can be organized according to a graph-oriented analytical\nperspective. An efficient model, aDjacency lIst oRiented rElational faCT\n(DIRECT), is proposed based on this analytical framework. To alleviate\nchallenges of error propagation and sub-task loss equilibrium, DIRECT employs a\nnovel adaptive multi-task learning strategy with dynamic sub-task loss\nbalancing. Extensive experiments are conducted on two benchmark datasets, and\nresults prove that the proposed model outperforms a series of state-of-the-art\n(SoTA) models for relational triplet extraction.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:57:08 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhao", "Fubang", ""], ["Jiang", "Zhuoren", ""], ["Kang", "Yangyang", ""], ["Sun", "Changlong", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "2106.01560", "submitter": "Vijay Viswanathan", "authors": "Vijay Viswanathan, Graham Neubig, Pengfei Liu", "title": "CitationIE: Leveraging the Citation Graph for Scientific Information\n  Extraction", "comments": "ACL-IJCNLP 2021 camera-ready (long paper in main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically extracting key information from scientific documents has the\npotential to help scientists work more efficiently and accelerate the pace of\nscientific progress. Prior work has considered extracting document-level entity\nclusters and relations end-to-end from raw scientific text, which can improve\nliterature search and help identify methods and materials for a given problem.\nDespite the importance of this task, most existing works on scientific\ninformation extraction (SciIE) consider extraction solely based on the content\nof an individual paper, without considering the paper's place in the broader\nliterature. In contrast to prior work, we augment our text representations by\nleveraging a complementary source of document context: the citation graph of\nreferential links between citing and cited papers. On a test set of\nEnglish-language scientific documents, we show that simple ways of utilizing\nthe structure and content of the citation graph can each lead to significant\ngains in different scientific information extraction tasks. When these tasks\nare combined, we observe a sizable improvement in end-to-end information\nextraction over the state-of-the-art, suggesting the potential for future work\nalong this direction. We release software tools to facilitate citation-aware\nSciIE development.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 03:00:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Viswanathan", "Vijay", ""], ["Neubig", "Graham", ""], ["Liu", "Pengfei", ""]]}, {"id": "2106.01561", "submitter": "Cunxiang Wang", "authors": "Cunxiang Wang and Pai Liu and Yue Zhang", "title": "Can Generative Pre-trained Language Models Serve as Knowledge Bases for\n  Closed-book QA?", "comments": "Accepted By ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has investigated the interesting question using pre-trained\nlanguage models (PLMs) as knowledge bases for answering open questions.\nHowever, existing work is limited in using small benchmarks with high\ntest-train overlaps. We construct a new dataset of closed-book QA using SQuAD,\nand investigate the performance of BART. Experiments show that it is\nchallenging for BART to remember training facts in high precision, and also\nchallenging to answer closed-book questions even if relevant knowledge is\nretained. Some promising directions are found, including decoupling the\nknowledge memorizing process and the QA finetune process, forcing the model to\nrecall relevant knowledge when question answering.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 03:04:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wang", "Cunxiang", ""], ["Liu", "Pai", ""], ["Zhang", "Yue", ""]]}, {"id": "2106.01562", "submitter": "Wang Xu", "authors": "Wang Xu, Kehai Chen, Tiejun Zhao", "title": "Discriminative Reasoning for Document-level Relation Extraction", "comments": "11 pages, 4 figures, 5 tables. Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction (DocRE) models generally use graph\nnetworks to implicitly model the reasoning skill (i.e., pattern recognition,\nlogical reasoning, coreference reasoning, etc.) related to the relation between\none entity pair in a document. In this paper, we propose a novel discriminative\nreasoning framework to explicitly model the paths of these reasoning skills\nbetween each entity pair in this document. Thus, a discriminative reasoning\nnetwork is designed to estimate the relation probability distribution of\ndifferent reasoning paths based on the constructed graph and vectorized\ndocument contexts for each entity pair, thereby recognizing their relation.\nExperimental results show that our method outperforms the previous\nstate-of-the-art performance on the large-scale DocRE dataset. The code is\npublicly available at https://github.com/xwjim/DRN.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 03:09:38 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xu", "Wang", ""], ["Chen", "Kehai", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2106.01580", "submitter": "Yuchen Li", "authors": "Yuchen Li, Andrej Risteski", "title": "The Limitations of Limited Context for Constituency Parsing", "comments": "To be published in ACL 2021 (https://2021.aclweb.org/) as a long\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating syntax into neural approaches in NLP has a multitude of\npractical and scientific benefits. For instance, a language model that is\nsyntax-aware is likely to be able to produce better samples; even a\ndiscriminative model like BERT with a syntax module could be used for core NLP\ntasks like unsupervised syntactic parsing. Rapid progress in recent years was\narguably spurred on by the empirical success of the Parsing-Reading-Predict\narchitecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM\nof (Shen et al., 2019). Most notably, this is the first time neural approaches\nwere able to successfully perform unsupervised syntactic parsing (evaluated by\nvarious metrics like F-1 score).\n  However, even heuristic (much less fully mathematical) understanding of why\nand when these architectures work is lagging severely behind. In this work, we\nanswer representational questions raised by the architectures in (Shen et al.,\n2018a, 2019), as well as some transition-based syntax-aware language models\n(Dyer et al., 2016): what kind of syntactic structure can current neural\napproaches to syntax represent? Concretely, we ground this question in the\nsandbox of probabilistic context-free-grammars (PCFGs), and identify a key\naspect of the representational power of these approaches: the amount and\ndirectionality of context that the predictor has access to when forced to make\nparsing decision. We show that with limited context (either bounded, or\nunidirectional), there are PCFGs, for which these approaches cannot represent\nthe max-likelihood parse; conversely, if the context is unlimited, they can\nrepresent the max-likelihood parse of any PCFG.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 03:58:35 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Yuchen", ""], ["Risteski", "Andrej", ""]]}, {"id": "2106.01581", "submitter": "William Timkey", "authors": "Matt Wilber, William Timkey, Marten Van Schijndel", "title": "To Point or Not to Point: Understanding How Abstractive Summarizers\n  Paraphrase Text", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstractive neural summarization models have seen great improvements in\nrecent years, as shown by ROUGE scores of the generated summaries. But despite\nthese improved metrics, there is limited understanding of the strategies\ndifferent models employ, and how those strategies relate their understanding of\nlanguage. To understand this better, we run several experiments to characterize\nhow one popular abstractive model, the pointer-generator model of See et al.\n(2017), uses its explicit copy/generation switch to control its level of\nabstraction (generation) vs extraction (copying). On an extractive-biased\ndataset, the model utilizes syntactic boundaries to truncate sentences that are\notherwise often copied verbatim. When we modify the copy/generation switch and\nforce the model to generate, only simple paraphrasing abilities are revealed\nalongside factual inaccuracies and hallucinations. On an abstractive-biased\ndataset, the model copies infrequently but shows similarly limited abstractive\nabilities. In line with previous research, these results suggest that\nabstractive summarization models lack the semantic understanding necessary to\ngenerate paraphrases that are both abstractive and faithful to the source\ndocument.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 04:03:15 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wilber", "Matt", ""], ["Timkey", "William", ""], ["Van Schijndel", "Marten", ""]]}, {"id": "2106.01586", "submitter": "Vardaan Pahuja", "authors": "Vardaan Pahuja, Yu Gu, Wenhu Chen, Mehdi Bahrami, Lei Liu, Wei-Peng\n  Chen and Yu Su", "title": "A Systematic Investigation of KB-Text Embedding Alignment at Scale", "comments": "Accepted to ACL-IJCNLP 2021. 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) and text often contain complementary knowledge: KBs\nstore structured knowledge that can support long range reasoning, while text\nstores more comprehensive and timely knowledge in an unstructured way.\nSeparately embedding the individual knowledge sources into vector spaces has\ndemonstrated tremendous successes in encoding the respective knowledge, but how\nto jointly embed and reason with both knowledge sources to fully leverage the\ncomplementary information is still largely an open problem. We conduct a\nlarge-scale, systematic investigation of aligning KB and text embeddings for\njoint reasoning. We set up a novel evaluation framework with two evaluation\ntasks, few-shot link prediction and analogical reasoning, and evaluate an array\nof KB-text embedding alignment methods. We also demonstrate how such alignment\ncan infuse textual information into KB embeddings for more accurate link\nprediction on emerging entities and events, using COVID-19 as a case study.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 04:14:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Pahuja", "Vardaan", ""], ["Gu", "Yu", ""], ["Chen", "Wenhu", ""], ["Bahrami", "Mehdi", ""], ["Liu", "Lei", ""], ["Chen", "Wei-Peng", ""], ["Su", "Yu", ""]]}, {"id": "2106.01597", "submitter": "Kaushal Kumar Maurya", "authors": "Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano and\n  Kumari Deepshikha", "title": "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language\n  Generation", "comments": "Accepted in Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent advancement in NLP research, cross-lingual transfer for\nnatural language generation is relatively understudied. In this work, we\ntransfer supervision from high resource language (HRL) to multiple low-resource\nlanguages (LRLs) for natural language generation (NLG). We consider four NLG\ntasks (text summarization, question generation, news headline generation, and\ndistractor generation) and three syntactically diverse languages, i.e.,\nEnglish, Hindi, and Japanese. We propose an unsupervised cross-lingual language\ngeneration framework (called ZmBART) that does not use any parallel or\npseudo-parallel/back-translated data. In this framework, we further pre-train\nmBART sequence-to-sequence denoising auto-encoder model with an auxiliary task\nusing monolingual data of three languages. The objective function of the\nauxiliary task is close to the target tasks which enriches the multi-lingual\nlatent representation of mBART and provides good initialization for target\ntasks. Then, this model is fine-tuned with task-specific supervised English\ndata and directly evaluated with low-resource languages in the Zero-shot\nsetting. To overcome catastrophic forgetting and spurious correlation issues,\nwe applied freezing model component and data argumentation approaches\nrespectively. This simple modeling approach gave us promising results.We\nexperimented with few-shot training (with 1000 supervised data points) which\nboosted the model performance further. We performed several ablations and\ncross-lingual transferability analyses to demonstrate the robustness of ZmBART.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:08:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Maurya", "Kaushal Kumar", ""], ["Desarkar", "Maunendra Sankar", ""], ["Kano", "Yoshinobu", ""], ["Deepshikha", "Kumari", ""]]}, {"id": "2106.01598", "submitter": "Son T. Luu", "authors": "Hanh Hong-Phuc Vo, Hieu Trung Tran, Son T. Luu", "title": "Automatically Detecting Cyberbullying Comments on Online Game Forums", "comments": "Accepted at RIVF 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Online game forums are popular to most of game players. They use it to\ncommunicate and discuss the strategy of the game, or even to make friends.\nHowever, game forums also contain abusive and harassment speech, disturbing and\nthreatening players. Therefore, it is necessary to automatically detect and\nremove cyberbullying comments to keep the game forum clean and friendly. We use\nthe Cyberbullying dataset collected from World of Warcraft (WoW) and League of\nLegends (LoL) forums and train classification models to automatically detect\nwhether a comment of a player is abusive or not. The result obtains 82.69% of\nmacro F1-score for LoL forum and 83.86% of macro F1-score for WoW forum by the\nToxic-BERT model on the Cyberbullying dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:08:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Vo", "Hanh Hong-Phuc", ""], ["Tran", "Hieu Trung", ""], ["Luu", "Son T.", ""]]}, {"id": "2106.01601", "submitter": "Jiao Sun", "authors": "Jiao Sun and Nanyun Peng", "title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities can be seen as sequences of events, which are crucial to\nunderstanding societies. Disproportional event distribution for different\ndemographic groups can manifest and amplify social stereotypes, and potentially\njeopardize the ability of members in some groups to pursue certain goals. In\nthis paper, we present the first event-centric study of gender biases in a\nWikipedia corpus. To facilitate the study, we curate a corpus of career and\npersonal life descriptions with demographic information consisting of 7,854\nfragments from 10,412 celebrities. Then we detect events with a\nstate-of-the-art event detection model, calibrate the results using\nstrategically generated templates, and extract events that have asymmetric\nassociations with genders. Our study discovers that the Wikipedia pages tend to\nintermingle personal life events with professional events for females but not\nfor males, which calls for the awareness of the Wikipedia community to\nformalize guidelines and train the editors to mind the implicit biases that\ncontributors carry. Our work also lays the foundation for future works on\nquantifying and discovering event biases at the corpus level.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:22:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sun", "Jiao", ""], ["Peng", "Nanyun", ""]]}, {"id": "2106.01607", "submitter": "Satyapriya Krishna", "authors": "Michiel de Jong, Satyapriya Krishna, Anuva Agarwal", "title": "Grounding Complex Navigational Instructions Using Scene Graphs", "comments": "arXiv admin note: text overlap with arXiv:1706.07230 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a reinforcement learning agent to carry out natural language\ninstructions is limited by the available supervision, i.e. knowing when the\ninstruction has been carried out. We adapt the CLEVR visual question answering\ndataset to generate complex natural language navigation instructions and\naccompanying scene graphs, yielding an environment-agnostic supervised dataset.\nTo demonstrate the use of this data set, we map the scenes to the VizDoom\nenvironment and use the architecture in \\citet{gatedattention} to train an\nagent to carry out these more complex language instructions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:45:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["de Jong", "Michiel", ""], ["Krishna", "Satyapriya", ""], ["Agarwal", "Anuva", ""]]}, {"id": "2106.01609", "submitter": "Piji Li", "authors": "Piji Li and Shuming Shi", "title": "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese\n  Grammatical Error Correction", "comments": "Accepted in the main conference of ACL 2021. Code:\n  https://github.com/lipiji/TtT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of Chinese Grammatical Error Correction (CGEC) and\npresent a new framework named Tail-to-Tail (\\textbf{TtT}) non-autoregressive\nsequence prediction to address the deep issues hidden in CGEC. Considering that\nmost tokens are correct and can be conveyed directly from source to target, and\nthe error positions can be estimated and corrected based on the bidirectional\ncontext information, thus we employ a BERT-initialized Transformer Encoder as\nthe backbone model to conduct information modeling and conveying. Considering\nthat only relying on the same position substitution cannot handle the\nvariable-length correction cases, various operations such substitution,\ndeletion, insertion, and local paraphrasing are required jointly. Therefore, a\nConditional Random Fields (CRF) layer is stacked on the up tail to conduct\nnon-autoregressive sequence prediction by modeling the token dependencies.\nSince most tokens are correct and easily to be predicted/conveyed to the\ntarget, then the models may suffer from a severe class imbalance issue. To\nalleviate this problem, focal loss penalty strategies are integrated into the\nloss functions. Moreover, besides the typical fix-length error correction\ndatasets, we also construct a variable-length corpus to conduct experiments.\nExperimental results on standard datasets, especially on the variable-length\ndatasets, demonstrate the effectiveness of TtT in terms of sentence-level\nAccuracy, Precision, Recall, and F1-Measure on tasks of error Detection and\nCorrection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:56:57 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 07:55:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Piji", ""], ["Shi", "Shuming", ""]]}, {"id": "2106.01623", "submitter": "Junyi Li", "authors": "Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing\n  Yuan and Ji-Rong Wen", "title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language\n  Models", "comments": "Accepted to ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how to automatically generate a natural language text that\ndescribes the facts in knowledge graph (KG). Considering the few-shot setting,\nwe leverage the excellent capacities of pretrained language models (PLMs) in\nlanguage understanding and generation. We make three major technical\ncontributions, namely representation alignment for bridging the semantic gap\nbetween KG encodings and PLMs, relation-biased KG linearization for deriving\nbetter input representations, and multi-task learning for learning the\ncorrespondence between KG and text. Extensive experiments on three benchmark\ndatasets have demonstrated the effectiveness of our model on KG-to-text\ngeneration task. In particular, our model outperforms all comparison methods on\nboth fully-supervised and few-shot settings. Our code and datasets are\navailable at https://github.com/RUCAIBox/Few-Shot-KG2Text.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 06:48:00 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Junyi", ""], ["Tang", "Tianyi", ""], ["Zhao", "Wayne Xin", ""], ["Wei", "Zhicheng", ""], ["Yuan", "Nicholas Jing", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2106.01625", "submitter": "Wanzheng Zhu", "authors": "Wanzheng Zhu and Suma Bhat", "title": "Generate, Prune, Select: A Pipeline for Counterspeech Generation against\n  Online Hate Speech", "comments": "The 59th Annual Meeting of the Association for Computational\n  Linguistics and the 11th International Joint Conference on Natural Language\n  Processing (ACL-IJCNLP): Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Countermeasures to effectively fight the ever increasing hate speech online\nwithout blocking freedom of speech is of great social interest. Natural\nLanguage Generation (NLG), is uniquely capable of developing scalable\nsolutions. However, off-the-shelf NLG methods are primarily\nsequence-to-sequence neural models and they are limited in that they generate\ncommonplace, repetitive and safe responses regardless of the hate speech (e.g.,\n\"Please refrain from using such language.\") or irrelevant responses, making\nthem ineffective for de-escalating hateful conversations. In this paper, we\ndesign a three-module pipeline approach to effectively improve the diversity\nand relevance. Our proposed pipeline first generates various counterspeech\ncandidates by a generative model to promote diversity, then filters the\nungrammatical ones using a BERT model, and finally selects the most relevant\ncounterspeech response using a novel retrieval-based method. Extensive\nExperiments on three representative datasets demonstrate the efficacy of our\napproach in generating diverse and relevant counterspeech.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 06:54:03 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhu", "Wanzheng", ""], ["Bhat", "Suma", ""]]}, {"id": "2106.01635", "submitter": "Venelin Kovatchev", "authors": "Venelin Kovatchev, Phillip Smith, Mark Lee, and Rory Devine", "title": "Can vectors read minds better than experts? Comparing data augmentation\n  strategies for the automated scoring of children's mindreading ability", "comments": "The paper will be presented at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we implement and compare 7 different data augmentation\nstrategies for the task of automatic scoring of children's ability to\nunderstand others' thoughts, feelings, and desires (or \"mindreading\").\n  We recruit in-domain experts to re-annotate augmented samples and determine\nto what extent each strategy preserves the original rating. We also carry out\nmultiple experiments to measure how much each augmentation strategy improves\nthe performance of automatic scoring systems. To determine the capabilities of\nautomatic systems to generalize to unseen data, we create UK-MIND-20 - a new\ncorpus of children's performance on tests of mindreading, consisting of 10,320\nquestion-answer pairs.\n  We obtain a new state-of-the-art performance on the MIND-CA corpus, improving\nmacro-F1-score by 6 points. Results indicate that both the number of training\nexamples and the quality of the augmentation strategies affect the performance\nof the systems. The task-specific augmentations generally outperform\ntask-agnostic augmentations. Automatic augmentations based on vectors (GloVe,\nFastText) perform the worst.\n  We find that systems trained on MIND-CA generalize well to UK-MIND-20. We\ndemonstrate that data augmentation strategies also improve the performance on\nunseen data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:12:00 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kovatchev", "Venelin", ""], ["Smith", "Phillip", ""], ["Lee", "Mark", ""], ["Devine", "Rory", ""]]}, {"id": "2106.01644", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "M. A. Barchiesi, A. Fronzetti Colladon", "title": "Corporate core values and social responsibility: What really matters to\n  whom", "comments": null, "journal-ref": "Technological Forecasting and Social Change 170, 120907 (2021)", "doi": "10.1016/j.techfore.2021.120907", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study uses an innovative measure, the Semantic Brand Score, to assess\nthe interest of stakeholders in different company core values. Among others, we\nfocus on corporate social responsibility (CSR) core value statements, and on\nthe attention they receive from five categories of stakeholders (customers,\ncompany communication teams, employees, associations and media). Combining big\ndata methods and tools of Social Network Analysis and Text Mining, we analyzed\nabout 58,000 Italian tweets and found that different stakeholders have\ndifferent prevailing interests. CSR gets much less attention than expected.\nCore values related to customers and employees are in the foreground.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:25:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Barchiesi", "M. A.", ""], ["Colladon", "A. Fronzetti", ""]]}, {"id": "2106.01649", "submitter": "Xinyu Zuo", "authors": "Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng and\n  Yuguang Chen", "title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event\n  Causality Identification", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern models for event causality identification (ECI) are mainly based on\nsupervised learning, which are prone to the data lacking problem.\nUnfortunately, the existing NLP-related augmentation methods cannot directly\nproduce the available data required for this task. To solve the data lacking\nproblem, we introduce a new approach to augment training data for event\ncausality identification, by iteratively generating new examples and\nclassifying event causality in a dual learning framework. On the one hand, our\napproach is knowledge-guided, which can leverage existing knowledge bases to\ngenerate well-formed new sentences. On the other hand, our approach employs a\ndual mechanism, which is a learnable augmentation framework and can\ninteractively adjust the generation process to generate task-related sentences.\nExperimental results on two benchmarks EventStoryLine and Causal-TimeBank show\nthat 1) our method can augment suitable task-related training data for ECI; 2)\nour method outperforms previous methods on EventStoryLine and Causal-TimeBank\n(+2.5 and +2.1 points on F1 value respectively).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:42:20 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zuo", "Xinyu", ""], ["Cao", "Pengfei", ""], ["Chen", "Yubo", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""], ["Peng", "Weihua", ""], ["Chen", "Yuguang", ""]]}, {"id": "2106.01654", "submitter": "Xinyu Zuo", "authors": "Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng and\n  Yuguang Chen", "title": "Improving Event Causality Identification via Self-Supervised\n  Representation Learning on External Causal Statement", "comments": "Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current models for event causality identification (ECI) mainly adopt a\nsupervised framework, which heavily rely on labeled data for training.\nUnfortunately, the scale of current annotated datasets is relatively limited,\nwhich cannot provide sufficient support for models to capture useful indicators\nfrom causal statements, especially for handing those new, unseen cases. To\nalleviate this problem, we propose a novel approach, shortly named CauSeRL,\nwhich leverages external causal statements for event causality identification.\nFirst of all, we design a self-supervised framework to learn context-specific\ncausal patterns from external causal statements. Then, we adopt a contrastive\ntransfer strategy to incorporate the learned context-specific causal patterns\ninto the target ECI model. Experimental results show that our method\nsignificantly outperforms previous methods on EventStoryLine and\nCausal-TimeBank (+2.0 and +3.4 points on F1 value respectively).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:50:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zuo", "Xinyu", ""], ["Cao", "Pengfei", ""], ["Chen", "Yubo", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""], ["Peng", "Weihua", ""], ["Chen", "Yuguang", ""]]}, {"id": "2106.01666", "submitter": "Kai-Hui Liang", "authors": "Kai-Hui Liang, Weiyan Shi, Yoojung Oh, Jingwen Zhang, Zhou Yu", "title": "Discovering Chatbot's Self-Disclosure's Impact on User Trust, Affinity,\n  and Recommendation Effectiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, chatbots have been empowered to engage in social\nconversations with humans and have the potential to elicit people to disclose\ntheir personal experiences, opinions, and emotions. However, how and to what\nextent people respond to chabots' self-disclosure remain less known. In this\nwork, we designed a social chatbot with three self-disclosure levels that\nconducted small talks and provided relevant recommendations to people. 372\nMTurk participants were randomized to one of the four groups with different\nself-disclosure levels to converse with the chatbot on two topics, movies, and\nCOVID-19. We found that people's self-disclosure level was strongly reciprocal\nto a chatbot's self-disclosure level. Chatbots' self-disclosure also positively\nimpacted engagement and users' perception of the bot and led to a more\neffective recommendation such that participants enjoyed and agreed more with\nthe recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:16:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liang", "Kai-Hui", ""], ["Shi", "Weiyan", ""], ["Oh", "Yoojung", ""], ["Zhang", "Jingwen", ""], ["Yu", "Zhou", ""]]}, {"id": "2106.01702", "submitter": "Hao Sun", "authors": "Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, Minlie Huang", "title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental\n  Health Support", "comments": "Accepted to Findings of ACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great research interests have been attracted to devise AI services that are\nable to provide mental health support. However, the lack of corpora is a main\nobstacle to this research, particularly in Chinese language. In this paper, we\npropose PsyQA, a Chinese dataset of psychological health support in the form of\nquestion and answer pair. PsyQA is crawled from a Chinese mental health service\nplatform, and contains 22K questions and 56K long and well-structured answers.\nBased on the psychological counseling theories, we annotate a portion of answer\ntexts with typical strategies for providing support, and further present\nin-depth analysis of both lexical features and strategy patterns in the\ncounseling answers. We also evaluate the performance of generating counseling\nanswers with the generative pretrained models. Results show that utilizing\nstrategies enhances the fluency and helpfulness of generated answers, but there\nis still a large space for future research.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:06:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sun", "Hao", ""], ["Lin", "Zhenru", ""], ["Zheng", "Chujie", ""], ["Liu", "Siyang", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.01703", "submitter": "Nirav Diwan", "authors": "Nirav Diwan, Tanmoy Chakravorty, Zubair Shafiq", "title": "Fingerprinting Fine-tuned Language Models in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are concerns that the ability of language models (LMs) to generate high\nquality synthetic text can be misused to launch spam, disinformation, or\npropaganda. Therefore, the research community is actively working on developing\napproaches to detect whether a given text is organic or synthetic. While this\nis a useful first step, it is important to be able to further fingerprint the\nauthor LM to attribute its origin. Prior work on fingerprinting LMs is limited\nto attributing synthetic text generated by a handful (usually < 10) of\npre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad\nof ways (e.g., on a domain-specific text corpus) before being used to generate\nsynthetic text. It is challenging to fingerprinting fine-tuned LMs because the\nuniverse of fine-tuned LMs is much larger in realistic scenarios. To address\nthis challenge, we study the problem of large-scale fingerprinting of\nfine-tuned LMs in the wild. Using a real-world dataset of synthetic text\ngenerated by 108 different fine-tuned LMs, we conduct comprehensive experiments\nto demonstrate the limitations of existing fingerprinting approaches. Our\nresults show that fine-tuning itself is the most effective in attributing the\nsynthetic text generated by fine-tuned LMs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:07:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Diwan", "Nirav", ""], ["Chakravorty", "Tanmoy", ""], ["Shafiq", "Zubair", ""]]}, {"id": "2106.01706", "submitter": "Saeid Hosseini", "authors": "Sara Kamran, Raziyeh Zall, Mohammad Reza Kangavari, Saeid Hosseini,\n  Sana Rahmani, and Wen Hua", "title": "EmoDNN: Understanding emotions from short texts through a deep neural\n  network ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The latent knowledge in the emotions and the opinions of the individuals that\nare manifested via social networks are crucial to numerous applications\nincluding social management, dynamical processes, and public security.\nAffective computing, as an interdisciplinary research field, linking artificial\nintelligence to cognitive inference, is capable to exploit emotion-oriented\nknowledge from brief contents. The textual contents convey hidden information\nsuch as personality and cognition about corresponding authors that can\ndetermine both correlations and variations between users. Emotion recognition\nfrom brief contents should embrace the contrast between authors where the\ndifferences in personality and cognition can be traced within emotional\nexpressions. To tackle this challenge, we devise a framework that, on the one\nhand, infers latent individual aspects, from brief contents and, on the other\nhand, presents a novel ensemble classifier equipped with dynamic dropout\nconvnets to extract emotions from textual context. To categorize short text\ncontents, our proposed method conjointly leverages cognitive factors and\nexploits hidden information. We utilize the outcome vectors in a novel\nembedding model to foster emotion-pertinent features that are collectively\nassembled by lexicon inductions. Experimental results show that compared to\nother competitors, our proposed model can achieve a higher performance in\nrecognizing emotion from noisy contents.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:17:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kamran", "Sara", ""], ["Zall", "Raziyeh", ""], ["Kangavari", "Mohammad Reza", ""], ["Hosseini", "Saeid", ""], ["Rahmani", "Sana", ""], ["Hua", "Wen", ""]]}, {"id": "2106.01709", "submitter": "Shuang Zeng", "authors": "Shuang Zeng, Yuting Wu and Baobao Chang", "title": "SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level\n  Relation Extraction", "comments": "11 pages, 3 figures, 3 tables, Long paper accepted by Findings of\n  ACL-IJCNLP 2021", "journal-ref": "ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Document-level relation extraction has attracted much attention in recent\nyears. It is usually formulated as a classification problem that predicts\nrelations for all entity pairs in the document. However, previous works\nindiscriminately represent intra- and inter-sentential relations in the same\nway, confounding the different patterns for predicting them. Besides, they\ncreate a document graph and use paths between entities on the graph as clues\nfor logical reasoning. However, not all entity pairs can be connected with a\npath and have the correct logical reasoning paths in their graph. Thus many\ncases of logical reasoning cannot be covered. This paper proposes an effective\narchitecture, SIRE, to represent intra- and inter-sentential relations in\ndifferent ways. We design a new and straightforward form of logical reasoning\nmodule that can cover more logical reasoning chains. Experiments on the public\ndatasets show SIRE outperforms the previous state-of-the-art methods. Further\nanalysis shows that our predictions are reliable and explainable. Our code is\navailable at https://github.com/DreamInvoker/SIRE.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:25:44 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zeng", "Shuang", ""], ["Wu", "Yuting", ""], ["Chang", "Baobao", ""]]}, {"id": "2106.01732", "submitter": "Ziqing Yang", "authors": "Ziqing Yang, Wentao Ma, Yiming Cui, Jiani Ye, Wanxiang Che, Shijin\n  Wang", "title": "Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual pre-trained models have achieved remarkable transfer performance\nby pre-trained on rich kinds of languages. Most of the models such as mBERT are\npre-trained on unlabeled corpora. The static and contextual embeddings from the\nmodels could not be aligned very well. In this paper, we aim to improve the\nzero-shot cross-lingual transfer performance by aligning the embeddings better.\nWe propose a pre-training task named Alignment Language Model (AlignLM), which\nuses the statistical alignment information as the prior knowledge to guide\nbilingual word prediction. We evaluate our method on multilingual machine\nreading comprehension and natural language interface tasks. The results show\nAlignLM can improve the zero-shot performance significantly on MLQA and XNLI\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:18:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Yang", "Ziqing", ""], ["Ma", "Wentao", ""], ["Cui", "Yiming", ""], ["Ye", "Jiani", ""], ["Che", "Wanxiang", ""], ["Wang", "Shijin", ""]]}, {"id": "2106.01735", "submitter": "\\c{S}\\\"ukr\\\"u Ozan", "authors": "D. Emre Ta\\c{s}ar,\\c{S}\\\"ukr\\\"u Ozan, Umut \\\"Ozdil, M. Fatih Akca,\n  O\\u{g}uzhan \\\"Olmez, Semih G\\\"ul\\\"um, Se\\c{c}ilay Kutal, Ceren Belhan", "title": "Auto-tagging of Short Conversational Sentences using Transformer Methods", "comments": "in Turkish language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of categorizing short speech sentences according to their\nsemantic features with high accuracy is a subject studied in natural language\nprocessing. In this study, a data set created with samples classified in 46\ndifferent categories was used. Examples consist of sentences taken from chat\nconversations between a company's customer representatives and the company's\nwebsite visitors. The primary purpose is to automatically tag questions and\nrequests from visitors in the most accurate way for 46 predetermined categories\nfor use in a chat application to generate meaningful answers to the questions\nasked by the website visitors. For this, different BERT models and one GPT-2\nmodel, pre-trained in Turkish, were preferred. The classification performances\nof the relevant models were analyzed in detail and reported accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:23:58 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 07:26:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ta\u015far", "D. Emre", ""], ["Ozan", "\u015e\u00fckr\u00fc", ""], ["\u00d6zdil", "Umut", ""], ["Akca", "M. Fatih", ""], ["\u00d6lmez", "O\u011fuzhan", ""], ["G\u00fcl\u00fcm", "Semih", ""], ["Kutal", "Se\u00e7ilay", ""], ["Belhan", "Ceren", ""]]}, {"id": "2106.01751", "submitter": "Sawan Kumar", "authors": "Sawan Kumar, Partha Talukdar", "title": "Reordering Examples Helps during Priming-based Few-Shot Learning", "comments": "12 pages, 1 figure, Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn from limited data, or few-shot learning, is a desirable\nand often critical requirement for NLP systems. While many existing methods do\npoorly at learning from a handful of examples, large pretrained language models\nhave recently been shown to be efficient few-shot learners. One approach to\nfew-shot learning, which does not require finetuning of model parameters, is to\naugment the language model's input with priming text which is typically\nconstructed using task specific descriptions and examples. In this work, we\nfurther explore priming-based few-shot learning, with focus on using examples\nas prompts. We show that presenting examples in the right order is key for\ngeneralization. We introduce PERO (Prompting with Examples in the Right Order),\nwhere we formulate few-shot learning as search over the set of permutations of\nthe training examples. We show that PERO can learn to generalize efficiently\nusing as few as 10 examples, in contrast to existing approaches. While the\nnewline token is a natural choice for separating the examples in the prompt, we\nshow that learning a new separator token can potentially provide further gains\nin performance. We demonstrate the effectiveness of the proposed method on the\ntasks of sentiment classification, natural language inference and fact\nretrieval. Finally, we analyze the learned prompts to reveal novel insights,\nincluding the idea that two training examples in the right order alone can\nprovide competitive performance for sentiment classification and natural\nlanguage inference.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 11:02:36 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kumar", "Sawan", ""], ["Talukdar", "Partha", ""]]}, {"id": "2106.01760", "submitter": "Leyang Cui", "authors": "Leyang Cui, Yu Wu, Jian Liu, Sen Yang, Yue Zhang", "title": "Template-Based Named Entity Recognition Using BART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent interest in investigating few-shot NER, where the\nlow-resource target domain has different label sets compared with a\nresource-rich source domain. Existing methods use a similarity-based metric.\nHowever, they cannot make full use of knowledge transfer in NER model\nparameters. To address the issue, we propose a template-based method for NER,\ntreating NER as a language model ranking problem in a sequence-to-sequence\nframework, where original sentences and statement templates filled by candidate\nnamed entity span are regarded as the source sequence and the target sequence,\nrespectively. For inference, the model is required to classify each candidate\nspan based on the corresponding template scores. Our experiments demonstrate\nthat the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource\ntask), and significantly better than fine-tuning BERT 10.88%, 15.34%, and\n11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS\n(low-resource task), respectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 11:29:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cui", "Leyang", ""], ["Wu", "Yu", ""], ["Liu", "Jian", ""], ["Yang", "Sen", ""], ["Zhang", "Yue", ""]]}, {"id": "2106.01793", "submitter": "Quzhe Huang", "authors": "Quzhe Huang, Shengqi Zhu, Yansong Feng, Yuan Ye, Yuxuan Lai, Dongyan\n  Zhao", "title": "Three Sentences Are All You Need: Local Path Enhanced Document Relation\n  Extraction", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level Relation Extraction (RE) is a more challenging task than\nsentence RE as it often requires reasoning over multiple sentences. Yet, human\nannotators usually use a small number of sentences to identify the relationship\nbetween a given entity pair. In this paper, we present an embarrassingly simple\nbut effective method to heuristically select evidence sentences for\ndocument-level RE, which can be easily combined with BiLSTM to achieve good\nperformance on benchmark datasets, even better than fancy graph neural network\nbased methods. We have released our code at\nhttps://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:29:40 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Huang", "Quzhe", ""], ["Zhu", "Shengqi", ""], ["Feng", "Yansong", ""], ["Ye", "Yuan", ""], ["Lai", "Yuxuan", ""], ["Zhao", "Dongyan", ""]]}, {"id": "2106.01797", "submitter": "Pengda Qin", "authors": "Pengda Qin, Yuhong Li, Kefeng Deng, Qiang Wu", "title": "TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among ubiquitous multimodal data in the real world, text is the modality\ngenerated by human, while image reflects the physical world honestly. In a\nvisual understanding application, machines are expected to understand images\nlike human. Inspired by this, we propose a novel self-supervised learning\nmethod, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual\nrepresentations by fully utilizing the naturally-existing multimodal data. Our\ncore idea of self-supervised learning is to maximize the mutual information\nbetween features extracted from multiple views of a shared context to a\nrational degree. Different from previous methods which only consider multiple\nviews from a single modality, our work produces multiple views from different\nmodalities, and jointly optimizes the mutual information for features pairs of\nintra-modality and inter-modality. Considering the information gap between\ninter-modality features pairs from data noise, we adopt a \\emph{ranking-based}\ncontrastive learning to optimize the mutual information. During evaluation, we\ndirectly use the pre-trained visual representations to complete various image\nclassification tasks. Experimental results show that, TVDIM significantly\noutperforms previous visual self-supervised methods when processing the same\nset of images.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:36:01 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:53:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Qin", "Pengda", ""], ["Li", "Yuhong", ""], ["Deng", "Kefeng", ""], ["Wu", "Qiang", ""]]}, {"id": "2106.01804", "submitter": "Haiyang Xu", "authors": "Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming\n  Xiao and Fei Huang", "title": "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual\n  Learning", "comments": "ACL2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-language pre-training (VLP) on large-scale image-text pairs has\nachieved huge success for the cross-modal downstream tasks. The most existing\npre-training methods mainly adopt a two-step training procedure, which firstly\nemploys a pre-trained object detector to extract region-based visual features,\nthen concatenates the image representation and text embedding as the input of\nTransformer to train. However, these methods face problems of using\ntask-specific visual representation of the specific object detector for generic\ncross-modal understanding, and the computation inefficiency of two-stage\npipeline. In this paper, we propose the first end-to-end vision-language\npre-trained model for both V+L understanding and generation, namely E2E-VLP,\nwhere we build a unified Transformer framework to jointly learn visual\nrepresentation, and semantic alignments between image and text. We incorporate\nthe tasks of object detection and image captioning into pre-training with a\nunified Transformer encoder-decoder architecture for enhancing visual learning.\nAn extensive set of experiments have been conducted on well-established\nvision-language downstream tasks to demonstrate the effectiveness of this novel\nVLP paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:50:26 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 06:56:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Xu", "Haiyang", ""], ["Yan", "Ming", ""], ["Li", "Chenliang", ""], ["Bi", "Bin", ""], ["Huang", "Songfang", ""], ["Xiao", "Wenming", ""], ["Huang", "Fei", ""]]}, {"id": "2106.01809", "submitter": "Quzhe Huang", "authors": "Quzhe Huang, Shengqi Zhu, Yansong Feng, Dongyan Zhao", "title": "Exploring Distantly-Labeled Rationales in Neural Network Models", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies strive to incorporate various human rationales into neural\nnetworks to improve model performance, but few pay attention to the quality of\nthe rationales. Most existing methods distribute their models' focus to\ndistantly-labeled rationale words entirely and equally, while ignoring the\npotential important non-rationale words and not distinguishing the importance\nof different rationale words. In this paper, we propose two novel auxiliary\nloss functions to make better use of distantly-labeled rationales, which\nencourage models to maintain their focus on important words beyond labeled\nrationales (PINs) and alleviate redundant training on non-helpful rationales\n(NoIRs). Experiments on two representative classification tasks show that our\nproposed methods can push a classification model to effectively learn crucial\nclues from non-perfect rationales while maintaining the ability to spread its\nfocus to other unlabeled important words, thus significantly outperform\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:00:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Huang", "Quzhe", ""], ["Zhu", "Shengqi", ""], ["Feng", "Yansong", ""], ["Zhao", "Dongyan", ""]]}, {"id": "2106.01810", "submitter": "Jiwei Li", "authors": "Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei\n  Li, Tianwei Zhang", "title": "Defending against Backdoor Attacks in Natural Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The frustratingly fragile nature of neural network models make current\nnatural language generation (NLG) systems prone to backdoor attacks and\ngenerate malicious sequences that could be sexist or offensive. Unfortunately,\nlittle effort has been invested to how backdoor attacks can affect current NLG\nmodels and how to defend against these attacks. In this work, we investigate\nthis problem on two important NLG tasks, machine translation and dialogue\ngeneration. By giving a formal definition for backdoor attack and defense, and\ndeveloping corresponding benchmarks, we design methods to attack NLG models,\nwhich achieve high attack success to ask NLG models to generate malicious\nsequences. To defend against these attacks, we propose to detect the attack\ntrigger by examining the effect of deleting or replacing certain words on the\ngeneration outputs, which we find successful for certain types of attacks. We\nwill discuss the limitation of this work, and hope this work can raise the\nawareness of backdoor risks concealed in deep NLG systems. (Code and data are\navailable at https://github.com/ShannonAI/backdoor_nlg.)\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:00:28 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Fan", "Chun", ""], ["Li", "Xiaoya", ""], ["Meng", "Yuxian", ""], ["Sun", "Xiaofei", ""], ["Ao", "Xiang", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""], ["Zhang", "Tianwei", ""]]}, {"id": "2106.01890", "submitter": "Yixin Liu", "authors": "Yixin Liu, Pengfei Liu", "title": "SimCLS: A Simple Framework for Contrastive Learning of Abstractive\n  Summarization", "comments": "Published as a short paper at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a conceptually simple while empirically powerful\nframework for abstractive summarization, SimCLS, which can bridge the gap\nbetween the learning objective and evaluation metrics resulting from the\ncurrently dominated sequence-to-sequence learning framework by formulating text\ngeneration as a reference-free evaluation problem (i.e., quality estimation)\nassisted by contrastive learning. Experimental results show that, with minor\nmodification over existing top-scoring systems, SimCLS can improve the\nperformance of existing top-performing models by a large margin. Particularly,\n2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on\nthe CNN/DailyMail dataset, driving the state-of-the-art performance to a new\nlevel. We have open-sourced our codes and results:\nhttps://github.com/yixinL7/SimCLS. Results of our proposed models have been\ndeployed into ExplainaBoard platform, which allows researchers to understand\nour systems in a more fine-grained way.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:34:17 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Yixin", ""], ["Liu", "Pengfei", ""]]}, {"id": "2106.01904", "submitter": "Lorenzo Bertolini", "authors": "Lorenzo Bertolini, Julie Weeds, David Weir, Qiwei Peng", "title": "Representing Syntax and Composition with Geometric Transformations", "comments": "to appear in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploitation of syntactic graphs (SyGs) as a word's context has been\nshown to be beneficial for distributional semantic models (DSMs), both at the\nlevel of individual word representations and in deriving phrasal\nrepresentations via composition. However, notwithstanding the potential\nperformance benefit, the syntactically-aware DSMs proposed to date have huge\nnumbers of parameters (compared to conventional DSMs) and suffer from data\nsparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic\nrelations) has been largely limited to linear maps. The knowledge graphs'\nliterature, on the other hand, has proposed light-weight models employing\ndifferent geometric transformations (GTs) to encode edges in a knowledge graph\n(KG). Our work explores the possibility of adopting this family of models to\nencode SyGs. Furthermore, we investigate which GT better encodes syntactic\nrelations, so that these representations can be used to enhance phrase-level\ncomposition via syntactic contextualisation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:53:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bertolini", "Lorenzo", ""], ["Weeds", "Julie", ""], ["Weir", "David", ""], ["Peng", "Qiwei", ""]]}, {"id": "2106.01925", "submitter": "Libo Qin", "authors": "Libo Qin, Fuxuan Wei, Tianbao Xie, Xiao Xu, Wanxiang Che, Ting Liu", "title": "GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple\n  Intent Detection and Slot Filling", "comments": "Accepted at ACL2021 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-intent SLU can handle multiple intents in an utterance, which has\nattracted increasing attention. However, the state-of-the-art joint models\nheavily rely on autoregressive approaches, resulting in two issues: slow\ninference speed and information leakage. In this paper, we explore a\nnon-autoregressive model for joint multiple intent detection and slot filling,\nachieving more fast and accurate. Specifically, we propose a Global-Locally\nGraph Interaction Network (GL-GIN) where a local slot-aware graph interaction\nlayer is proposed to model slot dependency for alleviating uncoordinated slots\nproblem while a global intent-slot graph interaction layer is introduced to\nmodel the interaction between multiple intents and all slots in the utterance.\nExperimental results on two public datasets show that our framework achieves\nstate-of-the-art performance while being 11.5 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:22:38 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Qin", "Libo", ""], ["Wei", "Fuxuan", ""], ["Xie", "Tianbao", ""], ["Xu", "Xiao", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2106.01933", "submitter": "David Gaddy", "authors": "David Gaddy and Dan Klein", "title": "An Improved Model for Voicing Silent Speech", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an improved model for voicing silent speech, where\naudio is synthesized from facial electromyography (EMG) signals. To give our\nmodel greater flexibility to learn its own input features, we directly use EMG\nsignals as input in the place of hand-designed features used by prior work. Our\nmodel uses convolutional layers to extract features from the signals and\nTransformer layers to propagate information across longer distances. To provide\nbetter signal for learning, we also introduce an auxiliary task of predicting\nphoneme labels in addition to predicting speech audio features. On an open\nvocabulary intelligibility evaluation, our model improves the state of the art\nfor this task by an absolute 25.8%.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:33:23 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 15:36:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gaddy", "David", ""], ["Klein", "Dan", ""]]}, {"id": "2106.01950", "submitter": "Ulme Wennberg", "authors": "Ulme Wennberg, Gustav Eje Henter", "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based\n  Language Models", "comments": "11 pages, 8 figures, Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanisms for encoding positional information are central for\ntransformer-based language models. In this paper, we analyze the position\nembeddings of existing language models, finding strong evidence of translation\ninvariance, both for the embeddings themselves and for their effect on\nself-attention. The degree of translation invariance increases during training\nand correlates positively with model performance. Our findings lead us to\npropose translation-invariant self-attention (TISA), which accounts for the\nrelative position between tokens in an interpretable fashion without needing\nconventional position embeddings. Our proposal has several theoretical\nadvantages over existing position-representation approaches. Experiments show\nthat it improves on regular ALBERT on GLUE tasks, while only adding orders of\nmagnitude less positional parameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:56:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wennberg", "Ulme", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2106.01960", "submitter": "Gaurav Sahu", "authors": "Olga Vechtomova, Gaurav Sahu, Dhruv Kumar", "title": "LyricJam: A system for generating lyrics for live instrumental music", "comments": "Accepted to International Conference on Computational Creativity\n  (ICCC) 2021 [Oral]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a real-time system that receives a live audio stream from a jam\nsession and generates lyric lines that are congruent with the live music being\nplayed. Two novel approaches are proposed to align the learned latent spaces of\naudio and text representations that allow the system to generate novel lyric\nlines matching live instrumental music. One approach is based on adversarial\nalignment of latent representations of audio and lyrics, while the other\napproach learns to transfer the topology from the music latent space to the\nlyric latent space. A user study with music artists using the system showed\nthat the system was useful not only in lyric composition, but also encouraged\nthe artists to improvise and find new musical expressions. Another user study\ndemonstrated that users preferred the lines generated using the proposed\nmethods to the lines generated by a baseline model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:06:46 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Vechtomova", "Olga", ""], ["Sahu", "Gaurav", ""], ["Kumar", "Dhruv", ""]]}, {"id": "2106.01972", "submitter": "Ruochen Zhang", "authors": "Ruochen Zhang and Carsten Eickhoff", "title": "SOCCER: An Information-Sparse Discourse State Tracking Collection in the\n  Sports Commentary Domain", "comments": "Accepted at NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the pursuit of natural language understanding, there has been a long\nstanding interest in tracking state changes throughout narratives. Impressive\nprogress has been made in modeling the state of transaction-centric dialogues\nand procedural texts. However, this problem has been less intensively studied\nin the realm of general discourse where ground truth descriptions of states may\nbe loosely defined and state changes are less densely distributed over\nutterances. This paper proposes to turn to simplified, fully observable systems\nthat show some of these properties: Sports events. We curated 2,263 soccer\nmatches including time-stamped natural language commentary accompanied by\ndiscrete events such as a team scoring goals, switching players or being\npenalized with cards. We propose a new task formulation where, given paragraphs\nof commentary of a game at different timestamps, the system is asked to\nrecognize the occurrence of in-game events. This domain allows for rich\ndescriptions of state while avoiding the complexities of many other real-world\nsettings. As an initial point of performance measurement, we include two\nbaseline methods from the perspectives of sentence classification with temporal\ndependence and current state-of-the-art generative model, respectively, and\ndemonstrate that even sophisticated existing methods struggle on the state\ntracking task when the definition of state broadens or non-event chatter\nbecomes prevalent.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:21:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Ruochen", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2106.01978", "submitter": "Dou Hu", "authors": "Dou Hu, Lingwei Wei, Xiaoyong Huai", "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in\n  Conversations", "comments": "11 pages, accepted by ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion Recognition in Conversations (ERC) has gained increasing attention\nfor developing empathetic machines. Recently, many approaches have been devoted\nto perceiving conversational context by deep learning models. However, these\napproaches are insufficient in understanding the context due to lacking the\nability to extract and integrate emotional clues. In this work, we propose\nnovel Contextual Reasoning Networks (DialogueCRN) to fully understand the\nconversational context from a cognitive perspective. Inspired by the Cognitive\nTheory of Emotion, we design multi-turn reasoning modules to extract and\nintegrate emotional clues. The reasoning module iteratively performs an\nintuitive retrieving process and a conscious reasoning process, which imitates\nhuman unique cognitive thinking. Extensive experiments on three public\nbenchmark datasets demonstrate the effectiveness and superiority of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:47:38 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:28:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hu", "Dou", ""], ["Wei", "Lingwei", ""], ["Huai", "Xiaoyong", ""]]}, {"id": "2106.01979", "submitter": "Wenhao Li", "authors": "Wenhao Li, Fanchao Qi, Maosong Sun, Xiaoyuan Yi, Jiarui Zhang", "title": "CCPM: A Chinese Classical Poetry Matching Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Poetry is one of the most important art forms of human languages. Recently\nmany studies have focused on incorporating some linguistic features of poetry,\nsuch as style and sentiment, into its understanding or generation system.\nHowever, there is no focus on understanding or evaluating the semantics of\npoetry. Therefore, we propose a novel task to assess a model's semantic\nunderstanding of poetry by poem matching. Specifically, this task requires the\nmodel to select one line of Chinese classical poetry among four candidates\naccording to the modern Chinese translation of a line of poetry. To construct\nthis dataset, we first obtain a set of parallel data of Chinese classical\npoetry and modern Chinese translation. Then we retrieve similar lines of poetry\nwith the lines in a poetry corpus as negative choices. We name the dataset\nChinese Classical Poetry Matching Dataset (CCPM) and release it at\nhttps://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance\nthe study on incorporating deep semantics into the understanding and generation\nsystem of Chinese classical poetry. We also preliminarily run two variants of\nBERT on this dataset as the baselines for this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:49:03 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Wenhao", ""], ["Qi", "Fanchao", ""], ["Sun", "Maosong", ""], ["Yi", "Xiaoyuan", ""], ["Zhang", "Jiarui", ""]]}, {"id": "2106.02009", "submitter": "Mario Graff", "authors": "Eric S. Tellez, Sabino Miranda-Jim\\'enez, Mario Graff, Daniela\n  Moctezuma, Oscar S. Siodia, and Elio A. Villase\\~nor", "title": "A Case Study of Spanish Text Transformations for Twitter Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2017.03.071", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a text mining task that determines the polarity of a\ngiven text, i.e., its positiveness or negativeness. Recently, it has received a\nlot of attention given the interest in opinion mining in micro-blogging\nplatforms. These new forms of textual expressions present new challenges to\nanalyze text given the use of slang, orthographic and grammatical errors, among\nothers. Along with these challenges, a practical sentiment classifier should be\nable to handle efficiently large workloads.\n  The aim of this research is to identify which text transformations\n(lemmatization, stemming, entity removal, among others), tokenizers (e.g.,\nwords $n$-grams), and tokens weighting schemes impact the most the accuracy of\na classifier (Support Vector Machine) trained on two Spanish corpus. The\nmethodology used is to exhaustively analyze all the combinations of the text\ntransformations and their respective parameters to find out which\ncharacteristics the best performing classifiers have in common. Furthermore,\namong the different text transformations studied, we introduce a novel approach\nbased on the combination of word based $n$-grams and character based $q$-grams.\nThe results show that this novel combination of words and characters produces a\nclassifier that outperforms the traditional word based combination by $11.17\\%$\nand $5.62\\%$ on the INEGI and TASS'15 dataset, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:24:31 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tellez", "Eric S.", ""], ["Miranda-Jim\u00e9nez", "Sabino", ""], ["Graff", "Mario", ""], ["Moctezuma", "Daniela", ""], ["Siodia", "Oscar S.", ""], ["Villase\u00f1or", "Elio A.", ""]]}, {"id": "2106.02011", "submitter": "Siyu Zhang", "authors": "Siyu Zhang, Zhongliang Yang, Jinshuai Yang, Yongfeng Huang", "title": "Provably Secure Generative Linguistic Steganography", "comments": "Accepted by ACL-IJCNLP 2021: findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative linguistic steganography mainly utilized language models and\napplied steganographic sampling (stegosampling) to generate high-security\nsteganographic text (stegotext). However, previous methods generally lead to\nstatistical differences between the conditional probability distributions of\nstegotext and natural text, which brings about security risks. In this paper,\nto further ensure security, we present a novel provably secure generative\nlinguistic steganographic method ADG, which recursively embeds secret\ninformation by Adaptive Dynamic Grouping of tokens according to their\nprobability given by an off-the-shelf language model. We not only prove the\nsecurity of ADG mathematically, but also conduct extensive experiments on three\npublic corpora to further verify its imperceptibility. The experimental results\nreveal that the proposed method is able to generate stegotext with nearly\nperfect security.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:27:10 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Siyu", ""], ["Yang", "Zhongliang", ""], ["Yang", "Jinshuai", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.02016", "submitter": "Somnath Roy", "authors": "Somnath Roy", "title": "Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for\n  End Usability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in supervised, semi-supervised and self-supervised deep\nlearning algorithms have shown significant improvement in the performance of\nautomatic speech recognition(ASR) systems. The state-of-the-art systems have\nachieved a word error rate (WER) less than 5%. However, in the past,\nresearchers have argued the non-suitability of the WER metric for the\nevaluation of ASR systems for downstream tasks such as spoken language\nunderstanding (SLU) and information retrieval. The reason is that the WER works\nat the surface level and does not include any syntactic and semantic\nknowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate\nthe ASR transcripts for downstream applications in general. The SWER can be\neasily customized for any down-stream task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:35:14 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Roy", "Somnath", ""]]}, {"id": "2106.02017", "submitter": "Mozhi Zhang", "authors": "Mozhi Zhang, Wei Wang, Budhaditya Deb, Guoqing Zheng, Milad Shokouhi,\n  Ahmed Hassan Awadallah", "title": "A Dataset and Baselines for Multilingual Reply Suggestion", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reply suggestion models help users process emails and chats faster. Previous\nwork only studies English reply suggestion. Instead, we present MRS, a\nmultilingual reply suggestion dataset with ten languages. MRS can be used to\ncompare two families of models: 1) retrieval models that select the reply from\na fixed set and 2) generation models that produce the reply from scratch.\nTherefore, MRS complements existing cross-lingual generalization benchmarks\nthat focus on classification and sequence labeling tasks. We build a generation\nmodel and a retrieval model as baselines for MRS. The two models have different\nstrengths in the monolingual setting, and they require different strategies to\ngeneralize across languages. MRS is publicly available at\nhttps://github.com/zhangmozhi/mrs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:36:32 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Mozhi", ""], ["Wang", "Wei", ""], ["Deb", "Budhaditya", ""], ["Zheng", "Guoqing", ""], ["Shokouhi", "Milad", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2106.02076", "submitter": "Justin Edwards", "authors": "Justin Edwards, Leigh Clark and Allison Perrone", "title": "LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in\n  Chatbots", "comments": null, "journal-ref": null, "doi": "10.1145/3469595.3469597", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chatbots are popular machine partners for task-oriented and social\ninteractions. Human-human computer-mediated communication research has explored\nhow people express their gender and sexuality in online social interactions,\nbut little is known about whether and in what way chatbots do the same. We\nconducted semi-structured interviews with 5 text-based conversational agents to\nexplore this topic Through these interviews, we identified 6 common themes\naround the expression of gender and sexual identity: identity description,\nidentity formation, peer acceptance, positive reflection, uncomfortable\nfeelings and off-topic responses. Chatbots express gender and sexuality\nexplicitly and through relation of experience and emotions, mimicking the human\nlanguage on which they are trained. It is nevertheless evident that chatbots\ndiffer from human dialogue partners as they lack the flexibility and\nunderstanding enabled by lived human experience. While chatbots are proficient\nin using language to express identity, they also display a lack of authentic\nexperiences of gender and sexuality.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 18:47:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Edwards", "Justin", ""], ["Clark", "Leigh", ""], ["Perrone", "Allison", ""]]}, {"id": "2106.02082", "submitter": "Dian Yu", "authors": "Dian Yu and Taiqi He and Kenji Sagae", "title": "Language Embeddings for Typology and Cross-lingual Transfer Learning", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual language tasks typically require a substantial amount of\nannotated data or parallel translation data. We explore whether language\nrepresentations that capture relationships among languages can be learned and\nsubsequently leveraged in cross-lingual tasks without the use of parallel data.\nWe generate dense embeddings for 29 languages using a denoising autoencoder,\nand evaluate the embeddings using the World Atlas of Language Structures (WALS)\nand two extrinsic tasks in a zero-shot setting: cross-lingual dependency\nparsing and cross-lingual natural language inference.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:00:02 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yu", "Dian", ""], ["He", "Taiqi", ""], ["Sagae", "Kenji", ""]]}, {"id": "2106.02083", "submitter": "Anna Kapron-King", "authors": "Anna Kapron-King and Yang Xu", "title": "A diachronic evaluation of gender asymmetry in euphemism", "comments": "11 pages, 5 figures. To appear in Proceedings of the 2nd\n  International Workshop on Computational Approaches to Historical Language\n  Change, ACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of euphemisms is a known driver of language change. It has been\nproposed that women use euphemisms more than men. Although there have been\nseveral studies investigating gender differences in language, the claim about\neuphemism usage has not been tested comprehensively through time. If women do\nuse euphemisms more, this could mean that women also lead the formation of new\neuphemisms and language change over time. Using four large diachronic text\ncorpora of English, we evaluate the claim that women use euphemisms more than\nmen through a quantitative analysis. We assembled a list of 106 euphemism-taboo\npairs to analyze their relative use through time by each gender in the corpora.\nContrary to the existing belief, our results show that women do not use\neuphemisms with a higher proportion than men. We repeated the analysis using\ndifferent subsets of the euphemism-taboo pairs list and found that our result\nwas robust. Our study indicates that in a broad range of settings involving\nboth speech and writing, and with varying degrees of formality, women do not\nuse or form euphemisms more than men.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:00:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kapron-King", "Anna", ""], ["Xu", "Yang", ""]]}, {"id": "2106.02124", "submitter": "Abteen Ebrahimi", "authors": "Abteen Ebrahimi and Katharina Kann", "title": "How to Adapt Your Pretrained Multilingual Model to 1600 Languages", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained multilingual models (PMMs) enable zero-shot learning via\ncross-lingual transfer, performing best for languages seen during pretraining.\nWhile methods exist to improve performance for unseen languages, they have\nalmost exclusively been evaluated using amounts of raw text only available for\na small fraction of the world's languages. In this paper, we evaluate the\nperformance of existing methods to adapt PMMs to new languages using a resource\navailable for over 1600 languages: the New Testament. This is challenging for\ntwo reasons: (1) the small corpus size, and (2) the narrow domain. While\nperformance drops for all approaches, we surprisingly still see gains of up to\n$17.69\\%$ accuracy for part-of-speech tagging and $6.29$ F1 for NER on average\nover all languages as compared to XLM-R. Another unexpected finding is that\ncontinued pretraining, the simplest approach, performs best. Finally, we\nperform a case study to disentangle the effects of domain and size and to shed\nlight on the influence of the finetuning source language.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 20:50:02 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ebrahimi", "Abteen", ""], ["Kann", "Katharina", ""]]}, {"id": "2106.02134", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad, Haoran Li, Kai-Wei Chang, Yashar Mehdad", "title": "Syntax-augmented Multilingual BERT for Cross-lingual Transfer", "comments": "ACL 2021 (camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen a colossal effort in pre-training multilingual\ntext encoders using large-scale corpora in many languages to facilitate\ncross-lingual transfer learning. However, due to typological differences across\nlanguages, the cross-lingual transfer is challenging. Nevertheless, language\nsyntax, e.g., syntactic dependencies, can bridge the typological gap. Previous\nworks have shown that pre-trained multilingual encoders, such as mBERT\n\\cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual\ntransfer. This work shows that explicitly providing language syntax and\ntraining mBERT using an auxiliary objective to encode the universal dependency\ntree structure helps cross-lingual transfer. We perform rigorous experiments on\nfour NLP tasks, including text classification, question answering, named entity\nrecognition, and task-oriented semantic parsing. The experiment results show\nthat syntax-augmented mBERT improves cross-lingual transfer on popular\nbenchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across\nall languages. In the \\emph{generalized} transfer setting, the performance\nboosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 21:12:50 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Li", "Haoran", ""], ["Chang", "Kai-Wei", ""], ["Mehdad", "Yashar", ""]]}, {"id": "2106.02170", "submitter": "Saurabhchand Bhati", "authors": "Saurabhchand Bhati, Jes\\'us Villalba, Piotr \\.Zelasko, Laureano\n  Moro-Velazquez, Najim Dehak", "title": "Segmental Contrastive Predictive Coding for Unsupervised Word\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of phoneme or word-like units is one of the core\nobjectives in zero-resource speech processing. Recent attempts employ\nself-supervised training methods, such as contrastive predictive coding (CPC),\nwhere the next frame is predicted given past context. However, CPC only looks\nat the audio signal's frame-level structure. We overcome this limitation with a\nsegmental contrastive predictive coding (SCPC) framework that can model the\nsignal structure at a higher level e.g. at the phoneme level. In this\nframework, a convolutional neural network learns frame-level representation\nfrom the raw waveform via noise-contrastive estimation (NCE). A differentiable\nboundary detector finds variable-length segments, which are then used to\noptimize a segment encoder via NCE to learn segment representations. The\ndifferentiable boundary detector allows us to train frame-level and\nsegment-level encoders jointly. Typically, phoneme and word segmentation are\ntreated as separate tasks. We unify them and experimentally show that our\nsingle model outperforms existing phoneme and word segmentation methods on\nTIMIT and Buckeye datasets. We analyze the impact of boundary threshold and\nwhen is the right time to include the segmental loss in the learning process.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 23:12:05 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bhati", "Saurabhchand", ""], ["Villalba", "Jes\u00fas", ""], ["\u017belasko", "Piotr", ""], ["Moro-Velazquez", "Laureano", ""], ["Dehak", "Najim", ""]]}, {"id": "2106.02171", "submitter": "Aditya Siddhant", "authors": "Mihir Kale, Aditya Siddhant, Noah Constant, Melvin Johnson, Rami\n  Al-Rfou, Linting Xue", "title": "nmT5 -- Is parallel data still relevant for pre-training massively\n  multilingual language models?", "comments": "Accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, mT5 - a massively multilingual version of T5 - leveraged a unified\ntext-to-text format to attain state-of-the-art results on a wide variety of\nmultilingual NLP tasks. In this paper, we investigate the impact of\nincorporating parallel data into mT5 pre-training. We find that multi-tasking\nlanguage modeling with objectives such as machine translation during\npre-training is a straightforward way to improve performance on downstream\nmultilingual and cross-lingual tasks. However, the gains start to diminish as\nthe model capacity increases, suggesting that parallel data might not be as\nessential for larger models. At the same time, even at larger model sizes, we\nfind that pre-training with parallel data still provides benefits in the\nlimited labelled data regime.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 23:12:27 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kale", "Mihir", ""], ["Siddhant", "Aditya", ""], ["Constant", "Noah", ""], ["Johnson", "Melvin", ""], ["Al-Rfou", "Rami", ""], ["Xue", "Linting", ""]]}, {"id": "2106.02182", "submitter": "Chenyu You", "authors": "Nuo Chen, Chenyu You, Yuexian Zou", "title": "Self-supervised Dialogue Learning for Spoken Conversational Question\n  Answering", "comments": "To Appear Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 00:09:38 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 19:53:17 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 16:31:29 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chen", "Nuo", ""], ["You", "Chenyu", ""], ["Zou", "Yuexian", ""]]}, {"id": "2106.02183", "submitter": "Noura Al Moubayed", "authors": "Elizabeth Excell and Noura Al Moubayed", "title": "Towards Equal Gender Representation in the Annotations of Toxic Language\n  Detection", "comments": "Paper is accepted at GeBNLP2021 workshop at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classifiers tend to propagate biases present in the data on which they are\ntrained. Hence, it is important to understand how the demographic identities of\nthe annotators of comments affect the fairness of the resulting model. In this\npaper, we focus on the differences in the ways men and women annotate comments\nfor toxicity, investigating how these differences result in models that amplify\nthe opinions of male annotators. We find that the BERT model as-sociates toxic\ncomments containing offensive words with male annotators, causing the model to\npredict 67.7% of toxic comments as having been annotated by men. We show that\nthis disparity between gender predictions can be mitigated by removing\noffensive words and highly toxic comments from the training data. We then apply\nthe learned associations between gender and language to toxic language\nclassifiers, finding that models trained exclusively on female-annotated data\nperform 1.8% better than those trained solely on male-annotated data and that\ntraining models on data after removing all offensive words reduces bias in the\nmodel by 55.5% while increasing the sensitivity by 0.4%.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 00:12:38 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Excell", "Elizabeth", ""], ["Moubayed", "Noura Al", ""]]}, {"id": "2106.02192", "submitter": "Khyathi Raghavi Chandu", "authors": "Khyathi Raghavi Chandu, Yonatan Bisk, Alan W Black", "title": "Grounding 'Grounding' in NLP", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NLP community has seen substantial recent interest in grounding to\nfacilitate interaction between language technologies and the world. However, as\na community, we use the term broadly to reference any linking of text to data\nor non-textual modality. In contrast, Cognitive Science more formally defines\n\"grounding\" as the process of establishing what mutual information is required\nfor successful communication between two interlocutors -- a definition which\nmight implicitly capture the NLP usage but differs in intent and scope. We\ninvestigate the gap between these definitions and seek answers to the following\nquestions: (1) What aspects of grounding are missing from NLP tasks? Here we\npresent the dimensions of coordination, purviews and constraints. (2) How is\nthe term \"grounding\" used in the current research? We study the trends in\ndatasets, domains, and tasks introduced in recent NLP conferences. And finally,\n(3) How to advance our current definition to bridge the gap with Cognitive\nScience? We present ways to both create new tasks or repurpose existing ones to\nmake advancements towards achieving a more complete sense of grounding.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 00:40:59 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chandu", "Khyathi Raghavi", ""], ["Bisk", "Yonatan", ""], ["Black", "Alan W", ""]]}, {"id": "2106.02208", "submitter": "Inigo Jauregi Unanue", "authors": "Inigo Jauregi Unanue, Jacob Parnell, Massimo Piccardi", "title": "BERTTune: Fine-Tuning Neural Machine Translation with BERTScore", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation models are often biased toward the limited\ntranslation references seen during training. To amend this form of overfitting,\nin this paper we propose fine-tuning the models with a novel training objective\nbased on the recently-proposed BERTScore evaluation metric. BERTScore is a\nscoring function based on contextual embeddings that overcomes the typical\nlimitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing\ntranslations that are different from the references, yet close in the\ncontextual embedding space, to be treated as substantially correct. To be able\nto use BERTScore as a training objective, we propose three approaches for\ngenerating soft predictions, allowing the network to remain completely\ndifferentiable end-to-end. Experiments carried out over four, diverse language\npairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up\nto 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:13:59 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Unanue", "Inigo Jauregi", ""], ["Parnell", "Jacob", ""], ["Piccardi", "Massimo", ""]]}, {"id": "2106.02210", "submitter": "Fei Huang", "authors": "Fei Huang, Zikai Chen, Chen Henry Wu, Qihan Guo, Xiaoyan Zhu, Minlie\n  Huang", "title": "NAST: A Non-Autoregressive Generator with Word Alignment for\n  Unsupervised Text Style Transfer", "comments": "Accepted by ACL 2021: Findings (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive models have been widely used in unsupervised text style\ntransfer. Despite their success, these models still suffer from the content\npreservation problem that they usually ignore part of the source sentence and\ngenerate some irrelevant words with strong styles. In this paper, we propose a\nNon-Autoregressive generator for unsupervised text Style Transfer (NAST), which\nalleviates the problem from two aspects. First, we observe that most words in\nthe transferred sentence can be aligned with related words in the source\nsentence, so we explicitly model word alignments to suppress irrelevant words.\nSecond, existing models trained with the cycle loss align sentences in two\nstylistic text spaces, which lacks fine-grained control at the word level. The\nproposed non-autoregressive generator focuses on the connections between\naligned words, which learns the word-level transfer between styles. For\nexperiments, we integrate the proposed generator into two base models and\nevaluate them on two style transfer tasks. The results show that NAST can\nsignificantly improve the overall performance and provide explainable word\nalignments. Moreover, the non-autoregressive generator achieves over 10x\nspeedups at inference. Our codes are available at\nhttps://github.com/thu-coai/NAST.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:23:54 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Huang", "Fei", ""], ["Chen", "Zikai", ""], ["Wu", "Chen Henry", ""], ["Guo", "Qihan", ""], ["Zhu", "Xiaoyan", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.02227", "submitter": "Zekang Li", "authors": "Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, Jie Zhou", "title": "Conversations Are Not Flat: Modeling the Dynamic Information Flow across\n  Dialogue Utterances", "comments": "ACL2021 main conference (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, open-domain dialogue models can generate acceptable responses\naccording to the historical context based on the large-scale pre-trained\nlanguage models. However, they generally concatenate the dialogue history\ndirectly as the model input to predict the response, which we named as the flat\npattern and ignores the dynamic information flow across dialogue utterances. In\nthis work, we propose the DialoFlow model, in which we introduce a dynamic flow\nmechanism to model the context flow, and design three training objectives to\ncapture the information dynamics across dialogue utterances by addressing the\nsemantic influence brought about by each utterance in large-scale pre-training.\nExperiments on the multi-reference Reddit Dataset and DailyDialog Dataset\ndemonstrate that our DialoFlow significantly outperforms the DialoGPT on the\ndialogue generation task. Besides, we propose the Flow score, an effective\nautomatic metric for evaluating interactive human-bot conversation quality\nbased on the pre-trained DialoFlow, which presents high chatbot-level\ncorrelation ($r=0.9$) with human ratings among 11 chatbots. Code and\npre-trained models will be public.\n\\footnote{\\url{https://github.com/ictnlp/DialoFlow}}\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 03:04:06 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Zekang", ""], ["Zhang", "Jinchao", ""], ["Fei", "Zhengcong", ""], ["Feng", "Yang", ""], ["Zhou", "Jie", ""]]}, {"id": "2106.02228", "submitter": "Zekang Li", "authors": "Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, Jie Zhou", "title": "Addressing Inquiries about History: An Efficient and Practical Framework\n  for Evaluating Open-domain Chatbot Consistency", "comments": "Findings of ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good open-domain chatbot should avoid presenting contradictory responses\nabout facts or opinions in a conversational session, known as its consistency\ncapacity. However, evaluating the consistency capacity of a chatbot is still\nchallenging. Employing human judges to interact with chatbots on purpose to\ncheck their capacities is costly and low-efficient, and difficult to get rid of\nsubjective bias. In this paper, we propose the Addressing Inquiries about\nHistory (AIH), an efficient and practical framework for the consistency\nevaluation. At the conversation stage, AIH attempts to address appropriate\ninquiries about the dialogue history to induce the chatbot to redeclare the\nhistorical facts or opinions. We carry out the conversation between chatbots,\nwhich is more efficient than the human-bot interaction and can also alleviate\nthe subjective bias. In this way, we manage to rapidly obtain a dialog session\nthat contains responses with high contradiction possibilities. At the\ncontradiction recognition stage, we can either employ human judges or a natural\nlanguage inference (NLI) model to recognize whether the answers to the\ninquiries are contradictory with history. Finally, we are able to rank chatbots\naccording to the contradiction statistics. Experiments on open-domain chatbots\nshow that our approach can efficiently and reliably assess the consistency\ncapacity of chatbots and achieve a high ranking correlation with the human\nevaluation. We release the framework and hope to help improve the consistency\ncapacity of chatbots. \\footnote{\\url{https://github.com/ictnlp/AIH}}\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 03:04:13 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Zekang", ""], ["Zhang", "Jinchao", ""], ["Fei", "Zhengcong", ""], ["Feng", "Yang", ""], ["Zhou", "Jie", ""]]}, {"id": "2106.02232", "submitter": "Qianlan Ying", "authors": "Qianlan Ying, Payal Bajaj, Budhaditya Deb, Yu Yang, Wei Wang, Bojia\n  Lin, Milad Shokouhi, Xia Song, Yang Yang, and Daxin Jiang", "title": "Language Scaling for Universal Suggested Replies Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of scaling automated suggested replies for Outlook\nemail system to multiple languages. Faced with increased compute requirements\nand low resources for language expansion, we build a single universal model for\nimproving the quality and reducing run-time costs of our production system.\nHowever, restricted data movement across regional centers prevents joint\ntraining across languages. To this end, we propose a multi-task continual\nlearning framework, with auxiliary tasks and language adapters to learn\nuniversal language representation across regions. The experimental results show\npositive cross-lingual transfer across languages while reducing catastrophic\nforgetting across regions. Our online results on real user traffic show\nsignificant gains in CTR and characters saved, as well as 65% training cost\nreduction compared with per-language models. As a consequence, we have scaled\nthe feature in multiple languages including low-resource markets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 03:15:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ying", "Qianlan", ""], ["Bajaj", "Payal", ""], ["Deb", "Budhaditya", ""], ["Yang", "Yu", ""], ["Wang", "Wei", ""], ["Lin", "Bojia", ""], ["Shokouhi", "Milad", ""], ["Song", "Xia", ""], ["Yang", "Yang", ""], ["Jiang", "Daxin", ""]]}, {"id": "2106.02241", "submitter": "Jiaxiang Liu", "authors": "Weiyue Su, Xuyi Chen, Shikun Feng, Jiaxiang Liu, Weixin Liu, Yu Sun,\n  Hao Tian, Hua Wu, Haifeng Wang", "title": "ERNIE-Tiny : A Progressive Distillation Framework for Pretrained\n  Transformer Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models (PLMs) such as BERT adopt a training paradigm\nwhich first pretrain the model in general data and then finetune the model on\ntask-specific data, and have recently achieved great success. However, PLMs are\nnotorious for their enormous parameters and hard to be deployed on real-life\napplications. Knowledge distillation has been prevailing to address this\nproblem by transferring knowledge from a large teacher to a much smaller\nstudent over a set of data. We argue that the selection of thee three key\ncomponents, namely teacher, training data, and learning objective, is crucial\nto the effectiveness of distillation. We, therefore, propose a four-stage\nprogressive distillation framework ERNIE-Tiny to compress PLM, which varies the\nthree components gradually from general level to task-specific level.\nSpecifically, the first stage, General Distillation, performs distillation with\nguidance from pretrained teacher, gerenal data and latent distillation loss.\nThen, General-Enhanced Distillation changes teacher model from pretrained\nteacher to finetuned teacher. After that, Task-Adaptive Distillation shifts\ntraining data from general data to task-specific data. In the end,\nTask-Specific Distillation, adds two additional losses, namely Soft-Label and\nHard-Label loss onto the last stage. Empirical results demonstrate the\neffectiveness of our framework and generalization gain brought by ERNIE-Tiny.In\nparticular, experiments show that a 4-layer ERNIE-Tiny maintains over\n98.0%performance of its 12-layer teacher BERT base on GLUE benchmark,\nsurpassing state-of-the-art (SOTA) by 1.0% GLUE score with the same amount of\nparameters. Moreover, ERNIE-Tiny achieves a new compression SOTA on five\nChinese NLP tasks, outperforming BERT base by 0.4% accuracy with 7.5x fewer\nparameters and9.4x faster inference speed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:00:16 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Su", "Weiyue", ""], ["Chen", "Xuyi", ""], ["Feng", "Shikun", ""], ["Liu", "Jiaxiang", ""], ["Liu", "Weixin", ""], ["Sun", "Yu", ""], ["Tian", "Hao", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2106.02242", "submitter": "Shijie Geng", "authors": "Peng Gao, Shijie Geng, Yu Qiao, Xiaogang Wang, Jifeng Dai, Hongsheng\n  Li", "title": "Scalable Transformers for Neural Machine Translation", "comments": "Mostly overlapping with version 1, with minor updates/revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer has been widely adopted in Neural Machine Translation (NMT)\nbecause of its large capacity and parallel training of sequence generation.\nHowever, the deployment of Transformer is challenging because different\nscenarios require models of different complexities and scales. Naively training\nmultiple Transformers is redundant in terms of both computation and memory. In\nthis paper, we propose a novel Scalable Transformers, which naturally contains\nsub-Transformers of different scales and have shared parameters. Each\nsub-Transformer can be easily obtained by cropping the parameters of the\nlargest Transformer. A three-stage training scheme is proposed to tackle the\ndifficulty of training the Scalable Transformers, which introduces additional\nsupervisions from word-level and sequence-level self-distillation. Extensive\nexperiments were conducted on WMT EN-De and En-Fr to validate our proposed\nScalable Transformers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:04:10 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 01:01:07 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Gao", "Peng", ""], ["Geng", "Shijie", ""], ["Qiao", "Yu", ""], ["Wang", "Xiaogang", ""], ["Dai", "Jifeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2106.02248", "submitter": "Muhao Chen", "authors": "Zequn Sun, Muhao Chen, Wei Hu", "title": "Knowing the No-match: Entity Alignment with Dangling Cases", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper studies a new problem setting of entity alignment for knowledge\ngraphs (KGs). Since KGs possess different sets of entities, there could be\nentities that cannot find alignment across them, leading to the problem of\ndangling entities. As the first attempt to this problem, we construct a new\ndataset and design a multi-task learning framework for both entity alignment\nand dangling entity detection. The framework can opt to abstain from predicting\nalignment for the detected dangling entities. We propose three techniques for\ndangling entity detection that are based on the distribution of\nnearest-neighbor distances, i.e., nearest neighbor classification, marginal\nranking and background ranking. After detecting and removing dangling entities,\nan incorporated entity alignment model in our framework can provide more robust\nalignment for remaining entities. Comprehensive experiments and analyses\ndemonstrate the effectiveness of our framework. We further discover that the\ndangling entity detection module can, in turn, improve alignment learning and\nthe final performance. The contributed resource is publicly available to foster\nfurther research.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:28:36 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sun", "Zequn", ""], ["Chen", "Muhao", ""], ["Hu", "Wei", ""]]}, {"id": "2106.02278", "submitter": "Richard Yuanzhe Pang", "authors": "Richard Yuanzhe Pang, Adam D. Lelkes, Vinh Q. Tran, Cong Yu", "title": "AgreeSum: Agreement-Oriented Multi-Document Summarization", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to renew interest in a particular multi-document summarization (MDS)\ntask which we call AgreeSum: agreement-oriented multi-document summarization.\nGiven a cluster of articles, the goal is to provide abstractive summaries that\nrepresent information common and faithful to all input articles. Given the lack\nof existing datasets, we create a dataset for AgreeSum, and provide annotations\non article-summary entailment relations for a subset of the clusters in the\ndataset. We aim to create strong baselines for the task by applying the\ntop-performing pretrained single-document summarization model PEGASUS onto\nAgreeSum, leveraging both annotated clusters by supervised losses, and\nunannotated clusters by T5-based entailment-related and language-related\nlosses. Compared to other baselines, both automatic evaluation and human\nevaluation show better article-summary and cluster-summary entailment in\ngenerated summaries. On a separate note, we hope that our article-summary\nentailment annotations contribute to the community's effort in improving\nabstractive summarization faithfulness.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:17:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Pang", "Richard Yuanzhe", ""], ["Lelkes", "Adam D.", ""], ["Tran", "Vinh Q.", ""], ["Yu", "Cong", ""]]}, {"id": "2106.02280", "submitter": "Amanpreet Singh", "authors": "Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Alberto Lopez\n  Magana, Wojciech Galuba, Devi Parikh, Douwe Kiela", "title": "Human-Adversarial Visual Question Answering", "comments": "22 pages, 13 figures. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance on the most commonly used Visual Question Answering dataset (VQA\nv2) is starting to approach human accuracy. However, in interacting with\nstate-of-the-art VQA models, it is clear that the problem is far from being\nsolved. In order to stress test VQA models, we benchmark them against\nhuman-adversarial examples. Human subjects interact with a state-of-the-art VQA\nmodel, and for each image in the dataset, attempt to find a question where the\nmodel's predicted answer is incorrect. We find that a wide range of\nstate-of-the-art models perform poorly when evaluated on these examples. We\nconduct an extensive analysis of the collected adversarial examples and provide\nguidance on future research directions. We hope that this Adversarial VQA\n(AdVQA) benchmark can help drive progress in the field and advance the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:25:32 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sheng", "Sasha", ""], ["Singh", "Amanpreet", ""], ["Goswami", "Vedanuj", ""], ["Magana", "Jose Alberto Lopez", ""], ["Galuba", "Wojciech", ""], ["Parikh", "Devi", ""], ["Kiela", "Douwe", ""]]}, {"id": "2106.02282", "submitter": "Zhi Chen", "authors": "Zhi Chen, Lu Chen, Hanqi Li, Ruisheng Cao, Da Ma, Mengyue Wu and Kai\n  Yu", "title": "Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn\n  Text-to-SQL", "comments": "12 pages, 3 figures, accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Text-to-SQL for multi-turn dialogue has attracted great interest.\nHere, the user input of the current turn is parsed into the corresponding SQL\nquery of the appropriate database, given all previous dialogue history. Current\napproaches mostly employ end-to-end models and consequently face two\nchallenges. First, dialogue history modeling and Text-to-SQL parsing are\nimplicitly combined, hence it is hard to carry out interpretable analysis and\nobtain targeted improvement. Second, SQL annotation of multi-turn dialogue is\nvery expensive, leading to training data sparsity. In this paper, we propose a\nnovel decoupled multi-turn Text-to-SQL framework, where an utterance rewrite\nmodel first explicitly solves completion of dialogue context, and then a\nsingle-turn Text-to-SQL parser follows. A dual learning approach is also\nproposed for the utterance rewrite model to address the data sparsity problem.\nCompared with end-to-end approaches, the proposed decoupled method can achieve\nexcellent performance without any annotated in-domain data. With just a few\nannotated rewrite cases, the decoupled method outperforms the released\nstate-of-the-art end-to-end models on both SParC and CoSQL datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:31:39 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 03:19:13 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chen", "Zhi", ""], ["Chen", "Lu", ""], ["Li", "Hanqi", ""], ["Cao", "Ruisheng", ""], ["Ma", "Da", ""], ["Wu", "Mengyue", ""], ["Yu", "Kai", ""]]}, {"id": "2106.02287", "submitter": "Cha\\\"im van Toledo", "authors": "Cha\\\"im van Toledo, Friso van Dijk, Marco Spruit", "title": "Dutch Named Entity Recognition and De-identification Methods for the\n  Human Resource Domain", "comments": null, "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.9,\n  No.6, December 2020", "doi": "10.5121/ijnlc.2020.9602", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human resource (HR) domain contains various types of privacy-sensitive\ntextual data, such as e-mail correspondence and performance appraisal. Doing\nresearch on these documents brings several challenges, one of them\nanonymisation. In this paper, we evaluate the current Dutch text\nde-identification methods for the HR domain in four steps. First, by updating\none of these methods with the latest named entity recognition (NER) models. The\nresult is that the NER model based on the CoNLL 2002 corpus in combination with\nthe BERTje transformer give the best combination for suppressing persons\n(recall 0.94) and locations (recall 0.82). For suppressing gender, DEDUCE is\nperforming best (recall 0.53). Second NER evaluation is based on both strict\nde-identification of entities (a person must be suppressed as a person) and\nthird evaluation on a loose sense of de-identification (no matter what how a\nperson is suppressed, as long it is suppressed). In the fourth and last step a\nnew kind of NER dataset is tested for recognising job titles in texts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:59:25 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["van Toledo", "Cha\u00efm", ""], ["van Dijk", "Friso", ""], ["Spruit", "Marco", ""]]}, {"id": "2106.02289", "submitter": "Tiago Pimentel", "authors": "Irene Nikkarinen, Tiago Pimentel, Dami\\'an E. Blasi, Ryan Cotterell", "title": "Modeling the Unigram Distribution", "comments": "Irene Nikkarinen and Tiago Pimentel contributed equally to this work.\n  Accepted to the findings of ACL 2021. Code available in\n  https://github.com/irenenikk/modelling-unigram", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unigram distribution is the non-contextual probability of finding a\nspecific word form in a corpus. While of central importance to the study of\nlanguage, it is commonly approximated by each word's sample frequency in the\ncorpus. This approach, being highly dependent on sample size, assigns zero\nprobability to any out-of-vocabulary (oov) word form. As a result, it produces\nnegatively biased probabilities for any oov word form, while positively biased\nprobabilities to in-corpus words. In this work, we argue in favor of properly\nmodeling the unigram distribution -- claiming it should be a central task in\nnatural language processing. With this in mind, we present a novel model for\nestimating it in a language (a neuralization of Goldwater et al.'s (2011)\nmodel) and show it produces much better estimates across a diverse set of 7\nlanguages than the na\\\"ive use of neural character-level language models.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:02:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Nikkarinen", "Irene", ""], ["Pimentel", "Tiago", ""], ["Blasi", "Dami\u00e1n E.", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.02293", "submitter": "Yanda Chen", "authors": "Yanda Chen, Chris Kedzie, Suraj Nair, Petra Galu\\v{s}\\v{c}\\'akov\\'a,\n  Rui Zhang, Douglas W. Oard, Kathleen McKeown", "title": "Cross-language Sentence Selection via Data Augmentation and Rationale\n  Training", "comments": "ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to cross-language sentence selection in a\nlow-resource setting. It uses data augmentation and negative sampling\ntechniques on noisy parallel sentence data to directly learn a cross-lingual\nembedding-based query relevance model. Results show that this approach performs\nas well as or better than multiple state-of-the-art machine translation +\nmonolingual retrieval systems trained on the same parallel data. Moreover, when\na rationale training secondary objective is applied to encourage the model to\nmatch word alignment hints from a phrase-based statistical machine translation\nmodel, consistent improvements are seen across three language pairs\n(English-Somali, English-Swahili and English-Tagalog) over a variety of\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:08:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chen", "Yanda", ""], ["Kedzie", "Chris", ""], ["Nair", "Suraj", ""], ["Galu\u0161\u010d\u00e1kov\u00e1", "Petra", ""], ["Zhang", "Rui", ""], ["Oard", "Douglas W.", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2106.02300", "submitter": "Huiqiang Jiang", "authors": "Weile Chen, Huiqiang Jiang, Qianhui Wu, B\\\"orje F. Karlsson and Yi\n  Guan", "title": "AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial\n  Discriminator for Cross-Lingual NER", "comments": "This paper has been accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural methods have been shown to achieve high performance in Named Entity\nRecognition (NER), but rely on costly high-quality labeled data for training,\nwhich is not always available across languages. While previous works have shown\nthat unlabeled data in a target language can be used to improve cross-lingual\nmodel performance, we propose a novel adversarial approach (AdvPicker) to\nbetter leverage such data and further improve results. We design an adversarial\nlearning framework in which an encoder learns entity domain knowledge from\nlabeled source-language data and better shared features are captured via\nadversarial training - where a discriminator selects less language-dependent\ntarget-language data via similarity to the source language. Experimental\nresults on standard benchmark datasets well demonstrate that the proposed\nmethod benefits strongly from this data selection process and outperforms\nexisting state-of-the-art methods; without requiring any additional external\nresources (e.g., gazetteers or via machine translation). The code is available\nat https://aka.ms/AdvPicker\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:17:18 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 05:49:34 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Weile", ""], ["Jiang", "Huiqiang", ""], ["Wu", "Qianhui", ""], ["Karlsson", "B\u00f6rje F.", ""], ["Guan", "Yi", ""]]}, {"id": "2106.02302", "submitter": "Zhong Meng", "authors": "Zhong Meng, Yu Wu, Naoyuki Kanda, Liang Lu, Xie Chen, Guoli Ye, Eric\n  Sun, Jinyu Li, Yifan Gong", "title": "Minimum Word Error Rate Training with Language Model Fusion for\n  End-to-End Speech Recognition", "comments": "5 pages, Interspeech 2021", "journal-ref": "Interspeech 2021, Brno, Czech Republic", "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Integrating external language models (LMs) into end-to-end (E2E) models\nremains a challenging task for domain-adaptive speech recognition. Recently,\ninternal language model estimation (ILME)-based LM fusion has shown significant\nword error rate (WER) reduction from Shallow Fusion by subtracting a weighted\ninternal LM score from an interpolation of E2E model and external LM scores\nduring beam search. However, on different test sets, the optimal LM\ninterpolation weights vary over a wide range and have to be tuned extensively\non well-matched validation sets. In this work, we perform LM fusion in the\nminimum WER (MWER) training of an E2E model to obviate the need for LM weights\ntuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),\nwe propose a novel MWER training with ILME (MWER-ILME) where the ILME-based\nfusion is conducted to generate N-best hypotheses and their posteriors.\nAdditional gradient is induced when internal LM is engaged in MWER-ILME loss\ncomputation. During inference, LM weights pre-determined in MWER training\nenable robust LM integrations on test sets from different domains. Experimented\nwith 30K-hour trained transformer transducers, MWER-ILME achieves on average\n8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,\nrespectively, on 6 different test sets\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:24:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Meng", "Zhong", ""], ["Wu", "Yu", ""], ["Kanda", "Naoyuki", ""], ["Lu", "Liang", ""], ["Chen", "Xie", ""], ["Ye", "Guoli", ""], ["Sun", "Eric", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2106.02309", "submitter": "Giovanna D'Agostino", "authors": "Giovanna D'Agostino and Nicola Cotumaccio and Alberto Policriti and\n  Nicola Prezza", "title": "On (co-lex) Ordering Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The states of a deterministic finite automaton A can be identified with\ncollections of words in Pf(L(A)) -- the set of prefixes of words belonging to\nthe regular language accepted by A. But words can be ordered and among the many\npossible orders a very natural one is the co-lexicographic one. Such\nnaturalness stems from the fact that it suggests a transfer of the order from\nwords to the automaton's states. In a number of papers automata admitting a\ntotal ordering of states coherent with the ordering of the set of words\nreaching them have been proposed. Such class of ordered automata -- the Wheeler\nautomata -- turned out to be efficiently stored/searched using an index.\nUnfortunately not all automata can be totally ordered as previously outlined.\nHowever, automata can always be partially ordered and an intrinsic measure of\ntheir complexity can be defined and effectively determined, as the minimum\nwidth of one of their admissible partial orders. As shown in previous works,\nthis new concept of width of an automaton has useful consequences in the fields\nof graph compression, indexing data structures, and automata theory. In this\npaper we prove that a canonical, minimum-width, partially-ordered automaton\naccepting a language L -- dubbed the Hasse automaton H of L -- can be\nexhibited. H provides, in a precise sense, the best possible way to (partially)\norder the states of any automaton accepting L, as long as we want to maintain\nan operational link with the (co-lexicographic) order of Pf(L(A)). Using H we\nprove that the width of the language can be effectively computed from the\nminimum automaton recognizing the language. Finally, we explore the\nrelationship between two (often conflicting) objectives: minimizing the width\nand minimizing the number of states of an automaton.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:41:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["D'Agostino", "Giovanna", ""], ["Cotumaccio", "Nicola", ""], ["Policriti", "Alberto", ""], ["Prezza", "Nicola", ""]]}, {"id": "2106.02317", "submitter": "Li Yunhao", "authors": "Yunhao Li, Yunyi Yang, Xiaojun Quan, Jianxing Yu", "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory", "comments": "Acceptdd to ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue policy learning, a subtask that determines the content of system\nresponse generation and then the degree of task completion, is essential for\ntask-oriented dialogue systems. However, the unbalanced distribution of system\nactions in dialogue datasets often causes difficulty in learning to generate\ndesired actions and responses. In this paper, we propose a\nretrieve-and-memorize framework to enhance the learning of system actions.\nSpecially, we first design a neural context-aware retrieval module to retrieve\nmultiple candidate system actions from the training set given a dialogue\ncontext. Then, we propose a memory-augmented multi-decoder network to generate\nthe system actions conditioned on the candidate actions, which allows the\nnetwork to adaptively select key information in the candidate actions and\nignore noises. We conduct experiments on the large-scale multi-domain\ntask-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental\nresults show that our method achieves competitive performance among several\nstate-of-the-art models in the context-to-response generation task.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:53:56 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 02:15:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Li", "Yunhao", ""], ["Yang", "Yunyi", ""], ["Quan", "Xiaojun", ""], ["Yu", "Jianxing", ""]]}, {"id": "2106.02318", "submitter": "Jun Yan", "authors": "Jun Yan, Nasser Zalmout, Yan Liang, Christan Grant, Xiang Ren, Xin\n  Luna Dong", "title": "AdaTag: Multi-Attribute Value Extraction from Product Profiles with\n  Adaptive Decoding", "comments": "Accepted to ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of product attribute values is an important enabling\ntechnology in e-Commerce platforms. This task is usually modeled using sequence\nlabeling architectures, with several extensions to handle multi-attribute\nextraction. One line of previous work constructs attribute-specific models,\nthrough separate decoders or entirely separate models. However, this approach\nconstrains knowledge sharing across different attributes. Other contributions\nuse a single multi-attribute model, with different techniques to embed\nattribute information. But sharing the entire network parameters across all\nattributes can limit the model's capacity to capture attribute-specific\ncharacteristics. In this paper we present AdaTag, which uses adaptive decoding\nto handle extraction. We parameterize the decoder with pretrained attribute\nembeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This\nallows for separate, but semantically correlated, decoders to be generated on\nthe fly for different attributes. This approach facilitates knowledge sharing,\nwhile maintaining the specificity of each attribute. Our experiments on a\nreal-world e-Commerce dataset show marked improvements over previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:54:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yan", "Jun", ""], ["Zalmout", "Nasser", ""], ["Liang", "Yan", ""], ["Grant", "Christan", ""], ["Ren", "Xiang", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2106.02325", "submitter": "Genta Indra Winata", "authors": "Etsuko Ishii, Genta Indra Winata, Samuel Cahyawijaya, Divesh Lala,\n  Tatsuya Kawahara, Pascale Fung", "title": "ERICA: An Empathetic Android Companion for Covid-19 Quarantine", "comments": "Accepted in SIGDIAL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past year, research in various domains, including Natural Language\nProcessing (NLP), has been accelerated to fight against the COVID-19 pandemic,\nyet such research has just started on dialogue systems. In this paper, we\nintroduce an end-to-end dialogue system which aims to ease the isolation of\npeople under self-quarantine. We conduct a control simulation experiment to\nassess the effects of the user interface, a web-based virtual agent called Nora\nvs. the android ERICA via a video call. The experimental results show that the\nandroid offers a more valuable user experience by giving the impression of\nbeing more empathetic and engaging in the conversation due to its nonverbal\ninformation, such as facial expressions and body gestures.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:14:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ishii", "Etsuko", ""], ["Winata", "Genta Indra", ""], ["Cahyawijaya", "Samuel", ""], ["Lala", "Divesh", ""], ["Kawahara", "Tatsuya", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.02327", "submitter": "Ruikun Luo", "authors": "Ruikun Luo, Guanhuan Huang, Xiaojun Quan", "title": "Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major paradigm of applying a pre-trained language model to downstream\ntasks is to fine-tune it on labeled task data, which often suffers instability\nand low performance when the labeled examples are scarce.~One way to alleviate\nthis problem is to apply post-training on unlabeled task data before\nfine-tuning, adapting the pre-trained model to target domains by contrastive\nlearning that considers either token-level or sequence-level similarity.\nInspired by the success of sequence masking, we argue that both token-level and\nsequence-level similarities can be captured with a pair of masked\nsequences.~Therefore, we propose complementary random masking (CRM) to generate\na pair of masked sequences from an input sequence for sequence-level\ncontrastive learning and then develop contrastive masked language modeling\n(CMLM) for post-training to integrate both token-level and sequence-level\ncontrastive learnings.~Empirical results show that CMLM surpasses several\nrecent post-training methods in few-shot settings without the need for data\naugmentation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:17:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Luo", "Ruikun", ""], ["Huang", "Guanhuan", ""], ["Quan", "Xiaojun", ""]]}, {"id": "2106.02340", "submitter": "Abhilash Nandy", "authors": "Abhilash Nandy, Sayantan Adak, Tanurima Halder, Sai Mahesh Pokala", "title": "cs60075_team2 at SemEval-2021 Task 1 : Lexical Complexity Prediction\n  using Transformer-based Language Models pre-trained on various text corpora", "comments": "6 pages, 1 figure, Semeval-2021 Task 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the performance of the team cs60075_team2 at SemEval\n2021 Task 1 - Lexical Complexity Prediction. The main contribution of this\npaper is to fine-tune transformer-based language models pre-trained on several\ntext corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the\ncorpora from which the CompLex Dataset was extracted, and others being from\nother specific domains such as Finance, Law, etc. We perform ablation studies\non selecting the transformer models and how their individual complexity scores\nare aggregated to get the resulting complexity scores. Our method achieves a\nbest Pearson Correlation of $0.784$ in sub-task 1 (single word) and $0.836$ in\nsub-task 2 (multiple word expressions).\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:42:00 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Nandy", "Abhilash", ""], ["Adak", "Sayantan", ""], ["Halder", "Tanurima", ""], ["Pokala", "Sai Mahesh", ""]]}, {"id": "2106.02359", "submitter": "Zhijing Jin", "authors": "Zhijing Jin, Geeticka Chauhan, Brian Tse, Mrinmaya Sachan, Rada\n  Mihalcea", "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social\n  Impact", "comments": "Findings of ACL 2021; also accepted at the NLP for Positive Impact\n  workshop@ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at http://github.com/zhijing-jin/nlp4sg_acl2021. In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:17:15 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 19:26:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jin", "Zhijing", ""], ["Chauhan", "Geeticka", ""], ["Tse", "Brian", ""], ["Sachan", "Mrinmaya", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2106.02363", "submitter": "Cheng Wang", "authors": "Cheng Wang, Sungjin Lee, Sunghyun Park, Han Li, Young-Bum Kim, Ruhi\n  Sarikaya", "title": "Learning Slice-Aware Representations with Mixture of Attentions", "comments": "Findings of the ACL: ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world machine learning systems are achieving remarkable performance in\nterms of coarse-grained metrics like overall accuracy and F-1 score. However,\nmodel improvement and development often require fine-grained modeling on\nindividual data subsets or slices, for instance, the data slices where the\nmodels have unsatisfactory results. In practice, it gives tangible values for\ndeveloping such models that can pay extra attention to critical or interested\nslices while retaining the original overall performance. This work extends the\nrecent slice-based learning (SBL)~\\cite{chen2019slice} with a mixture of\nattentions (MoA) to learn slice-aware dual attentive representations. We\nempirically show that the MoA approach outperforms the baseline method as well\nas the original SBL approach on monitored slices with two natural language\nunderstanding (NLU) tasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:22:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Cheng", ""], ["Lee", "Sungjin", ""], ["Park", "Sunghyun", ""], ["Li", "Han", ""], ["Kim", "Young-Bum", ""], ["Sarikaya", "Ruhi", ""]]}, {"id": "2106.02382", "submitter": "Ji-Ung Lee", "authors": "Ji-Ung Lee and Jan-Christoph Klie and Iryna Gurevych", "title": "Annotation Curricula to Implicitly Train Non-Expert Annotators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Annotation studies often require annotators to familiarize themselves with\nthe task, its annotation scheme, and the data domain. This can be overwhelming\nin the beginning, mentally taxing, and induce errors into the resulting\nannotations; especially in citizen science or crowd sourcing scenarios where\ndomain expertise is not required and only annotation guidelines are provided.\nTo alleviate these issues, we propose annotation curricula, a novel approach to\nimplicitly train annotators. Our goal is to gradually introduce annotators into\nthe task by ordering instances that are annotated according to a learning\ncurriculum. To do so, we first formalize annotation curricula for sentence- and\nparagraph-level annotation tasks, define an ordering strategy, and identify\nwell-performing heuristics and interactively trained models on three existing\nEnglish datasets. We then conduct a user study with 40 voluntary participants\nwho are asked to identify the most fitting misconception for English tweets\nabout the Covid-19 pandemic. Our results show that using a simple heuristic to\norder instances can already significantly reduce the total annotation time\nwhile preserving a high annotation quality. Annotation curricula thus can\nprovide a novel way to improve data collection. To facilitate future research,\nwe further share our code and data consisting of 2,400 annotations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:48:28 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 07:57:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lee", "Ji-Ung", ""], ["Klie", "Jan-Christoph", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2106.02397", "submitter": "Tillmann Miltzow", "authors": "Tillmann Miltzow and Reinier F. Schmiermann", "title": "On Classifying Continuous Constraint Satisfaction problems", "comments": "40 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.CL cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A continuous constraint satisfaction problem (CCSP) is a constraint\nsatisfaction problem (CSP) with a domain $U \\subset \\mathbb{R}$. We engage in a\nsystematic study to classify CCSPs that are complete of the Existential Theory\nof the Reals, i.e., ER-complete. To define this class, we first consider the\nproblem ETR, which also stands for Existential Theory of the Reals. In an\ninstance of this problem we are given some sentence of the form $\\exists x_1,\n\\ldots, x_n \\in \\mathbb{R} : \\Phi(x_1, \\ldots, x_n)$, where $\\Phi$ is a\nwell-formed quantifier-free formula consisting of the symbols $\\{0, 1, +,\n\\cdot, \\geq, >, \\wedge, \\vee, \\neg\\}$, the goal is to check whether this\nsentence is true. Now the class ER is the family of all problems that admit a\npolynomial-time reduction to ETR. It is known that NP $\\subseteq$ ER\n$\\subseteq$ PSPACE.\n  We restrict our attention on CCSPs with addition constraints ($x + y = z$)\nand some other mild technical condition. Previously, it was shown that\nmultiplication constraints ($x \\cdot y = z$), squaring constraints ($x^2 = y$),\nor inversion constraints ($x\\cdot y = 1$) are sufficient to establish\nER-completeness. We extend this in the strongest possible sense for equality\nconstraints as follows. We show that CCSPs (with addition constraints and some\nother mild technical condition) that have any one well-behaved curved equality\nconstraint ($f(x,y) = 0$) are ER-complete. We further extend our results to\ninequality constraints. We show that any well-behaved convexly curved and any\nwell-behaved concavely curved inequality constraint ($f(x,y) \\geq 0$ and\n$g(x,y) \\geq 0$) imply ER-completeness on the class of such CCSPs.\n  We apply our findings to geometric packing and answer an open question by\nAbrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing\nconvex pieces into a square container under rotations and translations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:23:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Miltzow", "Tillmann", ""], ["Schmiermann", "Reinier F.", ""]]}, {"id": "2106.02399", "submitter": "Mucheng Ren", "authors": "Mucheng Ren, Heyan Huang and Yang Gao", "title": "Prediction or Comparison: Toward Interpretable Qualitative Reasoning", "comments": "12 pages. Accepted as ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative relationships illustrate how changing one property (e.g., moving\nvelocity) affects another (e.g., kinetic energy) and constitutes a considerable\nportion of textual knowledge. Current approaches use either semantic parsers to\ntransform natural language inputs into logical expressions or a \"black-box\"\nmodel to solve them in one step. The former has a limited application range,\nwhile the latter lacks interpretability. In this work, we categorize\nqualitative reasoning tasks into two types: prediction and comparison. In\nparticular, we adopt neural network modules trained in an end-to-end manner to\nsimulate the two reasoning processes. Experiments on two qualitative reasoning\nquestion answering datasets, QuaRTz and QuaRel, show our methods' effectiveness\nand generalization capability, and the intermediate outputs provided by the\nmodules make the reasoning process interpretable.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:27:55 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ren", "Mucheng", ""], ["Huang", "Heyan", ""], ["Gao", "Yang", ""]]}, {"id": "2106.02401", "submitter": "Shan Yang", "authors": "Shan Yang, Yongfei Zhang, Guanglin Niu, Qinghua Zhao, Shiliang Pu", "title": "Entity Concept-enhanced Few-shot Relation Extraction", "comments": "Accepted at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot relation extraction (FSRE) is of great importance in long-tail\ndistribution problem, especially in special domain with low-resource data. Most\nexisting FSRE algorithms fail to accurately classify the relations merely based\non the information of the sentences together with the recognized entity pairs,\ndue to limited samples and lack of knowledge. To address this problem, in this\npaper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction\nscheme (ConceptFERE), which introduces the inherent concepts of entities to\nprovide clues for relation prediction and boost the relations classification\nperformance. Firstly, a concept-sentence attention module is developed to\nselect the most appropriate concept from multiple concepts of each entity by\ncalculating the semantic similarity between sentences and concepts. Secondly, a\nself-attention based fusion module is presented to bridge the gap of concept\nembedding and sentence embedding from different semantic spaces. Extensive\nexperiments on the FSRE benchmark dataset FewRel have demonstrated the\neffectiveness and the superiority of the proposed ConceptFERE scheme as\ncompared to the state-of-the-art baselines. Code is available at\nhttps://github.com/LittleGuoKe/ConceptFERE.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:36:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yang", "Shan", ""], ["Zhang", "Yongfei", ""], ["Niu", "Guanglin", ""], ["Zhao", "Qinghua", ""], ["Pu", "Shiliang", ""]]}, {"id": "2106.02417", "submitter": "Anton Ragni", "authors": "Zhengxiong Wang and Anton Ragni", "title": "Approximate Fixed-Points in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are widely used in speech and language processing.\nDue to dependency on the past, standard algorithms for training these models,\nsuch as back-propagation through time (BPTT), cannot be efficiently\nparallelised. Furthermore, applying these models to more complex structures\nthan sequences requires inference time approximations, which introduce\ninconsistency between inference and training. This paper shows that recurrent\nneural networks can be reformulated as fixed-points of non-linear equation\nsystems. These fixed-points can be computed using an iterative algorithm\nexactly and in as many iterations as the length of any given sequence. Each\niteration of this algorithm adds one additional Markovian-like order of\ndependencies such that upon termination all dependencies modelled by the\nrecurrent neural networks have been incorporated. Although exact fixed-points\ninherit the same parallelization and inconsistency issues, this paper shows\nthat approximate fixed-points can be computed in parallel and used consistently\nin training and inference including tasks such as lattice rescoring.\nExperimental validation is performed in two tasks, Penn Tree Bank and\nWikiText-2, and shows that approximate fixed-points yield competitive\nprediction performance to recurrent neural networks trained using the BPTT\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 11:33:34 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Zhengxiong", ""], ["Ragni", "Anton", ""]]}, {"id": "2106.02435", "submitter": "Shaokun Zhang", "authors": "Shaokun Zhang, Xiawu Zheng, Chenyi Yang, Yuchao Li, Yan Wang, Fei\n  Chao, Mengdi Wang, Shen Li, Jun Yang, Rongrong Ji", "title": "You Only Compress Once: Towards Effective and Elastic BERT Compression\n  via Exploit-Explore Stochastic Nature Gradient", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite superior performance on various natural language processing tasks,\npre-trained models such as BERT are challenged by deploying on\nresource-constraint devices. Most existing model compression approaches require\nre-compression or fine-tuning across diverse constraints to accommodate various\nhardware deployments. This practically limits the further application of model\ncompression. Moreover, the ineffective training and searching process of\nexisting elastic compression paradigms[4,27] prevents the direct migration to\nBERT compression. Motivated by the necessity of efficient inference across\nvarious constraints on BERT, we propose a novel approach, YOCO-BERT, to achieve\ncompress once and deploy everywhere. Specifically, we first construct a huge\nsearch space with 10^13 architectures, which covers nearly all configurations\nin BERT model. Then, we propose a novel stochastic nature gradient optimization\nmethod to guide the generation of optimal candidate architecture which could\nkeep a balanced trade-off between explorations and exploitation. When a certain\nresource constraint is given, a lightweight distribution optimization approach\nis utilized to obtain the optimal network for target deployment without\nfine-tuning. Compared with state-of-the-art algorithms, YOCO-BERT provides more\ncompact models, yet achieving 2.1%-4.5% average accuracy improvement on the\nGLUE benchmark. Besides, YOCO-BERT is also more effective, e.g.,the training\ncomplexity is O(1)for N different devices. Code is\navailablehttps://github.com/MAC-AutoML/YOCO-BERT.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 12:17:44 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhang", "Shaokun", ""], ["Zheng", "Xiawu", ""], ["Yang", "Chenyi", ""], ["Li", "Yuchao", ""], ["Wang", "Yan", ""], ["Chao", "Fei", ""], ["Wang", "Mengdi", ""], ["Li", "Shen", ""], ["Yang", "Jun", ""], ["Ji", "Rongrong", ""]]}, {"id": "2106.02443", "submitter": "Abhijeet Awasthi", "authors": "Abhijeet Awasthi, Kevin Kilgour, Hassan Rom", "title": "Teaching keyword spotters to spot new keywords with limited examples", "comments": "In INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to recognize new keywords with just a few examples is essential for\npersonalizing keyword spotting (KWS) models to a user's choice of keywords.\nHowever, modern KWS models are typically trained on large datasets and\nrestricted to a small vocabulary of keywords, limiting their transferability to\na broad range of unseen keywords. Towards easily customizable KWS models, we\npresent KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained\non the task of recognizing a large number of keywords. Speech representations\noffered by KeySEM are highly effective for learning new keywords from a limited\nnumber of examples. Comparisons with a diverse range of related work across\nseveral datasets show that our method achieves consistently superior\nperformance with fewer training examples. Although KeySEM was pre-trained only\non English utterances, the performance gains also extend to datasets from four\nother languages indicating that KeySEM learns useful representations well\naligned with the task of keyword spotting. Finally, we demonstrate KeySEM's\nability to learn new keywords sequentially without requiring to re-train on\npreviously learned keywords. Our experimental observations suggest that KeySEM\nis well suited to on-device environments where post-deployment learning and\nease of customization are often desirable.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 12:43:36 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Awasthi", "Abhijeet", ""], ["Kilgour", "Kevin", ""], ["Rom", "Hassan", ""]]}, {"id": "2106.02490", "submitter": "Thomas Conley", "authors": "Thomas Conley and Jugal Kalita", "title": "Language Model Metrics and Procrustes Analysis for Improved Vector\n  Transformation of NLP Embeddings", "comments": null, "journal-ref": "Proceedings of the 17th International Conference on Natural\n  Language Processing, pages 170-174, Patna, India, December 18-21, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Neural networks are mathematical models at their core. This\ntruismpresents some fundamental difficulty when networks are tasked with\nNatural Language Processing. A key problem lies in measuring the similarity or\ndistance among vectors in NLP embedding space, since the mathematical concept\nof distance does not always agree with the linguistic concept. We suggest that\nthe best way to measure linguistic distance among vectors is by employing the\nLanguage Model (LM) that created them. We introduce Language Model Distance\n(LMD) for measuring accuracy of vector transformations based on the\nDistributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric\nby applying it to a simple neural network learning the Procrustes algorithm for\nbilingual word mapping.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:56:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Conley", "Thomas", ""], ["Kalita", "Jugal", ""]]}, {"id": "2106.02497", "submitter": "Debjit Paul", "authors": "Debjit Paul and Anette Frank", "title": "COINS: Dynamically Generating COntextualized Inference Rules for\n  Narrative Story Completion", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent successes of large pre-trained language models in solving\nreasoning tasks, their inference capabilities remain opaque. We posit that such\nmodels can be made more interpretable by explicitly generating interim\ninference rules, and using them to guide the generation of task-specific\ntextual outputs. In this paper we present COINS, a recursive inference\nframework that i) iteratively reads context sentences, ii) dynamically\ngenerates contextualized inference rules, encodes them, and iii) uses them to\nguide task-specific output generation. We apply COINS to a Narrative Story\nCompletion task that asks a model to complete a story with missing sentences,\nto produce a coherent story with plausible logical connections, causal\nrelationships, and temporal dependencies. By modularizing inference and\nsentence generation steps in a recurrent model, we aim to make reasoning steps\nand their effects on next sentence generation transparent. Our automatic and\nmanual evaluations show that the model generates better story sentences than\nSOTA baselines, especially in terms of coherence. We further demonstrate\nimproved performance over strong pre-trained LMs in generating commonsense\ninference rules. The recursive nature of COINS holds the potential for\ncontrolled generation of longer sequences.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:06:33 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Paul", "Debjit", ""], ["Frank", "Anette", ""]]}, {"id": "2106.02516", "submitter": "Thomas Conley", "authors": "Thomas Conley, Jack St. Clair, Jugal Kalita", "title": "Improving Computer Generated Dialog with Auxiliary Loss Functions and\n  Custom Evaluation Metrics", "comments": null, "journal-ref": "Proceedings of ICON-2018, Patiala, India. December 2018, pages\n  143--149", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although people have the ability to engage in vapid dialogue without effort,\nthis may not be a uniquely human trait. Since the 1960's researchers have been\ntrying to create agents that can generate artificial conversation. These\nprograms are commonly known as chatbots. With increasing use of neural networks\nfor dialog generation, some conclude that this goal has been achieved. This\nresearch joins the quest by creating a dialog generating Recurrent Neural\nNetwork (RNN) and by enhancing the ability of this network with auxiliary loss\nfunctions and a beam search. Our custom loss functions achieve better cohesion\nand coherence by including calculations of Maximum Mutual Information (MMI) and\nentropy. We demonstrate the effectiveness of this system by using a set of\ncustom evaluation metrics inspired by an abundance of previous research and\nbased on tried-and-true principles of Natural Language Processing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:35:05 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Conley", "Thomas", ""], ["Clair", "Jack St.", ""], ["Kalita", "Jugal", ""]]}, {"id": "2106.02524", "submitter": "James Mullenbach", "authors": "James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale,\n  Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David Sontag", "title": "CLIP: A Dataset for Extracting Action Items for Physicians from Hospital\n  Discharge Notes", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuity of care is crucial to ensuring positive health outcomes for\npatients discharged from an inpatient hospital setting, and improved\ninformation sharing can help. To share information, caregivers write discharge\nnotes containing action items to share with patients and their future\ncaregivers, but these action items are easily lost due to the lengthiness of\nthe documents. In this work, we describe our creation of a dataset of clinical\naction items annotated over MIMIC-III, the largest publicly available dataset\nof real clinical notes. This dataset, which we call CLIP, is annotated by\nphysicians and covers 718 documents representing 100K sentences. We describe\nthe task of extracting the action items from these documents as multi-aspect\nextractive summarization, with each aspect representing a type of action to be\ntaken. We evaluate several machine learning models on this task, and show that\nthe best models exploit in-domain language model pre-training on 59K\nunannotated documents, and incorporate context from neighboring sentences. We\nalso propose an approach to pre-training data selection that allows us to\nexplore the trade-off between size and domain-specificity of pre-training\ndatasets for this task.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:49:02 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Mullenbach", "James", ""], ["Pruksachatkun", "Yada", ""], ["Adler", "Sean", ""], ["Seale", "Jennifer", ""], ["Swartz", "Jordan", ""], ["McKelvey", "T. Greg", ""], ["Dai", "Hui", ""], ["Yang", "Yi", ""], ["Sontag", "David", ""]]}, {"id": "2106.02559", "submitter": "Rowan Hall Maudslay", "authors": "Rowan Hall Maudslay, Ryan Cotterell", "title": "Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysing whether neural language models encode linguistic information has\nbecome popular in NLP. One method of doing so, which is frequently cited to\nsupport the claim that models like BERT encode syntax, is called probing;\nprobes are small supervised models trained to extract linguistic information\nfrom another model's output. If a probe is able to predict a particular\nstructure, it is argued that the model whose output it is trained on must have\nimplicitly learnt to encode it. However, drawing a generalisation about a\nmodel's linguistic knowledge about a specific phenomena based on what a probe\nis able to learn may be problematic: in this work, we show that semantic cues\nin training data means that syntactic probes do not properly isolate syntax. We\ngenerate a new corpus of semantically nonsensical but syntactically well-formed\nJabberwocky sentences, which we use to evaluate two probes trained on normal\ndata. We train the probes on several popular language models (BERT, GPT, and\nRoBERTa), and find that in all settings they perform worse when evaluated on\nthese data, for one probe by an average of 15.4 UUAS points absolute. Although\nin most cases they still outperform the baselines, their lead is reduced\nsubstantially, e.g. by 53% in the case of BERT for one probe. This begs the\nquestion: what empirical scores constitute knowing syntax?\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:46:39 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Maudslay", "Rowan Hall", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.02561", "submitter": "Ruixiang Cui", "authors": "Ruixiang Cui, Daniel Hershcovich", "title": "Great Service! Fine-grained Parsing of Implicit Arguments", "comments": "Accepted to IWPT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Broad-coverage meaning representations in NLP mostly focus on explicitly\nexpressed content. More importantly, the scarcity of datasets annotating\ndiverse implicit roles limits empirical studies into their linguistic nuances.\nFor example, in the web review \"Great service!\", the provider and consumer are\nimplicit arguments of different types. We examine an annotated corpus of\nfine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully\nre-annotating it, resolving several inconsistencies. Subsequently, we present\nthe first transition-based neural parser that can handle implicit arguments\ndynamically, and experiment with two different transition systems on the\nimproved dataset. We find that certain types of implicit arguments are more\ndifficult to parse than others and that the simpler system is more accurate in\nrecovering implicit arguments, despite having a lower overall parsing score,\nattesting current reasoning limitations of NLP models. This work will\nfacilitate a better understanding of implicit and underspecified language, by\nincorporating it holistically into meaning representations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:50:35 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:42:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Cui", "Ruixiang", ""], ["Hershcovich", "Daniel", ""]]}, {"id": "2106.02562", "submitter": "Zhaoxin Luo", "authors": "Zhaoxin Luo and Michael Zhu", "title": "Recurrent Neural Networks with Mixed Hierarchical Structures for Natural\n  Language Processing", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical structures exist in both linguistics and Natural Language\nProcessing (NLP) tasks. How to design RNNs to learn hierarchical\nrepresentations of natural languages remains a long-standing challenge. In this\npaper, we define two different types of boundaries referred to as static and\ndynamic boundaries, respectively, and then use them to construct a multi-layer\nhierarchical structure for document classification tasks. In particular, we\nfocus on a three-layer hierarchical structure with static word- and sentence-\nlayers and a dynamic phrase-layer. LSTM cells and two boundary detectors are\nused to implement the proposed structure, and the resulting network is called\nthe {\\em Recurrent Neural Network with Mixed Hierarchical Structures}\n(MHS-RNN). We further add three layers of attention mechanisms to the MHS-RNN\nmodel. Incorporating attention mechanisms allows our model to use more\nimportant content to construct document representation and enhance its\nperformance on document classification tasks. Experiments on five different\ndatasets show that the proposed architecture outperforms previous methods on\nall the five tasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:50:42 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Luo", "Zhaoxin", ""], ["Zhu", "Michael", ""]]}, {"id": "2106.02569", "submitter": "Chao Jiang", "authors": "Wuwei Lan, Chao Jiang, Wei Xu", "title": "Neural semi-Markov CRF for Monolingual Word Alignment", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monolingual word alignment is important for studying fine-grained editing\noperations (i.e., deletion, addition, and substitution) in text-to-text\ngeneration tasks, such as paraphrase generation, text simplification,\nneutralizing biased language, etc. In this paper, we present a novel neural\nsemi-Markov CRF alignment model, which unifies word and phrase alignments\nthrough variable-length spans. We also create a new benchmark with human\nannotations that cover four different text genres to evaluate monolingual word\nalignment models in more realistic settings. Experimental results show that our\nproposed model outperforms all previous approaches for monolingual word\nalignment as well as a competitive QA-based baseline, which was previously only\napplied to bilingual data. Our model demonstrates good generalizability to\nthree out-of-domain datasets and shows great utility in two downstream\napplications: automatic text simplification and sentence pair classification\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:04:00 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 20:21:51 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lan", "Wuwei", ""], ["Jiang", "Chao", ""], ["Xu", "Wei", ""]]}, {"id": "2106.02596", "submitter": "Svetlana Kiritchenko", "authors": "Kathleen C. Fraser, Isar Nejadgholi, Svetlana Kiritchenko", "title": "Understanding and Countering Stereotypes: A Computational Approach to\n  the Stereotype Content Model", "comments": "In Proceedings of the Joint Conference of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International\n  Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereotypical language expresses widely-held beliefs about different social\ncategories. Many stereotypes are overtly negative, while others may appear\npositive on the surface, but still lead to negative consequences. In this work,\nwe present a computational approach to interpreting stereotypes in text through\nthe Stereotype Content Model (SCM), a comprehensive causal theory from social\npsychology. The SCM proposes that stereotypes can be understood along two\nprimary dimensions: warmth and competence. We present a method for defining\nwarmth and competence axes in semantic embedding space, and show that the four\nquadrants defined by this subspace accurately represent the warmth and\ncompetence concepts, according to annotated lexicons. We then apply our\ncomputational SCM model to textual stereotype data and show that it compares\nfavourably with survey-based studies in the psychological literature.\nFurthermore, we explore various strategies to counter stereotypical beliefs\nwith anti-stereotypes. It is known that countering stereotypes with\nanti-stereotypical examples is one of the most effective ways to reduce biased\nthinking, yet the problem of generating anti-stereotypes has not been\npreviously studied. Thus, a better understanding of how to generate realistic\nand effective anti-stereotypes can contribute to addressing pressing societal\nconcerns of stereotyping, prejudice, and discrimination.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:53:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Fraser", "Kathleen C.", ""], ["Nejadgholi", "Isar", ""], ["Kiritchenko", "Svetlana", ""]]}, {"id": "2106.02607", "submitter": "Anusua Trivedi", "authors": "Anusua Trivedi, Alyssa Suhm, Prathamesh Mahankal, Subhiksha\n  Mukuntharaj, Meghana D. Parab, Malvika Mohan, Meredith Berger, Arathi\n  Sethumadhavan, Ashish Jaiman, Rahul Dodhia", "title": "Defending Democracy: Using Deep Learning to Identify and Prevent\n  Misinformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise in online misinformation in recent years threatens democracies by\ndistorting authentic public discourse and causing confusion, fear, and even, in\nextreme cases, violence. There is a need to understand the spread of false\ncontent through online networks for developing interventions that disrupt\nmisinformation before it achieves virality. Using a Deep Bidirectional\nTransformer for Language Understanding (BERT) and propagation graphs, this\nstudy classifies and visualizes the spread of misinformation on a social media\nnetwork using publicly available Twitter data. The results confirm prior\nresearch around user clusters and the virality of false content while improving\nthe precision of deep learning models for misinformation detection. The study\nfurther demonstrates the suitability of BERT for providing a scalable model for\nfalse information detection, which can contribute to the development of more\ntimely and accurate interventions to slow the spread of misinformation in\nonline environments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:34:54 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Trivedi", "Anusua", ""], ["Suhm", "Alyssa", ""], ["Mahankal", "Prathamesh", ""], ["Mukuntharaj", "Subhiksha", ""], ["Parab", "Meghana D.", ""], ["Mohan", "Malvika", ""], ["Berger", "Meredith", ""], ["Sethumadhavan", "Arathi", ""], ["Jaiman", "Ashish", ""], ["Dodhia", "Rahul", ""]]}, {"id": "2106.02636", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park,\n  Jize Cao, Ali Farhadi, Yejin Choi", "title": "MERLOT: Multimodal Neural Script Knowledge Models", "comments": "project page at https://rowanzellers.com/merlot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans, we understand events in the visual world contextually, performing\nmultimodal reasoning across time to make inferences about the past, present,\nand future. We introduce MERLOT, a model that learns multimodal script\nknowledge by watching millions of YouTube videos with transcribed speech -- in\nan entirely label-free, self-supervised manner. By pretraining with a mix of\nboth frame-level (spatial) and video-level (temporal) objectives, our model not\nonly learns to match images to temporally corresponding words, but also to\ncontextualize what is happening globally over time. As a result, MERLOT\nexhibits strong out-of-the-box representations of temporal commonsense, and\nachieves state-of-the-art performance on 12 different video QA datasets when\nfinetuned. It also transfers well to the world of static images, allowing\nmodels to reason about the dynamic context behind visual scenes. On Visual\nCommonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,\noutperforming state-of-the-art models of similar size by over 3%, even those\nthat make heavy use of auxiliary supervised data (like object bounding boxes).\n  Ablation analyses demonstrate the complementary importance of: 1) training on\nvideos versus static images; 2) scaling the magnitude and diversity of the\npretraining video corpus; and 3) using diverse objectives that encourage\nfull-stack multimodal reasoning, from the recognition to cognition level.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:57:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:28:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zellers", "Rowan", ""], ["Lu", "Ximing", ""], ["Hessel", "Jack", ""], ["Yu", "Youngjae", ""], ["Park", "Jae Sung", ""], ["Cao", "Jize", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "2106.02658", "submitter": "Patrick Huber", "authors": "Patrick Huber, Wen Xiao and Giuseppe Carenini", "title": "W-RST: Towards a Weighted RST-style Discourse Framework", "comments": "9 pages, Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming for a better integration of data-driven and linguistically-inspired\napproaches, we explore whether RST Nuclearity, assigning a binary assessment of\nimportance between text segments, can be replaced by automatically generated,\nreal-valued scores, in what we call a Weighted-RST framework. In particular, we\nfind that weighted discourse trees from auxiliary tasks can benefit key NLP\ndownstream applications, compared to nuclearity-centered approaches. We further\nshow that real-valued importance distributions partially and interestingly\nalign with the assessment and uncertainty of human annotators.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 18:12:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huber", "Patrick", ""], ["Xiao", "Wen", ""], ["Carenini", "Giuseppe", ""]]}, {"id": "2106.02668", "submitter": "Jesse Mu", "authors": "Jesse Mu, Noah Goodman", "title": "Emergent Communication of Generalizations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build agents that can collaborate effectively with others, recent research\nhas trained artificial agents to communicate with each other in Lewis-style\nreferential games. However, this often leads to successful but uninterpretable\ncommunication. We argue that this is due to the game objective: communicating\nabout a single object in a shared visual context is prone to overfitting and\ndoes not encourage language useful beyond concrete reference. In contrast,\nhuman language conveys a rich variety of abstract ideas. To promote such\nskills, we propose games that require communicating generalizations over sets\nof objects representing abstract visual concepts, optionally with separate\ncontexts for each agent. We find that these games greatly improve systematicity\nand interpretability of the learned languages, according to several metrics in\nthe literature. Finally, we propose a method for identifying logical operations\nembedded in the emergent languages by learning an approximate compositional\nreconstruction of the language.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:02:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mu", "Jesse", ""], ["Goodman", "Noah", ""]]}, {"id": "2106.02679", "submitter": "Joel Lamy-Poirier", "authors": "Joel Lamy-Poirier", "title": "Layered gradient accumulation and modular pipeline parallelism: fast and\n  efficient training of large language models", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The advent of the transformer has sparked a quick growth in the size of\nlanguage models, far outpacing hardware improvements. (Dense) transformers are\nexpected to reach the trillion-parameter scale in the near future, for which\ntraining requires thousands or even tens of thousands of GPUs. We investigate\nthe challenges of training at this scale and beyond on commercially available\nhardware. In particular, we analyse the shortest possible training time for\ndifferent configurations of distributed training, leveraging empirical scaling\nlaws for language models to estimate the optimal (critical) batch size.\nContrary to popular belief, we find no evidence for a memory wall, and instead\nargue that the real limitation -- other than the cost -- lies in the training\nduration.\n  In addition to this analysis, we introduce two new methods, \\textit{layered\ngradient accumulation} and \\textit{modular pipeline parallelism}, which\ntogether cut the shortest training time by half. The methods also reduce data\nmovement, lowering the network requirement to a point where a fast InfiniBand\nconnection is not necessary. This increased network efficiency also improve on\nthe methods introduced with the ZeRO optimizer, reducing the memory usage to a\ntiny fraction of the available GPU memory.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:21:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lamy-Poirier", "Joel", ""]]}, {"id": "2106.02692", "submitter": "David Gros", "authors": "David Gros, Yu Li, Zhou Yu", "title": "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting\n  User Questions About Human or Non-Human Identity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humans are increasingly interacting with machines through language, sometimes\nin contexts where the user may not know they are talking to a machine (like\nover the phone or a text chatbot). We aim to understand how system designers\nand researchers might allow their systems to confirm its non-human identity. We\ncollect over 2,500 phrasings related to the intent of ``Are you a robot?\". This\nis paired with over 2,500 adversarially selected utterances where only\nconfirming the system is non-human would be insufficient or disfluent. We\ncompare classifiers to recognize the intent and discuss the precision/recall\nand model complexity tradeoffs. Such classifiers could be integrated into\ndialog systems to avoid undesired deception. We then explore how both a\ngenerative research model (Blender) as well as two deployed systems (Amazon\nAlexa, Google Assistant) handle this intent, finding that systems often fail to\nconfirm their non-human identity. Finally, we try to understand what a good\nresponse to the intent would be, and conduct a user study to compare the\nimportant aspects when responding to this intent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:04:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gros", "David", ""], ["Li", "Yu", ""], ["Yu", "Zhou", ""]]}, {"id": "2106.02725", "submitter": "Siyi Liu", "authors": "Siyi Liu, Sihao Chen, Xander Uyttendaele, Dan Roth", "title": "MultiOpEd: A Corpus of Multi-Perspective News Editorials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose MultiOpEd, an open-domain news editorial corpus that supports\nvarious tasks pertaining to the argumentation structure in news editorials,\nfocusing on automatic perspective discovery. News editorial is a genre of\npersuasive text, where the argumentation structure is usually implicit.\nHowever, the arguments presented in an editorial typically center around a\nconcise, focused thesis, which we refer to as their perspective. MultiOpEd aims\nat supporting the study of multiple tasks relevant to automatic perspective\ndiscovery, where a system is expected to produce a single-sentence thesis\nstatement summarizing the arguments presented. We argue that identifying and\nabstracting such natural language perspectives from editorials is a crucial\nstep toward studying the implicit argumentation structure in news editorials.\nWe first discuss the challenges and define a few conceptual tasks towards our\ngoal. To demonstrate the utility of MultiOpEd and the induced tasks, we study\nthe problem of perspective summarization in a multi-task learning setting, as a\ncase study. We show that, with the induced tasks as auxiliary tasks, we can\nimprove the quality of the perspective summary generated. We hope that\nMultiOpEd will be a useful resource for future studies on argumentation in the\nnews editorial domain.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:23:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Siyi", ""], ["Chen", "Sihao", ""], ["Uyttendaele", "Xander", ""], ["Roth", "Dan", ""]]}, {"id": "2106.02736", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer, Taylor Berg-Kirkpatrick", "title": "Exposing the Implicit Energy Networks behind Masked Language Models via\n  Metropolis--Hastings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent work has shown that scores from models trained by the ubiquitous\nmasked language modeling (MLM) objective effectively discriminate probable and\nimprobable sequences, it is still an open question if these MLMs specify a\nprincipled probability distribution over the space of possible sequences. In\nthis paper, we interpret MLMs as energy-based sequence models and propose two\nenergy parametrizations derivable from the trained MLMs. In order to draw\nsamples correctly from these models, we develop a tractable \\emph{sampling}\nscheme based on the Metropolis--Hastings Monte Carlo algorithm. In our\napproach, samples are proposed from the same masked conditionals used for\ntraining the masked language models, and they are accepted or rejected based on\ntheir energy values according to the target distribution. We validate the\neffectiveness of the proposed parametrizations by exploring the quality of\nsamples drawn from these energy-based models on the conditional generation task\nof machine translation. We theoretically and empirically justify our sampling\nalgorithm by showing that the masked conditionals on their own do not yield a\nMarkov chain whose stationary distribution is that of our target distribution,\nand our approach generates higher quality samples than other recently proposed\nundirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,\n2019).\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:04:30 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2106.02787", "submitter": "Zhaojiang Lin", "authors": "Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Peng Xu, Feijun\n  Jiang, Yuxiang Hu, Chen Shi, Pascale Fung", "title": "BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue\n  Modeling", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialogue (ToD) benchmarks provide an important avenue to\nmeasure progress and develop better conversational agents. However, existing\ndatasets for end-to-end ToD modeling are limited to a single language,\nhindering the development of robust end-to-end ToD systems for multilingual\ncountries and regions. Here we introduce BiToD, the first bilingual\nmulti-domain dataset for end-to-end task-oriented dialogue modeling. BiToD\ncontains over 7k multi-domain dialogues (144k utterances) with a large and\nrealistic bilingual knowledge base. It serves as an effective benchmark for\nevaluating bilingual ToD systems and cross-lingual transfer learning\napproaches. We provide state-of-the-art baselines under three evaluation\nsettings (monolingual, bilingual, and cross-lingual). The analysis of our\nbaselines in different settings highlights 1) the effectiveness of training a\nbilingual ToD system compared to two independent monolingual ToD systems, and\n2) the potential of leveraging a bilingual knowledge base and cross-lingual\ntransfer learning to improve the system performance under low resource\ncondition.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 03:38:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lin", "Zhaojiang", ""], ["Madotto", "Andrea", ""], ["Winata", "Genta Indra", ""], ["Xu", "Peng", ""], ["Jiang", "Feijun", ""], ["Hu", "Yuxiang", ""], ["Shi", "Chen", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.02792", "submitter": "Chenghao Yang", "authors": "Chenghao Yang, Yudong Zhang, Smaranda Muresan", "title": "Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related\n  Domains", "comments": "ACL 2021 short paper. Code is available at\n  https://github.com/yangalan123/WM-SRA (under construction)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media has become a valuable resource for the study of suicidal\nideation and the assessment of suicide risk. Among social media platforms,\nReddit has emerged as the most promising one due to its anonymity and its focus\non topic-based communities (subreddits) that can be indicative of someone's\nstate of mind or interest regarding mental health disorders such as\nr/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on\nsuicide risk assessment has been the small amount of labeled data. We propose\nan empirical investigation into several classes of weakly-supervised\napproaches, and show that using pseudo-labeling based on related issues around\nmental health (e.g., anxiety, depression) helps improve model performance for\nsuicide risk assessment.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 04:31:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 00:30:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yang", "Chenghao", ""], ["Zhang", "Yudong", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2106.02821", "submitter": "Jing Qian", "authors": "Jing Qian, Hong Wang, Mai ElSherief, Xifeng Yan", "title": "Lifelong Learning of Hate Speech Classification on Social Media", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing work on automated hate speech classification assumes that the\ndataset is fixed and the classes are pre-defined. However, the amount of data\nin social media increases every day, and the hot topics changes rapidly,\nrequiring the classifiers to be able to continuously adapt to new data without\nforgetting the previously learned knowledge. This ability, referred to as\nlifelong learning, is crucial for the real-word application of hate speech\nclassifiers in social media. In this work, we propose lifelong learning of hate\nspeech classification on social media. To alleviate catastrophic forgetting, we\npropose to use Variational Representation Learning (VRL) along with a memory\nmodule based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural\nNetwork). Experimentally, we show that combining variational representation\nlearning and the LB-SOINN memory module achieves better performance than the\ncommonly-used lifelong learning techniques.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 07:14:34 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Qian", "Jing", ""], ["Wang", "Hong", ""], ["ElSherief", "Mai", ""], ["Yan", "Xifeng", ""]]}, {"id": "2106.02833", "submitter": "Varun Gangal", "authors": "Varun Gangal, Harsh Jhamtani, Eduard Hovy, Taylor Berg-Kirkpatrick", "title": "Improving Automated Evaluation of Open Domain Dialog via Diverse\n  Reference Augmentation", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple different responses are often plausible for a given open domain\ndialog context. Prior work has shown the importance of having multiple valid\nreference responses for meaningful and robust automated evaluations. In such\ncases, common practice has been to collect more human written references.\nHowever, such collection can be expensive, time consuming, and not easily\nscalable. Instead, we propose a novel technique for automatically expanding a\nhuman generated reference to a set of candidate references. We fetch plausible\nreferences from knowledge sources, and adapt them so that they are more fluent\nin context of the dialog instance in question. More specifically, we use (1) a\ncommonsense knowledge base to elicit a large number of plausible reactions\ngiven the dialog history (2) relevant instances retrieved from dialog corpus,\nusing similar past as well as future contexts. We demonstrate that our\nautomatically expanded reference sets lead to large improvements in\ncorrelations of automated metrics with human ratings of system outputs for\nDailyDialog dataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 08:18:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gangal", "Varun", ""], ["Jhamtani", "Harsh", ""], ["Hovy", "Eduard", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2106.02834", "submitter": "Simran Khanuja", "authors": "Simran Khanuja, Melvin Johnson, Partha Talukdar", "title": "MergeDistill: Merging Pre-trained Language Models using Distillation", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained multilingual language models (LMs) have achieved state-of-the-art\nresults in cross-lingual transfer, but they often lead to an inequitable\nrepresentation of languages due to limited capacity, skewed pre-training data,\nand sub-optimal vocabularies. This has prompted the creation of an ever-growing\npre-trained model universe, where each model is trained on large amounts of\nlanguage or domain specific data with a carefully curated, linguistically\ninformed vocabulary. However, doing so brings us back full circle and prevents\none from leveraging the benefits of multilinguality. To address the gaps at\nboth ends of the spectrum, we propose MergeDistill, a framework to merge\npre-trained LMs in a way that can best leverage their assets with minimal\ndependencies, using task-agnostic knowledge distillation. We demonstrate the\napplicability of our framework in a practical setting by leveraging\npre-existing teacher LMs and training student LMs that perform competitively\nwith or even outperform teacher LMs trained on several orders of magnitude more\ndata and with a fixed model capacity. We also highlight the importance of\nteacher selection and its impact on student model performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 08:22:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Khanuja", "Simran", ""], ["Johnson", "Melvin", ""], ["Talukdar", "Partha", ""]]}, {"id": "2106.02902", "submitter": "Jonas Wallat", "authors": "Jonas Wallat, Jaspreet Singh, Avishek Anand", "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.09313", "journal-ref": null, "doi": null, "report-no": "BN-EXT-JRNL-02", "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\narticle, we probe BERT specifically to understand and measure the relational\nknowledge it captures in its parametric memory. While probing for linguistic\nunderstanding is commonly applied to all layers of BERT as well as fine-tuned\nmodels, this has not been done for factual knowledge. We utilize existing\nknowledge base completion tasks (LAMA) to probe every layer of pre-trained as\nwell as fine-tuned BERT models(ranking, question answering, NER). Our findings\nshow that knowledge is not just contained in BERT's final layers. Intermediate\nlayers contribute a significant amount (17-60%) to the total knowledge found.\nProbing intermediate layers also reveals how different types of knowledge\nemerge at varying rates. When BERT is fine-tuned, relational knowledge is\nforgotten. The extent of forgetting is impacted by the fine-tuning objective\nand the training data. We found that ranking models forget the least and retain\nmore knowledge in their final layer compared to masked language modeling and\nquestion-answering. However, masked language modeling performed the best at\nacquiring new knowledge from the training data. When it comes to learning\nfacts, we found that capacity and fact density are key factors. We hope this\ninitial work will spur further research into understanding the parametric\nmemory of language models and the effect of training objectives on factual\nknowledge. The code to repeat the experiments is publicly available on GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 14:23:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wallat", "Jonas", ""], ["Singh", "Jaspreet", ""], ["Anand", "Avishek", ""]]}, {"id": "2106.02954", "submitter": "Avi Caciularu", "authors": "Avi Caciularu, Ido Dagan, Jacob Goldberger", "title": "Denoising Word Embeddings by Averaging in a Shared Space", "comments": "Accepted to *SEM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new approach for smoothing and improving the quality of word\nembeddings. We consider a method of fusing word embeddings that were trained on\nthe same corpus but with different initializations. We project all the models\nto a shared vector space using an efficient implementation of the Generalized\nProcrustes Analysis (GPA) procedure, previously used in multilingual word\ntranslation. Our word representation demonstrates consistent improvements over\nthe raw models as well as their simplistic average, on a range of tasks. As the\nnew representations are more stable and reliable, there is a noticeable\nimprovement in rare word evaluations.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 19:49:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Caciularu", "Avi", ""], ["Dagan", "Ido", ""], ["Goldberger", "Jacob", ""]]}, {"id": "2106.02960", "submitter": "Yingjun Du", "authors": "Yingjun Du, Nithin Holla, Xiantong Zhen, Cees G.M. Snoek, Ekaterina\n  Shutova", "title": "Meta-Learning with Variational Semantic Memory for Word Sense\n  Disambiguation", "comments": "15 pages, 5 figures", "journal-ref": "ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A critical challenge faced by supervised word sense disambiguation (WSD) is\nthe lack of large annotated datasets with sufficient coverage of words in their\ndiversity of senses. This inspired recent research on few-shot WSD using\nmeta-learning. While such work has successfully applied meta-learning to learn\nnew word senses from very few examples, its performance still lags behind its\nfully supervised counterpart. Aiming to further close this gap, we propose a\nmodel of semantic memory for WSD in a meta-learning setting. Semantic memory\nencapsulates prior experiences seen throughout the lifetime of the model, which\naids better generalization in limited data settings. Our model is based on\nhierarchical variational inference and incorporates an adaptive memory update\nrule via a hypernetwork. We show our model advances the state of the art in\nfew-shot WSD, supports effective learning in extremely data scarce (e.g.\none-shot) scenarios and produces meaning prototypes that capture similar senses\nof distinct words.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 20:40:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Du", "Yingjun", ""], ["Holla", "Nithin", ""], ["Zhen", "Xiantong", ""], ["Snoek", "Cees G. M.", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2106.02972", "submitter": "Prasoon Goyal", "authors": "Prasoon Goyal, Raymond J. Mooney, Scott Niekum", "title": "Zero-shot Task Adaptation using Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning and instruction-following are two common approaches to\ncommunicate a user's intent to a learning agent. However, as the complexity of\ntasks grows, it could be beneficial to use both demonstrations and language to\ncommunicate with an agent. In this work, we propose a novel setting where an\nagent is given both a demonstration and a description, and must combine\ninformation from both the modalities. Specifically, given a demonstration for a\ntask (the source task), and a natural language description of the differences\nbetween the demonstrated task and a related but different task (the target\ntask), our goal is to train an agent to complete the target task in a zero-shot\nsetting, that is, without any demonstrations for the target task. To this end,\nwe introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a\nsource demonstration and a linguistic description of how the target task\ndiffers, learns to output a reward / value function that accurately describes\nthe target task. Our experiments show that on a diverse set of adaptations, our\napproach is able to complete more than 95% of target tasks when using\ntemplate-based descriptions, and more than 70% when using free-form natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 21:39:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Goyal", "Prasoon", ""], ["Mooney", "Raymond J.", ""], ["Niekum", "Scott", ""]]}, {"id": "2106.02974", "submitter": "Qingkai Zeng", "authors": "Qingkai Zeng and Jinfeng Lin and Wenhao Yu and Jane Cleland-Huang and\n  Meng Jiang", "title": "Enhancing Taxonomy Completion with Concept Generation via Fusing\n  Relational Representations", "comments": null, "journal-ref": null, "doi": "10.1145/3447548.3467308", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic construction of a taxonomy supports many applications in\ne-commerce, web search, and question answering. Existing taxonomy expansion or\ncompletion methods assume that new concepts have been accurately extracted and\ntheir embedding vectors learned from the text corpus. However, one critical and\nfundamental challenge in fixing the incompleteness of taxonomies is the\nincompleteness of the extracted concepts, especially for those whose names have\nmultiple words and consequently low frequency in the corpus. To resolve the\nlimitations of extraction-based methods, we propose GenTaxo to enhance taxonomy\ncompletion by identifying positions in existing taxonomies that need new\nconcepts and then generating appropriate concept names. Instead of relying on\nthe corpus for concept embeddings, GenTaxo learns the contextual embeddings\nfrom their surrounding graph-based and language-based relational information,\nand leverages the corpus for pre-training a concept name generator.\nExperimental results demonstrate that GenTaxo improves the completeness of\ntaxonomies over existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 21:50:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zeng", "Qingkai", ""], ["Lin", "Jinfeng", ""], ["Yu", "Wenhao", ""], ["Cleland-Huang", "Jane", ""], ["Jiang", "Meng", ""]]}, {"id": "2106.03020", "submitter": "Johannes Mario Meissner", "authors": "Johannes Mario Meissner, Napat Thumwanit, Saku Sugawara, Akiko Aizawa", "title": "Embracing Ambiguity: Shifting the Training Target of NLI Models", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI) datasets contain examples with highly\nambiguous labels. While many research works do not pay much attention to this\nfact, several recent efforts have been made to acknowledge and embrace the\nexistence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore\nthe option of training directly on the estimated label distribution of the\nannotators in the NLI task, using a learning loss based on this ambiguity\ndistribution instead of the gold-labels. We prepare AmbiNLI, a trial dataset\nobtained from readily available sources, and show it is possible to reduce\nChaosNLI divergence scores when finetuning on this data, a promising first step\ntowards learning how to capture linguistic ambiguity. Additionally, we show\nthat training on the same amount of data but targeting the ambiguity\ndistribution instead of gold-labels can result in models that achieve higher\nperformance and learn better representations for downstream tasks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 03:18:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Meissner", "Johannes Mario", ""], ["Thumwanit", "Napat", ""], ["Sugawara", "Saku", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2106.03031", "submitter": "Masato Mita", "authors": "Masato Mita and Hitomi Yanaka", "title": "Do Grammatical Error Correction Models Realize Grammatical\n  Generalization?", "comments": "ACL 2021 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increased interest in data generation approaches to\ngrammatical error correction (GEC) using pseudo data. However, these approaches\nsuffer from several issues that make them inconvenient for real-world\ndeployment including a demand for large amounts of training data. On the other\nhand, some errors based on grammatical rules may not necessarily require a\nlarge amount of data if GEC models can realize grammatical generalization. This\nstudy explores to what extent GEC models generalize grammatical knowledge\nrequired for correcting errors. We introduce an analysis method using synthetic\nand real GEC datasets with controlled vocabularies to evaluate whether models\ncan generalize to unseen errors. We found that a current standard\nTransformer-based GEC model fails to realize grammatical generalization even in\nsimple settings with limited vocabulary and syntax, suggesting that it lacks\nthe generalization ability required to correct errors from provided training\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 04:59:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mita", "Masato", ""], ["Yanaka", "Hitomi", ""]]}, {"id": "2106.03036", "submitter": "Ritu Gala", "authors": "Ritu Gala, Revathi Vijayaraghavan, Valmik Nikam, Arvind Kiwelekar", "title": "Real-Time Cognitive Evaluation of Online Learners through Automatically\n  Generated Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the increased adoption of E-learning platforms, keeping online learners\nengaged throughout a lesson is challenging. One approach to tackle this\nchallenge is to probe learn-ers periodically by asking questions. The paper\npresents an approach to generate questions from a given video lecture\nautomatically. The generated questions are aimed to evaluate learners'\nlower-level cognitive abilities. The approach automatically extracts text from\nvideo lectures to generates wh-kinds of questions. When learners respond with\nan answer, the proposed approach further evaluates the response and provides\nfeedback. Besides enhancing learner's engagement, this approach's main benefits\nare that it frees instructors from design-ing questions to check the\ncomprehension of a topic. Thus, instructors can spend this time productively on\nother activities.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 05:45:56 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gala", "Ritu", ""], ["Vijayaraghavan", "Revathi", ""], ["Nikam", "Valmik", ""], ["Kiwelekar", "Arvind", ""]]}, {"id": "2106.03044", "submitter": "Jiayi Liu", "authors": "Wei Wei, Jiayi Liu, Xianling Mao, Guibing Guo, Feida Zhu, Pan Zhou,\n  Yuchong Hu", "title": "Emotion-aware Chat Machine: Automatic Emotional Response Generation for\n  Human-like Emotional Interaction", "comments": "Accepted at CIKM 2019. arXiv admin note: substantial text overlap\n  with arXiv:2011.07432", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consistency of a response to a given post at semantic-level and\nemotional-level is essential for a dialogue system to deliver human-like\ninteractions. However, this challenge is not well addressed in the literature,\nsince most of the approaches neglect the emotional information conveyed by a\npost while generating responses. This article addresses this problem by\nproposing a unifed end-to-end neural architecture, which is capable of\nsimultaneously encoding the semantics and the emotions in a post for generating\nmore intelligent responses with appropriately expressed emotions. Extensive\nexperiments on real-world data demonstrate that the proposed method outperforms\nthe state-of-the-art methods in terms of both content coherence and emotion\nappropriateness.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 06:26:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wei", "Wei", ""], ["Liu", "Jiayi", ""], ["Mao", "Xianling", ""], ["Guo", "Guibing", ""], ["Zhu", "Feida", ""], ["Zhou", "Pan", ""], ["Hu", "Yuchong", ""]]}, {"id": "2106.03046", "submitter": "Fuli Feng", "authors": "Fuli Feng, Jizhi Zhang, Xiangnan He, Hanwang Zhang, Tat-Seng Chua", "title": "Empowering Language Understanding with Counterfactual Reasoning", "comments": "Accepted by Findings of ACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present language understanding methods have demonstrated extraordinary\nability of recognizing patterns in texts via machine learning. However,\nexisting methods indiscriminately use the recognized patterns in the testing\nphase that is inherently different from us humans who have counterfactual\nthinking, e.g., to scrutinize for the hard testing samples. Inspired by this,\nwe propose a Counterfactual Reasoning Model, which mimics the counterfactual\nthinking by learning from few counterfactual samples. In particular, we devise\na generation module to generate representative counterfactual samples for each\nfactual sample, and a retrospective module to retrospect the model prediction\nby comparing the counterfactual and factual samples. Extensive experiments on\nsentiment analysis (SA) and natural language inference (NLI) validate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 06:36:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Feng", "Fuli", ""], ["Zhang", "Jizhi", ""], ["He", "Xiangnan", ""], ["Zhang", "Hanwang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2106.03048", "submitter": "Nadav Borenstein", "authors": "Chen Shani, Nadav Borenstein, Dafna Shahaf", "title": "How Did This Get Funded?! Automatically Identifying Quirky Scientific\n  Achievements", "comments": "To be published in the main conference of ACL-IJCNLP2021. Code and\n  dataset can be found here: https://github.com/nadavborenstein/Iggy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humor is an important social phenomenon, serving complex social and\npsychological functions. However, despite being studied for millennia humor is\ncomputationally not well understood, often considered an AI-complete problem.\nIn this work, we introduce a novel setting in humor mining: automatically\ndetecting funny and unusual scientific papers. We are inspired by the Ig Nobel\nprize, a satirical prize awarded annually to celebrate funny scientific\nachievements (example past winner: \"Are cows more likely to lie down the longer\nthey stand?\"). This challenging task has unique characteristics that make it\nparticularly suitable for automatic learning. We construct a dataset containing\nthousands of funny papers and use it to learn classifiers, combining findings\nfrom psychology and linguistics with recent advances in NLP. We use our models\nto identify potentially funny papers in a large dataset of over 630,000\narticles. The results demonstrate the potential of our methods, and more\nbroadly the utility of integrating state-of-the-art NLP methods with insights\nfrom more traditional disciplines.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 06:54:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shani", "Chen", ""], ["Borenstein", "Nadav", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2106.03065", "submitter": "Chen Henry Wu", "authors": "Chen Henry Wu, Yinhe Zheng, Yida Wang, Zhenyu Yang, Minlie Huang", "title": "Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose to combine pretrained language models with the\nmodular dialogue paradigm for open-domain dialogue modeling. Our method,\nsemantic-enhanced finetuning, instantiates conversation understanding,\nplanning, and response generation as a language model finetuning task. At\ninference, we disentangle semantic and token variations by specifying sampling\nmethods and constraints for each module separately. For training and\nevaluation, we present X-Weibo, a Chinese multi-turn open-domain dialogue\ndataset with automatic annotation for emotions, DAs, and topical words.\nExperiments show that semantic-enhanced finetuning outperforms strong baselines\non non-semantic and semantic metrics, improves the human-evaluated relevance,\ncoherence, and informativeness, and exhibits considerable controllability over\nsemantic variables.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 09:03:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wu", "Chen Henry", ""], ["Zheng", "Yinhe", ""], ["Wang", "Yida", ""], ["Yang", "Zhenyu", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.03084", "submitter": "Jinpeng Zhang", "authors": "Jinpeng Zhang, Baijun Ji, Nini Xiao, Xiangyu Duan, Min Zhang, Yangbin\n  Shi, Weihua Luo", "title": "Combining Static Word Embeddings and Contextual Representations for\n  Bilingual Lexicon Induction", "comments": "Accepted to Findings of ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bilingual Lexicon Induction (BLI) aims to map words in one language to their\ntranslations in another, and is typically through learning linear projections\nto align monolingual word representation spaces. Two classes of word\nrepresentations have been explored for BLI: static word embeddings and\ncontextual representations, but there is no studies to combine both. In this\npaper, we propose a simple yet effective mechanism to combine the static word\nembeddings and the contextual representations to utilize the advantages of both\nparadigms. We test the combination mechanism on various language pairs under\nthe supervised and unsupervised BLI benchmark settings. Experiments show that\nour mechanism consistently improves performances over robust BLI baselines on\nall language pairs by averagely improving 3.2 points in the supervised setting,\nand 3.1 points in the unsupervised setting.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:31:02 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 06:48:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhang", "Jinpeng", ""], ["Ji", "Baijun", ""], ["Xiao", "Nini", ""], ["Duan", "Xiangyu", ""], ["Zhang", "Min", ""], ["Shi", "Yangbin", ""], ["Luo", "Weihua", ""]]}, {"id": "2106.03103", "submitter": "Ximing Zhang", "authors": "Ximing Zhang, Qian-Wen Zhang, Zhao Yan, Ruifang Liu and Yunbo Cao", "title": "Enhancing Label Correlation Feedback in Multi-Label Text Classification\n  via Multi-Task Learning", "comments": "Accepted by ACL 2021 (Finding)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In multi-label text classification (MLTC), each given document is associated\nwith a set of correlated labels. To capture label correlations, previous\nclassifier-chain and sequence-to-sequence models transform MLTC to a sequence\nprediction task. However, they tend to suffer from label order dependency,\nlabel combination over-fitting and error propagation problems. To address these\nproblems, we introduce a novel approach with multi-task learning to enhance\nlabel correlation feedback. We first utilize a joint embedding (JE) mechanism\nto obtain the text and label representation simultaneously. In MLTC task, a\ndocument-label cross attention (CA) mechanism is adopted to generate a more\ndiscriminative document representation. Furthermore, we propose two auxiliary\nlabel co-occurrence prediction tasks to enhance label correlation learning: 1)\nPairwise Label Co-occurrence Prediction (PLCP), and 2) Conditional Label\nCo-occurrence Prediction (CLCP). Experimental results on AAPD and RCV1-V2\ndatasets show that our method outperforms competitive baselines by a large\nmargin. We analyze low-frequency label performance, label dependency, label\ncombination diversity and coverage speed to show the effectiveness of our\nproposed method on label correlation learning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 12:26:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Ximing", ""], ["Zhang", "Qian-Wen", ""], ["Yan", "Zhao", ""], ["Liu", "Ruifang", ""], ["Cao", "Yunbo", ""]]}, {"id": "2106.03111", "submitter": "Sinan Kurtyigit", "authors": "Sinan Kurtyigit, Maike Park, Dominik Schlechtweg, Jonas Kuhn, Sabine\n  Schulte im Walde", "title": "Lexical Semantic Change Discovery", "comments": "ACL 2021, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there is a large amount of research in the field of Lexical Semantic\nChange Detection, only few approaches go beyond a standard benchmark evaluation\nof existing models. In this paper, we propose a shift of focus from change\ndetection to change discovery, i.e., discovering novel word senses over time\nfrom the full corpus vocabulary. By heavily fine-tuning a type-based and a\ntoken-based approach on recently published German data, we demonstrate that\nboth models can successfully be applied to discover new words undergoing\nmeaning change. Furthermore, we provide an almost fully automated framework for\nboth evaluation and discovery.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 13:02:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kurtyigit", "Sinan", ""], ["Park", "Maike", ""], ["Schlechtweg", "Dominik", ""], ["Kuhn", "Jonas", ""], ["Walde", "Sabine Schulte im", ""]]}, {"id": "2106.03143", "submitter": "Tatiana Likhomanenko", "authors": "Tatiana Likhomanenko, Qiantong Xu, Ronan Collobert, Gabriel Synnaeve,\n  Alex Rogozhnikov", "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:54:55 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 02:42:35 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Likhomanenko", "Tatiana", ""], ["Xu", "Qiantong", ""], ["Collobert", "Ronan", ""], ["Synnaeve", "Gabriel", ""], ["Rogozhnikov", "Alex", ""]]}, {"id": "2106.03151", "submitter": "Qianren Mao", "authors": "Qianren Mao, Xi Li, Hao Peng, Bang Liu, Shu Guo, Jianxin Li, Lihong\n  Wang, Philip S. Yu", "title": "Attend and Select: A Segment Attention based Selection Mechanism for\n  Microblog Hashtag Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic microblog hashtag generation can help us better and faster\nunderstand or process the critical content of microblog posts.\n  Conventional sequence-to-sequence generation methods can produce phrase-level\nhashtags and have achieved remarkable performance on this task. However, they\nare incapable of filtering out secondary information and not good at capturing\nthe discontinuous semantics among crucial tokens.\n  A hashtag is formed by tokens or phrases that may originate from various\nfragmentary segments of the original text.\n  In this work, we propose an end-to-end Transformer-based generation model\nwhich consists of three phases: encoding, segments-selection, and decoding. The\nmodel transforms discontinuous semantic segments from the source text into a\nsequence of hashtags.\n  Specifically, we introduce a novel Segments Selection Mechanism (SSM) for\nTransformer to obtain segmental representations tailored to phrase-level\nhashtag generation.\n  Besides, we introduce two large-scale hashtag generation datasets, which are\nnewly collected from Chinese Weibo and English Twitter.\n  Extensive evaluations on the two datasets reveal our approach's superiority\nwith significant improvements to extraction and generation baselines. The code\nand datasets are available at \\url{https://github.com/OpenSUM/HashtagGen}.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:13:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mao", "Qianren", ""], ["Li", "Xi", ""], ["Peng", "Hao", ""], ["Liu", "Bang", ""], ["Guo", "Shu", ""], ["Li", "Jianxin", ""], ["Wang", "Lihong", ""], ["Yu", "Philip S.", ""]]}, {"id": "2106.03153", "submitter": "Dongchan Min", "authors": "Dongchan Min, Dong Bok Lee, Eunho Yang, Sung Ju Hwang", "title": "Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With rapid progress in neural text-to-speech (TTS) models, personalized\nspeech generation is now in high demand for many applications. For practical\napplicability, a TTS model should generate high-quality speech with only a few\naudio samples from the given speaker, that are also short in length. However,\nexisting methods either require to fine-tune the model or achieve low\nadaptation quality without fine-tuning. In this work, we propose StyleSpeech, a\nnew TTS model which not only synthesizes high-quality speech but also\neffectively adapts to new speakers. Specifically, we propose Style-Adaptive\nLayer Normalization (SALN) which aligns gain and bias of the text input\naccording to the style extracted from a reference speech audio. With SALN, our\nmodel effectively synthesizes speech in the style of the target speaker even\nfrom single speech audio. Furthermore, to enhance StyleSpeech's adaptation to\nspeech from new speakers, we extend it to Meta-StyleSpeech by introducing two\ndiscriminators trained with style prototypes, and performing episodic training.\nThe experimental results show that our models generate high-quality speech\nwhich accurately follows the speaker's voice with single short-duration (1-3\nsec) speech audio, significantly outperforming baselines.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:34:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:50:24 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 16:57:10 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Min", "Dongchan", ""], ["Lee", "Dong Bok", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2106.03161", "submitter": "Lukas Pukelis Dr", "authors": "Jogil\\.e Ulinskait\\.e and Lukas Pukelis", "title": "Identifying Populist Paragraphs in Text: A machine-learning approach", "comments": "18 pages, 2 Figures, 3 Tables in main text, 2 tables in Annexes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstract: In this paper we present an approach to develop a\ntext-classification model which would be able to identify populist content in\ntext. The developed BERT-based model is largely successful in identifying\npopulist content in text and produces only a negligible amount of False\nNegatives, which makes it well-suited as a content analysis automation tool,\nwhich shortlists potentially relevant content for human validation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:58:34 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 11:29:43 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ulinskait\u0117", "Jogil\u0117", ""], ["Pukelis", "Lukas", ""]]}, {"id": "2106.03164", "submitter": "Ruidan He", "authors": "Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng,\n  Jia-Wei Low, Lidong Bing, Luo Si", "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language\n  Model Adaptation", "comments": "Accepted by ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapter-based tuning has recently arisen as an alternative to fine-tuning. It\nworks by adding light-weight adapter modules to a pretrained language model\n(PrLM) and only updating the parameters of adapter modules when learning on a\ndownstream task. As such, it adds only a few trainable parameters per new task,\nallowing a high degree of parameter sharing. Prior studies have shown that\nadapter-based tuning often achieves comparable results to fine-tuning. However,\nexisting work only focuses on the parameter-efficient aspect of adapter-based\ntuning while lacking further investigation on its effectiveness. In this paper,\nwe study the latter. We first show that adapter-based tuning better mitigates\nforgetting issues than fine-tuning since it yields representations with less\ndeviation from those generated by the initial PrLM. We then empirically compare\nthe two tuning methods on several downstream NLP tasks and settings. We\ndemonstrate that 1) adapter-based tuning outperforms fine-tuning on\nlow-resource and cross-lingual tasks; 2) it is more robust to overfitting and\nless sensitive to changes in learning rates.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 16:10:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["He", "Ruidan", ""], ["Liu", "Linlin", ""], ["Ye", "Hai", ""], ["Tan", "Qingyu", ""], ["Ding", "Bosheng", ""], ["Cheng", "Liying", ""], ["Low", "Jia-Wei", ""], ["Bing", "Lidong", ""], ["Si", "Luo", ""]]}, {"id": "2106.03170", "submitter": "Nadine Ruecker", "authors": "Nadine Ruecker, Andreas Maier", "title": "FlexParser -- the adaptive log file parser for continuous results in a\n  changing world", "comments": "18 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any modern system writes events into files, called log files. Those contain\ncrucial information which are subject to various analyses. Examples range from\ncybersecurity, intrusion detection over usage analyses to trouble shooting.\nBefore data analysis is possible, desired information needs to be extracted\nfirst out of the semi-structured log messages. State of the art event parsing\noften assumes static log events. However, any modern system is updated\nconsistently and with updates also log file structures can change. We call\nthose changes 'mutations' and study parsing performance for different mutation\ncases. Latest research discovers mutations using anomaly detection post mortem,\nhowever, does not cover actual continuous parsing. Thus, we propose a novel,\nflexible parser, called FlexParser which can extract desired values despite\ngradual changes in the log messages. It implies basic text preprocessing\nfollowed by a supervised Deep Learning method. We train a stateful LSTM on\nparsing one event per data set. Statefulness enforces the model to learn log\nmessage structures across several messages. Our model was tested on seven\ndifferent, publicly available log file data sets and various kinds of\nmutations. Exhibiting an average F1-Score of 0.98, it outperforms other Deep\nLearning methods as well as state-of-the-art unsupervised parsers.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 16:30:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ruecker", "Nadine", ""], ["Maier", "Andreas", ""]]}, {"id": "2106.03181", "submitter": "Katsuma Inoue", "authors": "Katsuma Inoue, Soh Ohara, Yasuo Kuniyoshi, and Kohei Nakajima", "title": "Transient Chaos in BERT", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG math.DS nlin.CD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language is an outcome of our complex and dynamic human-interactions and the\ntechnique of natural language processing (NLP) is hence built on human\nlinguistic activities. Bidirectional Encoder Representations from Transformers\n(BERT) has recently gained its popularity by establishing the state-of-the-art\nscores in several NLP benchmarks. A Lite BERT (ALBERT) is literally\ncharacterized as a lightweight version of BERT, in which the number of BERT\nparameters is reduced by repeatedly applying the same neural network called\nTransformer's encoder layer. By pre-training the parameters with a massive\namount of natural language data, ALBERT can convert input sentences into\nversatile high-dimensional vectors potentially capable of solving multiple NLP\ntasks. In that sense, ALBERT can be regarded as a well-designed\nhigh-dimensional dynamical system whose operator is the Transformer's encoder,\nand essential structures of human language are thus expected to be encapsulated\nin its dynamics. In this study, we investigated the embedded properties of\nALBERT to reveal how NLP tasks are effectively solved by exploiting its\ndynamics. We thereby aimed to explore the nature of human language from the\ndynamical expressions of the NLP model. Our short-term analysis clarified that\nthe pre-trained model stably yields trajectories with higher dimensionality,\nwhich would enhance the expressive capacity required for NLP tasks. Also, our\nlong-term analysis revealed that ALBERT intrinsically shows transient chaos, a\ntypical nonlinear phenomenon showing chaotic dynamics only in its transient,\nand the pre-trained ALBERT model tends to produce the chaotic trajectory for a\nsignificantly longer time period compared to a randomly-initialized one. Our\nresults imply that local chaoticity would contribute to improving NLP\nperformance, uncovering a novel aspect in the role of chaotic dynamics in human\nlanguage behaviors.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:02:29 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 02:25:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Inoue", "Katsuma", ""], ["Ohara", "Soh", ""], ["Kuniyoshi", "Yasuo", ""], ["Nakajima", "Kohei", ""]]}, {"id": "2106.03192", "submitter": "Murathan Kurfal{\\i}", "authors": "Murathan Kurfal{\\i} and Robert \\\"Ostling", "title": "Let's be explicit about that: Distant supervision for implicit discourse\n  relation classification via connective prediction", "comments": "To be presented at Unimplicit 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In implicit discourse relation classification, we want to predict the\nrelation between adjacent sentences in the absence of any overt discourse\nconnectives. This is challenging even for humans, leading to shortage of\nannotated data, a fact that makes the task even more difficult for supervised\nmachine learning approaches. In the current study, we perform implicit\ndiscourse relation classification without relying on any labeled implicit\nrelation. We sidestep the lack of data through explicitation of implicit\nrelations to reduce the task to two sub-problems: language modeling and\nexplicit discourse relation classification, a much easier problem. Our\nexperimental results show that this method can even marginally outperform the\nstate-of-the-art, in spite of being much simpler than alternative models of\ncomparable performance. Moreover, we show that the achieved performance is\nrobust across domains as suggested by the zero-shot experiments on a completely\ndifferent domain. This indicates that recent advances in language modeling have\nmade language models sufficiently good at capturing inter-sentence relations\nwithout the help of explicit discourse markers.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:57:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kurfal\u0131", "Murathan", ""], ["\u00d6stling", "Robert", ""]]}, {"id": "2106.03193", "submitter": "Angela Fan", "authors": "Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume\n  Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzman,\n  Angela Fan", "title": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges hindering progress in low-resource and\nmultilingual machine translation is the lack of good evaluation benchmarks.\nCurrent evaluation benchmarks either lack good coverage of low-resource\nlanguages, consider only restricted domains, or are low quality because they\nare constructed using semi-automatic procedures. In this work, we introduce the\nFLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from\nEnglish Wikipedia and covering a variety of different topics and domains. These\nsentences have been translated in 101 languages by professional translators\nthrough a carefully controlled process. The resulting dataset enables better\nassessment of model quality on the long tail of low-resource languages,\nincluding the evaluation of many-to-many multilingual translation systems, as\nall translations are multilingually aligned. By publicly releasing such a\nhigh-quality and high-coverage dataset, we hope to foster progress in the\nmachine translation community and beyond.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:58:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Goyal", "Naman", ""], ["Gao", "Cynthia", ""], ["Chaudhary", "Vishrav", ""], ["Chen", "Peng-Jen", ""], ["Wenzek", "Guillaume", ""], ["Ju", "Da", ""], ["Krishnan", "Sanjana", ""], ["Ranzato", "Marc'Aurelio", ""], ["Guzman", "Francisco", ""], ["Fan", "Angela", ""]]}, {"id": "2106.03232", "submitter": "Ethan Wilcox", "authors": "Ethan Gotlieb Wilcox, Pranali Vani, Roger P. Levy", "title": "A Targeted Assessment of Incremental Processing in Neural LanguageModels\n  and Humans", "comments": "To appear at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a targeted, scaled-up comparison of incremental processing in\nhumans and neural language models by collecting by-word reaction time data for\nsixteen different syntactic test suites across a range of structural phenomena.\nHuman reaction time data comes from a novel online experimental paradigm called\nthe Interpolated Maze task. We compare human reaction times to by-word\nprobabilities for four contemporary language models, with different\narchitectures and trained on a range of data set sizes. We find that across\nmany phenomena, both humans and language models show increased processing\ndifficulty in ungrammatical sentence regions with human and model `accuracy'\nscores (a la Marvin and Linzen(2018)) about equal. However, although language\nmodel outputs match humans in direction, we show that models systematically\nunder-predict the difference in magnitude of incremental processing difficulty\nbetween grammatical and ungrammatical sentences. Specifically, when models\nencounter syntactic violations they fail to accurately predict the longer\nreaction times observed in the human data. These results call into question\nwhether contemporary language models are approaching human-like performance for\nsensitivity to syntactic violations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 20:04:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wilcox", "Ethan Gotlieb", ""], ["Vani", "Pranali", ""], ["Levy", "Roger P.", ""]]}, {"id": "2106.03246", "submitter": "Athar Sefid", "authors": "Athar Sefid, Jian Wu, Prasenjit Mitra, Lee Giles", "title": "Extractive Research Slide Generation Using Windowed Labeling Ranking", "comments": null, "journal-ref": "NAACL/Proceedings of the Second Workshop on Scholarly Document\n  Processing 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Presentation slides describing the content of scientific and technical papers\nare an efficient and effective way to present that work. However, manually\ngenerating presentation slides is labor intensive. We propose a method to\nautomatically generate slides for scientific papers based on a corpus of 5000\npaper-slide pairs compiled from conference proceedings websites. The sentence\nlabeling module of our method is based on SummaRuNNer, a neural sequence model\nfor extractive summarization. Instead of ranking sentences based on semantic\nsimilarities in the whole document, our algorithm measures importance and\nnovelty of sentences by combining semantic and lexical features within a\nsentence window. Our method outperforms several baseline methods including\nSummaRuNNer by a significant margin in terms of ROUGE score.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 20:56:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sefid", "Athar", ""], ["Wu", "Jian", ""], ["Mitra", "Prasenjit", ""], ["Giles", "Lee", ""]]}, {"id": "2106.03257", "submitter": "Bailin Wang", "authors": "Bailin Wang, Mirella Lapata and Ivan Titov", "title": "Structured Reordering for Modeling Latent Alignments in Sequence\n  Transduction", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite success in many domains, neural models struggle in settings where\ntrain and test examples are drawn from different distributions. In particular,\nin contrast to humans, conventional sequence-to-sequence (seq2seq) models fail\nto generalize systematically, i.e., interpret sentences representing novel\ncombinations of concepts (e.g., text segments) seen in training. Traditional\ngrammar formalisms excel in such settings by implicitly encoding alignments\nbetween input and output segments, but are hard to scale and maintain. Instead\nof engineering a grammar, we directly model segment-to-segment alignments as\ndiscrete structured latent variables within a neural seq2seq model. To\nefficiently explore the large space of alignments, we introduce a reorder-first\nalign-later framework whose central component is a neural reordering module\nproducing {\\it separable} permutations. We present an efficient dynamic\nprogramming algorithm performing exact marginal inference of separable\npermutations, and, thus, enabling end-to-end differentiable training of our\nmodel. The resulting seq2seq model exhibits better systematic generalization\nthan standard models on synthetic problems and NLP tasks (i.e., semantic\nparsing and machine translation).\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 21:53:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:57:19 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Bailin", ""], ["Lapata", "Mirella", ""], ["Titov", "Ivan", ""]]}, {"id": "2106.03269", "submitter": "Rahul Aralikatte", "authors": "Rahul Aralikatte, Miryam de Lhoneux, Anoop Kunchukuttan, Anders\n  S{\\o}gaard", "title": "Itihasa: A large-scale corpus for Sanskrit to English translation", "comments": "WAT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces Itihasa, a large-scale translation dataset containing\n93,000 pairs of Sanskrit shlokas and their English translations. The shlokas\nare extracted from two Indian epics viz., The Ramayana and The Mahabharata. We\nfirst describe the motivation behind the curation of such a dataset and follow\nup with empirical analysis to bring out its nuances. We then benchmark the\nperformance of standard translation models on this corpus and show that even\nstate-of-the-art transformer architectures perform poorly, emphasizing the\ncomplexity of the dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 22:58:13 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 16:48:17 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Aralikatte", "Rahul", ""], ["de Lhoneux", "Miryam", ""], ["Kunchukuttan", "Anoop", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2106.03270", "submitter": "Shang-Wen Li", "authors": "Hongyin Luo, Shuyan Dong, Yung-Sung Chuang, Shang-Wen Li", "title": "Meta-learning for downstream aware and agnostic pretraining", "comments": "Extended abstract", "journal-ref": "Meta Learning and Its Applications to Natural Language Processing\n  workshop at ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network pretraining is gaining attention due to its outstanding\nperformance in natural language processing applications. However, pretraining\nusually leverages predefined task sequences to learn general linguistic clues.\nThe lack of mechanisms in choosing proper tasks during pretraining makes the\nlearning and knowledge encoding inefficient. We thus propose using\nmeta-learning to select tasks that provide the most informative learning\nsignals in each episode of pretraining. With the proposed method, we aim to\nachieve better efficiency in computation and memory usage for the pretraining\nprocess and resulting networks while maintaining the performance. In this\npreliminary work, we discuss the algorithm of the method and its two variants,\ndownstream-aware and downstream-agnostic pretraining. Our experiment plan is\nalso summarized, while empirical results will be shared in our future works.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 23:08:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Luo", "Hongyin", ""], ["Dong", "Shuyan", ""], ["Chuang", "Yung-Sung", ""], ["Li", "Shang-Wen", ""]]}, {"id": "2106.03297", "submitter": "Shuo Wang", "authors": "Shuo Wang, Zhaopeng Tu, Zhixing Tan, Shuming Shi, Maosong Sun, Yang\n  Liu", "title": "On the Language Coverage Bias for Neural Machine Translation", "comments": "ACL 2021, Long Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language coverage bias, which indicates the content-dependent differences\nbetween sentence pairs originating from the source and target languages, is\nimportant for neural machine translation (NMT) because the target-original\ntraining data is not well exploited in current practice. By carefully designing\nexperiments, we provide comprehensive analyses of the language coverage bias in\nthe training data, and find that using only the source-original data achieves\ncomparable performance with using full training data. Based on these\nobservations, we further propose two simple and effective approaches to\nalleviate the language coverage bias problem through explicitly distinguishing\nbetween the source- and target-original training data, which consistently\nimprove the performance over strong baselines on six WMT20 translation tasks.\nComplementary to the translationese effect, language coverage bias provides\nanother explanation for the performance drop caused by back-translation. We\nalso apply our approach to both back- and forward-translation and find that\nmitigating the language coverage bias can improve the performance of both the\ntwo representative data augmentation methods and their tagged variants.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 01:55:34 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Shuo", ""], ["Tu", "Zhaopeng", ""], ["Tan", "Zhixing", ""], ["Shi", "Shuming", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "2106.03315", "submitter": "Zhexue Chen", "authors": "Zhexue Chen, Hong Huang, Bang Liu, Xuanhua Shi, Hai Jin", "title": "Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from\nsentences, where each triplet includes an entity, its associated sentiment, and\nthe opinion span explaining the reason for the sentiment. Most existing\nresearch addresses this problem in a multi-stage pipeline manner, which\nneglects the mutual information between such three elements and has the problem\nof error propagation. In this paper, we propose a Semantic and Syntactic\nEnhanced aspect Sentiment triplet Extraction model (S3E2) to fully exploit the\nsyntactic and semantic relationships between the triplet elements and jointly\nextract them. Specifically, we design a Graph-Sequence duel representation and\nmodeling paradigm for the task of ASTE: we represent the semantic and syntactic\nrelationships between word pairs in a sentence by graph and encode it by Graph\nNeural Networks (GNNs), as well as modeling the original sentence by LSTM to\npreserve the sequential information. Under this setting, we further apply a\nmore efficient inference strategy for the extraction of triplets. Extensive\nevaluations on four benchmark datasets show that S3E2 significantly outperforms\nexisting approaches, which proves our S3E2's superiority and flexibility in an\nend-to-end fashion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 03:16:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Zhexue", ""], ["Huang", "Hong", ""], ["Liu", "Bang", ""], ["Shi", "Xuanhua", ""], ["Jin", "Hai", ""]]}, {"id": "2106.03331", "submitter": "Peizhao Li", "authors": "Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao,\n  Rajiv Jain, Varun Manjunatha, Hongfu Liu", "title": "SelfDoc: Self-Supervised Document Representation Learning", "comments": "To appear in CVPR'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose SelfDoc, a task-agnostic pre-training framework for document image\nunderstanding. Because documents are multimodal and are intended for sequential\nreading, our framework exploits the positional, textual, and visual information\nof every semantically meaningful component in a document, and it models the\ncontextualization between each block of content. Unlike existing document\npre-training models, our model is coarse-grained instead of treating individual\nwords as input, therefore avoiding an overly fine-grained with excessive\ncontextualization. Beyond that, we introduce cross-modal learning in the model\npre-training phase to fully leverage multimodal information from unlabeled\ndocuments. For downstream usage, we propose a novel modality-adaptive attention\nmechanism for multimodal feature fusion by adaptively emphasizing language and\nvision signals. Our framework benefits from self-supervised pre-training on\ndocuments without requiring annotations by a feature masking training strategy.\nIt achieves superior performance on multiple downstream tasks with\nsignificantly fewer document images used in the pre-training stage compared to\nprevious works.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:19:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Peizhao", ""], ["Gu", "Jiuxiang", ""], ["Kuen", "Jason", ""], ["Morariu", "Vlad I.", ""], ["Zhao", "Handong", ""], ["Jain", "Rajiv", ""], ["Manjunatha", "Varun", ""], ["Liu", "Hongfu", ""]]}, {"id": "2106.03337", "submitter": "Chulaka Gunasekara", "authors": "Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Sachindra\n  Joshi, David Konopnicki", "title": "Summary Grounded Conversation Generation", "comments": "Findings of ACL - 2021, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many conversation datasets have been constructed in the recent years using\ncrowdsourcing. However, the data collection process can be time consuming and\npresents many challenges to ensure data quality. Since language generation has\nimproved immensely in recent years with the advancement of pre-trained language\nmodels, we investigate how such models can be utilized to generate entire\nconversations, given only a summary of a conversation as the input. We explore\nthree approaches to generate summary grounded conversations, and evaluate the\ngenerated conversations using automatic measures and human judgements. We also\nshow that the accuracy of conversation summarization can be improved by\naugmenting a conversation summarization dataset with generated conversations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:46:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gunasekara", "Chulaka", ""], ["Feigenblat", "Guy", ""], ["Sznajder", "Benjamin", ""], ["Joshi", "Sachindra", ""], ["Konopnicki", "David", ""]]}, {"id": "2106.03345", "submitter": "Jingxuan Yang", "authors": "Jingxuan Yang, Kerui Xu, Jun Xu, Si Li, Sheng Gao, Jun Guo, Nianwen\n  Xue and Ji-Rong Wen", "title": "A Joint Model for Dropped Pronoun Recovery and Conversational Discourse\n  Parsing in Chinese Conversational Speech", "comments": "Accepted by ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a neural model for joint dropped pronoun recovery\n(DPR) and conversational discourse parsing (CDP) in Chinese conversational\nspeech. We show that DPR and CDP are closely related, and a joint model\nbenefits both tasks. We refer to our model as DiscProReco, and it first encodes\nthe tokens in each utterance in a conversation with a directed Graph\nConvolutional Network (GCN). The token states for an utterance are then\naggregated to produce a single state for each utterance. The utterance states\nare then fed into a biaffine classifier to construct a conversational discourse\ngraph. A second (multi-relational) GCN is then applied to the utterance states\nto produce a discourse relation-augmented representation for the utterances,\nwhich are then fused together with token states in each utterance as input to a\ndropped pronoun recovery layer. The joint model is trained and evaluated on a\nnew Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) dataset that we\nannotated with both two types of information. Experimental results on the SPDPR\ndataset and other benchmarks show that DiscProReco significantly outperforms\nthe state-of-the-art baselines of both tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:22:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Jingxuan", ""], ["Xu", "Kerui", ""], ["Xu", "Jun", ""], ["Li", "Si", ""], ["Gao", "Sheng", ""], ["Guo", "Jun", ""], ["Xue", "Nianwen", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2106.03376", "submitter": "Chenyang Huang", "authors": "Chenyang Huang, Wei Yang, Yanshuai Cao, Osmar Za\\\"iane, Lili Mou", "title": "A Globally Normalized Neural Model for Semantic Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a globally normalized model for context-free\ngrammar (CFG)-based semantic parsing. Instead of predicting a probability, our\nmodel predicts a real-valued score at each step and does not suffer from the\nlabel bias problem. Experiments show that our approach outperforms locally\nnormalized models on small datasets, but it does not yield improvement on a\nlarge dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:06:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huang", "Chenyang", ""], ["Yang", "Wei", ""], ["Cao", "Yanshuai", ""], ["Za\u00efane", "Osmar", ""], ["Mou", "Lili", ""]]}, {"id": "2106.03379", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Vishrav Chaudhary, Yuqing Tang, Francisco Guzm\\'an", "title": "LAWDR: Language-Agnostic Weighted Document Representations from\n  Pre-trained Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-lingual document representations enable language understanding in\nmultilingual contexts and allow transfer learning from high-resource to\nlow-resource languages at the document level. Recently large pre-trained\nlanguage models such as BERT, XLM and XLM-RoBERTa have achieved great success\nwhen fine-tuned on sentence-level downstream tasks. It is tempting to apply\nthese cross-lingual models to document representation learning. However, there\nare two challenges: (1) these models impose high costs on long document\nprocessing and thus many of them have strict length limit; (2) model\nfine-tuning requires extra data and computational resources, which is not\npractical in resource-limited settings. In this work, we address these\nchallenges by proposing unsupervised Language-Agnostic Weighted Document\nRepresentations (LAWDR). We study the geometry of pre-trained sentence\nembeddings and leverage it to derive document representations without\nfine-tuning. Evaluated on cross-lingual document alignment, LAWDR demonstrates\ncomparable performance to state-of-the-art models on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:14:00 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gong", "Hongyu", ""], ["Chaudhary", "Vishrav", ""], ["Tang", "Yuqing", ""], ["Guzm\u00e1n", "Francisco", ""]]}, {"id": "2106.03389", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen, Khalid Alnajjar, Niko Partanen, Jack Rueter", "title": "Never guess what I heard... Rumor Detection in Finnish News: a Dataset\n  and a Baseline", "comments": "2021 Workshop on NLP4IF: Censorship, Disinformation, and Propaganda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a new dataset on rumor detection in Finnish language news\nheadlines. We have evaluated two different LSTM based models and two different\nBERT models, and have found very significant differences in the results. A\nfine-tuned FinBERT reaches the best overall accuracy of 94.3% and rumor label\naccuracy of 96.0% of the time. However, a model fine-tuned on Multilingual BERT\nreaches the best factual label accuracy of 97.2%. Our results suggest that the\nperformance difference is due to a difference in the original training data.\nFurthermore, we find that a regular LSTM model works better than one trained\nwith a pretrained word2vec model. These findings suggest that more work needs\nto be done for pretrained models in Finnish language as they have been trained\non small and biased corpora.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:36:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Alnajjar", "Khalid", ""], ["Partanen", "Niko", ""], ["Rueter", "Jack", ""]]}, {"id": "2106.03391", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Jack Rueter, Mar\\'ilia Fernanda Pereira de Freitas, Sidney da Silva\n  Facundes, Mika H\\\"am\\\"al\\\"ainen, Niko Partanen", "title": "Apurin\\~a Universal Dependencies Treebank", "comments": "The First Workshop on NLP for Indigenous Languages of the Americas\n  (AmericasNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents and discusses the first Universal Dependencies treebank\nfor the Apurin\\~a language. The treebank contains 76 fully annotated sentences,\napplies 14 parts-of-speech, as well as seven augmented or new features - some\nof which are unique to Apurin\\~a. The construction of the treebank has also\nserved as an opportunity to develop finite-state description of the language\nand facilitate the transfer of open-source infrastructure possibilities to an\nendangered language of the Amazon. The source materials used in the initial\ntreebank represent fieldwork practices where not all tokens of all sentences\nare equally annotated. For this reason, establishing regular annotation\npractices for the entire Apurin\\~a treebank is an ongoing project.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:42:00 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rueter", "Jack", ""], ["de Freitas", "Mar\u00edlia Fernanda Pereira", ""], ["Facundes", "Sidney da Silva", ""], ["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Partanen", "Niko", ""]]}, {"id": "2106.03410", "submitter": "Bin Sun", "authors": "Bin Sun, Shaoxiong Feng, Yiwei Li, Jiamou Liu, Kan Li", "title": "Generating Relevant and Coherent Dialogue Responses using Self-separated\n  Conditional Variational AutoEncoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Variational AutoEncoder (CVAE) effectively increases the\ndiversity and informativeness of responses in open-ended dialogue generation\ntasks through enriching the context vector with sampled latent variables.\nHowever, due to the inherent one-to-many and many-to-one phenomena in human\ndialogues, the sampled latent variables may not correctly reflect the contexts'\nsemantics, leading to irrelevant and incoherent generated responses. To resolve\nthis problem, we propose Self-separated Conditional Variational AutoEncoder\n(abbreviated as SepaCVAE) that introduces group information to regularize the\nlatent variables, which enhances CVAE by improving the responses' relevance and\ncoherence while maintaining their diversity and informativeness. SepaCVAE\nactively divides the input data into groups, and then widens the absolute\ndifference between data pairs from distinct groups, while narrowing the\nrelative distance between data pairs in the same group. Empirical results from\nautomatic evaluation and detailed analysis demonstrate that SepaCVAE can\nsignificantly boost responses in well-established open-domain dialogue\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:19:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sun", "Bin", ""], ["Feng", "Shaoxiong", ""], ["Li", "Yiwei", ""], ["Liu", "Jiamou", ""], ["Li", "Kan", ""]]}, {"id": "2106.03427", "submitter": "Yichi Zhang", "authors": "Yichi Zhang and Joyce Chai", "title": "Hierarchical Task Learning from Language Instructions with Unified\n  Transformers and Self-Monitoring", "comments": "Accepted by ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress, learning new tasks through language instructions\nremains an extremely challenging problem. On the ALFRED benchmark for task\nlearning, the published state-of-the-art system only achieves a task success\nrate of less than 10% in an unseen environment, compared to the human\nperformance of over 90%. To address this issue, this paper takes a closer look\nat task learning. In a departure from a widely applied end-to-end architecture,\nwe decomposed task learning into three sub-problems: sub-goal planning, scene\nnavigation, and object manipulation; and developed a model HiTUT (stands for\nHierarchical Tasks via Unified Transformers) that addresses each sub-problem in\na unified manner to learn a hierarchical task structure. On the ALFRED\nbenchmark, HiTUT has achieved the best performance with a remarkably higher\ngeneralization ability. In the unseen environment, HiTUT achieves over 160%\nperformance gain in success rate compared to the previous state of the art. The\nexplicit representation of task structures also enables an in-depth\nunderstanding of the nature of the problem and the ability of the agent, which\nprovides insight for future benchmark development and evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:48:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Yichi", ""], ["Chai", "Joyce", ""]]}, {"id": "2106.03441", "submitter": "Shengqiang Zhang", "authors": "Shengqiang Zhang, Xingxing Zhang, Hangbo Bao, Furu Wei", "title": "Attention Temperature Matters in Abstractive Summarization Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress of abstractive text summarization largely relies on large\npre-trained sequence-to-sequence Transformer models, which are computationally\nexpensive. This paper aims to distill these large models into smaller ones for\nfaster inference and minimal performance loss. Pseudo-labeling based methods\nare popular in sequence-to-sequence model distillation. In this paper, we find\nsimply manipulating attention temperatures in Transformers can make pseudo\nlabels easier to learn for student models. Our experiments on three\nsummarization datasets show our proposed method consistently improves over\nvanilla pseudo-labeling based methods. We also find that both the pseudo labels\nand summaries produced by our students are shorter and more abstractive. We\nwill make our code and models publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:18:21 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:09:45 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Shengqiang", ""], ["Zhang", "Xingxing", ""], ["Bao", "Hangbo", ""], ["Wei", "Furu", ""]]}, {"id": "2106.03469", "submitter": "Menglin Xia", "authors": "Menglin Xia, Emilio Monti", "title": "Multilingual Neural Semantic Parsing for Low-Resourced Languages", "comments": "Accepted at *SEM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multilingual semantic parsing is a cost-effective method that allows a single\nmodel to understand different languages. However, researchers face a great\nimbalance of availability of training data, with English being resource rich,\nand other languages having much less data. To tackle the data limitation\nproblem, we propose using machine translation to bootstrap multilingual\ntraining data from the more abundant English data. To compensate for the data\nquality of machine translated training data, we utilize transfer learning from\npretrained multilingual encoders to further improve the model. To evaluate our\nmultilingual models on human-written sentences as opposed to machine translated\nones, we introduce a new multilingual semantic parsing dataset in English,\nItalian and Japanese based on the Facebook Task Oriented Parsing (TOP) dataset.\nWe show that joint multilingual training with pretrained encoders substantially\noutperforms our baselines on the TOP dataset and outperforms the\nstate-of-the-art model on the public NLMaps dataset. We also establish a new\nbaseline for zero-shot learning on the TOP dataset. We find that a semantic\nparser trained only on English data achieves a zero-shot performance of 44.9%\nexact-match accuracy on Italian sentences.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:53:02 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 16:51:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xia", "Menglin", ""], ["Monti", "Emilio", ""]]}, {"id": "2106.03471", "submitter": "Lisa Beinborn", "authors": "Nora Hollenstein and Lisa Beinborn", "title": "Relative Importance in Sentence Processing", "comments": "accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Determining the relative importance of the elements in a sentence is a key\nfactor for effortless natural language understanding. For human language\nprocessing, we can approximate patterns of relative importance by measuring\nreading fixations using eye-tracking technology. In neural language models,\ngradient-based saliency methods indicate the relative importance of a token for\nthe target objective. In this work, we compare patterns of relative importance\nin English language processing by humans and models and analyze the underlying\nlinguistic patterns. We find that human processing patterns in English\ncorrelate strongly with saliency-based importance in language models and not\nwith attention-based importance. Our results indicate that saliency could be a\ncognitively more plausible metric for interpreting neural language models. The\ncode is available on GitHub: https://github.com/beinborn/relative_importance\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:56:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hollenstein", "Nora", ""], ["Beinborn", "Lisa", ""]]}, {"id": "2106.03484", "submitter": "Faidon Mitzalis", "authors": "Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, Lucia Specia", "title": "BERTGEN: Multi-task Generation through BERT", "comments": "Accepted to ACL 2021 Main Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BERTGEN, a novel generative, decoder-only model which extends BERT\nby fusing multimodal and multilingual pretrained models VL-BERT and M-BERT,\nrespectively. BERTGEN is auto-regressively trained for language generation\ntasks, namely image captioning, machine translation and multimodal machine\ntranslation, under a multitask setting. With a comprehensive set of\nevaluations, we show that BERTGEN outperforms many strong baselines across the\ntasks explored. We also show BERTGEN's ability for zero-shot language\ngeneration, where it exhibits competitive performance to supervised\ncounterparts. Finally, we conduct ablation studies which demonstrate that\nBERTGEN substantially benefits from multi-tasking and effectively transfers\nrelevant inductive biases from the pre-trained models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:17:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mitzalis", "Faidon", ""], ["Caglayan", "Ozan", ""], ["Madhyastha", "Pranava", ""], ["Specia", "Lucia", ""]]}, {"id": "2106.03518", "submitter": "Hanqi Yan", "authors": "Hanqi Yan, Lin Gui, Gabriele Pergola, Yulan He", "title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion\n  Cause Extraction", "comments": "ACL2021 Main Conference Long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Emotion Cause Extraction (ECE)} task aims to identify clauses which\ncontain emotion-evoking information for a particular emotion expressed in text.\nWe observe that a widely-used ECE dataset exhibits a bias that the majority of\nannotated cause clauses are either directly before their associated emotion\nclauses or are the emotion clauses themselves. Existing models for ECE tend to\nexplore such relative position information and suffer from the dataset bias. To\ninvestigate the degree of reliance of existing ECE models on clause relative\npositions, we propose a novel strategy to generate adversarial examples in\nwhich the relative position information is no longer the indicative feature of\ncause clauses. We test the performance of existing models on such adversarial\nexamples and observe a significant performance drop. To address the dataset\nbias, we propose a novel graph-based method to explicitly model the emotion\ntriggering paths by leveraging the commonsense knowledge to enhance the\nsemantic dependencies between a candidate clause and an emotion clause.\nExperimental results show that our proposed approach performs on par with the\nexisting state-of-the-art methods on the original ECE dataset, and is more\nrobust against adversarial attacks compared to existing models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:14:58 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 10:27:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yan", "Hanqi", ""], ["Gui", "Lin", ""], ["Pergola", "Gabriele", ""], ["He", "Yulan", ""]]}, {"id": "2106.03521", "submitter": "Anne Lauscher", "authors": "Soumya Barikeri, Anne Lauscher, Ivan Vuli\\'c, and Goran Glava\\v{s}", "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of\n  Conversational Language Models", "comments": "Accepted for ACL21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Text representation models are prone to exhibit a range of societal biases,\nreflecting the non-controlled and biased nature of the underlying pretraining\ndata, which consequently leads to severe ethical issues and even bias\namplification. Recent work has predominantly focused on measuring and\nmitigating bias in pretrained language models. Surprisingly, the landscape of\nbias measurements and mitigation resources and methods for conversational\nlanguage models is still very scarce: it is limited to only a few types of\nbias, artificially constructed resources, and completely ignores the impact\nthat debiasing methods may have on the final performance in dialog tasks, e.g.,\nconversational response generation. In this work, we present RedditBias, the\nfirst conversational data set grounded in the actual human conversations from\nReddit, allowing for bias measurement and mitigation across four important bias\ndimensions: gender, race, religion, and queerness. Further, we develop an\nevaluation framework which simultaneously 1) measures bias on the developed\nRedditBias resource, and 2) evaluates model capability in dialog tasks after\nmodel debiasing. We use the evaluation framework to benchmark the widely used\nconversational DialoGPT model along with the adaptations of four debiasing\nmethods. Our results indicate that DialoGPT is biased with respect to religious\ngroups and that some debiasing techniques can remove this bias while preserving\ndownstream task performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:22:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Barikeri", "Soumya", ""], ["Lauscher", "Anne", ""], ["Vuli\u0107", "Ivan", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2106.03530", "submitter": "Yan Xu", "authors": "Etsuko Ishii, Yan Xu, Genta Indra Winata, Zhaojiang Lin, Andrea\n  Madotto, Zihan Liu, Peng Xu, Pascale Fung", "title": "CAiRE in DialDoc21: Data Augmentation for Information-Seeking Dialogue\n  System", "comments": "Accepted in DialDoc21 Workshop in ACL 2021. Etsuko Ishii and Yan Xu\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-seeking dialogue systems, including knowledge identification and\nresponse generation, aim to respond to users with fluent, coherent, and\ninformative responses based on users' needs, which. To tackle this challenge,\nwe utilize data augmentation methods and several training techniques with the\npre-trained language models to learn a general pattern of the task and thus\nachieve promising performance. In DialDoc21 competition, our system achieved\n74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU\nscore in subtask 2. Empirical analysis is provided to explain the effectiveness\nof our approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:40:55 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:07:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ishii", "Etsuko", ""], ["Xu", "Yan", ""], ["Winata", "Genta Indra", ""], ["Lin", "Zhaojiang", ""], ["Madotto", "Andrea", ""], ["Liu", "Zihan", ""], ["Xu", "Peng", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.03554", "submitter": "Wil van der Aalst", "authors": "Wil M.P. van der Aalst", "title": "Free-Choice Nets With Home Clusters Are Lucent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A marked Petri net is lucent if there are no two different reachable markings\nenabling the same set of transitions, i.e., states are fully characterized by\nthe transitions they enable. Characterizing the class of systems that are\nlucent is a foundational and also challenging question. However, little\nresearch has been done on the topic. In this paper, it is shown that all\nfree-choice nets having a home cluster are lucent. These nets have a so-called\nhome marking such that it is always possible to reach this marking again. Such\na home marking can serve as a regeneration point or as an end-point. The result\nis highly relevant because in many applications, we want the system to be\nlucent and many well-behaved process models fall into the class identified in\nthis paper. Unlike previous work, we do not require the marked Petri net to be\nlive and strongly connected. Most of the analysis techniques for free-choice\nnets are tailored towards well-formed nets. The approach presented in this\npaper provides a novel perspective enabling new analysis techniques for\nfree-choice nets that do not need to be well-formed. Therefore, we can also\nmodel systems and processes that are terminating and/or have an initialization\nphase.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 12:34:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["van der Aalst", "Wil M. P.", ""]]}, {"id": "2106.03598", "submitter": "Long Phan", "authors": "Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol\n  Bahadroglu, Alec Peltekian, Gr\\'egoire Altan-Bonnet", "title": "SciFive: a text-to-text transformer model for biomedical literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we introduce SciFive, a domain-specific T5 model that has\nbeen pre-trained on large biomedical corpora. Our model outperforms the current\nSOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation,\nrelation extraction, natural language inference, and question-answering. We\nshow that text-generation methods have significant potential in a broad array\nof biomedical NLP tasks, particularly those requiring longer, more complex\noutputs. Our results support the exploration of more difficult text generation\ntasks and the development of new methods in this area\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:09:23 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Phan", "Long N.", ""], ["Anibal", "James T.", ""], ["Tran", "Hieu", ""], ["Chanana", "Shaurya", ""], ["Bahadroglu", "Erol", ""], ["Peltekian", "Alec", ""], ["Altan-Bonnet", "Gr\u00e9goire", ""]]}, {"id": "2106.03613", "submitter": "Jianlei Yang", "authors": "Xin Guo, Jianlei Yang, Haoyi Zhou, Xucheng Ye, Jianxin Li", "title": "RoSearch: Search for Robust Student Architectures When Distilling\n  Pre-trained Language Models", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models achieve outstanding performance in NLP tasks.\nVarious knowledge distillation methods have been proposed to reduce the heavy\ncomputation and storage requirements of pre-trained language models. However,\nfrom our observations, student models acquired by knowledge distillation suffer\nfrom adversarial attacks, which limits their usage in security sensitive\nscenarios. In order to overcome these security problems, RoSearch is proposed\nas a comprehensive framework to search the student models with better\nadversarial robustness when performing knowledge distillation. A directed\nacyclic graph based search space is built and an evolutionary search strategy\nis utilized to guide the searching approach. Each searched architecture is\ntrained by knowledge distillation on pre-trained language model and then\nevaluated under a robustness-, accuracy- and efficiency-aware metric as\nenvironmental fitness. Experimental results show that RoSearch can improve\nrobustness of student models from 7%~18% up to 45.8%~47.8% on different\ndatasets with comparable weight compression ratio to existing distillation\nmethods (4.6$\\times$~6.5$\\times$ improvement from teacher model BERT_BASE) and\nlow accuracy drop. In addition, we summarize the relationship between student\narchitecture and robustness through statistics of searched models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:38:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Guo", "Xin", ""], ["Yang", "Jianlei", ""], ["Zhou", "Haoyi", ""], ["Ye", "Xucheng", ""], ["Li", "Jianxin", ""]]}, {"id": "2106.03618", "submitter": "Ningyu Zhang", "authors": "Ningyu Zhang, Xiang Chen, Xin Xie, Shumin Deng, Chuanqi Tan, Mosha\n  Chen, Fei Huang, Luo Si, Huajun Chen", "title": "Document-level Relation Extraction as Semantic Segmentation", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction aims to extract relations among multiple\nentity pairs from a document. Previously proposed graph-based or\ntransformer-based models utilize the entities independently, regardless of\nglobal information among relational triples. This paper approaches the problem\nby predicting an entity-level relation matrix to capture local and global\ninformation, parallel to the semantic segmentation task in computer vision.\nHerein, we propose a Document U-shaped Network for document-level relation\nextraction. Specifically, we leverage an encoder module to capture the context\ninformation of entities and a U-shaped segmentation module over the image-style\nfeature map to capture global interdependency among triples. Experimental\nresults show that our approach can obtain state-of-the-art performance on three\nbenchmark datasets DocRED, CDR, and GDA.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:44:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Ningyu", ""], ["Chen", "Xiang", ""], ["Xie", "Xin", ""], ["Deng", "Shumin", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""], ["Chen", "Huajun", ""]]}, {"id": "2106.03631", "submitter": "Ehsan Shareghi", "authors": "Lan Zhang, Victor Prokhorov, Ehsan Shareghi", "title": "Unsupervised Representation Disentanglement of Text: An Evaluation on\n  Synthetic Datasets", "comments": "Accepted to RepL4NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To highlight the challenges of achieving representation disentanglement for\ntext domain in an unsupervised setting, in this paper we select a\nrepresentative set of successfully applied models from the image domain. We\nevaluate these models on 6 disentanglement metrics, as well as on downstream\nclassification tasks and homotopy. To facilitate the evaluation, we propose two\nsynthetic datasets with known generative factors. Our experiments highlight the\nexisting gap in the text domain and illustrate that certain elements such as\nrepresentation sparsity (as an inductive bias), or representation coupling with\nthe decoder could impact disentanglement. To the best of our knowledge, our\nwork is the first attempt on the intersection of unsupervised representation\ndisentanglement and text, and provides the experimental framework and datasets\nfor examining future developments in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:04:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Lan", ""], ["Prokhorov", "Victor", ""], ["Shareghi", "Ehsan", ""]]}, {"id": "2106.03634", "submitter": "Stephane Aroca-Ouellette", "authors": "St\\'ephane Aroca-Ouellette, Cory Paik, Alessandro Roncone, and\n  Katharina Kann", "title": "PROST: Physical Reasoning of Objects through Space and Time", "comments": "Accepted to ACL-Findings 2021, 9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new probing dataset named PROST: Physical Reasoning about\nObjects Through Space and Time. This dataset contains 18,736 multiple-choice\nquestions made from 14 manually curated templates, covering 10 physical\nreasoning concepts. All questions are designed to probe both causal and masked\nlanguage models in a zero-shot setting. We conduct an extensive analysis which\ndemonstrates that state-of-the-art pretrained models are inadequate at physical\nreasoning: they are influenced by the order in which answer options are\npresented to them, they struggle when the superlative in a question is inverted\n(e.g., most <-> least), and increasing the amount of pretraining data and\nparameters only yields minimal improvements. These results provide support for\nthe hypothesis that current pretrained models' ability to reason about physical\ninteractions is inherently limited by a lack of real world experience. By\nhighlighting these limitations, we hope to motivate the development of models\nwith a human-like understanding of the physical world.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:06:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Aroca-Ouellette", "St\u00e9phane", ""], ["Paik", "Cory", ""], ["Roncone", "Alessandro", ""], ["Kann", "Katharina", ""]]}, {"id": "2106.03635", "submitter": "Lei Shen", "authors": "Lei Shen, Fandong Meng, Jinchao Zhang, Yang Feng, Jie Zhou", "title": "GTM: A Generative Triple-Wise Model for Conversational Question\n  Generation", "comments": "To appear at ACL 2021 main conference (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating some appealing questions in open-domain conversations is an\neffective way to improve human-machine interactions and lead the topic to a\nbroader or deeper direction. To avoid dull or deviated questions, some\nresearchers tried to utilize answer, the \"future\" information, to guide\nquestion generation. However, they separate a post-question-answer (PQA) triple\ninto two parts: post-question (PQ) and question-answer (QA) pairs, which may\nhurt the overall coherence. Besides, the QA relationship is modeled as a\none-to-one mapping that is not reasonable in open-domain conversations. To\ntackle these problems, we propose a generative triple-wise model with\nhierarchical variations for open-domain conversational question generation\n(CQG). Latent variables in three hierarchies are used to represent the shared\nbackground of a triple and one-to-many semantic mappings in both PQ and QA\npairs. Experimental results on a large-scale CQG dataset show that our method\nsignificantly improves the quality of questions in terms of fluency, coherence\nand diversity over competitive baselines.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:07:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shen", "Lei", ""], ["Meng", "Fandong", ""], ["Zhang", "Jinchao", ""], ["Feng", "Yang", ""], ["Zhou", "Jie", ""]]}, {"id": "2106.03706", "submitter": "Yi-Ting Yeh", "authors": "Yi-Ting Yeh, Maxine Eskenazi, Shikib Mehri", "title": "A Comprehensive Assessment of Dialog Evaluation Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 23\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:17:03 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 17:07:04 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 18:31:56 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 20:33:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yeh", "Yi-Ting", ""], ["Eskenazi", "Maxine", ""], ["Mehri", "Shikib", ""]]}, {"id": "2106.03717", "submitter": "Domenic Donato", "authors": "Domenic Donato, Lei Yu, Chris Dyer", "title": "Diverse Pretrained Context Encodings Improve Document Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture for adapting a sentence-level\nsequence-to-sequence transformer by incorporating multiple pretrained document\ncontext signals and assess the impact on translation performance of (1)\ndifferent pretraining approaches for generating these signals, (2) the quantity\nof parallel data for which document context is available, and (3) conditioning\non source, target, or source and target contexts. Experiments on the NIST\nChinese-English, and IWSLT and WMT English-German tasks support four general\nconclusions: that using pretrained context representations markedly improves\nsample efficiency, that adequate parallel data resources are crucial for\nlearning to use document context, that jointly conditioning on multiple context\nrepresentations outperforms any single representation, and that source context\nis more valuable for translation performance than target side context. Our best\nmulti-context model consistently outperforms the best existing context-aware\ntransformers.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:28:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Donato", "Domenic", ""], ["Yu", "Lei", ""], ["Dyer", "Chris", ""]]}, {"id": "2106.03730", "submitter": "Raheel Qader", "authors": "Melissa Ailem, Jinghsu Liu, Raheel Qader", "title": "Encouraging Neural Machine Translation to Satisfy Terminology\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new approach to encourage neural machine translation to satisfy\nlexical constraints. Our method acts at the training step and thereby avoiding\nthe introduction of any extra computational overhead at inference step. The\nproposed method combines three main ingredients. The first one consists in\naugmenting the training data to specify the constraints. Intuitively, this\nencourages the model to learn a copy behavior when it encounters constraint\nterms. Compared to previous work, we use a simplified augmentation strategy\nwithout source factors. The second ingredient is constraint token masking,\nwhich makes it even easier for the model to learn the copy behavior and\ngeneralize better. The third one, is a modification of the standard cross\nentropy loss to bias the model towards assigning high probabilities to\nconstraint words. Empirical results show that our method improves upon related\nbaselines in terms of both BLEU score and the percentage of generated\nconstraint terms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:46:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ailem", "Melissa", ""], ["Liu", "Jinghsu", ""], ["Qader", "Raheel", ""]]}, {"id": "2106.03777", "submitter": "Zihan Liu", "authors": "Zihan Liu, Genta Indra Winata, Peng Xu, Pascale Fung", "title": "X2Parser: Cross-Lingual and Cross-Domain Framework for Task-Oriented\n  Compositional Semantic Parsing", "comments": "Accepted in RepL4NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented compositional semantic parsing (TCSP) handles complex nested\nuser queries and serves as an essential component of virtual assistants.\nCurrent TCSP models rely on numerous training data to achieve decent\nperformance but fail to generalize to low-resource target languages or domains.\nIn this paper, we present X2Parser, a transferable Cross-lingual and\nCross-domain Parser for TCSP. Unlike previous models that learn to generate the\nhierarchical representations for nested intents and slots, we propose to\npredict flattened intents and slots representations separately and cast both\nprediction tasks into sequence labeling problems. After that, we further\npropose a fertility-based slot predictor that first learns to dynamically\ndetect the number of labels for each token, and then predicts the slot types.\nExperimental results illustrate that our model can significantly outperform\nexisting strong baselines in cross-lingual and cross-domain settings, and our\nmodel can also achieve a good generalization ability on target languages of\ntarget domains. Furthermore, our model tackles the problem in an efficient\nnon-autoregressive way that reduces the latency by up to 66% compared to the\ngenerative model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:40:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Zihan", ""], ["Winata", "Genta Indra", ""], ["Xu", "Peng", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.03794", "submitter": "Tuhin Chakrabarty Mr", "authors": "Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan", "title": "COVID-Fact: Fact Extraction and Verification of Real-World Claims on\n  COVID-19 Pandemic", "comments": "ACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a FEVER-like dataset COVID-Fact of $4,086$ claims concerning the\nCOVID-19 pandemic. The dataset contains claims, evidence for the claims, and\ncontradictory claims refuted by the evidence. Unlike previous approaches, we\nautomatically detect true claims and their source articles and then generate\ncounter-claims using automatic methods rather than employing human annotators.\nAlong with our constructed resource, we formally present the task of\nidentifying relevant evidence for the claims and verifying whether the evidence\nrefutes or supports a given claim. In addition to scientific claims, our data\ncontains simplified general claims from media sources, making it better suited\nfor detecting general misinformation regarding COVID-19. Our experiments\nindicate that COVID-Fact will provide a challenging testbed for the development\nof new systems and our approach will reduce the costs of building\ndomain-specific datasets for detecting misinformation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:59:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Saakyan", "Arkadiy", ""], ["Chakrabarty", "Tuhin", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2106.03806", "submitter": "Shinhyeok Oh", "authors": "Shinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam Park, Gaeun Seo,\n  EungGyun Kim and Harksoo Kim", "title": "Deep Context- and Relation-Aware Learning for Aspect-based Sentiment\n  Analysis", "comments": "Accepted to ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works for aspect-based sentiment analysis (ABSA) have adopted a\nunified approach, which allows the interactive relations among subtasks.\nHowever, we observe that these methods tend to predict polarities based on the\nliteral meaning of aspect and opinion terms and mainly consider relations\nimplicitly among subtasks at the word level. In addition, identifying multiple\naspect-opinion pairs with their polarities is much more challenging. Therefore,\na comprehensive understanding of contextual information w.r.t. the aspect and\nopinion are further required in ABSA. In this paper, we propose Deep\nContextualized Relation-Aware Network (DCRAN), which allows interactive\nrelations among subtasks with deep contextual information based on two modules\n(i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies).\nEspecially, we design novel self-supervised strategies for ABSA, which have\nstrengths in dealing with multiple aspects. Experimental results show that\nDCRAN significantly outperforms previous state-of-the-art methods by large\nmargins on three widely used benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:16:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Oh", "Shinhyeok", ""], ["Lee", "Dongyub", ""], ["Whang", "Taesun", ""], ["Park", "IlNam", ""], ["Seo", "Gaeun", ""], ["Kim", "EungGyun", ""], ["Kim", "Harksoo", ""]]}, {"id": "2106.03816", "submitter": "Akash Kumar Mohankumar", "authors": "Akash Kumar Mohankumar, Nikit Begwani, Amit Singh", "title": "Diversity driven Query Rewriting in Search Advertising", "comments": "Accepted in KDD 2021, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving keywords (bidwords) with the same intent as query, referred to as\nclose variant keywords, is of prime importance for effective targeted search\nadvertising. For head and torso search queries, sponsored search engines use a\nhuge repository of same intent queries and keywords, mined ahead of time.\nOnline, this repository is used to rewrite the query and then lookup the\nrewrite in a repository of bid keywords contributing to significant revenue.\nRecently generative retrieval models have been shown to be effective at the\ntask of generating such query rewrites. We observe two main limitations of such\ngenerative models. First, rewrites generated by these models exhibit low\nlexical diversity, and hence the rewrites fail to retrieve relevant keywords\nthat have diverse linguistic variations. Second, there is a misalignment\nbetween the training objective - the likelihood of training data, v/s what we\ndesire - improved quality and coverage of rewrites. In this work, we introduce\nCLOVER, a framework to generate both high-quality and diverse rewrites by\noptimizing for human assessment of rewrite quality using our diversity-driven\nreinforcement learning algorithm. We use an evaluation model, trained to\npredict human judgments, as the reward function to finetune the generation\npolicy. We empirically show the effectiveness of our proposed approach through\noffline experiments on search queries across geographies spanning three major\nlanguages. We also perform online A/B experiments on Bing, a large commercial\nsearch engine, which shows (i) better user engagement with an average increase\nin clicks by 12.83% accompanied with an average defect reduction by 13.97%, and\n(ii) improved revenue by 21.29%.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:30:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mohankumar", "Akash Kumar", ""], ["Begwani", "Nikit", ""], ["Singh", "Amit", ""]]}, {"id": "2106.03821", "submitter": "Baptiste Pouthier", "authors": "Baptiste Pouthier, Laurent Pilati, Leela K. Gudupudi, Charles\n  Bouveyron and Frederic Precioso", "title": "Active Speaker Detection as a Multi-Objective Optimization with\n  Uncertainty-based Multimodal Fusion", "comments": "In INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well established from a variety of studies that there is a\nsignificant benefit from combining video and audio data in detecting active\nspeakers. However, either of the modalities can potentially mislead audiovisual\nfusion by inducing unreliable or deceptive information. This paper outlines\nactive speaker detection as a multi-objective learning problem to leverage best\nof each modalities using a novel self-attention, uncertainty-based multimodal\nfusion scheme. Results obtained show that the proposed multi-objective learning\narchitecture outperforms traditional approaches in improving both mAP and AUC\nscores. We further demonstrate that our fusion strategy surpasses, in active\nspeaker detection, other modality fusion methods reported in various\ndisciplines. We finally show that the proposed method significantly improves\nthe state-of-the-art on the AVA-ActiveSpeaker dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:38:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Pouthier", "Baptiste", ""], ["Pilati", "Laurent", ""], ["Gudupudi", "Leela K.", ""], ["Bouveyron", "Charles", ""], ["Precioso", "Frederic", ""]]}, {"id": "2106.03826", "submitter": "Mo Yu", "authors": "Xiangyang Mou, Chenghao Yang, Mo Yu, Bingsheng Yao, Xiaoxiao Guo,\n  Saloni Potdar, Hui Su", "title": "Narrative Question Answering with Cutting-Edge Open-Domain QA\n  Techniques: A Comprehensive Study", "comments": "Accepted to TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in open-domain question answering (ODQA), i.e., finding\nanswers from large open-domain corpus like Wikipedia, have led to human-level\nperformance on many datasets. However, progress in QA over book stories (Book\nQA) lags behind despite its similar task formulation to ODQA. This work\nprovides a comprehensive and quantitative analysis about the difficulty of Book\nQA: (1) We benchmark the research on the NarrativeQA dataset with extensive\nexperiments with cutting-edge ODQA techniques. This quantifies the challenges\nBook QA poses, as well as advances the published state-of-the-art with a\n$\\sim$7\\% absolute improvement on Rouge-L. (2) We further analyze the detailed\nchallenges in Book QA through human\nstudies.\\footnote{\\url{https://github.com/gorov/BookQA}.} Our findings indicate\nthat the event-centric questions dominate this task, which exemplifies the\ninability of existing QA models to handle event-oriented scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:46:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mou", "Xiangyang", ""], ["Yang", "Chenghao", ""], ["Yu", "Mo", ""], ["Yao", "Bingsheng", ""], ["Guo", "Xiaoxiao", ""], ["Potdar", "Saloni", ""], ["Su", "Hui", ""]]}, {"id": "2106.03830", "submitter": "Sascha Rothe", "authors": "Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause,\n  Aliaksei Severyn", "title": "A Simple Recipe for Multilingual Grammatical Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple recipe to train state-of-the-art multilingual\nGrammatical Error Correction (GEC) models. We achieve this by first proposing a\nlanguage-agnostic method to generate a large number of synthetic examples. The\nsecond ingredient is to use large-scale multilingual language models (up to 11B\nparameters). Once fine-tuned on language-specific supervised sets we surpass\nthe previous state-of-the-art results on GEC benchmarks in four languages:\nEnglish, Czech, German and Russian. Having established a new set of baselines\nfor GEC, we make our results easily reproducible and accessible by releasing a\ncLang-8 dataset. It is produced by using our best model, which we call gT5, to\nclean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly\nsimplifies typical GEC training pipelines composed of multiple fine-tuning\nstages -- we demonstrate that performing a single fine-tuning step on cLang-8\nwith the off-the-shelf language models yields further accuracy improvements\nover an already top-performing gT5 model for English.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:47:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rothe", "Sascha", ""], ["Mallinson", "Jonathan", ""], ["Malmi", "Eric", ""], ["Krause", "Sebastian", ""], ["Severyn", "Aliaksei", ""]]}, {"id": "2106.03831", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Wenhu Chen, Michael Saxon, William Yang Wang", "title": "Counterfactual Maximum Likelihood Estimation for Training Deep Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have driven state-of-the-art performance on a\nwide array of tasks, they are prone to learning spurious correlations that\nshould not be learned as predictive clues. To mitigate this problem, we propose\na causality-based training framework to reduce the spurious correlations caused\nby observable confounders. We give theoretical analysis on the underlying\ngeneral Structural Causal Model (SCM) and propose to perform Maximum Likelihood\nEstimation (MLE) on the interventional distribution instead of the\nobservational distribution, namely Counterfactual Maximum Likelihood Estimation\n(CMLE). As the interventional distribution, in general, is hidden from the\nobservational data, we then derive two different upper bounds of the expected\nnegative log-likelihood and propose two general algorithms, Implicit CMLE and\nExplicit CMLE, for causal predictions of deep learning models using\nobservational data. We conduct experiments on two real-world tasks: Natural\nLanguage Inference (NLI) and Image Captioning. The results show that CMLE\nmethods outperform the regular MLE method in terms of out-of-domain\ngeneralization performance and reducing spurious correlations, while\nmaintaining comparable performance on the regular evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:47:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Xinyi", ""], ["Chen", "Wenhu", ""], ["Saxon", "Michael", ""], ["Wang", "William Yang", ""]]}, {"id": "2106.03873", "submitter": "Dorottya Demszky", "authors": "Dorottya Demszky, Jing Liu, Zid Mancenido, Julie Cohen, Heather Hill,\n  Dan Jurafsky, Tatsunori Hashimoto", "title": "Measuring Conversational Uptake: A Case Study on Student-Teacher\n  Interactions", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In conversation, uptake happens when a speaker builds on the contribution of\ntheir interlocutor by, for example, acknowledging, repeating or reformulating\nwhat they have said. In education, teachers' uptake of student contributions\nhas been linked to higher student achievement. Yet measuring and improving\nteachers' uptake at scale is challenging, as existing methods require expensive\nannotation by experts. We propose a framework for computationally measuring\nuptake, by (1) releasing a dataset of student-teacher exchanges extracted from\nUS math classroom transcripts annotated for uptake by experts; (2) formalizing\nuptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next\nutterance classification; (3) conducting a linguistically-motivated comparison\nof different unsupervised measures and (4) correlating these measures with\neducational outcomes. We find that although repetition captures a significant\npart of uptake, pJSD outperforms repetition-based baselines, as it is capable\nof identifying a wider range of uptake phenomena like question answering and\nreformulation. We apply our uptake measure to three different educational\ndatasets with outcome indicators. Unlike baseline measures, pJSD correlates\nsignificantly with instruction quality in all three, providing evidence for its\ngeneralizability and for its potential to serve as an automated professional\ndevelopment tool for teachers.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 18:00:06 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Demszky", "Dorottya", ""], ["Liu", "Jing", ""], ["Mancenido", "Zid", ""], ["Cohen", "Julie", ""], ["Hill", "Heather", ""], ["Jurafsky", "Dan", ""], ["Hashimoto", "Tatsunori", ""]]}, {"id": "2106.03895", "submitter": "Sabrina Mielke", "authors": "Elizabeth Salesky, Badr M. Abdullah, Sabrina J. Mielke, Elena\n  Klyachko, Oleg Serikov, Edoardo Ponti, Ritesh Kumar, Ryan Cotterell,\n  Ekaterina Vylomova", "title": "SIGTYP 2021 Shared Task: Robust Spoken Language Identification", "comments": "The first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While language identification is a fundamental speech and language processing\ntask, for many languages and language families it remains a challenging task.\nFor many low-resource and endangered languages this is in part due to resource\navailability: where larger datasets exist, they may be single-speaker or have\ndifferent domains than desired application scenarios, demanding a need for\ndomain and speaker-invariant language identification systems. This year's\nshared task on robust spoken language identification sought to investigate just\nthis scenario: systems were to be trained on largely single-speaker speech from\none domain, but evaluated on data in other domains recorded from speakers under\ndifferent recording circumstances, mimicking realistic low-resource scenarios.\nWe see that domain and speaker mismatch proves very challenging for current\nmethods which can perform above 95% accuracy in-domain, which domain adaptation\ncan address to some degree, but that these conditions merit further\ninvestigation to make spoken language identification accessible in many\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 18:12:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Salesky", "Elizabeth", ""], ["Abdullah", "Badr M.", ""], ["Mielke", "Sabrina J.", ""], ["Klyachko", "Elena", ""], ["Serikov", "Oleg", ""], ["Ponti", "Edoardo", ""], ["Kumar", "Ritesh", ""], ["Cotterell", "Ryan", ""], ["Vylomova", "Ekaterina", ""]]}, {"id": "2106.03921", "submitter": "Piotr Pi\\k{e}kos", "authors": "Piotr Pi\\k{e}kos, Henryk Michalewski, Mateusz Malinowski", "title": "Measuring and Improving BERT's Mathematical Abilities by Predicting the\n  Order of Reasoning", "comments": "The paper has been accepted to the ACL-IJCNLP 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imagine you are in a supermarket. You have two bananas in your basket and\nwant to buy four apples. How many fruits do you have in total? This seemingly\nstraightforward question can be challenging for data-driven language models,\neven if trained at scale. However, we would expect such generic language models\nto possess some mathematical abilities in addition to typical linguistic\ncompetence. Towards this goal, we investigate if a commonly used language\nmodel, BERT, possesses such mathematical abilities and, if so, to what degree.\nFor that, we fine-tune BERT on a popular dataset for word math problems,\nAQuA-RAT, and conduct several tests to understand learned representations\nbetter. Since we teach models trained on natural language to do formal\nmathematics, we hypothesize that such models would benefit from training on\nsemi-formal steps that explain how math results are derived. To better\naccommodate such training, we also propose new pretext tasks for learning\nmathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or\nNROP). With this new model, we achieve significantly better outcomes than\ndata-driven baselines and even on-par with more tailored models. We also show\nhow to reduce positional bias in such models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 19:08:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Pi\u0119kos", "Piotr", ""], ["Michalewski", "Henryk", ""], ["Malinowski", "Mateusz", ""]]}, {"id": "2106.03952", "submitter": "Shlok Gilda", "authors": "Shlok Gilda, Mirela Silva, Luiz Giovanini, Daniela Oliveira", "title": "Predicting Different Types of Subtle Toxicity in Unhealthy Online\n  Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the use of machine learning models for the\nclassification of unhealthy online conversations containing one or more forms\nof subtler abuse, such as hostility, sarcasm, and generalization. We leveraged\na public dataset of 44K online comments containing healthy and unhealthy\ncomments labeled with seven forms of subtle toxicity. We were able to\ndistinguish between these comments with a top micro F1-score, macro F1-score,\nand ROC-AUC of 88.76%, 67.98%, and 0.71, respectively. Hostile comments were\neasier to detect than other types of unhealthy comments. We also conducted a\nsentiment analysis which revealed that most types of unhealthy comments were\nassociated with a slight negative sentiment, with hostile comments being the\nmost negative ones.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:32:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gilda", "Shlok", ""], ["Silva", "Mirela", ""], ["Giovanini", "Luiz", ""], ["Oliveira", "Daniela", ""]]}, {"id": "2106.03953", "submitter": "Ignacio Tampe Palma", "authors": "Ignacio Tampe Palma, Marcelo Mendoza, and Evangelos Milios", "title": "Neural Abstractive Unsupervised Summarization of Online News Discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Summarization has usually relied on gold standard summaries to train\nextractive or abstractive models. Social media brings a hurdle to summarization\ntechniques since it requires addressing a multi-document multi-author approach.\nWe address this challenging task by introducing a novel method that generates\nabstractive summaries of online news discussions. Our method extends a\nBERT-based architecture, including an attention encoding that fed comments'\nlikes during the training stage. To train our model, we define a task which\nconsists of reconstructing high impact comments based on popularity (likes).\nAccordingly, our model learns to summarize online discussions based on their\nmost relevant comments. Our novel approach provides a summary that represents\nthe most relevant aspects of a news item that users comment on, incorporating\nthe social context as a source of information to summarize texts in online\nsocial networks. Our model is evaluated using ROUGE scores between the\ngenerated summary and each comment on the thread. Our model, including the\nsocial attention encoding, significantly outperforms both extractive and\nabstractive summarization methods based on such evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:33:51 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 01:02:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Palma", "Ignacio Tampe", ""], ["Mendoza", "Marcelo", ""], ["Milios", "Evangelos", ""]]}, {"id": "2106.03958", "submitter": "Vaidehi Patil", "authors": "Yash Khemchandani, Sarvesh Mehtani, Vaidehi Patil, Abhijeet Awasthi,\n  Partha Talukdar, Sunita Sarawagi", "title": "Exploiting Language Relatedness for Low Web-Resource Language Model\n  Adaptation: An Indic Languages Study", "comments": "Accepted to ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in multilingual language models (LM) has demonstrated their\nability to effectively handle multiple languages in a single model. This holds\npromise for low web-resource languages (LRL) as multilingual models can enable\ntransfer of supervision from high resource languages to LRLs. However,\nincorporating a new language in an LM still remains a challenge, particularly\nfor languages with limited corpora and in unseen scripts. In this paper we\nargue that relatedness among languages in a language family may be exploited to\novercome some of the corpora limitations of LRLs, and propose RelateLM. We\nfocus on Indian languages, and exploit relatedness along two dimensions: (1)\nscript (since many Indic scripts originated from the Brahmic script), and (2)\nsentence structure. RelateLM uses transliteration to convert the unseen script\nof limited LRL text into the script of a Related Prominent Language (RPL)\n(Hindi in our case). While exploiting similar sentence structures, RelateLM\nutilizes readily available bilingual dictionaries to pseudo translate RPL text\ninto LRL corpora. Experiments on multiple real-world benchmark datasets provide\nvalidation to our hypothesis that using a related language as pivot, along with\ntransliteration and pseudo translation based data augmentation, can be an\neffective way to adapt LMs for LRLs, rather than direct training or pivoting\nthrough English.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:43:02 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:16:47 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Khemchandani", "Yash", ""], ["Mehtani", "Sarvesh", ""], ["Patil", "Vaidehi", ""], ["Awasthi", "Abhijeet", ""], ["Talukdar", "Partha", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "2106.03973", "submitter": "Debjit Paul", "authors": "Debjit Paul and Anette Frank", "title": "Generating Hypothetical Events for Abductive Inference", "comments": "Proceedings of The Tenth Joint Conference on Lexical and\n  Computational Semantics (STARSEM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abductive reasoning starts from some observations and aims at finding the\nmost plausible explanation for these observations. To perform abduction, humans\noften make use of temporal and causal inferences, and knowledge about how some\nhypothetical situation can result in different outcomes. This work offers the\nfirst study of how such knowledge impacts the Abductive NLI task -- which\nconsists in choosing the more likely explanation for given observations. We\ntrain a specialized language model LMI that is tasked to generate what could\nhappen next from a hypothetical scenario that evolves from a given event. We\nthen propose a multi-task model MTL to solve the Abductive NLI task, which\npredicts a plausible explanation by a) considering different possible events\nemerging from candidate hypotheses -- events generated by LMI -- and b)\nselecting the one that is most similar to the observed outcome. We show that\nour MTL model improves over prior vanilla pre-trained LMs fine-tuned on\nAbductive NLI. Our manual evaluation and analysis suggest that learning about\npossible next events from different hypothetical scenarios supports abductive\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 21:34:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Paul", "Debjit", ""], ["Frank", "Anette", ""]]}, {"id": "2106.03982", "submitter": "Shangmin Guo", "authors": "Shangmin Guo, Yi Ren, Kory Mathewson, Simon Kirby, Stefano V.\n  Albrecht, Kenny Smith", "title": "Expressivity of Emergent Language is a Trade-off between Contextual\n  Complexity and Unpredictability", "comments": "17 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Researchers are now using deep learning models to explore the emergence of\nlanguage in various language games, where simulated agents interact and develop\nan emergent language to solve a task. Although it is quite intuitive that\ndifferent types of language games posing different communicative challenges\nmight require emergent languages which encode different levels of information,\nthere is no existing work exploring the expressivity of the emergent languages.\nIn this work, we propose a definition of partial order between expressivity\nbased on the generalisation performance across different language games. We\nalso validate the hypothesis that expressivity of emergent languages is a\ntrade-off between the complexity and unpredictability of the context those\nlanguages are used in. Our second novel contribution is introducing contrastive\nloss into the implementation of referential games. We show that using our\ncontrastive loss alleviates the collapse of message types seen using standard\nreferential loss functions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 21:57:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Guo", "Shangmin", ""], ["Ren", "Yi", ""], ["Mathewson", "Kory", ""], ["Kirby", "Simon", ""], ["Albrecht", "Stefano V.", ""], ["Smith", "Kenny", ""]]}, {"id": "2106.03983", "submitter": "Hai Hu", "authors": "Hai Hu, He Zhou, Zuoyu Tian, Yiwen Zhang, Yina Ma, Yanting Li, Yixin\n  Nie, Kyle Richardson", "title": "Investigating Transfer Learning in Multilingual Pre-trained Language\n  Models through Chinese Natural Language Inference", "comments": "accepted to ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual transformers (XLM, mT5) have been shown to have remarkable\ntransfer skills in zero-shot settings. Most transfer studies, however, rely on\nautomatically translated resources (XNLI, XQuAD), making it hard to discern the\nparticular linguistic knowledge that is being transferred, and the role of\nexpert annotated monolingual datasets when developing task-specific models. We\ninvestigate the cross-lingual transfer abilities of XLM-R for Chinese and\nEnglish natural language inference (NLI), with a focus on the recent\nlarge-scale Chinese dataset OCNLI. To better understand linguistic transfer, we\ncreated 4 categories of challenge and adversarial tasks (totaling 17 new\ndatasets) for Chinese that build on several well-known resources for English\n(e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on\nEnglish NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our\nchallenge categories, they perform as well/better than the best monolingual\nmodels, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro\ndrop). These results, however, come with important caveats: cross-lingual\nmodels often perform best when trained on a mixture of English and high-quality\nmonolingual NLI data (OCNLI), and are often hindered by automatically\ntranslated resources (XNLI-zh). For many phenomena, all models continue to\nstruggle, highlighting the need for our new diagnostics to help benchmark\nChinese and cross-lingual models. All new datasets/code are released at\nhttps://github.com/huhailinguist/ChineseNLIProbing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:00:18 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hu", "Hai", ""], ["Zhou", "He", ""], ["Tian", "Zuoyu", ""], ["Zhang", "Yiwen", ""], ["Ma", "Yina", ""], ["Li", "Yanting", ""], ["Nie", "Yixin", ""], ["Richardson", "Kyle", ""]]}, {"id": "2106.03993", "submitter": "Ekin Aky\\\"urek", "authors": "Ekin Aky\\\"urek and Jacob Andreas", "title": "Lexicon Learning for Few-Shot Neural Sequence Modeling", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence transduction is the core problem in language processing\napplications as diverse as semantic parsing, machine translation, and\ninstruction following. The neural network models that provide the dominant\nsolution to these problems are brittle, especially in low-resource settings:\nthey fail to generalize correctly or systematically from small datasets. Past\nwork has shown that many failures of systematic generalization arise from\nneural models' inability to disentangle lexical phenomena from syntactic ones.\nTo address this, we augment neural decoders with a lexical translation\nmechanism that generalizes existing copy mechanisms to incorporate learned,\ndecontextualized, token-level translation rules. We describe how to initialize\nthis mechanism using a variety of lexicon learning algorithms, and show that it\nimproves systematic generalization on a diverse set of sequence modeling tasks\ndrawn from cognitive science, formal semantics, and machine translation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:35:04 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Aky\u00fcrek", "Ekin", ""], ["Andreas", "Jacob", ""]]}, {"id": "2106.04016", "submitter": "Aditya Gupta", "authors": "Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, Manaal Faruqui", "title": "Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question\n  Answering", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disfluencies is an under-studied topic in NLP, even though it is ubiquitous\nin human conversation. This is largely due to the lack of datasets containing\ndisfluencies. In this paper, we present a new challenge question answering\ndataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual\ndisfluencies in previously fluent questions. Disfl-QA contains a variety of\nchallenging disfluencies that require a more comprehensive understanding of the\ntext than what was necessary in prior datasets. Experiments show that the\nperformance of existing state-of-the-art question answering models degrades\nsignificantly when tested on Disfl-QA in a zero-shot setting.We show data\naugmentation methods partially recover the loss in performance and also\ndemonstrate the efficacy of using gold data for fine-tuning. We argue that we\nneed large-scale disfluency datasets in order for NLP models to be robust to\nthem. The dataset is publicly available at:\nhttps://github.com/google-research-datasets/disfl-qa.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:03:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gupta", "Aditya", ""], ["Xu", "Jiacheng", ""], ["Upadhyay", "Shyam", ""], ["Yang", "Diyi", ""], ["Faruqui", "Manaal", ""]]}, {"id": "2106.04060", "submitter": "Yong Cheng", "authors": "Yong Cheng, Wei Wang, Lu Jiang, Wolfgang Macherey", "title": "Self-supervised and Supervised Joint Training for Resource-rich Machine\n  Translation", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised pre-training of text representations has been successfully\napplied to low-resource Neural Machine Translation (NMT). However, it usually\nfails to achieve notable gains on resource-rich NMT. In this paper, we propose\na joint training approach, $F_2$-XEnDec, to combine self-supervised and\nsupervised learning to optimize NMT models. To exploit complementary\nself-supervised signals for supervised learning, NMT models are trained on\nexamples that are interbred from monolingual and parallel sentences through a\nnew process called crossover encoder-decoder. Experiments on two resource-rich\ntranslation benchmarks, WMT'14 English-German and WMT'14 English-French,\ndemonstrate that our approach achieves substantial improvements over several\nstrong baseline methods and obtains a new state of the art of 46.19 BLEU on\nEnglish-French when incorporating back translation. Results also show that our\napproach is capable of improving model robustness to input perturbations such\nas code-switching noise which frequently appears on social media.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:35:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cheng", "Yong", ""], ["Wang", "Wei", ""], ["Jiang", "Lu", ""], ["Macherey", "Wolfgang", ""]]}, {"id": "2106.04080", "submitter": "Jacob Parnell", "authors": "Jacob Parnell, Inigo Jauregi Unanue and Massimo Piccardi", "title": "RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation", "comments": "5th Workshop on Structured Prediction for NLP; held in conjunction\n  with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most abstractive summarisation models have relied on variants of the\nnegative log-likelihood (NLL) as their training objective. In some cases,\nreinforcement learning has been added to train the models with an objective\nthat is closer to their evaluation measures (e.g. ROUGE). However, the reward\nfunction to be used within the reinforcement learning approach can play a key\nrole for performance and is still partially unexplored. For this reason, in\nthis paper, we propose two reward functions for the task of abstractive\nsummarisation: the first function, referred to as RwB-Hinge, dynamically\nselects the samples for the gradient update. The second function, nicknamed\nRISK, leverages a small pool of strong candidates to inform the reward. In the\nexperiments, we probe the proposed approach by fine-tuning an NLL pre trained\nmodel over nine summarisation datasets of diverse size and nature. The\nexperimental results show a consistent improvement over the negative\nlog-likelihood baselines.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 03:30:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Parnell", "Jacob", ""], ["Unanue", "Inigo Jauregi", ""], ["Piccardi", "Massimo", ""]]}, {"id": "2106.04081", "submitter": "Tao Na", "authors": "Tao Na, Wei Cheng, Dongming Li, Wanyu Lu, Hongjiang Li", "title": "Insight from NLP Analysis: COVID-19 Vaccines Sentiments on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is an appropriate source for analyzing public attitudes towards\nthe COVID-19 vaccine and various brands. Nevertheless, there are few relevant\nstudies. In the research, we collected tweet posts by the UK and US residents\nfrom the Twitter API during the pandemic and designed experiments to answer\nthree main questions concerning vaccination. To get the dominant sentiment of\nthe civics, we performed sentiment analysis by VADER and proposed a new method\nthat can count the individual's influence. This allows us to go a step further\nin sentiment analysis and explain some of the fluctuations in the data\nchanging. The results indicated that celebrities could lead the opinion shift\non social media in vaccination progress. Moreover, at the peak, nearly 40\\% of\nthe population in both countries have a negative attitude towards COVID-19\nvaccines. Besides, we investigated how people's opinions toward different\nvaccine brands are. We found that the Pfizer vaccine enjoys the most popular\namong people. By applying the sentiment analysis tool, we discovered most\npeople hold positive views toward the COVID-19 vaccine manufactured by most\nbrands. In the end, we carried out topic modelling by using the LDA model. We\nfound residents in the two countries are willing to share their views and\nfeelings concerning the vaccine. Several death cases have occurred after\nvaccination. Due to these negative events, US residents are more worried about\nthe side effects and safety of the vaccine.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 03:37:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Na", "Tao", ""], ["Cheng", "Wei", ""], ["Li", "Dongming", ""], ["Lu", "Wanyu", ""], ["Li", "Hongjiang", ""]]}, {"id": "2106.04098", "submitter": "Hongliang Dai", "authors": "Hongliang Dai, Yangqiu Song, Haixun Wang", "title": "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language\n  Model", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is an effort to extend fine-grained entity typing by using a\nricher and ultra-fine set of types, and labeling noun phrases including\npronouns and nominal nouns instead of just named entity mentions. A key\nchallenge for this ultra-fine entity typing task is that human annotated data\nare extremely scarce, and the annotation ability of existing distant or weak\nsupervision approaches is very limited. To remedy this problem, in this paper,\nwe propose to obtain training data for ultra-fine entity typing by using a BERT\nMasked Language Model (MLM). Given a mention in a sentence, our approach\nconstructs an input for the BERT MLM so that it predicts context dependent\nhypernyms of the mention, which can be used as type labels. Experimental\nresults demonstrate that, with the help of these automatically generated\nlabels, the performance of an ultra-fine entity typing model can be improved\nsubstantially. We also show that our approach can be applied to improve\ntraditional fine-grained entity typing after performing simple type mapping.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:43:28 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Dai", "Hongliang", ""], ["Song", "Yangqiu", ""], ["Wang", "Haixun", ""]]}, {"id": "2106.04102", "submitter": "Mina Lee", "authors": "Mina Lee, Chris Donahue, Robin Jia, Alexander Iyabor, Percy Liang", "title": "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage\n  and Quality", "comments": "Published as a conference paper at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We release a new benchmark for lexical substitution, the task of finding\nappropriate substitutes for a target word in a context. To assist humans with\nwriting, lexical substitution systems can suggest words that humans cannot\neasily think of. However, existing benchmarks depend on human recall as the\nonly source of data, and therefore lack coverage of the substitutes that would\nbe most helpful to humans. Furthermore, annotators often provide substitutes of\nlow quality, which are not actually appropriate in the given context. We\ncollect higher-coverage and higher-quality data by framing lexical substitution\nas a classification problem, guided by the intuition that it is easier for\nhumans to judge the appropriateness of candidate substitutes than conjure them\nfrom memory. To this end, we use a context-free thesaurus to produce candidates\nand rely on human judgement to determine contextual appropriateness. Compared\nto the previous largest benchmark, our Swords benchmark has 4.1x more\nsubstitutes per target word for the same level of quality, and its substitutes\nare 1.5x more appropriate (based on human judgement) for the same number of\nsubstitutes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:58:29 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 18:42:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lee", "Mina", ""], ["Donahue", "Chris", ""], ["Jia", "Robin", ""], ["Iyabor", "Alexander", ""], ["Liang", "Percy", ""]]}, {"id": "2106.04134", "submitter": "Hoang Nguyen Hung Van", "authors": "Hoang Van, Vikas Yadav, Mihai Surdeanu", "title": "Cheap and Good? Simple and Effective Data Augmentation for Low Resource\n  Machine Reading", "comments": "5 pages, 1 figure, SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463099", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and effective strategy for data augmentation for\nlow-resource machine reading comprehension (MRC). Our approach first pretrains\nthe answer extraction components of a MRC system on the augmented data that\ncontains approximate context of the correct answers, before training it on the\nexact answer spans. The approximate context helps the QA method components in\nnarrowing the location of the answers. We demonstrate that our simple strategy\nsubstantially improves both document retrieval and answer extraction\nperformance by providing larger context of the answers and additional training\ndata. In particular, our method significantly improves the performance of BERT\nbased retriever (15.12\\%), and answer extractor (4.33\\% F1) on TechQA, a\ncomplex, low-resource MRC task. Further, our data augmentation strategy yields\nsignificant improvements of up to 3.9\\% exact match (EM) and 2.7\\% F1 for\nanswer extraction on PolicyQA, another practical but moderate sized QA dataset\nthat also contains long answer spans.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:46:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Van", "Hoang", ""], ["Yadav", "Vikas", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "2106.04174", "submitter": "Zijun Yao", "authors": "Zijun Yao, Chengjiang Li, Tiansi Dong, Xin Lv, Jifan Yu, Lei Hou,\n  Juanzi Li, Yichi Zhang, Zelin Dai", "title": "Interpretable and Low-Resource Entity Matching via Decoupling Feature\n  Learning from Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Matching (EM) aims at recognizing entity records that denote the same\nreal-world object. Neural EM models learn vector representation of entity\ndescriptions and match entities end-to-end. Though robust, these methods\nrequire many resources for training, and lack of interpretability. In this\npaper, we propose a novel EM framework that consists of Heterogeneous\nInformation Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple\nfeature representation from matching decision. Using self-supervised learning\nand mask mechanism in pre-trained language modeling, HIF learns the embeddings\nof noisy attribute values by inter-attribute attention with unlabeled data.\nUsing a set of comparison features and a limited amount of annotated data, KAT\nInduction learns an efficient decision tree that can be interpreted by\ngenerating entity matching rules whose structure is advocated by domain\nexperts. Experiments on 6 public datasets and 3 industrial datasets show that\nour method is highly efficient and outperforms SOTA EM models in most cases.\nOur codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:27:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yao", "Zijun", ""], ["Li", "Chengjiang", ""], ["Dong", "Tiansi", ""], ["Lv", "Xin", ""], ["Yu", "Jifan", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Zhang", "Yichi", ""], ["Dai", "Zelin", ""]]}, {"id": "2106.04192", "submitter": "Arie Cattan", "authors": "Arie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar Joshi, Ido Dagan", "title": "Realistic Evaluation Principles for Cross-document Coreference\n  Resolution", "comments": "*SEM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We point out that common evaluation practices for cross-document coreference\nresolution have been unrealistically permissive in their assumed settings,\nyielding inflated results. We propose addressing this issue via two evaluation\nmethodology principles. First, as in other tasks, models should be evaluated on\npredicted mentions rather than on gold mentions. Doing this raises a subtle\nissue regarding singleton coreference clusters, which we address by decoupling\nthe evaluation of mention detection from that of coreference linking. Second,\nwe argue that models should not exploit the synthetic topic structure of the\nstandard ECB+ dataset, forcing models to confront the lexical ambiguity\nchallenge, as intended by the dataset creators. We demonstrate empirically the\ndrastic impact of our more realistic evaluation principles on a competitive\nmodel, yielding a score which is 33 F1 lower compared to evaluating by prior\nlenient practices.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:05:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cattan", "Arie", ""], ["Eirew", "Alon", ""], ["Stanovsky", "Gabriel", ""], ["Joshi", "Mandar", ""], ["Dagan", "Ido", ""]]}, {"id": "2106.04216", "submitter": "Mark Anderson", "authors": "Mark Anderson and Carlos G\\'omez Rodr\\'iguez", "title": "A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021", "comments": "To be published in proceedings of the 17th International Conference\n  on Parsing Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We evaluate three leading dependency parser systems from different paradigms\non a small yet diverse subset of languages in terms of their\naccuracy-efficiency Pareto front. As we are interested in efficiency, we\nevaluate core parsers without pretrained language models (as these are\ntypically huge networks and would constitute most of the compute time) or other\naugmentations that can be transversally applied to any of them. Biaffine\nparsing emerges as a well-balanced default choice, with sequence-labelling\nparsing being preferable if inference speed (but not training energy cost) is\nthe priority.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:55:47 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 09:48:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Anderson", "Mark", ""], ["Rodr\u00edguez", "Carlos G\u00f3mez", ""]]}, {"id": "2106.04222", "submitter": "Mark Anderson", "authors": "Mark Anderson and Mathieu Dehouck and Carlos G\\'omez Rodr\\'iguez", "title": "A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags\n  for Low Resource UD Parsing", "comments": "To be published in proceedings of the 16th International Conference\n  on Parsing Technologies. Earlier versions were rejected at the 16th\n  Conference of the European Chapter of the Association for Computational\n  Linguistics and the 23rd Nordic Conference on Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We evaluate the efficacy of predicted UPOS tags as input features for\ndependency parsers in lower resource settings to evaluate how treebank size\naffects the impact tagging accuracy has on parsing performance. We do this for\nreal low resource universal dependency treebanks, artificially low resource\ndata with varying treebank sizes, and for very small treebanks with varying\namounts of augmented data. We find that predicted UPOS tags are somewhat\nhelpful for low resource treebanks, especially when fewer fully-annotated trees\nare available. We also find that this positive impact diminishes as the amount\nof data increases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 10:04:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Anderson", "Mark", ""], ["Dehouck", "Mathieu", ""], ["Rodr\u00edguez", "Carlos G\u00f3mez", ""]]}, {"id": "2106.04252", "submitter": "Bailin Wang", "authors": "Henry Conklin, Bailin Wang, Kenny Smith and Ivan Titov", "title": "Meta-Learning to Compositionally Generalize", "comments": "ACL2021 Camera Ready; fix a small typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language is compositional; the meaning of a sentence is a function of\nthe meaning of its parts. This property allows humans to create and interpret\nnovel sentences, generalizing robustly outside their prior experience. Neural\nnetworks have been shown to struggle with this kind of generalization, in\nparticular performing poorly on tasks designed to assess compositional\ngeneralization (i.e. where training and testing distributions differ in ways\nthat would be trivial for a compositional strategy to resolve). Their poor\nperformance on these tasks may in part be due to the nature of supervised\nlearning which assumes training and testing data to be drawn from the same\ndistribution. We implement a meta-learning augmented version of supervised\nlearning whose objective directly optimizes for out-of-distribution\ngeneralization. We construct pairs of tasks for meta-learning by sub-sampling\nexisting training data. Each pair of tasks is constructed to contain relevant\nexamples, as determined by a similarity metric, in an effort to inhibit models\nfrom memorizing their input. Experimental results on the COGS and SCAN datasets\nshow that our similarity-driven meta-learning can improve generalization\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:21:48 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 17:00:06 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Conklin", "Henry", ""], ["Wang", "Bailin", ""], ["Smith", "Kenny", ""], ["Titov", "Ivan", ""]]}, {"id": "2106.04258", "submitter": "Roberto Dess\\`i", "authors": "Roberto Dess\\`i, Eugene Kharitonov, Marco Baroni", "title": "Interpretable agent communication from scratch(with a generic visual\n  processor emerging on the side)", "comments": "9 pages main text, 13 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As deep networks begin to be deployed as autonomous agents, the issue of how\nthey can communicate with each other becomes important. Here, we train two deep\nnets from scratch to perform realistic referent identification through\nunsupervised emergent communication. We show that the largely interpretable\nemergent protocol allows the nets to successfully communicate even about object\ntypes they did not see at training time. The visual representations induced as\na by-product of our training regime, moreover, show comparable quality, when\nre-used as generic visual features, to a recent self-supervised learning model.\nOur results provide concrete evidence of the viability of (interpretable)\nemergent deep net communication in a more realistic scenario than previously\nconsidered, as well as establishing an intriguing link between this field and\nself-supervised visual learning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:32:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Dess\u00ec", "Roberto", ""], ["Kharitonov", "Eugene", ""], ["Baroni", "Marco", ""]]}, {"id": "2106.04262", "submitter": "Megha Srivastava", "authors": "Megha Srivastava and Noah Goodman", "title": "Question Generation for Adaptive Education", "comments": "10 pages, 3 figures, ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent and adaptive online education systems aim to make high-quality\neducation available for a diverse range of students. However, existing systems\nusually depend on a pool of hand-made questions, limiting how fine-grained and\nopen-ended they can be in adapting to individual students. We explore targeted\nquestion generation as a controllable sequence generation task. We first show\nhow to fine-tune pre-trained language models for deep knowledge tracing\n(LM-KT). This model accurately predicts the probability of a student answering\na question correctly, and generalizes to questions not seen in training. We\nthen use LM-KT to specify the objective and data for training a model to\ngenerate questions conditioned on the student and target difficulty. Our\nresults show we succeed at generating novel, well-calibrated language\ntranslation questions for second language learners from a real online education\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:46:59 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Srivastava", "Megha", ""], ["Goodman", "Noah", ""]]}, {"id": "2106.04279", "submitter": "Sainbayar Sukhbaatar", "authors": "Da Ju, Stephen Roller, Sainbayar Sukhbaatar, Jason Weston", "title": "Staircase Attention for Recurrent Processing of Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have become a standard tool for sequence modeling tasks,\nin particular by stacking self-attention layers over the entire input sequence\nas in the Transformer architecture. In this work we introduce a novel attention\nprocedure called staircase attention that, unlike self-attention, operates\nacross the sequence (in time) recurrently processing the input by adding\nanother step of processing. A step in the staircase comprises of backward\ntokens (encoding the sequence so far seen) and forward tokens (ingesting a new\npart of the sequence), or an extreme Ladder version with a forward step of zero\nthat simply repeats the Transformer on each step of the ladder, sharing the\nweights. We thus describe a family of such models that can trade off\nperformance and compute, by either increasing the amount of recurrence through\ntime, the amount of sequential processing via recurrence in depth, or both.\nStaircase attention is shown to be able to solve tasks that involve tracking\nthat conventional Transformers cannot, due to this recurrence. Further, it is\nshown to provide improved modeling power for the same size model (number of\nparameters) compared to self-attentive Transformers on large language modeling\nand dialogue tasks, yielding significant perplexity gains.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:19:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ju", "Da", ""], ["Roller", "Stephen", ""], ["Sukhbaatar", "Sainbayar", ""], ["Weston", "Jason", ""]]}, {"id": "2106.04298", "submitter": "Marcely Zanon Boito", "authors": "Marcely Zanon Boito, Bolaji Yusuf, Lucas Ondel, Aline Villavicencio,\n  Laurent Besacier", "title": "Unsupervised Word Segmentation from Discrete Speech Units in\n  Low-Resource Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When documenting oral-languages, Unsupervised Word Segmentation (UWS) from\nspeech is a useful, yet challenging, task. It can be performed from phonetic\ntranscriptions, or in the absence of these, from the output of unsupervised\nspeech discretization models. These discretization models are trained using raw\nspeech only, producing discrete speech units which can be applied for\ndownstream (text-based) tasks. In this paper we compare five of these models:\nthree Bayesian and two neural approaches, with regards to the exploitability of\nthe produced units for UWS. Two UWS models are experimented with and we report\nresults for Finnish, Hungarian, Mboshi, Romanian and Russian in a low-resource\nsetting (using only 5k sentences). Our results suggest that neural models for\nspeech discretization are difficult to exploit in our setting, and that it\nmight be necessary to adapt them to limit sequence length. We obtain our best\nUWS results by using the SHMM and H-SHMM Bayesian models, which produce high\nquality, yet compressed, discrete representations of the input speech signal.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:50:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Boito", "Marcely Zanon", ""], ["Yusuf", "Bolaji", ""], ["Ondel", "Lucas", ""], ["Villavicencio", "Aline", ""], ["Besacier", "Laurent", ""]]}, {"id": "2106.04300", "submitter": "Hang Yan", "authors": "Hang Yan, Junqi Dai, Tuo ji, Xipeng Qiu, Zheng Zhang", "title": "A Unified Generative Framework for Aspect-Based Sentiment Analysis", "comments": "Accepted by ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms,\ntheir corresponding sentiment polarities, and the opinion terms. There exist\nseven subtasks in ABSA. Most studies only focus on the subsets of these\nsubtasks, which leads to various complicated ABSA models while hard to solve\nthese subtasks in a unified framework. In this paper, we redefine every subtask\ntarget as a sequence mixed by pointer indexes and sentiment class indexes,\nwhich converts all ABSA subtasks into a unified generative formulation. Based\non the unified formulation, we exploit the pre-training sequence-to-sequence\nmodel BART to solve all ABSA subtasks in an end-to-end framework. Extensive\nexperiments on four ABSA datasets for seven subtasks demonstrate that our\nframework achieves substantial performance gain and provides a real unified\nend-to-end solution for the whole ABSA subtasks, which could benefit multiple\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:55:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yan", "Hang", ""], ["Dai", "Junqi", ""], ["ji", "Tuo", ""], ["Qiu", "Xipeng", ""], ["Zhang", "Zheng", ""]]}, {"id": "2106.04302", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta and Martin Jaggi", "title": "Obtaining Better Static Word Embeddings Using Contextual Embedding\n  Models", "comments": "ACL 2021 accept", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of contextual word embeddings -- representations of words which\nincorporate semantic and syntactic information from their context -- has led to\ntremendous improvements on a wide variety of NLP tasks. However, recent\ncontextual models have prohibitively high computational cost in many use-cases\nand are often hard to interpret. In this work, we demonstrate that our proposed\ndistillation method, which is a simple extension of CBOW-based training, allows\nto significantly improve computational efficiency of NLP applications, while\noutperforming the quality of existing static embeddings trained from scratch as\nwell as those distilled from previously proposed methods. As a side-effect, our\napproach also allows a fair comparison of both contextual and static embeddings\nvia standard lexical evaluation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:59:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gupta", "Prakhar", ""], ["Jaggi", "Martin", ""]]}, {"id": "2106.04311", "submitter": "Sebastien Montella", "authors": "Sebastien Montella, Lina Rojas-Barahona, Johannes Heinecke", "title": "Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time\n  Curvatures", "comments": "Accepted to Findings of ACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph (KG) completion has been excessively studied with a massive\nnumber of models proposed for the Link Prediction (LP) task. The main\nlimitation of such models is their insensitivity to time. Indeed, the temporal\naspect of stored facts is often ignored. To this end, more and more works\nconsider time as a parameter to complete KGs. In this paper, we first\ndemonstrate that, by simply increasing the number of negative samples, the\nrecent AttH model can achieve competitive or even better performance than the\nstate-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further\npropose Hercules, a time-aware extension of AttH model, which defines the\ncurvature of a Riemannian manifold as the product of both relation and time.\nOur experiments show that both Hercules and AttH achieve competitive or new\nstate-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore,\none should raise awareness when learning TKGs representations to identify\nwhether time truly boosts performances.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:13:43 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Montella", "Sebastien", ""], ["Rojas-Barahona", "Lina", ""], ["Heinecke", "Johannes", ""]]}, {"id": "2106.04383", "submitter": "Jaafar Hammoud", "authors": "Jaafar Hammoud and Ali Eisa and Natalia Dobrenko and Natalia Gusarova", "title": "Using a New Nonlinear Gradient Method for Solving Large Scale Convex\n  Optimization Problems with an Application on Arabic Medical Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradient methods have applications in multiple fields, including signal\nprocessing, image processing, and dynamic systems. In this paper, we present a\nnonlinear gradient method for solving convex supra-quadratic functions by\ndeveloping the search direction, that done by hybridizing between the two\nconjugate coefficients HRM [2] and NHS [1]. The numerical results proved the\neffectiveness of the presented method by applying it to solve standard problems\nand reaching the exact solution if the objective function is quadratic convex.\nAlso presented in this article, an application to the problem of named entities\nin the Arabic medical language, as it proved the stability of the proposed\nmethod and its efficiency in terms of execution time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:13:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 12:27:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hammoud", "Jaafar", ""], ["Eisa", "Ali", ""], ["Dobrenko", "Natalia", ""], ["Gusarova", "Natalia", ""]]}, {"id": "2106.04398", "submitter": "Jonas Groschwitz", "authors": "Jonas Groschwitz, Meaghan Fowlie and Alexander Koller", "title": "Learning compositional structures for semantic graph parsing", "comments": "Accepted at the 5th Workshop on Structured Prediction for NLP\n  (http://structuredprediction.github.io/SPNLP21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AM dependency parsing is a method for neural semantic graph parsing that\nexploits the principle of compositionality. While AM dependency parsers have\nbeen shown to be fast and accurate across several graphbanks, they require\nexplicit annotations of the compositional tree structures for training. In the\npast, these were obtained using complex graphbank-specific heuristics written\nby experts. Here we show how they can instead be trained directly on the graphs\nwith a neural latent-variable model, drastically reducing the amount and\ncomplexity of manual heuristics. We demonstrate that our model picks up on\nseveral linguistic phenomena on its own and achieves comparable accuracy to\nsupervised training, greatly facilitating the use of AM dependency parsing for\nnew sembanks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:20:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Groschwitz", "Jonas", ""], ["Fowlie", "Meaghan", ""], ["Koller", "Alexander", ""]]}, {"id": "2106.04403", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Ioannis Kazakos, Carles Ventura, Miriam Bellver, Carina Silberer and\n  Xavier Giro-i-Nieto", "title": "SynthRef: Generation of Synthetic Referring Expressions for Object\n  Segmentation", "comments": "Accepted as poster at the NAACL 2021 Visually Grounded Interaction\n  and Language (ViGIL) Workshop. 4 pages. Project website:\n  https://imatge-upc.github.io/synthref/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in deep learning have brought significant progress in visual\ngrounding tasks such as language-guided video object segmentation. However,\ncollecting large datasets for these tasks is expensive in terms of annotation\ntime, which represents a bottleneck. To this end, we propose a novel method,\nnamely SynthRef, for generating synthetic referring expressions for target\nobjects in an image (or video frame), and we also present and disseminate the\nfirst large-scale dataset with synthetic referring expressions for video object\nsegmentation. Our experiments demonstrate that by training with our synthetic\nreferring expressions one can improve the ability of a model to generalize\nacross different datasets, without any additional annotation cost. Moreover,\nour formulation allows its application to any object detection or segmentation\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:28:13 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:39:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kazakos", "Ioannis", ""], ["Ventura", "Carles", ""], ["Bellver", "Miriam", ""], ["Silberer", "Carina", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2106.04426", "submitter": "Jason  Weston", "authors": "Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston", "title": "Hash Layers For Large Sparse Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:54:24 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 15:30:23 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 13:46:33 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Roller", "Stephen", ""], ["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Weston", "Jason", ""]]}, {"id": "2106.04437", "submitter": "Ziqing Yang", "authors": "Ziqing Yang, Yiming Cui, Chenglei Si, Wanxiang Che, Ting Liu, Shijin\n  Wang, Guoping Hu", "title": "Adversarial Training for Machine Reading Comprehension with Virtual\n  Embeddings", "comments": "Accepted to *SEM 2021 workshop at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) as a regularization method has proved its\neffectiveness on various tasks. Though there are successful applications of AT\non some NLP tasks, the distinguishing characteristics of NLP tasks have not\nbeen exploited. In this paper, we aim to apply AT on machine reading\ncomprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing\na novel adversarial training method called PQAT that perturbs the embedding\nmatrix instead of word vectors. To differentiate the roles of passages and\nquestions, PQAT uses additional virtual P/Q-embedding matrices to gather the\nglobal perturbations of words from passages and questions separately. We test\nthe method on a wide range of MRC tasks, including span-based extractive RC and\nmultiple-choice RC. The results show that adversarial training is effective\nuniversally, and PQAT further improves the performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:16:34 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yang", "Ziqing", ""], ["Cui", "Yiming", ""], ["Si", "Chenglei", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2106.04441", "submitter": "Feifei Pan", "authors": "Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, Peter Fox", "title": "CLTR: An End-to-End, Transformer-Based System for Cell Level Table\n  Retrieval and Table Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the first end-to-end, transformer-based table question answering\n(QA) system that takes natural language questions and massive table corpus as\ninputs to retrieve the most relevant tables and locate the correct table cells\nto answer the question. Our system, CLTR, extends the current state-of-the-art\nQA over tables model to build an end-to-end table QA architecture. This system\nhas successfully tackled many real-world table QA problems with a simple,\nunified pipeline. Our proposed system can also generate a heatmap of candidate\ncolumns and rows over complex tables and allow users to quickly identify the\ncorrect cells to answer questions. In addition, we introduce two new\nopen-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural\nlanguage questions over 76,242 tables. The benchmarks are designed to validate\nCLTR as well as accommodate future table retrieval and end-to-end table QA\nresearch and experiments. Our experiments demonstrate that our system is the\ncurrent state-of-the-art model on the table retrieval task and produces\npromising results for end-to-end table QA.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:22:10 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:09:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pan", "Feifei", ""], ["Canim", "Mustafa", ""], ["Glass", "Michael", ""], ["Gliozzo", "Alfio", ""], ["Fox", "Peter", ""]]}, {"id": "2106.04447", "submitter": "Gabriel Orlanski", "authors": "Gabriel Orlanski and Alex Gittens", "title": "Reading StackOverflow Encourages Cheating: Adding Question Text Improves\n  Extractive Code Generation", "comments": "To be published in ACL-IJCNLP NLP4Prog workshop. (The First Workshop\n  on Natural Language Processing for Programming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering a programming question using only its title is difficult as salient\ncontextual information is omitted. Based on this observation, we present a\ncorpus of over 40,000 StackOverflow question texts to be used in conjunction\nwith their corresponding intents from the CoNaLa dataset (Yin et al., 2018).\nUsing both the intent and question body, we use BART to establish a baseline\nBLEU score of 34.35 for this new task. We find further improvements of $2.8\\%$\nby combining the mined CoNaLa data with the labeled data to achieve a 35.32\nBLEU score. We evaluate prior state-of-the-art CoNaLa models with this\nadditional data and find that our proposed method of using the body and mined\ndata beats the BLEU score of the prior state-of-the-art by $71.96\\%$. Finally,\nwe perform ablations to demonstrate that BART is an unsupervised multimodal\nlearner and examine its extractive behavior. The code and data can be found\nhttps://github.com/gabeorlanski/stackoverflow-encourages-cheating.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:28:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Orlanski", "Gabriel", ""], ["Gittens", "Alex", ""]]}, {"id": "2106.04476", "submitter": "Marco Damonte", "authors": "Marco Damonte, Emilio Monti", "title": "One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task\n  Learning on Semantic Parsing Datasets", "comments": null, "journal-ref": "The Tenth Joint Conference on Lexical and Computational Semantics\n  (*SEM 2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic parsers map natural language utterances to meaning representations.\nThe lack of a single standard for meaning representations led to the creation\nof a plethora of semantic parsing datasets. To unify different datasets and\ntrain a single model for them, we investigate the use of Multi-Task Learning\n(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,\nOvernight, AMR). We find that an MTL architecture that shares the entire\nnetwork across datasets yields competitive or better parsing accuracies than\nthe single-task baselines, while reducing the total number of parameters by\n68%. We further provide evidence that MTL has also better compositional\ngeneralization than single-task models. We also present a comparison of task\nsampling methods and propose a competitive alternative to widespread\nproportional sampling strategies.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:03:42 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 10:24:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Damonte", "Marco", ""], ["Monti", "Emilio", ""]]}, {"id": "2106.04484", "submitter": "Daniel Rosenberg", "authors": "Daniel Rosenberg, Itai Gat, Amir Feder, Roi Reichart", "title": "Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused\n  Interventions", "comments": "ACL 2021. Our code and data are available at\n  https://danrosenberg.github.io/rad-measure/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:09:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Rosenberg", "Daniel", ""], ["Gat", "Itai", ""], ["Feder", "Amir", ""], ["Reichart", "Roi", ""]]}, {"id": "2106.04489", "submitter": "Rabeeh Karimi Mahabadi", "authors": "Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, James\n  Henderson", "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared\n  Hypernetworks", "comments": "accepted in ACL, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art parameter-efficient fine-tuning methods rely on introducing\nadapter modules between the layers of a pretrained language model. However,\nsuch modules are trained separately for each task and thus do not enable\nsharing information across tasks. In this paper, we show that we can learn\nadapter parameters for all layers and tasks by generating them using shared\nhypernetworks, which condition on task, adapter position, and layer id in a\ntransformer model. This parameter-efficient multi-task learning framework\nallows us to achieve the best of both worlds by sharing knowledge across tasks\nvia hypernetworks while enabling the model to adapt to each individual task\nthrough task-specific adapters. Experiments on the well-known GLUE benchmark\nshow improved performance in multi-task learning while adding only 0.29%\nparameters per task. We additionally demonstrate substantial performance\nimprovements in few-shot domain generalization across a variety of tasks. Our\ncode is publicly available in https://github.com/rabeehk/hyperformer.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:16:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mahabadi", "Rabeeh Karimi", ""], ["Ruder", "Sebastian", ""], ["Dehghani", "Mostafa", ""], ["Henderson", "James", ""]]}, {"id": "2106.04506", "submitter": "Faisal Bin Ashraf", "authors": "Md Faisal Ahmed, Zalish Mahmud, Zarin Tasnim Biash, Ahmed Ann Noor\n  Ryen, Arman Hossain, Faisal Bin Ashraf", "title": "Cyberbullying Detection Using Deep Neural Network from Social Media\n  Comments in Bangla Language", "comments": "9 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cyberbullying or Online harassment detection on social media for various\nmajor languages is currently being given a good amount of focus by researchers\nworldwide. Being the seventh most speaking language in the world and increasing\nusage of online platform among the Bengali speaking people urge to find\neffective detection technique to handle the online harassment. In this paper,\nwe have proposed binary and multiclass classification model using hybrid neural\nnetwork for bully expression detection in Bengali language. We have used 44,001\nusers comments from popular public Facebook pages, which fall into five classes\n- Non-bully, Sexual, Threat, Troll and Religious. We have examined the\nperformance of our proposed models from different perspective. Our binary\nclassification model gives 87.91% accuracy, whereas introducing ensemble\ntechnique after neural network for multiclass classification, we got 85%\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:47:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ahmed", "Md Faisal", ""], ["Mahmud", "Zalish", ""], ["Biash", "Zarin Tasnim", ""], ["Ryen", "Ahmed Ann Noor", ""], ["Hossain", "Arman", ""], ["Ashraf", "Faisal Bin", ""]]}, {"id": "2106.04554", "submitter": "Tianyang Lin", "authors": "Tianyang Lin, Yuxin Wang, Xiangyang Liu, Xipeng Qiu", "title": "A Survey of Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have achieved great success in many artificial intelligence\nfields, such as natural language processing, computer vision, and audio\nprocessing. Therefore, it is natural to attract lots of interest from academic\nand industry researchers. Up to the present, a great variety of Transformer\nvariants (a.k.a. X-formers) have been proposed, however, a systematic and\ncomprehensive literature review on these Transformer variants is still missing.\nIn this survey, we provide a comprehensive review of various X-formers. We\nfirst briefly introduce the vanilla Transformer and then propose a new taxonomy\nof X-formers. Next, we introduce the various X-formers from three perspectives:\narchitectural modification, pre-training, and applications. Finally, we outline\nsome potential directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:43:08 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:56:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lin", "Tianyang", ""], ["Wang", "Yuxin", ""], ["Liu", "Xiangyang", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2106.04559", "submitter": "Peng Xu", "authors": "Peng Xu, Wenjie Zi, Hamidreza Shahidi, \\'Akos K\\'ad\\'ar, Keyi Tang,\n  Wei Yang, Jawad Ateeq, Harsh Barot, Meidan Alon, Yanshuai Cao", "title": "Turing: an Accurate and Interpretable Multi-Hypothesis Cross-Domain\n  Natural Language Database Interface", "comments": "ACL 2021 demonstration track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A natural language database interface (NLDB) can democratize data-driven\ninsights for non-technical users. However, existing Text-to-SQL semantic\nparsers cannot achieve high enough accuracy in the cross-database setting to\nallow good usability in practice. This work presents Turing, a NLDB system\ntoward bridging this gap. The cross-domain semantic parser of Turing with our\nnovel value prediction method achieves $75.1\\%$ execution accuracy, and\n$78.3\\%$ top-5 beam execution accuracy on the Spider validation set. To benefit\nfrom the higher beam accuracy, we design an interactive system where the SQL\nhypotheses in the beam are explained step-by-step in natural language, with\ntheir differences highlighted. The user can then compare and judge the\nhypotheses to select which one reflects their intention if any. The English\nexplanations of SQL queries in Turing are produced by our high-precision\nnatural language generation system based on synchronous grammars.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:46:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xu", "Peng", ""], ["Zi", "Wenjie", ""], ["Shahidi", "Hamidreza", ""], ["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Tang", "Keyi", ""], ["Yang", "Wei", ""], ["Ateeq", "Jawad", ""], ["Barot", "Harsh", ""], ["Alon", "Meidan", ""], ["Cao", "Yanshuai", ""]]}, {"id": "2106.04563", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Ahmed Hassan Awadallah, Jianfeng Gao", "title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation", "comments": "Code and checkpoints released (links in draft)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep and large pre-trained models are the state-of-the-art for various\nnatural language processing tasks, their huge size poses significant challenges\nfor practical uses in resource constrained settings. Recent works in knowledge\ndistillation propose task-agnostic as well as task-specific methods to compress\nthese models, with task-specific ones often yielding higher compression rate.\nIn this work, we develop a new task-agnostic distillation framework\nXtremeDistilTransformers that leverages the advantage of task-specific methods\nfor learning a small universal model that can be applied to arbitrary tasks and\nlanguages. To this end, we study the transferability of several source tasks,\naugmentation resources and model architecture for distillation. We evaluate our\nmodel performance on multiple tasks, including the General Language\nUnderstanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and\na massive multi-lingual NER dataset with 41 languages. We release three\ndistilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters\nobtaining SOTA performance in several tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:49:33 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 03:59:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Awadallah", "Ahmed Hassan", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2106.04564", "submitter": "Jianguo Zhang", "authors": "Jian-Guo Zhang, Kazuma Hashimoto, Yao Wan, Ye Liu, Caiming Xiong,\n  Philip S. Yu", "title": "Are Pretrained Transformers Robust in Intent Classification? A Missing\n  Ingredient in Evaluation of Out-of-Scope Intent Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained Transformer-based models were reported to be robust in intent\nclassification. In this work, we first point out the importance of in-domain\nout-of-scope detection in few-shot intent recognition tasks and then illustrate\nthe vulnerability of pretrained Transformer-based models against samples that\nare in-domain but out-of-scope (ID-OOS). We empirically show that pretrained\nmodels do not perform well on both ID-OOS examples and general out-of-scope\nexamples, especially on fine-grained few-shot intent detection tasks. To figure\nout how the models mistakenly classify ID-OOS intents as in-scope intents, we\nfurther conduct analysis on confidence scores and the overlapping keywords and\nprovide several prospective directions for future work. We release the relevant\nresources to facilitate future research.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:51:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Jian-Guo", ""], ["Hashimoto", "Kazuma", ""], ["Wan", "Yao", ""], ["Liu", "Ye", ""], ["Xiong", "Caiming", ""], ["Yu", "Philip S.", ""]]}, {"id": "2106.04565", "submitter": "Juri Opitz", "authors": "Sarah Uhrig, Yoalli Rezepka Garcia, Juri Opitz, Anette Frank", "title": "Translate, then Parse! A strong baseline for Cross-Lingual AMR Parsing", "comments": "IWPT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers\ndevelop models that project sentences from various languages onto their AMRs to\ncapture their essential semantic structures: given a sentence in any language,\nwe aim to capture its core semantic content through concepts connected by\nmanifold types of semantic relations. Methods typically leverage large silver\ntraining data to learn a single model that is able to project non-English\nsentences to AMRs. However, we find that a simple baseline tends to be\nover-looked: translating the sentences to English and projecting their AMR with\na monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this\nsimple two-step base-line, and enhance it with a strong NMT system and a strong\nAMR parser. Our experiments show that T+P outperforms a recent state-of-the-art\nsystem across all tested languages: German, Italian, Spanish and Mandarin with\n+14.6, +12.6, +14.3 and +16.0 Smatch points.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:52:48 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Uhrig", "Sarah", ""], ["Garcia", "Yoalli Rezepka", ""], ["Opitz", "Juri", ""], ["Frank", "Anette", ""]]}, {"id": "2106.04570", "submitter": "Canwen Xu", "authors": "Wangchunshu Zhou and Canwen Xu and Julian McAuley", "title": "Meta Learning for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Meta Learning for Knowledge Distillation (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models. The code is available at\nhttps://github.com/JetRunner/MetaDistil\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:59:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhou", "Wangchunshu", ""], ["Xu", "Canwen", ""], ["McAuley", "Julian", ""]]}, {"id": "2106.04571", "submitter": "Lianhui Qin", "authors": "Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi and\n  Manaal Faruqui", "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Everyday conversations require understanding everyday events, which in turn,\nrequires understanding temporal commonsense concepts interwoven with those\nevents. Despite recent progress with massive pre-trained language models (LMs)\nsuch as T5 and GPT-3, their capability of temporal reasoning in dialogs remains\nlargely under-explored. In this paper, we present the first study to\ninvestigate pre-trained LMs for their temporal reasoning capabilities in\ndialogs by introducing a new task and a crowd-sourced English challenge set,\nTIMEDIAL. We formulate TIME-DIAL as a multiple-choice cloze task with over 1.1K\ncarefully curated dialogs. Empirical results demonstrate that even the best\nperforming models struggle on this task compared to humans, with 23 absolute\npoints of gap in accuracy. Furthermore, our analysis reveals that the models\nfail to reason about dialog context correctly; instead, they rely on shallow\ncues based on existing temporal patterns in context, motivating future research\nfor modeling temporal concepts in text and robust contextual reasoning about\nthem. The dataset is publicly available at:\nhttps://github.com/google-research-datasets/timedial.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:59:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Qin", "Lianhui", ""], ["Gupta", "Aditya", ""], ["Upadhyay", "Shyam", ""], ["He", "Luheng", ""], ["Choi", "Yejin", ""], ["Faruqui", "Manaal", ""]]}, {"id": "2106.04612", "submitter": "Shauli Ravfogel", "authors": "Shauli Ravfogel, Hillel Taub-Tabib, Yoav Goldberg", "title": "Neural Extractive Search", "comments": "Accepted as a demo paper in ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain experts often need to extract structured information from large\ncorpora. We advocate for a search paradigm called ``extractive search'', in\nwhich a search query is enriched with capture-slots, to allow for such rapid\nextraction. Such an extractive search system can be built around syntactic\nstructures, resulting in high-precision, low-recall results. We show how the\nrecall can be improved using neural retrieval and alignment. The goals of this\npaper are to concisely introduce the extractive-search paradigm; and to\ndemonstrate a prototype neural retrieval system for extractive search and its\nbenefits and potential. Our prototype is available at\n\\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is\navailable at \\url{https://vimeo.com/559586687}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:03:31 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ravfogel", "Shauli", ""], ["Taub-Tabib", "Hillel", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2106.04630", "submitter": "Rongmei Lin", "authors": "Rongmei Lin, Xiang He, Jie Feng, Nasser Zalmout, Yan Liang, Li Xiong,\n  Xin Luna Dong", "title": "PAM: Understanding Product Images in Cross Product Category Attribute\n  Extraction", "comments": "KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467164", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding product attributes plays an important role in improving online\nshopping experience for customers and serves as an integral part for\nconstructing a product knowledge graph. Most existing methods focus on\nattribute extraction from text description or utilize visual information from\nproduct images such as shape and color. Compared to the inputs considered in\nprior works, a product image in fact contains more information, represented by\na rich mixture of words and visual clues with a layout carefully designed to\nimpress customers. This work proposes a more inclusive framework that fully\nutilizes these different modalities for attribute extraction. Inspired by\nrecent works in visual question answering, we use a transformer based sequence\nto sequence model to fuse representations of product text, Optical Character\nRecognition (OCR) tokens and visual objects detected in the product image. The\nframework is further extended with the capability to extract attribute value\nacross multiple product categories with a single model, by training the decoder\nto predict both product category and attribute value and conditioning its\noutput on product category. The model provides a unified attribute extraction\nsolution desirable at an e-commerce platform that offers numerous product\ncategories with a diverse body of product attributes. We evaluated the model on\ntwo product attributes, one with many possible values and one with a small set\nof possible values, over 14 product categories and found the model could\nachieve 15% gain on the Recall and 10% gain on the F1 score compared to\nexisting methods using text-only features.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:30:17 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lin", "Rongmei", ""], ["He", "Xiang", ""], ["Feng", "Jie", ""], ["Zalmout", "Nasser", ""], ["Liang", "Yan", ""], ["Xiong", "Li", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2106.04631", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar, Michele Donini, Dylan Slack, C\\'edric\n  Archambeau, Sanjiv Das, Krishnaram Kenthapadi", "title": "On the Lack of Robust Interpretability of Neural Text Classifiers", "comments": "Appearing at ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing complexity of neural language models, practitioners\nhave turned to methods for understanding the predictions of these models. One\nof the most well-adopted approaches for model interpretability is feature-based\ninterpretability, i.e., ranking the features in terms of their impact on model\npredictions. Several prior studies have focused on assessing the fidelity of\nfeature-based interpretability methods, i.e., measuring the impact of dropping\nthe top-ranked features on the model output. However, relatively little work\nhas been conducted on quantifying the robustness of interpretations. In this\nwork, we assess the robustness of interpretations of neural text classifiers,\nspecifically, those based on pretrained Transformer encoders, using two\nrandomization tests. The first compares the interpretations of two models that\nare identical except for their initializations. The second measures whether the\ninterpretations differ between a model with trained parameters and a model with\nrandom parameters. Both tests show surprising deviations from expected\nbehavior, raising questions about the extent of insights that practitioners may\ndraw from interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:31:02 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Donini", "Michele", ""], ["Slack", "Dylan", ""], ["Archambeau", "C\u00e9dric", ""], ["Das", "Sanjiv", ""], ["Kenthapadi", "Krishnaram", ""]]}, {"id": "2106.04632", "submitter": "Linjie Li", "authors": "Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai,\n  Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara Lee Berg,\n  Mohit Bansal, Jingjing Liu, Lijuan Wang, Zicheng Liu", "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding\n  Evaluation", "comments": "VALUE is available at https://value-leaderboard.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-leaderboard.github.io/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:34:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Li", "Linjie", ""], ["Lei", "Jie", ""], ["Gan", "Zhe", ""], ["Yu", "Licheng", ""], ["Chen", "Yen-Chun", ""], ["Pillai", "Rohit", ""], ["Cheng", "Yu", ""], ["Zhou", "Luowei", ""], ["Wang", "Xin Eric", ""], ["Wang", "William Yang", ""], ["Berg", "Tamara Lee", ""], ["Bansal", "Mohit", ""], ["Liu", "Jingjing", ""], ["Wang", "Lijuan", ""], ["Liu", "Zicheng", ""]]}, {"id": "2106.04641", "submitter": "Shohreh Shaghaghian Ms", "authors": "Nicolai Pogrebnyakov, Shohreh Shaghaghian", "title": "Predicting the Success of Domain Adaptation in Text Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:02:15 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 19:19:40 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pogrebnyakov", "Nicolai", ""], ["Shaghaghian", "Shohreh", ""]]}, {"id": "2106.04647", "submitter": "Rabeeh Karimi Mahabadi", "authors": "Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers.\n  Specifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared ``slow'' weights and ``fast'' rank-one\nmatrices defined per Compacter layer. By only training 0.047% of a pretrained\nmodel's parameters, Compacter performs on par with standard fine-tuning on GLUE\nand outperforms fine-tuning in low-resource settings. Our code is publicly\navailable in https://github.com/rabeehk/compacter/\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:17:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mahabadi", "Rabeeh Karimi", ""], ["Henderson", "James", ""], ["Ruder", "Sebastian", ""]]}, {"id": "2106.04653", "submitter": "Pritish Sahu", "authors": "Pritish Sahu, Michael Cogswell, Sara Rutherford-Quach, Ajay Divakaran", "title": "Comprehension Based Question Answering using Bloom's Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current pre-trained language models have lots of knowledge, but a more\nlimited ability to use that knowledge. Bloom's Taxonomy helps educators teach\nchildren how to use knowledge by categorizing comprehension skills, so we use\nit to analyze and improve the comprehension skills of large pre-trained\nlanguage models. Our experiments focus on zero-shot question answering, using\nthe taxonomy to provide proximal context that helps the model answer questions\nby being relevant to those questions. We show targeting context in this manner\nimproves performance across 4 popular common sense question answer datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:32:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sahu", "Pritish", ""], ["Cogswell", "Michael", ""], ["Rutherford-Quach", "Sara", ""], ["Divakaran", "Ajay", ""]]}, {"id": "2106.04660", "submitter": "Anderson Avila", "authors": "Yiran Cao, Nihal Potdar, Anderson R. Avila", "title": "Sequential End-to-End Intent and Slot Label Classification and\n  Localization", "comments": "Accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-computer interaction (HCI) is significantly impacted by delayed\nresponses from a spoken dialogue system. Hence, end-to-end (e2e) spoken\nlanguage understanding (SLU) solutions have recently been proposed to decrease\nlatency. Such approaches allow for the extraction of semantic information\ndirectly from the speech signal, thus bypassing the need for a transcript from\nan automatic speech recognition (ASR) system. In this paper, we propose a\ncompact e2e SLU architecture for streaming scenarios, where chunks of the\nspeech signal are processed continuously to predict intent and slot values. Our\nmodel is based on a 3D convolutional neural network (3D-CNN) and a\nunidirectional long short-term memory (LSTM). We compare the performance of two\nalignment-free losses: the connectionist temporal classification (CTC) method\nand its adapted version, namely connectionist temporal localization (CTL). The\nlatter performs not only the classification but also localization of sequential\naudio events. The proposed solution is evaluated on the Fluent Speech Command\ndataset and results show our model ability to process incoming speech signal,\nreaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on\nsingle-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL\non two-label prediction.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:53:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cao", "Yiran", ""], ["Potdar", "Nihal", ""], ["Avila", "Anderson R.", ""]]}, {"id": "2106.04681", "submitter": "Djamila Romaissa Beddiar", "authors": "Djamila Romaissa Beddiar and Md Saroar Jahan and Mourad Oussalah", "title": "Data Expansion using Back Translation and Paraphrasing for Hate Speech\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With proliferation of user generated contents in social media platforms,\nestablishing mechanisms to automatically identify toxic and abusive content\nbecomes a prime concern for regulators, researchers, and society. Keeping the\nbalance between freedom of speech and respecting each other dignity is a major\nconcern of social media platform regulators. Although, automatic detection of\noffensive content using deep learning approaches seems to provide encouraging\nresults, training deep learning-based models requires large amounts of\nhigh-quality labeled data, which is often missing. In this regard, we present\nin this paper a new deep learning-based method that fuses a Back Translation\nmethod, and a Paraphrasing technique for data augmentation. Our pipeline\ninvestigates different word-embedding-based architectures for classification of\nhate speech. The back translation technique relies on an encoder-decoder\narchitecture pre-trained on a large corpus and mostly used for machine\ntranslation. In addition, paraphrasing exploits the transformer model and the\nmixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are\ncompared to seek enhanced classification results. We evaluate our proposal on\nfive publicly available datasets; namely, AskFm corpus, Formspring dataset,\nWarner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The\nperformance of the proposal together with comparison to some related\nstate-of-art results demonstrate the effectiveness and soundness of our\nproposal.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:52:42 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Beddiar", "Djamila Romaissa", ""], ["Jahan", "Md Saroar", ""], ["Oussalah", "Mourad", ""]]}, {"id": "2106.04718", "submitter": "Yu Yan", "authors": "Yu Yan, Fei Hu, Jiusheng Chen, Nikhil Bhendawade, Ting Ye, Yeyun Gong,\n  Nan Duan, Desheng Cui, Bingyu Chi and Ruofei Zhang", "title": "FastSeq: Make Sequence Generation Faster", "comments": "ACL 2021 Demo Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 22:25:28 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 22:24:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Yan", "Yu", ""], ["Hu", "Fei", ""], ["Chen", "Jiusheng", ""], ["Bhendawade", "Nikhil", ""], ["Ye", "Ting", ""], ["Gong", "Yeyun", ""], ["Duan", "Nan", ""], ["Cui", "Desheng", ""], ["Chi", "Bingyu", ""], ["Zhang", "Ruofei", ""]]}, {"id": "2106.04726", "submitter": "Ashkan Kazemi", "authors": "Ashkan Kazemi, Kiran Garimella, Gautam Kishore Shahi, Devin Gaffney,\n  Scott A. Hale", "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study\n  of the 2019 Indian Election on WhatsApp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:08:47 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 17:49:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kazemi", "Ashkan", ""], ["Garimella", "Kiran", ""], ["Shahi", "Gautam Kishore", ""], ["Gaffney", "Devin", ""], ["Hale", "Scott A.", ""]]}, {"id": "2106.04747", "submitter": "Eleftheria Briakou", "authors": "Eleftheria Briakou, Sweta Agrawal, Ke Zhang, Joel Tetreault and Marine\n  Carpuat", "title": "A Review of Human Evaluation for Style Transfer", "comments": "GEM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reviews and summarizes human evaluation practices described in 97\nstyle transfer papers with respect to three main evaluation aspects: style\ntransfer, meaning preservation, and fluency. In principle, evaluations by human\nraters should be the most reliable. However, in style transfer papers, we find\nthat protocols for human evaluations are often underspecified and not\nstandardized, which hampers the reproducibility of research in this field and\nprogress toward better human and automatic evaluation methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 00:29:42 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Briakou", "Eleftheria", ""], ["Agrawal", "Sweta", ""], ["Zhang", "Ke", ""], ["Tetreault", "Joel", ""], ["Carpuat", "Marine", ""]]}, {"id": "2106.04753", "submitter": "Wei Zhang", "authors": "Wei Zhang, Ziming Huang, Yada Zhu, Guangnan Ye, Xiaodong Cui, Fan\n  Zhang", "title": "On Sample Based Explanation Methods for NLP:Efficiency, Faithfulness,\n  and Semantic Evaluation", "comments": "13 pages; Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the recent advances of natural language processing, the scale of the\nstate-of-the-art models and datasets is usually extensive, which challenges the\napplication of sample-based explanation methods in many aspects, such as\nexplanation interpretability, efficiency, and faithfulness. In this work, for\nthe first time, we can improve the interpretability of explanations by allowing\narbitrary text sequences as the explanation unit. On top of this, we implement\na hessian-free method with a model faithfulness guarantee. Finally, to compare\nour method with the others, we propose a semantic-based evaluation metric that\ncan better align with humans' judgment of explanations than the widely adopted\ndiagnostic or re-training measures. The empirical results on multiple real data\nsets demonstrate the proposed method's superior performance to popular\nexplanation techniques such as Influence Function or TracIn on semantic\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 00:49:56 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Wei", ""], ["Huang", "Ziming", ""], ["Zhu", "Yada", ""], ["Ye", "Guangnan", ""], ["Cui", "Xiaodong", ""], ["Zhang", "Fan", ""]]}, {"id": "2106.04791", "submitter": "Danqi Liao", "authors": "Danqi Liao", "title": "Sentence Embeddings using Supervised Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence embeddings encode sentences in fixed dense vectors and have played\nan important role in various NLP tasks and systems. Methods for building\nsentence embeddings include unsupervised learning such as Quick-Thoughts and\nsupervised learning such as InferSent. With the success of pretrained NLP\nmodels, recent research shows that fine-tuning pretrained BERT on SNLI and\nMulti-NLI data creates state-of-the-art sentence embeddings, outperforming\nprevious sentence embeddings methods on various evaluation benchmarks. In this\npaper, we propose a new method to build sentence embeddings by doing supervised\ncontrastive learning. Specifically our method fine-tunes pretrained BERT on\nSNLI data, incorporating both supervised crossentropy loss and supervised\ncontrastive loss. Compared with baseline where fine-tuning is only done with\nsupervised cross-entropy loss similar to current state-of-the-art method SBERT,\nour supervised contrastive method improves 2.8% in average on Semantic Textual\nSimilarity (STS) benchmarks and 1.05% in average on various sentence transfer\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:30:29 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liao", "Danqi", ""]]}, {"id": "2106.04814", "submitter": "Yitao Cai", "authors": "Yitao Cai, Zhe Lin and Xiaojun Wan", "title": "Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstract Meaning Representation (AMR) is a rooted, labeled, acyclic graph\nrepresenting the semantics of natural language. As previous works show,\nalthough AMR is designed for English at first, it can also represent semantics\nin other languages. However, they find that concepts in their predicted AMR\ngraphs are less specific. We argue that the misprediction of concepts is due to\nthe high relevance between English tokens and AMR concepts. In this work, we\nintroduce bilingual input, namely the translated texts as well as non-English\ntexts, in order to enable the model to predict more accurate concepts. Besides,\nwe also introduce an auxiliary task, requiring the decoder to predict the\nEnglish sequences at the same time. The auxiliary task can help the decoder\nunderstand what exactly the corresponding English tokens are. Our proposed\ncross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6\npoints on Smatch F1 score. The ablation study also demonstrates the efficacy of\nour proposed modules.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 05:14:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cai", "Yitao", ""], ["Lin", "Zhe", ""], ["Wan", "Xiaojun", ""]]}, {"id": "2106.04830", "submitter": "Nir Sweed", "authors": "Nir Sweed, Dafna Shahaf", "title": "Catchphrase: Automatic Detection of Cultural References", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A snowclone is a customizable phrasal template that can be realized in\nmultiple, instantly recognized variants. For example, ``* is the new *\" (Orange\nis the new black, 40 is the new 30). Snowclones are extensively used in social\nmedia. In this paper, we study snowclones originating from pop-culture quotes;\nour goal is to automatically detect cultural references in text. We introduce a\nnew, publicly available data set of pop-culture quotes and their corresponding\nsnowclone usages and train models on them. We publish code for Catchphrase, an\ninternet browser plugin to automatically detect and mark references in\nreal-time, and examine its performance via a user study. Aside from assisting\npeople to better comprehend cultural references, we hope that detecting\nsnowclones can complement work on paraphrasing and help to tackle long-standing\nquestions in social science about the dynamics of information propagation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:31:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sweed", "Nir", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2106.04831", "submitter": "Yosephine Susanto", "authors": "Ng Bee Chin, Yosephine Susanto and Erik Cambria", "title": "MICE: A Crosslinguistic Emotion Corpus in Malay, Indonesian, Chinese and\n  English", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MICE is a corpus of emotion words in four languages which is currently\nworking progress. There are two sections to this study, Part I: Emotion word\ncorpus and Part II: Emotion word survey. In Part 1, the method of how the\nemotion data is culled for each of the four languages will be described and\nvery preliminary data will be presented. In total, we identified 3,750 emotion\nexpressions in Malay, 6,657 in Indonesian, 3,347 in Mandarin Chinese and 8,683\nin English. We are currently evaluating and double checking the corpus and\ndoing further analysis on the distribution of these emotion expressions. Part\nII Emotion word survey involved an online language survey which collected\ninformation on how speakers assigned the emotion words into basic emotion\ncategories, the rating for valence and intensity as well as biographical\ninformation of all the respondents.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:33:30 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chin", "Ng Bee", ""], ["Susanto", "Yosephine", ""], ["Cambria", "Erik", ""]]}, {"id": "2106.04832", "submitter": "Murathan Kurfal{\\i}", "authors": "Murathan Kurfal{\\i}, Robert \\\"Ostling", "title": "Probing Multilingual Language Models for Discourse", "comments": "To be presented at RepL4NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained multilingual language models have become an important building\nblock in multilingual natural language processing. In the present paper, we\ninvestigate a range of such models to find out how well they transfer\ndiscourse-level knowledge across languages. This is done with a systematic\nevaluation on a broader set of discourse-level tasks than has been previously\nbeen assembled. We find that the XLM-RoBERTa family of models consistently show\nthe best performance, by simultaneously being good monolingual models and\ndegrading relatively little in a zero-shot setting. Our results also indicate\nthat model distillation may hurt the ability of cross-lingual transfer of\nsentence representations, while language dissimilarity at most has a modest\neffect. We hope that our test suite, covering 5 tasks with a total of 22\nlanguages in 10 distinct families, will serve as a useful evaluation platform\nfor multilingual performance at and beyond the sentence level.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:34:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kurfal\u0131", "Murathan", ""], ["\u00d6stling", "Robert", ""]]}, {"id": "2106.04833", "submitter": "Xingshan Zeng", "authors": "Xingshan Zeng, Liangyou Li, Qun Liu", "title": "RealTranS: End-to-End Simultaneous Speech Translation with Convolutional\n  Weighted-Shrinking Transformer", "comments": "Accepted by ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end simultaneous speech translation (SST), which directly translates\nspeech in one language into text in another language in real-time, is useful in\nmany scenarios but has not been fully investigated. In this work, we propose\nRealTranS, an end-to-end model for SST. To bridge the modality gap between\nspeech and text, RealTranS gradually downsamples the input speech with\ninterleaved convolution and unidirectional Transformer layers for acoustic\nmodeling, and then maps speech features into text space with a\nweighted-shrinking operation and a semantic encoder. Besides, to improve the\nmodel performance in simultaneous scenarios, we propose a blank penalty to\nenhance the shrinking quality and a Wait-K-Stride-N strategy to allow local\nreranking during decoding. Experiments on public and widely-used datasets show\nthat RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end\nmodels as well as cascaded models in diverse latency settings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:35:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zeng", "Xingshan", ""], ["Li", "Liangyou", ""], ["Liu", "Qun", ""]]}, {"id": "2106.04835", "submitter": "Zichuan Lin", "authors": "Zichuan Lin, Jing Huang, Bowen Zhou, Xiaodong He, Tengyu Ma", "title": "Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work (Takanobu et al., 2020) proposed the system-wise evaluation on\ndialog systems and found that improvement on individual components (e.g., NLU,\npolicy) in prior work may not necessarily bring benefit to pipeline systems in\nsystem-wise evaluation. To improve the system-wise performance, in this paper,\nwe propose new joint system-wise optimization techniques for the pipeline\ndialog system. First, we propose a new data augmentation approach which\nautomates the labeling process for NLU training. Second, we propose a novel\nstochastic policy parameterization with Poisson distribution that enables\nbetter exploration and offers a principled way to compute policy gradient.\nThird, we propose a reward bonus to help policy explore successful dialogs. Our\napproaches outperform the competitive pipeline systems from Takanobu et al.\n(2020) by big margins of 12% success rate in automatic system-wise evaluation\nand of 16% success rate in human evaluation on the standard multi-domain\nbenchmark dataset MultiWOZ 2.1, and also outperform the recent state-of-the-art\nend-to-end trained model from DSTC9.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:44:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lin", "Zichuan", ""], ["Huang", "Jing", ""], ["Zhou", "Bowen", ""], ["He", "Xiaodong", ""], ["Ma", "Tengyu", ""]]}, {"id": "2106.04847", "submitter": "Wei Liu", "authors": "Huanqin Wu, Wei Liu, Lei Li, Dan Nie, Tao Chen, Feng Zhang, Di Wang", "title": "UniKeyphrase: A Unified Extraction and Generation Framework for\n  Keyphrase Prediction", "comments": "11pages, 6 figures, 6 tables, to be published in ACL 2021 findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyphrase Prediction (KP) task aims at predicting several keyphrases that can\nsummarize the main idea of the given document. Mainstream KP methods can be\ncategorized into purely generative approaches and integrated models with\nextraction and generation. However, these methods either ignore the diversity\namong keyphrases or only weakly capture the relation across tasks implicitly.\nIn this paper, we propose UniKeyphrase, a novel end-to-end learning framework\nthat jointly learns to extract and generate keyphrases. In UniKeyphrase,\nstacked relation layer and bag-of-words constraint are proposed to fully\nexploit the latent semantic relation between extraction and generation in the\nview of model structure and training process, respectively. Experiments on KP\nbenchmarks demonstrate that our joint approach outperforms mainstream methods\nby a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:09:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wu", "Huanqin", ""], ["Liu", "Wei", ""], ["Li", "Lei", ""], ["Nie", "Dan", ""], ["Chen", "Tao", ""], ["Zhang", "Feng", ""], ["Wang", "Di", ""]]}, {"id": "2106.04853", "submitter": "Bharathi Raja Chakravarthi", "authors": "Bharathi Raja Chakravarthi and Jishnu Parameswaran P.K and Premjith B\n  and K.P Soman and Rahul Ponnusamy and Prasanna Kumar Kumaresan and Kingston\n  Pal Thamburaj and John P. McCrae", "title": "DravidianMultiModality: A Dataset for Multi-modal Sentiment Analysis in\n  Tamil and Malayalam", "comments": "31", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human communication is inherently multimodal and asynchronous. Analyzing\nhuman emotions and sentiment is an emerging field of artificial intelligence.\nWe are witnessing an increasing amount of multimodal content in local languages\non social media about products and other topics. However, there are not many\nmultimodal resources available for under-resourced Dravidian languages. Our\nstudy aims to create a multimodal sentiment analysis dataset for the\nunder-resourced Tamil and Malayalam languages. First, we downloaded product or\nmovies review videos from YouTube for Tamil and Malayalam. Next, we created\ncaptions for the videos with the help of annotators. Then we labelled the\nvideos for sentiment, and verified the inter-annotator agreement using Fleiss's\nKappa. This is the first multimodal sentiment analysis dataset for Tamil and\nMalayalam by volunteer annotators.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:25:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chakravarthi", "Bharathi Raja", ""], ["K", "Jishnu Parameswaran P.", ""], ["B", "Premjith", ""], ["Soman", "K. P", ""], ["Ponnusamy", "Rahul", ""], ["Kumaresan", "Prasanna Kumar", ""], ["Thamburaj", "Kingston Pal", ""], ["McCrae", "John P.", ""]]}, {"id": "2106.04897", "submitter": "Hanan Aldarmaki", "authors": "Hanan Aldarmaki, Asad Ullah, Nazar Zaki", "title": "Unsupervised Automatic Speech Recognition: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic Speech Recognition (ASR) systems can be trained to achieve\nremarkable performance given large amounts of manually transcribed speech, but\nlarge labeled data sets can be difficult or expensive to acquire for all\nlanguages of interest. In this paper, we review the research literature to\nidentify models and ideas that could lead to fully unsupervised ASR, including\nunsupervised segmentation of the speech signal, unsupervised mapping from\nspeech segments to text, and semi-supervised models with nominal amounts of\nlabeled examples. The objective of the study is to identify the limitations of\nwhat can be learned from speech data alone and to understand the minimum\nrequirements for speech recognition. Identifying these limitations would help\noptimize the resources and efforts in ASR development for low-resource\nlanguages.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:33:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Aldarmaki", "Hanan", ""], ["Ullah", "Asad", ""], ["Zaki", "Nazar", ""]]}, {"id": "2106.04903", "submitter": "Matiss Rikters", "authors": "Maija K\\=ale and Mat\\=iss Rikters", "title": "Fragmented and Valuable: Following Sentiment Changes in Food Tweets", "comments": null, "journal-ref": "Published in Smell, Taste, and Temperature Interfaces CHI 2021\n  workshop", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analysed sentiment and frequencies related to smell, taste and temperature\nexpressed by food tweets in the Latvian language. To get a better understanding\nof the role of smell, taste and temperature in the mental map of food\nassociations, we looked at such categories as 'tasty' and 'healthy', which\nturned out to be mutually exclusive. By analysing the occurrence frequency of\nwords associated with these categories, we discovered that food discourse\noverall was permeated by `tasty' while the category of 'healthy' was relatively\nsmall. Finally, we used the analysis of temporal dynamics to see if we can\ntrace seasonality or other temporal aspects in smell, taste and temperature as\nreflected in food tweets. Understanding the composition of social media content\nwith relation to smell, taste and temperature in food tweets allows us to\ndevelop our work further - on food culture/seasonality and its relation to\ntemperature, on our limited capacity to express smell-related sentiments, and\nthe lack of the paradigm of taste in discussing food healthiness.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:42:14 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["K\u0101le", "Maija", ""], ["Rikters", "Mat\u012bss", ""]]}, {"id": "2106.04905", "submitter": "Kun Zhang", "authors": "Kun Zhang, Guangyi Lv, Meng Wang, and Enhong Chen", "title": "DGA-Net Dynamic Gaussian Attention Network for Sentence Semantic\n  Matching", "comments": "Accepted by CICAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentence semantic matching requires an agent to determine the semantic\nrelation between two sentences, where much recent progress has been made by the\nadvancement of representation learning techniques and inspiration of human\nbehaviors. Among all these methods, attention mechanism plays an essential role\nby selecting important parts effectively. However, current attention methods\neither focus on all the important parts in a static way or only select one\nimportant part at one attention step dynamically, which leaves a large space\nfor further improvement. To this end, in this paper, we design a novel Dynamic\nGaussian Attention Network (DGA-Net) to combine the advantages of current\nstatic and dynamic attention methods. More specifically, we first leverage\npre-trained language model to encode the input sentences and construct semantic\nrepresentations from a global perspective. Then, we develop a Dynamic Gaussian\nAttention (DGA) to dynamically capture the important parts and corresponding\nlocal contexts from a detailed perspective. Finally, we combine the global\ninformation and detailed local information together to decide the semantic\nrelation of sentences comprehensively and precisely. Extensive experiments on\ntwo popular sentence semantic matching tasks demonstrate that our proposed\nDGA-Net is effective in improving the ability of attention mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:43:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Kun", ""], ["Lv", "Guangyi", ""], ["Wang", "Meng", ""], ["Chen", "Enhong", ""]]}, {"id": "2106.04908", "submitter": "Alexander Schindler", "authors": "Sch\\\"utz Mina, Boeck Jaqueline, Liakhovets Daria, Slijep\\v{c}evi\\'c\n  Djordje, Kirchknopf Armin, Hecht Manuel, Bogensperger Johannes, Schlarb Sven,\n  Schindler Alexander, Zeppelzauer Matthias", "title": "Automatic Sexism Detection with Multilingual Transformer Models", "comments": "Technical Report to the AIT_FHSTP EXIST 2021 Challenge contribution\n  (under review) http://nlp.uned.es/exist2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sexism has become an increasingly major problem on social networks during the\nlast years. The first shared task on sEXism Identification in Social neTworks\n(EXIST) at IberLEF 2021 is an international competition in the field of Natural\nLanguage Processing (NLP) with the aim to automatically identify sexism in\nsocial media content by applying machine learning methods. Thereby sexism\ndetection is formulated as a coarse (binary) classification problem and a\nfine-grained classification task that distinguishes multiple types of sexist\ncontent (e.g., dominance, stereotyping, and objectification). This paper\npresents the contribution of the AIT_FHSTP team at the EXIST2021 benchmark for\nboth tasks. To solve the tasks we applied two multilingual transformer models,\none based on multilingual BERT and one based on XLM-R. Our approach uses two\ndifferent strategies to adapt the transformers to the detection of sexist\ncontent: first, unsupervised pre-training with additional data and second,\nsupervised fine-tuning with additional and augmented data. For both tasks our\nbest model is XLM-R with unsupervised pre-training on the EXIST data and\nadditional datasets and fine-tuning on the provided dataset. The best run for\nthe binary classification (task 1) achieves a macro F1-score of 0.7752 and\nscores 5th rank in the benchmark; for the multiclass classification (task 2)\nour best submission scores 6th rank with a macro F1-score of 0.5589.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:45:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mina", "Sch\u00fctz", ""], ["Jaqueline", "Boeck", ""], ["Daria", "Liakhovets", ""], ["Djordje", "Slijep\u010devi\u0107", ""], ["Armin", "Kirchknopf", ""], ["Manuel", "Hecht", ""], ["Johannes", "Bogensperger", ""], ["Sven", "Schlarb", ""], ["Alexander", "Schindler", ""], ["Matthias", "Zeppelzauer", ""]]}, {"id": "2106.04935", "submitter": "Sara Meftah", "authors": "Sara Meftah, Nasredine Semmar, Youssef Tamaazousti, Hassane Essafi,\n  Fatiha Sadat", "title": "Neural Supervised Domain Adaptation by Augmenting Pre-trained Models\n  with Random Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Transfer Learning (TL) is becoming ubiquitous in Natural Language\nProcessing (NLP), thanks to its high performance on many tasks, especially in\nlow-resourced scenarios. Notably, TL is widely used for neural domain\nadaptation to transfer valuable knowledge from high-resource to low-resource\ndomains. In the standard fine-tuning scheme of TL, a model is initially\npre-trained on a source domain and subsequently fine-tuned on a target domain\nand, therefore, source and target domains are trained using the same\narchitecture. In this paper, we show through interpretation methods that such\nscheme, despite its efficiency, is suffering from a main limitation. Indeed,\nalthough capable of adapting to new domains, pre-trained neurons struggle with\nlearning certain patterns that are specific to the target domain. Moreover, we\nshed light on the hidden negative transfer occurring despite the high\nrelatedness between source and target domains, which may mitigate the final\ngain brought by transfer learning. To address these problems, we propose to\naugment the pre-trained model with normalised, weighted and randomly\ninitialised units that foster a better adaptation while maintaining the\nvaluable source knowledge. We show that our approach exhibits significant\nimprovements to the standard fine-tuning scheme for neural domain adaptation\nfrom the news domain to the social media domain on four NLP tasks:\npart-of-speech tagging, chunking, named entity recognition and morphosyntactic\ntagging.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:29:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Meftah", "Sara", ""], ["Semmar", "Nasredine", ""], ["Tamaazousti", "Youssef", ""], ["Essafi", "Hassane", ""], ["Sadat", "Fatiha", ""]]}, {"id": "2106.04939", "submitter": "Mohammad Reza Feizi Derakhshi", "authors": "Narjes Nikzad-Khasmakhi, Mohammad-Reza Feizi-Derakhshi, Meysam\n  Asgari-Chenaghlu, Mohammad-Ali Balafar, Ali-Reza Feizi-Derakhshi, Taymaz\n  Rahkar-Farshi, Majid Ramezani, Zoleikha Jahanbakhsh-Nagadeh, Elnaz\n  Zafarani-Moattar, Mehrdad Ranjbar-Khadivi", "title": "Phraseformer: Multimodal Key-phrase Extraction using Transformer and\n  Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Keyword extraction is a popular research topic in the field of\nnatural language processing. Keywords are terms that describe the most relevant\ninformation in a document. The main problem that researchers are facing is how\nto efficiently and accurately extract the core keywords from a document.\nHowever, previous keyword extraction approaches have utilized the text and\ngraph features, there is the lack of models that can properly learn and combine\nthese features in a best way.\n  Methods: In this paper, we develop a multimodal Key-phrase extraction\napproach, namely Phraseformer, using transformer and graph embedding\ntechniques. In Phraseformer, each keyword candidate is presented by a vector\nwhich is the concatenation of the text and structure learning representations.\nPhraseformer takes the advantages of recent researches such as BERT and ExEm to\npreserve both representations. Also, the Phraseformer treats the key-phrase\nextraction task as a sequence labeling problem solved using classification\ntask.\n  Results: We analyze the performance of Phraseformer on three datasets\nincluding Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we\ninvestigate the performance of different classifiers on Phraseformer method\nover Inspec dataset. Experimental results demonstrate the effectiveness of\nPhraseformer method over the three datasets used. Additionally, the Random\nForest classifier gain the highest F1-score among all classifiers.\n  Conclusions: Due to the fact that the combination of BERT and ExEm is more\nmeaningful and can better represent the semantic of words. Hence, Phraseformer\nsignificantly outperforms single-modality methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:32:17 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Nikzad-Khasmakhi", "Narjes", ""], ["Feizi-Derakhshi", "Mohammad-Reza", ""], ["Asgari-Chenaghlu", "Meysam", ""], ["Balafar", "Mohammad-Ali", ""], ["Feizi-Derakhshi", "Ali-Reza", ""], ["Rahkar-Farshi", "Taymaz", ""], ["Ramezani", "Majid", ""], ["Jahanbakhsh-Nagadeh", "Zoleikha", ""], ["Zafarani-Moattar", "Elnaz", ""], ["Ranjbar-Khadivi", "Mehrdad", ""]]}, {"id": "2106.04959", "submitter": "\\c{S}\\\"ukr\\\"u Ozan", "authors": "\\c{S}\\\"ukr\\\"u Ozan, D. Emre Ta\\c{s}ar", "title": "Auto-tagging of Short Conversational Sentences using Natural Language\n  Processing Methods", "comments": "in Turkish language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we aim to find a method to auto-tag sentences specific to a\ndomain. Our training data comprises short conversational sentences extracted\nfrom chat conversations between company's customer representatives and web site\nvisitors. We manually tagged approximately 14 thousand visitor inputs into ten\nbasic categories, which will later be used in a transformer-based language\nmodel with attention mechanisms for the ultimate goal of developing a chatbot\napplication that can produce meaningful dialogue. We considered three different\nstate-of-the-art models and reported their auto-tagging capabilities. We\nachieved the best performance with the bidirectional encoder representation\nfrom transformers (BERT) model. Implementation of the models used in these\nexperiments can be cloned from our GitHub repository and tested for similar\nauto-tagging problems without much effort.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:14:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ozan", "\u015e\u00fckr\u00fc", ""], ["Ta\u015far", "D. Emre", ""]]}, {"id": "2106.04963", "submitter": "Yang Tao", "authors": "Tao Yang, Feifan Yang, Haolan Ouyang, Xiaojun Quan", "title": "Psycholinguistic Tripartite Graph Network for Personality Detection", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the recent work on personality detection from online posts adopts\nmultifarious deep neural networks to represent the posts and builds predictive\nmodels in a data-driven manner, without the exploitation of psycholinguistic\nknowledge that may unveil the connections between one's language usage and his\npsychological traits. In this paper, we propose a psycholinguistic\nknowledge-based tripartite graph network, TrigNet, which consists of a\ntripartite graph network and a BERT-based graph initializer. The graph network\ninjects structural psycholinguistic knowledge from LIWC, a computerized\ninstrument for psycholinguistic analysis, by constructing a heterogeneous\ntripartite graph. The graph initializer is employed to provide initial\nembeddings for the graph nodes. To reduce the computational cost in graph\nlearning, we further propose a novel flow graph attention network (GAT) that\nonly transmits messages between neighboring parties in the tripartite graph.\nBenefiting from the tripartite graph, TrigNet can aggregate post information\nfrom a psychological perspective, which is a novel way of exploiting domain\nknowledge. Extensive experiments on two datasets show that TrigNet outperforms\nthe existing state-of-art model by 3.47 and 2.10 points in average F1.\nMoreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%,\nrespectively, in comparison to the original GAT in our setting.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:18:50 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yang", "Tao", ""], ["Yang", "Feifan", ""], ["Ouyang", "Haolan", ""], ["Quan", "Xiaojun", ""]]}, {"id": "2106.04970", "submitter": "Tao Ge", "authors": "Xin Sun, Tao Ge, Furu Wei, Houfeng Wang", "title": "Instantaneous Grammatical Error Correction with Shallow Aggressive\n  Decoding", "comments": "Accepted by ACL2021 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the\nonline inference efficiency of the Transformer for instantaneous Grammatical\nError Correction (GEC). SAD optimizes the online inference efficiency for GEC\nby two innovations: 1) it aggressively decodes as many tokens as possible in\nparallel instead of always decoding only one token in each step to improve\ncomputational parallelism; 2) it uses a shallow decoder instead of the\nconventional Transformer architecture with balanced encoder-decoder depth to\nreduce the computational cost during inference. Experiments in both English and\nChinese GEC benchmarks show that aggressive decoding could yield the same\npredictions as greedy decoding but with a significant speedup for online\ninference. Its combination with the shallow decoder could offer an even higher\nonline inference speedup over the powerful Transformer baseline without quality\nloss. Not only does our approach allow a single model to achieve the\nstate-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14\nand 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference\nspeedup over the Transformer-big model, but also it is easily adapted to other\nlanguages. Our code is available at\nhttps://github.com/AutoTemp/Shallow-Aggressive-Decoding.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:30:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sun", "Xin", ""], ["Ge", "Tao", ""], ["Wei", "Furu", ""], ["Wang", "Houfeng", ""]]}, {"id": "2106.04985", "submitter": "Tomek Korbak", "authors": "Tomasz Korbak and Hady Elsahar and Marc Dymetman and Germ\\'an\n  Kruszewski", "title": "Energy-Based Models for Code Generation under Compilability Constraints", "comments": "Accepted for the First Workshop on Natural Language Processing for\n  Programming, ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural language models can be successfully trained on source code, leading to\napplications such as code completion. However, their versatile autoregressive\nself-supervision objective overlooks important global sequence-level features\nthat are present in the data such as syntactic correctness or compilability. In\nthis work, we pose the problem of learning to generate compilable code as\nconstraint satisfaction. We define an Energy-Based Model (EBM) representing a\npre-trained generative model with an imposed constraint of generating only\ncompilable sequences. We then use the KL-Adaptive Distributional Policy\nGradient algorithm (Khalifa et al., 2021) to train a generative model\napproximating the EBM. We conduct experiments showing that our proposed\napproach is able to improve compilability rates without sacrificing diversity\nand complexity of the generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:06:32 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Korbak", "Tomasz", ""], ["Elsahar", "Hady", ""], ["Dymetman", "Marc", ""], ["Kruszewski", "Germ\u00e1n", ""]]}, {"id": "2106.04995", "submitter": "Tamali Banerjee", "authors": "Tamali Banerjee, Rudra Murthy V, Pushpak Bhattacharyya", "title": "Crosslingual Embeddings are Essential in UNMT for Distant Languages: An\n  English to IndoAryan Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Unsupervised Neural Machine Translation (UNMT) have\nminimized the gap between supervised and unsupervised machine translation\nperformance for closely related language pairs. However, the situation is very\ndifferent for distant language pairs. Lack of lexical overlap and low syntactic\nsimilarities such as between English and Indo-Aryan languages leads to poor\ntranslation quality in existing UNMT systems. In this paper, we show that\ninitializing the embedding layer of UNMT models with cross-lingual embeddings\nshows significant improvements in BLEU score over existing approaches with\nembeddings randomly initialized. Further, static embeddings (freezing the\nembedding layer weights) lead to better gains compared to updating the\nembedding layer weights during training (non-static). We experimented using\nMasked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT\napproaches for three distant language pairs. The proposed cross-lingual\nembedding initialization yields BLEU score improvement of as much as ten times\nover the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our\nanalysis shows the importance of cross-lingual embedding, comparisons between\napproaches, and the scope of improvements in these systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:31:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Banerjee", "Tamali", ""], ["Murthy", "Rudra", "V"], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2106.05006", "submitter": "Moshe Hazoom", "authors": "Moshe Hazoom, Vibhor Malik and Ben Bogin", "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack\n  Exchange Data", "comments": "NLP4Prog 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most available semantic parsing datasets, comprising of pairs of natural\nutterances and logical forms, were collected solely for the purpose of training\nand evaluation of natural language understanding systems. As a result, they do\nnot contain any of the richness and variety of natural-occurring utterances,\nwhere humans ask about data they need or are curious about. In this work, we\nrelease SEDE, a dataset with 12,023 pairs of utterances and SQL queries\ncollected from real usage on the Stack Exchange website. We show that these\npairs contain a variety of real-world challenges which were rarely reflected so\nfar in any other semantic parsing dataset, propose an evaluation metric based\non comparison of partial query clauses that is more suitable for real-world\nqueries, and conduct experiments with strong baselines, showing a large gap\nbetween the performance on SEDE compared to other common datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:09:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hazoom", "Moshe", ""], ["Malik", "Vibhor", ""], ["Bogin", "Ben", ""]]}, {"id": "2106.05093", "submitter": "Zhaopeng Tu", "authors": "Cunxiao Du and Zhaopeng Tu and Jing Jiang", "title": "Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation", "comments": "ICML 2021 (Oral), Code at\n  https://github.com/tencent-ailab/ICML21_OAXE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new training objective named order-agnostic cross entropy (OaXE)\nfor fully non-autoregressive translation (NAT) models. OaXE improves the\nstandard cross-entropy loss to ameliorate the effect of word reordering, which\nis a common source of the critical multimodality problem in NAT. Concretely,\nOaXE removes the penalty for word order errors, and computes the cross entropy\nloss based on the best possible alignment between model predictions and target\ntokens. Since the log loss is very sensitive to invalid references, we leverage\ncross entropy initialization and loss truncation to ensure the model focuses on\na good part of the search space. Extensive experiments on major WMT benchmarks\nshow that OaXE substantially improves translation performance, setting new\nstate of the art for fully NAT models. Further analyses show that OaXE\nalleviates the multimodality problem by reducing token repetitions and\nincreasing prediction confidence. Our code, data, and trained models are\navailable at https://github.com/tencent-ailab/ICML21_OAXE.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:15:12 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Du", "Cunxiao", ""], ["Tu", "Zhaopeng", ""], ["Jiang", "Jing", ""]]}, {"id": "2106.05111", "submitter": "Shigeki Karita", "authors": "Shigeki Karita, Yotaro Kubo, Michiel Adriaan Unico Bacchiani, Llion\n  Jones", "title": "A Comparative Study on Neural Architectures and Training Methods for\n  Japanese Speech Recognition", "comments": "to be published in INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) modeling is advantageous for automatic speech recognition\n(ASR) especially for Japanese since word-based tokenization of Japanese is not\ntrivial, and E2E modeling is able to model character sequences directly. This\npaper focuses on the latest E2E modeling techniques, and investigates their\nperformances on character-based Japanese ASR by conducting comparative\nexperiments. The results are analyzed and discussed in order to understand the\nrelative advantages of long short-term memory (LSTM), and Conformer models in\ncombination with connectionist temporal classification, transducer, and\nattention-based loss functions. Furthermore, the paper investigates on\neffectivity of the recent training techniques such as data augmentation\n(SpecAugment), variational noise injection, and exponential moving average. The\nbest configuration found in the paper achieved the state-of-the-art character\nerror rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ)\neval1, eval2, and eval3 tasks, respectively. The system is also shown to be\ncomputationally efficient thanks to the efficiency of Conformer transducers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:42:29 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Karita", "Shigeki", ""], ["Kubo", "Yotaro", ""], ["Bacchiani", "Michiel Adriaan Unico", ""], ["Jones", "Llion", ""]]}, {"id": "2106.05141", "submitter": "Tasnim Mohiuddin", "authors": "Tasnim Mohiuddin, M Saiful Bari, Shafiq Joty", "title": "AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT", "comments": "ACL-2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of Neural Machine Translation (NMT) largely depends on the\navailability of large bitext training corpora. Due to the lack of such large\ncorpora in low-resource language pairs, NMT systems often exhibit poor\nperformance. Extra relevant monolingual data often helps, but acquiring it\ncould be quite expensive, especially for low-resource languages. Moreover,\ndomain mismatch between bitext (train/test) and monolingual data might degrade\nthe performance. To alleviate such issues, we propose AUGVIC, a novel data\naugmentation framework for low-resource NMT which exploits the vicinal samples\nof the given bitext without using any extra monolingual data explicitly. It can\ndiversify the in-domain bitext data with finer level control. Through extensive\nexperiments on four low-resource language pairs comprising data from different\ndomains, we have shown that our method is comparable to the traditional\nback-translation that uses extra in-domain monolingual data. When we combine\nthe synthetic parallel data generated from AUGVIC with the ones from the extra\nmonolingual data, we achieve further improvements. We show that AUGVIC helps to\nattenuate the discrepancies between relevant and distant-domain monolingual\ndata in traditional back-translation. To understand the contributions of\ndifferent components of AUGVIC, we perform an in-depth framework analysis.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:29:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mohiuddin", "Tasnim", ""], ["Bari", "M Saiful", ""], ["Joty", "Shafiq", ""]]}, {"id": "2106.05160", "submitter": "\\c{S}\\\"ukr\\\"u Ozan", "authors": "\\c{S}\\\"ukr\\\"u Ozan", "title": "Case Studies on using Natural Language Processing Techniques in Customer\n  Relationship Management Software", "comments": "Pre-print version of the article titled \"Case Studies on using\n  Natural Language Processing Techniques in Customer Relationship Management\n  Software\"", "journal-ref": "Journal of Intelligent Information Systems volume 56 233-253 2021", "doi": "10.1007/s10844-020-00619-4", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can a text corpus stored in a customer relationship management (CRM)\ndatabase be used for data mining and segmentation? In order to answer this\nquestion we inherited the state of the art methods commonly used in natural\nlanguage processing (NLP) literature, such as word embeddings, and deep\nlearning literature, such as recurrent neural networks (RNN). We used the text\nnotes from a CRM system which are taken by customer representatives of an\ninternet ads consultancy agency between years 2009 and 2020. We trained word\nembeddings by using the corresponding text corpus and showed that these word\nembeddings can not only be used directly for data mining but also be used in\nRNN architectures, which are deep learning frameworks built with long short\nterm memory (LSTM) units, for more comprehensive segmentation objectives. The\nresults prove that structured text data in a CRM can be used to mine out very\nvaluable information and any CRM can be equipped with useful NLP features once\nthe problem definitions are properly built and the solution methods are\nconveniently implemented.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:07:07 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ozan", "\u015e\u00fckr\u00fc", ""]]}, {"id": "2106.05166", "submitter": "Yinpeng Guo", "authors": "Yinpeng Guo, Liangyou Li, Xin Jiang and Qun Liu", "title": "Learning Multilingual Representation for Natural Language Understanding\n  with Enhanced Cross-Lingual Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-training multilingual language models has shown great potential\nin learning multilingual representation, a crucial topic of natural language\nprocessing. Prior works generally use a single mixed attention (MA) module,\nfollowing TLM (Conneau and Lample, 2019), for attending to intra-lingual and\ncross-lingual contexts equivalently and simultaneously. In this paper, we\npropose a network named decomposed attention (DA) as a replacement of MA. The\nDA consists of an intra-lingual attention (IA) and a cross-lingual attention\n(CA), which model intralingual and cross-lingual supervisions respectively. In\naddition, we introduce a language-adaptive re-weighting strategy during\ntraining to further boost the model's performance. Experiments on various\ncross-lingual natural language understanding (NLU) tasks show that the proposed\narchitecture and learning strategy significantly improve the model's\ncross-lingual transferability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:12:13 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guo", "Yinpeng", ""], ["Li", "Liangyou", ""], ["Jiang", "Xin", ""], ["Liu", "Qun", ""]]}, {"id": "2106.05221", "submitter": "Shuoran Jiang", "authors": "Shuoran Jiang, Qingcai Chen, Xin Liu, Baotian Hu, Lisai Zhang", "title": "Multi-hop Graph Convolutional Network with High-order Chebyshev\n  Approximation for Text Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional network (GCN) has become popular in various natural\nlanguage processing (NLP) tasks with its superiority in long-term and\nnon-consecutive word interactions. However, existing single-hop graph reasoning\nin GCN may miss some important non-consecutive dependencies. In this study, we\ndefine the spectral graph convolutional network with the high-order dynamic\nChebyshev approximation (HDGCN), which augments the multi-hop graph reasoning\nby fusing messages aggregated from direct and long-term dependencies into one\nconvolutional layer. To alleviate the over-smoothing in high-order Chebyshev\napproximation, a multi-vote-based cross-attention (MVCAttn) with linear\ncomputation complexity is also proposed. The empirical results on four\ntransductive and inductive NLP tasks and the ablation study verify the efficacy\nof the proposed model. Our source code is available at\nhttps://github.com/MathIsAll/HDGCN-pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:49:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Jiang", "Shuoran", ""], ["Chen", "Qingcai", ""], ["Liu", "Xin", ""], ["Hu", "Baotian", ""], ["Zhang", "Lisai", ""]]}, {"id": "2106.05249", "submitter": "Ananya Ganesh", "authors": "Ananya Ganesh, Martha Palmer, and Katharina Kann", "title": "What Would a Teacher Do? Predicting Future Talk Moves", "comments": "13 pages, 3 figures; To appear in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in natural language processing (NLP) have the ability to\ntransform how classroom learning takes place. Combined with the increasing\nintegration of technology in today's classrooms, NLP systems leveraging\nquestion answering and dialog processing techniques can serve as private tutors\nor participants in classroom discussions to increase student engagement and\nlearning. To progress towards this goal, we use the classroom discourse\nframework of academically productive talk (APT) to learn strategies that make\nfor the best learning experience. In this paper, we introduce a new task,\ncalled future talk move prediction (FTMP): it consists of predicting the next\ntalk move -- an utterance strategy from APT -- given a conversation history\nwith its corresponding talk moves. We further introduce a neural network model\nfor this task, which outperforms multiple baselines by a large margin. Finally,\nwe compare our model's performance on FTMP to human performance and show\nseveral similarities between the two.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:45:16 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ganesh", "Ananya", ""], ["Palmer", "Martha", ""], ["Kann", "Katharina", ""]]}, {"id": "2106.05251", "submitter": "Shujian Zhang", "authors": "Shujian Zhang, Xinjie Fan, Bo Chen, Mingyuan Zhou", "title": "Bayesian Attention Belief Networks", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention-based neural networks have achieved state-of-the-art results on a\nwide range of tasks. Most such models use deterministic attention while\nstochastic attention is less explored due to the optimization difficulties or\ncomplicated model design. This paper introduces Bayesian attention belief\nnetworks, which construct a decoder network by modeling unnormalized attention\nweights with a hierarchy of gamma distributions, and an encoder network by\nstacking Weibull distributions with a deterministic-upward-stochastic-downward\nstructure to approximate the posterior. The resulting auto-encoding networks\ncan be optimized in a differentiable way with a variational lower bound. It is\nsimple to convert any models with deterministic attention, including pretrained\nones, to the proposed Bayesian attention belief networks. On a variety of\nlanguage understanding tasks, we show that our method outperforms deterministic\nattention and state-of-the-art stochastic attention in accuracy, uncertainty\nestimation, generalization across domains, and robustness to adversarial\nattacks. We further demonstrate the general applicability of our method on\nneural machine translation and visual question answering, showing great\npotential of incorporating our method into various attention-related tasks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:46:22 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Shujian", ""], ["Fan", "Xinjie", ""], ["Chen", "Bo", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2106.05299", "submitter": "Adriana Correia", "authors": "A. D. Correia, M. Moortgat, H. T. C. Stoof", "title": "Grover's Algorithm for Question Answering", "comments": "16 pages, 9 figures. Presented at SemSpace21", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Grover's algorithm, a well-know quantum search algorithm, allows one to find\nthe correct item in a database, with quadratic speedup. In this paper we adapt\nGrover's algorithm to the problem of finding a correct answer to a natural\nlanguage question in English, thus contributing to the growing field of Quantum\nNatural Language Processing. Using a grammar that can be interpreted as tensor\ncontractions, each word is represented as a quantum state that serves as input\nto the quantum circuit. We here introduce a quantum measurement to contract the\nrepresentations of words, resulting in the representation of larger text\nfragments. Using this framework, a representation for the question is found\nthat contains all the possible answers in equal quantum superposition, and\nallows for the building of an oracle that can detect a correct answer, being\nagnostic to the specific question. Furthermore, we show that our construction\ncan deal with certain types of ambiguous phrases by keeping the various\ndifferent meanings in quantum superposition.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:00:13 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 10:43:32 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Correia", "A. D.", ""], ["Moortgat", "M.", ""], ["Stoof", "H. T. C.", ""]]}, {"id": "2106.05346", "submitter": "Devendra Singh Sachan", "authors": "Devendra Singh Sachan and Siva Reddy and William Hamilton and Chris\n  Dyer and Dani Yogatama", "title": "End-to-End Training of Multi-Document Reader and Retriever for\n  Open-Domain Question Answering", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end differentiable training method for\nretrieval-augmented open-domain question answering systems that combine\ninformation from multiple retrieved documents when generating answers. We model\nretrieval decisions as latent variables over sets of relevant documents. Since\nmarginalizing over sets of retrieved documents is computationally hard, we\napproximate this using an expectation-maximization algorithm. We iteratively\nestimate the value of our latent variable (the set of relevant documents for a\ngiven question) and then use this estimate to update the retriever and reader\nparameters. We hypothesize that such end-to-end training allows training\nsignals to flow to the reader and then to the retriever better than staged-wise\ntraining. This results in a retriever that is able to select more relevant\ndocuments for a question and a reader that is trained on more accurate\ndocuments to generate an answer. Experiments on three benchmark datasets\ndemonstrate that our proposed method outperforms all existing approaches of\ncomparable size by 2-3% absolute exact match points, achieving new\nstate-of-the-art results. Our results also demonstrate the feasibility of\nlearning to retrieve to improve answer generation without explicit supervision\nof retrieval decisions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:25:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Sachan", "Devendra Singh", ""], ["Reddy", "Siva", ""], ["Hamilton", "William", ""], ["Dyer", "Chris", ""], ["Yogatama", "Dani", ""]]}, {"id": "2106.05365", "submitter": "Weijia Shi", "authors": "Weijia Shi, Mandar Joshi, Luke Zettlemoyer", "title": "DESCGEN: A Distantly Supervised Dataset for Generating Abstractive\n  Entity Descriptions", "comments": null, "journal-ref": "ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Short textual descriptions of entities provide summaries of their key\nattributes and have been shown to be useful sources of background knowledge for\ntasks such as entity linking and question answering. However, generating entity\ndescriptions, especially for new and long-tail entities, can be challenging\nsince relevant information is often scattered across multiple sources with\nvaried content and style. We introduce DESCGEN: given mentions spread over\nmultiple documents, the goal is to generate an entity summary description.\nDESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each\npaired with nine evidence documents on average. The documents were collected\nusing a combination of entity linking and hyperlinks to the Wikipedia and\nFandom entity pages, which together provide high-quality distant supervision.\nThe resulting summaries are more abstractive than those found in existing\ndatasets and provide a better proxy for the challenge of describing new and\nemerging entities. We also propose a two-stage extract-then-generate baseline\nand show that there exists a large gap (19.9% in ROUGE-L) between\nstate-of-the-art models and human performance, suggesting that the data will\nsupport significant future work.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 20:10:48 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 21:53:58 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Shi", "Weijia", ""], ["Joshi", "Mandar", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2106.05387", "submitter": "Keerthiram Murugesan", "authors": "Keerthiram Murugesan, Subhajit Chaudhury, Kartik Talamadupula", "title": "Eye of the Beholder: Improved Relation Generalization for Text-based\n  Reinforcement Learning Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based games (TBGs) have become a popular proving ground for the\ndemonstration of learning-based agents that make decisions in quasi real-world\nsettings. The crux of the problem for a reinforcement learning agent in such\nTBGs is identifying the objects in the world, and those objects' relations with\nthat world. While the recent use of text-based resources for increasing an\nagent's knowledge and improving its generalization have shown promise, we posit\nin this paper that there is much yet to be learned from visual representations\nof these same worlds. Specifically, we propose to retrieve images that\nrepresent specific instances of text observations from the world and train our\nagents on such images. This improves the agent's overall understanding of the\ngame 'scene' and objects' relationships to the world around them, and the\nvariety of visual representations on offer allow the agent to generate a better\ngeneralization of a relationship. We show that incorporating such images\nimproves the performance of agents in various TBG settings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:02:07 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:10:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Murugesan", "Keerthiram", ""], ["Chaudhury", "Subhajit", ""], ["Talamadupula", "Kartik", ""]]}, {"id": "2106.05426", "submitter": "Richard Antonello", "authors": "Richard Antonello, Javier Turek, Vy Vo, and Alexander Huth", "title": "Low-Dimensional Structure in the Space of Language Representations is\n  Reflected in Brain Responses", "comments": "Preprint, submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 22:59:12 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 19:31:26 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 21:43:24 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Antonello", "Richard", ""], ["Turek", "Javier", ""], ["Vo", "Vy", ""], ["Huth", "Alexander", ""]]}, {"id": "2106.05437", "submitter": "Shashank Bujimalla", "authors": "Shashank Bujimalla, Mahesh Subedar, Omesh Tickoo", "title": "Data augmentation to improve robustness of image captioning solutions", "comments": "CVPR VizWiz 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the impact of motion blur, a common quality flaw in\nreal world images, on a state-of-the-art two-stage image captioning solution,\nand notice a degradation in solution performance as blur intensity increases.\nWe investigate techniques to improve the robustness of the solution to motion\nblur using training data augmentation at each or both stages of the solution,\ni.e., object detection and captioning, and observe improved results. In\nparticular, augmenting both the stages reduces the CIDEr-D degradation for high\nmotion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to\n6.8 on Vizwiz dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 00:17:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bujimalla", "Shashank", ""], ["Subedar", "Mahesh", ""], ["Tickoo", "Omesh", ""]]}, {"id": "2106.05450", "submitter": "Katsuki Chousa", "authors": "Katsuki Chousa and Makoto Morishita", "title": "Input Augmentation Improves Constrained Beam Search for Neural Machine\n  Translation: NTT at WAT 2021", "comments": "9 pages, 4 figures, WAT 2021 Restricted Translation Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our systems that were submitted to the restricted\ntranslation task at WAT 2021. In this task, the systems are required to output\ntranslated sentences that contain all given word constraints. Our system\ncombined input augmentation and constrained beam search algorithms. Through\nexperiments, we found that this combination significantly improves translation\naccuracy and can save inference time while containing all the constraints in\nthe output. For both En->Ja and Ja->En, our systems obtained the best\nevaluation performances in automatic evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 01:39:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chousa", "Katsuki", ""], ["Morishita", "Makoto", ""]]}, {"id": "2106.05469", "submitter": "Rabeeh Karimi Mahabadi", "authors": "Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson", "title": "Variational Information Bottleneck for Effective Low-Resource\n  Fine-Tuning", "comments": "ICLR, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large-scale pretrained language models have obtained impressive results\nwhen fine-tuned on a wide variety of tasks, they still often suffer from\noverfitting in low-resource scenarios. Since such models are general-purpose\nfeature extractors, many of these features are inevitably irrelevant for a\ngiven target task. We propose to use Variational Information Bottleneck (VIB)\nto suppress irrelevant features when fine-tuning on low-resource target tasks,\nand show that our method successfully reduces overfitting. Moreover, we show\nthat our VIB model finds sentence representations that are more robust to\nbiases in natural language inference datasets, and thereby obtains better\ngeneralization to out-of-domain datasets. Evaluation on seven low-resource\ndatasets in different tasks shows that our method significantly improves\ntransfer learning in low-resource scenarios, surpassing prior work. Moreover,\nit improves generalization on 13 out of 15 out-of-domain natural language\ninference benchmarks. Our code is publicly available in\nhttps://github.com/rabeehk/vibert.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:08:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mahabadi", "Rabeeh Karimi", ""], ["Belinkov", "Yonatan", ""], ["Henderson", "James", ""]]}, {"id": "2106.05505", "submitter": "Tyler A. Chang", "authors": "Tyler A. Chang, Yifan Xu, Weijian Xu, and Zhuowen Tu", "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in\n  Pre-trained Language Models", "comments": "Accepted to ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we detail the relationship between convolutions and\nself-attention in natural language tasks. We show that relative position\nembeddings in self-attention layers are equivalent to recently-proposed dynamic\nlightweight convolutions, and we consider multiple new ways of integrating\nconvolutions into Transformer self-attention. Specifically, we propose\ncomposite attention, which unites previous relative position embedding methods\nunder a convolutional framework. We conduct experiments by training BERT with\ncomposite attention, finding that convolutions consistently improve performance\non multiple downstream tasks, replacing absolute position embeddings. To inform\nfuture work, we present results comparing lightweight convolutions, dynamic\nconvolutions, and depthwise-separable convolutions in language model\npre-training, considering multiple injection points for convolutions in\nself-attention layers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 05:11:35 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chang", "Tyler A.", ""], ["Xu", "Yifan", ""], ["Xu", "Weijian", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2106.05532", "submitter": "Anjana Arunkumar", "authors": "Swaroop Mishra, Anjana Arunkumar", "title": "How Robust are Model Rankings: A Leaderboard Customization Approach for\n  Equitable Evaluation", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that top leaderboards often perform unsatisfactorily when deployed in\nreal world applications; this has necessitated rigorous and expensive\npre-deployment model testing. A hitherto unexplored facet of model performance\nis: Are our leaderboards doing equitable evaluation? In this paper, we\nintroduce a task-agnostic method to probe leaderboards by weighting samples\nbased on their `difficulty' level. We find that leaderboards can be\nadversarially attacked and top performing models may not always be the best\nmodels. We subsequently propose alternate evaluation metrics. Our experiments\non 10 models show changes in model ranking and an overall reduction in\npreviously reported performance -- thus rectifying the overestimation of AI\nsystems' capabilities. Inspired by behavioral testing principles, we further\ndevelop a prototype of a visual analytics tool that enables leaderboard\nrevamping through customization, based on an end user's focus area. This helps\nusers analyze models' strengths and weaknesses, and guides them in the\nselection of a model best suited for their application scenario. In a user\nstudy, members of various commercial product development teams, covering 5\nfocus areas, find that our prototype reduces pre-deployment development and\ntesting effort by 41% on average.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:47:35 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mishra", "Swaroop", ""], ["Arunkumar", "Anjana", ""]]}, {"id": "2106.05544", "submitter": "Yuqi Ren", "authors": "Yuqi Ren and Deyi Xiong", "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive\n  Language Processing Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:10:25 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:29:35 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ren", "Yuqi", ""], ["Xiong", "Deyi", ""]]}, {"id": "2106.05546", "submitter": "Liang Ding", "authors": "Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao,\n  Zhaopeng Tu", "title": "Progressive Multi-Granularity Training for Non-Autoregressive\n  Translation", "comments": "ACL 2021, Short Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Non-autoregressive translation (NAT) significantly accelerates the inference\nprocess via predicting the entire target sequence. However, recent studies show\nthat NAT is weak at learning high-mode of knowledge such as one-to-many\ntranslations. We argue that modes can be divided into various granularities\nwhich can be learned from easy to hard. In this study, we empirically show that\nNAT models are prone to learn fine-grained lower-mode knowledge, such as words\nand phrases, compared with sentences. Based on this observation, we propose\nprogressive multi-granularity training for NAT. More specifically, to make the\nmost of the training data, we break down the sentence-level examples into three\ntypes, i.e. words, phrases, sentences, and with the training goes, we\nprogressively increase the granularities. Experiments on Romanian-English,\nEnglish-German, Chinese-English, and Japanese-English demonstrate that our\napproach improves the phrase translation accuracy and model reordering ability,\ntherefore resulting in better translation quality against strong NAT baselines.\nAlso, we show that more deterministic fine-grained knowledge can further\nenhance performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:16:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:22:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ding", "Liang", ""], ["Wang", "Longyue", ""], ["Liu", "Xuebo", ""], ["Wong", "Derek F.", ""], ["Tao", "Dacheng", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2106.05555", "submitter": "Tom\\'a\\v{s} Nekvinda", "authors": "Tom\\'a\\v{s} Nekvinda and Ond\\v{r}ej Du\\v{s}ek", "title": "Shades of BLEU, Flavours of Success: The Case of MultiWOZ", "comments": "Accepted to GEM Workshop at ACL 2021; for the source files, see\n  https://github.com/Tomiinek/MultiWOZ_Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for\nbenchmarking context-to-response abilities of task-oriented dialogue systems.\nIn this work, we identify inconsistencies in data preprocessing and reporting\nof three corpus-based metrics used on this dataset, i.e., BLEU score and Inform\n& Success rates. We point out a few problems of the MultiWOZ benchmark such as\nunsatisfactory preprocessing, insufficient or under-specified evaluation\nmetrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy\noptimization models in as-fair-as-possible setups, and we show that their\nreported scores cannot be directly compared. To facilitate comparison of future\nsystems, we release our stand-alone standardized evaluation scripts. We also\ngive basic recommendations for corpus-based benchmarking in future works.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:33:53 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Nekvinda", "Tom\u00e1\u0161", ""], ["Du\u0161ek", "Ond\u0159ej", ""]]}, {"id": "2106.05580", "submitter": "Xinnuo Xu", "authors": "Xinnuo Xu, Ond\\v{r}ej Du\\v{s}ek, Verena Rieser, Ioannis Konstas", "title": "AGGGEN: Ordering and Aggregating while Generating", "comments": "Correct the first citation in the Zero-shot Few-shot scenarios\n  paragraph in Section 7", "journal-ref": "Proceedings of the 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present AGGGEN (pronounced 'again'), a data-to-text model which\nre-introduces two explicit sentence planning stages into neural data-to-text\nsystems: input ordering and input aggregation. In contrast to previous work\nusing sentence planning, our model is still end-to-end: AGGGEN performs\nsentence planning at the same time as generating text by learning latent\nalignments (via semantic facts) between input representation and target text.\nExperiments on the WebNLG and E2E challenge data show that by using fact-based\nalignments our approach is more interpretable, expressive, robust to noise, and\neasier to control, while retaining the advantages of end-to-end systems in\nterms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:14:59 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 07:53:44 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xu", "Xinnuo", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Rieser", "Verena", ""], ["Konstas", "Ioannis", ""]]}, {"id": "2106.05589", "submitter": "Xinnuo Xu", "authors": "Xinnuo Xu, Guoyin Wang, Young-Bum Kim, Sungjin Lee", "title": "AUGNLG: Few-shot Natural Language Generation using Self-trained Data\n  Augmentation", "comments": null, "journal-ref": "Proceedings of the 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Generation (NLG) is a key component in a task-oriented\ndialogue system, which converts the structured meaning representation (MR) to\nthe natural language. For large-scale conversational systems, where it is\ncommon to have over hundreds of intents and thousands of slots, neither\ntemplate-based approaches nor model-based approaches are scalable. Recently,\nneural NLGs started leveraging transfer learning and showed promising results\nin few-shot settings. This paper proposes AUGNLG, a novel data augmentation\napproach that combines a self-trained neural retrieval model with a few-shot\nlearned NLU model, to automatically create MR-to-Text data from open-domain\ntexts. The proposed system mostly outperforms the state-of-the-art methods on\nthe FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm\nimproved results on the FewShotSGD data and provide comprehensive analysis\nresults on key components of our system. Our code and data are available at\nhttps://github.com/XinnuoXu/AugNLG.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:45:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Xu", "Xinnuo", ""], ["Wang", "Guoyin", ""], ["Kim", "Young-Bum", ""], ["Lee", "Sungjin", ""]]}, {"id": "2106.05606", "submitter": "Lei Cui", "authors": "Tengchao Lv, Lei Cui, Momcilo Vasilijevic, Furu Wei", "title": "VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and\n  Summarization", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video transcript summarization is a fundamental task for video understanding.\nConventional approaches for transcript summarization are usually built upon the\nsummarization data for written language such as news articles, while the domain\ndiscrepancy may degrade the model performance on spoken text. In this paper, we\npresent VT-SSum, a benchmark dataset with spoken language for video transcript\nsegmentation and summarization, which includes 125K transcript-summary pairs\nfrom 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET\nby leveraging the slides content as the weak supervision to generate the\nextractive summary for video transcripts. Experiments with a state-of-the-art\ndeep learning approach show that the model trained with VT-SSum brings a\nsignificant improvement on the AMI spoken text summarization benchmark. VT-SSum\nis publicly available at https://github.com/Dod-o/VT-SSum to support the future\nresearch of video transcript segmentation and summarization tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:19:58 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 06:13:31 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lv", "Tengchao", ""], ["Cui", "Lei", ""], ["Vasilijevic", "Momcilo", ""], ["Wei", "Furu", ""]]}, {"id": "2106.05630", "submitter": "Xu Tan", "authors": "Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, Tie-Yan Liu", "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training", "comments": "Accepted by ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic music understanding, which refers to the understanding of music from\nthe symbolic data (e.g., MIDI format, but not audio), covers many music\napplications such as genre classification, emotion classification, and music\npieces matching. While good music representations are beneficial for these\napplications, the lack of training data hinders representation learning.\nInspired by the success of pre-training models in natural language processing,\nin this paper, we develop MusicBERT, a large-scale pre-trained model for music\nunderstanding. To this end, we construct a large-scale symbolic music corpus\nthat contains more than 1 million music songs. Since symbolic music contains\nmore structural (e.g., bar, position) and diverse information (e.g., tempo,\ninstrument, and pitch), simply adopting the pre-training techniques from NLP to\nsymbolic music only brings marginal gains. Therefore, we design several\nmechanisms, including OctupleMIDI encoding and bar-level masking strategy, to\nenhance pre-training with symbolic music data. Experiments demonstrate the\nadvantages of MusicBERT on four music understanding tasks, including melody\ncompletion, accompaniment suggestion, genre classification, and style\nclassification. Ablation studies also verify the effectiveness of our designs\nof OctupleMIDI encoding and bar-level masking strategy in MusicBERT.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:13:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zeng", "Mingliang", ""], ["Tan", "Xu", ""], ["Wang", "Rui", ""], ["Ju", "Zeqian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.05634", "submitter": "Christos Baziotis", "authors": "Christos Baziotis, Ivan Titov, Alexandra Birch, Barry Haddow", "title": "Exploring Unsupervised Pretraining Objectives for Machine Translation", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised cross-lingual pretraining has achieved strong results in neural\nmachine translation (NMT), by drastically reducing the need for large parallel\ndata. Most approaches adapt masked-language modeling (MLM) to\nsequence-to-sequence architectures, by masking parts of the input and\nreconstructing them in the decoder. In this work, we systematically compare\nmasking with alternative objectives that produce inputs resembling real (full)\nsentences, by reordering and replacing words based on their context. We\npretrain models with different methods on English$\\leftrightarrow$German,\nEnglish$\\leftrightarrow$Nepali and English$\\leftrightarrow$Sinhala monolingual\ndata, and evaluate them on NMT. In (semi-) supervised NMT, varying the\npretraining objective leads to surprisingly small differences in the finetuned\nperformance, whereas unsupervised NMT is much more sensitive to it. To\nunderstand these results, we thoroughly study the pretrained models using a\nseries of probes and verify that they encode and use information in different\nways. We conclude that finetuning on parallel data is mostly sensitive to few\nproperties that are shared by most models, such as a strong decoder, in\ncontrast to unsupervised NMT that also requires models with strong\ncross-lingual abilities.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:18:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Baziotis", "Christos", ""], ["Titov", "Ivan", ""], ["Birch", "Alexandra", ""], ["Haddow", "Barry", ""]]}, {"id": "2106.05642", "submitter": "Binbin Zhang", "authors": "Di Wu, Binbin Zhang, Chao Yang, Zhendong Peng, Wenjing Xia, Xiaoyu\n  Chen, Xin Lei", "title": "U2++: Unified Two-pass Bidirectional End-to-end Model for Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The unified streaming and non-streaming two-pass (U2) end-to-end model for\nspeech recognition has shown great performance in terms of streaming\ncapability, accuracy, real-time factor (RTF), and latency. In this paper, we\npresent U2++, an enhanced version of U2 to further improve the accuracy. The\ncore idea of U2++ is to use the forward and the backward information of the\nlabeling sequences at the same time at training to learn richer information,\nand combine the forward and backward prediction at decoding to give more\naccurate recognition results. We also proposed a new data augmentation method\ncalled SpecSub to help the U2++ model to be more accurate and robust. Our\nexperiments show that, compared with U2, U2++ shows faster convergence at\ntraining, better robustness to the decoding method, as well as consistent 5\\% -\n8\\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we\nachieve a 4.63\\% character error rate (CER) with a non-streaming setup and\n5.05\\% with a streaming setup with 320ms latency by U2++. To the best of our\nknowledge, 5.05\\% is the best-published streaming result on the AISHELL-1 test\nset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:25:15 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:38:58 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wu", "Di", ""], ["Zhang", "Binbin", ""], ["Yang", "Chao", ""], ["Peng", "Zhendong", ""], ["Xia", "Wenjing", ""], ["Chen", "Xiaoyu", ""], ["Lei", "Xin", ""]]}, {"id": "2106.05664", "submitter": "Rishav Hada", "authors": "Rishav Hada, Sohi Sudhir, Pushkar Mishra, Helen Yannakoudakis, Saif M.\n  Mohammad, Ekaterina Shutova", "title": "Ruddit: Norms of Offensiveness for English Reddit Comments", "comments": "Camera-ready version in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On social media platforms, hateful and offensive language negatively impact\nthe mental well-being of users and the participation of people from diverse\nbackgrounds. Automatic methods to detect offensive language have largely relied\non datasets with categorical labels. However, comments can vary in their degree\nof offensiveness. We create the first dataset of English language Reddit\ncomments that has fine-grained, real-valued scores between -1 (maximally\nsupportive) and 1 (maximally offensive). The dataset was annotated using\nBest--Worst Scaling, a form of comparative annotation that has been shown to\nalleviate known biases of using rating scales. We show that the method produces\nhighly reliable offensiveness scores. Finally, we evaluate the ability of\nwidely-used neural models to predict offensiveness scores on this new dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:27:47 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:41:58 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hada", "Rishav", ""], ["Sudhir", "Sohi", ""], ["Mishra", "Pushkar", ""], ["Yannakoudakis", "Helen", ""], ["Mohammad", "Saif M.", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2106.05677", "submitter": "Benjamin Murauer", "authors": "Benjamin Murauer and G\\\"unther Specht", "title": "DT-grams: Structured Dependency Grammar Stylometry for Cross-Language\n  Authorship Attribution", "comments": "To be published in: \"32. GI-Workshop Grundlagen von Datenbanken\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cross-language authorship attribution problems rely on either translation to\nenable the use of single-language features, or language-independent feature\nextraction methods. Until recently, the lack of datasets for this problem\nhindered the development of the latter, and single-language solutions were\nperformed on machine-translated corpora. In this paper, we present a novel\nlanguage-independent feature for authorship analysis based on dependency graphs\nand universal part of speech tags, called DT-grams (dependency tree grams),\nwhich are constructed by selecting specific sub-parts of the dependency graph\nof sentences. We evaluate DT-grams by performing cross-language authorship\nattribution on untranslated datasets of bilingual authors, showing that, on\naverage, they achieve a macro-averaged F1 score of 0.081 higher than previous\nmethods across five different language pairs. Additionally, by providing\nresults for a diverse set of features for comparison, we provide a baseline on\nthe previously undocumented task of untranslated cross-language authorship\nattribution.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:50:07 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Murauer", "Benjamin", ""], ["Specht", "G\u00fcnther", ""]]}, {"id": "2106.05691", "submitter": "Yuanxin Liu", "authors": "Yuanxin Liu and Fandong Meng and Zheng Lin and Weiping Wang and Jie\n  Zhou", "title": "Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT\n  Knowledge Distillation", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, knowledge distillation (KD) has shown great success in BERT\ncompression. Instead of only learning from the teacher's soft label as in\nconventional KD, researchers find that the rich information contained in the\nhidden layers of BERT is conducive to the student's performance. To better\nexploit the hidden knowledge, a common practice is to force the student to\ndeeply mimic the teacher's hidden states of all the tokens in a layer-wise\nmanner. In this paper, however, we observe that although distilling the\nteacher's hidden state knowledge (HSK) is helpful, the performance gain\n(marginal utility) diminishes quickly as more HSK is distilled. To understand\nthis effect, we conduct a series of analysis. Specifically, we divide the HSK\nof BERT into three dimensions, namely depth, length and width. We first\ninvestigate a variety of strategies to extract crucial knowledge for each\nsingle dimension and then jointly compress the three dimensions. In this way,\nwe show that 1) the student's performance can be improved by extracting and\ndistilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the\nsame performance as extensive HSK distillation. Based on the second finding, we\nfurther propose an efficient KD paradigm to compress BERT, which does not\nrequire loading the teacher during the training of student. For two kinds of\nstudent models and computing devices, the proposed KD paradigm gives rise to\ntraining speedup of 2.7x ~ 3.4x.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:21:47 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liu", "Yuanxin", ""], ["Meng", "Fandong", ""], ["Lin", "Zheng", ""], ["Wang", "Weiping", ""], ["Zhou", "Jie", ""]]}, {"id": "2106.05707", "submitter": "Rami Aly", "authors": "Rami Aly, Zhijiang Guo, Michael Schlichtkrull, James Thorne, Andreas\n  Vlachos, Christos Christodoulopoulos, Oana Cocarascu, Arpit Mittal", "title": "FEVEROUS: Fact Extraction and VERification Over Unstructured and\n  Structured information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:47:36 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Aly", "Rami", ""], ["Guo", "Zhijiang", ""], ["Schlichtkrull", "Michael", ""], ["Thorne", "James", ""], ["Vlachos", "Andreas", ""], ["Christodoulopoulos", "Christos", ""], ["Cocarascu", "Oana", ""], ["Mittal", "Arpit", ""]]}, {"id": "2106.05723", "submitter": "Jihye Park", "authors": "Jihye Park, Hye Jin Lee, Sungzoon Cho", "title": "Automatic Construction of Context-Aware Sentiment Lexicon in the\n  Financial Domain Using Direction-Dependent Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing attention has been drawn to the sentiment analysis of financial\ndocuments. The most popular examples of such documents include analyst reports\nand economic news, the analysis of which is frequently used to capture the\ntrends in market sentiments. On the other hand, the significance of the role\nsentiment analysis plays in the financial domain has given rise to the efforts\nto construct a financial domain-specific sentiment lexicon. Sentiment lexicons\nlend a hand for solving various text mining tasks, such as unsupervised\nclassification of text data, while alleviating the arduous human labor required\nfor manual labeling. One of the challenges in the construction of an effective\nsentiment lexicon is that the semantic orientation of a word may change\ndepending on the context in which it appears. For instance, the word ``profit\"\nusually conveys positive sentiments; however, when the word is juxtaposed with\nanother word ``decrease,\" the sentiment associated with the phrase ``profit\ndecreases\" now becomes negative. Hence, the sentiment of a given word may shift\nas one begins to consider the context surrounding the word. In this paper, we\naddress this issue by incorporating context when building sentiment lexicon\nfrom a given corpus. Specifically, we construct a lexicon named Senti-DD for\nthe Sentiment lexicon composed of Direction-Dependent words, which expresses\neach term a pair of a directional word and a direction-dependent word.\nExperiment results show that higher classification performance is achieved with\nSenti-DD, proving the effectiveness of our method for automatically\nconstructing a context-aware sentiment lexicon in the financial domain.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:08:00 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Park", "Jihye", ""], ["Lee", "Hye Jin", ""], ["Cho", "Sungzoon", ""]]}, {"id": "2106.05752", "submitter": "Sourav Das", "authors": "Sourav Das and Anup Kumar Kolya", "title": "Parallel Deep Learning-Driven Sarcasm Detection from Pop Culture Text\n  and English Humor Literature", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": "In RAAI 2020. Advances in Intelligent Systems and Computing, vol\n  1355 (2021)", "doi": "10.1007/978-981-16-1543-6_6", "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sarcasm is a sophisticated way of wrapping any immanent truth, mes-sage, or\neven mockery within a hilarious manner. The advent of communications using\nsocial networks has mass-produced new avenues of socialization. It can be\nfurther said that humor, irony, sarcasm, and wit are the four chariots of being\nsocially funny in the modern days. In this paper, we manually extract the\nsarcastic word distribution features of a benchmark pop culture sarcasm corpus,\ncontaining sarcastic dialogues and monologues. We generate input sequences\nformed of the weighted vectors from such words. We further propose an\namalgamation of four parallel deep long-short term networks (pLSTM), each with\ndistinctive activation classifier. These modules are primarily aimed at\nsuccessfully detecting sarcasm from the text corpus. Our proposed model for\ndetecting sarcasm peaks a training accuracy of 98.95% when trained with the\ndiscussed dataset. Consecutively, it obtains the highest of 98.31% overall\nvalidation accuracy on two handpicked Project Gutenberg English humor\nliterature among all the test cases. Our approach transcends previous\nstate-of-the-art works on several sarcasm corpora and results in a new gold\nstandard performance for sarcasm detection.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:01:07 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Das", "Sourav", ""], ["Kolya", "Anup Kumar", ""]]}, {"id": "2106.05762", "submitter": "Iv\\'an Vall\\'es P\\'erez", "authors": "Iv\\'an Vall\\'es-P\\'erez, Julian Roth, Grzegorz Beringer, Roberto\n  Barra-Chicote, Jasha Droppo", "title": "Improving multi-speaker TTS prosody variance with a residual encoder and\n  normalizing flows", "comments": "in Proceedings of Interspeech 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-speech systems recently achieved almost indistinguishable quality\nfrom human speech. However, the prosody of those systems is generally flatter\nthan natural speech, producing samples with low expressiveness. Disentanglement\nof speaker id and prosody is crucial in text-to-speech systems to improve on\nnaturalness and produce more variable syntheses. This paper proposes a new\nneural text-to-speech model that approaches the disentanglement problem by\nconditioning a Tacotron2-like architecture on flow-normalized speaker\nembeddings, and by substituting the reference encoder with a new learned latent\ndistribution responsible for modeling the intra-sentence variability due to the\nprosody. By removing the reference encoder dependency, the speaker-leakage\nproblem typically happening in this kind of systems disappears, producing more\ndistinctive syntheses at inference time. The new model achieves significantly\nhigher prosody variance than the baseline in a set of quantitative prosody\nfeatures, as well as higher speaker distinctiveness, without decreasing the\nspeaker intelligibility. Finally, we observe that the normalized speaker\nembeddings enable much richer speaker interpolations, substantially improving\nthe distinctiveness of the new interpolated speakers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:08:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Vall\u00e9s-P\u00e9rez", "Iv\u00e1n", ""], ["Roth", "Julian", ""], ["Beringer", "Grzegorz", ""], ["Barra-Chicote", "Roberto", ""], ["Droppo", "Jasha", ""]]}, {"id": "2106.05768", "submitter": "Sophia Althammer", "authors": "Sophia Althammer, Mark Buckley, Sebastian Hofst\\\"atter, Allan Hanbury", "title": "Linguistically Informed Masking for Representation Learning in the\n  Patent Domain", "comments": "Published at SIGIR 2021 PatentSemTech workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain-specific contextualized language models have demonstrated substantial\neffectiveness gains for domain-specific downstream tasks, like similarity\nmatching, entity recognition or information retrieval. However successfully\napplying such models in highly specific language domains requires domain\nadaptation of the pre-trained models. In this paper we propose the empirically\nmotivated Linguistically Informed Masking (LIM) method to focus\ndomain-adaptative pre-training on the linguistic patterns of patents, which use\na highly technical sublanguage. We quantify the relevant differences between\npatent, scientific and general-purpose language and demonstrate for two\ndifferent language models (BERT and SciBERT) that domain adaptation with LIM\nleads to systematically improved representations by evaluating the performance\nof the domain-adapted representations of patent language on two independent\ndownstream tasks, the IPC classification and similarity matching. We\ndemonstrate the impact of balancing the learning from different information\nsources during domain adaptation for the patent domain. We make the source code\nas well as the domain-adaptive pre-trained patent language models publicly\navailable at https://github.com/sophiaalthammer/patent-lim.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:20:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Althammer", "Sophia", ""], ["Buckley", "Mark", ""], ["Hofst\u00e4tter", "Sebastian", ""], ["Hanbury", "Allan", ""]]}, {"id": "2106.05784", "submitter": "Tal Schuster", "authors": "Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, Adam Tauman Kalai", "title": "Programming Puzzles", "comments": "The puzzles repo:\n  https://github.com/microsoft/PythonProgrammingPuzzles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of programming challenge called programming puzzles,\nas an objective and comprehensive evaluation of program synthesis, and release\nan open-source dataset of Python Programming Puzzles (P3). Each puzzle is\ndefined by a short Python program $f$, and the goal is to find an input $x$\nwhich makes $f$ output \"True\". The puzzles are objective in that each one is\nspecified entirely by the source code of its verifier $f$, so evaluating $f(x)$\nis all that is needed to test a candidate solution $x$. They do not require an\nanswer key or input/output examples, nor do they depend on natural language\nunderstanding. The dataset is comprehensive in that it spans problems of a\nrange of difficulties and domains, ranging from trivial string manipulation\nproblems that are immediately obvious to human programmers (but not necessarily\nto AI), to classic programming puzzles (e.g., Towers of Hanoi), to\ninterview/competitive-programming problems (e.g., dynamic programming), to\nlongstanding open problems in algorithms and mathematics (e.g., factoring). The\nobjective nature of P3 readily supports self-supervised bootstrapping. We\ndevelop baseline enumerative program synthesis and GPT-3 solvers that are\ncapable of solving easy puzzles -- even without access to any reference\nsolutions -- by learning from their own past solutions. Based on a small user\nstudy, we find puzzle difficulty to correlate between human programmers and the\nbaseline AI solvers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:37:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Schuster", "Tal", ""], ["Kalyan", "Ashwin", ""], ["Polozov", "Oleksandr", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "2106.05822", "submitter": "Ivan Chelombiev", "authors": "Ivan Chelombiev, Daniel Justus, Douglas Orr, Anastasia Dietrich,\n  Frithjof Gressmann, Alexandros Koliousis, Carlo Luschi", "title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Attention based language models have become a critical component in\nstate-of-the-art natural language processing systems. However, these models\nhave significant computational requirements, due to long training times, dense\noperations and large parameter count. In this work we demonstrate a set of\nmodifications to the structure of a Transformer layer, producing a more\nefficient architecture. First, we add a convolutional module to complement the\nself-attention module, decoupling the learning of local and global\ninteractions. Secondly, we rely on grouped transformations to reduce the\ncomputational cost of dense feed-forward layers and convolutions, while\npreserving the expressivity of the model. We apply the resulting architecture\nto language representation learning and demonstrate its superior performance\ncompared to BERT models of different scales. We further highlight its improved\nefficiency, both in terms of floating-point operations (FLOPs) and\ntime-to-train.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:41:53 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chelombiev", "Ivan", ""], ["Justus", "Daniel", ""], ["Orr", "Douglas", ""], ["Dietrich", "Anastasia", ""], ["Gressmann", "Frithjof", ""], ["Koliousis", "Alexandros", ""], ["Luschi", "Carlo", ""]]}, {"id": "2106.05823", "submitter": "Usama Yaseen", "authors": "Usama Yaseen, Stefan Langer", "title": "Neural Text Classification and Stacked Heterogeneous Embeddings for\n  Named Entity Recognition in SMM4H 2021", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our findings from participating in the SMM4H Shared Task\n2021. We addressed Named Entity Recognition (NER) and Text Classification. To\naddress NER we explored BiLSTM-CRF with Stacked Heterogeneous Embeddings and\nlinguistic features. We investigated various machine learning algorithms\n(logistic regression, Support Vector Machine (SVM) and Neural Networks) to\naddress text classification. Our proposed approaches can be generalized to\ndifferent languages and we have shown its effectiveness for English and\nSpanish. Our text classification submissions (team:MIC-NLP) have achieved\ncompetitive performance with F1-score of $0.46$ and $0.90$ on ADE\nClassification (Task 1a) and Profession Classification (Task 7a) respectively.\nIn the case of NER, our submissions scored F1-score of $0.50$ and $0.82$ on ADE\nSpan Detection (Task 1b) and Profession Span detection (Task 7b) respectively.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:43:21 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 13:23:15 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yaseen", "Usama", ""], ["Langer", "Stefan", ""]]}, {"id": "2106.05830", "submitter": "Dingmin Wang", "authors": "Dingmin Wang, Ziyao Chen, Wanwei He, Li Zhong, Yunzhe Tao, Min Yang", "title": "A Template-guided Hybrid Pointer Network for\n  Knowledge-basedTask-oriented Dialogue Systems", "comments": "DialDoc workshop@ACL-IJCNLP-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing neural network based task-oriented dialogue systems follow\nencoder-decoder paradigm, where the decoder purely depends on the source texts\nto generate a sequence of words, usually suffering from instability and poor\nreadability. Inspired by the traditional template-based generation approaches,\nwe propose a template-guided hybrid pointer network for the knowledge-based\ntask-oriented dialogue system, which retrieves several potentially relevant\nanswers from a pre-constructed domain-specific conversational repository as\nguidance answers, and incorporates the guidance answers into both the encoding\nand decoding processes. Specifically, we design a memory pointer network model\nwith a gating mechanism to fully exploit the semantic correlation between the\nretrieved answers and the ground-truth response. We evaluate our model on four\nwidely used task-oriented datasets, including one simulated and three manually\ncreated datasets. The experimental results demonstrate that the proposed model\nachieves significantly better performance than the state-of-the-art methods\nover different automatic evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:49:26 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Dingmin", ""], ["Chen", "Ziyao", ""], ["He", "Wanwei", ""], ["Zhong", "Li", ""], ["Tao", "Yunzhe", ""], ["Yang", "Min", ""]]}, {"id": "2106.05852", "submitter": "Devaraja Adiga", "authors": "Devaraja Adiga, Rishabh Kumar, Amrith Krishna, Preethi Jyothi, Ganesh\n  Ramakrishnan, Pawan Goyal", "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and\n  Modelling Insights", "comments": "Accepted paper at the 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2021 Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 18:06:32 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 07:16:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Adiga", "Devaraja", ""], ["Kumar", "Rishabh", ""], ["Krishna", "Amrith", ""], ["Jyothi", "Preethi", ""], ["Ramakrishnan", "Ganesh", ""], ["Goyal", "Pawan", ""]]}, {"id": "2106.05885", "submitter": "Amir Hussein", "authors": "Amir Hussein, Shammur Chowdhury, Ahmed Ali", "title": "KARI: KAnari/QCRI's End-to-End systems for the INTERSPEECH 2021 Indian\n  Languages Code-Switching Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present the Kanari/QCRI (KARI) system and the modeling\nstrategies used to participate in the Interspeech 2021 Code-switching (CS)\nchallenge for low-resource Indian languages. The subtask involved developing a\nspeech recognition system for two CS datasets: Hindi-English and\nBengali-English, collected in a real-life scenario. To tackle the CS\nchallenges, we use transfer learning for incorporating the publicly available\nmonolingual Hindi, Bengali, and English speech data. In this work, we study the\neffectiveness of two steps transfer learning protocol for low-resourced CS\ndata: monolingual pretraining, followed by fine-tuning. For acoustic modeling,\nwe develop an end-to-end convolution-augmented transformer (Conformer). We show\nthat selecting the percentage of each monolingual data affects model biases\ntowards using one language character set over the other in a CS scenario. The\nmodels pretrained on well-aligned and accurate monolingual data showed\nrobustness against misalignment between the segments and the transcription.\nFinally, we develop word-level n-gram language models (LM) to rescore ASR\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:12:51 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hussein", "Amir", ""], ["Chowdhury", "Shammur", ""], ["Ali", "Ahmed", ""]]}, {"id": "2106.05894", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Yulia Tsvetkov, Jeffrey P. Bigham", "title": "Synthesizing Adversarial Negative Responses for Robust Response Ranking\n  and Evaluation", "comments": "Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open-domain neural dialogue models have achieved high performance in response\nranking and evaluation tasks. These tasks are formulated as a binary\nclassification of responses given in a dialogue context, and models generally\nlearn to make predictions based on context-response content similarity.\nHowever, over-reliance on content similarity makes the models less sensitive to\nthe presence of inconsistencies, incorrect time expressions and other factors\nimportant for response appropriateness and coherence. We propose approaches for\nautomatically creating adversarial negative training data to help ranking and\nevaluation models learn features beyond content similarity. We propose\nmask-and-fill and keyword-guided approaches that generate negative examples for\ntraining more robust dialogue systems. These generated adversarial responses\nhave high content similarity with the contexts but are either incoherent,\ninappropriate or not fluent. Our approaches are fully data-driven and can be\neasily incorporated in existing models and datasets. Experiments on\nclassification, ranking and evaluation tasks across multiple datasets\ndemonstrate that our approaches outperform strong baselines in providing\ninformative negative examples for training dialogue systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:20:55 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gupta", "Prakhar", ""], ["Tsvetkov", "Yulia", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "2106.05903", "submitter": "Scott A. Hale", "authors": "Austin Botelho and Bertie Vidgen and Scott A. Hale", "title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for\n  Multimodal Hate", "comments": "Please note the paper contains examples of hateful content", "journal-ref": "Findings of ACL, 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and classification of online hate is a difficult task.\nImplicit hate is particularly challenging as such content tends to have unusual\nsyntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This\nproblem is heightened with multimodal content, such as memes (combinations of\ntext and images), as they are often harder to decipher than unimodal content\n(e.g., text alone). This paper evaluates the role of semantic and multimodal\ncontext for detecting implicit and explicit hate. We show that both text- and\nvisual- enrichment improves model performance, with the multimodal model\n(0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While\nthe unimodal-text context-aware (transformer) model was the most accurate on\nthe subtask of implicit hate detection, the multimodal model outperformed it\noverall because of a lower propensity towards false positives. We find that all\nmodels perform better on content with full annotator agreement and that\nmultimodal models are best at classifying the content where annotators\ndisagree. To conduct these investigations, we undertook high-quality annotation\nof a sample of 5,000 multimodal entries. Tweets were annotated for primary\ncategory, modality, and strategy. We make this corpus, along with the codebook,\ncode, and final model, freely available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:29:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Botelho", "Austin", ""], ["Vidgen", "Bertie", ""], ["Hale", "Scott A.", ""]]}, {"id": "2106.05933", "submitter": "Cheng-I Lai", "authors": "Cheng-I Jeff Lai, Yang Zhang, Alexander H. Liu, Shiyu Chang, Yi-Lun\n  Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khurana, David Cox, James Glass", "title": "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work on speech self-supervised learning (speech SSL) demonstrated the\nbenefits of scale in learning rich and transferable representations for\nAutomatic Speech Recognition (ASR) with limited parallel data. It is then\nnatural to investigate the existence of sparse and transferrable subnetworks in\npre-trained speech SSL models that can achieve even better low-resource ASR\nperformance. However, directly applying widely adopted pruning methods such as\nthe Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost\nneeded. Moreover, contrary to what LTH predicts, the discovered subnetworks\nyield minimal performance gain compared to the original dense network. In this\nwork, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes\nsubnetworks for much better ASR performance, while only requiring a single\ndownstream finetuning run. PARP is inspired by our surprising observation that\nsubnetworks pruned for pre-training tasks only needed to be slightly adjusted\nto achieve a sizeable performance boost in downstream ASR tasks. Extensive\nexperiments on low-resource English and multi-lingual ASR show (1) sparse\nsubnetworks exist in pre-trained speech SSL, and (2) the computational\nadvantage and performance gain of PARP over baseline pruning methods. On the\n10min Librispeech split without LM decoding, PARP discovers subnetworks from\nwav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full\nmodel. We demonstrate PARP mitigates performance degradation in cross-lingual\nmask transfer, and investigate the possibility of discovering a single\nsubnetwork for 10 spoken languages in one run.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:32:25 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lai", "Cheng-I Jeff", ""], ["Zhang", "Yang", ""], ["Liu", "Alexander H.", ""], ["Chang", "Shiyu", ""], ["Liao", "Yi-Lun", ""], ["Chuang", "Yung-Sung", ""], ["Qian", "Kaizhi", ""], ["Khurana", "Sameer", ""], ["Cox", "David", ""], ["Glass", "James", ""]]}, {"id": "2106.05970", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Xin Eric Wang, An Yan, Miguel Eckstein, William Yang Wang", "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural\n  Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluations for natural language generation (NLG) conventionally\nrely on token-level or embedding-level comparisons with the text references.\nThis is different from human language processing, for which visual imaginations\noften improve comprehension. In this work, we propose ImaginE, an\nimagination-based automatic evaluation metric for natural language generation.\nWith the help of CLIP and DALL-E, two cross-modal models pre-trained on\nlarge-scale image-text pairs, we automatically generate an image as the\nembodied imagination for the text snippet and compute the imagination\nsimilarity using contextual embeddings. Experiments spanning several text\ngeneration tasks demonstrate that adding imagination with our ImaginE displays\ngreat potential in introducing multi-modal information into NLG evaluation, and\nimproves existing automatic metrics' correlations with human similarity\njudgments in many circumstances.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:52 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhu", "Wanrong", ""], ["Wang", "Xin Eric", ""], ["Yan", "An", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "2106.06002", "submitter": "Austin Blodgett", "authors": "Austin Blodgett and Nathan Schneider", "title": "Probabilistic, Structure-Aware Algorithms for Improved Variety,\n  Accuracy, and Coverage of AMR Alignments", "comments": "ACL 2021 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present algorithms for aligning components of Abstract Meaning\nRepresentation (AMR) graphs to spans in English sentences. We leverage\nunsupervised learning in combination with heuristics, taking the best of both\nworlds from previous AMR aligners. Our unsupervised models, however, are more\nsensitive to graph substructures, without requiring a separate syntactic parse.\nOur approach covers a wider variety of AMR substructures than previously\nconsidered, achieves higher coverage of nodes and edges, and does so with\nhigher accuracy. We will release our LEAMR datasets and aligner for use in\nresearch on AMR parsing, generation, and evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 18:46:32 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Blodgett", "Austin", ""], ["Schneider", "Nathan", ""]]}, {"id": "2106.06004", "submitter": "Sai Muralidhar Jayanthi", "authors": "Sai Muralidhar Jayanthi, Kavya Nerella, Khyathi Raghavi Chandu, Alan W\n  Black", "title": "CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing", "comments": "Accepted at the Fifth Workshop on Computational Approaches to\n  Linguistic Code-Switching-CALCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NLP community has witnessed steep progress in a variety of tasks across\nthe realms of monolingual and multilingual language processing recently. These\nsuccesses, in conjunction with the proliferating mixed language interactions on\nsocial media have boosted interest in modeling code-mixed texts. In this work,\nwe present CodemixedNLP, an open-source library with the goals of bringing\ntogether the advances in code-mixed NLP and opening it up to a wider machine\nlearning community. The library consists of tools to develop and benchmark\nversatile model architectures that are tailored for mixed texts, methods to\nexpand training sets, techniques to quantify mixing styles, and fine-tuned\nstate-of-the-art models for 7 tasks in Hinglish. We believe this work has a\npotential to foster a distributed yet collaborative and sustainable ecosystem\nin an otherwise dispersed space of code-mixing research. The toolkit is\ndesigned to be simple, easily extensible, and resourceful to both researchers\nas well as practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 18:49:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Jayanthi", "Sai Muralidhar", ""], ["Nerella", "Kavya", ""], ["Chandu", "Khyathi Raghavi", ""], ["Black", "Alan W", ""]]}, {"id": "2106.06017", "submitter": "Sabit Hassan", "authors": "Sabit Hassan, Shaden Shaar, Kareem Darwish", "title": "Cross-lingual Emotion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion detection is of great importance for understanding humans.\nConstructing annotated datasets to train automated models can be expensive. We\nexplore the efficacy of cross-lingual approaches that would use data from a\nsource language to build models for emotion detection in a target language. We\ncompare three approaches, namely: i) using inherently multilingual models; ii)\ntranslating training data into the target language; and iii) using an\nautomatically tagged parallel corpus. In our study, we consider English as the\nsource language with Arabic and Spanish as target languages. We study the\neffectiveness of different classification models such as BERT and SVMs trained\nwith different features. Our BERT-based monolingual models that are trained on\ntarget language data surpass state-of-the-art (SOTA) by 4% and 5% absolute\nJaccard score for Arabic and Spanish respectively. Next, we show that using\ncross-lingual approaches with English data alone, we can achieve more than 90%\nand 80% relative effectiveness of the Arabic and Spanish BERT models\nrespectively. Lastly, we use LIME to interpret the differences between models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:52:06 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hassan", "Sabit", ""], ["Shaar", "Shaden", ""], ["Darwish", "Kareem", ""]]}, {"id": "2106.06038", "submitter": "Jishnu Ray Chowdhury", "authors": "Jishnu Ray Chowdhury, Cornelia Caragea", "title": "Modeling Hierarchical Structures with Continuous Recursive Neural\n  Networks", "comments": "Accepted in ICML 2021 (long talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recursive Neural Networks (RvNNs), which compose sequences according to their\nunderlying hierarchical syntactic structure, have performed well in several\nnatural language processing tasks compared to similar models without structural\nbiases. However, traditional RvNNs are incapable of inducing the latent\nstructure in a plain text sequence on their own. Several extensions have been\nproposed to overcome this limitation. Nevertheless, these extensions tend to\nrely on surrogate gradients or reinforcement learning at the cost of higher\nbias or variance. In this work, we propose Continuous Recursive Neural Network\n(CRvNN) as a backpropagation-friendly alternative to address the aforementioned\nlimitations. This is done by incorporating a continuous relaxation to the\ninduced structure. We demonstrate that CRvNN achieves strong performance in\nchallenging synthetic tasks such as logical inference and ListOps. We also show\nthat CRvNN performs comparably or better than prior latent structure models on\nreal-world tasks such as sentiment analysis and natural language inference.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 20:42:05 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chowdhury", "Jishnu Ray", ""], ["Caragea", "Cornelia", ""]]}, {"id": "2106.06052", "submitter": "Douwe Kiela", "authors": "Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu,\n  Robin Jia, Christopher Potts, Adina Williams, Douwe Kiela", "title": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic\n  Next-Generation Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Dynaboard, an evaluation-as-a-service framework for hosting\nbenchmarks and conducting holistic model comparison, integrated with the\nDynabench platform. Our platform evaluates NLP models directly instead of\nrelying on self-reported metrics or predictions on a single dataset. Under this\nparadigm, models are submitted to be evaluated in the cloud, circumventing the\nissues of reproducibility, accessibility, and backwards compatibility that\noften hinder benchmarking in NLP. This allows users to interact with uploaded\nmodels in real time to assess their quality, and permits the collection of\nadditional metrics such as memory use, throughput, and robustness, which --\ndespite their importance to practitioners -- have traditionally been absent\nfrom leaderboards. On each task, models are ranked according to the Dynascore,\na novel utility-based aggregation of these statistics, which users can\ncustomize to better reflect their preferences, placing more/less weight on a\nparticular axis of evaluation or dataset. As state-of-the-art NLP models push\nthe limits of traditional benchmarks, Dynaboard offers a standardized solution\nfor a more diverse and comprehensive evaluation of model quality.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:17:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ma", "Zhiyi", ""], ["Ethayarajh", "Kawin", ""], ["Thrush", "Tristan", ""], ["Jain", "Somya", ""], ["Wu", "Ledell", ""], ["Jia", "Robin", ""], ["Potts", "Christopher", ""], ["Williams", "Adina", ""], ["Kiela", "Douwe", ""]]}, {"id": "2106.06082", "submitter": "Bradley Hauer", "authors": "Bradley Hauer, Grzegorz Kondrak", "title": "One Sense Per Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The idea of using lexical translations to define sense inventories has a long\nhistory in lexical semantics. We propose a theoretical framework which allows\nus to answer the question of why this apparently reasonable idea failed to\nproduce useful results. We formally prove several propositions on how the\ntranslations of a word relate to its senses, as well as on the relationship\nbetween synonymy and polysemy. We empirically validate our theoretical findings\non BabelNet, and demonstrate how they could be used to perform unsupervised\nword sense disambiguation of a substantial fraction of the lexicon.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 23:24:26 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hauer", "Bradley", ""], ["Kondrak", "Grzegorz", ""]]}, {"id": "2106.06087", "submitter": "Aaron Mueller", "authors": "Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber,\n  Tal Linzen, Yonatan Belinkov", "title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language\n  Models", "comments": "Accepted to ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted syntactic evaluations have demonstrated the ability of language\nmodels to perform subject-verb agreement given difficult contexts. To elucidate\nthe mechanisms by which the models accomplish this behavior, this study applies\ncausal mediation analysis to pre-trained neural language models. We investigate\nthe magnitude of models' preferences for grammatical inflections, as well as\nwhether neurons process subject-verb agreement similarly across sentences with\ndifferent syntactic structures. We uncover similarities and differences across\narchitectures and model sizes -- notably, that larger models do not necessarily\nlearn stronger preferences. We also observe two distinct mechanisms for\nproducing subject-verb agreement depending on the syntactic structure of the\ninput sentence. Finally, we find that language models rely on similar sets of\nneurons when given sentences with similar syntactic structure.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 23:50:51 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 21:55:42 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 05:40:01 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Finlayson", "Matthew", ""], ["Mueller", "Aaron", ""], ["Gehrmann", "Sebastian", ""], ["Shieber", "Stuart", ""], ["Linzen", "Tal", ""], ["Belinkov", "Yonatan", ""]]}, {"id": "2106.06090", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Yu Chen, Kai Shen, Xiaojie Guo, Hanning Gao, Shucheng Li,\n  Jian Pei, Bo Long", "title": "Graph Neural Networks for Natural Language Processing: A Survey", "comments": "127 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has become the dominant approach in coping with various tasks\nin Natural LanguageProcessing (NLP). Although text inputs are typically\nrepresented as a sequence of tokens, there isa rich variety of NLP problems\nthat can be best expressed with a graph structure. As a result, thereis a surge\nof interests in developing new deep learning techniques on graphs for a large\nnumberof NLP tasks. In this survey, we present a comprehensive overview onGraph\nNeural Networks(GNNs) for Natural Language Processing. We propose a new\ntaxonomy of GNNs for NLP, whichsystematically organizes existing research of\nGNNs for NLP along three axes: graph construction,graph representation\nlearning, and graph based encoder-decoder models. We further introducea large\nnumber of NLP applications that are exploiting the power of GNNs and summarize\nthecorresponding benchmark datasets, evaluation metrics, and open-source codes.\nFinally, we discussvarious outstanding challenges for making the full use of\nGNNs for NLP as well as future researchdirections. To the best of our\nknowledge, this is the first comprehensive overview of Graph NeuralNetworks for\nNatural Language Processing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 23:59:26 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wu", "Lingfei", ""], ["Chen", "Yu", ""], ["Shen", "Kai", ""], ["Guo", "Xiaojie", ""], ["Gao", "Hanning", ""], ["Li", "Shucheng", ""], ["Pei", "Jian", ""], ["Long", "Bo", ""]]}, {"id": "2106.06125", "submitter": "Xin Liu", "authors": "Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Min\n  Zhang, Haiying Zhang, Jinsong Su", "title": "Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language\n  Generation", "comments": "Accepted by ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known limitation in pretrain-finetune paradigm lies in its\ninflexibility caused by the one-size-fits-all vocabulary. This potentially\nweakens the effect when applying pretrained models into natural language\ngeneration (NLG) tasks, especially for the subword distributions between\nupstream and downstream tasks with significant discrepancy. Towards approaching\nthis problem, we extend the vanilla pretrain-finetune pipeline with an extra\nembedding transfer step. Specifically, a plug-and-play embedding generator is\nintroduced to produce the representation of any input token, according to\npre-trained embeddings of its morphologically similar ones. Thus, embeddings of\nmismatch tokens in downstream tasks can also be efficiently initialized. We\nconduct experiments on a variety of NLG tasks under the pretrain-finetune\nfashion. Experimental results and extensive analyses show that the proposed\nstrategy offers us opportunities to feel free to transfer the vocabulary,\nleading to more efficient and better performed downstream NLG models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:16:13 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liu", "Xin", ""], ["Yang", "Baosong", ""], ["Liu", "Dayiheng", ""], ["Zhang", "Haibo", ""], ["Luo", "Weihua", ""], ["Zhang", "Min", ""], ["Zhang", "Haiying", ""], ["Su", "Jinsong", ""]]}, {"id": "2106.06132", "submitter": "Yash Kumar Lal", "authors": "Yash Kumar Lal, Nathanael Chambers, Raymond Mooney and Niranjan\n  Balasubramanian", "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives", "comments": "Accepted to Findings of ACL, 2021 Data available at\n  http://lunr.cs.stonybrook.edu/tellmewhy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:40:06 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lal", "Yash Kumar", ""], ["Chambers", "Nathanael", ""], ["Mooney", "Raymond", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2106.06139", "submitter": "Kristen Moore", "authors": "Kristen Moore, Shenjun Zhong, Zhen He, Torsten Rudolf, Nils Fisher,\n  Brandon Victor, Neha Jindal", "title": "A comprehensive solution to retrieval-based chatbot construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present the results of our experiments in training and\ndeploying a self-supervised retrieval-based chatbot trained with contrastive\nlearning for assisting customer support agents. In contrast to most existing\nresearch papers in this area where the focus is on solving just one component\nof a deployable chatbot, we present an end-to-end set of solutions to take the\nreader from an unlabelled chatlogs to a deployed chatbot. This set of solutions\nincludes creating a self-supervised dataset and a weakly labelled dataset from\nchatlogs, as well as a systematic approach to selecting a fixed list of canned\nresponses. We present a hierarchical-based RNN architecture for the response\nselection model, chosen for its ability to cache intermediate utterance\nembeddings, which helped to meet deployment inference speed requirements. We\ncompare the performance of this architecture across 3 different learning\nobjectives: self-supervised contrastive learning, binary classification, and\nmulti-class classification. We find that using a self-supervised contrastive\nlearning model outperforms training the binary and multi-class classification\nmodels on a weakly labelled dataset. Our results validate that the\nself-supervised contrastive learning approach can be effectively used for a\nreal-world chatbot scenario.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:54:33 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Moore", "Kristen", ""], ["Zhong", "Shenjun", ""], ["He", "Zhen", ""], ["Rudolf", "Torsten", ""], ["Fisher", "Nils", ""], ["Victor", "Brandon", ""], ["Jindal", "Neha", ""]]}, {"id": "2106.06147", "submitter": "Jerome Abdelnour", "authors": "Jerome Abdelnour, Jean Rouat, Giampiero Salvi", "title": "NAAQA: A Neural Architecture for Acoustic Question Answering", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (PAMI) in April 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the Acoustic Question Answering (AQA) task is to answer a\nfree-form text question about the content of an acoustic scene. It was inspired\nby the Visual Question Answering (VQA) task. In this paper, based on the\npreviously introduced CLEAR dataset, we propose a new benchmark for AQA that\nemphasizes the specific challenges of acoustic inputs, e.g. variable duration\nscenes. We also introduce NAAQA, a neural architecture that leverages specific\nproperties of acoustic inputs. The usage of time and frequency 1D convolutions\nto process 2D spectro-temporal representations of acoustic content shows\npromising results and enables reductions in model complexity. NAAQA achieves\n91.6% of accuracy on the AQA task with about 7 times fewer parameters than the\npreviously explored VQA model. We provide a detailed analysis of the results\nfor the different question types. The effectiveness of coordinate maps in this\nacoustic context was also studied and we show that time coordinate maps augment\ntemporal localization capabilities which enhance performance of the network by\nabout 17 percentage points.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 03:05:48 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Abdelnour", "Jerome", ""], ["Rouat", "Jean", ""], ["Salvi", "Giampiero", ""]]}, {"id": "2106.06157", "submitter": "Yejin Bang", "authors": "Yejin Bang, Nayeon Lee, Etsuko Ishii, Andrea Madotto, Pascale Fung", "title": "Assessing Political Prudence of Open-domain Chatbots", "comments": "SIGDIAL 2021 - Safety for E2E Conversational AI (Camera-ready\n  Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Politically sensitive topics are still a challenge for open-domain chatbots.\nHowever, dealing with politically sensitive content in a responsible,\nnon-partisan, and safe behavior way is integral for these chatbots. Currently,\nthe main approach to handling political sensitivity is by simply changing such\na topic when it is detected. This is safe but evasive and results in a chatbot\nthat is less engaging. In this work, as a first step towards a politically safe\nchatbot, we propose a group of metrics for assessing their political prudence.\nWe then conduct political prudence analysis of various chatbots and discuss\ntheir behavior from multiple angles through our automatic metric and human\nevaluation metrics. The testsets and codebase are released to promote research\nin this area.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:03:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bang", "Yejin", ""], ["Lee", "Nayeon", ""], ["Ishii", "Etsuko", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.06160", "submitter": "Eric Le Ferrand", "authors": "\\'Eric Le Ferrand, Steven Bird, Laurent Besacier", "title": "Spoken Term Detection Methods for Sparse Transcription in Very\n  Low-resource Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the efficiency of two very different spoken term detection\napproaches for transcription when the available data is insufficient to train a\nrobust ASR system. This work is grounded in very low-resource language\ndocumentation scenario where only few minutes of recording have been\ntranscribed for a given language so far.Experiments on two oral languages show\nthat a pretrained universal phone recognizer, fine-tuned with only a few\nminutes of target language speech, can be used for spoken term detection with a\nbetter overall performance than a dynamic time warping approach. In addition,\nwe show that representing phoneme recognition ambiguity in a graph structure\ncan further boost the recall while maintaining high precision in the low\nresource spoken term detection task.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:09:54 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ferrand", "\u00c9ric Le", ""], ["Bird", "Steven", ""], ["Besacier", "Laurent", ""]]}, {"id": "2106.06169", "submitter": "Haoyu Song", "authors": "Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, Ting Liu", "title": "BoB: BERT Over BERT for Training Persona-based Dialogue Models from\n  Limited Personalized Data", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining consistent personas is essential for dialogue agents. Although\ntremendous advancements have been brought, the limited-scale of annotated\npersona-dense data are still barriers towards training robust and consistent\npersona-based dialogue models. In this work, we show how the challenges can be\naddressed by disentangling persona-based dialogue generation into two sub-tasks\nwith a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a\nBERT-based encoder and two BERT-based decoders, where one decoder is for\nresponse generation, and another is for consistency understanding. In\nparticular, to learn the ability of consistency understanding from large-scale\nnon-dialogue inference data, we train the second decoder in an unlikelihood\nmanner. Under different limited data settings, both automatic and human\nevaluations demonstrate that the proposed model outperforms strong baselines in\nresponse quality and persona consistency.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 05:02:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 01:52:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Song", "Haoyu", ""], ["Wang", "Yan", ""], ["Zhang", "Kaiyan", ""], ["Zhang", "Wei-Nan", ""], ["Liu", "Ting", ""]]}, {"id": "2106.06183", "submitter": "Swayambhu Nath Ray", "authors": "Swayambhu Nath Ray, Soumyajit Mitra, Raghavendra Bilgi, Sri Garimella", "title": "Improving RNN-T ASR Performance with Date-Time and Location Awareness", "comments": "To appear in TSD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we explore the benefits of incorporating context into a\nRecurrent Neural Network (RNN-T) based Automatic Speech Recognition (ASR) model\nto improve the speech recognition for virtual assistants. Specifically, we use\nmeta information extracted from the time at which the utterance is spoken and\nthe approximate location information to make ASR context aware. We show that\nthese contextual information, when used individually, improves overall\nperformance by as much as 3.48% relative to the baseline and when the contexts\nare combined, the model learns complementary features and the recognition\nimproves by 4.62%. On specific domains, these contextual signals show\nimprovements as high as 11.5%, without any significant degradation on others.\nWe ran experiments with models trained on data of sizes 30K hours and 10K\nhours. We show that the scale of improvement with the 10K hours dataset is much\nhigher than the one obtained with 30K hours dataset. Our results indicate that\nwith limited data to train the ASR model, contextual signals can improve the\nperformance significantly.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 05:57:30 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 05:47:10 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ray", "Swayambhu Nath", ""], ["Mitra", "Soumyajit", ""], ["Bilgi", "Raghavendra", ""], ["Garimella", "Sri", ""]]}, {"id": "2106.06200", "submitter": "Huan Lin", "authors": "Huan Lin, Liang Yao, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua\n  Luo, Degen Huang, Jinsong Su", "title": "Towards User-Driven Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A good translation should not only translate the original content\nsemantically, but also incarnate personal traits of the original text. For a\nreal-world neural machine translation (NMT) system, these user traits (e.g.,\ntopic preference, stylistic characteristics and expression habits) can be\npreserved in user behavior (e.g., historical inputs). However, current NMT\nsystems marginally consider the user behavior due to: 1) the difficulty of\nmodeling user portraits in zero-shot scenarios, and 2) the lack of\nuser-behavior annotated parallel dataset. To fill this gap, we introduce a\nnovel framework called user-driven NMT. Specifically, a cache-based module and\na user-driven contrastive learning method are proposed to offer NMT the ability\nto capture potential user traits from their historical inputs under a zero-shot\nlearning fashion. Furthermore, we contribute the first Chinese-English parallel\ncorpus annotated with user behavior called UDT-Corpus. Experimental results\nconfirm that the proposed user-driven NMT can generate user-specific\ntranslations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 07:06:20 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lin", "Huan", ""], ["Yao", "Liang", ""], ["Yang", "Baosong", ""], ["Liu", "Dayiheng", ""], ["Zhang", "Haibo", ""], ["Luo", "Weihua", ""], ["Huang", "Degen", ""], ["Su", "Jinsong", ""]]}, {"id": "2106.06213", "submitter": "Henry Weld", "authors": "Henry Weld, Guanghao Huang, Jean Lee, Tongshu Zhang, Kunze Wang,\n  Xinghong Guo, Siqu Long, Josiah Poon, Soyeon Caren Han", "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity\n  understanding and detection", "comments": "Accepted by ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional toxicity detection models have focused on the single utterance\nlevel without deeper understanding of context. We introduce CONDA, a new\ndataset for in-game toxic language detection enabling joint intent\nclassification and slot filling analysis, which is the core task of Natural\nLanguage Understanding (NLU). The dataset consists of 45K utterances from 12K\nconversations from the chat logs of 1.9K completed Dota 2 matches. We propose a\nrobust dual semantic-level toxicity framework, which handles utterance and\ntoken-level patterns, and rich contextual chatting history. Accompanying the\ndataset is a thorough in-game toxicity analysis, which provides comprehensive\nunderstanding of context at utterance, token, and dual levels. Inspired by NLU,\nwe also apply its metrics to the toxicity detection tasks for assessing\ntoxicity and game-specific aspects. We evaluate strong NLU models on CONDA,\nproviding fine-grained results for different intent classes and slot classes.\nFurthermore, we examine the coverage of toxicity nature in our dataset by\ncomparing it with other toxicity datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 07:42:12 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 03:29:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Weld", "Henry", ""], ["Huang", "Guanghao", ""], ["Lee", "Jean", ""], ["Zhang", "Tongshu", ""], ["Wang", "Kunze", ""], ["Guo", "Xinghong", ""], ["Long", "Siqu", ""], ["Poon", "Josiah", ""], ["Han", "Soyeon Caren", ""]]}, {"id": "2106.06216", "submitter": "Luca Mazzola", "authors": "Andreas Waldis and Luca Mazzola", "title": "Nested and Balanced Entity Recognition using Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Recognition (ER) within a text is a fundamental exercise in Natural\nLanguage Processing, enabling further depending tasks such as Knowledge\nExtraction, Text Summarisation, or Keyphrase Extraction. An entity consists of\nsingle words or of a consecutive sequence of terms, constituting the basic\nbuilding blocks for communication. Mainstream ER approaches are mainly limited\nto flat structures, concentrating on the outermost entities while ignoring the\ninner ones. This paper introduces a partly-layered network architecture that\ndeals with the complexity of overlapping and nested cases. The proposed\narchitecture consists of two parts: (1) a shared Sequence Layer and (2) a\nstacked component with multiple Tagging Layers. The adoption of such an\narchitecture has the advantage of preventing overfit to a specific word-length,\nthus maintaining performance for longer entities despite their lower frequency.\nTo verify the proposed architecture's effectiveness, we train and evaluate this\narchitecture to recognise two kinds of entities - Concepts (CR) and Named\nEntities (NER). Our approach achieves state-of-the-art NER performances, while\nit outperforms previous CR approaches. Considering these promising results, we\nsee the possibility to evolve the architecture for other cases such as the\nextraction of events or the detection of argumentative components.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 07:52:32 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Waldis", "Andreas", ""], ["Mazzola", "Luca", ""]]}, {"id": "2106.06228", "submitter": "Shan Wu", "authors": "Shan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun, Weipeng Zhang,\n  Jiansong Chen, Fan Yang, Xunliang Cai", "title": "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via\n  Synchronous Semantic Decoding", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Semantic parsing is challenging due to the structure gap and the semantic gap\nbetween utterances and logical forms. In this paper, we propose an unsupervised\nsemantic parsing method - Synchronous Semantic Decoding (SSD), which can\nsimultaneously resolve the semantic gap and the structure gap by jointly\nleveraging paraphrasing and grammar constrained decoding. Specifically, we\nreformulate semantic parsing as a constrained paraphrasing problem: given an\nutterance, our model synchronously generates its canonical utterance and\nmeaning representation. During synchronous decoding: the utterance paraphrasing\nis constrained by the structure of the logical form, therefore the canonical\nutterance can be paraphrased controlledly; the semantic decoding is guided by\nthe semantics of the canonical utterance, therefore its logical form can be\ngenerated unsupervisedly. Experimental results show that SSD is a promising\napproach and can achieve competitive unsupervised semantic parsing performance\non multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:16:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wu", "Shan", ""], ["Chen", "Bo", ""], ["Xin", "Chunlei", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Zhang", "Weipeng", ""], ["Chen", "Jiansong", ""], ["Yang", "Fan", ""], ["Cai", "Xunliang", ""]]}, {"id": "2106.06230", "submitter": "Ren\\'e Peinl", "authors": "Ren\\'e Peinl", "title": "Sprachsynthese -- State-of-the-Art in englischer und deutscher Sprache", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Reading text aloud is an important feature for modern computer applications.\nIt not only facilitates access to information for visually impaired people, but\nis also a pleasant convenience for non-impaired users. In this article, the\nstate of the art of speech synthesis is presented separately for\nmel-spectrogram generation and vocoders. It concludes with an overview of\navailable data sets for English and German with a discussion of the\ntransferability of the good speech synthesis results from English to German\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:25:08 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Peinl", "Ren\u00e9", ""]]}, {"id": "2106.06233", "submitter": "Jingbei Li", "authors": "Jingbei Li, Yi Meng, Chenyi Li, Zhiyong Wu, Helen Meng, Chao Weng and\n  Dan Su", "title": "Spoken Style Learning with Multi-modal Hierarchical Context Encoding for\n  Conversational Text-to-Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For conversational text-to-speech (TTS) systems, it is vital that the systems\ncan adjust the spoken styles of synthesized speech according to different\ncontent and spoken styles in historical conversations. However, the study about\nlearning spoken styles from historical conversations is still in its infancy.\nOnly the transcripts of the historical conversations are considered, which\nneglects the spoken styles in historical speeches. Moreover, only the\ninteractions of the global aspect between speakers are modeled, missing the\nparty aspect self interactions inside each speaker. In this paper, to achieve\nbetter spoken style learning for conversational TTS, we propose a spoken style\nlearning approach with multi-modal hierarchical context encoding. The textual\ninformation and spoken styles in the historical conversations are processed\nthrough multiple hierarchical recurrent neural networks to learn the spoken\nstyle related features in global and party aspects. The attention mechanism is\nfurther employed to summarize these features into a conversational context\nencoding. Experimental results demonstrate the effectiveness of our proposed\napproach, which outperform a baseline method using context encoding learnt only\nfrom the transcripts in global aspects, with MOS score on the naturalness of\nsynthesized speech increasing from 3.138 to 3.408 and ABX preference rate\nexceeding the baseline method by 36.45%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:33:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Li", "Jingbei", ""], ["Meng", "Yi", ""], ["Li", "Chenyi", ""], ["Wu", "Zhiyong", ""], ["Meng", "Helen", ""], ["Weng", "Chao", ""], ["Su", "Dan", ""]]}, {"id": "2106.06247", "submitter": "Jean Lee", "authors": "Jean Lee, Hoyoul Luis Youn, Nicholas Stevens, Josiah Poon, Soyeon\n  Caren Han", "title": "FedNLP: An interpretable NLP System to Decode Federal Reserve\n  Communications", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462785", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Federal Reserve System (the Fed) plays a significant role in affecting\nmonetary policy and financial conditions worldwide. Although it is important to\nanalyse the Fed's communications to extract useful information, it is generally\nlong-form and complex due to the ambiguous and esoteric nature of content. In\nthis paper, we present FedNLP, an interpretable multi-component Natural\nLanguage Processing system to decode Federal Reserve communications. This\nsystem is designed for end-users to explore how NLP techniques can assist their\nholistic understanding of the Fed's communications with NO coding. Behind the\nscenes, FedNLP uses multiple NLP models from traditional machine learning\nalgorithms to deep neural network architectures in each downstream task. The\ndemonstration shows multiple results at once including sentiment analysis,\nsummary of the document, prediction of the Federal Funds Rate movement and\nvisualization for interpreting the prediction model's result.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:58:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lee", "Jean", ""], ["Youn", "Hoyoul Luis", ""], ["Stevens", "Nicholas", ""], ["Poon", "Josiah", ""], ["Han", "Soyeon Caren", ""]]}, {"id": "2106.06292", "submitter": "Sebastin Santy", "authors": "Sebastin Santy and Prasanta Bhattacharya", "title": "A Discussion on Building Practical NLP Leaderboards: The Case of Machine\n  Translation", "comments": "pre-print: comments and suggestions welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in AI and ML applications have benefited from rapid progress\nin NLP research. Leaderboards have emerged as a popular mechanism to track and\naccelerate progress in NLP through competitive model development. While this\nhas increased interest and participation, the over-reliance on single, and\naccuracy-based metrics have shifted focus from other important metrics that\nmight be equally pertinent to consider in real-world contexts. In this paper,\nwe offer a preliminary discussion of the risks associated with focusing\nexclusively on accuracy metrics and draw on recent discussions to highlight\nprescriptive suggestions on how to develop more practical and effective\nleaderboards that can better reflect the real-world utility of models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:24:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Santy", "Sebastin", ""], ["Bhattacharya", "Prasanta", ""]]}, {"id": "2106.06297", "submitter": "Spurthi Amba Hombaiah", "authors": "Spurthi Amba Hombaiah and Tao Chen and Mingyang Zhang and Michael\n  Bendersky and Marc Najork", "title": "Dynamic Language Models for Continuously Evolving Content", "comments": null, "journal-ref": "KDD 2021", "doi": "10.1145/3447548.3467162", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The content on the web is in a constant state of flux. New entities, issues,\nand ideas continuously emerge, while the semantics of the existing conversation\ntopics gradually shift. In recent years, pre-trained language models like BERT\ngreatly improved the state-of-the-art for a large spectrum of content\nunderstanding tasks. Therefore, in this paper, we aim to study how these\nlanguage models can be adapted to better handle continuously evolving web\ncontent. In our study, we first analyze the evolution of 2013 - 2019 Twitter\ndata, and unequivocally confirm that a BERT model trained on past tweets would\nheavily deteriorate when directly applied to data from later years. Then, we\ninvestigate two possible sources of the deterioration: the semantic shift of\nexisting tokens and the sub-optimal or failed understanding of new tokens. To\nthis end, we both explore two different vocabulary composition methods, as well\nas propose three sampling methods which help in efficient incremental training\nfor BERT-like models. Compared to a new model trained from scratch offline, our\nincremental training (a) reduces the training costs, (b) achieves better\nperformance on evolving content, and (c) is suitable for online deployment. The\nsuperiority of our methods is validated using two downstream tasks. We\ndemonstrate significant improvements when incrementally evolving the model from\na particular base year, on the task of Country Hashtag Prediction, as well as\non the OffensEval 2019 task.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:33:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hombaiah", "Spurthi Amba", ""], ["Chen", "Tao", ""], ["Zhang", "Mingyang", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2106.06309", "submitter": "Ren\\'e Peinl", "authors": "Pascal Puchtler, Johannes Wirth and Ren\\'e Peinl", "title": "HUI-Audio-Corpus-German: A high quality TTS dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The increasing availability of audio data on the internet lead to a multitude\nof datasets for development and training of text to speech applications, based\non neural networks. Highly differing quality of voice, low sampling rates, lack\nof text normalization and disadvantageous alignment of audio samples to\ncorresponding transcript sentences still limit the performance of deep neural\nnetworks trained on this task. Additionally, data resources in languages like\nGerman are still very limited. We introduce the \"HUI-Audio-Corpus-German\", a\nlarge, open-source dataset for TTS engines, created with a processing pipeline,\nwhich produces high quality audio to transcription alignments and decreases\nmanual effort needed for creation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:59:09 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Puchtler", "Pascal", ""], ["Wirth", "Johannes", ""], ["Peinl", "Ren\u00e9", ""]]}, {"id": "2106.06361", "submitter": "Fanchao Qi", "authors": "Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun", "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word\n  Substitution", "comments": "Accepted by the main conference of ACL-IJCNLP as a long paper.\n  Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies show that neural natural language processing (NLP) models are\nvulnerable to backdoor attacks. Injected with backdoors, models perform\nnormally on benign examples but produce attacker-specified predictions when the\nbackdoor is activated, presenting serious security threats to real-world\napplications. Since existing textual backdoor attacks pay little attention to\nthe invisibility of backdoors, they can be easily detected and blocked. In this\nwork, we present invisible backdoors that are activated by a learnable\ncombination of word substitution. We show that NLP models can be injected with\nbackdoors that lead to a nearly 100% attack success rate, whereas being highly\ninvisible to existing defense strategies and even human inspections. The\nresults raise a serious alarm to the security of NLP models, which requires\nfurther research to be resolved. All the data and code of this paper are\nreleased at https://github.com/thunlp/BkdAtk-LWS.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:03:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qi", "Fanchao", ""], ["Yao", "Yuan", ""], ["Xu", "Sophia", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2106.06363", "submitter": "Thomas Scialom", "authors": "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin\n  Piwowarski, Jacopo Staiano", "title": "To Beam Or Not To Beam: That is a Question of Cooperation for Language\n  GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the discrete nature of words, language GANs require to be optimized\nfrom rewards provided by discriminator networks, via reinforcement learning\nmethods. This is a much harder setting than for continuous tasks, which enjoy\ngradient flows from discriminators to generators, usually leading to dramatic\nlearning instabilities. However, we claim that this can be solved by making\ndiscriminator and generator networks cooperate to produce output sequences\nduring training. These cooperative outputs, inherently built to obtain higher\ndiscrimination scores, not only provide denser rewards for training, but also\nform a more compact artificial set for discriminator training, hence improving\nits accuracy and stability. In this paper, we show that our SelfGAN framework,\nbuilt on this cooperative principle, outperforms Teacher Forcing and obtains\nstate-of-the-art results on two challenging tasks, Summarization and Question\nGeneration.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:04:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Scialom", "Thomas", ""], ["Dray", "Paul-Alexis", ""], ["Lamprier", "Sylvain", ""], ["Piwowarski", "Benjamin", ""], ["Staiano", "Jacopo", ""]]}, {"id": "2106.06381", "submitter": "Li Dong", "authors": "Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan\n  Huang, Furu Wei", "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word\n  Alignment", "comments": "ACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-lingual language models are typically pretrained with masked\nlanguage modeling on multilingual text or parallel sentences. In this paper, we\nintroduce denoising word alignment as a new cross-lingual pre-training task.\nSpecifically, the model first self-labels word alignments for parallel\nsentences. Then we randomly mask tokens in a bitext pair. Given a masked token,\nthe model uses a pointer network to predict the aligned token in the other\nlanguage. We alternately perform the above two steps in an\nexpectation-maximization manner. Experimental results show that our method\nimproves cross-lingual transferability on various datasets, especially on the\ntoken-level tasks, such as question answering, and structured prediction.\nMoreover, the model can serve as a pretrained word aligner, which achieves\nreasonably low error rates on the alignment benchmarks. The code and pretrained\nparameters are available at https://github.com/CZWin32768/XLM-Align.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:36:01 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chi", "Zewen", ""], ["Dong", "Li", ""], ["Zheng", "Bo", ""], ["Huang", "Shaohan", ""], ["Mao", "Xian-Ling", ""], ["Huang", "Heyan", ""], ["Wei", "Furu", ""]]}, {"id": "2106.06411", "submitter": "Mahdi Namazifar", "authors": "Devamanyu Hazarika, Mahdi Namazifar, Dilek Hakkani-T\\\"ur", "title": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling neural network-based models for natural language generation (NLG)\nhas broad applications in numerous areas such as machine translation, document\nsummarization, and dialog systems. Approaches that enable such control in a\nzero-shot manner would be of great importance as, among other reasons, they\nremove the need for additional annotated data and training. In this work, we\npropose novel approaches for controlling encoder-decoder transformer-based NLG\nmodels in zero-shot. This is done by introducing three control knobs, namely,\nattention biasing, decoder mixing, and context augmentation, that are applied\nto these models at generation time. These knobs control the generation process\nby directly manipulating trained NLG models (e.g., biasing cross-attention\nlayers) to realize the desired attributes in the generated outputs. We show\nthat not only are these NLG models robust to such manipulations, but also their\nbehavior could be controlled without an impact on their generation performance.\nThese results, to the best of our knowledge, are the first of their kind.\nThrough these control knobs, we also investigate the role of transformer\ndecoder's self-attention module and show strong evidence that its primary role\nis maintaining fluency of sentences generated by these models. Based on this\nhypothesis, we show that alternative architectures for transformer decoders\ncould be viable options. We also study how this hypothesis could lead to more\nefficient ways for training encoder-decoder transformer models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:07:19 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 16:25:11 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hazarika", "Devamanyu", ""], ["Namazifar", "Mahdi", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "2106.06462", "submitter": "Bradley Hauer", "authors": "Bradley Hauer, Grzegorz Kondrak, Yixing Luan, Arnob Mallik, Lili Mou", "title": "Semi-Supervised and Unsupervised Sense Annotation via Translations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Acquisition of multilingual training data continues to be a challenge in word\nsense disambiguation (WSD). To address this problem, unsupervised approaches\nhave been developed in recent years that automatically generate sense\nannotations suitable for training supervised WSD systems. We present three new\nmethods to creating sense-annotated corpora, which leverage translations,\nparallel corpora, lexical resources, and contextual and synset embeddings. Our\nsemi-supervised method applies machine translation to transfer existing sense\nannotations to other languages. Our two unsupervised methods use a\nknowledge-based WSD system to annotate a parallel corpus, and refine the\nresulting sense annotations by identifying lexical translations. We obtain\nstate-of-the-art results on standard WSD benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:32:46 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hauer", "Bradley", ""], ["Kondrak", "Grzegorz", ""], ["Luan", "Yixing", ""], ["Mallik", "Arnob", ""], ["Mou", "Lili", ""]]}, {"id": "2106.06471", "submitter": "Xingyi Yang", "authors": "Xingyi Yang, Muchao Ye, Quanzeng You, Fenglong Ma", "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report\n  Generation", "comments": "Accepted by ACL 2021, Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical report generation is one of the most challenging tasks in medical\nimage analysis. Although existing approaches have achieved promising results,\nthey either require a predefined template database in order to retrieve\nsentences or ignore the hierarchical nature of medical report generation. To\naddress these issues, we propose MedWriter that incorporates a novel\nhierarchical retrieval mechanism to automatically extract both report and\nsentence-level templates for clinically accurate report generation. MedWriter\nfirst employs the Visual-Language Retrieval~(VLR) module to retrieve the most\nrelevant reports for the given images. To guarantee the logical coherence\nbetween sentences, the Language-Language Retrieval~(LLR) module is introduced\nto retrieve relevant sentences based on the previous generated description. At\nlast, a language decoder fuses image features and features from retrieved\nreports and sentences to generate meaningful medical reports. We verified the\neffectiveness of our model by automatic evaluation and human evaluation on two\ndatasets, i.e., Open-I and MIMIC-CXR.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:47:23 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yang", "Xingyi", ""], ["Ye", "Muchao", ""], ["You", "Quanzeng", ""], ["Ma", "Fenglong", ""]]}, {"id": "2106.06504", "submitter": "Felix Gervits", "authors": "Felix Gervits, Antonio Roque, Gordon Briggs, Matthias Scheutz, Matthew\n  Marge", "title": "How Should Agents Ask Questions For Situated Learning? An Annotated\n  Dialogue Corpus", "comments": "Corpus available at https://github.com/USArmyResearchLab/ARL-HuRDL .\n  To appear in proceedings of SIGDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents that are confronted with novel concepts in situated\nenvironments will need to ask their human teammates questions to learn about\nthe physical world. To better understand this problem, we need data about\nasking questions in situated task-based interactions. To this end, we present\nthe Human-Robot Dialogue Learning (HuRDL) Corpus - a novel dialogue corpus\ncollected in an online interactive virtual environment in which human\nparticipants play the role of a robot performing a collaborative\ntool-organization task. We describe the corpus data and a corresponding\nannotation scheme to offer insight into the form and content of questions that\nhumans ask to facilitate learning in a situated environment. We provide the\ncorpus as an empirically-grounded resource for improving question generation in\nsituated intelligent agents.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:58:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Gervits", "Felix", ""], ["Roque", "Antonio", ""], ["Briggs", "Gordon", ""], ["Scheutz", "Matthias", ""], ["Marge", "Matthew", ""]]}, {"id": "2106.06519", "submitter": "Pakhi Bamdev", "authors": "Karthik Ganesan, Pakhi Bamdev, Jaivarsan B, Amresh Venugopal, Abhinav\n  Tushar", "title": "N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR\n  Hypotheses", "comments": "6 pages, 3 figures, Accepted at ACL 2021 as a main conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Spoken Language Understanding (SLU) systems parse speech into semantic\nstructures like dialog acts and slots. This involves the use of an Automatic\nSpeech Recognizer (ASR) to transcribe speech into multiple text alternatives\n(hypotheses). Transcription errors, common in ASRs, impact downstream SLU\nperformance negatively. Approaches to mitigate such errors involve using richer\ninformation from the ASR, either in form of N-best hypotheses or word-lattices.\nWe hypothesize that transformer models learn better with a simpler utterance\nrepresentation using the concatenation of the N-best ASR alternatives, where\neach alternative is separated by a special delimiter [SEP]. In our work, we\ntest our hypothesis by using concatenated N-best ASR alternatives as the input\nto transformer encoder models, namely BERT and XLM-RoBERTa, and achieve\nperformance equivalent to the prior state-of-the-art model on DSTC2 dataset. We\nalso show that our approach significantly outperforms the prior\nstate-of-the-art when subjected to the low data regime. Additionally, this\nmethodology is accessible to users of third-party ASR APIs which do not provide\nword-lattice information.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:29:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ganesan", "Karthik", ""], ["Bamdev", "Pakhi", ""], ["B", "Jaivarsan", ""], ["Venugopal", "Amresh", ""], ["Tushar", "Abhinav", ""]]}, {"id": "2106.06524", "submitter": "S\\'eri\\'e Emmanuel", "authors": "Emmanuel S\\'eri\\'e", "title": "WAX-ML: A Python library for machine learning and feedback loops on\n  streaming data", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wax is what you put on a surfboard to avoid slipping. It is an essential tool\nto go surfing... We introduce WAX-ML a research-oriented Python library\nproviding tools to design powerful machine learning algorithms and feedback\nloops working on streaming data. It strives to complement JAX with tools\ndedicated to time series. WAX-ML makes JAX-based programs easy to use for\nend-users working with pandas and xarray for data manipulation. It provides a\nsimple mechanism for implementing feedback loops, allows the implementation of\nonline learning and reinforcement learning algorithms with functions, and makes\nthem easy to integrate by end-users working with the object-oriented\nreinforcement learning framework from the Gym library. It is released with an\nApache open-source license on GitHub at https://github.com/eserie/wax-ml.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:42:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["S\u00e9ri\u00e9", "Emmanuel", ""]]}, {"id": "2106.06528", "submitter": "Yi-Lin Tuan", "authors": "Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang Wang", "title": "Local Explanation of Dialogue Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparison to the interpretation of classification models, the explanation\nof sequence generation models is also an important problem, however it has seen\nlittle attention. In this work, we study model-agnostic explanations of a\nrepresentative text generation task -- dialogue response generation. Dialog\nresponse generation is challenging with its open-ended sentences and multiple\nacceptable responses. To gain insights into the reasoning process of a\ngeneration model, we propose anew method, local explanation of response\ngeneration (LERG) that regards the explanations as the mutual interaction of\nsegments in input and output sentences. LERG views the sequence prediction as\nuncertainty estimation of a human response and then creates explanations by\nperturbing the input and calculating the certainty change over the human\nresponse. We show that LERG adheres to desired properties of explanations for\ntext generation including unbiased approximation, consistency and cause\nidentification. Empirically, our results show that our method consistently\nimproves other widely used methods on proposed automatic- and human- evaluation\nmetrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can\nextract both explicit and implicit relations between input and output segments.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:58:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Tuan", "Yi-Lin", ""], ["Pryor", "Connor", ""], ["Chen", "Wenhu", ""], ["Getoor", "Lise", ""], ["Wang", "William Yang", ""]]}, {"id": "2106.06566", "submitter": "Saujas Vaduguru", "authors": "Saujas Vaduguru, Aalok Sathe, Monojit Choudhury, Dipti Misra Sharma", "title": "Sample-efficient Linguistic Generalizations through Program Synthesis:\n  Experiments with Phonology Problems", "comments": "SIGMORPHON 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural models excel at extracting statistical patterns from large amounts of\ndata, but struggle to learn patterns or reason about language from only a few\nexamples. In this paper, we ask: Can we learn explicit rules that generalize\nwell from only a few examples? We explore this question using program\nsynthesis. We develop a synthesis model to learn phonology rules as programs in\na domain-specific language. We test the ability of our models to generalize\nfrom few training examples using our new dataset of problems from the\nLinguistics Olympiad, a challenging set of tasks that require strong linguistic\nreasoning ability. In addition to being highly sample-efficient, our approach\ngenerates human-readable programs, and allows control over the generalizability\nof the learnt programs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:36:07 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Vaduguru", "Saujas", ""], ["Sathe", "Aalok", ""], ["Choudhury", "Monojit", ""], ["Sharma", "Dipti Misra", ""]]}, {"id": "2106.06588", "submitter": "Sophia Henn", "authors": "Sophia Henn, Abigail Sticha, Timothy Burley, Ernesto Verdeja, Paul\n  Brenner", "title": "Visualization Techniques to Enhance Automated Event Extraction", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust visualization of complex data is critical for the effective use of NLP\nfor event classification, as the volume of data is large and the\nhigh-dimensional structure of text makes data challenging to summarize\nsuccinctly. In event extraction tasks in particular, visualization can aid in\nunderstanding and illustrating the textual relationships from which machine\nlearning tools produce insights. Through our case study which seeks to identify\npotential triggers of state-led mass killings from news articles using NLP, we\ndemonstrate how visualizations can aid in each stage, from exploratory analysis\nof raw data, to machine learning training analysis, and finally post-inference\nvalidation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:24:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Henn", "Sophia", ""], ["Sticha", "Abigail", ""], ["Burley", "Timothy", ""], ["Verdeja", "Ernesto", ""], ["Brenner", "Paul", ""]]}, {"id": "2106.06598", "submitter": "Suwon Shon", "authors": "Suwon Shon, Pablo Brusco, Jing Pan, Kyu J. Han, Shinji Watanabe", "title": "Leveraging Pre-trained Language Model for Speech Sentiment Analysis", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the use of pre-trained language models to learn\nsentiment information of written texts for speech sentiment analysis. First, we\ninvestigate how useful a pre-trained language model would be in a 2-step\npipeline approach employing Automatic Speech Recognition (ASR) and\ntranscripts-based sentiment analysis separately. Second, we propose a pseudo\nlabel-based semi-supervised training strategy using a language model on an\nend-to-end speech sentiment approach to take advantage of a large, but\nunlabeled speech dataset for training. Although spoken and written texts have\ndifferent linguistic characteristics, they can complement each other in\nunderstanding sentiment. Therefore, the proposed system can not only model\nacoustic characteristics to bear sentiment-specific information in speech\nsignals, but learn latent information to carry sentiments in the text\nrepresentation. In these experiments, we demonstrate the proposed approaches\nimprove F1 scores consistently compared to systems without a language model.\nMoreover, we also show that the proposed framework can reduce 65% of human\nsupervision by leveraging a large amount of data without human sentiment\nannotation and boost performance in a low-resource condition where the human\nsentiment annotation is not available enough.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:15:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shon", "Suwon", ""], ["Brusco", "Pablo", ""], ["Pan", "Jing", ""], ["Han", "Kyu J.", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2106.06600", "submitter": "Michihiro Yasunaga", "authors": "Michihiro Yasunaga, Percy Liang", "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair", "comments": "ICML 2021. Code & data available at\n  https://github.com/michiyasunaga/bifi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no syntax\nerrors). Existing works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:31:04 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 16:36:31 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yasunaga", "Michihiro", ""], ["Liang", "Percy", ""]]}, {"id": "2106.06605", "submitter": "Sravana Reddy", "authors": "Sravana Reddy, Marina Lazarova, Yongze Yu, and Rosie Jones", "title": "Modeling Language Usage and Listener Engagement in Podcasts", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there is an abundance of popular writing targeted to podcast creators\non how to speak in ways that engage their listeners, there has been little\ndata-driven analysis of podcasts that relates linguistic style with listener\nengagement. In this paper, we investigate how various factors -- vocabulary\ndiversity, distinctiveness, emotion, and syntax, among others -- correlate with\nengagement, based on analysis of the creators' written descriptions and\ntranscripts of the audio. We build models with different textual\nrepresentations, and show that the identified features are highly predictive of\nengagement. Our analysis tests popular wisdom about stylistic elements in\nhigh-engagement podcasts, corroborating some aspects, and adding new\nperspectives on others.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:40:15 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Reddy", "Sravana", ""], ["Lazarova", "Marina", ""], ["Yu", "Yongze", ""], ["Jones", "Rosie", ""]]}, {"id": "2106.06636", "submitter": "Junkun Chen", "authors": "Junkun Chen, Mingbo Ma, Renjie Zheng, Liang Huang", "title": "Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized\n  Streaming ASR", "comments": "accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous speech-to-text translation is widely useful in many scenarios.\nThe conventional cascaded approach uses a pipeline of streaming ASR followed by\nsimultaneous MT, but suffers from error propagation and extra latency. To\nalleviate these issues, recent efforts attempt to directly translate the source\nspeech into target text simultaneously, but this is much harder due to the\ncombination of two separate tasks. We instead propose a new paradigm with the\nadvantages of both cascaded and end-to-end approaches. The key idea is to use\ntwo separate, but synchronized, decoders on streaming ASR and direct\nspeech-to-text translation (ST), respectively, and the intermediate results of\nASR guide the decoding policy of (but is not fed as input to) ST. During\ntraining time, we use multitask learning to jointly learn these two tasks with\na shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset\ndemonstrate that our proposed technique achieves substantially better\ntranslation quality at similar levels of latency.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 23:22:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Junkun", ""], ["Ma", "Mingbo", ""], ["Zheng", "Renjie", ""], ["Huang", "Liang", ""]]}, {"id": "2106.06673", "submitter": "Zeeshan Sayyed", "authors": "Zeeshan Ali Sayyed", "title": "Study of sampling methods in sentiment analysis of imbalanced data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work investigates the application of sampling methods for sentiment\nanalysis on two different highly imbalanced datasets. One dataset contains\nonline user reviews from the cooking platform Epicurious and the other contains\ncomments given to the Planned Parenthood organization. In both these datasets,\nthe classes of interest are rare. Word n-grams were used as features from these\ndatasets. A feature selection technique based on information gain is first\napplied to reduce the number of features to a manageable space. A number of\ndifferent sampling methods were then applied to mitigate the class imbalance\nproblem which are then analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 03:16:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sayyed", "Zeeshan Ali", ""]]}, {"id": "2106.06683", "submitter": "Jialu Wang", "authors": "Jialu Wang, Yang Liu, Xin Eric Wang", "title": "Assessing Multilingual Fairness in Pre-trained Multimodal\n  Representations", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently pre-trained multimodal models, such as CLIP, have received a surge\nof attention for their exceptional capabilities towards connecting images and\nnatural language. The textual representations in English can be desirably\ntransferred to multilingualism and support promising downstream multimodal\ntasks for different languages. Nevertheless, previous fairness discourse in\nvision-and-language learning mainly focuses on monolingual representational\nbiases, and rarely scrutinizes the principles of multilingual fairness in this\nmultimodal setting, where one language is equated to a group of individuals and\nimages provide the universal grounding for bridging different languages.\n  In this paper, we provide a nuanced understanding of individual fairness and\ngroup fairness by viewing language as the recipient of fairness notions. We\ndefine new fairness notions within multilingual context and analytically\narticulate that, pre-trained vision-and-language representations are\nindividually fair across languages but not guaranteed to group fairness.\nFurthermore, we conduct extensive experiments to explore the prevalent group\ndisparity across languages and protected groups including race, gender and age.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 03:57:05 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Jialu", ""], ["Liu", "Yang", ""], ["Wang", "Xin Eric", ""]]}, {"id": "2106.06689", "submitter": "Mamoru Komachi", "authors": "Zhousi Chen, Longtu Zhang, Aizhan Imankulova, and Mamoru Komachi", "title": "Neural Combinatory Constituency Parsing", "comments": "Findings of ACL 2021; 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose two fast neural combinatory models for constituency parsing:\nbinary and multi-branching. Our models decompose the bottom-up parsing process\ninto 1) classification of tags, labels, and binary orientations or chunks and\n2) vector composition based on the computed orientations or chunks. These\nmodels have theoretical sub-quadratic complexity and empirical linear\ncomplexity. The binary model achieves an F1 score of 92.54 on Penn Treebank,\nspeeding at 1327.2 sents/sec. Both the models with XLNet provide near\nstate-of-the-art accuracies for English. Syntactic branching tendency and\nheadedness of a language are observed during the training and inference\nprocesses for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese).\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 05:14:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Zhousi", ""], ["Zhang", "Longtu", ""], ["Imankulova", "Aizhan", ""], ["Komachi", "Mamoru", ""]]}, {"id": "2106.06697", "submitter": "Francesco Ventura", "authors": "Francesco Ventura, Salvatore Greco, Daniele Apiletti, Tania\n  Cerquitelli", "title": "Explaining the Deep Natural Language Processing by Mining Textual\n  Interpretable Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the high accuracy offered by state-of-the-art deep natural-language\nmodels (e.g. LSTM, BERT), their application in real-life settings is still\nwidely limited, as they behave like a black-box to the end-user. Hence,\nexplainability is rapidly becoming a fundamental requirement of\nfuture-generation data-driven systems based on deep-learning approaches.\nSeveral attempts to fulfill the existing gap between accuracy and\ninterpretability have been done. However, robust and specialized xAI\n(Explainable Artificial Intelligence) solutions tailored to deep\nnatural-language models are still missing. We propose a new framework, named\nT-EBAnO, which provides innovative prediction-local and class-based\nmodel-global explanation strategies tailored to black-box deep natural-language\nmodels. Given a deep NLP model and the textual input data, T-EBAnO provides an\nobjective, human-readable, domain-specific assessment of the reasons behind the\nautomatic decision-making process. Specifically, the framework extracts sets of\ninterpretable features mining the inner knowledge of the model. Then, it\nquantifies the influence of each feature during the prediction process by\nexploiting the novel normalized Perturbation Influence Relation index at the\nlocal level and the novel Global Absolute Influence and Global Relative\nInfluence indexes at the global level. The effectiveness and the quality of the\nlocal and global explanations obtained with T-EBAnO are proved on (i) a\nsentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic\ncomment classification task performed by an LSTM model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 06:25:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ventura", "Francesco", ""], ["Greco", "Salvatore", ""], ["Apiletti", "Daniele", ""], ["Cerquitelli", "Tania", ""]]}, {"id": "2106.06719", "submitter": "Linzi Xing", "authors": "Linzi Xing, Giuseppe Carenini", "title": "Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair\n  Coherence Scoring", "comments": "Long paper accepted at SIGDIAL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue topic segmentation is critical in several dialogue modeling\nproblems. However, popular unsupervised approaches only exploit surface\nfeatures in assessing topical coherence among utterances. In this work, we\naddress this limitation by leveraging supervisory signals from the\nutterance-pair coherence scoring task. First, we present a simple yet effective\nstrategy to generate a training corpus for utterance-pair coherence scoring.\nThen, we train a BERT-based neural utterance-pair coherence model with the\nobtained training corpus. Finally, such model is used to measure the topical\nrelevance between utterances, acting as the basis of the segmentation\ninference. Experiments on three public datasets in English and Chinese\ndemonstrate that our proposal outperforms the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:49:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xing", "Linzi", ""], ["Carenini", "Giuseppe", ""]]}, {"id": "2106.06731", "submitter": "Ning Shi", "authors": "Ning Shi, Wei Wang, Boxin Wang, Jinfeng Li, Xiangyu Liu and Zhouhan\n  Lin", "title": "Incorporating External POS Tagger for Punctuation Restoration", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Punctuation restoration is an important post-processing step in automatic\nspeech recognition. Among other kinds of external information, part-of-speech\n(POS) taggers provide informative tags, suggesting each input token's syntactic\nrole, which has been shown to be beneficial for the punctuation restoration\ntask. In this work, we incorporate an external POS tagger and fuse its\npredicted labels into the existing language model to provide syntactic\ninformation. Besides, we propose sequence boundary sampling (SBS) to learn\npunctuation positions more efficiently as a sequence tagging task. Experimental\nresults show that our methods can consistently obtain performance gains and\nachieve a new state-of-the-art on the common IWSLT benchmark. Further ablation\nstudies illustrate that both large pre-trained language models and the external\nPOS tagger take essential parts to improve the model's performance.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 09:58:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shi", "Ning", ""], ["Wang", "Wei", ""], ["Wang", "Boxin", ""], ["Li", "Jinfeng", ""], ["Liu", "Xiangyu", ""], ["Lin", "Zhouhan", ""]]}, {"id": "2106.06738", "submitter": "Jinghui Lu", "authors": "Jinghui Lu, Maeve Henchion, Ivan Bacher, Brian Mac Namee", "title": "A Sentence-level Hierarchical BERT Model for Document Classification\n  with Limited Labelled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training deep learning models with limited labelled data is an attractive\nscenario for many NLP tasks, including document classification. While with the\nrecent emergence of BERT, deep learning language models can achieve reasonably\ngood performance in document classification with few labelled instances, there\nis a lack of evidence in the utility of applying BERT-like models on long\ndocument classification. This work introduces a long-text-specific model -- the\nHierarchical BERT Model (HBM) -- that learns sentence-level features of the\ntext and works well in scenarios with limited labelled data. Various evaluation\nexperiments have demonstrated that HBM can achieve higher performance in\ndocument classification than the previous state-of-the-art methods with only 50\nto 200 labelled instances, especially when documents are long. Also, as an\nextra benefit of HBM, the salient sentences identified by learned HBM are\nuseful as explanations for labelling documents based on a user study.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:45:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lu", "Jinghui", ""], ["Henchion", "Maeve", ""], ["Bacher", "Ivan", ""], ["Mac Namee", "Brian", ""]]}, {"id": "2106.06739", "submitter": "L Siddharth", "authors": "L Siddharth, Lucienne T.M. Blessing, Kristin L. Wood, Jianxi Luo", "title": "Engineering Knowledge Graph from Patent Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a large, scalable engineering knowledge graph, comprising sets of\n(entity, relationship, entity) triples that are real-world engineering facts\nfound in the patent database. We apply a set of rules based on the syntactic\nand lexical properties of claims in a patent document to extract facts. We\naggregate these facts within each patent document and integrate the aggregated\nsets of facts across the patent database to obtain the engineering knowledge\ngraph. Such a knowledge graph is expected to support inference, reasoning, and\nrecalling in various engineering tasks. The knowledge graph has a greater size\nand coverage in comparison with the previously used knowledge graphs and\nsemantic networks in the engineering literature.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:54:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Siddharth", "L", ""], ["Blessing", "Lucienne T. M.", ""], ["Wood", "Kristin L.", ""], ["Luo", "Jianxi", ""]]}, {"id": "2106.06751", "submitter": "Yang Feng", "authors": "Yang Feng, Shuhao Gu, Dengji Guo, Zhengxin Yang, Chenze Shao", "title": "Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation", "comments": "Accepted by ACL-IJCNLP 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although teacher forcing has become the main training paradigm for neural\nmachine translation, it usually makes predictions only conditioned on past\ninformation, and hence lacks global planning for the future. To address this\nproblem, we introduce another decoder, called seer decoder, into the\nencoder-decoder framework during training, which involves future information in\ntarget predictions. Meanwhile, we force the conventional decoder to simulate\nthe behaviors of the seer decoder via knowledge distillation. In this way, at\ntest the conventional decoder can perform like the seer decoder without the\nattendance of it. Experiment results on the Chinese-English, English-German and\nEnglish-Romanian translation tasks show our method can outperform competitive\nbaselines significantly and achieves greater improvements on the bigger data\nsets. Besides, the experiments also prove knowledge distillation the best way\nto transfer knowledge from the seer decoder to the conventional decoder\ncompared to adversarial learning and L2 regularization.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 11:38:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Feng", "Yang", ""], ["Gu", "Shuhao", ""], ["Guo", "Dengji", ""], ["Yang", "Zhengxin", ""], ["Shao", "Chenze", ""]]}, {"id": "2106.06758", "submitter": "Roy Bar-Haim", "authors": "Roy Bar-Haim, Lilach Eden, Yoav Kantor, Roni Friedman, Noam Slonim", "title": "Every Bite Is an Experience: Key Point Analysis of Business Reviews", "comments": "ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on review summarization focused on measuring the sentiment\ntoward the main aspects of the reviewed product or business, or on creating a\ntextual summary. These approaches provide only a partial view of the data:\naspect-based sentiment summaries lack sufficient explanation or justification\nfor the aspect rating, while textual summaries do not quantify the significance\nof each element, and are not well-suited for representing conflicting views.\nRecently, Key Point Analysis (KPA) has been proposed as a summarization\nframework that provides both textual and quantitative summary of the main\npoints in the data. We adapt KPA to review data by introducing Collective Key\nPoint Mining for better key point extraction; integrating sentiment analysis\ninto KPA; identifying good key point candidates for review summaries; and\nleveraging the massive amount of available reviews and their metadata. We show\nempirically that these novel extensions of KPA substantially improve its\nperformance. We demonstrate that promising results can be achieved without any\ndomain-specific annotation, while human supervision can lead to further\nimprovement.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 12:22:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bar-Haim", "Roy", ""], ["Eden", "Lilach", ""], ["Kantor", "Yoav", ""], ["Friedman", "Roni", ""], ["Slonim", "Noam", ""]]}, {"id": "2106.06766", "submitter": "Surangika Ranathunga", "authors": "Dilan Sachintha, Lakmali Piyarathna, Charith Rajitha, Surangika\n  Ranathunga", "title": "Exploiting Parallel Corpora to Improve Multilingual Embedding based\n  Document and Sentence Alignment", "comments": "21 pages, 2 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual sentence representations pose a great advantage for low-resource\nlanguages that do not have enough data to build monolingual models on their\nown. These multilingual sentence representations have been separately exploited\nby few research for document and sentence alignment. However, most of the\nlow-resource languages are under-represented in these pre-trained models. Thus,\nin the context of low-resource languages, these models have to be fine-tuned\nfor the task at hand, using additional data sources. This paper presents a\nweighting mechanism that makes use of available small-scale parallel corpora to\nimprove the performance of multilingual sentence representations on document\nand sentence alignment. Experiments are conducted with respect to two\nlow-resource languages, Sinhala and Tamil. Results on a newly created dataset\nof Sinhala-English, Tamil-English, and Sinhala-Tamil show that this new\nweighting mechanism significantly improves both document and sentence\nalignment. This dataset, as well as the source-code, is publicly released.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 13:00:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sachintha", "Dilan", ""], ["Piyarathna", "Lakmali", ""], ["Rajitha", "Charith", ""], ["Ranathunga", "Surangika", ""]]}, {"id": "2106.06786", "submitter": "Alex Lamb", "authors": "Alex Lamb, Tarin Clanuwat, Siyu Han, Mikel Bober-Irizar, Asanobu\n  Kitamoto", "title": "Predicting the Ordering of Characters in Japanese Historical Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Japan is a unique country with a distinct cultural heritage, which is\nreflected in billions of historical documents that have been preserved.\nHowever, the change in Japanese writing system in 1900 made these documents\ninaccessible for the general public. A major research project has been to make\nthese historical documents accessible and understandable. An increasing amount\nof research has focused on the character recognition task and the location of\ncharacters on image, yet less research has focused on how to predict the\nsequential ordering of the characters. This is because sequence in classical\nJapanese is very different from modern Japanese. Ordering characters into a\nsequence is important for making the document text easily readable and\nsearchable. Additionally, it is a necessary step for any kind of natural\nlanguage processing on the data (e.g. machine translation, language modeling,\nand word embeddings). We explore a few approaches to the task of predicting the\nsequential ordering of the characters: one using simple hand-crafted rules,\nanother using hand-crafted rules with adaptive thresholds, and another using a\ndeep recurrent sequence model trained with teacher forcing. We provide a\nquantitative and qualitative comparison of these techniques as well as their\ndistinct trade-offs. Our best-performing system has an accuracy of 98.65\\% and\nhas a perfect accuracy on 49\\% of the books in our dataset, suggesting that the\ntechnique is able to predict the order of the characters well enough for many\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 14:39:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lamb", "Alex", ""], ["Clanuwat", "Tarin", ""], ["Han", "Siyu", ""], ["Bober-Irizar", "Mikel", ""], ["Kitamoto", "Asanobu", ""]]}, {"id": "2106.06797", "submitter": "Sachin Kumar", "authors": "Sachin Kumar, Antonios Anastasopoulos, Shuly Wintner, Yulia Tsvetkov", "title": "Machine Translation into Low-resource Language Varieties", "comments": "The Joint Conference of the 59th Annual Meeting of the Association\n  for Computational Linguistics and the 11th International Joint Conference on\n  Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine translation (MT) systems are typically trained to\ngenerate the \"standard\" target language; however, many languages have multiple\nvarieties (regional varieties, dialects, sociolects, non-native varieties) that\nare different from the standard language. Such varieties are often\nlow-resource, and hence do not benefit from contemporary NLP solutions, MT\nincluded. We propose a general framework to rapidly adapt MT systems to\ngenerate language varieties that are close to, but different from, the standard\ntarget language, using no parallel (source--variety) data. This also includes\nadaptation of MT systems to low-resource typologically-related target\nlanguages. We experiment with adapting an English--Russian MT system to\ngenerate Ukrainian and Belarusian, an English--Norwegian Bokm{\\aa}l system to\ngenerate Nynorsk, and an English--Arabic system to generate four Arabic\ndialects, obtaining significant improvements over competitive baselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 15:28:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kumar", "Sachin", ""], ["Anastasopoulos", "Antonios", ""], ["Wintner", "Shuly", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2106.06811", "submitter": "Mir Mehedi Ahsan Pritom", "authors": "Mir Mehedi A. Pritom, Rosana Montanez Rodriguez, Asad Ali Khan,\n  Sebastian A. Nugroho, Esra'a Alrashydah, Beatrice N. Ruiz, Anthony Rios", "title": "Case Study on Detecting COVID-19 Health-Related Misinformation in Social\n  Media", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  COVID-19 pandemic has generated what public health officials called an\ninfodemic of misinformation. As social distancing and stay-at-home orders came\ninto effect, many turned to social media for socializing. This increase in\nsocial media usage has made it a prime vehicle for the spreading of\nmisinformation. This paper presents a mechanism to detect COVID-19\nhealth-related misinformation in social media following an interdisciplinary\napproach. Leveraging social psychology as a foundation and existing\nmisinformation frameworks, we defined misinformation themes and associated\nkeywords incorporated into the misinformation detection mechanism using applied\nmachine learning techniques. Next, using the Twitter dataset, we explored the\nperformance of the proposed methodology using multiple state-of-the-art machine\nlearning classifiers. Our method shows promising results with at most 78%\naccuracy in classifying health-related misinformation versus true information\nusing uni-gram-based NLP feature generations from tweets and the Decision Tree\nclassifier. We also provide suggestions on alternatives for countering\nmisinformation and ethical consideration for the study.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 16:26:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Pritom", "Mir Mehedi A.", ""], ["Rodriguez", "Rosana Montanez", ""], ["Khan", "Asad Ali", ""], ["Nugroho", "Sebastian A.", ""], ["Alrashydah", "Esra'a", ""], ["Ruiz", "Beatrice N.", ""], ["Rios", "Anthony", ""]]}, {"id": "2106.06822", "submitter": "Yifan Wu", "authors": "Yifan Wu, Min Zeng, Ying Yu, Min Li", "title": "A Pseudo Label-wise Attention Network for Automatic ICD Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic International Classification of Diseases (ICD) coding is defined as\na kind of text multi-label classification problem, which is difficult because\nthe number of labels is very large and the distribution of labels is\nunbalanced. The label-wise attention mechanism is widely used in automatic ICD\ncoding because it can assign weights to every word in full Electronic Medical\nRecords (EMR) for different ICD codes. However, the label-wise attention\nmechanism is computational redundant and costly. In this paper, we propose a\npseudo label-wise attention mechanism to tackle the problem. Instead of\ncomputing different attention modes for different ICD codes, the pseudo\nlabel-wise attention mechanism automatically merges similar ICD codes and\ncomputes only one attention mode for the similar ICD codes, which greatly\ncompresses the number of attention modes and improves the predicted accuracy.\nIn addition, we apply a more convenient and effective way to obtain the ICD\nvectors, and thus our model can predict new ICD codes by calculating the\nsimilarities between EMR vectors and ICD vectors. Extensive experiments show\nthe superior performance of our model. On the public MIMIC-III dataset and\nprivate Xiangya dataset, our model achieves micro f1 of 0.583 and 0.806,\nrespectively, which outperforms other competing models. Furthermore, we verify\nthe ability of our model in predicting new ICD codes. The case study shows how\npseudo label-wise attention works, and demonstrates the effectiveness of pseudo\nlabel-wise attention mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 17:03:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:33:53 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 01:26:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wu", "Yifan", ""], ["Zeng", "Min", ""], ["Yu", "Ying", ""], ["Li", "Min", ""]]}, {"id": "2106.06823", "submitter": "Bhargavi Paranjape", "authors": "Bhargavi Paranjape, Julian Michael, Marjan Ghazvininejad, Luke\n  Zettlemoyer and Hannaneh Hajishirzi", "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many commonsense reasoning NLP tasks involve choosing between one or more\npossible answers to a question or prompt based on knowledge that is often\nimplicit. Large pretrained language models (PLMs) can achieve near-human\nperformance on such tasks, while providing little human-interpretable evidence\nof the underlying reasoning they use. In this work, we show how to use these\nsame models to generate such evidence: inspired by the contrastive nature of\nhuman explanations, we use PLMs to complete explanation prompts which contrast\nalternatives according to the key attribute(s) required to justify the correct\nanswer (for example, peanuts are usually salty while raisins are sweet).\nConditioning model decisions on these explanations improves performance on two\ncommonsense reasoning benchmarks, as compared to previous non-contrastive\nalternatives. These explanations are also judged by humans to be more relevant\nfor solving the task, and facilitate a novel method to evaluate explanation\nfaithfulfness.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 17:06:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Paranjape", "Bhargavi", ""], ["Michael", "Julian", ""], ["Ghazvininejad", "Marjan", ""], ["Zettlemoyer", "Luke", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2106.06830", "submitter": "Anthony Chen", "authors": "Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, Sameer\n  Singh", "title": "Evaluating Entity Disambiguation and the Role of Popularity in\n  Retrieval-Based NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retrieval is a core component for open-domain NLP tasks. In open-domain\ntasks, multiple entities can share a name, making disambiguation an inherent\nyet under-explored problem. We propose an evaluation benchmark for assessing\nthe entity disambiguation capabilities of these retrievers, which we call\nAmbiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection\nof entities that share a name along with queries about those entities. By\ncovering the set of entities for polysemous names, AmbER sets act as a\nchallenging test of entity disambiguation. We create AmbER sets for three\npopular open-domain tasks: fact checking, slot filling, and question answering,\nand evaluate a diverse set of retrievers. We find that the retrievers exhibit\npopularity bias, significantly under-performing on rarer entities that share a\nname, e.g., they are twice as likely to retrieve erroneous documents on queries\nfor the less popular entity under the same name. These experiments on AmbER\nsets show their utility as an evaluation tool and highlight the weaknesses of\npopular retrieval systems.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 18:27:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Anthony", ""], ["Gudipati", "Pallavi", ""], ["Longpre", "Shayne", ""], ["Ling", "Xiao", ""], ["Singh", "Sameer", ""]]}, {"id": "2106.06849", "submitter": "Antonio Laverghetta Jr.", "authors": "Antonio Laverghetta Jr., Animesh Nighojkar, Jamshidbek Mirzakhalov and\n  John Licato", "title": "Can Transformer Language Models Predict Psychometric Properties?", "comments": "Proceedings of the 10th Joint Conference on Lexical and Computational\n  Semantics (*SEM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer-based language models (LMs) continue to advance state-of-the-art\nperformance on NLP benchmark tasks, including tasks designed to mimic\nhuman-inspired \"commonsense\" competencies. To better understand the degree to\nwhich LMs can be said to have certain linguistic reasoning skills, researchers\nare beginning to adapt the tools and concepts of the field of psychometrics.\nBut to what extent can the benefits flow in the other direction? I.e., can LMs\nbe of use in predicting what the psychometric properties of test items will be\nwhen those items are given to human participants? We gather responses from\nnumerous human participants and LMs (transformer and non-transformer-based) on\na broad diagnostic test of linguistic competencies. We then use the responses\nto calculate standard psychometric properties of the items in the diagnostic\ntest, using the human responses and the LM responses separately. We then\ndetermine how well these two sets of predictions match. We find cases in which\ntransformer-based LMs predict psychometric properties consistently well in\ncertain categories but consistently poorly in others, thus providing new\ninsights into fundamental similarities and differences between human and LM\nreasoning.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 20:05:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Laverghetta", "Antonio", "Jr."], ["Nighojkar", "Animesh", ""], ["Mirzakhalov", "Jamshidbek", ""], ["Licato", "John", ""]]}, {"id": "2106.06875", "submitter": "Rajat Bhatnagar", "authors": "Rajat Bhatnagar, Ananya Ganesh, Katharina Kann", "title": "Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine\n  Translation Data", "comments": "5 pages, 1 figure, ACL-IJCNLP 2021 submission, Natural Language\n  Processing, Data Collection, Monolingual Speakers, Machine Translation, GIFs,\n  Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  High-performing machine translation (MT) systems can help overcome language\nbarriers while making it possible for everyone to communicate and use language\ntechnologies in the language of their choice. However, such systems require\nlarge amounts of parallel sentences for training, and translators can be\ndifficult to find and expensive. Here, we present a data collection strategy\nfor MT which, in contrast, is cheap and simple, as it does not require\nbilingual speakers. Based on the insight that humans pay specific attention to\nmovements, we use graphics interchange formats (GIFs) as a pivot to collect\nparallel sentences from monolingual annotators. We use our strategy to collect\ndata in Hindi, Tamil and English. As a baseline, we also collect data using\nimages as a pivot. We perform an intrinsic evaluation by manually evaluating a\nsubset of the sentence pairs and an extrinsic evaluation by finetuning mBART on\nthe collected data. We find that sentences collected via GIFs are indeed of\nhigher quality.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 22:29:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bhatnagar", "Rajat", ""], ["Ganesh", "Ananya", ""], ["Kann", "Katharina", ""]]}, {"id": "2106.06899", "submitter": "Ankit Gupta", "authors": "Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, Jonathan Berant", "title": "Memory-efficient Transformers via Top-$k$ Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the success of dot-product attention in Transformers, numerous\napproximations have been recently proposed to address its quadratic complexity\nwith respect to the input length. While these variants are memory and compute\nefficient, it is not possible to directly use them with popular pre-trained\nlanguage models trained using vanilla attention, without an expensive\ncorrective pre-training stage. In this work, we propose a simple yet highly\naccurate approximation for vanilla attention. We process the queries in chunks,\nand for each query, compute the top-$k$ scores with respect to the keys. Our\napproach offers several advantages: (a) its memory usage is linear in the input\nsize, similar to linear attention variants, such as Performer and RFA (b) it is\na drop-in replacement for vanilla attention that does not require any\ncorrective pre-training, and (c) it can also lead to significant memory savings\nin the feed-forward layers after casting them into the familiar query-key-value\nframework. We evaluate the quality of top-$k$ approximation for multi-head\nattention layers on the Long Range Arena Benchmark, and for feed-forward layers\nof T5 and UnifiedQA on multiple QA datasets. We show our approach leads to\naccuracy that is nearly-identical to vanilla attention in multiple setups\nincluding training from scratch, fine-tuning, and zero-shot inference.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 02:30:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gupta", "Ankit", ""], ["Dar", "Guy", ""], ["Goodman", "Shaya", ""], ["Ciprut", "David", ""], ["Berant", "Jonathan", ""]]}, {"id": "2106.06905", "submitter": "Pengda Qin", "authors": "Runshi Liu, Pengda Qin, Yuhong Li, Weigao Wen, Dong Li, Kefeng Deng,\n  Qiang Wu", "title": "InfoBehavior: Self-supervised Representation Learning for Ultra-long\n  Behavior Sequence via Hierarchical Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce companies have to face abnormal sellers who sell potentially-risky\nproducts. Typically, the risk can be identified by jointly considering product\ncontent (e.g., title and image) and seller behavior. This work focuses on\nbehavior feature extraction as behavior sequences can provide valuable clues\nfor the risk discovery by reflecting the sellers' operation habits. Traditional\nfeature extraction techniques heavily depend on domain experts and adapt poorly\nto new tasks. In this paper, we propose a self-supervised method InfoBehavior\nto automatically extract meaningful representations from ultra-long raw\nbehavior sequences instead of the costly feature selection procedure.\nInfoBehavior utilizes Bidirectional Transformer as feature encoder due to its\nexcellent capability in modeling long-term dependency. However, it is\nintractable for commodity GPUs because the time and memory required by\nTransformer grow quadratically with the increase of sequence length. Thus, we\npropose a hierarchical grouping strategy to aggregate ultra-long raw behavior\nsequences to length-processable high-level embedding sequences. Moreover, we\nintroduce two types of pretext tasks. Sequence-related pretext task defines a\ncontrastive-based training objective to correctly select the masked-out\ncoarse-grained/fine-grained behavior sequences against other \"distractor\"\nbehavior sequences; Domain-related pretext task designs a classification\ntraining objective to correctly predict the domain-specific statistical results\nof anomalous behavior. We show that behavior representations from the\npre-trained InfoBehavior can be directly used or integrated with features from\nother side information to support a wide range of downstream tasks.\nExperimental results demonstrate that InfoBehavior significantly improves the\nperformance of Product Risk Management and Intellectual Property Protection.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 03:45:45 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Runshi", ""], ["Qin", "Pengda", ""], ["Li", "Yuhong", ""], ["Wen", "Weigao", ""], ["Li", "Dong", ""], ["Deng", "Kefeng", ""], ["Wu", "Qiang", ""]]}, {"id": "2106.06909", "submitter": "Guoguo Chen", "authors": "Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang,\n  Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev\n  Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen\n  Yao, Yongqing Wang, Yujun Wang, Zhao You, Zhiyong Yan", "title": "GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of\n  Transcribed Audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces GigaSpeech, an evolving, multi-domain English speech\nrecognition corpus with 10,000 hours of high quality labeled audio suitable for\nsupervised training, and 40,000 hours of total audio suitable for\nsemi-supervised and unsupervised training. Around 40,000 hours of transcribed\naudio is first collected from audiobooks, podcasts and YouTube, covering both\nread and spontaneous speaking styles, and a variety of topics, such as arts,\nscience, sports, etc. A new forced alignment and segmentation pipeline is\nproposed to create sentence segments suitable for speech recognition training,\nand to filter out segments with low-quality transcription. For system training,\nGigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,\nand 10000h. For our 10,000-hour XL training subset, we cap the word error rate\nat 4% during the filtering/validation stage, and for all our other smaller\ntraining subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the\nother hand, are re-processed by professional human transcribers to ensure high\ntranscription quality. Baseline systems are provided for popular speech\nrecognition toolkits, namely Athena, ESPnet, Kaldi and Pika.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 04:09:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Guoguo", ""], ["Chai", "Shuzhou", ""], ["Wang", "Guanbo", ""], ["Du", "Jiayu", ""], ["Zhang", "Wei-Qiang", ""], ["Weng", "Chao", ""], ["Su", "Dan", ""], ["Povey", "Daniel", ""], ["Trmal", "Jan", ""], ["Zhang", "Junbo", ""], ["Jin", "Mingjie", ""], ["Khudanpur", "Sanjeev", ""], ["Watanabe", "Shinji", ""], ["Zhao", "Shuaijiang", ""], ["Zou", "Wei", ""], ["Li", "Xiangang", ""], ["Yao", "Xuchen", ""], ["Wang", "Yongqing", ""], ["Wang", "Yujun", ""], ["You", "Zhao", ""], ["Yan", "Zhiyong", ""]]}, {"id": "2106.06910", "submitter": "Sourav Das", "authors": "Arunava Kumar Chakraborty, Sourav Das and Anup Kumar Kolya", "title": "Sentiment Analysis of Covid-19 Tweets using Evolutionary\n  Classification-Based LSTM Model", "comments": "11 pages, 8 figures, 5 tables", "journal-ref": "In RAAI 2020. Advances in Intelligent Systems and Computing, vol\n  1355 (2021)", "doi": "10.1007/978-981-16-1543-6_7", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As the Covid-19 outbreaks rapidly all over the world day by day and also\naffects the lives of million, a number of countries declared complete lock-down\nto check its intensity. During this lockdown period, social media plat-forms\nhave played an important role to spread information about this pandemic across\nthe world, as people used to express their feelings through the social\nnetworks. Considering this catastrophic situation, we developed an experimental\napproach to analyze the reactions of people on Twitter taking into ac-count the\npopular words either directly or indirectly based on this pandemic. This paper\nrepresents the sentiment analysis on collected large number of tweets on\nCoronavirus or Covid-19. At first, we analyze the trend of public sentiment on\nthe topics related to Covid-19 epidemic using an evolutionary classification\nfollowed by the n-gram analysis. Then we calculated the sentiment ratings on\ncollected tweet based on their class. Finally, we trained the long-short term\nnetwork using two types of rated tweets to predict sentiment on Covid-19 data\nand obtained an overall accuracy of 84.46%.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 04:27:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chakraborty", "Arunava Kumar", ""], ["Das", "Sourav", ""], ["Kolya", "Anup Kumar", ""]]}, {"id": "2106.06922", "submitter": "Shih-Hsuan Chiu", "authors": "Shih-Hsuan Chiu, Tien-Hong Lo and Berlin Chen", "title": "Cross-sentence Neural Language Models for Conversational Speech\n  Recognition", "comments": "More extensions and experiments are under exploration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important research direction in automatic speech recognition (ASR) has\ncentered around the development of effective methods to rerank the output\nhypotheses of an ASR system with more sophisticated language models (LMs) for\nfurther gains. A current mainstream school of thoughts for ASR N-best\nhypothesis reranking is to employ a recurrent neural network (RNN)-based LM or\nits variants, with performance superiority over the conventional n-gram LMs\nacross a range of ASR tasks. In real scenarios such as a long conversation, a\nsequence of consecutive sentences may jointly contain ample cues of\nconversation-level information such as topical coherence, lexical entrainment\nand adjacency pairs, which however remains to be underexplored. In view of\nthis, we first formulate ASR N-best reranking as a prediction problem, putting\nforward an effective cross-sentence neural LM approach that reranks the ASR\nN-best hypotheses of an upcoming sentence by taking into consideration the word\nusage in its precedent sentences. Furthermore, we also explore to extract\ntask-specific global topical information of the cross-sentence history in an\nunsupervised manner for better ASR performance. Extensive experiments conducted\non the AMI conversational benchmark corpus indicate the effectiveness and\nfeasibility of our methods in comparison to several state-of-the-art reranking\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 05:30:16 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 04:44:55 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 07:55:00 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chiu", "Shih-Hsuan", ""], ["Lo", "Tien-Hong", ""], ["Chen", "Berlin", ""]]}, {"id": "2106.06937", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, Xiang Ren", "title": "Common Sense Beyond English: Evaluating and Improving Multilingual\n  Language Models for Commonsense Reasoning", "comments": "Accepted to ACL-IJCNLP 2021 (long paper at main conference). Project\n  website: https://inklab.usc.edu/XCSR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Commonsense reasoning research has so far been limited to English. We aim to\nevaluate and improve popular multilingual language models (ML-LMs) to help\nadvance commonsense reasoning (CSR) beyond English. We collect the Mickey\nCorpus, consisting of 561k sentences in 11 different languages, which can be\nused for analyzing and improving ML-LMs. We propose Mickey Probe, a\nlanguage-agnostic probing task for fairly evaluating the common sense of\npopular ML-LMs across different languages. In addition, we also create two new\ndatasets, X-CSQA and X-CODAH, by translating their English versions to 15 other\nlanguages, so that we can evaluate popular ML-LMs for cross-lingual commonsense\nreasoning. To improve the performance beyond English, we propose a simple yet\neffective method -- multilingual contrastive pre-training (MCP). It\nsignificantly enhances sentence representations, yielding a large performance\ngain on both benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 07:14:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Lee", "Seyeon", ""], ["Qiao", "Xiaoyang", ""], ["Ren", "Xiang", ""]]}, {"id": "2106.06944", "submitter": "Furao Shen", "authors": "Hua Yan, Feng Han, Junyi An, Weikang Xiao, Jian Zhao, Furao Shen", "title": "SASICM A Multi-Task Benchmark For Subtext Recognition", "comments": "34 pages, 6 figures, 6 tables. Submitted to the journal of artificial\n  intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subtext is a kind of deep semantics which can be acquired after one or more\nrounds of expression transformation. As a popular way of expressing one's\nintentions, it is well worth studying. In this paper, we try to make computers\nunderstand whether there is a subtext by means of machine learning. We build a\nChinese dataset whose source data comes from the popular social media (e.g.\nWeibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a\nbaseline model called SASICM to deal with subtext recognition. The F1 score of\nSASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%\nhigher than that of BERT based model, 12.7% higher than that of traditional\nmethods on average, including support vector machine, logistic regression\nclassifier, maximum entropy classifier, naive bayes classifier and decision\ntree and 2.39% higher than that of the state-of-the-art, including MARIN and\nBTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,\nwhich is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and\nSASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of\nother methods which are mentioned before.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 08:29:15 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 02:13:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yan", "Hua", ""], ["Han", "Feng", ""], ["An", "Junyi", ""], ["Xiao", "Weikang", ""], ["Zhao", "Jian", ""], ["Shen", "Furao", ""]]}, {"id": "2106.06963", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou", "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology\n  Report Generation", "comments": "Accepted by CVPR 2021 (2021 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR2021))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:10:02 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 17:52:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Fenglin", ""], ["Wu", "Xian", ""], ["Ge", "Shen", ""], ["Fan", "Wei", ""], ["Zou", "Yuexian", ""]]}, {"id": "2106.06964", "submitter": "Alexey Tikhonov", "authors": "Alexey Tikhonov", "title": "Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces", "comments": "3 pages, 2 figures, EEML-2021 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained word representations became a key component in many NLP tasks.\nHowever, the global geometry of the word embeddings remains poorly understood.\nIn this paper, we demonstrate that a typical word embeddings cloud is shaped as\na high-dimensional simplex with interpretable vertices and propose a simple yet\neffective method for enumeration of these vertices. We show that the proposed\nmethod can detect and describe vertices of the simplex for GloVe and fasttext\nspaces.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:19:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tikhonov", "Alexey", ""]]}, {"id": "2106.06965", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, Xu Sun", "title": "Contrastive Attention for Automatic Chest X-ray Report Generation", "comments": "Appear in Findings of ACL 2021 (The Joint Conference of the 59th\n  Annual Meeting of the Association for Computational Linguistics and the 11th\n  International Joint Conference on Natural Language Processing (ACL-IJCNLP\n  2021))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:20:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Fenglin", ""], ["Yin", "Changchang", ""], ["Wu", "Xian", ""], ["Ge", "Shen", ""], ["Zhang", "Ping", ""], ["Sun", "Xu", ""]]}, {"id": "2106.06981", "submitter": "Gail Weiss", "authors": "Gail Weiss, Yoav Goldberg, Eran Yahav", "title": "Thinking Like Transformers", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 13:04:46 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 11:22:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Weiss", "Gail", ""], ["Goldberg", "Yoav", ""], ["Yahav", "Eran", ""]]}, {"id": "2106.07055", "submitter": "Shikib Mehri", "authors": "Shikib Mehri, Maxine Eskenazi", "title": "GenSF: Simultaneous Adaptation of Generative Pre-trained Models and Slot\n  Filling", "comments": "Accepted at SIGDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In transfer learning, it is imperative to achieve strong alignment between a\npre-trained model and a downstream task. Prior work has done this by proposing\ntask-specific pre-training objectives, which sacrifices the inherent\nscalability of the transfer learning paradigm. We instead achieve strong\nalignment by simultaneously modifying both the pre-trained model and the\nformulation of the downstream task, which is more efficient and preserves the\nscalability of transfer learning. We present GenSF (Generative Slot Filling),\nwhich leverages a generative pre-trained open-domain dialog model for slot\nfilling. GenSF (1) adapts the pre-trained model by incorporating inductive\nbiases about the task and (2) adapts the downstream task by reformulating slot\nfilling to better leverage the pre-trained model's capabilities. GenSF achieves\nstate-of-the-art results on two slot filling datasets with strong gains in\nfew-shot and zero-shot settings. We achieve a 9 F1 score improvement in\nzero-shot slot filling. This highlights the value of strong alignment between\nthe pre-trained model and the downstream task.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:44:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mehri", "Shikib", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "2106.07056", "submitter": "Shikib Mehri", "authors": "Shikib Mehri, Maxine Eskenazi", "title": "Schema-Guided Paradigm for Zero-Shot Dialog", "comments": "Accepted at SIGDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing mechanisms that flexibly adapt dialog systems to unseen tasks and\ndomains is a major challenge in dialog research. Neural models implicitly\nmemorize task-specific dialog policies from the training data. We posit that\nthis implicit memorization has precluded zero-shot transfer learning. To this\nend, we leverage the schema-guided paradigm, wherein the task-specific dialog\npolicy is explicitly provided to the model. We introduce the Schema Attention\nModel (SAM) and improved schema representations for the STAR corpus. SAM\nobtains significant improvement in zero-shot settings, with a +22 F1 score\nimprovement over prior work. These results validate the feasibility of\nzero-shot generalizability in dialog. Ablation experiments are also presented\nto demonstrate the efficacy of SAM.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:44:45 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mehri", "Shikib", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "2106.07117", "submitter": "Heeyoung Kwon", "authors": "Heeyoung Kwon, Nathanael Chambers, and Niranjan Balasubramanian", "title": "Toward Diverse Precondition Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language understanding must identify the logical connections between events\nin a discourse, but core events are often unstated due to their commonsense\nnature. This paper fills in these missing events by generating precondition\nevents. Precondition generation can be framed as a sequence-to-sequence\nproblem: given a target event, generate a possible precondition. However, in\nmost real-world scenarios, an event can have several preconditions, requiring\ndiverse generation -- a challenge for standard seq2seq approaches. We propose\nDiP, a Diverse Precondition generation system that can generate unique and\ndiverse preconditions. DiP uses a generative process with three components --\nan event sampler, a candidate generator, and a post-processor. The event\nsampler provides control codes (precondition triggers) which the candidate\ngenerator uses to focus its generation. Unlike other conditional generation\nsystems, DiP automatically generates control codes without training on diverse\nexamples. Analysis against baselines reveals that DiP improves the diversity of\npreconditions significantly while also generating more preconditions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 00:33:29 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kwon", "Heeyoung", ""], ["Chambers", "Nathanael", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2106.07131", "submitter": "Alberto Olmo", "authors": "Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati", "title": "GPT3-to-plan: Extracting plans from text using GPT-3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operations in many essential industries including finance and banking are\noften characterized by the need to perform repetitive sequential tasks. Despite\ntheir criticality to the business, workflows are rarely fully automated or even\nformally specified, though there may exist a number of natural language\ndocuments describing these procedures for the employees of the company. Plan\nextraction methods provide us with the possibility of extracting structure\nplans from such natural language descriptions of the plans/workflows, which\ncould then be leveraged by an automated system. In this paper, we investigate\nthe utility of generalized language models in performing such extractions\ndirectly from such texts. Such models have already been shown to be quite\neffective in multiple translation tasks, and our initial results seem to point\nto their effectiveness also in the context of plan extractions. Particularly,\nwe show that GPT-3 is able to generate plan extraction results that are\ncomparable to many of the current state of the art plan extraction methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 01:45:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Olmo", "Alberto", ""], ["Sreedharan", "Sarath", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2106.07137", "submitter": "Ting-Rui Chiang", "authors": "Ting-Rui Chiang, Yun-Nung Chen", "title": "Why Can You Lay Off Heads? Investigating How BERT Heads Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The huge size of the widely used BERT family models has led to recent efforts\nabout model distillation. The main goal of distillation is to create a\ntask-agnostic pre-trained model that can be fine-tuned on downstream tasks\nwithout fine-tuning its full-sized version. Despite the progress of\ndistillation, to what degree and for what reason a task-agnostic model can be\ncreated from distillation has not been well studied. Also, the mechanisms\nbehind transfer learning of those BERT models are not well investigated either.\nTherefore, this work focuses on analyzing the acceptable deduction when\ndistillation for guiding the future distillation procedure. Specifically, we\nfirst inspect the prunability of the Transformer heads in RoBERTa and ALBERT\nusing their head importance estimation proposed by Michel et al. (2019), and\nthen check the coherence of the important heads between the pre-trained task\nand downstream tasks. Hence, the acceptable deduction of performance on the\npre-trained task when distilling a model can be derived from the results, and\nwe further compare the behavior of the pruned model before and after\nfine-tuning. Our studies provide guidance for future directions about BERT\nfamily model distillation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:27:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chiang", "Ting-Rui", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2106.07139", "submitter": "Han Xu", "authors": "Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo,\n  Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan,\n  Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong\n  Wen, Jinhui Yuan, Wayne Xin Zhao, Jun Zhu", "title": "Pre-Trained Models: Past, Present and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained models (PTMs) such as BERT and GPT have recently\nachieved great success and become a milestone in the field of artificial\nintelligence (AI). Owing to sophisticated pre-training objectives and huge\nmodel parameters, large-scale PTMs can effectively capture knowledge from\nmassive labeled and unlabeled data. By storing knowledge into huge parameters\nand fine-tuning on specific tasks, the rich knowledge implicitly encoded in\nhuge parameters can benefit a variety of downstream tasks, which has been\nextensively demonstrated via experimental verification and empirical analysis.\nIt is now the consensus of the AI community to adopt PTMs as backbone for\ndownstream tasks rather than learning models from scratch. In this paper, we\ntake a deep look into the history of pre-training, especially its special\nrelation with transfer learning and self-supervised learning, to reveal the\ncrucial position of PTMs in the AI development spectrum. Further, we\ncomprehensively review the latest breakthroughs of PTMs. These breakthroughs\nare driven by the surge of computational power and the increasing availability\nof data, towards four important directions: designing effective architectures,\nutilizing rich contexts, improving computational efficiency, and conducting\ninterpretation and theoretical analysis. Finally, we discuss a series of open\nproblems and research directions of PTMs, and hope our view can inspire and\nadvance the future study of PTMs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:40:32 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:08:31 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Han", "Xu", ""], ["Zhang", "Zhengyan", ""], ["Ding", "Ning", ""], ["Gu", "Yuxian", ""], ["Liu", "Xiao", ""], ["Huo", "Yuqi", ""], ["Qiu", "Jiezhong", ""], ["Zhang", "Liang", ""], ["Han", "Wentao", ""], ["Huang", "Minlie", ""], ["Jin", "Qin", ""], ["Lan", "Yanyan", ""], ["Liu", "Yang", ""], ["Liu", "Zhiyuan", ""], ["Lu", "Zhiwu", ""], ["Qiu", "Xipeng", ""], ["Song", "Ruihua", ""], ["Tang", "Jie", ""], ["Wen", "Ji-Rong", ""], ["Yuan", "Jinhui", ""], ["Zhao", "Wayne Xin", ""], ["Zhu", "Jun", ""]]}, {"id": "2106.07167", "submitter": "Andreas Stolcke", "authors": "Yi Chieh Liu and Eunjung Han and Chul Lee and Andreas Stolcke", "title": "End-to-end Neural Diarization: From Transformer to Conformer", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new end-to-end neural diarization (EEND) system that is based on\nConformer, a recently proposed neural architecture that combines convolutional\nmappings and Transformer to model both local and global dependencies in speech.\nWe first show that data augmentation and convolutional subsampling layers\nenhance the original self-attentive EEND in the Transformer-based EEND, and\nthen Conformer gives an additional gain over the Transformer-based EEND.\nHowever, we notice that the Conformer-based EEND does not generalize as well\nfrom simulated to real conversation data as the Transformer-based model. This\nleads us to quantify the mismatch between simulated data and real speaker\nbehavior in terms of temporal statistics reflecting turn-taking between\nspeakers, and investigate its correlation with diarization error. By mixing\nsimulated and real data in EEND training, we mitigate the mismatch further,\nwith Conformer-based EEND achieving 24% error reduction over the baseline\nSA-EEND system, and 10% improvement over the best augmented Transformer-based\nsystem, on two-speaker CALLHOME data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 05:21:08 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Yi Chieh", ""], ["Han", "Eunjung", ""], ["Lee", "Chul", ""], ["Stolcke", "Andreas", ""]]}, {"id": "2106.07174", "submitter": "Zhihong Shao", "authors": "Zhihong Shao, Lifeng Shang, Qun Liu, Minlie Huang", "title": "A Mutual Information Maximization Approach for the Spurious Solution\n  Problem in Weakly Supervised Question Answering", "comments": "ACL2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised question answering usually has only the final answers as\nsupervision signals while the correct solutions to derive the answers are not\nprovided. This setting gives rise to the spurious solution problem: there may\nexist many spurious solutions that coincidentally derive the correct answer,\nbut training on such solutions can hurt model performance (e.g., producing\nwrong solutions or answers). For example, for discrete reasoning tasks as on\nDROP, there may exist many equations to derive a numeric answer, and typically\nonly one of them is correct. Previous learning methods mostly filter out\nspurious solutions with heuristics or using model confidence, but do not\nexplicitly exploit the semantic correlations between a question and its\nsolution. In this paper, to alleviate the spurious solution problem, we propose\nto explicitly exploit such semantic correlations by maximizing the mutual\ninformation between question-answer pairs and predicted solutions. Extensive\nexperiments on four question answering datasets show that our method\nsignificantly outperforms previous learning methods in terms of task\nperformance and is more effective in training models to produce correct\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 05:47:41 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shao", "Zhihong", ""], ["Shang", "Lifeng", ""], ["Liu", "Qun", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.07176", "submitter": "Ru He", "authors": "Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang,\n  Ying Nian Wu", "title": "SAS: Self-Augmented Strategy for Language Model Pre-training", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The core of a self-supervised learning method for pre-training language\nmodels includes the design of appropriate data augmentation and corresponding\npre-training task(s). Most data augmentations in language model pre-training\nare context-independent. The seminal contextualized augmentation recently\nproposed by the ELECTRA requires a separate generator, which leads to extra\ncomputation cost as well as the challenge in adjusting the capability of its\ngenerator relative to that of the other model component(s). We propose a\nself-augmented strategy (SAS) that uses a single forward pass through the model\nto augment the input data for model training in the next epoch. Essentially our\nstrategy eliminates a separate generator network and uses only one network to\ngenerate the data augmentation and undertake two pre-training tasks (the MLM\ntask and the RTD task) jointly, which naturally avoids the challenge in\nadjusting the generator's capability as well as reduces the computation cost.\nAdditionally, our SAS is a general strategy such that it can seamlessly\nincorporate many new techniques emerging recently or in the future, such as the\ndisentangled attention mechanism recently proposed by the DeBERTa model. Our\nexperiments show that our SAS is able to outperform the ELECTRA and other\nstate-of-the-art models in the GLUE tasks with the same or less computation\ncost.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 05:57:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xu", "Yifei", ""], ["Zhang", "Jingqiao", ""], ["He", "Ru", ""], ["Ge", "Liangzhu", ""], ["Yang", "Chao", ""], ["Yang", "Cheng", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2106.07192", "submitter": "Zeqiu Wu", "authors": "Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Bill Dolan", "title": "Automatic Document Sketching: Generating Drafts from Analogous Texts", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of large pre-trained language models has made it possible to make\nhigh-quality predictions on how to add or change a sentence in a document.\nHowever, the high branching factor inherent to text generation impedes the\nability of even the strongest language models to offer useful editing\nsuggestions at a more global or document level. We introduce a new task,\ndocument sketching, which involves generating entire draft documents for the\nwriter to review and revise. These drafts are built from sets of documents that\noverlap in form - sharing large segments of potentially reusable text - while\ndiverging in content. To support this task, we introduce a Wikipedia-based\ndataset of analogous documents and investigate the application of weakly\nsupervised methods, including use of a transformer-based mixture of experts,\ntogether with reinforcement learning. We report experiments using automated and\nhuman evaluation methods and discuss relative merits of these models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 06:46:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Zeqiu", ""], ["Galley", "Michel", ""], ["Brockett", "Chris", ""], ["Zhang", "Yizhe", ""], ["Dolan", "Bill", ""]]}, {"id": "2106.07207", "submitter": "Xiang Lin", "authors": "Xiang Lin, Simeng Han, Shafiq Joty", "title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text\n  Generation", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced large-scale neural language models have led to significant success\nin many language generation tasks. However, the most commonly used training\nobjective, Maximum Likelihood Estimation (MLE), has been shown problematic,\nwhere the trained model prefers using dull and repetitive phrases. In this\nwork, we introduce ScaleGrad, a modification straight to the gradient of the\nloss function, to remedy the degeneration issue of the standard MLE objective.\nBy directly maneuvering the gradient information, ScaleGrad makes the model\nlearn to use novel tokens. Empirical results show the effectiveness of our\nmethod not only in open-ended generation, but also in directed generation\ntasks. With the simplicity in architecture, our method can serve as a general\ntraining objective that is applicable to most of the neural text generation\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 07:46:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lin", "Xiang", ""], ["Han", "Simeng", ""], ["Joty", "Shafiq", ""]]}, {"id": "2106.07225", "submitter": "Shaykh Siddique", "authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, and Md.\n  Mohsin Uddin", "title": "English to Bangla Machine Translation Using Recurrent Neural Network", "comments": "6 pages, 7 figures, Published with International Journal of Future\n  Computer and Communication (IJFCC)", "journal-ref": "International Journal of Future Computer and Communication 9.2\n  (2020)", "doi": "10.18178/ijfcc.2020.9.2.564", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:26:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Siddique", "Shaykh", ""], ["Ahmed", "Tahmid", ""], ["Talukder", "Md. Rifayet Azam", ""], ["Uddin", "Md. Mohsin", ""]]}, {"id": "2106.07237", "submitter": "Arthur Jacobs M", "authors": "Arthur M. Jacobs and Annette Kinder", "title": "Is Einstein more agreeable and less neurotic than Hitler? A\n  computational exploration of the emotional and personality profiles of\n  historical persons", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress in distributed semantic models (DSM) offers new ways to\nestimate personality traits of both fictive and real people. In this\nexploratory study we applied an extended version of the algorithm developed in\nJacobs (2019) to compute the likeability scores, emotional figure profiles and\nBIG5 personality traits for 100 historical persons from the arts, politics or\nscience domains whose names are rather unique (e.g., Einstein, Kahlo, Picasso).\nWe compared the results produced by static (word2vec) and dynamic (BERT)\nlanguage model representations in four studies. The results show both the\npotential and limitations of such DSM-based computations of personality\nprofiles and point ways to further develop this approach to become a useful\ntool in data science, psychology or computational and neurocognitive poetics\n(Jacobs, 2015).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:45:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Jacobs", "Arthur M.", ""], ["Kinder", "Annette", ""]]}, {"id": "2106.07240", "submitter": "Yung-Sung Chuang", "authors": "Yung-Sung Chuang, Mingye Gao, Hongyin Luo, James Glass, Hung-yi Lee,\n  Yun-Nung Chen, Shang-Wen Li", "title": "Mitigating Biases in Toxic Language Detection through Invariant\n  Rationalization", "comments": "The 5th Workshop on Online Abuse and Harms at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of toxic language plays an essential role in protecting\nsocial media users, especially minority groups, from verbal abuse. However,\nbiases toward some attributes, including gender, race, and dialect, exist in\nmost training datasets for toxicity detection. The biases make the learned\nmodels unfair and can even exacerbate the marginalization of people.\nConsidering that current debiasing methods for general natural language\nunderstanding tasks cannot effectively mitigate the biases in the toxicity\ndetectors, we propose to use invariant rationalization (InvRat), a\ngame-theoretic framework consisting of a rationale generator and a predictor,\nto rule out the spurious correlation of certain syntactic patterns (e.g.,\nidentity mentions, dialect) to toxicity labels. We empirically show that our\nmethod yields lower false positive rate in both lexical and dialectal\nattributes than previous debiasing methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:49:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chuang", "Yung-Sung", ""], ["Gao", "Mingye", ""], ["Luo", "Hongyin", ""], ["Glass", "James", ""], ["Lee", "Hung-yi", ""], ["Chen", "Yun-Nung", ""], ["Li", "Shang-Wen", ""]]}, {"id": "2106.07241", "submitter": "Andargachew Mekonnen Gezmu", "authors": "Andargachew Mekonnen Gezmu, Binyam Ephrem Seyoum, Michael Gasser and\n  Andreas N\\\"urnberger", "title": "Contemporary Amharic Corpus: Automatically Morpho-Syntactically Tagged\n  Amharic Corpus", "comments": "Published in Proceedings of the First Workshop on Linguistic\n  Resources for Natural Language Processing at COLING 2018", "journal-ref": "Proceedings of the First Workshop on Linguistic Resources for\n  Natural Language Processing, pp. 65-70. 2018", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduced the contemporary Amharic corpus, which is automatically tagged\nfor morpho-syntactic information. Texts are collected from 25,199 documents\nfrom different domains and about 24 million orthographic words are tokenized.\nSince it is partly a web corpus, we made some automatic spelling error\ncorrection. We have also modified the existing morphological analyzer,\nHornMorpho, to use it for the automatic tagging.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:49:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gezmu", "Andargachew Mekonnen", ""], ["Seyoum", "Binyam Ephrem", ""], ["Gasser", "Michael", ""], ["N\u00fcrnberger", "Andreas", ""]]}, {"id": "2106.07250", "submitter": "Hidetaka Kamigaito", "authors": "Hidetaka Kamigaito, Katsuhiko Hayashi", "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling:\n  With Case Study for Knowledge Graph Embedding", "comments": "Accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 09:07:02 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 07:12:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kamigaito", "Hidetaka", ""], ["Hayashi", "Katsuhiko", ""]]}, {"id": "2106.07275", "submitter": "David Thulke", "authors": "Nico Daheim, David Thulke, Christian Dugast, Hermann Ney", "title": "Cascaded Span Extraction and Response Generation for Document-Grounded\n  Dialog", "comments": "Accepted by 1st DialDoc Workshop at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our entries to both subtasks of the first DialDoc\nshared task which focuses on the agent response prediction task in\ngoal-oriented document-grounded dialogs. The task is split into two subtasks:\npredicting a span in a document that grounds an agent turn and generating an\nagent response based on a dialog and grounding document. In the first subtask,\nwe restrict the set of valid spans to the ones defined in the dataset, use a\nbiaffine classifier to model spans, and finally use an ensemble of different\nmodels. For the second subtask, we use a cascaded model which grounds the\nresponse prediction on the predicted span instead of the full document. With\nthese approaches, we obtain significant improvements in both subtasks compared\nto the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 10:04:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Daheim", "Nico", ""], ["Thulke", "David", ""], ["Dugast", "Christian", ""], ["Ney", "Hermann", ""]]}, {"id": "2106.07285", "submitter": "Israa Alghanmi", "authors": "Israa Alghanmi, Luis Espinosa-Anke, Steven Schockaert", "title": "Probing Pre-Trained Language Models for Disease Knowledge", "comments": "Accepted by ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models such as ClinicalBERT have achieved impressive\nresults on tasks such as medical Natural Language Inference. At first glance,\nthis may suggest that these models are able to perform medical reasoning tasks,\nsuch as mapping symptoms to diseases. However, we find that standard benchmarks\nsuch as MedNLI contain relatively few examples that require such forms of\nreasoning. To better understand the medical reasoning capabilities of existing\nlanguage models, in this paper we introduce DisKnE, a new benchmark for Disease\nKnowledge Evaluation. To construct this benchmark, we annotated each positive\nMedNLI example with the types of medical reasoning that are needed. We then\ncreated negative examples by corrupting these positive examples in an\nadversarial way. Furthermore, we define training-test splits per disease,\nensuring that no knowledge about test diseases can be learned from the training\ndata, and we canonicalize the formulation of the hypotheses to avoid the\npresence of artefacts. This leads to a number of binary classification\nproblems, one for each type of reasoning and each disease. When analysing\npre-trained models for the clinical/biomedical domain on the proposed\nbenchmark, we find that their performance drops considerably.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 10:31:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Alghanmi", "Israa", ""], ["Espinosa-Anke", "Luis", ""], ["Schockaert", "Steven", ""]]}, {"id": "2106.07306", "submitter": "Sean Papay", "authors": "Sean Papay, Roman Klinger and Sebastian Pad\\'o", "title": "Constraining Linear-chain CRFs to Regular Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In structured prediction, a major challenge for models is to represent the\ninterdependencies within their output structures. For the common case where\noutputs are structured as a sequence, linear-chain conditional random fields\n(CRFs) are a widely used model class which can learn local dependencies in\noutput sequences. However, the CRF's Markov assumption makes it impossible for\nthese models to capture nonlocal dependencies, and standard CRFs are unable to\nrespect nonlocal constraints of the data (such as global arity constraints on\noutput labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show using\nsynthetic data that it can be substantially better in practice. Additionally,\nwe demonstrate a practical benefit on downstream tasks by incorporating a\nRegCCRF into a deep neural model for semantic role labeling, exceeding\nstate-of-the-art results on a standard dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:23:59 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 13:15:59 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Papay", "Sean", ""], ["Klinger", "Roman", ""], ["Pad\u00f3", "Sebastian", ""]]}, {"id": "2106.07338", "submitter": "Nidhika Yadav", "authors": "Nidhika Yadav", "title": "Neighborhood Rough Set based Multi-document Summarization", "comments": "7 pages, original paper not submitted anywhere else", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research paper proposes a novel Neighbourhood Rough Set based approach\nfor supervised Multi-document Text Summarization (MDTS) with analysis and\nimpact on the summarization results for MDTS. Here, Rough Set based LERS\nalgorithm is improved using Neighborhood Rough Set which is itself a novel\ncombination called Neighborhood-LERS to be experimented for evaluations of\nefficacy and efficiency. In this paper, we shall apply and evaluate the\nproposed Neighborhood-LERS for Multi-document Summarization which here is\nproved experimentally to be superior to the base LERS technique for MDTS.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 00:43:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yadav", "Nidhika", ""]]}, {"id": "2106.07340", "submitter": "Jia Shen", "authors": "Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan,\n  Xintao Wu, Dongwon Lee", "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in\n  Mathematics Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Due to the transfer learning nature of BERT model, researchers have achieved\nbetter performance than base BERT by further pre-training the original BERT on\na huge domain-specific corpus. Due to the special nature of mathematical texts\nwhich often contain math equations and symbols, the original BERT model\npre-trained on general English context will not fit Natural Language Processing\n(NLP) tasks in mathematical education well. Therefore, we propose MathBERT, a\nBERT pre-trained on large mathematical corpus including pre-k to graduate level\nmathematical content to tackle math-specific tasks. In addition, We generate a\ncustomized mathematical vocabulary to pre-train with MathBERT and compare the\nperformance to the MathBERT pre-trained with the original BERT vocabulary. We\nselect three important tasks in mathematical education such as knowledge\ncomponent, auto-grading, and knowledge tracing prediction to evaluate the\nperformance of MathBERT. Our experiments show that MathBERT outperforms the\nbase BERT by 2-9\\% margin. In some cases, MathBERT pre-trained with\nmathematical vocabulary is better than MathBERT trained with original\nvocabulary.To our best knowledge, MathBERT is the first pre-trained model for\ngeneral purpose mathematics education tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:43:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shen", "Jia Tracy", ""], ["Yamashita", "Michiharu", ""], ["Prihar", "Ethan", ""], ["Heffernan", "Neil", ""], ["Wu", "Xintao", ""], ["Lee", "Dongwon", ""]]}, {"id": "2106.07341", "submitter": "Rachit Garg", "authors": "Rachit Garg, Arvind W Kiwelekar, Laxman D Netak, Akshay Ghodake", "title": "i-Pulse: A NLP based novel approach for employee engagement in logistics\n  organization", "comments": "11 Pages 7 Figures. International Journal of Information Management\n  Data Insights (Elsevier) 2021", "journal-ref": null, "doi": "10.1016/j.jjimei.2021.100011", "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:20:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Garg", "Rachit", ""], ["Kiwelekar", "Arvind W", ""], ["Netak", "Laxman D", ""], ["Ghodake", "Akshay", ""]]}, {"id": "2106.07343", "submitter": "Yutai Hou", "authors": "Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che, Ting Liu", "title": "Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent\n  Detection and Slot Filling", "comments": "Accepted by ACL 2021 findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate few-shot joint learning for dialogue language\nunderstanding. Most existing few-shot models learn a single task each time with\nonly a few examples. However, dialogue language understanding contains two\nclosely related tasks, i.e., intent detection and slot filling, and often\nbenefits from jointly learning the two tasks. This calls for new few-shot\nlearning techniques that are able to capture task relations from only a few\nexamples and jointly learn multiple tasks. To achieve this, we propose a\nsimilarity-based few-shot learning scheme, named Contrastive Prototype Merging\nnetwork (ConProm), that learns to bridge metric spaces of intent and slot on\ndata-rich domains, and then adapt the bridged metric space to the specific\nfew-shot domain. Experiments on two public datasets, Snips and FewJoint, show\nthat our model significantly outperforms the strong baselines in one and five\nshots settings.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:07:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hou", "Yutai", ""], ["Lai", "Yongkui", ""], ["Chen", "Cheng", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2106.07344", "submitter": "Pervaiz Khan", "authors": "Pervaiz Iqbal Khan, Imran Razzak, Andreas Dengel, Sheraz Ahmed", "title": "Understanding Information Spreading Mechanisms During COVID-19 Pandemic\n  by Analyzing the Impact of Tweet Text and User Features for Retweet\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 has affected the world economy and the daily life routine of almost\neveryone. It has been a hot topic on social media platforms such as Twitter,\nFacebook, etc. These social media platforms enable users to share information\nwith other users who can reshare this information, thus causing this\ninformation to spread. Twitter's retweet functionality allows users to share\nthe existing content with other users without altering the original content.\nAnalysis of social media platforms can help in detecting emergencies during\npandemics that lead to taking preventive measures. One such type of analysis is\npredicting the number of retweets for a given COVID-19 related tweet. Recently,\nCIKM organized a retweet prediction challenge for COVID-19 tweets focusing on\nusing numeric features only. However, our hypothesis is, tweet text may play a\nvital role in an accurate retweet prediction. In this paper, we combine numeric\nand text features for COVID-19 related retweet predictions. For this purpose,\nwe propose two CNN and RNN based models and evaluate the performance of these\nmodels on a publicly available TweetsCOV19 dataset using seven different\nevaluation metrics. Our evaluation results show that combining tweet text with\nnumeric features improves the performance of retweet prediction significantly.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:55:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Khan", "Pervaiz Iqbal", ""], ["Razzak", "Imran", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2106.07345", "submitter": "Taeuk Kim", "authors": "Taeuk Kim, Kang Min Yoo, Sang-goo Lee", "title": "Self-Guided Contrastive Learning for BERT Sentence Representations", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although BERT and its variants have reshaped the NLP landscape, it still\nremains unclear how best to derive sentence embeddings from such pre-trained\nTransformers. In this work, we propose a contrastive learning method that\nutilizes self-guidance for improving the quality of BERT sentence\nrepresentations. Our method fine-tunes BERT in a self-supervised fashion, does\nnot rely on data augmentation, and enables the usual [CLS] token embeddings to\nfunction as sentence vectors. Moreover, we redesign the contrastive learning\nobjective (NT-Xent) and apply it to sentence representation learning. We\ndemonstrate with extensive experiments that our approach is more effective than\ncompetitive baselines on diverse sentence-related tasks. We also show it is\nefficient at inference and robust to domain shifts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:52:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kim", "Taeuk", ""], ["Yoo", "Kang Min", ""], ["Lee", "Sang-goo", ""]]}, {"id": "2106.07348", "submitter": "Sohom Ghosh", "authors": "Sohom Ghosh", "title": "Is it a click bait? Let's predict using Machine Learning", "comments": "M.Tech Thesis defended at BITS, Pilani", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this era of digitisation, news reader tend to read news online. This is\nbecause, online media instantly provides access to a wide variety of content.\nThus, people don't have to wait for tomorrow's newspaper to know what's\nhappening today. Along with these virtues, online news have some vices as well.\nOne such vice is presence of social media posts (tweets) relating to news\narticles whose sole purpose is to draw attention of the users rather than\ndirecting them to read the actual content. Such posts are referred to as\nclickbaits. The objective of this project is to develop a system which would be\ncapable of predicting how likely are the social media posts (tweets) relating\nto new articles tend to be clickbait.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:07:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ghosh", "Sohom", ""]]}, {"id": "2106.07349", "submitter": "Anmol Nayak", "authors": "Anmol Nayak, Hari Prasad Timmapathini", "title": "Using Integrated Gradients to explain Linguistic Acceptability learnt by\n  BERT", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BERT has been a breakthrough in language understanding by leveraging the\nmulti-head self-attention mechanism in its architecture. To the best of our\nknowledge this work is the first to leverage Layer Integrated Gradients\nAttribution Scores (LIGAS) to explain the Linguistic Acceptability criteria\nthat are learnt by BERT on the Corpus of Linguistic Acceptability (CoLA)\nbenchmark dataset. Our experiments on 5 different categories of sentences lead\nto the following interesting findings: 1) LIGAS for Linguistically Acceptable\n(LA) sentences are significantly smaller in comparison to Linguistically\nUnacceptable (LUA) sentences, 2) There are specific subtrees of the\nConstituency Parse Tree (CPT) for LA and LUA sentences which contribute larger\nLIGAS, 3) Across the different categories of sentences we observed around 88%\nto 100% of the Correctly classified sentences had positive LIGAS, indicating a\nstrong positive relationship to the prediction confidence of the model, and 4)\nAround 57% of the Misclassified sentences had positive LIGAS, which we believe\ncan become correctly classified sentences if the LIGAS are parameterized in the\nloss function of the model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:17:45 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nayak", "Anmol", ""], ["Timmapathini", "Hari Prasad", ""]]}, {"id": "2106.07350", "submitter": "Zhe Liu", "authors": "Zhe Liu and Yibin Xu", "title": "THG: Transformer with Hyperbolic Geometry", "comments": "6 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer model architectures have become an indispensable staple in deep\nlearning lately for their effectiveness across a range of tasks. Recently, a\nsurge of \"X-former\" models have been proposed which improve upon the original\nTransformer architecture. However, most of these variants make changes only\naround the quadratic time and memory complexity of self-attention, i.e. the dot\nproduct between the query and the key. What's more, they are calculate solely\nin Euclidean space. In this work, we propose a novel Transformer with\nHyperbolic Geometry (THG) model, which take the advantage of both Euclidean\nspace and Hyperbolic space. THG makes improvements in linear transformations of\nself-attention, which are applied on the input sequence to get the query and\nthe key, with the proposed hyperbolic linear. Extensive experiments on sequence\nlabeling task, machine reading comprehension task and classification task\ndemonstrate the effectiveness and generalizability of our model. It also\ndemonstrates THG could alleviate overfitting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:09:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Zhe", ""], ["Xu", "Yibin", ""]]}, {"id": "2106.07352", "submitter": "Nicholas FitzGerald", "authors": "Nicholas FitzGerald, Jan A. Botha, Daniel Gillick, Daniel M. Bikel,\n  Tom Kwiatkowski, Andrew McCallum", "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation\n  Network", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an instance-based nearest neighbor approach to entity linking. In\ncontrast to most prior entity retrieval systems which represent each entity\nwith a single vector, we build a contextualized mention-encoder that learns to\nplace similar mentions of the same entity closer in vector space than mentions\nof different entities. This approach allows all mentions of an entity to serve\nas \"class prototypes\" as inference involves retrieving from the full set of\nlabeled entity mentions in the training set and applying the nearest mention\nneighbor's entity label. Our model is trained on a large multilingual corpus of\nmention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor\ninference on an index of 700 million mentions. It is simpler to train, gives\nmore interpretable predictions, and outperforms all other systems on two\nmultilingual entity linking benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:54:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["FitzGerald", "Nicholas", ""], ["Botha", "Jan A.", ""], ["Gillick", "Daniel", ""], ["Bikel", "Daniel M.", ""], ["Kwiatkowski", "Tom", ""], ["McCallum", "Andrew", ""]]}, {"id": "2106.07353", "submitter": "Yifan Ding", "authors": "Yifan Ding, Nicholas Botzer, Tim Weninger", "title": "Posthoc Verification and the Fallibility of the Ground Truth", "comments": "12 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifiers commonly make use of pre-annotated datasets, wherein a model is\nevaluated by pre-defined metrics on a held-out test set typically made of\nhuman-annotated labels. Metrics used in these evaluations are tied to the\navailability of well-defined ground truth labels, and these metrics typically\ndo not allow for inexact matches. These noisy ground truth labels and strict\nevaluation metrics may compromise the validity and realism of evaluation\nresults. In the present work, we discuss these concerns and conduct a\nsystematic posthoc verification experiment on the entity linking (EL) task.\nUnlike traditional methodologies, which asks annotators to provide free-form\nannotations, we ask annotators to verify the correctness of annotations after\nthe fact (i.e., posthoc). Compared to pre-annotation evaluation,\nstate-of-the-art EL models performed extremely well according to the posthoc\nevaluation methodology. Posthoc validation also permits the validation of the\nground truth dataset. Surprisingly, we find predictions from EL models had a\nsimilar or higher verification rate than the ground truth. We conclude with a\ndiscussion on these findings and recommendations for future evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:57:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ding", "Yifan", ""], ["Botzer", "Nicholas", ""], ["Weninger", "Tim", ""]]}, {"id": "2106.07359", "submitter": "Zeyd Boukhers", "authors": "Zeyd Boukhers and Nada Beili and Timo Hartmann and Prantik Goswami and\n  Muhammad Arslan Zafar", "title": "MexPub: Deep Transfer Learning for Metadata Extraction from German\n  Publications", "comments": "A long version of an accepted paper @ JCDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Extracting metadata from scientific papers can be considered a solved problem\nin NLP due to the high accuracy of state-of-the-art methods. However, this does\nnot apply to German scientific publications, which have a variety of styles and\nlayouts. In contrast to most of the English scientific publications that follow\nstandard and simple layouts, the order, content, position and size of metadata\nin German publications vary greatly among publications. This variety makes\ntraditional NLP methods fail to accurately extract metadata from these\npublications. In this paper, we present a method that extracts metadata from\nPDF documents with different layouts and styles by viewing the document as an\nimage. We used Mask R-CNN that is trained on COCO dataset and finetuned with\nPubLayNet dataset that consists of ~200K PDF snapshots with five basic classes\n(e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic\ndataset consisting of ~30K article snapshots to extract nine patterns (i.e.\nauthor, title, etc). Our synthetic dataset is generated using contents in both\nlanguages German and English and a finite set of challenging templates obtained\nfrom German publications. Our method achieved an average accuracy of around\n$90\\%$ which validates its capability to accurately extract metadata from a\nvariety of PDF documents with challenging templates.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:43:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Boukhers", "Zeyd", ""], ["Beili", "Nada", ""], ["Hartmann", "Timo", ""], ["Goswami", "Prantik", ""], ["Zafar", "Muhammad Arslan", ""]]}, {"id": "2106.07363", "submitter": "Saeid Hosseini", "authors": "Sayna Esmailzadeh, Saeid Hosseini, Mohammad Reza Kangavari, Wen Hua", "title": "Cognitive-aware Short-text Understanding for Inferring Professions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging short-text contents to estimate the occupation of microblog\nauthors has significant gains in many applications. Yet challenges abound.\nFirstly brief textual contents come with excessive lexical noise that makes the\ninference problem challenging. Secondly, cognitive-semantics are not evident,\nand important linguistic features are latent in short-text contents. Thirdly,\nit is hard to measure the correlation between the cognitive short-text\nsemantics and the features pertaining various occupations. We argue that the\nmulti-aspect cognitive features are needed to correctly associate short-text\ncontents to a particular job and discover suitable people for the careers. To\nthis end, we devise a novel framework that on the one hand, can infer\nshort-text contents and exploit cognitive features, and on the other hand,\nfuses various adopted novel algorithms, such as curve fitting, support vector,\nand boosting modules to better predict the occupation of the authors. The final\nestimation module manufactures the $R^w$-tree via coherence weight to tune the\nbest outcome in the inferring process. We conduct comprehensive experiments on\nreal-life Twitter data. The experimental results show that compared to other\nrivals, our cognitive multi-aspect model can achieve a higher performance in\nthe career estimation procedure, where it is inevitable to neglect the\ncontextual semantics of users.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:19:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Esmailzadeh", "Sayna", ""], ["Hosseini", "Saeid", ""], ["Kangavari", "Mohammad Reza", ""], ["Hua", "Wen", ""]]}, {"id": "2106.07364", "submitter": "Ruixiang Cui", "authors": "Ruixiang Cui, Daniel Hershcovich", "title": "Meaning Representation of Numeric Fused-Heads in UCCA", "comments": "UnImplicit Workshop at ACL 2021 (abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We exhibit that the implicit UCCA parser does not address numeric fused-heads\n(NFHs) consistently, which could result either from inconsistent annotation,\ninsufficient training data or a modelling limitation. and show which factors\nare involved. We consider this phenomenon important, as it is pervasive in text\nand critical for correct inference. Careful design and fine-grained annotation\nof NFHs in meaning representation frameworks would benefit downstream tasks\nsuch as machine translation, natural language inference and question answering,\nparticularly when they require numeric reasoning, as recovering and\ncategorizing them. We are investigating the treatment of this phenomenon by\nother meaning representations, such as AMR. We encourage researchers in meaning\nrepresentations, and computational linguistics in general, to address this\nphenomenon in future research.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:00:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cui", "Ruixiang", ""], ["Hershcovich", "Daniel", ""]]}, {"id": "2106.07381", "submitter": "Li Dong", "authors": "Li Dong, Matthew C. Spencer, Amir Biagi", "title": "A Semi-supervised Multi-task Learning Approach to Classify Customer\n  Contact Intents", "comments": "To be published in ACL-IJCNLP 2021 workshop ECNLP", "journal-ref": "https://aclanthology.org/2021.ecnlp-1.7/", "doi": "10.18653/v1/2021.ecnlp-1.7", "report-no": "2021.ecnlp-1.7", "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of customer support, understanding customers' intents is a\ncrucial step. Machine learning plays a vital role in this type of intent\nclassification. In reality, it is typical to collect confirmation from customer\nsupport representatives (CSRs) regarding the intent prediction, though it can\nunnecessarily incur prohibitive cost to ask CSRs to assign existing or new\nintents to the mis-classified cases. Apart from the confirmed cases with and\nwithout intent labels, there can be a number of cases with no human curation.\nThis data composition (Positives + Unlabeled + multiclass Negatives) creates\nunique challenges for model development. In response to that, we propose a\nsemi-supervised multi-task learning paradigm. In this manuscript, we share our\nexperience in building text-based intent classification models for a customer\nsupport service on an E-commerce website. We improve the performance\nsignificantly by evolving the model from multiclass classification to\nsemi-supervised multi-task learning by leveraging the negative cases, domain-\nand task-adaptively pretrained ALBERT on customer contact texts, and a number\nof un-curated data with no labels. In the evaluation, the final model boosts\nthe average AUC ROC by almost 20 points compared to the baseline finetuned\nmulticlass classification ALBERT model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:13:05 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Dong", "Li", ""], ["Spencer", "Matthew C.", ""], ["Biagi", "Amir", ""]]}, {"id": "2106.07385", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, S\\\"oren Auer and Ted Pedersen", "title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP\n  Contributions for a Research Knowledge Graph", "comments": "13 pages, 5 figures, 8 tables, In Proceedings of the Fifteenth\n  Workshop on Semantic Evaluation SemEval-2021 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n  Overall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:43:47 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 09:26:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""], ["Pedersen", "Ted", ""]]}, {"id": "2106.07400", "submitter": "Clara Meister", "authors": "Clara Meister, Martina Forster, Ryan Cotterell", "title": "Determinantal Beam Search", "comments": null, "journal-ref": "Proceedings of ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beam search is a go-to strategy for decoding neural sequence models. The\nalgorithm can naturally be viewed as a subset optimization problem, albeit one\nwhere the corresponding set function does not reflect interactions between\ncandidates. Empirically, this leads to sets often exhibiting high overlap,\ne.g., strings may differ by only a single word. Yet in use-cases that call for\nmultiple solutions, a diverse or representative set is often desired. To\naddress this issue, we propose a reformulation of beam search, which we call\ndeterminantal beam search. Determinantal beam search has a natural relationship\nto determinantal point processes (DPPs), models over sets that inherently\nencode intra-set interactions. By posing iterations in beam search as a series\nof subdeterminant maximization problems, we can turn the algorithm into a\ndiverse subset selection process. In a case study, we use the string\nsubsequence kernel to explicitly encourage n-gram coverage in text generated\nfrom a sequence model. We observe that our algorithm offers competitive\nperformance against other diverse set generation strategies in the context of\nlanguage generation, while providing a more general approach to optimizing for\ndiversity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:01:46 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 11:50:04 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 07:09:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Meister", "Clara", ""], ["Forster", "Martina", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2106.07410", "submitter": "Nengfeng Zhou", "authors": "Shafie Gholizadeh and Nengfeng Zhou", "title": "Model Explainability in Deep Learning Based Natural Language Processing", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) model explainability has received growing attention,\nespecially in the area related to model risk and regulations. In this paper, we\nreviewed and compared some popular ML model explainability methodologies,\nespecially those related to Natural Language Processing (NLP) models. We then\napplied one of the NLP explainability methods Layer-wise Relevance Propagation\n(LRP) to a NLP classification model. We used the LRP method to derive a\nrelevance score for each word in an instance, which is a local explainability.\nThe relevance scores are then aggregated together to achieve global variable\nimportance of the model. Through the case study, we also demonstrated how to\napply the local explainability method to false positive and false negative\ninstances to discover the weakness of a NLP model. These analysis can help us\nto understand NLP models better and reduce the risk due to the black-box nature\nof NLP models. We also identified some common issues due to the special natures\nof NLP models and discussed how explainability analysis can act as a control to\ndetect these issues after the model has been trained.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:23:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gholizadeh", "Shafie", ""], ["Zhou", "Nengfeng", ""]]}, {"id": "2106.07447", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,\n  Ruslan Salakhutdinov, Abdelrahman Mohamed", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:14:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Bolte", "Benjamin", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Lakhotia", "Kushal", ""], ["Salakhutdinov", "Ruslan", ""], ["Mohamed", "Abdelrahman", ""]]}, {"id": "2106.07485", "submitter": "Bob Coecke", "authors": "Bob Coecke and Vincent Wang", "title": "Grammar Equations", "comments": "10 pages, many pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagrammatically speaking, grammatical calculi such as pregroups provide\nwires between words in order to elucidate their interactions, and this enables\none to verify grammatical correctness of phrases and sentences. In this paper\nwe also provide wirings within words. This will enable us to identify\ngrammatical constructs that we expect to be either equal or closely related.\nHence, our work paves the way for a new theory of grammar, that provides novel\n`grammatical truths'. We give a nogo-theorem for the fact that our wirings for\nwords make no sense for preordered monoids, the form which grammatical calculi\nusually take. Instead, they require diagrams -- or equivalently, (free)\nmonoidal categories.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:16:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Coecke", "Bob", ""], ["Wang", "Vincent", ""]]}, {"id": "2106.07499", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal and Diyi Yang", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in\n  NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  NLP has achieved great progress in the past decade through the use of neural\nmodels and large labeled datasets. The dependence on abundant data prevents NLP\nmodels from being applied to low-resource settings or novel tasks where\nsignificant time, money, or expertise is required to label massive amounts of\ntextual data. Recently, data augmentation methods have been explored as a means\nof improving data efficiency in NLP. To date, there has been no systematic\nempirical overview of data augmentation for NLP in the limited labeled data\nsetting, making it difficult to understand which methods work in which\nsettings. In this paper, we provide an empirical survey of recent progress on\ndata augmentation for NLP in the limited labeled data setting, summarizing the\nlandscape of methods (including token-level augmentations, sentence-level\naugmentations, adversarial augmentations, and hidden-space augmentations) and\ncarrying out experiments on 11 datasets covering topics/news classification,\ninference tasks, paraphrasing tasks, and single-sentence tasks. Based on the\nresults, we draw several conclusions to help practitioners choose appropriate\naugmentations in different settings and discuss the current challenges and\nfuture directions for limited data learning in NLP.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:27:22 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Jiaao", ""], ["Tam", "Derek", ""], ["Raffel", "Colin", ""], ["Bansal", "Mohit", ""], ["Yang", "Diyi", ""]]}, {"id": "2106.07505", "submitter": "Vanessa Hahn", "authors": "Vanessa Hahn, Dana Ruiter, Thomas Kleinbauer, Dietrich Klakow", "title": "Modeling Profanity and Hate Speech in Social Media with Semantic\n  Subspaces", "comments": "9 pages, 4 figures, accepted as a long paper at Workshop on Online\n  Abuse and Harms 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech and profanity detection suffer from data sparsity, especially for\nlanguages other than English, due to the subjective nature of the tasks and the\nresulting annotation incompatibility of existing corpora. In this study, we\nidentify profane subspaces in word and sentence representations and explore\ntheir generalization capability on a variety of similar and distant target\ntasks in a zero-shot setting. This is done monolingually (German) and\ncross-lingually to closely-related (English), distantly-related (French) and\nnon-related (Arabic) tasks. We observe that, on both similar and distant target\ntasks and across all languages, the subspace-based representations transfer\nmore effectively than standard BERT representations in the zero-shot setting,\nwith improvements between F1 +10.9 and F1 +42.9 over the baselines across all\ntested monolingual and cross-lingual scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:34:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 10:04:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hahn", "Vanessa", ""], ["Ruiter", "Dana", ""], ["Kleinbauer", "Thomas", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2106.07540", "submitter": "Zaid Alyafeai Mr", "authors": "Zaid Alyafeai, Maged S. Al-shaibani, Mustafa Ghaleb, Irfan Ahmad", "title": "Evaluating Various Tokenizers for Arabic Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first step in any NLP pipeline is learning word vector representations.\nHowever, given a large text corpus, representing all the words is not\nefficient. In the literature, many tokenization algorithms have emerged to\ntackle this problem by creating subwords which in turn limits the vocabulary\nsize in any text corpus. However such algorithms are mostly language-agnostic\nand lack a proper way of capturing meaningful tokens. Not to mention the\ndifficulty of evaluating such techniques in practice. In this paper, we\nintroduce three new tokenization algorithms for Arabic and compare them to\nthree other baselines using unsupervised evaluations. In addition to that, we\ncompare all the six algorithms by evaluating them on three tasks which are\nsentiment analysis, news classification and poetry classification. Our\nexperiments show that the performance of such tokenization algorithms depends\non the size of the dataset, type of the task, and the amount of morphology that\nexists in the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:05:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Alyafeai", "Zaid", ""], ["Al-shaibani", "Maged S.", ""], ["Ghaleb", "Mustafa", ""], ["Ahmad", "Irfan", ""]]}, {"id": "2106.07583", "submitter": "Shogo Ujiie", "authors": "Shogo Ujiie, Hayate Iso, Eiji Aramaki", "title": "Biomedical Entity Linking with Contrastive Context Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BioCoM, a contrastive learning framework for biomedical entity\nlinking that uses only two resources: a small-sized dictionary and a large\nnumber of raw biomedical articles. Specifically, we build the training\ninstances from raw PubMed articles by dictionary matching and use them to train\na context-aware entity linking model with contrastive learning. We predict the\nnormalized biomedical entity at inference time through a nearest-neighbor\nsearch. Results found that BioCoM substantially outperforms state-of-the-art\nmodels, especially in low-resource settings, by effectively using the context\nof the entities.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:43:33 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 05:47:29 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ujiie", "Shogo", ""], ["Iso", "Hayate", ""], ["Aramaki", "Eiji", ""]]}, {"id": "2106.07691", "submitter": "Animesh Nighojkar", "authors": "Animesh Nighojkar and John Licato", "title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  If two sentences have the same meaning, it should follow that they are\nequivalent in their inferential properties, i.e., each sentence should\ntextually entail the other. However, many paraphrase datasets currently in\nwidespread use rely on a sense of paraphrase based on word overlap and syntax.\nCan we teach them instead to identify paraphrases in a way that draws on the\ninferential properties of the sentences, and is not over-reliant on lexical and\nsyntactic similarities of a sentence pair? We apply the adversarial paradigm to\nthis question, and introduce a new adversarial method of dataset creation for\nparaphrase identification: the Adversarial Paraphrasing Task (APT), which asks\nparticipants to generate semantically equivalent (in the sense of mutually\nimplicative) but lexically and syntactically disparate paraphrases. These\nsentence pairs can then be used both to test paraphrase identification models\n(which get barely random accuracy) and then improve their performance. To\naccelerate dataset generation, we explore automation of APT using T5, and show\nthat the resulting dataset also improves accuracy. We discuss implications for\nparaphrase detection and release our dataset in the hope of making paraphrase\ndetection models better able to detect sentence-level meaning equivalence.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:15:20 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Nighojkar", "Animesh", ""], ["Licato", "John", ""]]}, {"id": "2106.07699", "submitter": "William Hartmann", "authors": "Andrew Slottje, Shannon Wotherspoon, William Hartmann, Matthew Snover,\n  Owen Kimball", "title": "Using heterogeneity in semi-supervised transcription hypotheses to\n  improve code-switched speech recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling code-switched speech is an important problem in automatic speech\nrecognition (ASR). Labeled code-switched data are rare, so monolingual data are\noften used to model code-switched speech. These monolingual data may be more\nclosely matched to one of the languages in the code-switch pair. We show that\nsuch asymmetry can bias prediction toward the better-matched language and\ndegrade overall model performance. To address this issue, we propose a\nsemi-supervised approach for code-switched ASR. We consider the case of\nEnglish-Mandarin code-switching, and the problem of using monolingual data to\nbuild bilingual \"transcription models'' for annotation of unlabeled\ncode-switched data. We first build multiple transcription models so that their\nindividual predictions are variously biased toward either English or Mandarin.\nWe then combine these biased transcriptions using confidence-based selection.\nThis strategy generates a superior transcript for semi-supervised training, and\nobtains a 19% relative improvement compared to a semi-supervised system that\nrelies on a transcription model built with only the best-matched monolingual\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:39:18 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Slottje", "Andrew", ""], ["Wotherspoon", "Shannon", ""], ["Hartmann", "William", ""], ["Snover", "Matthew", ""], ["Kimball", "Owen", ""]]}, {"id": "2106.07704", "submitter": "Han Guo", "authors": "Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu", "title": "Text Generation with Efficient (Soft) Q-Learning", "comments": "Code available at\n  https://github.com/HanGuo97/soft-Q-learning-for-text-generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many applications, such as generating adversarial\nattacks or generating prompts to control language models. Reinforcement\nlearning (RL) on the other hand offers a more flexible solution by allowing\nusers to plug in arbitrary task metrics as reward. Yet previous RL algorithms\nfor text generation, such as policy gradient (on-policy RL) and Q-learning\n(off-policy RL), are often notoriously inefficient or unstable to train due to\nthe large sequence space and the sparse reward received only at the end of\nsequences. In this paper, we introduce a new RL formulation for text generation\nfrom the soft Q-learning perspective. It further enables us to draw from the\nlatest RL advances, such as path consistency learning, to combine the best of\non-/off-policy updates, and learn effectively from sparse reward. We apply the\napproach to a wide range of tasks, including learning from noisy/negative\nexamples, adversarial attacks, and prompt generation. Experiments show our\napproach consistently outperforms both task-specialized algorithms and the\nprevious RL methods. On standard supervised tasks where MLE prevails, our\napproach also achieves competitive performance and stability by training text\ngeneration from scratch.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:48:40 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 19:51:44 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Guo", "Han", ""], ["Tan", "Bowen", ""], ["Liu", "Zhengzhong", ""], ["Xing", "Eric P.", ""], ["Hu", "Zhiting", ""]]}, {"id": "2106.07716", "submitter": "William Hartmann", "authors": "Chak-Fai Li, Francis Keith, William Hartmann, Matthew Snover, Owen\n  Kimball", "title": "Overcoming Domain Mismatch in Low Resource Sequence-to-Sequence ASR\n  Models using Hybrid Generated Pseudotranscripts", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence (seq2seq) models are competitive with hybrid models for\nautomatic speech recognition (ASR) tasks when large amounts of training data\nare available. However, data sparsity and domain adaptation are more\nproblematic for seq2seq models than their hybrid counterparts. We examine\ncorpora of five languages from the IARPA MATERIAL program where the transcribed\ndata is conversational telephone speech (CTS) and evaluation data is broadcast\nnews (BN). We show that there is a sizable initial gap in such a data condition\nbetween hybrid and seq2seq models, and the hybrid model is able to further\nimprove through the use of additional language model (LM) data. We use an\nadditional set of untranscribed data primarily in the BN domain for\nsemisupervised training. In semisupervised training, a seed model trained on\ntranscribed data generates hypothesized transcripts for unlabeled\ndomain-matched data for further training. By using a hybrid model with an\nexpanded language model for pseudotranscription, we are able to improve our\nseq2seq model from an average word error rate (WER) of 66.7% across all five\nlanguages to 29.0% WER. While this puts the seq2seq model at a competitive\noperating point, hybrid models are still able to use additional LM data to\nmaintain an advantage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:25:57 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Li", "Chak-Fai", ""], ["Keith", "Francis", ""], ["Hartmann", "William", ""], ["Snover", "Matthew", ""], ["Kimball", "Owen", ""]]}, {"id": "2106.07719", "submitter": "Mahdi Hajiaghayi", "authors": "Mahdi Hajiaghayi, Monir Hajiaghayi, Mark Bolin", "title": "Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-lingual sentence encoder that can be used\nin search engines as a query and document encoder. This embedding enables a\nsemantic similarity score between queries and documents that can be an\nimportant feature in document ranking and relevancy. To train such a customized\nsentence encoder, it is beneficial to leverage users search data in the form of\nquery-document clicked pairs however, we must avoid relying too much on search\nclick data as it is biased and does not cover many unseen cases. The search\ndata is heavily skewed towards short queries and for long queries is small and\noften noisy. The goal is to design a universal multi-lingual encoder that works\nfor all cases and covers both short and long queries. We select a number of\npublic NLI datasets in different languages and translation data and together\nwith user search data we train a language model using a multi-task approach. A\nchallenge is that these datasets are not homogeneous in terms of content, size\nand the balance ratio. While the public NLI datasets are usually two-sentence\nbased with the same portion of positive and negative pairs, the user search\ndata can contain multi-sentence documents and only positive pairs. We show how\nmulti-task training enables us to leverage all these datasets and exploit\nknowledge sharing across these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:19:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hajiaghayi", "Mahdi", ""], ["Hajiaghayi", "Monir", ""], ["Bolin", "Mark", ""]]}, {"id": "2106.07722", "submitter": "Jiarun Cao", "authors": "Jiarun Cao, Elke M van Veen, Niels Peek, Andrew G Renehan, Sophia\n  Ananiadou", "title": "EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from\n  Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To interpret the genetic profile present in a patient sample, it is necessary\nto know which mutations have important roles in the development of the\ncorresponding cancer type. Named entity recognition is a core step in the text\nmining pipeline which facilitates mining valuable cancer information from the\nscientific literature. However, due to the scarcity of related datasets,\nprevious NER attempts in this domain either suffer from low performance when\ndeep learning based models are deployed, or they apply feature based machine\nlearning models or rule based models to tackle this problem, which requires\nintensive efforts from domain experts, and limit the model generalization\ncapability. In this paper, we propose EPICURE, an ensemble pre trained model\nequipped with a conditional random field pattern layer and a span prediction\npattern layer to extract cancer mutations from text. We also adopt a data\naugmentation strategy to expand our training set from multiple datasets.\nExperimental results on three benchmark datasets show competitive results\ncompared to the baseline models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:08:15 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Cao", "Jiarun", ""], ["van Veen", "Elke M", ""], ["Peek", "Niels", ""], ["Renehan", "Andrew G", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "2106.07728", "submitter": "Minae Kwon", "authors": "Minae Kwon, Siddharth Karamcheti, Mariano-Florentino Cuellar, Dorsa\n  Sadigh", "title": "Targeted Data Acquisition for Evolving Negotiation Agents", "comments": "The Thirty-eighth International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Successful negotiators must learn how to balance optimizing for self-interest\nand cooperation. Yet current artificial negotiation agents often heavily depend\non the quality of the static datasets they were trained on, limiting their\ncapacity to fashion an adaptive response balancing self-interest and\ncooperation. For this reason, we find that these agents can achieve either high\nutility or cooperation, but not both. To address this, we introduce a targeted\ndata acquisition framework where we guide the exploration of a reinforcement\nlearning agent using annotations from an expert oracle. The guided exploration\nincentivizes the learning agent to go beyond its static dataset and develop new\nnegotiation strategies. We show that this enables our agents to obtain\nhigher-reward and more Pareto-optimal solutions when negotiating with both\nsimulated and human partners compared to standard supervised learning and\nreinforcement learning methods. This trend additionally holds when comparing\nagents using our targeted data acquisition framework to variants of agents\ntrained with a mix of supervised learning and reinforcement learning, or to\nagents using tailored reward functions that explicitly optimize for utility and\nPareto-optimality.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:45:59 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 17:49:13 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Kwon", "Minae", ""], ["Karamcheti", "Siddharth", ""], ["Cuellar", "Mariano-Florentino", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2106.07734", "submitter": "Rupak Vignesh Swaminathan", "authors": "Rupak Vignesh Swaminathan, Brian King, Grant P. Strimel, Jasha Droppo,\n  Athanasios Mouchtaris", "title": "CoDERT: Distilling Encoder Representations with Co-learning for\n  Transducer-based Speech Recognition", "comments": "Accepted at InterSpeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective method to compress an RNN-Transducer\n(RNN-T) through the well-known knowledge distillation paradigm. We show that\nthe transducer's encoder outputs naturally have a high entropy and contain rich\ninformation about acoustically similar word-piece confusions. This rich\ninformation is suppressed when combined with the lower entropy decoder outputs\nto produce the joint network logits. Consequently, we introduce an auxiliary\nloss to distill the encoder logits from a teacher transducer's encoder, and\nexplore training strategies where this encoder distillation works effectively.\nWe find that tandem training of teacher and student encoders with an inplace\nencoder distillation outperforms the use of a pre-trained and static teacher\ntransducer. We also report an interesting phenomenon we refer to as implicit\ndistillation, that occurs when the teacher and student encoders share the same\ndecoder. Our experiments show 5.37-8.4% relative word error rate reductions\n(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 20:03:57 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Swaminathan", "Rupak Vignesh", ""], ["King", "Brian", ""], ["Strimel", "Grant P.", ""], ["Droppo", "Jasha", ""], ["Mouchtaris", "Athanasios", ""]]}, {"id": "2106.07742", "submitter": "Alex Brandsen", "authors": "Alex Brandsen, Suzan Verberne, Karsten Lambers, Milco Wansleeben", "title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval\n  in the Archaeology Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of archaeological literature is growing rapidly. Until recently,\nthese data were only accessible through metadata search. We implemented a text\nretrieval engine for a large archaeological text collection ($\\sim 658$ Million\nwords). In archaeological IR, domain-specific entities such as locations, time\nperiods, and artefacts, play a central role. This motivated the development of\na named entity recognition (NER) model to annotate the full collection with\narchaeological named entities. In this paper, we present ArcheoBERTje, a BERT\nmodel pre-trained on Dutch archaeological texts. We compare the model's quality\nand output on a Named Entity Recognition task to a generic multilingual model\nand a generic Dutch model. We also investigate ensemble methods for combining\nmultiple BERT models, and combining the best BERT model with a domain thesaurus\nusing Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms\nboth the multilingual and Dutch model significantly with a smaller standard\ndeviation between runs, reaching an average F1 score of 0.735. The model also\noutperforms ensemble methods combining the three models. Combining ArcheoBERTje\npredictions and explicit domain knowledge from the thesaurus did not increase\nthe F1 score. We quantitatively and qualitatively analyse the differences\nbetween the vocabulary and output of the BERT models on the full collection and\nprovide some valuable insights in the effect of fine-tuning for specific\ndomains. Our results indicate that for a highly specific text domain such as\narchaeology, further pre-training on domain-specific data increases the model's\nquality on NER by a much larger margin than shown for other domains in the\nliterature, and that domain-specific pre-training makes the addition of domain\nknowledge from a thesaurus unnecessary.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 20:26:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Brandsen", "Alex", ""], ["Verberne", "Suzan", ""], ["Lambers", "Karsten", ""], ["Wansleeben", "Milco", ""]]}, {"id": "2106.07759", "submitter": "Vimal Manohar", "authors": "Vimal Manohar, Tatiana Likhomanenko, Qiantong Xu, Wei-Ning Hsu, Ronan\n  Collobert, Yatharth Saraf, Geoffrey Zweig, Abdelrahman Mohamed", "title": "Kaizen: Continuously improving teacher using Exponential Moving Average\n  for semi-supervised speech recognition", "comments": "4 figures, 7 pages; fixed author list going out of margin", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Kaizen framework that uses a continuously\nimproving teacher to generate pseudo-labels for semi-supervised training. The\nproposed approach uses a teacher model which is updated as the exponential\nmoving average of the student model parameters. This can be seen as a\ncontinuous version of the iterative pseudo-labeling approach for\nsemi-supervised training. It is applicable for different training criteria, and\nin this paper we demonstrate it for frame-level hybrid hidden Markov model -\ndeep neural network (HMM-DNN) models and sequence-level connectionist temporal\nclassification (CTC) based models. The proposed approach shows more than 10%\nword error rate (WER) reduction over standard teacher-student training and more\nthan 50\\% relative WER reduction over 10 hour supervised baseline when using\nlarge scale realistic unsupervised public videos in UK English and Italian\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 21:15:36 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Manohar", "Vimal", ""], ["Likhomanenko", "Tatiana", ""], ["Xu", "Qiantong", ""], ["Hsu", "Wei-Ning", ""], ["Collobert", "Ronan", ""], ["Saraf", "Yatharth", ""], ["Zweig", "Geoffrey", ""], ["Mohamed", "Abdelrahman", ""]]}, {"id": "2106.07794", "submitter": "Trang Tran", "authors": "Trang Tran and Mari Ostendorf", "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect\n  Transcripts", "comments": "Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores constituency parsing on automatically recognized\ntranscripts of conversational speech. The neural parser is based on a sentence\nencoder that leverages word vectors contextualized with prosodic features,\njointly learning prosodic feature extraction with parsing. We assess the\nutility of the prosody in parsing on imperfect transcripts, i.e. transcripts\nwith automatic speech recognition (ASR) errors, by applying the parser in an\nN-best reranking framework. In experiments on Switchboard, we obtain 13-15% of\nthe oracle N-best gain relative to parsing the 1-best ASR output, with\ninsignificant impact on word recognition error rate. Prosody provides a\nsignificant part of the gain, and analyses suggest that it leads to more\ngrammatical utterances via recovering function words.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 23:05:59 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Tran", "Trang", ""], ["Ostendorf", "Mari", ""]]}, {"id": "2106.07799", "submitter": "Olga V Patterson", "authors": "Hannah Eyre (1 and 2), Alec B Chapman (1 and 2), Kelly S Peterson (1\n  and 2), Jianlin Shi (2), Patrick R Alba (1 and 2), Makoto M Jones (1 and 2),\n  Tamara L Box (3), Scott L DuVall (1 and 2), Olga V Patterson (1 and 2) ((1)\n  VA Salt Lake City Health Care System, (2) University of Utah, Salt Lake City,\n  UT, USA, (3) Veterans Health Administration Office of Analytics and\n  Performance Integration)", "title": "Launching into clinical space with medspaCy: a new clinical text\n  processing toolkit in Python", "comments": "Accepted to AMIA Annual Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite impressive success of machine learning algorithms in clinical natural\nlanguage processing (cNLP), rule-based approaches still have a prominent role.\nIn this paper, we introduce medspaCy, an extensible, open-source cNLP library\nbased on spaCy framework that allows flexible integration of rule-based and\nmachine learning-based algorithms adapted to clinical text. MedspaCy includes a\nvariety of components that meet common cNLP needs such as context analysis and\nmapping to standard terminologies. By utilizing spaCy's clear and easy-to-use\nconventions, medspaCy enables development of custom pipelines that integrate\neasily with other spaCy-based modules. Our toolkit includes several core\ncomponents and facilitates rapid development of pipelines for clinical text.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 23:19:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Eyre", "Hannah", "", "1 and 2"], ["Chapman", "Alec B", "", "1 and 2"], ["Peterson", "Kelly S", "", "1\n  and 2"], ["Shi", "Jianlin", "", "1 and 2"], ["Alba", "Patrick R", "", "1 and 2"], ["Jones", "Makoto M", "", "1 and 2"], ["Box", "Tamara L", "", "1 and 2"], ["DuVall", "Scott L", "", "1 and 2"], ["Patterson", "Olga V", "", "1 and 2"]]}, {"id": "2106.07823", "submitter": "Mayank Singh", "authors": "Vivek Srivastava, Mayank Singh", "title": "Challenges and Considerations with Code-Mixed NLP for Multilingual\n  Societies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingualism refers to the high degree of proficiency in two or more\nlanguages in the written and oral communication modes. It often results in\nlanguage mixing, a.k.a. code-mixing, when a multilingual speaker switches\nbetween multiple languages in a single utterance of a text or speech. This\npaper discusses the current state of the NLP research, limitations, and\nforeseeable pitfalls in addressing five real-world applications for social good\ncrisis management, healthcare, political campaigning, fake news, and hate\nspeech for multilingual societies. We also propose futuristic datasets, models,\nand tools that can significantly advance the current research in multilingual\nNLP applications for the societal good. As a representative example, we\nconsider English-Hindi code-mixing but draw similar inferences for other\nlanguage pairs\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:53:55 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Srivastava", "Vivek", ""], ["Singh", "Mayank", ""]]}, {"id": "2106.07843", "submitter": "Jisi Zhang", "authors": "Jisi Zhang, Catalin Zorila, Rama Doddipatla, Jon Barker", "title": "Teacher-Student MixIT for Unsupervised and Semi-supervised Speech\n  Separation", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel semi-supervised learning framework for\nend-to-end speech separation. The proposed method first uses mixtures of\nunseparated sources and the mixture invariant training (MixIT) criterion to\ntrain a teacher model. The teacher model then estimates separated sources that\nare used to train a student model with standard permutation invariant training\n(PIT). The student model can be fine-tuned with supervised data, i.e., paired\nartificial mixtures and clean speech sources, and further improved via model\ndistillation. Experiments with single and multi channel mixtures show that the\nteacher-student training resolves the over-separation problem observed in the\noriginal MixIT method. Further, the semisupervised performance is comparable to\na fully-supervised separation system trained using ten times the amount of\nsupervised data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 02:26:42 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 08:25:29 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhang", "Jisi", ""], ["Zorila", "Catalin", ""], ["Doddipatla", "Rama", ""], ["Barker", "Jon", ""]]}, {"id": "2106.07847", "submitter": "Yujia Bao", "authors": "Yujia Bao, Shiyu Chang, Regina Barzilay", "title": "Learning Stable Classifiers by Transferring Unstable Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study transfer learning in the presence of spurious correlations. We\nexperimentally demonstrate that directly transferring the stable feature\nextractor learned on the source task may not eliminate these biases for the\ntarget task. However, we hypothesize that the unstable features in the source\ntask and those in the target task are directly related. By explicitly informing\nthe target classifier of the source task's unstable features, we can regularize\nthe biases in the target task. Specifically, we derive a representation that\nencodes the unstable features by contrasting different data environments in the\nsource task. On the target task, we cluster data from this representation, and\nachieve robustness by minimizing the worst-case risk across all clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 02:41:12 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bao", "Yujia", ""], ["Chang", "Shiyu", ""], ["Barzilay", "Regina", ""]]}, {"id": "2106.07857", "submitter": "Bin Li", "authors": "Bin Li, Bin Sun (Member, IEEE) and Shutao Li (Fellow, IEEE)", "title": "Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware\n  Fusion", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nbilateral personalized dialogue generation (BPDG) method with dynamic\npersona-aware fusion via multi-task transfer learning to generate responses\nconsistent with both personas. The proposed method aims to accomplish three\nlearning tasks: 1) an encoder is trained with dialogue utterances added with\ncorresponded personalized attributes and relative position (language model\ntask), 2) a dynamic persona-aware fusion module predicts the persona presence\nto adaptively fuse the contextual and bilateral personas encodings (persona\nprediction task) and 3) a decoder generates natural, fluent and personalized\nresponses (dialogue generation task). To make the generated responses more\npersonalized and bilateral persona-consistent, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted to select the final response\nfrom the generated candidates. The experimental results show that the proposed\nmethod outperforms several state-of-the-art methods in terms of both automatic\nand manual evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:21:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Li", "Bin", "", "Member, IEEE"], ["Sun", "Bin", "", "Member, IEEE"], ["Li", "Shutao", "", "Fellow, IEEE"]]}, {"id": "2106.07890", "submitter": "Tai-Danae Bradley", "authors": "Tai-Danae Bradley, John Terilla, Yiannis Vlassopoulos", "title": "An enriched category theory of language: from syntax to semantics", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a piece of text, the ability to generate a coherent extension of it\nimplies some sophistication, including a knowledge of grammar and semantics. In\nthis paper, we propose a mathematical framework for passing from probability\ndistributions on extensions of given texts to an enriched category containing\nsemantic information. Roughly speaking, we model probability distributions on\ntexts as a category enriched over the unit interval. Objects of this category\nare expressions in language and hom objects are conditional probabilities that\none expression is an extension of another. This category is syntactical: it\ndescribes what goes with what. We then pass to the enriched category of unit\ninterval-valued copresheaves on this syntactical category to find semantic\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 05:40:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bradley", "Tai-Danae", ""], ["Terilla", "John", ""], ["Vlassopoulos", "Yiannis", ""]]}, {"id": "2106.07922", "submitter": "Zhuohao Chen", "authors": "Zhuohao Chen, Nikolaos Flemotomos, Karan Singla, Torrey A. Creed,\n  David C. Atkins, Shrikanth Narayanan", "title": "An Automated Quality Evaluation Framework of Psychotherapy Conversations\n  with Local Quality Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational approaches for assessing the quality of conversation-based\npsychotherapy, such as Cognitive Behavioral Therapy (CBT) and Motivational\nInterviewing (MI), have been developed recently to support quality assurance\nand clinical training. However, due to the long session lengths and limited\nmodeling resources, computational methods largely rely on frequency-based\nlexical features or distribution of dialogue acts. In this work, we propose a\nhierarchical framework to automatically evaluate the quality of a CBT\ninteraction. We divide each psychotherapy session into conversation segments\nand input those into a BERT-based model to produce segment embeddings. We first\nfine-tune BERT for predicting segment-level (local) quality scores and then use\nsegment embeddings as lower-level input to a Bidirectional LSTM-based neural\nnetwork to predict session-level (global) quality estimates. In particular, the\nsegment-level quality scores are initialized with the session-level scores and\nwe model the global quality as a function of the local quality scores to\nachieve the accurate segment-level quality estimates. These estimated\nsegment-level scores benefit theBERT fine-tuning and in learning better segment\nembeddings. We evaluate the proposed framework on data drawn from real-world\nCBT clinical session recordings to predict multiple session-level behavior\ncodes. The results indicate that our approach leads to improved evaluation\naccuracy for most codes in both regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:18:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Zhuohao", ""], ["Flemotomos", "Nikolaos", ""], ["Singla", "Karan", ""], ["Creed", "Torrey A.", ""], ["Atkins", "David C.", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2106.07930", "submitter": "Liwei Wu", "authors": "Liwei Wu, Shanbo Cheng, Mingxuan Wang, Lei Li", "title": "Language Tags Matter for Zero-Shot Neural Machine Translation", "comments": "7 pages, 3 figures, Accepted by the Findings of ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual Neural Machine Translation (MNMT) has aroused widespread\ninterest due to its efficiency. An exciting advantage of MNMT models is that\nthey could also translate between unsupervised (zero-shot) language directions.\nLanguage tag (LT) strategies are often adopted to indicate the translation\ndirections in MNMT. In this paper, we demonstrate that the LTs are not only\nindicators for translation directions but also crucial to zero-shot translation\nqualities. Unfortunately, previous work tends to ignore the importance of LT\nstrategies. We demonstrate that a proper LT strategy could enhance the\nconsistency of semantic representations and alleviate the off-target issue in\nzero-shot directions. Experimental results show that by ignoring the source\nlanguage tag (SLT) and adding the target language tag (TLT) to the encoder, the\nzero-shot translations could achieve a +8 BLEU score difference over other LT\nstrategies in IWSLT17, Europarl, TED talks translation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:32:36 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wu", "Liwei", ""], ["Cheng", "Shanbo", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2106.07935", "submitter": "Joseph Marvin Imperial", "authors": "Joseph Marvin Imperial", "title": "Knowledge-Rich BERT Embeddings for Readability Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic readability assessment (ARA) is the task of evaluating the level of\nease or difficulty of text documents for a target audience. For researchers,\none of the many open problems in the field is to make such models trained for\nthe task show efficacy even for low-resource languages. In this study, we\npropose an alternative way of utilizing the information-rich embeddings of BERT\nmodels through a joint-learning method combined with handcrafted linguistic\nfeatures for readability assessment. Results show that the proposed method\noutperforms classical approaches in readability assessment using English and\nFilipino datasets, and obtaining as high as 12.4% increase in F1 performance.\nWe also show that the knowledge encoded in BERT embeddings can be used as a\nsubstitute feature set for low-resource languages like Filipino with limited\nsemantic and syntactic NLP tools to explicitly extract feature values for the\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:37:48 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Imperial", "Joseph Marvin", ""]]}, {"id": "2106.07936", "submitter": "Maria Heitmeier", "authors": "Maria Heitmeier, Yu-Ying Chuang, R. Harald Baayen", "title": "Modeling morphology with Linear Discriminative Learning: considerations\n  and design choices", "comments": "38 pages, 5 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study addresses a series of methodological questions that arise when\nmodeling inflectional morphology with Linear Discriminative Learning. Taking\nthe semi-productive German noun system as example, we illustrate how decisions\nmade about the representation of form and meaning influence model performance.\nWe clarify that for modeling frequency effects in learning, it is essential to\nmake use of incremental learning rather than the endstate of learning. We also\ndiscuss how the model can be set up to approximate the learning of inflected\nwords in context. In addition, we illustrate how in this approach the wug task\ncan be modeled in considerable detail. In general, the model provides an\nexcellent memory for known words, but appropriately shows more limited\nperformance for unseen data, in line with the semi-productivity of German noun\ninflection and generalization performance of native German speakers.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:37:52 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Heitmeier", "Maria", ""], ["Chuang", "Yu-Ying", ""], ["Baayen", "R. Harald", ""]]}, {"id": "2106.07947", "submitter": "Yixiao Wang", "authors": "Yixiao Wang, Zied Bouraoui, Luis Espinosa Anke, Steven Schockaert", "title": "Deriving Word Vectors from Contextualized Language Models using\n  Topic-Aware Mention Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the long-standing challenges in lexical semantics consists in learning\nrepresentations of words which reflect their semantic properties. The\nremarkable success of word embeddings for this purpose suggests that\nhigh-quality representations can be obtained by summarizing the sentence\ncontexts of word mentions. In this paper, we propose a method for learning word\nrepresentations that follows this basic strategy, but differs from standard\nword embeddings in two important ways. First, we take advantage of\ncontextualized language models (CLMs) rather than bags of word vectors to\nencode contexts. Second, rather than learning a word vector directly, we use a\ntopic model to partition the contexts in which words appear, and then learn\ndifferent topic-specific vectors for each word. Finally, we use a task-specific\nsupervision signal to make a soft selection of the resulting vectors. We show\nthat this simple strategy leads to high-quality word vectors, which are more\npredictive of semantic properties than word embeddings and existing CLM-based\nstrategies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:02:42 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Yixiao", ""], ["Bouraoui", "Zied", ""], ["Anke", "Luis Espinosa", ""], ["Schockaert", "Steven", ""]]}, {"id": "2106.07967", "submitter": "Jan Philip Wahle", "authors": "Jan Philip Wahle and Terry Ruas and Norman Meuschke and Bela Gipp", "title": "Incorporating Word Sense Disambiguation in Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present two supervised (pre-)training methods to incorporate gloss\ndefinitions from lexical resources into neural language models (LMs). The\ntraining improves our models' performance for Word Sense Disambiguation (WSD)\nbut also benefits general language understanding tasks while adding almost no\nparameters. We evaluate our techniques with seven different neural LMs and find\nthat XLNet is more suitable for WSD than BERT. Our best-performing methods\nexceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1\nand increase BERT's performance on the GLUE benchmark by 1.1% on average.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:44:08 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wahle", "Jan Philip", ""], ["Ruas", "Terry", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2106.07999", "submitter": "Shohei Tanaka", "authors": "Shohei Tanaka, Koichiro Yoshino, Katsuhito Sudoh, Satoshi Nakamura", "title": "ARTA: Collection and Classification of Ambiguous Requests and Thoughtful\n  Actions", "comments": "Accepted by The 22nd Annual Meeting of the Special Interest Group on\n  Discourse and Dialogue (SIGDIAL2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-assisting systems such as dialogue systems must take thoughtful,\nappropriate actions not only for clear and unambiguous user requests, but also\nfor ambiguous user requests, even if the users themselves are not aware of\ntheir potential requirements. To construct such a dialogue agent, we collected\na corpus and developed a model that classifies ambiguous user requests into\ncorresponding system actions. In order to collect a high-quality corpus, we\nasked workers to input antecedent user requests whose pre-defined actions could\nbe regarded as thoughtful. Although multiple actions could be identified as\nthoughtful for a single user request, annotating all combinations of user\nrequests and system actions is impractical. For this reason, we fully annotated\nonly the test data and left the annotation of the training data incomplete. In\norder to train the classification model on such training data, we applied the\npositive/unlabeled (PU) learning method, which assumes that only a part of the\ndata is labeled with positive examples. The experimental results show that the\nPU learning method achieved better performance than the general\npositive/negative (PN) learning method to classify thoughtful actions given an\nambiguous user request.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:28:39 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Tanaka", "Shohei", ""], ["Yoshino", "Koichiro", ""], ["Sudoh", "Katsuhito", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "2106.08004", "submitter": "Runqiu Xiao", "authors": "Runqiu Xiao", "title": "Adaptive Margin Circle Loss for Speaker Verification", "comments": "Accepted by Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-Neural-Network (DNN) based speaker verification sys-tems use the angular\nsoftmax loss with margin penalties toenhance the intra-class compactness of\nspeaker embeddings,which achieved remarkable performance. In this paper, we\npro-pose a novel angular loss function called adaptive margin cir-cle loss for\nspeaker verification. The stage-based margin andchunk-based margin are applied\nto improve the angular discrim-ination of circle loss on the training set. The\nanalysis on gradi-ents shows that, compared with the previous angular loss\nlikeAdditive Margin Softmax(Am-Softmax), circle loss has flexi-ble optimization\nand definite convergence status. Experimentsare carried out on the Voxceleb and\nSITW. By applying adap-tive margin circle loss, our best system achieves\n1.31%EER onVoxceleb1 and 2.13% on SITW core-core.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:35:59 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Xiao", "Runqiu", ""]]}, {"id": "2106.08007", "submitter": "Masaru Isonuma", "authors": "Masaru Isonuma, Junichiro Mori, Danushka Bollegala, Ichiro Sakata", "title": "Unsupervised Abstractive Opinion Summarization by Generating Sentences\n  with Tree-Structured Topic Guidance", "comments": "accepted to TACL, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel unsupervised abstractive summarization method for\nopinionated texts. While the basic variational autoencoder-based models assume\na unimodal Gaussian prior for the latent code of sentences, we alternate it\nwith a recursive Gaussian mixture, where each mixture component corresponds to\nthe latent code of a topic sentence and is mixed by a tree-structured topic\ndistribution. By decoding each Gaussian component, we generate sentences with\ntree-structured topic guidance, where the root sentence conveys generic\ncontent, and the leaf sentences describe specific topics. Experimental results\ndemonstrate that the generated topic sentences are appropriate as a summary of\nopinionated texts, which are more informative and cover more input contents\nthan those generated by the recent unsupervised summarization model\n(Bra\\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of\nlatent Gaussians represents the granularity of sentences, analogous to Gaussian\nword embedding (Vilnis and McCallum, 2015).\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:37:04 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Isonuma", "Masaru", ""], ["Mori", "Junichiro", ""], ["Bollegala", "Danushka", ""], ["Sakata", "Ichiro", ""]]}, {"id": "2106.08037", "submitter": "Valentina Pyatkin", "authors": "Valentina Pyatkin, Shoval Sadde, Aynat Rubinstein, Paul Portner, Reut\n  Tsarfaty", "title": "The Possible, the Plausible, and the Desirable: Event-Based Modality\n  Detection for Language Processing", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modality is the linguistic ability to describe events with added information\nsuch as how desirable, plausible, or feasible they are. Modality is important\nfor many NLP downstream tasks such as the detection of hedging, uncertainty,\nspeculation, and more. Previous studies that address modality detection in NLP\noften restrict modal expressions to a closed syntactic class, and the modal\nsense labels are vastly different across different studies, lacking an accepted\nstandard. Furthermore, these senses are often analyzed independently of the\nevents that they modify. This work builds on the theoretical foundations of the\nGeorgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to\npropose an event-based modality detection task where modal expressions can be\nwords of any syntactic class and sense labels are drawn from a comprehensive\ntaxonomy which harmonizes the modal concepts contributed by the different\nstudies. We present experiments on the GME corpus aiming to detect and classify\nfine-grained modal concepts and associate them with their modified events. We\nshow that detecting and classifying modal expressions is not only feasible, but\nalso improves the detection of modal events in their own right.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:47:57 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Pyatkin", "Valentina", ""], ["Sadde", "Shoval", ""], ["Rubinstein", "Aynat", ""], ["Portner", "Paul", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2106.08043", "submitter": "Arun Maiya", "authors": "Arun S. Maiya", "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:57:44 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 21:01:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Maiya", "Arun S.", ""]]}, {"id": "2106.08062", "submitter": "Soyoung Yoon", "authors": "Soyoung Yoon, Gyuwan Kim, Kyumin Park", "title": "SSMix: Saliency-Based Span Mixup for Text Classification", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation with mixup has shown to be effective on various computer\nvision tasks. Despite its great success, there has been a hurdle to apply mixup\nto NLP tasks since text consists of discrete tokens with variable length. In\nthis work, we propose SSMix, a novel mixup method where the operation is\nperformed on input text rather than on hidden vectors like previous approaches.\nSSMix synthesizes a sentence while preserving the locality of two original\ntexts by span-based mixing and keeping more tokens related to the prediction\nrelying on saliency information. With extensive experiments, we empirically\nvalidate that our method outperforms hidden-level mixup methods on a wide range\nof text classification benchmarks, including textual entailment, sentiment\nclassification, and question-type classification. Our code is available at\nhttps://github.com/clovaai/ssmix.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:40:23 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yoon", "Soyoung", ""], ["Kim", "Gyuwan", ""], ["Park", "Kyumin", ""]]}, {"id": "2106.08087", "submitter": "Ningyu Zhang", "authors": "Mosha Chen, Chuanqi Tan, Zhen Bi, Xiaozhuan Liang, Lei Li, Ningyu\n  Zhang, Xin Shang, Kangping Yin, Jian Xu, Fei Huang, Luo Si, Yuan Ni, Guotong\n  Xie, Zhifang Sui, Baobao Chang, Hui Zong, Zheng Yuan, Linfeng Li, Jun Yan,\n  Hongying Zan, Kunli Zhang, Buzhou Tang, Qingcai Chen", "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:25:30 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 09:51:13 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 12:25:56 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Mosha", ""], ["Tan", "Chuanqi", ""], ["Bi", "Zhen", ""], ["Liang", "Xiaozhuan", ""], ["Li", "Lei", ""], ["Zhang", "Ningyu", ""], ["Shang", "Xin", ""], ["Yin", "Kangping", ""], ["Xu", "Jian", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""], ["Ni", "Yuan", ""], ["Xie", "Guotong", ""], ["Sui", "Zhifang", ""], ["Chang", "Baobao", ""], ["Zong", "Hui", ""], ["Yuan", "Zheng", ""], ["Li", "Linfeng", ""], ["Yan", "Jun", ""], ["Zan", "Hongying", ""], ["Zhang", "Kunli", ""], ["Tang", "Buzhou", ""], ["Chen", "Qingcai", ""]]}, {"id": "2106.08117", "submitter": "Dongsheng Wang", "authors": "Dongsheng Wang", "title": "Semantic Representation and Inference for NLP", "comments": "PhD thesis, the University of Copenhagen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Semantic representation and inference is essential for Natural Language\nProcessing (NLP). The state of the art for semantic representation and\ninference is deep learning, and particularly Recurrent Neural Networks (RNNs),\nConvolutional Neural Networks (CNNs), and transformer Self-Attention models.\nThis thesis investigates the use of deep learning for novel semantic\nrepresentation and inference, and makes contributions in the following three\nareas: creating training data, improving semantic representations and extending\ninference learning. In terms of creating training data, we contribute the\nlargest publicly available dataset of real-life factual claims for the purpose\nof automatic claim verification (MultiFC), and we present a novel inference\nmodel composed of multi-scale CNNs with different kernel sizes that learn from\nexternal sources to infer fact checking labels. In terms of improving semantic\nrepresentations, we contribute a novel model that captures non-compositional\nsemantic indicators. By definition, the meaning of a non-compositional phrase\ncannot be inferred from the individual meanings of its composing words (e.g.,\nhot dog). Motivated by this, we operationalize the compositionality of a phrase\ncontextually by enriching the phrase representation with external word\nembeddings and knowledge graphs. Finally, in terms of inference learning, we\npropose a series of novel deep learning architectures that improve inference by\nusing syntactic dependencies, by ensembling role guided attention heads,\nincorporating gating layers, and concatenating multiple heads in novel and\neffective ways. This thesis consists of seven publications (five published and\ntwo under review).\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:22:48 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Dongsheng", ""]]}, {"id": "2106.08122", "submitter": "Chenze Shao", "authors": "Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, Jie Zhou", "title": "Sequence-Level Training for Non-Autoregressive Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Neural Machine Translation (NMT) has achieved notable\nresults in various translation tasks. However, the word-by-word generation\nmanner determined by the autoregressive mechanism leads to high translation\nlatency of the NMT and restricts its low-latency applications.\nNon-Autoregressive Neural Machine Translation (NAT) removes the autoregressive\nmechanism and achieves significant decoding speedup through generating target\nwords independently and simultaneously. Nevertheless, NAT still takes the\nword-level cross-entropy loss as the training objective, which is not optimal\nbecause the output of NAT cannot be properly evaluated due to the multimodality\nproblem. In this paper, we propose using sequence-level training objectives to\ntrain NAT models, which evaluate the NAT outputs as a whole and correlates well\nwith the real translation quality. Firstly, we propose training NAT models to\noptimize sequence-level evaluation metrics (e.g., BLEU) based on several novel\nreinforcement algorithms customized for NAT, which outperforms the conventional\nmethod by reducing the variance of gradient estimation. Secondly, we introduce\na novel training objective for NAT models, which aims to minimize the\nBag-of-Ngrams (BoN) difference between the model output and the reference\nsentence. The BoN training objective is differentiable and can be calculated\nefficiently without doing any approximations. Finally, we apply a three-stage\ntraining strategy to combine these two methods to train the NAT model. We\nvalidate our approach on four translation tasks (WMT14 En$\\leftrightarrow$De,\nWMT16 En$\\leftrightarrow$Ro), which shows that our approach largely outperforms\nNAT baselines and achieves remarkable performance on all translation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:30:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Shao", "Chenze", ""], ["Feng", "Yang", ""], ["Zhang", "Jinchao", ""], ["Meng", "Fandong", ""], ["Zhou", "Jie", ""]]}, {"id": "2106.08126", "submitter": "Oscar Koller", "authors": "Yuriy Arabskyy, Aashish Agarwal, Subhadeep Dey, Oscar Koller", "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to\n  Standard German Text: Microsoft's Submission to SwissText 2021", "comments": "to be published in SwissText 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the winning approach in the Shared Task 3 at SwissText\n2021 on Swiss German Speech to Standard German Text, a public competition on\ndialect recognition and translation. Swiss German refers to the multitude of\nAlemannic dialects spoken in the German-speaking parts of Switzerland. Swiss\nGerman differs significantly from standard German in pronunciation, word\ninventory and grammar. It is mostly incomprehensible to native German speakers.\nMoreover, it lacks a standardized written script. To solve the challenging\ntask, we propose a hybrid automatic speech recognition system with a lexicon\nthat incorporates translations, a 1st pass language model that deals with Swiss\nGerman particularities, a transfer-learned acoustic model and a strong neural\nlanguage model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a\nblind conversational test set and outperforms the second best competitor by a\n12% relative margin.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:34:02 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 10:58:23 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Arabskyy", "Yuriy", ""], ["Agarwal", "Aashish", ""], ["Dey", "Subhadeep", ""], ["Koller", "Oscar", ""]]}, {"id": "2106.08159", "submitter": "Stefan Gr\\\"unewald", "authors": "Stefan Gr\\\"unewald", "title": "Maximum Spanning Trees Are Invariant to Temperature Scaling in\n  Graph-based Dependency Parsing", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern graph-based syntactic dependency parsers operate by predicting, for\neach token within a sentence, a probability distribution over its possible\nsyntactic heads (i.e., all other tokens) and then extracting a maximum spanning\ntree from the resulting log-probabilities. Nowadays, virtually all such parsers\nutilize deep neural networks and may thus be susceptible to miscalibration (in\nparticular, overconfident predictions). In this paper, we prove that\ntemperature scaling, a popular technique for post-hoc calibration of neural\nnetworks, cannot change the output of the aforementioned procedure. We conclude\nthat other techniques are needed to tackle miscalibration in graph-based\ndependency parsers in a way that improves parsing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:57:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gr\u00fcnewald", "Stefan", ""]]}, {"id": "2106.08181", "submitter": "Mohammadreza Banaei", "authors": "Klaudia Ba{\\l}azy, Mohammadreza Banaei, R\\'emi Lebret, Jacek Tabor,\n  Karl Aberer", "title": "Direction is what you need: Improving Word Embedding Compression in\n  Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of Transformer-based models in natural language processing (NLP)\nhas led to great success using a massive number of parameters. However, due to\ndeployment constraints in edge devices, there has been a rising interest in the\ncompression of these models to improve their inference time and memory\nfootprint. This paper presents a novel loss objective to compress token\nembeddings in the Transformer-based models by leveraging an AutoEncoder\narchitecture. More specifically, we emphasize the importance of the direction\nof compressed embeddings with respect to original uncompressed embeddings. The\nproposed method is task-agnostic and does not require further language modeling\npre-training. Our method significantly outperforms the commonly used SVD-based\nmatrix-factorization approach in terms of initial language model Perplexity.\nMoreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several\ndownstream tasks from the GLUE benchmark, where we also outperform the baseline\nin most scenarios. Our code is public.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:28:00 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ba\u0142azy", "Klaudia", ""], ["Banaei", "Mohammadreza", ""], ["Lebret", "R\u00e9mi", ""], ["Tabor", "Jacek", ""], ["Aberer", "Karl", ""]]}, {"id": "2106.08190", "submitter": "Robin Jia", "authors": "Robin Jia, Mike Lewis, Luke Zettlemoyer", "title": "Question Answering Infused Pre-training of General-Purpose\n  Contextualized Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a pre-training objective based on question answering (QA)\nfor learning general-purpose contextual representations, motivated by the\nintuition that the representation of a phrase in a passage should encode all\nquestions that the phrase can answer in context. We accomplish this goal by\ntraining a bi-encoder QA model, which independently encodes passages and\nquestions, to match the predictions of a more accurate cross-encoder model on\n80 million synthesized QA pairs. By encoding QA-relevant information, the\nbi-encoder's token-level representations are useful for non-QA downstream tasks\nwithout extensive (or in some cases, any) fine-tuning. We show large\nimprovements over both RoBERTa-large and previous state-of-the-art results on\nzero-shot and few-shot paraphrase detection on four datasets, few-shot named\nentity recognition on two datasets, and zero-shot sentiment analysis on three\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:45:15 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Jia", "Robin", ""], ["Lewis", "Mike", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2106.08207", "submitter": "Venkatesh Ravichandran", "authors": "Long Chen, Venkatesh Ravichandran, Andreas Stolcke", "title": "Graph-based Label Propagation for Semi-Supervised Speaker Identification", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker identification in the household scenario (e.g., for smart speakers)\nis typically based on only a few enrollment utterances but a much larger set of\nunlabeled data, suggesting semisupervised learning to improve speaker profiles.\nWe propose a graph-based semi-supervised learning approach for speaker\nidentification in the household scenario, to leverage the unlabeled speech\nsamples. In contrast to most of the works in speaker recognition that focus on\nspeaker-discriminative embeddings, this work focuses on speaker label inference\n(scoring). Given a pre-trained embedding extractor, graph-based learning allows\nus to integrate information about both labeled and unlabeled utterances.\nConsidering each utterance as a graph node, we represent pairwise utterance\nsimilarity scores as edge weights. Graphs are constructed per household, and\nspeaker identities are propagated to unlabeled nodes to optimize a global\nconsistency criterion. We show in experiments on the VoxCeleb dataset that this\napproach makes effective use of unlabeled data and improves speaker\nidentification accuracy compared to two state-of-the-art scoring methods as\nwell as their semi-supervised variants based on pseudo-labels.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:10:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Long", ""], ["Ravichandran", "Venkatesh", ""], ["Stolcke", "Andreas", ""]]}, {"id": "2106.08226", "submitter": "Li Dong", "authors": "Bo Zheng, Li Dong, Shaohan Huang, Wenhui Wang, Zewen Chi, Saksham\n  Singhal, Wanxiang Che, Ting Liu, Xia Song, Furu Wei", "title": "Consistency Regularization for Cross-Lingual Fine-Tuning", "comments": "ACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning pre-trained cross-lingual language models can transfer\ntask-specific supervision from one language to the others. In this work, we\npropose to improve cross-lingual fine-tuning with consistency regularization.\nSpecifically, we use example consistency regularization to penalize the\nprediction sensitivity to four types of data augmentations, i.e., subword\nsampling, Gaussian noise, code-switch substitution, and machine translation. In\naddition, we employ model consistency to regularize the models trained with two\naugmented versions of the same training set. Experimental results on the XTREME\nbenchmark show that our method significantly improves cross-lingual fine-tuning\nacross various tasks, including text classification, question answering, and\nsequence labeling.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:35:44 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zheng", "Bo", ""], ["Dong", "Li", ""], ["Huang", "Shaohan", ""], ["Wang", "Wenhui", ""], ["Chi", "Zewen", ""], ["Singhal", "Saksham", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Song", "Xia", ""], ["Wei", "Furu", ""]]}, {"id": "2106.08235", "submitter": "Zhaozhuo Xu", "authors": "Zhaozhuo Xu, Minghao Yan, Junyan Zhang, Anshumali Shrivastava", "title": "PairConnect: A Compute-Efficient MLP Alternative to Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have demonstrated superior performance in natural language\nprocessing. The dot product self-attention in Transformer allows us to model\ninteractions between words. However, this modeling comes with significant\ncomputational overhead. In this work, we revisit the memory-compute trade-off\nassociated with Transformer, particularly multi-head attention, and show a\nmemory-heavy but significantly more compute-efficient alternative to\nTransformer. Our proposal, denoted as PairConnect, a multilayer perceptron\n(MLP), models the pairwise interaction between words by explicit pairwise word\nembeddings. As a result, PairConnect substitutes self dot product with a simple\nembedding lookup. We show mathematically that despite being an MLP, our\ncompute-efficient PairConnect is strictly more expressive than Transformer. Our\nexperiment on language modeling tasks suggests that PairConnect could achieve\ncomparable results with Transformer while reducing the computational cost\nassociated with inference significantly.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:39:45 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Xu", "Zhaozhuo", ""], ["Yan", "Minghao", ""], ["Zhang", "Junyan", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2106.08252", "submitter": "Nima Ebadi", "authors": "Nima Ebadi and Peyman Najafirad", "title": "Interpretable Self-supervised Multi-task Learning for COVID-19\n  Information Retrieval and Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapidly evolving literature of COVID-19 related articles makes it\nchallenging for NLP models to be effectively trained for information retrieval\nand extraction with the corresponding labeled data that follows the current\ndistribution of the pandemic. On the other hand, due to the uncertainty of the\nsituation, human experts' supervision would always be required to double check\nthe decision making of these models highlighting the importance of\ninterpretability. In the light of these challenges, this study proposes an\ninterpretable self-supervised multi-task learning model to jointly and\neffectively tackle the tasks of information retrieval (IR) and extraction (IE)\nduring the current emergency health crisis situation. Our results show that our\nmodel effectively leverage the multi-task and self-supervised learning to\nimprove generalization, data efficiency and robustness to the ongoing dataset\nshift problem. Our model outperforms baselines in IE and IR tasks, respectively\nby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In\nIE the zero- and few-shot learning performances are on average 0.32 and 0.19\nmicro-f score higher than those of the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:01:44 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ebadi", "Nima", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2106.08294", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov, Lidia Pivovarova", "title": "Three-part diachronic semantic change dataset for Russian", "comments": "Accepted to the 2nd International Workshop on Computational\n  Approaches to Historical Language Change 2021 (LChange'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a manually annotated lexical semantic change dataset for Russian:\nRuShiftEval. Its novelty is ensured by a single set of target words annotated\nfor their diachronic semantic shifts across three time periods, while the\nprevious work either used only two time periods, or different sets of target\nwords. The paper describes the composition and annotation procedure for the\ndataset. In addition, it is shown how the ternary nature of RuShiftEval allows\nto trace specific diachronic trajectories: `changed at a particular time period\nand stable afterwards' or `was changing throughout all time periods'. Based on\nthe analysis of the submissions to the recent shared task on semantic change\ndetection for Russian, we argue that correctly identifying such trajectories\ncan be an interesting sub-task itself.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:12:25 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Pivovarova", "Lidia", ""]]}, {"id": "2106.08298", "submitter": "Jason R.C. Nurse Dr", "authors": "Suraj Sharma and Joseph Brennan and Jason R. C. Nurse", "title": "StockBabble: A Conversational Financial Agent to support Stock Market\n  Investors", "comments": "CUI 2021 - 3rd Conference on Conversational User Interfaces", "journal-ref": null, "doi": "10.1145/3469595.3469620", "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce StockBabble, a conversational agent designed to support\nunderstanding and engagement with the stock market. StockBabble's value and\nnovelty is in its ability to empower retail investors -- many of which may be\nnew to investing -- and supplement their informational needs using a\nuser-friendly agent. Users have the ability to query information on companies\nto retrieve a general and financial overview of a stock, including accessing\nthe latest news and trading recommendations. They can also request charts which\ncontain live prices and technical investment indicators, and add shares to a\npersonal portfolio to allow performance monitoring over time. To evaluate our\nagent's potential, we conducted a user study with 15 participants. In total,\n73% (11/15) of respondents said that they felt more confident in investing\nafter using StockBabble, and all 15 would consider recommending it to others.\nThese results are encouraging and suggest a wider appeal for such agents.\nMoreover, we believe this research can help to inform the design and\ndevelopment of future intelligent, financial personal assistants.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:19:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sharma", "Suraj", ""], ["Brennan", "Joseph", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2106.08364", "submitter": "Bodhisattwa Prasad Majumder", "authors": "Bodhisattwa Prasad Majumder, Taylor Berg-Kirkpatrick, Julian McAuley,\n  Harsh Jhamtani", "title": "Unsupervised Enrichment of Persona-grounded Dialog with Background\n  Stories", "comments": "Accepted at ACL 2021 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans often refer to personal narratives, life experiences, and events to\nmake a conversation more engaging and rich. While persona-grounded dialog\nmodels are able to generate responses that follow a given persona, they often\nmiss out on stating detailed experiences or events related to a persona, often\nleaving conversations shallow and dull. In this work, we equip dialog models\nwith 'background stories' related to a persona by leveraging fictional\nnarratives from existing story datasets (e.g. ROCStories). Since current dialog\ndatasets do not contain such narratives as responses, we perform an\nunsupervised adaptation of a retrieved story for generating a dialog response\nusing a gradient-based rewriting technique. Our proposed method encourages the\ngenerated response to be fluent (i.e., highly likely) with the dialog history,\nminimally different from the retrieved story to preserve event ordering and\nconsistent with the original persona. We demonstrate that our method can\ngenerate responses that are more diverse, and are rated more engaging and\nhuman-like by human evaluators, compared to outputs from existing dialog\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:20:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Majumder", "Bodhisattwa Prasad", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["McAuley", "Julian", ""], ["Jhamtani", "Harsh", ""]]}, {"id": "2106.08367", "submitter": "Joe O'Connor", "authors": "Joe O'Connor and Jacob Andreas", "title": "What Context Features Can Transformer Language Models Use?", "comments": "14 pages, 7 figures, to be published at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based language models benefit from conditioning on contexts of\nhundreds to thousands of previous tokens. What aspects of these contexts\ncontribute to accurate model prediction? We describe a series of experiments\nthat measure usable information by selectively ablating lexical and structural\ninformation in transformer language models trained on English Wikipedia. In\nboth mid- and long-range contexts, we find that several extremely destructive\ncontext manipulations -- including shuffling word order within sentences and\ndeleting all words other than nouns -- remove less than 15% of the usable\ninformation. Our results suggest that long contexts, but not their detailed\nsyntactic and propositional content, are important for the low perplexity of\ncurrent transformer language models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:38:57 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["O'Connor", "Joe", ""], ["Andreas", "Jacob", ""]]}, {"id": "2106.08415", "submitter": "Kevin Moran Ph.D.", "authors": "Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios\n  Anastasopoulos, Kevin Moran", "title": "Code to Comment Translation: A Comparative Study on Model Effectiveness\n  & Errors", "comments": "Accepted to the 2021 NLP4Prog Workshop co-located with The Joint\n  Conference of the 59th Annual Meeting of the Association for Computational\n  Linguistics and the 11th International Joint Conference on Natural Language\n  Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated source code summarization is a popular software engineering\nresearch topic wherein machine translation models are employed to \"translate\"\ncode snippets into relevant natural language descriptions. Most evaluations of\nsuch models are conducted using automatic reference-based metrics. However,\ngiven the relatively large semantic gap between programming languages and\nnatural language, we argue that this line of research would benefit from a\nqualitative investigation into the various error modes of current\nstate-of-the-art models. Therefore, in this work, we perform both a\nquantitative and qualitative comparison of three recently proposed source code\nsummarization models. In our quantitative evaluation, we compare the models\nbased on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,\nand in our qualitative evaluation, we perform a manual open-coding of the most\ncommon errors committed by the models when compared to ground truth captions.\nOur investigation reveals new insights into the relationship between\nmetric-based performance and model prediction errors grounded in an empirically\nderived error taxonomy that can be used to drive future research efforts\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:13:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Mahmud", "Junayed", ""], ["Faisal", "Fahim", ""], ["Arnob", "Raihan Islam", ""], ["Anastasopoulos", "Antonios", ""], ["Moran", "Kevin", ""]]}, {"id": "2106.08427", "submitter": "Bence Halpern", "authors": "Marc Illa, Bence Mark Halpern, Rob van Son, Laureano Moro-Velazquez,\n  Odette Scharenborg", "title": "Pathological voice adaptation with autoencoder-based voice conversion", "comments": "6 pages, 3 figures. Accepted to the 11th ISCA Speech Synthesis\n  Workshop (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new approach to pathological speech synthesis.\nInstead of using healthy speech as a source, we customise an existing\npathological speech sample to a new speaker's voice characteristics. This\napproach alleviates the evaluation problem one normally has when converting\ntypical speech to pathological speech, as in our approach, the voice conversion\n(VC) model does not need to be optimised for speech degradation but only for\nthe speaker change. This change in the optimisation ensures that any\ndegradation found in naturalness is due to the conversion process and not due\nto the model exaggerating characteristics of a speech pathology. To show a\nproof of concept of this method, we convert dysarthric speech using the\nUASpeech database and an autoencoder-based VC technique. Subjective evaluation\nresults show reasonable naturalness for high intelligibility dysarthric\nspeakers, though lower intelligibility seems to introduce a marginal\ndegradation in naturalness scores for mid and low intelligibility speakers\ncompared to ground truth. Conversion of speaker characteristics for low and\nhigh intelligibility speakers is successful, but not for mid. Whether the\ndifferences in the results for the different intelligibility levels is due to\nthe intelligibility levels or due to the speakers needs to be further\ninvestigated.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:38:10 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Illa", "Marc", ""], ["Halpern", "Bence Mark", ""], ["van Son", "Rob", ""], ["Moro-Velazquez", "Laureano", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2106.08468", "submitter": "Rohola Zandie", "authors": "Rohola Zandie, Mohammad H. Mahoor, Julia Madsen, and Eshrat S. Emamian", "title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces RyanSpeech, a new speech corpus for research on\nautomated text-to-speech (TTS) systems. Publicly available TTS corpora are\noften noisy, recorded with multiple speakers, or lack quality male speech data.\nIn order to meet the need for a high quality, publicly available male speech\ncorpus within the field of speech recognition, we have designed and created\nRyanSpeech which contains textual materials from real-world conversational\nsettings. These materials contain over 10 hours of a professional male voice\nactor's speech recorded at 44.1 kHz. This corpus's design and pipeline make\nRyanSpeech ideal for developing TTS systems in real-world applications. To\nprovide a baseline for future research, protocols, and benchmarks, we trained 4\nstate-of-the-art speech models and a vocoder on RyanSpeech. The results show\n3.36 in mean opinion scores (MOS) in our best model. We have made both the\ncorpus and trained models for public use.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:24:38 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zandie", "Rohola", ""], ["Mahoor", "Mohammad H.", ""], ["Madsen", "Julia", ""], ["Emamian", "Eshrat S.", ""]]}, {"id": "2106.08484", "submitter": "Alexandros Papangelis", "authors": "Alexandros Papangelis and Karthik Gopalakrishnan and Aishwarya\n  Padmakumar and Seokhwan Kim and Gokhan Tur and Dilek Hakkani-Tur", "title": "Generative Conversational Networks", "comments": "SIGDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 23:19:37 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 21:48:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Gopalakrishnan", "Karthik", ""], ["Padmakumar", "Aishwarya", ""], ["Kim", "Seokhwan", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2106.08495", "submitter": "Feng Hou", "authors": "Feng Hou, Ruili Wang, Jun He, Yi Zhou", "title": "Improving Entity Linking through Semantic Reinforced Entity Embeddings", "comments": "6 pages, 3 figures, ACL 2020", "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics, 2020", "doi": "10.18653/v1/2020.acl-main.612", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Entity embeddings, which represent different aspects of each entity with a\nsingle vector like word embeddings, are a key component of neural entity\nlinking models. Existing entity embeddings are learned from canonical Wikipedia\narticles and local contexts surrounding target entities. Such entity embeddings\nare effective, but too distinctive for linking models to learn contextual\ncommonality. We propose a simple yet effective method, FGS2EE, to inject\nfine-grained semantic information into entity embeddings to reduce the\ndistinctiveness and facilitate the learning of contextual commonality. FGS2EE\nfirst uses the embeddings of semantic type words to generate semantic\nembeddings, and then combines them with existing entity embeddings through\nlinear aggregation. Extensive experiments show the effectiveness of such\nembeddings. Based on our entity embeddings, we achieved new sate-of-the-art\nperformance on entity linking.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 00:27:56 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hou", "Feng", ""], ["Wang", "Ruili", ""], ["He", "Jun", ""], ["Zhou", "Yi", ""]]}, {"id": "2106.08556", "submitter": "Zhengyuan Liu", "authors": "Zhengyuan Liu, Ke Shi, Nancy F. Chen", "title": "Coreference-Aware Dialogue Summarization", "comments": "accepted for presentation at SIGDIAL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarizing conversations via neural approaches has been gaining research\ntraction lately, yet it is still challenging to obtain practical solutions.\nExamples of such challenges include unstructured information exchange in\ndialogues, informal interactions between speakers, and dynamic role changes of\nspeakers as the dialogue evolves. Many of such challenges result in complex\ncoreference links. Therefore, in this work, we investigate different approaches\nto explicitly incorporate coreference information in neural abstractive\ndialogue summarization models to tackle the aforementioned challenges.\nExperimental results show that the proposed approaches achieve state-of-the-art\nperformance, implying it is useful to utilize coreference information in\ndialogue summarization. Evaluation results on factual correctness suggest such\ncoreference-aware models are better at tracing the information flow among\ninterlocutors and associating accurate status/actions with the corresponding\ninterlocutors and person mentions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 05:18:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Zhengyuan", ""], ["Shi", "Ke", ""], ["Chen", "Nancy F.", ""]]}, {"id": "2106.08571", "submitter": "Fang Xianghong", "authors": "Xianghong Fang and Haoli Bai and Jian Li and Zenglin Xu and Michael\n  Lyu and Irwin King", "title": "Discrete Auto-regressive Variational Attention Models for Text Modeling", "comments": "IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Variational autoencoders (VAEs) have been widely applied for text modeling.\nIn practice, however, they are troubled by two challenges: information\nunderrepresentation and posterior collapse. The former arises as only the last\nhidden state of LSTM encoder is transformed into the latent space, which is\ngenerally insufficient to summarize the data. The latter is a long-standing\nproblem during the training of VAEs as the optimization is trapped to a\ndisastrous local optimum. In this paper, we propose Discrete Auto-regressive\nVariational Attention Model (DAVAM) to address the challenges. Specifically, we\nintroduce an auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency from the input.\nWe further design discrete latent space for the variational attention and\nmathematically show that our model is free from posterior collapse. Extensive\nexperiments on language modeling tasks demonstrate the superiority of DAVAM\nagainst several VAE counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:36:26 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Fang", "Xianghong", ""], ["Bai", "Haoli", ""], ["Li", "Jian", ""], ["Xu", "Zenglin", ""], ["Lyu", "Michael", ""], ["King", "Irwin", ""]]}, {"id": "2106.08582", "submitter": "Rui Jiao", "authors": "Rui Jiao, Zonghan Yang, Maosong Sun and Yang Liu", "title": "Alternated Training with Synthetic and Authentic Data for Neural Machine\n  Translation", "comments": "ACL 2021, Short Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While synthetic bilingual corpora have demonstrated their effectiveness in\nlow-resource neural machine translation (NMT), adding more synthetic data often\ndeteriorates translation performance. In this work, we propose alternated\ntraining with synthetic and authentic data for NMT. The basic idea is to\nalternate synthetic and authentic corpora iteratively during training. Compared\nwith previous work, we introduce authentic data as guidance to prevent the\ntraining of NMT models from being disturbed by noisy synthetic data.\nExperiments on Chinese-English and German-English translation tasks show that\nour approach improves the performance over several strong baselines. We\nvisualize the BLEU landscape to further investigate the role of authentic and\nsynthetic data during alternated training. From the visualization, we find that\nauthentic data helps to direct the NMT model parameters towards points with\nhigher BLEU scores and leads to consistent translation performance improvement.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:13:16 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jiao", "Rui", ""], ["Yang", "Zonghan", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "2106.08616", "submitter": "Li-Ming Zhan", "authors": "Li-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, Xiao-Ming Wu, Albert Y.S.\n  Lam", "title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative\n  Training", "comments": "Published as long oral paper in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Out-of-scope intent detection is of practical importance in task-oriented\ndialogue systems. Since the distribution of outlier utterances is arbitrary and\nunknown in the training stage, existing methods commonly rely on strong\nassumptions on data distribution such as mixture of Gaussians to make\ninference, resulting in either complex multi-step training procedures or\nhand-crafted rules such as confidence threshold selection for outlier\ndetection. In this paper, we propose a simple yet effective method to train an\nout-of-scope intent classifier in a fully end-to-end manner by simulating the\ntest scenario in training, which requires no assumption on data distribution\nand no additional post-processing or threshold setting. Specifically, we\nconstruct a set of pseudo outliers in the training stage, by generating\nsynthetic outliers using inliner features via self-supervision and sampling\nout-of-scope sentences from easily available open-domain datasets. The pseudo\noutliers are used to train a discriminative classifier that can be directly\napplied to and generalize well on the test task. We evaluate our method\nextensively on four benchmark dialogue datasets and observe significant\nimprovements over state-of-the-art approaches. Our code has been released at\nhttps://github.com/liam0949/DCLOOS.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:17:18 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 09:25:31 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhan", "Li-Ming", ""], ["Liang", "Haowen", ""], ["Liu", "Bo", ""], ["Fan", "Lu", ""], ["Wu", "Xiao-Ming", ""], ["Lam", "Albert Y. S.", ""]]}, {"id": "2106.08629", "submitter": "Jialong Tang", "authors": "Jialong Tang, Hongyu Lin, Meng Liao, Yaojie Lu, Xianpei Han, Le Sun,\n  Weijian Xie, Jin Xu", "title": "From Discourse to Narrative: Knowledge Projection for Event Relation\n  Extraction", "comments": "11 pages", "journal-ref": "ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Current event-centric knowledge graphs highly rely on explicit connectives to\nmine relations between events. Unfortunately, due to the sparsity of\nconnectives, these methods severely undermine the coverage of EventKGs. The\nlack of high-quality labelled corpora further exacerbates that problem. In this\npaper, we propose a knowledge projection paradigm for event relation\nextraction: projecting discourse knowledge to narratives by exploiting the\ncommonalities between them. Specifically, we propose Multi-tier Knowledge\nProjection Network (MKPNet), which can leverage multi-tier discourse knowledge\neffectively for event relation extraction. In this way, the labelled data\nrequirement is significantly reduced, and implicit event relations can be\neffectively extracted. Intrinsic experimental results show that MKPNet achieves\nthe new state-of-the-art performance, and extrinsic experimental results verify\nthe value of the extracted event relations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:35:29 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tang", "Jialong", ""], ["Lin", "Hongyu", ""], ["Liao", "Meng", ""], ["Lu", "Yaojie", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Xie", "Weijian", ""], ["Xu", "Jin", ""]]}, {"id": "2106.08637", "submitter": "Tan Liu", "authors": "Tan Liu, Wu Guo, Bin Gu", "title": "Topic Classification on Spoken Documents Using Deep Acoustic and\n  Linguistic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic classification systems on spoken documents usually consist of two\nmodules: an automatic speech recognition (ASR) module to convert speech into\ntext and a text topic classification (TTC) module to predict the topic class\nfrom the decoded text. In this paper, instead of using the ASR transcripts, the\nfusion of deep acoustic and linguistic features is used for topic\nclassification on spoken documents. More specifically, a conventional CTC-based\nacoustic model (AM) using phonemes as output units is first trained, and the\noutputs of the layer before the linear phoneme classifier in the trained AM are\nused as the deep acoustic features of spoken documents. Furthermore, these deep\nacoustic features are fed to a phoneme-to-word (P2W) module to obtain deep\nlinguistic features. Finally, a local multi-head attention module is proposed\nto fuse these two types of deep features for topic classification. Experiments\nconducted on a subset selected from Switchboard corpus show that our proposed\nframework outperforms the conventional ASR+TTC systems and achieves a 3.13%\nimprovement in ACC.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:54:31 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Tan", ""], ["Guo", "Wu", ""], ["Gu", "Bin", ""]]}, {"id": "2106.08648", "submitter": "Danny Merkx", "authors": "Danny Merkx, Stefan L. Frank, Mirjam Ernestus", "title": "Semantic sentence similarity: size does not always matter", "comments": "This paper has been accepted at Interspeech 2021 where it will be\n  presented and appear in the conference proceedings in September 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the question whether visually grounded speech\nrecognition (VGS) models learn to capture sentence semantics without access to\nany prior linguistic knowledge. We produce synthetic and natural spoken\nversions of a well known semantic textual similarity database and show that our\nVGS model produces embeddings that correlate well with human semantic\nsimilarity judgements. Our results show that a model trained on a small\nimage-caption database outperforms two models trained on much larger databases,\nindicating that database size is not all that matters. We also investigate the\nimportance of having multiple captions per image and find that this is indeed\nhelpful even if the total number of images is lower, suggesting that\nparaphrasing is a valuable learning signal. While the general trend in the\nfield is to create ever larger datasets to train models on, our findings\nindicate other characteristics of the database can just as important important.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 09:22:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Merkx", "Danny", ""], ["Frank", "Stefan L.", ""], ["Ernestus", "Mirjam", ""]]}, {"id": "2106.08649", "submitter": "Adam Gabry\\'s", "authors": "Adam Gabry\\'s, Yunlong Jiao, Viacheslav Klimkov, Daniel Korzekwa,\n  Roberto Barra-Chicote", "title": "Improving the expressiveness of neural vocoding with non-affine\n  Normalizing Flows", "comments": "Accepted to Interspeech 2021, 5 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general enhancement to the Normalizing Flows (NF) used\nin neural vocoding. As a case study, we improve expressive speech vocoding with\na revamped Parallel Wavenet (PW). Specifically, we propose to extend the affine\ntransformation of PW to the more expressive invertible non-affine function. The\ngreater expressiveness of the improved PW leads to better-perceived signal\nquality and naturalness in the waveform reconstruction and text-to-speech (TTS)\ntasks. We evaluate the model across different speaking styles on a\nmulti-speaker, multi-lingual dataset. In the waveform reconstruction task, the\nproposed model closes the naturalness and signal quality gap from the original\nPW to recordings by $10\\%$, and from other state-of-the-art neural vocoding\nsystems by more than $60\\%$. We also demonstrate improvements in objective\nmetrics on the evaluation test set with L2 Spectral Distance and Cross-Entropy\nreduced by $3\\%$ and $6\\unicode{x2030}$ comparing to the affine PW.\nFurthermore, we extend the probability density distillation procedure proposed\nby the original PW paper, so that it works with any non-affine invertible and\ndifferentiable function.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 09:25:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gabry\u015b", "Adam", ""], ["Jiao", "Yunlong", ""], ["Klimkov", "Viacheslav", ""], ["Korzekwa", "Daniel", ""], ["Barra-Chicote", "Roberto", ""]]}, {"id": "2106.08657", "submitter": "Yiqing Xie", "authors": "Yiqing Xie, Jiaming Shen, Sha Li, Yuning Mao, Jiawei Han", "title": "Eider: Evidence-enhanced Document-level Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction (DocRE) aims at extracting the semantic\nrelations among entity pairs in a document. In DocRE, a subset of the sentences\nin a document, called the evidence sentences, might be sufficient for\npredicting the relation between a specific entity pair. To make better use of\nthe evidence sentences, in this paper, we propose a three-stage\nevidence-enhanced DocRE framework consisting of joint relation and evidence\nextraction, evidence-centered relation extraction (RE), and fusion of\nextraction results. We first jointly train an RE model with a simple and\nmemory-efficient evidence extraction model. Then, we construct pseudo documents\nbased on the extracted evidence sentences and run the RE model again. Finally,\nwe fuse the extraction results of the first two stages using a blending layer\nand make a final prediction. Extensive experiments show that our proposed\nframework achieves state-of-the-art performance on the DocRED dataset,\noutperforming the second-best method by 0.76/0.82 Ign F1/F1. In particular, our\nmethod significantly improves the performance on inter-sentence relations by\n1.23 Inter F1.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 09:43:16 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Xie", "Yiqing", ""], ["Shen", "Jiaming", ""], ["Li", "Sha", ""], ["Mao", "Yuning", ""], ["Han", "Jiawei", ""]]}, {"id": "2106.08680", "submitter": "Krithika Ramesh", "authors": "Gauri Gupta, Krithika Ramesh and Sanjay Singh", "title": "Evaluating Gender Bias in Hindi-English Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With language models being deployed increasingly in the real world, it is\nessential to address the issue of the fairness of their outputs. The word\nembedding representations of these language models often implicitly draw\nunwanted associations that form a social bias within the model. The nature of\ngendered languages like Hindi, poses an additional problem to the\nquantification and mitigation of bias, owing to the change in the form of the\nwords in the sentence, based on the gender of the subject. Additionally, there\nis sparse work done in the realm of measuring and debiasing systems for Indic\nlanguages. In our work, we attempt to evaluate and quantify the gender bias\nwithin a Hindi-English machine translation system. We implement a modified\nversion of the existing TGBI metric based on the grammatical considerations for\nHindi. We also compare and contrast the resulting bias measurements across\nmultiple metrics for pre-trained embeddings and the ones learned by our machine\ntranslation model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:35:51 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gupta", "Gauri", ""], ["Ramesh", "Krithika", ""], ["Singh", "Sanjay", ""]]}, {"id": "2106.08686", "submitter": "Badr M. Abdullah", "authors": "Badr M. Abdullah, Marius Mosbach, Iuliia Zaitova, Bernd M\\\"obius,\n  Dietrich Klakow", "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An\n  Empirical Study", "comments": "Accepted in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several variants of deep neural networks have been successfully employed for\nbuilding parametric models that project variable-duration spoken word segments\nonto fixed-size vector representations, or acoustic word embeddings (AWEs).\nHowever, it remains unclear to what degree we can rely on the distance in the\nemerging AWE space as an estimate of word-form similarity. In this paper, we\nask: does the distance in the acoustic embedding space correlate with\nphonological dissimilarity? To answer this question, we empirically investigate\nthe performance of supervised approaches for AWEs with different neural\narchitectures and learning objectives. We train AWE models in controlled\nsettings for two languages (German and Czech) and evaluate the embeddings on\ntwo tasks: word discrimination and phonological similarity. Our experiments\nshow that (1) the distance in the embedding space in the best cases only\nmoderately correlates with phonological distance, and (2) improving the\nperformance on the word discrimination task does not necessarily yield models\nthat better reflect word phonological similarity. Our findings highlight the\nnecessity to rethink the current intrinsic evaluations for AWEs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:47:56 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Abdullah", "Badr M.", ""], ["Mosbach", "Marius", ""], ["Zaitova", "Iuliia", ""], ["M\u00f6bius", "Bernd", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2106.08689", "submitter": "Yu Qiao", "authors": "Yu Qiao, Xuefeng Yin, Daniel Wiechmann, Elma Kerz", "title": "Alzheimer's Disease Detection from Spontaneous Speech through Combining\n  Linguistic Complexity and (Dis)Fluency Features with Pretrained Language\n  Models", "comments": "accepted at Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we combined linguistic complexity and (dis)fluency features\nwith pretrained language models for the task of Alzheimer's disease detection\nof the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous\nSpeech) challenge. An accuracy of 83.1% was achieved on the test set, which\namounts to an improvement of 4.23% over the baseline model. Our best-performing\nmodel that integrated component models using a stacking ensemble technique\nperformed equally well on cross-validation and test data, indicating that it is\nrobust against overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:50:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Qiao", "Yu", ""], ["Yin", "Xuefeng", ""], ["Wiechmann", "Daniel", ""], ["Kerz", "Elma", ""]]}, {"id": "2106.08694", "submitter": "Marco Baroni", "authors": "Marco Baroni", "title": "On the proper role of linguistically-oriented deep net analysis in\n  linguistic theorizing", "comments": "Submitted to collective volume on Algebraic Systems and the\n  Representation of Linguistic Knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lively research field has recently emerged that uses experimental methods\nto probe the linguistic behavior of modern deep networks. While work in this\ntradition often reports intriguing results about the grammatical skills of deep\nnets, it is not clear what their implications for linguistic theorizing should\nbe. As a consequence, linguistically-oriented deep net analysis has had very\nlittle impact on linguistics at large. In this chapter, I suggest that deep\nnetworks should be treated as theories making explicit predictions about the\nacceptability of linguistic utterances. I argue that, if we overcome some\nobstacles standing in the way of seriously pursuing this idea, we will gain a\npowerful new theoretical tool, complementary to mainstream algebraic\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:57:24 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Baroni", "Marco", ""]]}, {"id": "2106.08723", "submitter": "Ting Han", "authors": "Ting Han, Chongxuan Huang, Wei Peng", "title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State\n  Tracking", "comments": "Accepted by Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue State Tracking (DST), which is the process of inferring user goals\nby estimating belief states given the dialogue history, plays a critical role\nin task-oriented dialogue systems. A coreference phenomenon observed in\nmulti-turn conversations is not addressed by existing DST models, leading to\nsub-optimal performances. In this paper, we propose Coreference Dialogue State\nTracker (CDST) that explicitly models the coreference feature. In particular,\nat each turn, the proposed model jointly predicts the coreferred domain-slot\npair and extracts the coreference values from the dialogue context.\nExperimental results on MultiWOZ 2.1 dataset show that the proposed model\nachieves the state-of-the-art joint goal accuracy of 56.47%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:47:29 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Han", "Ting", ""], ["Huang", "Chongxuan", ""], ["Peng", "Wei", ""]]}, {"id": "2106.08785", "submitter": "Zaijing Li", "authors": "Zaijing Li, Fengxiao Tang, Tieyu Sun, Yusen Zhu, Ming Zhao", "title": "SEOVER: Sentence-level Emotion Orientation Vector based Conversation\n  Emotion Recognition Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of conversation emotion recognition, recent works focus on\nspeaker relationship modeling but ignore the role of utterance's emotional\ntendency.In this paper, we propose a new expression paradigm of sentence-level\nemotion orientation vector to model the potential correlation of emotions\nbetween sentence vectors. Based on it, we design an emotion recognition model,\nwhich extracts the sentence-level emotion orientation vectors from the language\nmodel and jointly learns from the dialogue sentiment analysis model and\nextracted sentence-level emotion orientation vectors to identify the speaker's\nemotional orientation during the conversation. We conduct experiments on two\nbenchmark datasets and compare them with the five baseline models.The\nexperimental results show that our model has better performance on all data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:44:03 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Li", "Zaijing", ""], ["Tang", "Fengxiao", ""], ["Sun", "Tieyu", ""], ["Zhu", "Yusen", ""], ["Zhao", "Ming", ""]]}, {"id": "2106.08801", "submitter": "Ziheng Zhang", "authors": "Zhiyuan Qi, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Yefeng Zheng", "title": "PRASEMap: A Probabilistic Reasoning and Semantic Embedding based\n  Knowledge Graph Alignment System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:06:09 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Qi", "Zhiyuan", ""], ["Zhang", "Ziheng", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Xi", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2106.08829", "submitter": "Gullal Singh Cheema", "authors": "Gullal S. Cheema and Sherzod Hakimov and Eric M\\\"uller-Budack and\n  Ralph Ewerth", "title": "A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment\n  Analysis Methods", "comments": "Accepted in Workshop on Multi-ModalPre-Training for Multimedia\n  Understanding (MMPT 2021), co-located with ICMR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Opinion and sentiment analysis is a vital task to characterize subjective\ninformation in social media posts. In this paper, we present a comprehensive\nexperimental evaluation and comparison with six state-of-the-art methods, from\nwhich we have re-implemented one of them. In addition, we investigate different\ntextual and visual feature embeddings that cover different aspects of the\ncontent, as well as the recently introduced multimodal CLIP embeddings.\nExperimental results are presented for two different publicly available\nbenchmark datasets of tweets and corresponding images. In contrast to the\nevaluation methodology of previous work, we introduce a reproducible and fair\nevaluation scheme to make results comparable. Finally, we conduct an error\nanalysis to outline the limitations of the methods and possibilities for the\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:44:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cheema", "Gullal S.", ""], ["Hakimov", "Sherzod", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.08838", "submitter": "Hsien-Chin Lin", "authors": "Hsien-chin Lin, Nurul Lubis, Songbo Hu, Carel van Niekerk, Christian\n  Geishauser, Michael Heck, Shutong Feng, Milica Ga\\v{s}i\\'c", "title": "Domain-independent User Simulation with Transformers for Task-oriented\n  Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dialogue policy optimisation via reinforcement learning requires a large\nnumber of training interactions, which makes learning with real users time\nconsuming and expensive. Many set-ups therefore rely on a user simulator\ninstead of humans. These user simulators have their own problems. While\nhand-coded, rule-based user simulators have been shown to be sufficient in\nsmall, simple domains, for complex domains the number of rules quickly becomes\nintractable. State-of-the-art data-driven user simulators, on the other hand,\nare still domain-dependent. This means that adaptation to each new domain\nrequires redesigning and retraining. In this work, we propose a\ndomain-independent transformer-based user simulator (TUS). The structure of our\nTUS is not tied to a specific domain, enabling domain generalisation and\nlearning of cross-domain user behaviour from data. We compare TUS with the\nstate of the art using automatic as well as human evaluations. TUS can compete\nwith rule-based user simulators on pre-defined domains and is able to\ngeneralise to unseen domains in a zero-shot fashion.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:56:04 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lin", "Hsien-chin", ""], ["Lubis", "Nurul", ""], ["Hu", "Songbo", ""], ["van Niekerk", "Carel", ""], ["Geishauser", "Christian", ""], ["Heck", "Michael", ""], ["Feng", "Shutong", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "2106.08846", "submitter": "Fu-Ming Guo", "authors": "Fu-Ming Guo, Austin Huang", "title": "Algorithm to Compilation Co-design: An Integrated View of Neural Network\n  Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reducing computation cost, inference latency, and memory footprint of neural\nnetworks are frequently cited as research motivations for pruning and sparsity.\nHowever, operationalizing those benefits and understanding the end-to-end\neffect of algorithm design and regularization on the runtime execution is not\noften examined in depth.\n  Here we apply structured and unstructured pruning to attention weights of\ntransformer blocks of the BERT language model, while also expanding block\nsparse representation (BSR) operations in the TVM compiler. Integration of BSR\noperations enables the TVM runtime execution to leverage structured pattern\nsparsity induced by model regularization.\n  This integrated view of pruning algorithms enables us to study relationships\nbetween modeling decisions and their direct impact on sparsity-enhanced\nexecution. Our main findings are: 1) we validate that performance benefits of\nstructured sparsity block regularization must be enabled by the BSR\naugmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x\nspeedup relative to standard TVM compilation (without expanded BSR support). 2)\nfor BERT attention weights, the end-to-end optimal block sparsity shape in this\nCPU inference context is not a square block (as in \\cite{gray2017gpu}) but\nrather a linear 32x1 block 3) the relationship between performance and block\nsize / shape is is suggestive of how model regularization parameters interact\nwith task scheduler optimizations resulting in the observed end-to-end\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:13:26 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 04:03:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Guo", "Fu-Ming", ""], ["Huang", "Austin", ""]]}, {"id": "2106.08858", "submitter": "Tristan Karch", "authors": "Tristan Karch, Laetitia Teodorescu, Katja Hofmann, Cl\\'ement\n  Moulin-Frier and Pierre-Yves Oudeyer", "title": "Grounding Spatio-Temporal Language with Transformers", "comments": "Contains main article and supplementaries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is an interface to the outside world. In order for embodied agents\nto use it, language must be grounded in other, sensorimotor modalities. While\nthere is an extended literature studying how machines can learn grounded\nlanguage, the topic of how to learn spatio-temporal linguistic concepts is\nstill largely uncharted. To make progress in this direction, we here introduce\na novel spatio-temporal language grounding task where the goal is to learn the\nmeaning of spatio-temporal descriptions of behavioral traces of an embodied\nagent. This is achieved by training a truth function that predicts if a\ndescription matches a given history of observations. The descriptions involve\ntime-extended predicates in past and present tense as well as spatio-temporal\nreferences to objects in the scene. To study the role of architectural biases\nin this task, we train several models including multimodal Transformer\narchitectures; the latter implement different attention computations between\nwords and objects across space and time. We test models on two classes of\ngeneralization: 1) generalization to randomly held-out sentences; 2)\ngeneralization to grammar primitives. We observe that maintaining object\nidentity in the attention computation of our Transformers is instrumental to\nachieving good performance on generalization overall, and that summarizing\nobject traces in a single token has little influence on performance. We then\ndiscuss how this opens new perspectives for language-guided autonomous embodied\nagents. We also release our code under open-source license as well as\npretrained models and datasets to encourage the wider community to build upon\nand extend our work in the future.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:28:22 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Karch", "Tristan", ""], ["Teodorescu", "Laetitia", ""], ["Hofmann", "Katja", ""], ["Moulin-Frier", "Cl\u00e9ment", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2106.08859", "submitter": "Kayode Olaleye", "authors": "Kayode Olaleye and Herman Kamper", "title": "Attention-Based Keyword Localisation in Speech using Visual Grounding", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually grounded speech models learn from images paired with spoken\ncaptions. By tagging images with soft text labels using a trained visual\nclassifier with a fixed vocabulary, previous work has shown that it is possible\nto train a model that can detect whether a particular text keyword occurs in\nspeech utterances or not. Here we investigate whether visually grounded speech\nmodels can also do keyword localisation: predicting where, within an utterance,\na given textual keyword occurs without any explicit text-based or alignment\nsupervision. We specifically consider whether incorporating attention into a\nconvolutional model is beneficial for localisation. Although absolute\nlocalisation performance with visually supervised models is still modest\n(compared to using unordered bag-of-word text labels for supervision), we show\nthat attention provides a large gain in performance over previous visually\ngrounded models. As in many other speech-image studies, we find that many of\nthe incorrect localisations are due to semantic confusions, e.g. locating the\nword 'backstroke' for the query keyword 'swimming'.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:29:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:57:46 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Olaleye", "Kayode", ""], ["Kamper", "Herman", ""]]}, {"id": "2106.08898", "submitter": "Haiqin Yang", "authors": "Xinyi Wang, Haiqin Yang, Liang Zhao, Yang Mo, Jianping Shen", "title": "RefBERT: Compressing BERT by Referencing to Pre-computed Representations", "comments": "8 pages, 1 figure, 3 tables, in IJCNN'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently developed large pre-trained language models, e.g., BERT, have\nachieved remarkable performance in many downstream natural language processing\napplications. These pre-trained language models often contain hundreds of\nmillions of parameters and suffer from high computation and latency in\nreal-world applications. It is desirable to reduce the computation overhead of\nthe models for fast training and inference while keeping the model performance\nin downstream applications. Several lines of work utilize knowledge\ndistillation to compress the teacher model to a smaller student model. However,\nthey usually discard the teacher's knowledge when in inference. Differently, in\nthis paper, we propose RefBERT to leverage the knowledge learned from the\nteacher, i.e., facilitating the pre-computed BERT representation on the\nreference sample and compressing BERT into a smaller student model. To\nguarantee our proposal, we provide theoretical justification on the loss\nfunction and the usage of reference samples. Significantly, the theoretical\nresult shows that including the pre-computed teacher's representations on the\nreference samples indeed increases the mutual information in learning the\nstudent model. Finally, we conduct the empirical evaluation and show that our\nRefBERT can beat the vanilla TinyBERT over 8.1\\% and achieves more than 94\\% of\nthe performance of $\\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is\n7.4x smaller and 9.5x faster on inference than BERT$_{\\rm BASE}$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 01:22:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Xinyi", ""], ["Yang", "Haiqin", ""], ["Zhao", "Liang", ""], ["Mo", "Yang", ""], ["Shen", "Jianping", ""]]}, {"id": "2106.08914", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "$C^3$: Compositional Counterfactual Constrastive Learning for\n  Video-grounded Dialogues", "comments": "22 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-grounded dialogue systems aim to integrate video understanding and\ndialogue understanding to generate responses that are relevant to both the\ndialogue and video context. Most existing approaches employ deep learning\nmodels and have achieved remarkable performance, given the relatively small\ndatasets available. However, the results are partly accomplished by exploiting\nbiases in the datasets rather than developing multimodal reasoning, resulting\nin limited generalization. In this paper, we propose a novel approach of\nCompositional Counterfactual Contrastive Learning ($C^3$) to develop\ncontrastive training between factual and counterfactual samples in\nvideo-grounded dialogues. Specifically, we design factual/counterfactual\nsampling based on the temporal steps in videos and tokens in dialogues and\npropose contrastive loss functions that exploit object-level or action-level\nvariance. Different from prior approaches, we focus on contrastive hidden state\nrepresentations among compositional output tokens to optimize the\nrepresentation space in a generation setting. We achieved promising performance\ngains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the\nbenefits of our approach in grounding video and dialogue context.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:05:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2106.08927", "submitter": "Wim Boes", "authors": "Wim Boes, Robbe Van Rompaey, Lyan Verwimp, Joris Pelemans, Hugo Van\n  hamme, Patrick Wambacq", "title": "On the long-term learning ability of LSTM LMs", "comments": null, "journal-ref": "ESANN 2020 proceedings, European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (2020) 625-630", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We inspect the long-term learning ability of Long Short-Term Memory language\nmodels (LSTM LMs) by evaluating a contextual extension based on the Continuous\nBag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and\nby analyzing its performance. We evaluate on text and speech. Sentence-level\nmodels using the long-term contextual module perform comparably to vanilla\ndiscourse-level LSTM LMs. On the other hand, the extension does not provide\ngains for discourse-level models. These findings indicate that discourse-level\nLSTM LMs already rely on contextual information to perform long-term learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:34:37 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Boes", "Wim", ""], ["Van Rompaey", "Robbe", ""], ["Verwimp", "Lyan", ""], ["Pelemans", "Joris", ""], ["Van hamme", "Hugo", ""], ["Wambacq", "Patrick", ""]]}, {"id": "2106.08942", "submitter": "Samuel Kiegeland", "authors": "Samuel Kiegeland and Julia Kreutzer", "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine\n  Translation", "comments": null, "journal-ref": "North American Chapter of the Association for Computational\n  Linguistics, 2021, 1673-1681", "doi": "10.18653/v1/2021.naacl-main.133", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient algorithms have found wide adoption in NLP, but have recently\nbecome subject to criticism, doubting their suitability for NMT. Choshen et al.\n(2020) identify multiple weaknesses and suspect that their success is\ndetermined by the shape of output distributions rather than the reward. In this\npaper, we revisit these claims and study them under a wider range of\nconfigurations. Our experiments on in-domain and cross-domain adaptation reveal\nthe importance of exploration and reward scaling, and provide empirical\ncounter-evidence to these claims.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:54:49 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Kiegeland", "Samuel", ""], ["Kreutzer", "Julia", ""]]}, {"id": "2106.08960", "submitter": "Varun Nagaraja", "authors": "Varun Nagaraja, Yangyang Shi, Ganesh Venkatesh, Ozlem Kalinli, Michael\n  L. Seltzer, Vikas Chandra", "title": "Collaborative Training of Acoustic Encoders for Speech Recognition", "comments": "INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device speech recognition requires training models of different sizes for\ndeploying on devices with various computational budgets. When building such\ndifferent models, we can benefit from training them jointly to take advantage\nof the knowledge shared between them. Joint training is also efficient since it\nreduces the redundancy in the training procedure's data handling operations. We\npropose a method for collaboratively training acoustic encoders of different\nsizes for speech recognition. We use a sequence transducer setup where\ndifferent acoustic encoders share a common predictor and joiner modules. The\nacoustic encoders are also trained using co-distillation through an auxiliary\ntask for frame level chenone prediction, along with the transducer loss. We\nperform experiments using the LibriSpeech corpus and demonstrate that the\ncollaboratively trained acoustic encoders can provide up to a 11% relative\nimprovement in the word error rate on both the test partitions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:05:47 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 21:35:43 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Nagaraja", "Varun", ""], ["Shi", "Yangyang", ""], ["Venkatesh", "Ganesh", ""], ["Kalinli", "Ozlem", ""], ["Seltzer", "Michael L.", ""], ["Chandra", "Vikas", ""]]}, {"id": "2106.08977", "submitter": "Haoming Jiang", "authors": "Haoming Jiang, Danqing Zhang, Tianyu Cao, Bing Yin, Tuo Zhao", "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly\n  Labeled Data", "comments": "The 59th Annual Meeting of the Association for Computational\n  Linguistics (ACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:18:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jiang", "Haoming", ""], ["Zhang", "Danqing", ""], ["Cao", "Tianyu", ""], ["Yin", "Bing", ""], ["Zhao", "Tuo", ""]]}, {"id": "2106.09009", "submitter": "Michael Saxon", "authors": "Michael Saxon, Samridhi Choudhary, Joseph P. McKenna, Athanasios\n  Mouchtaris", "title": "End-to-End Spoken Language Understanding for Generalized Voice\n  Assistants", "comments": "Accepted to Interspeech 2021; 5 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) spoken language understanding (SLU) systems predict\nutterance semantics directly from speech using a single model. Previous work in\nthis area has focused on targeted tasks in fixed domains, where the output\nsemantic structure is assumed a priori and the input speech is of limited\ncomplexity. In this work we present our approach to developing an E2E model for\ngeneralized SLU in commercial voice assistants (VAs). We propose a fully\ndifferentiable, transformer-based, hierarchical system that can be pretrained\nat both the ASR and NLU levels. This is then fine-tuned on both transcription\nand semantic classification losses to handle a diverse set of intent and\nargument combinations. This leads to an SLU system that achieves significant\nimprovements over baselines on a complex internal generalized VA dataset with a\n43% improvement in accuracy, while still meeting the 99% accuracy benchmark on\nthe popular Fluent Speech Commands dataset. We further evaluate our model on a\nhard test set, exclusively containing slot arguments unseen in training, and\ndemonstrate a nearly 20% improvement, showing the efficacy of our approach in\ntruly demanding VA scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:56:47 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 23:11:04 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Saxon", "Michael", ""], ["Choudhary", "Samridhi", ""], ["McKenna", "Joseph P.", ""], ["Mouchtaris", "Athanasios", ""]]}, {"id": "2106.09024", "submitter": "Duccio Pappadopulo", "authors": "Duccio Pappadopulo, Lisa Bauer, Marco Farina, Ozan \\.Irsoy, and Mohit\n  Bansal", "title": "Disentangling Online Chats with DAG-Structured LSTMs", "comments": "8 pages, 1 figure. Accepted at *SEM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern messaging systems allow fast and synchronous textual\ncommunication among many users. The resulting sequence of messages hides a more\ncomplicated structure in which independent sub-conversations are interwoven\nwith one another. This poses a challenge for any task aiming to understand the\ncontent of the chat logs or gather information from them. The ability to\ndisentangle these conversations is then tantamount to the success of many\ndownstream tasks such as summarization and question answering. Structured\ninformation accompanying the text such as user turn, user mentions, timestamps,\nis used as a cue by the participants themselves who need to follow the\nconversation and has been shown to be important for disentanglement. DAG-LSTMs,\na generalization of Tree-LSTMs that can handle directed acyclic dependencies,\nare a natural way to incorporate such information and its non-sequential\nnature. In this paper, we apply DAG-LSTMs to the conversation disentanglement\ntask. We perform our experiments on the Ubuntu IRC dataset. We show that the\nnovel model we propose achieves state of the art status on the task of\nrecovering reply-to relations and it is competitive on other disentanglement\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:00:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pappadopulo", "Duccio", ""], ["Bauer", "Lisa", ""], ["Farina", "Marco", ""], ["\u0130rsoy", "Ozan", ""], ["Bansal", "Mohit", ""]]}, {"id": "2106.09063", "submitter": "Ethan Chau", "authors": "Ethan C. Chau, Noah A. Smith", "title": "Specializing Multilingual Language Models: An Empirical Study", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized word representations from pretrained multilingual language\nmodels have become the de facto standard for addressing natural language tasks\nin many different languages, but the success of this approach is far from\nuniversal. For languages rarely or never seen by these models, directly using\nsuch models often results in suboptimal representation or use of data,\nmotivating additional model adaptations to achieve reasonably strong\nperformance. In this work, we study the performance, extensibility, and\ninteraction of two such adaptations for this low-resource setting: vocabulary\naugmentation and script transliteration. Our evaluations on a set of three\ntasks in nine diverse low-resource languages yield a mixed result, upholding\nthe viability of these approaches while raising new questions around how to\noptimally adapt multilingual models to low-resource settings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:13:55 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 23:28:20 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chau", "Ethan C.", ""], ["Smith", "Noah A.", ""]]}, {"id": "2106.09069", "submitter": "Sebastian Gehrmann", "authors": "Simon Mille, Kaustubh D. Dhole, Saad Mahamood, Laura\n  Perez-Beltrachini, Varun Gangal, Mihir Kale, Emiel van Miltenburg, Sebastian\n  Gehrmann", "title": "Automatic Construction of Evaluation Suites for Natural Language\n  Generation Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning approaches applied to NLP are often evaluated by summarizing\ntheir performance in a single number, for example accuracy. Since most test\nsets are constructed as an i.i.d. sample from the overall data, this approach\noverly simplifies the complexity of language and encourages overfitting to the\nhead of the data distribution. As such, rare language phenomena or text about\nunderrepresented groups are not equally included in the evaluation. To\nencourage more in-depth model analyses, researchers have proposed the use of\nmultiple test sets, also called challenge sets, that assess specific\ncapabilities of a model. In this paper, we develop a framework based on this\nidea which is able to generate controlled perturbations and identify subsets in\ntext-to-scalar, text-to-text, or data-to-text settings. By applying this\nframework to the GEM generation benchmark, we propose an evaluation suite made\nof 80 challenge sets, demonstrate the kinds of analyses that it enables and\nshed light onto the limits of current generation models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:20:58 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mille", "Simon", ""], ["Dhole", "Kaustubh D.", ""], ["Mahamood", "Saad", ""], ["Perez-Beltrachini", "Laura", ""], ["Gangal", "Varun", ""], ["Kale", "Mihir", ""], ["van Miltenburg", "Emiel", ""], ["Gehrmann", "Sebastian", ""]]}, {"id": "2106.09141", "submitter": "Aida Nematzadeh", "authors": "Lisa Anne Hendricks and Aida Nematzadeh", "title": "Probing Image-Language Transformers for Verb Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image-language transformers have achieved impressive results on a\nvariety of tasks that rely on fine-tuning (e.g., visual question answering and\nimage retrieval). We are interested in shedding light on the quality of their\npretrained representations -- in particular, if these models can distinguish\ndifferent types of verbs or if they rely solely on nouns in a given sentence.\nTo do so, we collect a dataset of image-sentence pairs (in English) consisting\nof 421 verbs that are either visual or commonly found in the pretraining data\n(i.e., the Conceptual Captions dataset). We use this dataset to evaluate\npretrained image-language transformers and find that they fail more in\nsituations that require verb understanding compared to other parts of speech.\nWe also investigate what category of verbs are particularly challenging.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:36:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Nematzadeh", "Aida", ""]]}, {"id": "2106.09174", "submitter": "Di Jin", "authors": "Di Jin, Seokhwan Kim, Dilek Hakkani-Tur", "title": "Can I Be of Further Assistance? Using Unstructured Knowledge Access to\n  Improve Task-oriented Conversational Modeling", "comments": "Presented as a DIALDOC workshop paper at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most prior work on task-oriented dialogue systems are restricted to limited\ncoverage of domain APIs. However, users oftentimes have requests that are out\nof the scope of these APIs. This work focuses on responding to these\nbeyond-API-coverage user turns by incorporating external, unstructured\nknowledge sources. Our approach works in a pipelined manner with\nknowledge-seeking turn detection, knowledge selection, and response generation\nin sequence. We introduce novel data augmentation methods for the first two\nsteps and demonstrate that the use of information extracted from dialogue\ncontext improves the knowledge selection and end-to-end performances. Through\nexperiments, we achieve state-of-the-art performance for both automatic and\nhuman evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the\neffectiveness of our contributions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 23:31:42 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Jin", "Di", ""], ["Kim", "Seokhwan", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2106.09204", "submitter": "Xueqing Liu", "authors": "Xueqing Liu, Chi Wang", "title": "An Empirical Study on Hyperparameter Optimization for Fine-Tuning\n  Pre-trained Language Models", "comments": "To appear in ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of fine-tuning pre-trained language models largely depends on\nthe hyperparameter configuration. In this paper, we investigate the performance\nof modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained\nlanguage models. First, we study and report three HPO algorithms' performances\non fine-tuning two state-of-the-art language models on the GLUE dataset. We\nfind that using the same time budget, HPO often fails to outperform grid search\ndue to two reasons: insufficient time budget and overfitting. We propose two\ngeneral strategies and an experimental procedure to systematically troubleshoot\nHPO's failure cases. By applying the procedure, we observe that HPO can succeed\nwith more appropriate settings in the search space and time budget; however, in\ncertain cases overfitting remains. Finally, we make suggestions for future\nwork. Our implementation can be found in\nhttps://github.com/microsoft/FLAML/tree/main/flaml/nlp/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:58:32 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Xueqing", ""], ["Wang", "Chi", ""]]}, {"id": "2106.09216", "submitter": "Jaesong Lee", "authors": "Jaesong Lee, Jingu Kang, Shinji Watanabe", "title": "Layer Pruning on Demand with Intermediate CTC", "comments": "Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying an end-to-end automatic speech recognition (ASR) model on\nmobile/embedded devices is a challenging task, since the device computational\npower and energy consumption requirements are dynamically changed in practice.\nTo overcome the issue, we present a training and pruning method for ASR based\non the connectionist temporal classification (CTC) which allows reduction of\nmodel depth at run-time without any extra fine-tuning. To achieve the goal, we\nadopt two regularization methods, intermediate CTC and stochastic depth, to\ntrain a model whose performance does not degrade much after pruning. We present\nan in-depth analysis of layer behaviors using singular vector canonical\ncorrelation analysis (SVCCA), and efficient strategies for finding layers which\nare safe to prune. Using the proposed method, we show that a Transformer-CTC\nmodel can be pruned in various depth on demand, improving real-time factor from\n0.005 to 0.002 on GPU, while each pruned sub-model maintains the accuracy of\nindividually trained model of the same depth.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 02:40:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lee", "Jaesong", ""], ["Kang", "Jingu", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2106.09231", "submitter": "Hongyu Lin", "authors": "Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao,\n  Tong Xue, Jin Xu", "title": "Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge\n  Bases", "comments": "Accepted to ACL2021(main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous literatures show that pre-trained masked language models (MLMs) such\nas BERT can achieve competitive factual knowledge extraction performance on\nsome datasets, indicating that MLMs can potentially be a reliable knowledge\nsource. In this paper, we conduct a rigorous study to explore the underlying\npredicting mechanisms of MLMs over different extraction paradigms. By\ninvestigating the behaviors of MLMs, we find that previous decent performance\nmainly owes to the biased prompts which overfit dataset artifacts. Furthermore,\nincorporating illustrative cases and external contexts improve knowledge\nprediction mainly due to entity type guidance and golden answer leakage. Our\nfindings shed light on the underlying predicting mechanisms of MLMs, and\nstrongly question the previous conclusion that current MLMs can potentially\nserve as reliable factual knowledge bases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 03:59:45 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Cao", "Boxi", ""], ["Lin", "Hongyu", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Yan", "Lingyong", ""], ["Liao", "Meng", ""], ["Xue", "Tong", ""], ["Xu", "Jin", ""]]}, {"id": "2106.09232", "submitter": "Hongyu Lin", "authors": "Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le\n  Sun, Meng Liao, Shaoyi Chen", "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end\n  Event Extraction", "comments": "Accepted to ACL2021 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event extraction is challenging due to the complex structure of event records\nand the semantic gap between text and event. Traditional methods usually\nextract event records by decomposing the complex structure prediction task into\nmultiple subtasks. In this paper, we propose Text2Event, a\nsequence-to-structure generation paradigm that can directly extract events from\nthe text in an end-to-end manner. Specifically, we design a\nsequence-to-structure network for unified event extraction, a constrained\ndecoding algorithm for event knowledge injection during inference, and a\ncurriculum learning algorithm for efficient model learning. Experimental\nresults show that, by uniformly modeling all tasks in a single model and\nuniversally predicting different labels, our method can achieve competitive\nperformance using only record-level annotations in both supervised learning and\ntransfer learning settings.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 04:00:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lu", "Yaojie", ""], ["Lin", "Hongyu", ""], ["Xu", "Jin", ""], ["Han", "Xianpei", ""], ["Tang", "Jialong", ""], ["Li", "Annan", ""], ["Sun", "Le", ""], ["Liao", "Meng", ""], ["Chen", "Shaoyi", ""]]}, {"id": "2106.09233", "submitter": "Hongyu Lin", "authors": "Wenkai Zhang, Hongyu Lin, Xianpei Han, Le Sun", "title": "De-biasing Distantly Supervised Named Entity Recognition via Causal\n  Intervention", "comments": "Accepted to ACL2021(main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distant supervision tackles the data bottleneck in NER by automatically\ngenerating training instances via dictionary matching. Unfortunately, the\nlearning of DS-NER is severely dictionary-biased, which suffers from spurious\ncorrelations and therefore undermines the effectiveness and the robustness of\nthe learned models. In this paper, we fundamentally explain the dictionary bias\nvia a Structural Causal Model (SCM), categorize the bias into intra-dictionary\nand inter-dictionary biases, and identify their causes. Based on the SCM, we\nlearn de-biased DS-NER via causal interventions. For intra-dictionary bias, we\nconduct backdoor adjustment to remove the spurious correlations introduced by\nthe dictionary confounder. For inter-dictionary bias, we propose a causal\ninvariance regularizer which will make DS-NER models more robust to the\nperturbation of dictionaries. Experiments on four datasets and three DS-NER\nmodels show that our method can significantly improve the performance of\nDS-NER.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 04:01:02 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Wenkai", ""], ["Lin", "Hongyu", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""]]}, {"id": "2106.09234", "submitter": "Hongyu Lin", "authors": "Wenkai Zhang, Hongyu Lin, Xianpei Han, Le Sun, Huidan Liu, Zhicheng\n  Wei, Nicholas Jing Yuan", "title": "Denoising Distantly Supervised Named Entity Recognition via a\n  Hypergeometric Probabilistic Model", "comments": "Accepted to AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Denoising is the essential step for distant supervision based named entity\nrecognition. Previous denoising methods are mostly based on instance-level\nconfidence statistics, which ignore the variety of the underlying noise\ndistribution on different datasets and entity types. This makes them difficult\nto be adapted to high noise rate settings. In this paper, we propose\nHypergeometric Learning (HGL), a denoising algorithm for distantly supervised\nNER that takes both noise distribution and instance-level confidence into\nconsideration. Specifically, during neural network training, we naturally model\nthe noise samples in each batch following a hypergeometric distribution\nparameterized by the noise-rate. Then each instance in the batch is regarded as\neither correct or noisy one according to its label confidence derived from\nprevious training step, as well as the noise distribution in this sampled\nbatch. Experiments show that HGL can effectively denoise the weakly-labeled\ndata retrieved from distant supervision, and therefore results in significant\nimprovements on the trained models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 04:01:25 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Wenkai", ""], ["Lin", "Hongyu", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Liu", "Huidan", ""], ["Wei", "Zhicheng", ""], ["Yuan", "Nicholas Jing", ""]]}, {"id": "2106.09248", "submitter": "Ashim Gupta", "authors": "Ashim Gupta and Vivek Srikumar", "title": "X-FACT: A New Benchmark Dataset for Multilingual Fact Checking", "comments": "ACL 2021; For data and code, see https://github.com/utahnlp/x-fact/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce X-FACT: the largest publicly available\nmultilingual dataset for factual verification of naturally existing real-world\nclaims. The dataset contains short statements in 25 languages and is labeled\nfor veracity by expert fact-checkers. The dataset includes a multilingual\nevaluation benchmark that measures both out-of-domain generalization, and\nzero-shot capabilities of the multilingual models. Using state-of-the-art\nmultilingual transformer-based models, we develop several automated\nfact-checking models that, along with textual claims, make use of additional\nmetadata and evidence from news stories retrieved using a search engine.\nEmpirically, our best model attains an F-score of around 40%, suggesting that\nour dataset is a challenging benchmark for evaluation of multilingual\nfact-checking models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 05:09:54 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Gupta", "Ashim", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2106.09317", "submitter": "Chenye Cui", "authors": "Chenye Cui, Yi Ren, Jinglin Liu, Feiyang Chen, Rongjie Huang, Ming\n  Lei, Zhou Zhao", "title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional\n  Text-to-Speech Model", "comments": "Accepted by Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been an increasing interest in neural speech synthesis.\nWhile the deep neural network achieves the state-of-the-art result in\ntext-to-speech (TTS) tasks, how to generate a more emotional and more\nexpressive speech is becoming a new challenge to researchers due to the\nscarcity of high-quality emotion speech dataset and the lack of advanced\nemotional TTS model. In this paper, we first briefly introduce and publicly\nrelease a Mandarin emotion speech dataset including 9,724 samples with audio\nfiles and its emotion human-labeled annotation. After that, we propose a simple\nbut efficient architecture for emotional speech synthesis called EMSpeech.\nUnlike those models which need additional reference audio as input, our model\ncould predict emotion labels just from the input text and generate more\nexpressive speech conditioned on the emotion embedding. In the experiment\nphase, we first validate the effectiveness of our dataset by an emotion\nclassification task. Then we train our model on the proposed dataset and\nconduct a series of subjective evaluations. Finally, by showing a comparable\nperformance in the emotional speech synthesis task, we successfully demonstrate\nthe ability of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:34:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Cui", "Chenye", ""], ["Ren", "Yi", ""], ["Liu", "Jinglin", ""], ["Chen", "Feiyang", ""], ["Huang", "Rongjie", ""], ["Lei", "Ming", ""], ["Zhao", "Zhou", ""]]}, {"id": "2106.09343", "submitter": "Dominik Mach\\'a\\v{c}ek", "authors": "Dominik Mach\\'a\\v{c}ek, Mat\\'u\\v{s} \\v{Z}ilinec, Ond\\v{r}ej Bojar", "title": "Lost in Interpreting: Speech Translation from Source or Interpreter?", "comments": "to be published at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interpreters facilitate multi-lingual meetings but the affordable set of\nlanguages is often smaller than what is needed. Automatic simultaneous speech\ntranslation can extend the set of provided languages. We investigate if such an\nautomatic system should rather follow the original speaker, or an interpreter\nto achieve better translation quality at the cost of increased delay.\n  To answer the question, we release Europarl Simultaneous Interpreting Corpus\n(ESIC), 10 hours of recordings and transcripts of European Parliament speeches\nin English, with simultaneous interpreting into Czech and German. We evaluate\nquality and latency of speaker-based and interpreter-based spoken translation\nsystems from English to Czech. We study the differences in implicit\nsimplification and summarization of the human interpreter compared to a machine\ntranslation system trained to shorten the output to some extent. Finally, we\nperform human evaluation to measure information loss of each of these\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 09:32:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mach\u00e1\u010dek", "Dominik", ""], ["\u017dilinec", "Mat\u00fa\u0161", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2106.09395", "submitter": "Xiao Liu", "authors": "Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov,\n  Yuxiao Dong, Jie Tang", "title": "A Self-supervised Method for Entity Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:22:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Xiao", ""], ["Hong", "Haoyun", ""], ["Wang", "Xinghao", ""], ["Chen", "Zeyi", ""], ["Kharlamov", "Evgeny", ""], ["Dong", "Yuxiao", ""], ["Tang", "Jie", ""]]}, {"id": "2106.09449", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Dragomir Radev, Caiming Xiong", "title": "DocNLI: A Large-scale Dataset for Document-level Natural Language\n  Inference", "comments": "ACL'21 Findings Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language inference (NLI) is formulated as a unified framework for\nsolving various NLP problems such as relation extraction, question answering,\nsummarization, etc. It has been studied intensively in the past few years\nthanks to the availability of large-scale labeled datasets. However, most\nexisting studies focus on merely sentence-level inference, which limits the\nscope of NLI's application in downstream NLP problems. This work presents\nDocNLI -- a newly-constructed large-scale dataset for document-level NLI.\nDocNLI is transformed from a broad range of NLP problems and covers multiple\ngenres of text. The premises always stay in the document granularity, whereas\nthe hypotheses vary in length from single sentences to passages with hundreds\nof words. Additionally, DocNLI has pretty limited artifacts which unfortunately\nwidely exist in some popular sentence-level NLI datasets. Our experiments\ndemonstrate that, even without fine-tuning, a model pretrained on DocNLI shows\npromising performance on popular sentence-level benchmarks, and generalizes\nwell to out-of-domain NLP tasks that rely on inference at document granularity.\nTask-specific fine-tuning can bring further improvements. Data, code, and\npretrained models can be found at https://github.com/salesforce/DocNLI.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:02:26 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yin", "Wenpeng", ""], ["Radev", "Dragomir", ""], ["Xiong", "Caiming", ""]]}, {"id": "2106.09460", "submitter": "Bharathi Raja Chakravarthi", "authors": "Bharathi Raja Chakravarthi, Ruba Priyadharshini, Vigneshwaran\n  Muralidaran, Navya Jose, Shardul Suryawanshi, Elizabeth Sherly, John P.\n  McCrae", "title": "DravidianCodeMix: Sentiment Analysis and Offensive Language\n  Identification Dataset for Dravidian Languages in Code-Mixed Text", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the development of a multilingual, manually annotated\ndataset for three under-resourced Dravidian languages generated from social\nmedia comments. The dataset was annotated for sentiment analysis and offensive\nlanguage identification for a total of more than 60,000 YouTube comments. The\ndataset consists of around 44,000 comments in Tamil-English, around 7,000\ncomments in Kannada-English, and around 20,000 comments in Malayalam-English.\nThe data was manually annotated by volunteer annotators and has a high\ninter-annotator agreement in Krippendorff's alpha. The dataset contains all\ntypes of code-mixing phenomena since it comprises user-generated content from a\nmultilingual country. We also present baseline experiments to establish\nbenchmarks on the dataset using machine learning methods. The dataset is\navailable on Github\n(https://github.com/bharathichezhiyan/DravidianCodeMix-Dataset) and Zenodo\n(https://zenodo.org/record/4750858\\#.YJtw0SYo\\_0M).\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:13:26 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Chakravarthi", "Bharathi Raja", ""], ["Priyadharshini", "Ruba", ""], ["Muralidaran", "Vigneshwaran", ""], ["Jose", "Navya", ""], ["Suryawanshi", "Shardul", ""], ["Sherly", "Elizabeth", ""], ["McCrae", "John P.", ""]]}, {"id": "2106.09462", "submitter": "Juan Manuel Perez", "authors": "Juan Manuel P\\'erez, Juan Carlos Giudici, Franco Luque", "title": "pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP\n  tasks", "comments": "4 pages, 2 tables Source code at\n  https://github.com/pysentimiento/pysentimiento/ Submitted to ASAI/JAIIO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting opinions from texts has gathered a lot of interest in the last\nyears, as we are experiencing an unprecedented volume of user-generated content\nin social networks and other places. A problem that social researchers find in\nusing opinion mining tools is that they are usually behind commercial APIs and\nunavailable for other languages than English. To address these issues, we\npresent pysentimiento, a multilingual Python toolkit for Sentiment Analysis and\nother Social NLP tasks. This open-source library brings state-of-the-art models\nfor Spanish and English in a black-box fashion, allowing researchers to easily\naccess these techniques.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:15:07 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["P\u00e9rez", "Juan Manuel", ""], ["Giudici", "Juan Carlos", ""], ["Luque", "Franco", ""]]}, {"id": "2106.09488", "submitter": "Jasha Droppo", "authors": "Jasha Droppo and Oguz Elibol", "title": "Scaling Laws for Acoustic Models", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent trend in machine learning to increase model quality by\ngrowing models to sizes previously thought to be unreasonable. Recent work has\nshown that autoregressive generative models with cross-entropy objective\nfunctions exhibit smooth power-law relationships, or scaling laws, that predict\nmodel quality from model size, training set size, and the available compute\nbudget. These scaling laws allow one to choose nearly optimal hyper-parameters\ngiven constraints on available training data, model parameter count, or\ntraining computation budget. In this paper, we demonstrate that acoustic models\ntrained with an auto-predictive coding loss behave as if they are subject to\nsimilar scaling laws. We extend previous work to jointly predict loss due to\nmodel size, to training set size, and to the inherent \"irreducible loss\" of the\ntask. We find that the scaling laws accurately match model performance over two\norders of magnitude in both model size and training set size, and make\npredictions about the limits of model performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:59:24 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Droppo", "Jasha", ""], ["Elibol", "Oguz", ""]]}, {"id": "2106.09493", "submitter": "Kartik Mehta", "authors": "Ravi Shankar Mishra, Kartik Mehta, Nikhil Rasiwasia", "title": "Scalable Approach for Normalizing E-commerce Text Attributes (SANTA)", "comments": "Accepted in ECNLP workshop of ACL-IJCNLP 2021\n  (https://sites.google.com/view/ecnlp)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present SANTA, a scalable framework to automatically\nnormalize E-commerce attribute values (e.g. \"Win 10 Pro\") to a fixed set of\npre-defined canonical values (e.g. \"Windows 10\"). Earlier works on attribute\nnormalization focused on fuzzy string matching (also referred as syntactic\nmatching in this paper). In this work, we first perform an extensive study of\nnine syntactic matching algorithms and establish that 'cosine' similarity leads\nto best results, showing 2.7% improvement over commonly used Jaccard index.\nNext, we argue that string similarity alone is not sufficient for attribute\nnormalization as many surface forms require going beyond syntactic matching\n(e.g. \"720p\" and \"HD\" are synonyms). While semantic techniques like\nunsupervised embeddings (e.g. word2vec/fastText) have shown good results in\nword similarity tasks, we observed that they perform poorly to distinguish\nbetween close canonical forms, as these close forms often occur in similar\ncontexts. We propose to learn token embeddings using a twin network with\ntriplet loss. We propose an embedding learning task leveraging raw attribute\nvalues and product titles to learn these embeddings in a self-supervised\nfashion. We show that providing supervision using our proposed task improves\nover both syntactic and unsupervised embeddings based techniques for attribute\nnormalization. Experiments on a real-world attribute normalization dataset of\n50 attributes show that the embeddings trained using our proposed approach\nobtain 2.3% improvement over best string matching and 19.3% improvement over\nbest unsupervised embeddings.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:45:56 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mishra", "Ravi Shankar", ""], ["Mehta", "Kartik", ""], ["Rasiwasia", "Nikhil", ""]]}, {"id": "2106.09502", "submitter": "Diego Garcia-Olano", "authors": "Diego Garcia-Olano, Yasumasa Onoe, Ioana Baldini, Joydeep Ghosh, Byron\n  C. Wallace, Kush R. Varshney", "title": "Biomedical Interpretable Entity Representations", "comments": "Accepted into Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models induce dense entity representations that offer\nstrong performance on entity-centric NLP tasks, but such representations are\nnot immediately interpretable. This can be a barrier to model uptake in\nimportant domains such as biomedicine. There has been recent work on general\ninterpretable representation learning (Onoe and Durrett, 2020), but these\ndomain-agnostic representations do not readily transfer to the important domain\nof biomedicine. In this paper, we create a new entity type system and training\nset from a large corpus of biomedical texts by mapping entities to concepts in\na medical ontology, and from these to Wikipedia pages whose categories are our\ntypes. From this mapping we derive Biomedical Interpretable Entity\nRepresentations(BIERs), in which dimensions correspond to fine-grained entity\ntypes, and values are predicted probabilities that a given entity is of the\ncorresponding type. We propose a novel method that exploits BIER's final sparse\nand intermediate dense representations to facilitate model and entity type\ndebugging. We show that BIERs achieve strong performance in biomedical tasks\nincluding named entity disambiguation and entity label classification, and we\nprovide error analysis to highlight the utility of their interpretability,\nparticularly in low-supervision settings. Finally, we provide our induced 68K\nbiomedical type system, the corresponding 37 million triples of derived data\nused to train BIER models and our best performing model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:52:10 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Garcia-Olano", "Diego", ""], ["Onoe", "Yasumasa", ""], ["Baldini", "Ioana", ""], ["Ghosh", "Joydeep", ""], ["Wallace", "Byron C.", ""], ["Varshney", "Kush R.", ""]]}, {"id": "2106.09532", "submitter": "Ashish Shenoy", "authors": "Ashish Shenoy, Sravan Bodapati, Katrin Kirchhoff", "title": "ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and\n  Multi-Task Language Modeling", "comments": "Accepted at ACL-IJCNLP 2021 Workshop on e-Commerce and NLP (ECNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Speech Recognition (ASR) robustness toward slot entities are\ncritical in e-commerce voice assistants that involve monetary transactions and\npurchases. Along with effective domain adaptation, it is intuitive that cross\nutterance contextual cues play an important role in disambiguating domain\nspecific content words from speech. In this paper, we investigate various\ntechniques to improve contextualization, content word robustness and domain\nadaptation of a Transformer-XL neural language model (NLM) to rescore ASR\nN-best hypotheses. To improve contextualization, we utilize turn level dialogue\nacts along with cross utterance context carry over. Additionally, to adapt our\ndomain-general NLM towards e-commerce on-the-fly, we use embeddings derived\nfrom a finetuned masked LM on in-domain data. Finally, to improve robustness\ntowards in-domain content words, we propose a multi-task model that can jointly\nperform content word detection and language modeling tasks. Compared to a\nnon-contextual LSTM LM baseline, our best performing NLM rescorer results in a\ncontent WER reduction of 19.2% on e-commerce audio test set and a slot labeling\nF1 improvement of 6.4%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 21:27:34 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Shenoy", "Ashish", ""], ["Bodapati", "Sravan", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "2106.09545", "submitter": "Sebastian P. Bayerl", "authors": "Sebastian P. Bayerl, Marc Wenninger, Jochen Schmidt, Alexander Wolff\n  von Gudenberg, Korbinian Riedhammer", "title": "STAN: A stuttering therapy analysis helper", "comments": null, "journal-ref": "Demo presented at 2021 IEEE Spoken Language Technology Workshop\n  (SLT)", "doi": null, "report-no": "https://rc.signalprocessingsociety.org/workshops/slt-2021/SLT21VID155.html?source=IBP", "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stuttering is a complex speech disorder identified by repeti-tions,\nprolongations of sounds, syllables or words and blockswhile speaking. Specific\nstuttering behaviour differs strongly,thus needing personalized therapy.\nTherapy sessions requirea high level of concentration by the therapist. We\nintroduceSTAN, a system to aid speech therapists in stuttering therapysessions.\nSuch an automated feedback system can lower thecognitive load on the therapist\nand thereby enable a more con-sistent therapy as well as allowing analysis of\nstuttering overthe span of multiple therapy sessions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:48:12 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bayerl", "Sebastian P.", ""], ["Wenninger", "Marc", ""], ["Schmidt", "Jochen", ""], ["von Gudenberg", "Alexander Wolff", ""], ["Riedhammer", "Korbinian", ""]]}, {"id": "2106.09553", "submitter": "Youssef Mroueh", "authors": "Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi,\n  Youssef Mroueh, Payel Das", "title": "Do Large Scale Molecular Language Representations Capture Important\n  Structural Information?", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting chemical properties from the structure of a molecule is of great\nimportance in many applications including drug discovery and material design.\nMachine learning based molecular property prediction holds the promise of\nenabling accurate predictions at much less complexity, when compared to, for\nexample Density Functional Theory (DFT) calculations. Features extracted from\nmolecular graphs, using graph neural nets in a supervised manner, have emerged\nas strong baselines for such tasks. However, the vast chemical space together\nwith the limited availability of labels makes supervised learning challenging,\ncalling for learning a general-purpose molecular representation. Recently,\npre-trained transformer-based language models (PTLMs) on large unlabeled corpus\nhave produced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, here we present molecular\nembeddings obtained by training an efficient transformer encoder model,\nreferred to as MoLFormer. This model was employed with a linear attention\nmechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion\nunlabeled molecules from the PubChem and ZINC datasets. Experiments show that\nthe learned molecular representation performs competitively, when compared to\nexisting graph-based and fingerprint-based supervised learning baselines, on\nthe challenging tasks of predicting properties of QM8 and QM9 molecules.\nFurther task-specific fine-tuning of the MoLFormerr representation improves\nperformance on several of those property prediction benchmarks. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient structural information to be able to accurately predict\nquantum chemical properties and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:33:55 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ross", "Jerret", ""], ["Belgodere", "Brian", ""], ["Chenthamarakshan", "Vijil", ""], ["Padhi", "Inkit", ""], ["Mroueh", "Youssef", ""], ["Das", "Payel", ""]]}, {"id": "2106.09558", "submitter": "Hongyu Lin", "authors": "Fangchao Liu, Lingyong Yan, Hongyu Lin, Xianpei Han, Le Sun", "title": "Element Intervention for Open Relation Extraction", "comments": "Accepted to ACL2021(main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open relation extraction aims to cluster relation instances referring to the\nsame underlying relation, which is a critical step for general relation\nextraction. Current OpenRE models are commonly trained on the datasets\ngenerated from distant supervision, which often results in instability and\nmakes the model easily collapsed. In this paper, we revisit the procedure of\nOpenRE from a causal view. By formulating OpenRE using a structural causal\nmodel, we identify that the above-mentioned problems stem from the spurious\ncorrelations from entities and context to the relation type. To address this\nissue, we conduct \\emph{Element Intervention}, which intervenes on the context\nand entities respectively to obtain the underlying causal effects of them. We\nalso provide two specific implementations of the interventions based on entity\nranking and context contrasting. Experimental results on unsupervised relation\nextraction datasets show that our methods outperform previous state-of-the-art\nmethods and are robust across different datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:37:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Fangchao", ""], ["Yan", "Lingyong", ""], ["Lin", "Hongyu", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""]]}, {"id": "2106.09572", "submitter": "Xiangpeng Wan", "authors": "Xiangpeng Wan, Michael C. Lucic, Hakim Ghazzai, Yehia Massoud", "title": "Topic Modeling and Progression of American Digital News Media During the\n  Onset of the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the world is in the midst of a severe global pandemic, which has\naffected all aspects of people's lives. As a result, there is a deluge of\nCOVID-related digital media articles published in the United States, due to the\ndisparate effects of the pandemic. This large volume of information is\ndifficult to consume by the audience in a reasonable amount of time. In this\npaper, we develop a Natural Language Processing (NLP) pipeline that is capable\nof automatically distilling various digital articles into manageable pieces of\ninformation, while also modelling the progression topics discussed over time in\norder to aid readers in rapidly gaining holistic perspectives on pressing\nissues (i.e., the COVID-19 pandemic) from a diverse array of sources. We\nachieve these goals by first collecting a large corpus of COVID-related\narticles during the onset of the pandemic. After, we apply unsupervised and\nsemi-supervised learning procedures to summarize articles, then cluster them\nbased on their similarities using the community detection methods. Next, we\nidentify the topic of each cluster of articles using the BART algorithm.\nFinally, we provide a detailed digital media analysis based on the NLP-pipeline\noutputs and show how the conversation surrounding COVID-19 evolved over time.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:27:47 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wan", "Xiangpeng", ""], ["Lucic", "Michael C.", ""], ["Ghazzai", "Hakim", ""], ["Massoud", "Yehia", ""]]}, {"id": "2106.09578", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu, Mark O. Riedl", "title": "Modeling Worlds in Text", "comments": "Preprint. Under review. Benchmark can be found at\n  https://github.com/JerichoWorld/JerichoWorld", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a dataset that enables the creation of learning agents that can\nbuild knowledge graph-based world models of interactive narratives. Interactive\nnarratives -- or text-adventure games -- are partially observable environments\nstructured as long puzzles or quests in which an agent perceives and interacts\nwith the world purely through textual natural language. Each individual game\ntypically contains hundreds of locations, characters, and objects -- each with\ntheir own unique descriptions -- providing an opportunity to study the problem\nof giving language-based agents the structured memory necessary to operate in\nsuch worlds. Our dataset provides 24198 mappings between rich natural language\nobservations and: (1) knowledge graphs that reflect the world state in the form\nof a map; (2) natural language actions that are guaranteed to cause a change in\nthat particular world state. The training data is collected across 27 games in\nmultiple genres and contains a further 7836 heldout instances over 9 additional\ngames in the test set. We further provide baseline models using rules-based,\nquestion-answering, and sequence learning approaches in addition to an analysis\nof the data and corresponding learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:02:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2106.09588", "submitter": "Peng Shi", "authors": "Peng Shi, Tao Yu, Patrick Ng, Zhiguo Wang", "title": "End-to-End Cross-Domain Text-to-SQL Semantic Parsing with Auxiliary Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we focus on two crucial components in the cross-domain\ntext-to-SQL semantic parsing task: schema linking and value filling. To\nencourage the model to learn better encoding ability, we propose a column\nselection auxiliary task to empower the encoder with the relevance matching\ncapability by using explicit learning targets. Furthermore, we propose two\nvalue filling methods to build the bridge from the existing zero-shot semantic\nparsers to real-world applications, considering most of the existing parsers\nignore the values filling in the synthesized SQL. With experiments on Spider,\nour proposed framework improves over the baselines on the execution accuracy\nand exact set match accuracy when database contents are unavailable, and\ndetailed analysis sheds light on future work.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:15:58 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Shi", "Peng", ""], ["Yu", "Tao", ""], ["Ng", "Patrick", ""], ["Wang", "Zhiguo", ""]]}, {"id": "2106.09589", "submitter": "Usman Naseem", "authors": "Usman Naseem, Matloob Khushi, Jinman Kim and Adam G. Dunn", "title": "Classifying vaccine sentiment tweets by modelling domain-specific\n  representation and commonsense knowledge into context-aware attentive GRU", "comments": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vaccines are an important public health measure, but vaccine hesitancy and\nrefusal can create clusters of low vaccine coverage and reduce the\neffectiveness of vaccination programs. Social media provides an opportunity to\nestimate emerging risks to vaccine acceptance by including geographical\nlocation and detailing vaccine-related concerns. Methods for classifying social\nmedia posts, such as vaccine-related tweets, use language models (LMs) trained\non general domain text. However, challenges to measuring vaccine sentiment at\nscale arise from the absence of tonal stress and gestural cues and may not\nalways have additional information about the user, e.g., past tweets or social\nconnections. Another challenge in LMs is the lack of commonsense knowledge that\nare apparent in users metadata, i.e., emoticons, positive and negative words\netc. In this study, to classify vaccine sentiment tweets with limited\ninformation, we present a novel end-to-end framework consisting of\ninterconnected components that use domain-specific LM trained on\nvaccine-related tweets and models commonsense knowledge into a bidirectional\ngated recurrent network (CK-BiGRU) with context-aware attention. We further\nleverage syntactical, user metadata and sentiment information to capture the\nsentiment of a tweet. We experimented using two popular vaccine-related Twitter\ndatasets and demonstrate that our proposed approach outperforms\nstate-of-the-art models in identifying pro-vaccine, anti-vaccine and neutral\ntweets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:16:08 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Naseem", "Usman", ""], ["Khushi", "Matloob", ""], ["Kim", "Jinman", ""], ["Dunn", "Adam G.", ""]]}, {"id": "2106.09608", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu, Mark O. Riedl", "title": "Learning Knowledge Graph-based World Models of Textual Environments", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World models improve a learning agent's ability to efficiently operate in\ninteractive and situated environments. This work focuses on the task of\nbuilding world models of text-based game environments. Text-based games, or\ninteractive narratives, are reinforcement learning environments in which agents\nperceive and interact with the world using textual natural language. These\nenvironments contain long, multi-step puzzles or quests woven through a world\nthat is filled with hundreds of characters, locations, and objects. Our world\nmodel learns to simultaneously: (1) predict changes in the world caused by an\nagent's actions when representing the world as a knowledge graph; and (2)\ngenerate the set of contextually relevant natural language actions required to\noperate in the world. We frame this task as a Set of Sequences generation\nproblem by exploiting the inherent structure of knowledge graphs and actions\nand introduce both a transformer-based multi-task architecture and a loss\nfunction to train it. A zero-shot ablation study on never-before-seen textual\nworlds shows that our methodology significantly outperforms existing textual\nworld modeling techniques as well as the importance of each of our\ncontributions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:45:54 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2106.09650", "submitter": "Liyuan Liu", "authors": "Liyuan Liu and Jialu Liu and Jiawei Han", "title": "Multi-head or Single-head? An Empirical Comparison for Transformer\n  Training", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-head attention plays a crucial role in the recent success of\nTransformer models, which leads to consistent performance improvements over\nconventional attention in various applications. The popular belief is that this\neffectiveness stems from the ability of jointly attending multiple positions.\nIn this paper, we first demonstrate that jointly attending multiple positions\nis not a unique feature of multi-head attention, as multi-layer single-head\nattention also attends multiple positions and is more effective. Then, we\nsuggest the main advantage of the multi-head attention is the training\nstability, since it has less number of layers than the single-head attention,\nwhen attending the same number of positions. For example, 24-layer 16-head\nTransformer (BERT-large) and 384-layer single-head Transformer has the same\ntotal attention head number and roughly the same model size, while the\nmulti-head one is significantly shallower. Meanwhile, we show that, with recent\nadvances in deep learning, we can successfully stabilize the training of the\n384-layer Transformer. As the training difficulty is no longer a bottleneck,\nsubstantially deeper single-head Transformer achieves consistent performance\nimprovements without tuning hyper-parameters.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 16:53:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Liyuan", ""], ["Liu", "Jialu", ""], ["Han", "Jiawei", ""]]}, {"id": "2106.09685", "submitter": "Edward J. Hu", "authors": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\n  Li, Shean Wang, Weizhu Chen", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, conventional fine-tuning, which\nretrains all model parameters, becomes less feasible. Using GPT-3 175B as an\nexample, deploying many independent instances of fine-tuned models, each with\n175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. For GPT-3,\nLoRA can reduce the number of trainable parameters by 10,000 times and the\ncomputation hardware requirement by 3 times compared to full fine-tuning. LoRA\nperforms on-par or better than fine-tuning in model quality on both GPT-3 and\nGPT-2, despite having fewer trainable parameters, a higher training throughput,\nand no additional inference latency. We also provide an empirical investigation\ninto rank-deficiency in language model adaptations, which sheds light on the\nefficacy of LoRA. We release our implementation in GPT-2 at\nhttps://github.com/microsoft/LoRA .\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:37:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hu", "Edward J.", ""], ["Shen", "Yelong", ""], ["Wallis", "Phillip", ""], ["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Wang", "Shean", ""], ["Chen", "Weizhu", ""]]}, {"id": "2106.09700", "submitter": "Rahul Nadkarni", "authors": "Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A. Smith, Hannaneh\n  Hajishirzi, Tom Hope", "title": "Scientific Language Models for Biomedical Knowledge Base Completion: An\n  Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical knowledge graphs (KGs) hold rich information on entities such as\ndiseases, drugs, and genes. Predicting missing links in these graphs can boost\nmany important applications, such as drug design and repurposing. Recent work\nhas shown that general-domain language models (LMs) can serve as \"soft\" KGs,\nand that they can be fine-tuned for the task of KG completion. In this work, we\nstudy scientific LMs for KG completion, exploring whether we can tap into their\nlatent knowledge to enhance biomedical link prediction. We evaluate several\ndomain-specific LMs, fine-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions.\nWe integrate the LM-based models with KG embedding models, using a router\nmethod that learns to assign each input example to either type of model and\nprovides a substantial boost in performance. Finally, we demonstrate the\nadvantage of LM models in the inductive setting with novel scientific entities.\nOur datasets and code are made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:55:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Nadkarni", "Rahul", ""], ["Wadden", "David", ""], ["Beltagy", "Iz", ""], ["Smith", "Noah A.", ""], ["Hajishirzi", "Hannaneh", ""], ["Hope", "Tom", ""]]}, {"id": "2106.09760", "submitter": "Kwangyoun Kim", "authors": "Kwangyoun Kim, Felix Wu, Prashant Sridhar, Kyu J. Han, Shinji Watanabe", "title": "Multi-mode Transformer Transducer with Stochastic Future Context", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) models make fewer errors when more\nsurrounding speech information is presented as context. Unfortunately,\nacquiring a larger future context leads to higher latency. There exists an\ninevitable trade-off between speed and accuracy. Naively, to fit different\nlatency requirements, people have to store multiple models and pick the best\none under the constraints. Instead, a more desirable approach is to have a\nsingle model that can dynamically adjust its latency based on different\nconstraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can\nfulfill various latency requirements during inference -- when a larger latency\nbecomes acceptable, the model can process longer future context to achieve\nhigher accuracy and when a latency budget is not flexible, the model can be\nless dependent on future context but still achieve reliable accuracy. In\npursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple\ntraining procedure that samples one streaming configuration in each iteration.\nThrough extensive experiments on AISHELL-1 and LibriSpeech datasets, we show\nthat a Multi-mode ASR model rivals, if not surpasses, a set of competitive\nstreaming baselines trained with different latency budgets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:42:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kim", "Kwangyoun", ""], ["Wu", "Felix", ""], ["Sridhar", "Prashant", ""], ["Han", "Kyu J.", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2106.09775", "submitter": "Md Mustafizur Rahman", "authors": "Md Mustafizur Rahman, Dinesh Balakrishnan, Dhiraj Murthy, Mucahid\n  Kutlu, Matthew Lease", "title": "An Information Retrieval Approach to Building Datasets for Hate Speech\n  Detection", "comments": "10 pages (Under review in CIKM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building a benchmark dataset for hate speech detection presents several\nchallenges. Firstly, because hate speech is relatively rare -- e.g., less than\n3\\% of Twitter posts are hateful \\citep{founta2018large} -- random sampling of\ntweets to annotate is inefficient in capturing hate speech. A common practice\nis to only annotate tweets containing known ``hate words'', but this risks\nyielding a biased benchmark that only partially captures the real-world\nphenomenon of interest. A second challenge is that definitions of hate speech\ntend to be highly variable and subjective. Annotators having diverse prior\nnotions of hate speech may not only disagree with one another but also struggle\nto conform to specified labeling guidelines. Our key insight is that the rarity\nand subjectivity of hate speech are akin to that of relevance in information\nretrieval (IR). This connection suggests that well-established methodologies\nfor creating IR test collections might also be usefully applied to create\nbetter benchmark datasets for hate speech detection. Firstly, to intelligently\nand efficiently select which tweets to annotate, we apply established IR\ntechniques of {\\em pooling} and {\\em active learning}. Secondly, to improve\nboth consistency and value of annotations, we apply {\\em task decomposition}\n\\cite{Zhang-sigir14} and {\\em annotator rationale} \\cite{mcdonnell16-hcomp}\ntechniques. Using the above techniques, we create and share a new benchmark\ndataset\\footnote{We will release the dataset upon publication.} for hate speech\ndetection with broader coverage than prior datasets. We also show a dramatic\ndrop in accuracy of existing detection models when tested on these broader\nforms of hate. Collected annotator rationales not only provide documented\nsupport for labeling decisions but also create exciting future work\nopportunities for dual-supervision and/or explanation generation in modeling.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 19:25:39 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 00:45:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rahman", "Md Mustafizur", ""], ["Balakrishnan", "Dinesh", ""], ["Murthy", "Dhiraj", ""], ["Kutlu", "Mucahid", ""], ["Lease", "Matthew", ""]]}, {"id": "2106.09790", "submitter": "Elsbeth Turcan", "authors": "Elsbeth Turcan, Shuai Wang, Rishita Anubhai, Kasturi Bhattacharjee,\n  Yaser Al-Onaizan, Smaranda Muresan", "title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause\n  Extraction", "comments": "15 pages, 6 figures. Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting what emotions are expressed in text is a well-studied problem in\nnatural language processing. However, research on finer grained emotion\nanalysis such as what causes an emotion is still in its infancy. We present\nsolutions that tackle both emotion recognition and emotion cause detection in a\njoint fashion. Considering that common-sense knowledge plays an important role\nin understanding implicitly expressed emotions and the reasons for those\nemotions, we propose novel methods that combine common-sense knowledge via\nadapted knowledge models with multi-task learning to perform joint emotion\nclassification and emotion cause tagging. We show performance improvement on\nboth tasks when including common-sense reasoning and a multitask framework. We\nprovide a thorough analysis to gain insights into model performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:11:04 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Turcan", "Elsbeth", ""], ["Wang", "Shuai", ""], ["Anubhai", "Rishita", ""], ["Bhattacharjee", "Kasturi", ""], ["Al-Onaizan", "Yaser", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2106.09795", "submitter": "Sairam Gurajada", "authors": "Hang Jiang, Sairam Gurajada, Qiuhao Lu, Sumit Neelam, Lucian Popa,\n  Prithviraj Sen, Yunyao Li, Alexander Gray", "title": "LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking (EL), the task of disambiguating mentions in text by linking\nthem to entities in a knowledge graph, is crucial for text understanding,\nquestion answering or conversational systems. Entity linking on short text\n(e.g., single sentence or question) poses particular challenges due to limited\ncontext. While prior approaches use either heuristics or black-box neural\nmethods, here we propose LNN-EL, a neuro-symbolic approach that combines the\nadvantages of using interpretable rules based on first-order logic with the\nperformance of neural learning. Even though constrained to using rules, LNN-EL\nperforms competitively against SotA black-box neural approaches, with the added\nbenefits of extensibility and transferability. In particular, we show that we\ncan easily blend existing rule templates given by a human expert, with multiple\ntypes of features (priors, BERT encodings, box embeddings, etc), and even\nscores resulting from previous EL methods, thus improving on such methods. For\ninstance, on the LC-QuAD-1.0 dataset, we show more than $4$\\% increase in F1\nscore over previous SotA. Finally, we show that the inductive bias offered by\nusing logic results in learned rules that transfer well across datasets, even\nwithout fine tuning, while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:22:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Jiang", "Hang", ""], ["Gurajada", "Sairam", ""], ["Lu", "Qiuhao", ""], ["Neelam", "Sumit", ""], ["Popa", "Lucian", ""], ["Sen", "Prithviraj", ""], ["Li", "Yunyao", ""], ["Gray", "Alexander", ""]]}, {"id": "2106.09889", "submitter": "Lin Su", "authors": "Lin Su and Nan Duan and Edward Cui and Lei Ji and Chenfei Wu and\n  Huaishao Luo and Yongfei Liu and Ming Zhong and Taroon Bharti and Arun\n  Sacheti", "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:14:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Su", "Lin", ""], ["Duan", "Nan", ""], ["Cui", "Edward", ""], ["Ji", "Lei", ""], ["Wu", "Chenfei", ""], ["Luo", "Huaishao", ""], ["Liu", "Yongfei", ""], ["Zhong", "Ming", ""], ["Bharti", "Taroon", ""], ["Sacheti", "Arun", ""]]}, {"id": "2106.09895", "submitter": "Hengyi Zheng", "authors": "Hengyi Zheng, Rui Wen, Xi Chen, Yifan Yang, Yunyan Zhang, Ziheng\n  Zhang, Ningyu Zhang, Bin Qin, Ming Xu, Yefeng Zheng", "title": "PRGC: Potential Relation and Global Correspondence Based Joint\n  Relational Triple Extraction", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Joint extraction of entities and relations from unstructured texts is a\ncrucial task in information extraction. Recent methods achieve considerable\nperformance but still suffer from some inherent limitations, such as redundancy\nof relation prediction, poor generalization of span-based extraction and\ninefficiency. In this paper, we decompose this task into three subtasks,\nRelation Judgement, Entity Extraction and Subject-object Alignment from a novel\nperspective and then propose a joint relational triple extraction framework\nbased on Potential Relation and Global Correspondence (PRGC). Specifically, we\ndesign a component to predict potential relations, which constrains the\nfollowing entity extraction to the predicted relation subset rather than all\nrelations; then a relation-specific sequence tagging component is applied to\nhandle the overlapping problem between subjects and objects; finally, a global\ncorrespondence component is designed to align the subject and object into a\ntriple with low-complexity. Extensive experiments show that PRGC achieves\nstate-of-the-art performance on public benchmarks with higher efficiency and\ndelivers consistent performance gain on complex scenarios of overlapping\ntriples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:38:07 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zheng", "Hengyi", ""], ["Wen", "Rui", ""], ["Chen", "Xi", ""], ["Yang", "Yifan", ""], ["Zhang", "Yunyan", ""], ["Zhang", "Ziheng", ""], ["Zhang", "Ningyu", ""], ["Qin", "Bin", ""], ["Xu", "Ming", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2106.09896", "submitter": "Lingzhi Wang", "authors": "Lingzhi Wang, Jing Li, Xingshan Zeng, Haisong Zhang, Kam-Fai Wong", "title": "Continuity of Topic, Interaction, and Query: Learning to Quote in Online\n  Conversations", "comments": "Accepted by EMNLP 2020, updated with dataset link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quotations are crucial for successful explanations and persuasions in\ninterpersonal communications. However, finding what to quote in a conversation\nis challenging for both humans and machines. This work studies automatic\nquotation generation in an online conversation and explores how language\nconsistency affects whether a quotation fits the given context. Here, we\ncapture the contextual consistency of a quotation in terms of latent topics,\ninteractions with the dialogue history, and coherence to the query turn's\nexisting content. Further, an encoder-decoder neural framework is employed to\ncontinue the context with a quotation via language generation. Experiment\nresults on two large-scale datasets in English and Chinese demonstrate that our\nquotation generation model outperforms the state-of-the-art models. Further\nanalysis shows that topic, interaction, and query consistency are all helpful\nto learn how to quote in online conversations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:38:48 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wang", "Lingzhi", ""], ["Li", "Jing", ""], ["Zeng", "Xingshan", ""], ["Zhang", "Haisong", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "2106.09898", "submitter": "Nicholas Boucher", "authors": "Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot", "title": "Bad Characters: Imperceptible NLP Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook\nand IBM. This novel series of attacks presents a significant threat to many\nlanguage processing systems: an attacker can affect systems in a targeted\nmanner without any assumptions about the underlying model. We conclude that\ntext-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:42:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Boucher", "Nicholas", ""], ["Shumailov", "Ilia", ""], ["Anderson", "Ross", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2106.09900", "submitter": "Kohei Makino", "authors": "Kohei Makino, Makoto Miwa, Yutaka Sasaki", "title": "A Neural Edge-Editing Approach for Document-Level Relation Graph\n  Extraction", "comments": "Accepted for publication at the Findings of the Association for\n  Computational Linguistics (Findings-ACL2021), 2021. 10 pages, 6 figures, 8\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel edge-editing approach to extract relation\ninformation from a document. We treat the relations in a document as a relation\ngraph among entities in this approach. The relation graph is iteratively\nconstructed by editing edges of an initial graph, which might be a graph\nextracted by another system or an empty graph. The way to edit edges is to\nclassify them in a close-first manner using the document and\ntemporally-constructed graph information; each edge is represented with a\ndocument context information by a pretrained transformer model and a graph\ncontext information by a graph convolutional neural network model. We evaluate\nour approach on the task to extract material synthesis procedures from\nmaterials science texts. The experimental results show the effectiveness of our\napproach in editing the graphs initialized by our in-house rule-based system\nand empty graphs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:46:49 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Makino", "Kohei", ""], ["Miwa", "Makoto", ""], ["Sasaki", "Yutaka", ""]]}, {"id": "2106.09929", "submitter": "Jingli Shi", "authors": "Jingli Shi, Weihua Li, Sira Yongchareon, Yi Yang and Quan Bai", "title": "Graph-based Joint Pandemic Concern and Relation Extraction on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public concern detection provides potential guidance to the authorities for\ncrisis management before or during a pandemic outbreak. Detecting people's\nconcerns and attention from online social media platforms has been widely\nacknowledged as an effective approach to relieve public panic and prevent a\nsocial crisis. However, detecting concerns in time from massive information in\nsocial media turns out to be a big challenge, especially when sufficient\nmanually labeled data is in the absence of public health emergencies, e.g.,\nCOVID-19. In this paper, we propose a novel end-to-end deep learning model to\nidentify people's concerns and the corresponding relations based on Graph\nConvolutional Network and Bi-directional Long Short Term Memory integrated with\nConcern Graph. Except for the sequential features from BERT embeddings, the\nregional features of tweets can be extracted by the Concern Graph module, which\nnot only benefits the concern detection but also enables our model to be high\nnoise-tolerant. Thus, our model can address the issue of insufficient manually\nlabeled data. We conduct extensive experiments to evaluate the proposed model\nby using both manually labeled tweets and automatically labeled tweets. The\nexperimental results show that our model can outperform the state-of-art models\non real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:06:35 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Shi", "Jingli", ""], ["Li", "Weihua", ""], ["Yongchareon", "Sira", ""], ["Yang", "Yi", ""], ["Bai", "Quan", ""]]}, {"id": "2106.09943", "submitter": "Dipendra Misra", "authors": "Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy and Dipendra Misra", "title": "Investigating the Role of Negatives in Contrastive Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise contrastive learning is a popular technique for unsupervised\nrepresentation learning. In this approach, a representation is obtained via\nreduction to supervised learning, where given a notion of semantic similarity,\nthe learner tries to distinguish a similar (positive) example from a collection\nof random (negative) examples. The success of modern contrastive learning\npipelines relies on many parameters such as the choice of data augmentation,\nthe number of negative examples, and the batch size; however, there is limited\nunderstanding as to how these parameters interact and affect downstream\nperformance. We focus on disambiguating the role of one of these parameters:\nthe number of negative examples. Theoretically, we show the existence of a\ncollision-coverage trade-off suggesting that the optimal number of negative\nexamples should scale with the number of underlying concepts in the data.\nEmpirically, we scrutinize the role of the number of negatives in both NLP and\nvision tasks. In the NLP task, we find that the results broadly agree with our\ntheory, while our vision experiments are murkier with performance sometimes\neven being insensitive to the number of negatives. We discuss plausible\nexplanations for this behavior and suggest future directions to better align\ntheory and practice.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:44:16 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ash", "Jordan T.", ""], ["Goel", "Surbhi", ""], ["Krishnamurthy", "Akshay", ""], ["Misra", "Dipendra", ""]]}, {"id": "2106.09983", "submitter": "Yeon Seonwoo", "authors": "Yeon Seonwoo, Sang-Woo Lee, Ji-Hoon Kim, Jung-Woo Ha, Alice Oh", "title": "Weakly Supervised Pre-Training for Multi-Hop Retriever", "comments": "ACL-Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-hop QA, answering complex questions entails iterative document\nretrieval for finding the missing entity of the question. The main steps of\nthis process are sub-question detection, document retrieval for the\nsub-question, and generation of a new query for the final document retrieval.\nHowever, building a dataset that contains complex questions with sub-questions\nand their corresponding documents requires costly human annotation. To address\nthe issue, we propose a new method for weakly supervised multi-hop retriever\npre-training without human efforts. Our method includes 1) a pre-training task\nfor generating vector representations of complex questions, 2) a scalable data\ngeneration method that produces the nested structure of question and\nsub-question as weak supervision for pre-training, and 3) a pre-training model\nstructure based on dense encoders. We conduct experiments to compare the\nperformance of our pre-trained retriever with several state-of-the-art models\non end-to-end multi-hop QA as well as document retrieval. The experimental\nresults show that our pre-trained retriever is effective and also robust on\nlimited data and computational resources.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:06:02 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Seonwoo", "Yeon", ""], ["Lee", "Sang-Woo", ""], ["Kim", "Ji-Hoon", ""], ["Ha", "Jung-Woo", ""], ["Oh", "Alice", ""]]}, {"id": "2106.09997", "submitter": "Hieu Tran Trung", "authors": "Hieu Tran, Long Phan, James Anibal, Binh T. Nguyen, and Truong-Son\n  Nguyen", "title": "SPBERT: An Efficient Pre-training BERT on SPARQL Queries for Question\n  Answering over Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose SPBERT, a transformer-based language model\npre-trained on massive SPARQL query logs. By incorporating masked language\nmodeling objectives and the word structural objective, SPBERT can learn\ngeneral-purpose representations in both natural language and SPARQL query\nlanguage. We investigate how SPBERT and encoder-decoder architecture can be\nadapted for Knowledge-based QA corpora. We conduct exhaustive experiments on\ntwo additional tasks, including SPARQL Query Construction and Answer\nVerbalization Generation. The experimental results show that SPBERT can obtain\npromising results, achieving state-of-the-art BLEU scores on several of these\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:39:26 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 18:31:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tran", "Hieu", ""], ["Phan", "Long", ""], ["Anibal", "James", ""], ["Nguyen", "Binh T.", ""], ["Nguyen", "Truong-Son", ""]]}, {"id": "2106.10002", "submitter": "Prasanna Raj Noel Dabre", "authors": "Raj Dabre and Atsushi Fujita", "title": "Recurrent Stacking of Layers in Neural Networks: An Application to\n  Neural Machine Translation", "comments": "22 pages. Under review. Work in progress. Extended version of\n  https://ojs.aaai.org//index.php/AAAI/article/view/4590 which is an extension\n  of arXiv:1807.05353 . The focus is on analyzing the limitations of\n  recurrently stacked layers and methods to overcome said limitations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In deep neural network modeling, the most common practice is to stack a\nnumber of recurrent, convolutional, or feed-forward layers in order to obtain\nhigh-quality continuous space representations which in turn improves the\nquality of the network's prediction. Conventionally, each layer in the stack\nhas its own parameters which leads to a significant increase in the number of\nmodel parameters. In this paper, we propose to share parameters across all\nlayers thereby leading to a recurrently stacked neural network model. We report\non an extensive case study on neural machine translation (NMT), where we apply\nour proposed method to an encoder-decoder based neural network model, i.e., the\nTransformer model, and experiment with three Japanese--English translation\ndatasets. We empirically demonstrate that the translation quality of a model\nthat recurrently stacks a single layer 6 times, despite having significantly\nfewer parameters, approaches that of a model that stacks 6 layers where each\nlayer has different parameters. We also explore the limits of recurrent\nstacking where we train extremely deep NMT models. This paper also examines the\nutility of our recurrently stacked model as a student model through transfer\nlearning via leveraging pre-trained parameters and knowledge distillation, and\nshows that it compensates for the performance drops in translation quality that\nthe direct training of recurrently stacked model brings. We also show how\ntransfer learning helps in faster decoding on top of the already reduced number\nof parameters due to recurrent stacking. Finally, we analyze the effects of\nrecurrently stacked layers by visualizing the attentions of models that use\nrecurrently stacked layers and models that do not.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:48:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Dabre", "Raj", ""], ["Fujita", "Atsushi", ""]]}, {"id": "2106.10004", "submitter": "Vukosi Marivate", "authors": "Michelle Terblanche and Vukosi Marivate", "title": "Towards Financial Sentiment Analysis in a South African Landscape", "comments": "Accepted for publication in Proceedings of CD-MAKE 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis as a sub-field of natural language processing has received\nincreased attention in the past decade enabling organisations to more\neffectively manage their reputation through online media monitoring. Many\ndrivers impact reputation, however, this thesis focuses only the aspect of\nfinancial performance and explores the gap with regards to financial sentiment\nanalysis in a South African context. Results showed that pre-trained sentiment\nanalysers are least effective for this task and that traditional lexicon-based\nand machine learning approaches are best suited to predict financial sentiment\nof news articles. The evaluated methods produced accuracies of 84\\%-94\\%. The\npredicted sentiments correlated quite well with share price and highlighted the\npotential use of sentiment as an indicator of financial performance. A main\ncontribution of the study was updating an existing sentiment dictionary for\nfinancial sentiment analysis. Model generalisation was less acceptable due to\nthe limited amount of training data used. Future work includes expanding the\ndata set to improve general usability and contribute to an open-source\nfinancial sentiment analyser for South African data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:48:47 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Terblanche", "Michelle", ""], ["Marivate", "Vukosi", ""]]}, {"id": "2106.10045", "submitter": "Cong Zhang", "authors": "Cong Zhang, Jian Zhu", "title": "Synchronising speech segments with musical beats in Mandarin and English\n  singing", "comments": "To be published in the Proceeding of Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generating synthesised singing voice with models trained on speech data has\nmany advantages due to the models' flexibility and controllability. However,\nsince the information about the temporal relationship between segments and\nbeats are lacking in speech training data, the synthesised singing may sound\noff-beat at times. Therefore, the availability of the information on the\ntemporal relationship between speech segments and music beats is crucial. The\ncurrent study investigated the segment-beat synchronisation in singing data,\nwith hypotheses formed based on the linguistics theories of P-centre and\nsonority hierarchy. A Mandarin corpus and an English corpus of professional\nsinging data were manually annotated and analysed. The results showed that the\npresence of musical beats was more dependent on segment duration than sonority.\nHowever, the sonority hierarchy and the P-centre theory were highly related to\nthe location of beats. Mandarin and English demonstrated cross-linguistic\nvariations despite exhibiting common patterns.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:32:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhang", "Cong", ""], ["Zhu", "Jian", ""]]}, {"id": "2106.10076", "submitter": "Rui Song", "authors": "Rui Song, Xingbing Chen, Zelong Liu, Haining An, Zhiqi Zhang,\n  Xiaoguang Wang, Hao Xu", "title": "Label Mask for Multi-Label Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "21", "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the key problems in multi-label text classification is how to take\nadvantage of the correlation among labels. However, it is very challenging to\ndirectly model the correlations among labels in a complex and unknown label\nspace. In this paper, we propose a Label Mask multi-label text classification\nmodel (LM-MTC), which is inspired by the idea of cloze questions of language\nmodel. LM-MTC is able to capture implicit relationships among labels through\nthe powerful ability of pre-train language models. On the basis, we assign a\ndifferent token to each potential label, and randomly mask the token with a\ncertain probability to build a label based Masked Language Model (MLM). We\ntrain the MTC and MLM together, further improving the generalization ability of\nthe model. A large number of experiments on multiple datasets demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:54:33 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Song", "Rui", ""], ["Chen", "Xingbing", ""], ["Liu", "Zelong", ""], ["An", "Haining", ""], ["Zhang", "Zhiqi", ""], ["Wang", "Xiaoguang", ""], ["Xu", "Hao", ""]]}, {"id": "2106.10084", "submitter": "Wei Liu", "authors": "Lei Li, Wei Liu, Marina Litvak, Natalia Vanetik, Jiacheng Pei, Yinan\n  Liu, Siya Qi", "title": "Subjective Bias in Abstractive Summarization", "comments": "10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the subjectivity of the summarization, it is a good practice to have\nmore than one gold summary for each training document. However, many modern\nlarge-scale abstractive summarization datasets have only one-to-one samples\nwritten by different human with different styles. The impact of this phenomenon\nis understudied. We formulate the differences among possible multiple\nexpressions summarizing the same content as subjective bias and examine the\nrole of this bias in the context of abstractive summarization. In this paper a\nlightweight and effective method to extract the feature embeddings of\nsubjective styles is proposed. Results of summarization models trained on\nstyle-clustered datasets show that there are certain types of styles that lead\nto better convergence, abstraction and generalization. The reproducible code\nand generated summaries are available online.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:17:55 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Lei", ""], ["Liu", "Wei", ""], ["Litvak", "Marina", ""], ["Vanetik", "Natalia", ""], ["Pei", "Jiacheng", ""], ["Liu", "Yinan", ""], ["Qi", "Siya", ""]]}, {"id": "2106.10123", "submitter": "Vivek Srivastava", "authors": "Vivek Srivastava, Mayank Singh", "title": "Challenges and Limitations with the Metrics Measuring the Complexity of\n  Code-Mixed Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-mixing is a frequent communication style among multilingual speakers\nwhere they mix words and phrases from two different languages in the same\nutterance of text or speech. Identifying and filtering code-mixed text is a\nchallenging task due to its co-existence with monolingual and noisy text. Over\nthe years, several code-mixing metrics have been extensively used to identify\nand validate code-mixed text quality. This paper demonstrates several inherent\nlimitations of code-mixing metrics with examples from the already existing\ndatasets that are popularly used across various experiments.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:26:48 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Srivastava", "Vivek", ""], ["Singh", "Mayank", ""]]}, {"id": "2106.10127", "submitter": "Disong Wang", "authors": "Disong Wang, Liqun Deng, Yu Ting Yeung, Xiao Chen, Xunying Liu, Helen\n  Meng", "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via\n  Domain Adversarial Training and Mutual Information Minimization", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dysarthric speech detection (DSD) systems aim to detect characteristics of\nthe neuromotor disorder from speech. Such systems are particularly susceptible\nto domain mismatch where the training and testing data come from the source and\ntarget domains respectively, but the two domains may differ in terms of speech\nstimuli, disease etiology, etc. It is hard to acquire labelled data in the\ntarget domain, due to high costs of annotating sizeable datasets. This paper\nmakes a first attempt to formulate cross-domain DSD as an unsupervised domain\nadaptation (UDA) problem. We use labelled source-domain data and unlabelled\ntarget-domain data, and propose a multi-task learning strategy, including\ndysarthria presence classification (DPC), domain adversarial training (DAT) and\nmutual information minimization (MIM), which aim to learn\ndysarthria-discriminative and domain-invariant biomarker embeddings.\nSpecifically, DPC helps biomarker embeddings capture critical indicators of\ndysarthria; DAT forces biomarker embeddings to be indistinguishable in source\nand target domains; and MIM further reduces the correlation between biomarker\nembeddings and domain-related cues. By treating the UASPEECH and TORGO corpora\nrespectively as the source and target domains, experiments show that the\nincorporation of UDA attains absolute increases of 22.2% and 20.0% respectively\nin utterance-level weighted average recall and speaker-level accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:34:36 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wang", "Disong", ""], ["Deng", "Liqun", ""], ["Yeung", "Yu Ting", ""], ["Chen", "Xiao", ""], ["Liu", "Xunying", ""], ["Meng", "Helen", ""]]}, {"id": "2106.10131", "submitter": "Georgi V. Georgiev Dr", "authors": "Georgi V. Georgiev, Danko D. Georgiev", "title": "Enhancing user creativity: Semantic measures for idea generation", "comments": "42 pages, 10 figures, 2 tables", "journal-ref": "Knowledge-Based Systems 2018; 151: 1-15", "doi": "10.1016/j.knosys.2018.03.016", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human creativity generates novel ideas to solve real-world problems. This\nthereby grants us the power to transform the surrounding world and extend our\nhuman attributes beyond what is currently possible. Creative ideas are not just\nnew and unexpected, but are also successful in providing solutions that are\nuseful, efficient and valuable. Thus, creativity optimizes the use of available\nresources and increases wealth. The origin of human creativity, however, is\npoorly understood, and semantic measures that could predict the success of\ngenerated ideas are currently unknown. Here, we analyze a dataset of design\nproblem-solving conversations in real-world settings by using 49 semantic\nmeasures based on WordNet 3.1 and demonstrate that a divergence of semantic\nsimilarity, an increased information content, and a decreased polysemy predict\nthe success of generated ideas. The first feedback from clients also enhances\ninformation content and leads to a divergence of successful ideas in creative\nproblem solving. These results advance cognitive science by identifying\nreal-world processes in human problem solving that are relevant to the success\nof produced solutions and provide tools for real-time monitoring of problem\nsolving, student training and skill acquisition. A selected subset of\ninformation content (IC S\\'anchez-Batet) and semantic similarity\n(Lin/S\\'anchez-Batet) measures, which are both statistically powerful and\ncomputationally fast, could support the development of technologies for\ncomputer-assisted enhancements of human creativity or for the implementation of\ncreativity in machines endowed with general artificial intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:47:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Georgiev", "Georgi V.", ""], ["Georgiev", "Danko D.", ""]]}, {"id": "2106.10132", "submitter": "Disong Wang", "authors": "Disong Wang, Liqun Deng, Yu Ting Yeung, Xiao Chen, Xunying Liu, Helen\n  Meng", "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised\n  Speech Representation Disentanglement for One-shot Voice Conversion", "comments": "Accepted to Interspeech 2021. Code, pre-trained models and demo are\n  available at https://github.com/Wendison/VQMIVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.MM cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot voice conversion (VC), which performs conversion across arbitrary\nspeakers with only a single target-speaker utterance for reference, can be\neffectively achieved by speech representation disentanglement. Existing work\ngenerally ignores the correlation between different speech representations\nduring training, which causes leakage of content information into the speaker\nrepresentation and thus degrades VC performance. To alleviate this issue, we\nemploy vector quantization (VQ) for content encoding and introduce mutual\ninformation (MI) as the correlation metric during training, to achieve proper\ndisentanglement of content, speaker and pitch representations, by reducing\ntheir inter-dependencies in an unsupervised manner. Experimental results\nreflect the superiority of the proposed method in learning effective\ndisentangled speech representations for retaining source linguistic content and\nintonation variations, while capturing target speaker characteristics. In doing\nso, the proposed approach achieves higher speech naturalness and speaker\nsimilarity than current state-of-the-art one-shot VC systems. Our code,\npre-trained models and demo are available at\nhttps://github.com/Wendison/VQMIVC.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:50:38 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Disong", ""], ["Deng", "Liqun", ""], ["Yeung", "Yu Ting", ""], ["Chen", "Xiao", ""], ["Liu", "Xunying", ""], ["Meng", "Helen", ""]]}, {"id": "2106.10156", "submitter": "Rosana Rego", "authors": "Rosana C. B. Rego, Ver\\^onica M. L. Silva", "title": "Predicting gender of Brazilian names using deep learning", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting gender by the name is not a simple task. In many applications,\nespecially in the natural language processing (NLP) field, this task may be\nnecessary, mainly when considering foreign names. Some machine learning\nalgorithms can satisfactorily perform the prediction. In this paper, we\nexamined and implemented feedforward and recurrent deep neural network models,\nsuch as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first\nname. A dataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, and confusion matrix to measure the\nmodels' performances. The results indicate that the gender prediction can be\nperformed from the feature extraction strategy looking at the names as a set of\nstrings. Some models accurately predict the gender in more than 90% of the\ncases. The recurrent models overcome the feedforward models in this binary\nclassification problem.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:45:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Rego", "Rosana C. B.", ""], ["Silva", "Ver\u00f4nica M. L.", ""]]}, {"id": "2106.10169", "submitter": "Ruirui Li", "authors": "Ruirui Li, Chelsea J.-T. Ju, Zeya Chen, Hongda Mao, Oguz Elibol,\n  Andreas Stolcke", "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent\n  and Independent Speaker Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether the speech\ncontent is constrained or not, both text-dependent (TD) and text-independent\n(TI) speaker recognition models may be used. We wish to combine the advantages\nof both types of models through an ensemble system to make more reliable\npredictions. However, any such combined approach has to be robust to incomplete\ninputs, i.e., when either TD or TI input is missing. As a solution we propose a\nfusion of embeddings network foenet architecture, combining joint learning with\nneural attention. We compare foenet with four competitive baseline methods on a\ndataset of voice assistant inputs, and show that it achieves higher accuracy\nthan the baseline and score fusion methods, especially in the presence of\nincomplete inputs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:01:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Ruirui", ""], ["Ju", "Chelsea J. -T.", ""], ["Chen", "Zeya", ""], ["Mao", "Hongda", ""], ["Elibol", "Oguz", ""], ["Stolcke", "Andreas", ""]]}, {"id": "2106.10199", "submitter": "Elad Ben Zaken", "authors": "Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based\n  Masked Language-models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that with small-to-medium training data, fine-tuning only the bias\nterms (or a subset of the bias terms) of pre-trained BERT models is competitive\nwith (and sometimes better than) fine-tuning the entire model. For larger data,\nbias-only fine-tuning is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:09:21 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 17:02:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zaken", "Elad Ben", ""], ["Ravfogel", "Shauli", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2106.10259", "submitter": "Katrin Tomanek", "authors": "Katrin Tomanek, Fran\\c{c}oise Beaufays, Julie Cattiau, Angad\n  Chandorkar, Khe Chai Sim", "title": "On-Device Personalization of Automatic Speech Recognition Models for\n  Disordered Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current state-of-the-art Automatic Speech Recognition (ASR) systems\nachieve high accuracy on typical speech, they suffer from significant\nperformance degradation on disordered speech and other atypical speech\npatterns. Personalization of ASR models, a commonly applied solution to this\nproblem, is usually performed in a server-based training environment posing\nproblems around data privacy, delayed model-update times, and communication\ncost for copying data and models between mobile device and server\ninfrastructure. In this paper, we present an approach to on-device based ASR\npersonalization with very small amounts of speaker-specific data. We test our\napproach on a diverse set of 100 speakers with disordered speech and find\nmedian relative word error rate improvement of 71% with only 50 short\nutterances required per speaker. When tested on a voice-controlled home\nautomation platform, on-device personalized models show a median task success\nrate of 81%, compared to only 40% of the unadapted models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:48:08 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Tomanek", "Katrin", ""], ["Beaufays", "Fran\u00e7oise", ""], ["Cattiau", "Julie", ""], ["Chandorkar", "Angad", ""], ["Sim", "Khe Chai", ""]]}, {"id": "2106.10328", "submitter": "Christy Dennison", "authors": "Irene Solaiman (1) and Christy Dennison (1) ((1) OpenAI)", "title": "Process for Adapting Language Models to Society (PALMS) with\n  Values-Targeted Datasets", "comments": "Both authors contributed equally. Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Language models can generate harmful and biased outputs and exhibit\nundesirable behavior. We propose a Process for Adapting Language Models to\nSociety (PALMS) with Values-Targeted Datasets, an iterative process to\nsignificantly change model behavior by crafting and fine-tuning on a dataset\nthat reflects a predetermined set of target values. We evaluate our process\nusing three metrics: quantitative metrics with human evaluations that score\noutput adherence to a target value, and toxicity scoring on outputs; and\nqualitative metrics analyzing the most common word associated with a given\nsocial category. Through each iteration, we add additional training dataset\nexamples based on observed shortcomings from evaluations. PALMS performs\nsignificantly better on all metrics compared to baseline and control models for\na broad range of GPT-3 language model sizes without compromising capability\nintegrity. We find that the effectiveness of PALMS increases with model size.\nWe show that significantly adjusting language model behavior is feasible with a\nsmall, hand-curated dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:38:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Solaiman", "Irene", "", "OpenAI"], ["Dennison", "Christy", "", "OpenAI"]]}, {"id": "2106.10434", "submitter": "Juyong Kim", "authors": "Juyong Kim, Pradeep Ravikumar, Joshua Ainslie, Santiago Onta\\~n\\'on", "title": "Improving Compositional Generalization in Classification Tasks via\n  Structure Annotations", "comments": "Accepted as a short paper at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional generalization is the ability to generalize systematically to a\nnew data distribution by combining known components. Although humans seem to\nhave a great ability to generalize compositionally, state-of-the-art neural\nmodels struggle to do so. In this work, we study compositional generalization\nin classification tasks and present two main contributions. First, we study\nways to convert a natural language sequence-to-sequence dataset to a\nclassification dataset that also requires compositional generalization. Second,\nwe show that providing structural hints (specifically, providing parse trees\nand entity links as attention masks for a Transformer model) helps\ncompositional generalization.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 06:07:27 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kim", "Juyong", ""], ["Ravikumar", "Pradeep", ""], ["Ainslie", "Joshua", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "2106.10454", "submitter": "Xin Jia", "authors": "Xin Jia, Hao Wang, Dawei Yin, Yunfang Wu", "title": "Enhancing Question Generation with Commonsense Knowledge", "comments": "Accepted by CCL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question generation (QG) is to generate natural and grammatical questions\nthat can be answered by a specific answer for a given context. Previous\nsequence-to-sequence models suffer from a problem that asking high-quality\nquestions requires commonsense knowledge as backgrounds, which in most cases\ncan not be learned directly from training data, resulting in unsatisfactory\nquestions deprived of knowledge. In this paper, we propose a multi-task\nlearning framework to introduce commonsense knowledge into question generation\nprocess. We first retrieve relevant commonsense knowledge triples from mature\ndatabases and select triples with the conversion information from source\ncontext to question. Based on these informative knowledge triples, we design\ntwo auxiliary tasks to incorporate commonsense knowledge into the main QG\nmodel, where one task is Concept Relation Classification and the other is Tail\nConcept Generation. Experimental results on SQuAD show that our proposed\nmethods are able to noticeably improve the QG performance on both automatic and\nhuman evaluation metrics, demonstrating that incorporating external commonsense\nknowledge with multi-task learning can help the model generate human-like and\nhigh-quality questions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 08:58:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jia", "Xin", ""], ["Wang", "Hao", ""], ["Yin", "Dawei", ""], ["Wu", "Yunfang", ""]]}, {"id": "2106.10468", "submitter": "Hou Pong Chan", "authors": "Hou Pong Chan and Irwin King", "title": "A Condense-then-Select Strategy for Text Summarization", "comments": "Accepted by Knowledge-Based Systems (KBS) journal", "journal-ref": null, "doi": "10.1016/j.knosys.2021.107235", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Select-then-compress is a popular hybrid, framework for text summarization\ndue to its high efficiency. This framework first selects salient sentences and\nthen independently condenses each of the selected sentences into a concise\nversion. However, compressing sentences separately ignores the context\ninformation of the document, and is therefore prone to delete salient\ninformation. To address this limitation, we propose a novel\ncondense-then-select framework for text summarization. Our framework first\nconcurrently condenses each document sentence. Original document sentences and\ntheir compressed versions then become the candidates for extraction. Finally,\nan extractor utilizes the context information of the document to select\ncandidates and assembles them into a summary. If salient information is deleted\nduring condensing, the extractor can select an original sentence to retain the\ninformation. Thus, our framework helps to avoid the loss of salient\ninformation, while preserving the high efficiency of sentence-level\ncompression. Experiment results on the CNN/DailyMail, DUC-2002, and Pubmed\ndatasets demonstrate that our framework outperforms the select-then-compress\nframework and other strong baselines.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 10:33:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chan", "Hou Pong", ""], ["King", "Irwin", ""]]}, {"id": "2106.10485", "submitter": "Karol Chlasta", "authors": "Agnieszka Wo{\\l}k, Karol Chlasta, Pawe{\\l} Holas", "title": "Hybrid approach to detecting symptoms of depression in social media\n  entries", "comments": "11 pages, 4 figures, 2 tables, The Pacific Asia Conference on\n  Information Systems (PACIS2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment and lexical analyses are widely used to detect depression or\nanxiety disorders. It has been documented that there are significant\ndifferences in the language used by a person with emotional disorders in\ncomparison to a healthy individual. Still, the effectiveness of these lexical\napproaches could be improved further because the current analysis focuses on\nwhat the social media entries are about, and not how they are written. In this\nstudy, we focus on aspects in which these short texts are similar to each\nother, and how they were created. We present an innovative approach to the\ndepression screening problem by applying Collgram analysis, which is a known\neffective method of obtaining linguistic information from texts. We compare\nthese results with sentiment analysis based on the BERT architecture. Finally,\nwe create a hybrid model achieving a diagnostic accuracy of 71%.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 12:28:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wo\u0142k", "Agnieszka", ""], ["Chlasta", "Karol", ""], ["Holas", "Pawe\u0142", ""]]}, {"id": "2106.10487", "submitter": "Olga Sopilnyak", "authors": "Pavel Voropaev, Olga Sopilnyak", "title": "Transformers for Headline Selection for Russian News Clusters", "comments": "Accepted to Dialogue 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore various multilingual and Russian pre-trained\ntransformer-based models for the Dialogue Evaluation 2021 shared task on\nheadline selection. Our experiments show that the combined approach is superior\nto individual multilingual and monolingual models. We present an analysis of a\nnumber of ways to obtain sentence embeddings and learn a ranking model on top\nof them. We achieve the result of 87.28% and 86.60% accuracy for the public and\nprivate test sets respectively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 12:34:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Voropaev", "Pavel", ""], ["Sopilnyak", "Olga", ""]]}, {"id": "2106.10502", "submitter": "Pei Ke", "authors": "Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan\n  Zhu, Minlie Huang", "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation\n  from Knowledge Graphs", "comments": "ACL 2021 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing pre-trained models for knowledge-graph-to-text (KG-to-text)\ngeneration simply fine-tune text-to-text pre-trained models such as BART or T5\non KG-to-text datasets, which largely ignore the graph structure during\nencoding and lack elaborate pre-training tasks to explicitly model graph-text\nalignments. To tackle these problems, we propose a graph-text joint\nrepresentation learning model called JointGT. During encoding, we devise a\nstructure-aware semantic aggregation module which is plugged into each\nTransformer layer to preserve the graph structure. Furthermore, we propose\nthree new pre-training tasks to explicitly enhance the graph-text alignment\nincluding respective text / graph reconstruction, and graph-text alignment in\nthe embedding space via Optimal Transport. Experiments show that JointGT\nobtains new state-of-the-art performance on various KG-to-text datasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 14:10:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ke", "Pei", ""], ["Ji", "Haozhe", ""], ["Ran", "Yu", ""], ["Cui", "Xin", ""], ["Wang", "Liwei", ""], ["Song", "Linfeng", ""], ["Zhu", "Xiaoyan", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.10512", "submitter": "Shruti Singh", "authors": "Viraj Shah, Shruti Singh, Mayank Singh", "title": "TweeNLP: A Twitter Exploration Portal for Natural Language Processing", "comments": "ACL-IJCNLP Demo Track 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present TweeNLP, a one-stop portal that organizes Twitter's natural\nlanguage processing (NLP) data and builds a visualization and exploration\nplatform. It curates 19,395 tweets (as of April 2021) from various NLP\nconferences and general NLP discussions. It supports multiple features such as\nTweetExplorer to explore tweets by topics, visualize insights from Twitter\nactivity throughout the organization cycle of conferences, discover popular\nresearch papers and researchers. It also builds a timeline of conference and\nworkshop submission deadlines. We envision TweeNLP to function as a collective\nmemory unit for the NLP community by integrating the tweets pertaining to\nresearch papers with the NLPExplorer scientific literature search engine. The\ncurrent system is hosted at http://nlpexplorer.org/twitter/CFP .\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 15:11:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shah", "Viraj", ""], ["Singh", "Shruti", ""], ["Singh", "Mayank", ""]]}, {"id": "2106.10608", "submitter": "Xing Han", "authors": "Xing Han, Jessica Lundin", "title": "Multi-Pair Text Style Transfer on Unbalanced Data", "comments": "Meta Learning and Its Applications to Natural Language Processing,\n  ACL 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-style transfer aims to convert text given in one domain into another by\nparaphrasing the sentence or substituting the keywords without altering the\ncontent. By necessity, state-of-the-art methods have evolved to accommodate\nnonparallel training data, as it is frequently the case there are multiple data\nsources of unequal size, with a mixture of labeled and unlabeled sentences.\nMoreover, the inherent style defined within each source might be distinct. A\ngeneric bidirectional (e.g., formal $\\Leftrightarrow$ informal) style transfer\nregardless of different groups may not generalize well to different\napplications. In this work, we developed a task adaptive meta-learning\nframework that can simultaneously perform a multi-pair text-style transfer\nusing a single model. The proposed method can adaptively balance the difference\nof meta-knowledge across multiple tasks. Results show that our method leads to\nbetter quantitative performance as well as coherent style variations. Common\nchallenges of unbalanced data and mismatched domains are handled well by this\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 03:20:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Xing", ""], ["Lundin", "Jessica", ""]]}, {"id": "2106.10619", "submitter": "Prasanna Parthasarathi", "authors": "Prasanna Parthasarathi, Mohamed Abdelsalam, Joelle Pineau, Sarath\n  Chandar", "title": "A Brief Study on the Effects of Training Generative Dialogue Models with\n  a Semantic loss", "comments": "Accepted at SIGDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural models trained for next utterance generation in dialogue task learn to\nmimic the n-gram sequences in the training set with training objectives like\nnegative log-likelihood (NLL) or cross-entropy. Such commonly used training\nobjectives do not foster generating alternate responses to a context. But, the\neffects of minimizing an alternate training objective that fosters a model to\ngenerate alternate response and score it on semantic similarity has not been\nwell studied. We hypothesize that a language generation model can improve on\nits diversity by learning to generate alternate text during training and\nminimizing a semantic loss as an auxiliary objective. We explore this idea on\ntwo different sized data sets on the task of next utterance generation in goal\noriented dialogues. We make two observations (1) minimizing a semantic\nobjective improved diversity in responses in the smaller data set (Frames) but\nonly as-good-as minimizing the NLL in the larger data set (MultiWoZ) (2) large\nlanguage model embeddings can be more useful as a semantic loss objective than\nas initialization for token embeddings.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 04:39:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Parthasarathi", "Prasanna", ""], ["Abdelsalam", "Mohamed", ""], ["Pineau", "Joelle", ""], ["Chandar", "Sarath", ""]]}, {"id": "2106.10622", "submitter": "Prasanna Parthasarathi", "authors": "Prasanna Parthasarathi, Joelle Pineau, Sarath Chandar", "title": "Do Encoder Representations of Generative Dialogue Models Encode\n  Sufficient Information about the Task ?", "comments": "Accepted at SIGDial 2021. arXiv admin note: substantial text overlap\n  with arXiv:2008.10427", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the next utterance in dialogue is contingent on encoding of users'\ninput text to generate appropriate and relevant response in data-driven\napproaches. Although the semantic and syntactic quality of the language\ngenerated is evaluated, more often than not, the encoded representation of\ninput is not evaluated. As the representation of the encoder is essential for\npredicting the appropriate response, evaluation of encoder representation is a\nchallenging yet important problem. In this work, we showcase evaluating the\ntext generated through human or automatic metrics is not sufficient to\nappropriately evaluate soundness of the language understanding of dialogue\nmodels and, to that end, propose a set of probe tasks to evaluate encoder\nrepresentation of different language encoders commonly used in dialogue models.\nFrom experiments, we observe that some of the probe tasks are easier and some\nare harder for even sophisticated model architectures to learn. And, through\nexperiments we observe that RNN based architectures have lower performance on\nautomatic metrics on text generation than transformer model but perform better\nthan the transformer model on the probe tasks indicating that RNNs might\npreserve task information better than the Transformers.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 04:52:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Parthasarathi", "Prasanna", ""], ["Pineau", "Joelle", ""], ["Chandar", "Sarath", ""]]}, {"id": "2106.10715", "submitter": "Zhengyan Zhang", "authors": "Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo\n  Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng,\n  Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu,\n  Maosong Sun", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the size of pre-trained language models (PLMs) has grown by\nleaps and bounds. However, efficiency issues of these large-scale PLMs limit\ntheir utilization in real-world scenarios. We present a suite of cost-effective\ntechniques for the use of PLMs to deal with the efficiency issues of\npre-training, fine-tuning, and inference. (1) We introduce knowledge\ninheritance to accelerate the pre-training process by exploiting existing PLMs\ninstead of training models from scratch. (2) We explore the best practice of\nprompt tuning with large-scale PLMs. Compared with conventional fine-tuning,\nprompt tuning significantly reduces the number of task-specific parameters. (3)\nWe implement a new inference toolkit, namely InfMoE, for using large-scale PLMs\nwith limited computational resources. Based on our cost-effective pipeline, we\npre-train two models: an encoder-decoder bilingual model with 11 billion\nparameters (CPM-2) and its corresponding MoE version with 198 billion\nparameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks.\nExperimental results show that CPM-2 has excellent general language\nintelligence. Moreover, we validate the efficiency of InfMoE when conducting\ninference of large-scale models having tens of billions of parameters on a\nsingle GPU. All source code and model parameters are available at\nhttps://github.com/TsinghuaAI/CPM.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 15:43:54 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 07:15:36 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 13:23:42 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhang", "Zhengyan", ""], ["Gu", "Yuxian", ""], ["Han", "Xu", ""], ["Chen", "Shengqi", ""], ["Xiao", "Chaojun", ""], ["Sun", "Zhenbo", ""], ["Yao", "Yuan", ""], ["Qi", "Fanchao", ""], ["Guan", "Jian", ""], ["Ke", "Pei", ""], ["Cai", "Yanzheng", ""], ["Zeng", "Guoyang", ""], ["Tan", "Zhixing", ""], ["Liu", "Zhiyuan", ""], ["Huang", "Minlie", ""], ["Han", "Wentao", ""], ["Liu", "Yang", ""], ["Zhu", "Xiaoyan", ""], ["Sun", "Maosong", ""]]}, {"id": "2106.10719", "submitter": "Hadeel Saadany", "authors": "Hadeel Saadany, Constantin Orasan, Rocio Caro Quintana, Felix do\n  Carmo, Leonardo Zilio", "title": "Challenges in Translation of Emotions in Multilingual User-Generated\n  Content: Twitter as a Case Study", "comments": null, "journal-ref": "Linguistik International 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although emotions are universal concepts, transferring the different shades\nof emotion from one language to another may not always be straightforward for\nhuman translators, let alone for machine translation systems. Moreover, the\ncognitive states are established by verbal explanations of experience which is\nshaped by both the verbal and cultural contexts. There are a number of verbal\ncontexts where expression of emotions constitutes the pivotal component of the\nmessage. This is particularly true for User-Generated Content (UGC) which can\nbe in the form of a review of a product or a service, a tweet, or a social\nmedia post. Recently, it has become common practice for multilingual websites\nsuch as Twitter to provide an automatic translation of UGC to reach out to\ntheir linguistically diverse users. In such scenarios, the process of\ntranslating the user's emotion is entirely automatic with no human\nintervention, neither for post-editing nor for accuracy checking. In this\nresearch, we assess whether automatic translation tools can be a successful\nreal-life utility in transferring emotion in user-generated multilingual data\nsuch as tweets. We show that there are linguistic phenomena specific of Twitter\ndata that pose a challenge in translation of emotions in different languages.\nWe summarise these challenges in a list of linguistic features and show how\nfrequent these features are in different language pairs. We also assess the\ncapacity of commonly used methods for evaluating the performance of an MT\nsystem with respect to the preservation of emotion in the source text.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 16:12:48 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Saadany", "Hadeel", ""], ["Orasan", "Constantin", ""], ["Quintana", "Rocio Caro", ""], ["Carmo", "Felix do", ""], ["Zilio", "Leonardo", ""]]}, {"id": "2106.10745", "submitter": "Zaid Alyafeai Mr", "authors": "Zaid Alyafeai, Maged S. Al-shaibani, Mustafa Ghaleb, Yousif Ahmed\n  Al-Wajih", "title": "Calliar: An Online Handwritten Dataset for Arabic Calligraphy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Calligraphy is an essential part of the Arabic heritage and culture. It has\nbeen used in the past for the decoration of houses and mosques. Usually, such\ncalligraphy is designed manually by experts with aesthetic insights. In the\npast few years, there has been a considerable effort to digitize such type of\nart by either taking a photo of decorated buildings or drawing them using\ndigital devices. The latter is considered an online form where the drawing is\ntracked by recording the apparatus movement, an electronic pen for instance, on\na screen. In the literature, there are many offline datasets collected with a\ndiversity of Arabic styles for calligraphy. However, there is no available\nonline dataset for Arabic calligraphy. In this paper, we illustrate our\napproach for the collection and annotation of an online dataset for Arabic\ncalligraphy called Calliar that consists of 2,500 sentences. Calliar is\nannotated for stroke, character, word and sentence level prediction.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 20:04:51 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 14:33:25 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Alyafeai", "Zaid", ""], ["Al-shaibani", "Maged S.", ""], ["Ghaleb", "Mustafa", ""], ["Al-Wajih", "Yousif Ahmed", ""]]}, {"id": "2106.10776", "submitter": "Matthias Grabmair", "authors": "Zihan Huang, Charles Low, Mengqiu Teng, Hongyi Zhang, Daniel E. Ho,\n  Mark S. Krass, Matthias Grabmair", "title": "Context-Aware Legal Citation Recommendation using Deep Learning", "comments": "10 pages published in Proceedings of ICAIL 2021; link to data here:\n  https://reglab.stanford.edu/data/bva-case-citation-dataset ; code available\n  here: https://github.com/TUMLegalTech/bva-citation-prediction", "journal-ref": null, "doi": "10.1145/3462757.3466066", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lawyers and judges spend a large amount of time researching the proper legal\nauthority to cite while drafting decisions. In this paper, we develop a\ncitation recommendation tool that can help improve efficiency in the process of\nopinion drafting. We train four types of machine learning models, including a\ncitation-list based method (collaborative filtering) and three context-based\nmethods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show\nthat leveraging local textual context improves recommendation, and that deep\nneural models achieve decent performance. We show that non-deep text-based\nmethods benefit from access to structured case metadata, but deep models only\nbenefit from such access when predicting from context of insufficient length.\nWe also find that, even after extensive training, RoBERTa does not outperform a\nrecurrent neural model, despite its benefits of pretraining. Our behavior\nanalysis of the RoBERTa model further shows that predictive performance is\nstable across time and citation classes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 23:23:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huang", "Zihan", ""], ["Low", "Charles", ""], ["Teng", "Mengqiu", ""], ["Zhang", "Hongyi", ""], ["Ho", "Daniel E.", ""], ["Krass", "Mark S.", ""], ["Grabmair", "Matthias", ""]]}, {"id": "2106.10786", "submitter": "Chen-Yu Lee", "authors": "Chen-Yu Lee, Chun-Liang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii,\n  Siyang Qin, Ashok Popat and Tomas Pfister", "title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based\n  Document Information Extraction", "comments": "Accepted to ACL-IJCNLP 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural reading orders of words are crucial for information extraction from\nform-like documents. Despite recent advances in Graph Convolutional Networks\n(GCNs) on modeling spatial layout patterns of documents, they have limited\nability to capture reading orders of given word-level node representations in a\ngraph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new\npositional encoding technique designed to apprehend the sequential presentation\nof words in documents. ROPE generates unique reading order codes for\nneighboring words relative to the target word given a word-level graph\nconnectivity. We study two fundamental document entity extraction tasks\nincluding word labeling and word grouping on the public FUNSD dataset and a\nlarge-scale payment dataset. We show that ROPE consistently improves existing\nGCNs with a margin up to 8.4% F1-score.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 00:48:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Li", "Chun-Liang", ""], ["Wang", "Chu", ""], ["Wang", "Renshen", ""], ["Fujii", "Yasuhisa", ""], ["Qin", "Siyang", ""], ["Popat", "Ashok", ""], ["Pfister", "Tomas", ""]]}, {"id": "2106.10816", "submitter": "Bowen Xing", "authors": "Bowen Xing and Ivor W. Tsang", "title": "Out of Context: A New Clue for Context Modeling of Aspect-based\n  Sentiment Analysis", "comments": "Submitted to JAIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis (ABSA) aims to predict the sentiment\nexpressed in a review with respect to a given aspect. The core of ABSA is to\nmodel the interaction between the context and given aspect to extract the\naspect-related information. In prior work, attention mechanisms and dependency\ngraph networks are commonly adopted to capture the relations between the\ncontext and given aspect. And the weighted sum of context hidden states is used\nas the final representation fed to the classifier. However, the information\nrelated to the given aspect may be already discarded and adverse information\nmay be retained in the context modeling processes of existing models. This\nproblem cannot be solved by subsequent modules and there are two reasons:\nfirst, their operations are conducted on the encoder-generated context hidden\nstates, whose value cannot change after the encoder; second, existing encoders\nonly consider the context while not the given aspect. To address this problem,\nwe argue the given aspect should be considered as a new clue out of context in\nthe context modeling process. As for solutions, we design several aspect-aware\ncontext encoders based on different backbones: an aspect-aware LSTM and three\naspect-aware BERTs. They are dedicated to generate aspect-aware hidden states\nwhich are tailored for ABSA task. In these aspect-aware context encoders, the\nsemantics of the given aspect is used to regulate the information flow.\nConsequently, the aspect-related information can be retained and\naspect-irrelevant information can be excluded in the generated hidden states.\nWe conduct extensive experiments on several benchmark datasets with empirical\nanalysis, demonstrating the efficacies and advantages of our proposed\naspect-aware context encoders.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 02:26:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xing", "Bowen", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "2106.10826", "submitter": "Yada Pruksachatkun Ms.", "authors": "Yada Pruksachatkun and Satyapriya Krishna and Jwala Dhamala and Rahul\n  Gupta and Kai-Wei Chang", "title": "Does Robustness Improve Fairness? Approaching Fairness with Word\n  Substitution Robustness Methods for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing bias mitigation methods to reduce disparities in model outcomes\nacross cohorts have focused on data augmentation, debiasing model embeddings,\nor adding fairness-based optimization objectives during training. Separately,\ncertified word substitution robustness methods have been developed to decrease\nthe impact of spurious features and synonym substitutions on model predictions.\nWhile their end goals are different, they both aim to encourage models to make\nthe same prediction for certain changes in the input. In this paper, we\ninvestigate the utility of certified word substitution robustness methods to\nimprove equality of odds and equality of opportunity on multiple text\nclassification tasks. We observe that certified robustness methods improve\nfairness, and using both robustness and bias mitigation methods in training\nresults in an improvement in both fronts\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:20:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pruksachatkun", "Yada", ""], ["Krishna", "Satyapriya", ""], ["Dhamala", "Jwala", ""], ["Gupta", "Rahul", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2106.10835", "submitter": "Tao Chen", "authors": "Tao Chen, Haochen Shi, Liyuan Liu, Siliang Tang, Jian Shao, Zhigang\n  Chen, Yueting Zhuang", "title": "Empower Distantly Supervised Relation Extraction with Collaborative\n  Adversarial Training", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in distantly supervised (DS) relation extraction (RE),\nconsiderable attention is attracted to leverage multi-instance learning (MIL)\nto distill high-quality supervision from the noisy DS. Here, we go beyond label\nnoise and identify the key bottleneck of DS-MIL to be its low data utilization:\nas high-quality supervision being refined by MIL, MIL abandons a large amount\nof training instances, which leads to a low data utilization and hinders model\ntraining from having abundant supervision. In this paper, we propose\ncollaborative adversarial training to improve the data utilization, which\ncoordinates virtual adversarial training (VAT) and adversarial training (AT) at\ndifferent levels. Specifically, since VAT is label-free, we employ the\ninstance-level VAT to recycle instances abandoned by MIL. Besides, we deploy AT\nat the bag-level to unleash the full potential of the high-quality supervision\ngot by MIL. Our proposed method brings consistent improvements (~ 5 absolute\nAUC score) to the previous state of the art, which verifies the importance of\nthe data utilization issue and the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:54:02 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Tao", ""], ["Shi", "Haochen", ""], ["Liu", "Liyuan", ""], ["Tang", "Siliang", ""], ["Shao", "Jian", ""], ["Chen", "Zhigang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2106.10840", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Yun Tang, Juan Pino, Xian Li", "title": "Pay Better Attention to Attention: Head Selection in Multilingual and\n  Multi-Domain Sequence Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-head attention has each of the attention heads collect salient\ninformation from different parts of an input sequence, making it a powerful\nmechanism for sequence modeling. Multilingual and multi-domain learning are\ncommon scenarios for sequence modeling, where the key challenge is to maximize\npositive transfer and mitigate negative transfer across languages and domains.\nIn this paper, we find that non-selective attention sharing is sub-optimal for\nachieving good generalization across all languages and domains. We further\npropose attention sharing strategies to facilitate parameter sharing and\nspecialization in multilingual and multi-domain sequence modeling. Our approach\nautomatically learns shared and specialized attention heads for different\nlanguages and domains to mitigate their interference. Evaluated in various\ntasks including speech recognition, text-to-text and speech-to-text\ntranslation, the proposed attention sharing strategies consistently bring gains\nto sequence models built upon multi-head attention. For speech-to-text\ntranslation, our approach yields an average of $+2.0$ BLEU over $13$ language\ndirections in multilingual setting and $+2.0$ BLEU over $3$ domains in\nmulti-domain setting.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:03:23 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gong", "Hongyu", ""], ["Tang", "Yun", ""], ["Pino", "Juan", ""], ["Li", "Xian", ""]]}, {"id": "2106.10855", "submitter": "Tao Chen", "authors": "Tao Chen, Haizhou Shi, Siliang Tang, Zhigang Chen, Fei Wu, Yueting\n  Zhuang", "title": "CIL: Contrastive Instance Learning Framework for Distantly Supervised\n  Relation Extraction", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The journey of reducing noise from distant supervision (DS) generated\ntraining data has been started since the DS was first introduced into the\nrelation extraction (RE) task. For the past decade, researchers apply the\nmulti-instance learning (MIL) framework to find the most reliable feature from\na bag of sentences. Although the pattern of MIL bags can greatly reduce DS\nnoise, it fails to represent many other useful sentence features in the\ndatasets. In many cases, these sentence features can only be acquired by extra\nsentence-level human annotation with heavy costs. Therefore, the performance of\ndistantly supervised RE models is bounded. In this paper, we go beyond typical\nMIL framework and propose a novel contrastive instance learning (CIL)\nframework. Specifically, we regard the initial MIL as the relational triple\nencoder and constraint positive pairs against negative pairs for each instance.\nExperiments demonstrate the effectiveness of our proposed framework, with\nsignificant improvements over the previous methods on NYT10, GDS and KBP.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:51:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Tao", ""], ["Shi", "Haizhou", ""], ["Tang", "Siliang", ""], ["Chen", "Zhigang", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2106.10862", "submitter": "Debanjana Kar", "authors": "Debanjana Kar, Sudeshna Sarkar, Pawan Goyal", "title": "ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument\n  Aggregation", "comments": "11 pages, 8 figures, Accepted in Challenges and Applications of\n  Automated Extraction of Socio-political Events from Text (CASE) @ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most of the existing information extraction frameworks (Wadden et al., 2019;\nVeysehet al., 2020) focus on sentence-level tasks and are hardly able to\ncapture the consolidated information from a given document. In our endeavour to\ngenerate precise document-level information frames from lengthy textual\nrecords, we introduce the task of Information Aggregation or Argument\nAggregation. More specifically, our aim is to filter irrelevant and redundant\nargument mentions that were extracted at a sentence level and render a document\nlevel information frame. Majority of the existing works have been observed to\nresolve related tasks of document-level event argument extraction (Yang et al.,\n2018a; Zheng et al., 2019a) and salient entity identification (Jain et\nal.,2020) using supervised techniques. To remove dependency from large amounts\nof labelled data, we explore the task of information aggregation using\nweakly-supervised techniques. In particular, we present an extractive algorithm\nwith multiple sieves which adopts active learning strategies to work\nefficiently in low-resource settings. For this task, we have annotated our own\ntest dataset comprising of 131 document information frames and have released\nthe code and dataset to further research prospects in this new domain. To the\nbest of our knowledge, we are the first to establish baseline results for this\ntask in English. Our data and code are publicly available at\nhttps://github.com/DebanjanaKar/ArgFuse.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 05:21:27 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kar", "Debanjana", ""], ["Sarkar", "Sudeshna", ""], ["Goyal", "Pawan", ""]]}, {"id": "2106.10870", "submitter": "Arun Baby", "authors": "Arun Baby, Pranav Jawale, Saranya Vinnaitherthan, Sumukh Badam,\n  Nagaraj Adiga, Sharath Adavanne", "title": "Non-native English lexicon creation for bilingual speech synthesis", "comments": "Accepted for Presentation at Speech Synthesis Workshop (SSW), 2021\n  (August 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual English speakers speak English as one of their languages. Their\nEnglish is of a non-native kind, and their conversations are of a code-mixed\nfashion. The intelligibility of a bilingual text-to-speech (TTS) system for\nsuch non-native English speakers depends on a lexicon that captures the phoneme\nsequence used by non-native speakers. However, due to the lack of non-native\nEnglish lexicon, existing bilingual TTS systems employ native English lexicons\nthat are widely available, in addition to their native language lexicon. Due to\nthe inconsistency between the non-native English pronunciation in the audio and\nnative English lexicon in the text, the intelligibility of synthesized speech\nin such TTS systems is significantly reduced.\n  This paper is motivated by the knowledge that the native language of the\nspeaker highly influences non-native English pronunciation. We propose a\ngeneric approach to obtain rules based on letter to phoneme alignment to map\nnative English lexicon to their non-native version. The effectiveness of such\nmapping is studied by comparing bilingual (Indian English and Hindi) TTS\nsystems trained with and without the proposed rules. The subjective evaluation\nshows that the bilingual TTS system trained with the proposed non-native\nEnglish lexicon rules obtains a 6% absolute improvement in preference.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:07:14 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Baby", "Arun", ""], ["Jawale", "Pranav", ""], ["Vinnaitherthan", "Saranya", ""], ["Badam", "Sumukh", ""], ["Adiga", "Nagaraj", ""], ["Adavanne", "Sharath", ""]]}, {"id": "2106.10899", "submitter": "\\c{S}\\\"ukr\\\"u Ozan", "authors": "Umut \\\"Ozdil, B\\\"u\\c{s}ra Arslan, D. Emre Ta\\c{s}ar, G\\\"ok\\c{c}e\n  Polat,\\c{S}\\\"ukr\\\"u Ozan", "title": "Ad Text Classification with Transformer-Based Natural Language\n  Processing Methods", "comments": "6 pages, in Turkish language, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a natural language processing-based (NLP-based) method is\nproposed for the sector-wise automatic classification of ad texts created on\nonline advertising platforms. Our data set consists of approximately 21,000\nlabeled advertising texts from 12 different sectors. In the study, the\nBidirectional Encoder Representations from Transformers (BERT) model, which is\na transformer-based language model that is recently used in fields such as text\nclassification in the natural language processing literature, was used. The\nclassification efficiencies obtained using a pre-trained BERT model for the\nTurkish language are shown in detail.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:38:31 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:28:59 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["\u00d6zdil", "Umut", ""], ["Arslan", "B\u00fc\u015fra", ""], ["Ta\u015far", "D. Emre", ""], ["Polat", "G\u00f6k\u00e7e", ""], ["Ozan", "\u015e\u00fckr\u00fc", ""]]}, {"id": "2106.10901", "submitter": "Quim Motger", "authors": "Quim Motger, Xavier Franch and Jordi Marco", "title": "Conversational Agents in Software Engineering: Survey, Taxonomy and\n  Challenges", "comments": "37 pages, 15 figures, 2 tables, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of natural language interfaces in the field of human-computer\ninteraction is undergoing intense study through dedicated scientific and\nindustrial research. The latest contributions in the field, including deep\nlearning approaches like recurrent neural networks, the potential of\ncontext-aware strategies and user-centred design approaches, have brought back\nthe attention of the community to software-based dialogue systems, generally\nknown as conversational agents or chatbots. Nonetheless, and given the novelty\nof the field, a generic, context-independent overview on the current state of\nresearch of conversational agents covering all research perspectives involved\nis missing. Motivated by this context, this paper reports a survey of the\ncurrent state of research of conversational agents through a systematic\nliterature review of secondary studies. The conducted research is designed to\ndevelop an exhaustive perspective through a clear presentation of the\naggregated knowledge published by recent literature within a variety of\ndomains, research focuses and contexts. As a result, this research proposes a\nholistic taxonomy of the different dimensions involved in the conversational\nagents' field, which is expected to help researchers and to lay the groundwork\nfor future research in the field of natural language interfaces.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:41:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Motger", "Quim", ""], ["Franch", "Xavier", ""], ["Marco", "Jordi", ""]]}, {"id": "2106.10928", "submitter": "Nawshad Farruque", "authors": "Nawshad Farruque, Randy Goebel, Osmar Zaiane, Sudhakar Sivapalan", "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable\n  Zero-shot modeling of clinical depression symptoms from text", "comments": "Fixed an algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:57:22 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 07:06:06 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Farruque", "Nawshad", ""], ["Goebel", "Randy", ""], ["Zaiane", "Osmar", ""], ["Sivapalan", "Sudhakar", ""]]}, {"id": "2106.10936", "submitter": "Zhihao Fan", "authors": "Zhihao Fan, Zhongyu Wei, Siyuan Wang, Ruize Wang, Zejun Li, Haijun\n  Shan, Xuanjing Huang", "title": "TCIC: Theme Concepts Learning Cross Language and Vision for Image\n  Captioning", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research for image captioning usually represents an image using a\nscene graph with low-level facts (objects and relations) and fails to capture\nthe high-level semantics. In this paper, we propose a Theme Concepts extended\nImage Captioning (TCIC) framework that incorporates theme concepts to represent\nhigh-level cross-modality semantics. In practice, we model theme concepts as\nmemory vectors and propose Transformer with Theme Nodes (TTN) to incorporate\nthose vectors for image captioning. Considering that theme concepts can be\nlearned from both images and captions, we propose two settings for their\nrepresentations learning based on TTN. On the vision side, TTN is configured to\ntake both scene graph based features and theme concepts as input for visual\nrepresentation learning. On the language side, TTN is configured to take both\ncaptions and theme concepts as input for text representation re-construction.\nBoth settings aim to generate target captions with the same transformer-based\ndecoder. During the training, we further align representations of theme\nconcepts learned from images and corresponding captions to enforce the\ncross-modality learning. Experimental results on MS COCO show the effectiveness\nof our approach compared to some state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:12:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fan", "Zhihao", ""], ["Wei", "Zhongyu", ""], ["Wang", "Siyuan", ""], ["Wang", "Ruize", ""], ["Li", "Zejun", ""], ["Shan", "Haijun", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2106.10955", "submitter": "Kastriot Kadriu", "authors": "Kastriot Kadriu and Milenko Obradovic", "title": "Extractive approach for text summarisation using graphs", "comments": "4 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language processing is an important discipline with the aim of\nunderstanding text by its digital representation, that due to the diverse way\nwe write and speak, is often not accurate enough. Our paper explores different\ngraph-related algorithms that can be used in solving the text summarization\nproblem using an extractive approach. We consider two metrics: sentence overlap\nand edit distance for measuring sentence similarity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:03:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kadriu", "Kastriot", ""], ["Obradovic", "Milenko", ""]]}, {"id": "2106.11013", "submitter": "Guoshun Nan Dr", "authors": "Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Hao Zhang, Wei\n  Lu", "title": "Interventional Video Grounding with Dual Contrastive Learning", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video grounding aims to localize a moment from an untrimmed video for a given\ntextual query. Existing approaches focus more on the alignment of visual and\nlanguage stimuli with various likelihood-based matching or regression\nstrategies, i.e., P(Y|X). Consequently, these models may suffer from spurious\ncorrelations between the language and video features due to the selection bias\nof the dataset. 1) To uncover the causality behind the model and data, we first\npropose a novel paradigm from the perspective of the causal inference, i.e.,\ninterventional video grounding (IVG) that leverages backdoor adjustment to\ndeconfound the selection bias based on structured causal model (SCM) and\ndo-calculus P(Y|do(X)). Then, we present a simple yet effective method to\napproximate the unobserved confounder as it cannot be directly sampled from the\ndataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)\nto better align the text and video by maximizing the mutual information (MI)\nbetween query and video clips, and the MI between start/end frames of a target\nmoment and the others within a video to learn more informative visual\nrepresentations. Experiments on three standard benchmarks show the\neffectiveness of our approaches. Our code is available on GitHub:\nhttps://github.com/nanguoshun/IVG.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:11:28 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 15:10:07 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nan", "Guoshun", ""], ["Qiao", "Rui", ""], ["Xiao", "Yao", ""], ["Liu", "Jun", ""], ["Leng", "Sicong", ""], ["Zhang", "Hao", ""], ["Lu", "Wei", ""]]}, {"id": "2106.11029", "submitter": "Shishir Adhikari", "authors": "Shishir Adhikari, Akshay Uppal, Robin Mermelstein, Tanya Berger-Wolf,\n  Elena Zheleva", "title": "Understanding the Dynamics between Vaping and Cannabis Legalization\n  Using Twitter Opinions", "comments": "Published at ICWSM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cannabis legalization has been welcomed by many U.S. states but its role in\nescalation from tobacco e-cigarette use to cannabis vaping is unclear.\nMeanwhile, cannabis vaping has been associated with new lung diseases and\nrising adolescent use. To understand the impact of cannabis legalization on\nescalation, we design an observational study to estimate the causal effect of\nrecreational cannabis legalization on the development of pro-cannabis attitude\nfor e-cigarette users. We collect and analyze Twitter data which contains\nopinions about cannabis and JUUL, a very popular e-cigarette brand. We use\nweakly supervised learning for personal tweet filtering and classification for\nstance detection. We discover that recreational cannabis legalization policy\nhas an effect on increased development of pro-cannabis attitudes for users\nalready in favor of e-cigarettes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:34:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Adhikari", "Shishir", ""], ["Uppal", "Akshay", ""], ["Mermelstein", "Robin", ""], ["Berger-Wolf", "Tanya", ""], ["Zheleva", "Elena", ""]]}, {"id": "2106.11053", "submitter": "Catherine Wong", "authors": "Catherine Wong and Kevin Ellis and Joshua B. Tenenbaum and Jacob\n  Andreas", "title": "Leveraging Language to Learn Program Abstractions and Search Heuristics", "comments": "appeared in Thirty-eighth International Conference on Machine\n  Learning (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inductive program synthesis, or inferring programs from examples of desired\nbehavior, offers a general paradigm for building interpretable, robust, and\ngeneralizable machine learning systems. Effective program synthesis depends on\ntwo key ingredients: a strong library of functions from which to build\nprograms, and an efficient search strategy for finding programs that solve a\ngiven task. We introduce LAPS (Language for Abstraction and Program Search), a\ntechnique for using natural language annotations to guide joint learning of\nlibraries and neurally-guided search models for synthesis. When integrated into\na state-of-the-art library learning system (DreamCoder), LAPS produces\nhigher-quality libraries and improves search efficiency and generalization on\nthree domains -- string editing, image composition, and abstract reasoning\nabout scenes -- even when no natural language hints are available at test time.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:08:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wong", "Catherine", ""], ["Ellis", "Kevin", ""], ["Tenenbaum", "Joshua B.", ""], ["Andreas", "Jacob", ""]]}, {"id": "2106.11057", "submitter": "Alejandro Moreo Fern\\'andez", "authors": "Alejandro Moreo, Andrea Esuli, Fabrizio Sebastiani", "title": "QuaPy: A Python-Based Framework for Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  QuaPy is an open-source framework for performing quantification (a.k.a.\nsupervised prevalence estimation), written in Python. Quantification is the\ntask of training quantifiers via supervised learning, where a quantifier is a\npredictor that estimates the relative frequencies (a.k.a. prevalence values) of\nthe classes of interest in a sample of unlabelled data. While quantification\ncan be trivially performed by applying a standard classifier to each unlabelled\ndata item and counting how many data items have been assigned to each class, it\nhas been shown that this \"classify and count\" method is outperformed by methods\nspecifically designed for quantification. QuaPy provides implementations of a\nnumber of baseline methods and advanced quantification methods, of routines for\nquantification-oriented model selection, of several broadly accepted evaluation\nmeasures, and of robust evaluation protocols routinely used in the field. QuaPy\nalso makes available datasets commonly used for testing quantifiers, and offers\nvisualization tools for facilitating the analysis and interpretation of the\nresults. The software is open-source and publicly available under a BSD-3\nlicence via https://github.com/HLT-ISTI/QuaPy, and can be installed via pip\n(https://pypi.org/project/QuaPy/)\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:57:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Moreo", "Alejandro", ""], ["Esuli", "Andrea", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "2106.11077", "submitter": "Kamran Kowsari", "authors": "Mojtaba Heidarysafa and Kamran Kowsari and Masoud Bashiri and Donald\n  E. Brown", "title": "Toward a Knowledge Discovery Framework for Data Science Job Market in\n  the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of the data science field requires better tools to understand such\na fast-paced growing domain. Moreover, individuals from different backgrounds\nbecame interested in following a career as data scientists. Therefore,\nproviding a quantitative guide for individuals and organizations to understand\nthe skills required in the job market would be crucial. This paper introduces a\nframework to analyze the job market for data science-related jobs within the US\nwhile providing an interface to access insights in this market. The proposed\nframework includes three sub-modules allowing continuous data collection,\ninformation extraction, and a web-based dashboard visualization to investigate\nthe spatial and temporal distribution of data science-related jobs and skills.\nThe result of this work shows important skills for the main branches of data\nscience jobs and attempts to provide a skill-based definition of these data\nscience branches. The current version of this application is deployed on the\nweb and allows individuals and institutes to investigate skills required for\ndata science positions through the industry lens.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 21:23:15 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 15:40:25 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Bashiri", "Masoud", ""], ["Brown", "Donald E.", ""]]}, {"id": "2106.11096", "submitter": "Yang Deng", "authors": "Yang Deng, Wenxuan Zhang, Wai Lam", "title": "Learning to Rank Question Answer Pairs with Bilateral Contrastive Data\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel and easy-to-apply data augmentation\nstrategy, namely Bilateral Generation (BiG), with a contrastive training\nobjective for improving the performance of ranking question answer pairs with\nexisting labeled data. In specific, we synthesize pseudo-positive QA pairs in\ncontrast to the original negative QA pairs with two pre-trained generation\nmodels, one for question generation, the other for answer generation, which are\nfine-tuned on the limited positive QA pairs from the original dataset. With the\naugmented dataset, we design a contrastive training objective for learning to\nrank question answer pairs. Experimental results on three benchmark datasets,\nnamely TREC-QA, WikiQA, and ANTIQUE, show that our method significantly\nimproves the performance of ranking models by making full use of existing\nlabeled data and can be easily applied to different ranking models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:29:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Deng", "Yang", ""], ["Zhang", "Wenxuan", ""], ["Lam", "Wai", ""]]}, {"id": "2106.11148", "submitter": "Peiyi Wang", "authors": "Peiyi Wang, Lianzhe Huang, Tianyu Liu, Damai Dai, Runxin Xu, Houfeng\n  Wang, Baobao Chang and Zhifang Sui", "title": "Explicit Interaction Network for Aspect Sentiment Triplet Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Sentiment Triplet Extraction (ASTE) aims to recognize targets, their\nsentiment polarities and opinions explaining the sentiment from a sentence.\nASTE could be naturally divided into 3 atom subtasks, namely target detection,\nopinion detection and sentiment classification. We argue that the proper\nsubtask combination, compositional feature extraction for target-opinion pairs,\nand interaction between subtasks would be the key to success. Prior work,\nhowever, may fail on `one-to-many' or `many-to-one' situations, or derive\nnon-existent sentiment triplets due to defective subtask formulation,\nsub-optimal feature representation or the lack of subtask interaction. In this\npaper, we divide ASTE into target-opinion joint detection and sentiment\nclassification subtasks, which is in line with human cognition, and\ncorrespondingly propose sequence encoder and table encoder. Table encoder\nextracts sentiment at token-pair level, so that the compositional feature\nbetween targets and opinions can be easily captured. To establish explicit\ninteraction between subtasks, we utilize the table representation to guide the\nsequence encoding, and inject the sequence features back into the table\nencoder. Experiments show that our model outperforms state-of-the-art methods\non six popular ASTE datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 14:36:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Peiyi", ""], ["Huang", "Lianzhe", ""], ["Liu", "Tianyu", ""], ["Dai", "Damai", ""], ["Xu", "Runxin", ""], ["Wang", "Houfeng", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "2106.11176", "submitter": "J\\'er\\^ome Durand-Lose", "authors": "J\\'er\\^ome Durand-Lose and Aur\\'elien Emmanuel", "title": "Abstract Geometrical Computation 11: Slanted Firing Squad\n  Synchronisation on Signal Machines", "comments": "21 pages,29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CL math.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Firing Squad Synchronisation on Cellular Automata is the dynamical\nsynchronisation of finitely many cells without any prior knowledge of their\nrange. This can be conceived as a signal with an infinite speed. Most of the\nproposed constructions naturally translate to the continuous setting of signal\nmachines and generate fractal figures with an accumulation on a horizontal\nline, i.e. synchronously, in the space-time diagram. Signal machines are\nstudied in a series of articles named Abstract Geometrical Computation.\n  In the present article, we design a signal machine that is able to\nsynchronise/accumulate on any non-infinite slope. The slope is encoded in the\ninitial configuration. This is done by constructing an infinite tree such that\neach node computes the way the tree expands.\n  The interest of Abstract Geometrical computation is to do away with the\nconstraint of discrete space, while tackling new difficulties from continuous\nspace. The interest of this paper in particular is to provide basic tools for\nfurther study of computable accumulation lines in the signal machine model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:15:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Durand-Lose", "J\u00e9r\u00f4me", ""], ["Emmanuel", "Aur\u00e9lien", ""]]}, {"id": "2106.11196", "submitter": "Benedikt Boenninghoff", "authors": "Benedikt Boenninghoff, Dorothea Kolossa, Robert M. Nickel", "title": "Self-Calibrating Neural-Probabilistic Model for Authorship Verification\n  Under Covariate Shift", "comments": "12th International Conference of the CLEF Association, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are addressing two fundamental problems in authorship verification (AV):\nTopic variability and miscalibration. Variations in the topic of two disputed\ntexts are a major cause of error for most AV systems. In addition, it is\nobserved that the underlying probability estimates produced by deep learning AV\nmechanisms oftentimes do not match the actual case counts in the respective\ntraining data. As such, probability estimates are poorly calibrated. We are\nexpanding our framework from PAN 2020 to include Bayes factor scoring (BFS) and\nan uncertainty adaptation layer (UAL) to address both problems. Experiments\nwith the 2020/21 PAN AV shared task data show that the proposed method\nsignificantly reduces sensitivities to topical variations and significantly\nimproves the system's calibration.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:33:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Boenninghoff", "Benedikt", ""], ["Kolossa", "Dorothea", ""], ["Nickel", "Robert M.", ""]]}, {"id": "2106.11197", "submitter": "Binzong Geng", "authors": "Binzong Geng, Min Yang, Fajie Yuan, Shupeng Wang, Xiang Ao, Ruifeng Xu", "title": "Iterative Network Pruning with Uncertainty Regularization for Lifelong\n  Sentiment Classification", "comments": "Accepted by the 44th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR), 2021", "journal-ref": null, "doi": "10.1145/3404835.3462902", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning capabilities are crucial for sentiment classifiers to\nprocess continuous streams of opinioned information on the Web. However,\nperforming lifelong learning is non-trivial for deep neural networks as\ncontinually training of incrementally available information inevitably results\nin catastrophic forgetting or interference. In this paper, we propose a novel\niterative network pruning with uncertainty regularization method for lifelong\nsentiment classification (IPRLS), which leverages the principles of network\npruning and weight regularization. By performing network pruning with\nuncertainty regularization in an iterative manner, IPRLS can adapta single BERT\nmodel to work with continuously arriving data from multiple domains while\navoiding catastrophic forgetting and interference. Specifically, we leverage an\niterative pruning method to remove redundant parameters in large deep networks\nso that the freed-up space can then be employed to learn new tasks, tackling\nthe catastrophic forgetting problem. Instead of keeping the old-tasks fixed\nwhen learning new tasks, we also use an uncertainty regularization based on the\nBayesian online learning framework to constrain the update of old tasks weights\nin BERT, which enables positive backward transfer, i.e. learning new tasks\nimproves performance on past tasks while protecting old knowledge from being\nlost. In addition, we propose a task-specific low-dimensional residual function\nin parallel to each layer of BERT, which makes IPRLS less prone to losing the\nknowledge saved in the base BERT network when learning a new task. Extensive\nexperiments on 16 popular review corpora demonstrate that the proposed IPRLS\nmethod sig-nificantly outperforms the strong baselines for lifelong sentiment\nclassification. For reproducibility, we submit the code and data\nat:https://github.com/siat-nlp/IPRLS.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:34:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Geng", "Binzong", ""], ["Yang", "Min", ""], ["Yuan", "Fajie", ""], ["Wang", "Shupeng", ""], ["Ao", "Xiang", ""], ["Xu", "Ruifeng", ""]]}, {"id": "2106.11292", "submitter": "Mandana Saebi", "authors": "Mandana Saebi, Ernest Pusateri, Aaksha Meghawat, Christophe Van Gysel", "title": "A Discriminative Entity-Aware Language Model for Virtual Assistants", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality automatic speech recognition (ASR) is essential for virtual\nassistants (VAs) to work well. However, ASR often performs poorly on VA\nrequests containing named entities. In this work, we start from the observation\nthat many ASR errors on named entities are inconsistent with real-world\nknowledge. We extend previous discriminative n-gram language modeling\napproaches to incorporate real-world knowledge from a Knowledge Graph (KG),\nusing features that capture entity type-entity and entity-entity relationships.\nWe apply our model through an efficient lattice rescoring process, achieving\nrelative sentence error rate reductions of more than 25% on some synthesized\ntest sets covering less popular entities, with minimal degradation on a\nuniformly sampled VA test set.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:50:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Saebi", "Mandana", ""], ["Pusateri", "Ernest", ""], ["Meghawat", "Aaksha", ""], ["Van Gysel", "Christophe", ""]]}, {"id": "2106.11342", "submitter": "Aston Zhang", "authors": "Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola", "title": "Dive into Deep Learning", "comments": "(HTML) https://D2L.ai (GitHub) https://github.com/d2l-ai/d2l-en/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:19:46 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 16:51:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Aston", ""], ["Lipton", "Zachary C.", ""], ["Li", "Mu", ""], ["Smola", "Alexander J.", ""]]}, {"id": "2106.11375", "submitter": "Junjie Hu", "authors": "Junjie Hu and Graham Neubig", "title": "Phrase-level Active Learning for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) is sensitive to domain shift. In this paper,\nwe address this problem in an active learning setting where we can spend a\ngiven budget on translating in-domain data, and gradually fine-tune a\npre-trained out-of-domain NMT model on the newly translated data. Existing\nactive learning methods for NMT usually select sentences based on uncertainty\nscores, but these methods require costly translation of full sentences even\nwhen only one or two key phrases within the sentence are informative. To\naddress this limitation, we re-examine previous work from the phrase-based\nmachine translation (PBMT) era that selected not full sentences, but rather\nindividual phrases. However, while incorporating these phrases into PBMT\nsystems was relatively simple, it is less trivial for NMT systems, which need\nto be trained on full sequences to capture larger structural properties of\nsentences unique to the new domain. To overcome these hurdles, we propose to\nselect both full sentences and individual phrases from unlabelled data in the\nnew domain for routing to human translators. In a German-English translation\ntask, our active learning approach achieves consistent improvements over\nuncertainty-based sentence selection methods, improving up to 1.2 BLEU score\nover strong active learning baselines.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:20:42 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hu", "Junjie", ""], ["Neubig", "Graham", ""]]}, {"id": "2106.11384", "submitter": "Huseyin Inan", "authors": "Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh,\n  Marcello Hasegawa", "title": "Membership Inference on Word Embedding and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n  In this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:37:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mahloujifar", "Saeed", ""], ["Inan", "Huseyin A.", ""], ["Chase", "Melissa", ""], ["Ghosh", "Esha", ""], ["Hasegawa", "Marcello", ""]]}, {"id": "2106.11388", "submitter": "Priyam Tejaswin", "authors": "Priyam Tejaswin, Dhruv Naik, Pengfei Liu", "title": "How well do you know your summarization datasets?", "comments": "Accepted into Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art summarization systems are trained and evaluated on massive\ndatasets scraped from the web. Despite their prevalence, we know very little\nabout the underlying characteristics (data noise, summarization complexity,\netc.) of these datasets, and how these affect system performance and the\nreliability of automatic metrics like ROUGE. In this study, we manually analyze\n600 samples from three popular summarization datasets. Our study is driven by a\nsix-class typology which captures different noise types (missing facts,\nentities) and degrees of summarization difficulty (extractive, abstractive). We\nfollow with a thorough analysis of 27 state-of-the-art summarization models and\n5 popular metrics, and report our key insights: (1) Datasets have distinct data\nquality and complexity distributions, which can be traced back to their\ncollection process. (2) The performance of models and reliability of metrics is\ndependent on sample complexity. (3) Faithful summaries often receive low scores\nbecause of the poor diversity of references. We release the code, annotated\ndata and model outputs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:44:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Tejaswin", "Priyam", ""], ["Naik", "Dhruv", ""], ["Liu", "Pengfei", ""]]}, {"id": "2106.11403", "submitter": "Yefeng Wang", "authors": "Yefeng Wang, Yunpeng Zhao, Jiang Bian, Rui Zhang", "title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event\n  Signals from Twitter", "comments": "1 Figure, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:35:01 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Yefeng", ""], ["Zhao", "Yunpeng", ""], ["Bian", "Jiang", ""], ["Zhang", "Rui", ""]]}, {"id": "2106.11410", "submitter": "Anjalie Field", "authors": "Anjalie Field, Su Lin Blodgett, Zeerak Waseem, Yulia Tsvetkov", "title": "A Survey of Race, Racism, and Anti-Racism in NLP", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:59:06 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 20:57:12 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Field", "Anjalie", ""], ["Blodgett", "Su Lin", ""], ["Waseem", "Zeerak", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2106.11437", "submitter": "Justin Leo", "authors": "Justin Leo and Jugal Kalita", "title": "Incremental Deep Neural Network Learning using Classification Confidence\n  Thresholding", "comments": "Accepted to IEEE TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2021.3087104", "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 22:46:28 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Leo", "Justin", ""], ["Kalita", "Jugal", ""]]}, {"id": "2106.11455", "submitter": "Chia-Hsuan Lee", "authors": "Chia-Hsuan Lee, Oleksandr Polozov, Matthew Richardson", "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers", "comments": "Published as a conference paper at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of database question answering is to enable natural language\nquerying of real-life relational databases in diverse application domains.\nRecently, large-scale datasets such as Spider and WikiSQL facilitated novel\nmodeling techniques for text-to-SQL parsing, improving zero-shot generalization\nto unseen databases. In this work, we examine the challenges that still prevent\nthese techniques from practical deployment. First, we present KaggleDBQA, a new\ncross-domain evaluation dataset of real Web databases, with domain-specific\ndata types, original formatting, and unrestricted questions. Second, we\nre-examine the choice of evaluation tasks for text-to-SQL parsers as applied in\nreal-life settings. Finally, we augment our in-domain evaluation task with\ndatabase documentation, a naturally occurring source of implicit domain\nknowledge. We show that KaggleDBQA presents a challenge to state-of-the-art\nzero-shot parsers but a more realistic evaluation setting and creative use of\nassociated database documentation boosts their accuracy by over 13.2%, doubling\ntheir performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 00:08:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lee", "Chia-Hsuan", ""], ["Polozov", "Oleksandr", ""], ["Richardson", "Matthew", ""]]}, {"id": "2106.11483", "submitter": "Tong Guo", "authors": "Tong Guo", "title": "A Comprehensive Exploration of Pre-training Language Models", "comments": "working in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor the transformer-encoder layers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:12:29 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Guo", "Tong", ""]]}, {"id": "2106.11517", "submitter": "Rivindu Weerasekera", "authors": "Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Suranga\n  Nanayakkara", "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for\n  Question-Answering", "comments": "for associated code, see\n  https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag-end2end-retriever", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we illustrate how to fine-tune the entire Retrieval Augment\nGeneration (RAG) architecture in an end-to-end manner. We highlighted the main\nengineering challenges that needed to be addressed to achieve this objective.\nWe also compare how end-to-end RAG architecture outperforms the original RAG\narchitecture for the task of question answering. We have open-sourced our\nimplementation in the HuggingFace Transformers library.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:17:59 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Siriwardhana", "Shamane", ""], ["Weerasekera", "Rivindu", ""], ["Wen", "Elliott", ""], ["Nanayakkara", "Suranga", ""]]}, {"id": "2106.11520", "submitter": "Weizhe Yuan", "authors": "Weizhe Yuan and Graham Neubig and Pengfei Liu", "title": "BARTScore: Evaluating Generated Text as Text Generation", "comments": "Demo at http://explainaboard.nlpedia.ai/leaderboard/task-meval/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A wide variety of NLP applications, such as machine translation,\nsummarization, and dialog, involve text generation. One major challenge for\nthese applications is how to evaluate whether such generated texts are actually\nfluent, accurate, or effective. In this work, we conceptualize the evaluation\nof generated text as a text generation problem, modeled using pre-trained\nsequence-to-sequence models. The general idea is that models trained to convert\nthe generated text to/from a reference output or the source text will achieve\nhigher scores when the generated text is better. We operationalize this idea\nusing BART, an encoder-decoder based pre-trained model, and propose a metric\nBARTScore with a number of variants that can be flexibly applied in an\nunsupervised fashion to evaluation of text from different perspectives (e.g.\ninformativeness, fluency, or factuality). BARTScore is conceptually simple and\nempirically effective. It can outperform existing top-scoring metrics in 16 of\n22 test settings, covering evaluation of 16 datasets (e.g., machine\ntranslation, text summarization) and 7 different perspectives (e.g.,\ninformativeness, factuality). Code to calculate BARTScore is available at\nhttps://github.com/neulab/BARTScore, and we have released an interactive\nleaderboard for meta-evaluation at\nhttp://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard\nplatform, which allows us to interactively understand the strengths,\nweaknesses, and complementarity of each metric.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:20:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yuan", "Weizhe", ""], ["Neubig", "Graham", ""], ["Liu", "Pengfei", ""]]}, {"id": "2106.11531", "submitter": "Yang Li", "authors": "Yang Li, Wei Zhao, Erik Cambria, Suhang Wang, Steffen Eger", "title": "Graph Routing between Capsules", "comments": null, "journal-ref": "Neural Network 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Routing methods in capsule networks often learn a hierarchical relationship\nfor capsules in successive layers, but the intra-relation between capsules in\nthe same layer is less studied, while this intra-relation is a key factor for\nthe semantic understanding in text data. Therefore, in this paper, we introduce\na new capsule network with graph routing to learn both relationships, where\ncapsules in each layer are treated as the nodes of a graph. We investigate\nstrategies to yield adjacency and degree matrix with three different distances\nfrom a layer of capsules, and propose the graph routing mechanism between those\ncapsules. We validate our approach on five text classification datasets, and\nour findings suggest that the approach combining bottom-up routing and top-down\nattention performs the best. Such an approach demonstrates generalization\ncapability across datasets. Compared to the state-of-the-art routing methods,\nthe improvements in accuracy in the five datasets we used were 0.82, 0.39,\n0.07, 1.01, and 0.02, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:00:57 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Li", "Yang", ""], ["Zhao", "Wei", ""], ["Cambria", "Erik", ""], ["Wang", "Suhang", ""], ["Eger", "Steffen", ""]]}, {"id": "2106.11533", "submitter": "Peifeng Wang", "authors": "Peifeng Wang, Filip Ilievski, Muhao Chen, Xiang Ren", "title": "Do Language Models Perform Generalizable Commonsense Inference?", "comments": "8 pages, 4 figures. Accepted to ACL'21 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by evidence that pretrained language models (LMs) encode commonsense\nknowledge, recent work has applied LMs to automatically populate commonsense\nknowledge graphs (CKGs). However, there is a lack of understanding on their\ngeneralization to multiple CKGs, unseen relations, and novel entities. This\npaper analyzes the ability of LMs to perform generalizable commonsense\ninference, in terms of knowledge capacity, transferability, and induction. Our\nexperiments with these three aspects show that: (1) LMs can adapt to different\nschemas defined by multiple CKGs but fail to reuse the knowledge to generalize\nto new relations. (2) Adapted LMs generalize well to unseen subjects, but less\nso on novel objects. Future work should investigate how to improve the\ntransferability and induction of commonsense mining from LMs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:17:19 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Peifeng", ""], ["Ilievski", "Filip", ""], ["Chen", "Muhao", ""], ["Ren", "Xiang", ""]]}, {"id": "2106.11566", "submitter": "Ruotian Ma", "authors": "Ruotian Ma, Tao Gui, Linyang Li, Qi Zhang, Yaqian Zhou and Xuanjing\n  Huang", "title": "SENT: Sentence-level Distant Relation Extraction via Negative Training", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision for relation extraction provides uniform bag labels for\neach sentence inside the bag, while accurate sentence labels are important for\ndownstream applications that need the exact relation type. Directly using bag\nlabels for sentence-level training will introduce much noise, thus severely\ndegrading performance. In this work, we propose the use of negative training\n(NT), in which a model is trained using complementary labels regarding that\n``the instance does not belong to these complementary labels\". Since the\nprobability of selecting a true label as a complementary label is low, NT\nprovides less noisy information. Furthermore, the model trained with NT is able\nto separate the noisy data from the training data. Based on NT, we propose a\nsentence-level framework, SENT, for distant relation extraction. SENT not only\nfilters the noisy data to construct a cleaner dataset, but also performs a\nre-labeling process to transform the noisy data into useful training data, thus\nfurther benefiting the model's performance. Experimental results show the\nsignificant improvement of the proposed method over previous methods on\nsentence-level evaluation and de-noise effect.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:49:05 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ma", "Ruotian", ""], ["Gui", "Tao", ""], ["Li", "Linyang", ""], ["Zhang", "Qi", ""], ["Zhou", "Yaqian", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2106.11575", "submitter": "Gangwoo Kim", "authors": "Gangwoo Kim, Hyunjae Kim, Jungsoo Park, Jaewoo Kang", "title": "Learn to Resolve Conversational Dependency: A Consistency Training\n  Framework for Conversational Question Answering", "comments": "12 pages, ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in conversational question answering (CQA) is to\nresolve the conversational dependency, such as anaphora and ellipsis. However,\nexisting approaches do not explicitly train QA models on how to resolve the\ndependency, and thus these models are limited in understanding human dialogues.\nIn this paper, we propose a novel framework, ExCorD (Explicit guidance on how\nto resolve Conversational Dependency) to enhance the abilities of QA models in\ncomprehending conversational context. ExCorD first generates self-contained\nquestions that can be understood without the conversation history, then trains\na QA model with the pairs of original and self-contained questions using a\nconsistency-based regularizer. In our experiments, we demonstrate that ExCorD\nsignificantly improves the QA models' performance by up to 1.2 F1 on QuAC, and\n5.2 F1 on CANARD, while addressing the limitations of the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 07:16:45 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kim", "Gangwoo", ""], ["Kim", "Hyunjae", ""], ["Park", "Jungsoo", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2106.11739", "submitter": "Michael Staniek", "authors": "Michael Staniek and Stefan Riezler", "title": "Error-Aware Interactive Semantic Parsing of OpenStreetMap", "comments": "Accepted at SpLU-RoboNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic parsing of geographical queries against real-world databases such\nas OpenStreetMap (OSM), unique correct answers do not necessarily exist.\nInstead, the truth might be lying in the eye of the user, who needs to enter an\ninteractive setup where ambiguities can be resolved and parsing mistakes can be\ncorrected. Our work presents an approach to interactive semantic parsing where\nan explicit error detection is performed, and a clarification question is\ngenerated that pinpoints the suspected source of ambiguity or error and\ncommunicates it to the human user. Our experimental results show that a\ncombination of entropy-based uncertainty detection and beam search, together\nwith multi-source training on clarification question, initial parse, and user\nanswer, results in improvements of 1.2% F1 score on a parser that already\nperforms at 90.26% on the NLMaps dataset for OSM semantic parsing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:18:42 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Staniek", "Michael", ""], ["Riezler", "Stefan", ""]]}, {"id": "2106.11740", "submitter": "Weihao Yu", "authors": "Weihao Yu, Zihang Jiang, Fei Chen, Qibin Hou and Jiashi Feng", "title": "LV-BERT: Exploiting Layer Variety for BERT", "comments": "Accepted to Findings of ACL 2021. The code and pre-trained models are\n  available at https://github.com/yuweihao/LV-BERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern pre-trained language models are mostly built upon backbones stacking\nself-attention and feed-forward layers in an interleaved order. In this paper,\nbeyond this stereotyped layer pattern, we aim to improve pre-trained models by\nexploiting layer variety from two aspects: the layer type set and the layer\norder. Specifically, besides the original self-attention and feed-forward\nlayers, we introduce convolution into the layer type set, which is\nexperimentally found beneficial to pre-trained models. Furthermore, beyond the\noriginal interleaved order, we explore more layer orders to discover more\npowerful architectures. However, the introduced layer variety leads to a large\narchitecture space of more than billions of candidates, while training a single\ncandidate model from scratch already requires huge computation cost, making it\nnot affordable to search such a space by directly training large amounts of\ncandidate models. To solve this problem, we first pre-train a supernet from\nwhich the weights of all candidate models can be inherited, and then adopt an\nevolutionary algorithm guided by pre-training accuracy to find the optimal\narchitecture. Extensive experiments show that LV-BERT model obtained by our\nmethod outperforms BERT and its variants on various downstream tasks. For\nexample, LV-BERT-small achieves 79.8 on the GLUE testing set, 1.8 higher than\nthe strong baseline ELECTRA-small.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:20:14 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 03:58:57 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yu", "Weihao", ""], ["Jiang", "Zihang", ""], ["Chen", "Fei", ""], ["Hou", "Qibin", ""], ["Feng", "Jiashi", ""]]}, {"id": "2106.11759", "submitter": "Vikramjit Mitra", "authors": "Vikramjit Mitra, Zifang Huang, Colin Lea, Lauren Tooley, Sarah Wu,\n  Darren Botten, Ashwini Palekar, Shrinath Thelapurath, Panayiotis Georgiou,\n  Sachin Kajarekar, Jefferey Bigham", "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech", "comments": "5 pages, 1 page reference, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.CV cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:58:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mitra", "Vikramjit", ""], ["Huang", "Zifang", ""], ["Lea", "Colin", ""], ["Tooley", "Lauren", ""], ["Wu", "Sarah", ""], ["Botten", "Darren", ""], ["Palekar", "Ashwini", ""], ["Thelapurath", "Shrinath", ""], ["Georgiou", "Panayiotis", ""], ["Kajarekar", "Sachin", ""], ["Bigham", "Jefferey", ""]]}, {"id": "2106.11783", "submitter": "Marco Guerini", "authors": "Yi-Ling Chung, Serra Sinem Tekiroglu, Marco Guerini", "title": "Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech", "comments": "To appear in \"Proceedings of the 59th Annual Meeting of the\n  Association for Computational Linguistics (ACL): Findings\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling online hatred using informed textual responses - called counter\nnarratives - has been brought under the spotlight recently. Accordingly, a\nresearch line has emerged to automatically generate counter narratives in order\nto facilitate the direct intervention in the hate discussion and to prevent\nhate content from further spreading. Still, current neural approaches tend to\nproduce generic/repetitive responses and lack grounded and up-to-date evidence\nsuch as facts, statistics, or examples. Moreover, these models can create\nplausible but not necessarily true arguments. In this paper we present the\nfirst complete knowledge-bound counter narrative generation pipeline, grounded\nin an external knowledge repository that can provide more informative content\nto fight online hatred. Together with our approach, we present a series of\nexperiments that show its feasibility to produce suitable and informative\ncounter narratives in in-domain and cross-domain settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:48:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chung", "Yi-Ling", ""], ["Tekiroglu", "Serra Sinem", ""], ["Guerini", "Marco", ""]]}, {"id": "2106.11791", "submitter": "Soujanya Poria", "authors": "Navonil Majumder, Deepanway Ghosal, Devamanyu Hazarika, Alexander\n  Gelbukh, Rada Mihalcea, Soujanya Poria", "title": "Exemplars-guided Empathetic Response Generation Controlled by the\n  Elements of Human Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of existing methods for empathetic response generation rely on\nthe emotion of the context to generate empathetic responses. However, empathy\nis much more than generating responses with an appropriate emotion. It also\noften entails subtle expressions of understanding and personal resonance with\nthe situation of the other interlocutor. Unfortunately, such qualities are\ndifficult to quantify and the datasets lack the relevant annotations. To\naddress this issue, in this paper we propose an approach that relies on\nexemplars to cue the generative model on fine stylistic properties that signal\nempathy to the interlocutor. To this end, we employ dense passage retrieval to\nextract relevant exemplary responses from the training set. Three elements of\nhuman communication -- emotional presence, interpretation, and exploration, and\nsentiment are additionally introduced using synthetic labels to guide the\ngeneration towards empathy. The human evaluation is also extended by these\nelements of human communication. We empirically show that these approaches\nyield significant improvements in empathetic response quality in terms of both\nautomated and human-evaluated metrics. The implementation is available at\nhttps://github.com/declare-lab/exemplary-empathy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:02:33 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Majumder", "Navonil", ""], ["Ghosal", "Deepanway", ""], ["Hazarika", "Devamanyu", ""], ["Gelbukh", "Alexander", ""], ["Mihalcea", "Rada", ""], ["Poria", "Soujanya", ""]]}, {"id": "2106.11796", "submitter": "Silin Gao", "authors": "Silin Gao, Ryuichi Takanobu, Minlie Huang", "title": "End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge\n  Management", "comments": "Submitted to IEEE/ACM TASLP, regular paper. arXiv admin note:\n  substantial text overlap with arXiv:2105.06041", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current task-oriented dialog (TOD) systems mostly manage structured knowledge\n(e.g. databases and tables) to guide the goal-oriented conversations. However,\nthey fall short of handling dialogs which also involve unstructured knowledge\n(e.g. reviews and documents). In this paper, we formulate a task of modeling\nTOD grounded on a fusion of structured and unstructured knowledge. To address\nthis task, we propose a TOD system with semi-structured knowledge management,\nSeKnow, which extends the belief state to manage knowledge with both structured\nand unstructured contents. Furthermore, we introduce two implementations of\nSeKnow based on a non-pretrained sequence-to-sequence model and a pretrained\nlanguage model, respectively. Both implementations use the end-to-end manner to\njointly optimize dialog modeling grounded on structured and unstructured\nknowledge. We conduct experiments on the modified version of MultiWOZ 2.1\ndataset, where dialogs are processed to involve semi-structured knowledge.\nExperimental results show that SeKnow has strong performances in both\nend-to-end dialog and intermediate knowledge management, compared to existing\nTOD systems and their extensions with pipeline knowledge management schemes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:07:22 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gao", "Silin", ""], ["Takanobu", "Ryuichi", ""], ["Huang", "Minlie", ""]]}, {"id": "2106.11891", "submitter": "Antonios Anastasopoulos", "authors": "Md Mahfuz ibn Alam, Antonios Anastasopoulos, Laurent Besacier, James\n  Cross, Matthias Gall\\'e, Philipp Koehn, Vassilina Nikoulina", "title": "On the Evaluation of Machine Translation for Terminology Consistency", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural machine translation (NMT) systems become an important part of\nprofessional translator pipelines, a growing body of work focuses on combining\nNMT with terminologies. In many scenarios and particularly in cases of domain\nadaptation, one expects the MT output to adhere to the constraints provided by\na terminology. In this work, we propose metrics to measure the consistency of\nMT output with regards to a domain terminology. We perform studies on the\nCOVID-19 domain over 5 languages, also performing terminology-targeted human\nevaluation. We open-source the code for computing all proposed metrics:\nhttps://github.com/mahfuzibnalam/terminology_evaluation\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:59:32 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 22:51:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Alam", "Md Mahfuz ibn", ""], ["Anastasopoulos", "Antonios", ""], ["Besacier", "Laurent", ""], ["Cross", "James", ""], ["Gall\u00e9", "Matthias", ""], ["Koehn", "Philipp", ""], ["Nikoulina", "Vassilina", ""]]}, {"id": "2106.11988", "submitter": "Chenhao Tan", "authors": "Chenhao Tan", "title": "On the Diversity and Limits of Human Explanations", "comments": "15 pages, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A growing effort in NLP aims to build datasets of human explanations.\nHowever, the term explanation encompasses a broad range of notions, each with\ndifferent properties and ramifications. Our goal is to provide an overview of\ndiverse types of explanations and human limitations, and discuss implications\nfor collecting and using explanations in NLP. Inspired by prior work in\npsychology and cognitive sciences, we group existing human explanations in NLP\ninto three categories: proximal mechanism, evidence, and procedure. These three\ntypes differ in nature and have implications for the resultant explanations.\nFor instance, procedure is not considered explanations in psychology and\nconnects with a rich body of work on learning from instructions. The diversity\nof explanations is further evidenced by proxy questions that are needed for\nannotators to interpret and answer open-ended why questions. Finally,\nexplanations may require different, often deeper, understandings than\npredictions, which casts doubt on whether humans can provide useful\nexplanations in some tasks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:00:07 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Tan", "Chenhao", ""]]}, {"id": "2106.12027", "submitter": "Yanjun Gao", "authors": "Yanjun Gao, Ting-hao Huang, Rebecca J. Passonneau", "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set\n  of Simple Sentences", "comments": "To appear in the proceeding of 59th Annual Meeting of the Association\n  for Computational Linguistics (ACL 2021) Main Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic clauses are fundamental text units for understanding complex\nsentences. Identifying the atomic sentences within complex sentences is\nimportant for applications such as summarization, argument mining, discourse\nanalysis, discourse parsing, and question answering. Previous work mainly\nrelies on rule-based methods dependent on parsing. We propose a new task to\ndecompose each complex sentence into simple sentences derived from the tensed\nclauses in the source, and a novel problem formulation as a graph edit task.\nOur neural model learns to Accept, Break, Copy or Drop elements of a graph that\ncombines word adjacency and grammatical dependencies. The full processing\npipeline includes modules for graph construction, graph editing, and sentence\ngeneration from the output graph. We introduce DeSSE, a new dataset designed to\ntrain and evaluate complex sentence decomposition, and MinWiki, a subset of\nMinWikiSplit. ABCD achieves comparable performance as two parsing baselines on\nMinWiki. On DeSSE, which has a more even balance of complex sentence types, our\nmodel achieves higher accuracy on the number of atomic sentences than an\nencoder-decoder baseline. Results include a detailed error analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:31:28 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gao", "Yanjun", ""], ["Huang", "Ting-hao", ""], ["Passonneau", "Rebecca J.", ""]]}, {"id": "2106.12030", "submitter": "Junxia Lin", "authors": "Junxia Lin (1), Johannes Ledolter (2) ((1) Georgetown University\n  Medical Center, Georgetown University, (2) Tippie College of Business,\n  University of Iowa)", "title": "A Simple and Practical Approach to Improve Misspellings in OCR Text", "comments": "11 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of our paper is the identification and correction of non-word\nerrors in OCR text. Such errors may be the result of incorrect insertion,\ndeletion, or substitution of a character, or the transposition of two adjacent\ncharacters within a single word. Or, it can be the result of word boundary\nproblems that lead to run-on errors and incorrect-split errors. The traditional\nN-gram correction methods can handle single-word errors effectively. However,\nthey show limitations when dealing with split and merge errors. In this paper,\nwe develop an unsupervised method that can handle both errors. The method we\ndevelop leads to a sizable improvement in the correction rates. This tutorial\npaper addresses very difficult word correction problems - namely incorrect\nrun-on and split errors - and illustrates what needs to be considered when\naddressing such problems. We outline a possible approach and assess its success\non a limited study.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:38:17 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Lin", "Junxia", ""], ["Ledolter", "Johannes", ""]]}, {"id": "2106.12056", "submitter": "Chenhao Tan", "authors": "Madhusudhan Aithal and Chenhao Tan", "title": "On Positivity Bias in Negative Reviews", "comments": "11 pages, 17 figures, ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work has revealed that positive words occur more frequently than\nnegative words in human expressions, which is typically attributed to\npositivity bias, a tendency for people to report positive views of reality. But\nwhat about the language used in negative reviews? Consistent with prior work,\nwe show that English negative reviews tend to contain more positive words than\nnegative words, using a variety of datasets. We reconcile this observation with\nprior findings on the pragmatics of negation, and show that negations are\ncommonly associated with positive words in negative reviews. Furthermore, in\nnegative reviews, the majority of sentences with positive words express\nnegative opinions based on sentiment classifiers, indicating some form of\nnegation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:04:25 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Aithal", "Madhusudhan", ""], ["Tan", "Chenhao", ""]]}, {"id": "2106.12066", "submitter": "Max Ryabinin", "authors": "Alexey Tikhonov, Max Ryabinin", "title": "It's All in the Heads: Using Attention Heads as a Baseline for\n  Cross-Lingual Transfer in Commonsense Reasoning", "comments": "Accepted to Findings of ACL 2021. 13 pages, 4 figures. Code:\n  https://github.com/yandex-research/crosslingual_winograd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning is one of the key problems in natural language\nprocessing, but the relative scarcity of labeled data holds back the progress\nfor languages other than English. Pretrained cross-lingual models are a source\nof powerful language-agnostic representations, yet their inherent reasoning\ncapabilities are still actively studied. In this work, we design a simple\napproach to commonsense reasoning which trains a linear classifier with weights\nof multi-head attention as features. To evaluate this approach, we create a\nmultilingual Winograd Schema corpus by processing several datasets from prior\nwork within a standardized pipeline and measure cross-lingual generalization\nability in terms of out-of-sample performance. The method performs\ncompetitively with recent supervised and unsupervised approaches for\ncommonsense reasoning, even when applied to other languages in a zero-shot\nmanner. Also, we demonstrate that most of the performance is given by the same\nsmall subset of attention heads for all studied languages, which provides\nevidence of universal reasoning capabilities in multilingual encoders.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:25:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Tikhonov", "Alexey", ""], ["Ryabinin", "Max", ""]]}, {"id": "2106.12089", "submitter": "Anup Sarma", "authors": "Anup Sarma, Sonali Singh, Huaipan Jiang, Rui Zhang, Mahmut T Kandemir\n  and Chita R Das", "title": "Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for\n  Efficient Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural Networks (RNNs), more specifically their Long Short-Term\nMemory (LSTM) variants, have been widely used as a deep learning tool for\ntackling sequence-based learning tasks in text and speech. Training of such\nLSTM applications is computationally intensive due to the recurrent nature of\nhidden state computation that repeats for each time step. While sparsity in\nDeep Neural Nets has been widely seen as an opportunity for reducing\ncomputation time in both training and inference phases, the usage of non-ReLU\nactivation in LSTM RNNs renders the opportunities for such dynamic sparsity\nassociated with neuron activation and gradient values to be limited or\nnon-existent. In this work, we identify dropout induced sparsity for LSTMs as a\nsuitable mode of computation reduction. Dropout is a widely used regularization\nmechanism, which randomly drops computed neuron values during each iteration of\ntraining. We propose to structure dropout patterns, by dropping out the same\nset of physical neurons within a batch, resulting in column (row) level hidden\nstate sparsity, which are well amenable to computation reduction at run-time in\ngeneral-purpose SIMD hardware as well as systolic arrays. We conduct our\nexperiments for three representative NLP tasks: language modelling on the PTB\ndataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi\ndatasets, and named entity recognition sequence labelling using the CoNLL-2003\nshared task. We demonstrate that our proposed approach can be used to translate\ndropout-based computation reduction into reduced training time, with\nimprovement ranging from 1.23x to 1.64x, without sacrificing the target metric.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:44:32 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sarma", "Anup", ""], ["Singh", "Sonali", ""], ["Jiang", "Huaipan", ""], ["Zhang", "Rui", ""], ["Kandemir", "Mahmut T", ""], ["Das", "Chita R", ""]]}, {"id": "2106.12131", "submitter": "Mana Ihori", "authors": "Mana Ihori, Naoki Makishima, Tomohiro Tanaka, Akihiko Takashima, Shota\n  Orihashi, Ryo Masumura", "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks\n  using Switching Tokens", "comments": "Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel spoken-text-style conversion method that\ncan simultaneously execute multiple style conversion modules such as\npunctuation restoration and disfluency deletion without preparing matched\ndatasets. In practice, transcriptions generated by automatic speech recognition\nsystems are not highly readable because they often include many disfluencies\nand do not include punctuation marks. To improve their readability, multiple\nspoken-text-style conversion modules that individually model a single\nconversion task are cascaded because matched datasets that simultaneously\nhandle multiple conversion tasks are often unavailable. However, the cascading\nis unstable against the order of tasks because of the chain of conversion\nerrors. Besides, the computation cost of the cascading must be higher than the\nsingle conversion. To execute multiple conversion tasks simultaneously without\npreparing matched datasets, our key idea is to distinguish individual\nconversion tasks using the on-off switch. In our proposed zero-shot joint\nmodeling, we switch the individual tasks using multiple switching tokens,\nenabling us to utilize a zero-shot learning approach to executing simultaneous\nconversions. Our experiments on joint modeling of disfluency deletion and\npunctuation restoration demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 02:53:14 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ihori", "Mana", ""], ["Makishima", "Naoki", ""], ["Tanaka", "Tomohiro", ""], ["Takashima", "Akihiko", ""], ["Orihashi", "Shota", ""], ["Masumura", "Ryo", ""]]}, {"id": "2106.12144", "submitter": "Mikhail Galkin", "authors": "Mikhail Galkin, Jiapeng Wu, Etienne Denis, William L. Hamilton", "title": "NodePiece: Compositional and Parameter-Efficient Representations of\n  Large Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 03:51:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Galkin", "Mikhail", ""], ["Wu", "Jiapeng", ""], ["Denis", "Etienne", ""], ["Hamilton", "William L.", ""]]}, {"id": "2106.12230", "submitter": "Xiang Dai", "authors": "Xiang Dai", "title": "Recognising Biomedical Names: Challenges and Solutions", "comments": "PhD thesis, University of Sydney", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growth rate in the amount of biomedical documents is staggering.\nUnlocking information trapped in these documents can enable researchers and\npractitioners to operate confidently in the information world. Biomedical NER,\nthe task of recognising biomedical names, is usually employed as the first step\nof the NLP pipeline. Standard NER models, based on sequence tagging technique,\nare good at recognising short entity mentions in the generic domain. However,\nthere are several open challenges of applying these models to recognise\nbiomedical names: 1) Biomedical names may contain complex inner structure\n(discontinuity and overlapping) which cannot be recognised using standard\nsequence tagging technique; 2) The training of NER models usually requires\nlarge amount of labelled data, which are difficult to obtain in the biomedical\ndomain; and, 3) Commonly used language representation models are pre-trained on\ngeneric data; a domain shift therefore exists between these models and target\nbiomedical data. To deal with these challenges, we explore several research\ndirections and make the following contributions: 1) we propose a\ntransition-based NER model which can recognise discontinuous mentions; 2) We\ndevelop a cost-effective approach that nominates the suitable pre-training\ndata; and, 3) We design several data augmentation methods for NER. Our\ncontributions have obvious practical implications, especially when new\nbiomedical applications are needed. Our proposed data augmentation methods can\nhelp the NER model achieve decent performance, requiring only a small amount of\nlabelled data. Our investigation regarding selecting pre-training data can\nimprove the model by incorporating language representation models, which are\npre-trained using in-domain data. Finally, our proposed transition-based NER\nmodel can further improve the performance by recognising discontinuous\nmentions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 08:20:13 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Dai", "Xiang", ""]]}, {"id": "2106.12314", "submitter": "Daniel Buschek", "authors": "Oliver Schmitt, Daniel Buschek", "title": "CharacterChat: Supporting the Creation of Fictional Characters through\n  Conversation and Progressive Manifestation with a Chatbot", "comments": "14 pages, 2 figures, 2 tables; ACM C&C 2021", "journal-ref": null, "doi": "10.1145/3450741.3465253", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CharacterChat, a concept and chatbot to support writers in\ncreating fictional characters. Concretely, writers progressively turn the bot\ninto their imagined character through conversation. We iteratively developed\nCharacterChat in a user-centred approach, starting with a survey on character\ncreation with writers (N=30), followed by two qualitative user studies (N=7 and\nN=8). Our prototype combines two modes: (1) Guided prompts help writers define\ncharacter attributes (e.g. User: \"Your name is Jane.\"), including suggestions\nfor attributes (e.g. Bot: \"What is my main motivation?\") and values, realised\nas a rule-based system with a concept network. (2) Open conversation with the\nchatbot helps writers explore their character and get inspiration, realised\nwith a language model that takes into account the defined character attributes.\nOur user studies reveal benefits particularly for early stages of character\ncreation, and challenges due to limited conversational capabilities. We\nconclude with lessons learned and ideas for future work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:22:27 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Schmitt", "Oliver", ""], ["Buschek", "Daniel", ""]]}, {"id": "2106.12373", "submitter": "Jiajie Zou", "authors": "Jiajie Zou, Yuran Zhang, Peiqing Jin, Cheng Luo, Xunyi Pan, Nai Ding", "title": "PALRACE: Reading Comprehension Dataset with Human Data and Labeled\n  Rationales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models achieves high performance on machine reading\ncomprehension (MRC) tasks but the results are hard to explain. An appealing\napproach to make models explainable is to provide rationales for its decision.\nTo facilitate supervised learning of human rationales, here we present PALRACE\n(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for\n800 passages selected from the RACE dataset. We further classified the question\nto each passage into 6 types. Each passage was read by at least 26\nparticipants, who labeled their rationales to answer the question. Besides, we\nconducted a rationale evaluation session in which participants were asked to\nanswering the question solely based on labeled rationales, confirming that the\nlabeled rationales were of high quality and can sufficiently support question\nanswering.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:12:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zou", "Jiajie", ""], ["Zhang", "Yuran", ""], ["Jin", "Peiqing", ""], ["Luo", "Cheng", ""], ["Pan", "Xunyi", ""], ["Ding", "Nai", ""]]}, {"id": "2106.12384", "submitter": "Qian Li", "authors": "Qian Li, Hao Peng, Jianxin Li, Yuanxing Ning, Lihong Wang, Philip S.\n  Yu, Zheng Wang", "title": "Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit\n  Argument Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event extraction is a fundamental task for natural language processing.\nFinding the roles of event arguments like event participants is essential for\nevent extraction. However, doing so for real-life event descriptions is\nchallenging because an argument's role often varies in different contexts.\nWhile the relationship and interactions between multiple arguments are useful\nfor settling the argument roles, such information is largely ignored by\nexisting approaches. This paper presents a better approach for event extraction\nby explicitly utilizing the relationships of event arguments. We achieve this\nthrough a carefully designed task-oriented dialogue system. To model the\nargument relation, we employ reinforcement learning and incremental learning to\nextract multiple arguments via a multi-turned, iterative process. Our approach\nleverages knowledge of the already extracted arguments of the same sentence to\ndetermine the role of arguments that would be difficult to decide individually.\nIt then uses the newly obtained information to improve the decisions of\npreviously extracted arguments. This two-way feedback process allows us to\nexploit the argument relations to effectively settle argument roles, leading to\nbetter sentence understanding and event extraction. Experimental results show\nthat our approach consistently outperforms seven state-of-the-art event\nextraction methods for the classification of events and argument role and\nargument identification.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:24:39 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Li", "Qian", ""], ["Peng", "Hao", ""], ["Li", "Jianxin", ""], ["Ning", "Yuanxing", ""], ["Wang", "Lihong", ""], ["Yu", "Philip S.", ""], ["Wang", "Zheng", ""]]}, {"id": "2106.12398", "submitter": "Du\\v{s}an Vari\\v{s}", "authors": "Josef Jon and Jo\\~ao Paulo Aires and Du\\v{s}an Vari\\v{s} and\n  Ond\\v{r}ej Bojar", "title": "End-to-End Lexically Constrained Machine Translation for Morphologically\n  Rich Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lexically constrained machine translation allows the user to manipulate the\noutput sentence by enforcing the presence or absence of certain words and\nphrases. Although current approaches can enforce terms to appear in the\ntranslation, they often struggle to make the constraint word form agree with\nthe rest of the generated output. Our manual analysis shows that 46% of the\nerrors in the output of a baseline constrained model for English to Czech\ntranslation are related to agreement. We investigate mechanisms to allow neural\nmachine translation to infer the correct word inflection given lemmatized\nconstraints. In particular, we focus on methods based on training the model\nwith constraints provided as part of the input sequence. Our experiments on the\nEnglish-Czech language pair show that this approach improves the translation of\nconstrained terms in both automatic and manual evaluation by reducing errors in\nagreement. Our approach thus eliminates inflection errors, without introducing\nnew errors or decreasing the overall quality of the translation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:40:13 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 14:21:48 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jon", "Josef", ""], ["Aires", "Jo\u00e3o Paulo", ""], ["Vari\u0161", "Du\u0161an", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2106.12475", "submitter": "Sara Papi", "authors": "Sara Papi, Edmondo Trentin, Roberto Gretter, Marco Matassoni, Daniele\n  Falavigna", "title": "Mixtures of Deep Neural Experts for Automated Speech Scoring", "comments": null, "journal-ref": "Proc. Interspeech 2020", "doi": "10.21437/Interspeech.2020-1055", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper copes with the task of automatic assessment of second language\nproficiency from the language learners' spoken responses to test prompts. The\ntask has significant relevance to the field of computer assisted language\nlearning. The approach presented in the paper relies on two separate modules:\n(1) an automatic speech recognition system that yields text transcripts of the\nspoken interactions involved, and (2) a multiple classifier system based on\ndeep learners that ranks the transcripts into proficiency classes. Different\ndeep neural network architectures (both feed-forward and recurrent) are\nspecialized over diverse representations of the texts in terms of: a reference\ngrammar, the outcome of probabilistic language models, several word embeddings,\nand two bag-of-word models. Combination of the individual classifiers is\nrealized either via a probabilistic pseudo-joint model, or via a neural mixture\nof experts. Using the data of the third Spoken CALL Shared Task challenge, the\nhighest values to date were obtained in terms of three popular evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:44:50 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Papi", "Sara", ""], ["Trentin", "Edmondo", ""], ["Gretter", "Roberto", ""], ["Matassoni", "Marco", ""], ["Falavigna", "Daniele", ""]]}, {"id": "2106.12479", "submitter": "Charaf Eddine  Benarab", "authors": "Charaf Eddine Benarab", "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer\n  Learning and Data Transformations", "comments": "Paper contains: 8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n  Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:53:38 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 09:58:03 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Benarab", "Charaf Eddine", ""]]}, {"id": "2106.12488", "submitter": "Abdellah El Mekki", "authors": "Abdelkader El Mahdaouy, Abdellah El Mekki, Kabil Essefar, Nabil El\n  Mamoun, Ismail Berrada, Ahmed Khoumsi", "title": "Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in\n  Arabic Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prominence of figurative language devices, such as sarcasm and irony,\nposes serious challenges for Arabic Sentiment Analysis (SA). While previous\nresearch works tackle SA and sarcasm detection separately, this paper\nintroduces an end-to-end deep Multi-Task Learning (MTL) model, allowing\nknowledge interaction between the two tasks. Our MTL model's architecture\nconsists of a Bidirectional Encoder Representation from Transformers (BERT)\nmodel, a multi-task attention interaction module, and two task classifiers. The\noverall obtained results show that our proposed model outperforms its\nsingle-task counterparts on both SA and sarcasm detection sub-tasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:00:32 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mahdaouy", "Abdelkader El", ""], ["Mekki", "Abdellah El", ""], ["Essefar", "Kabil", ""], ["Mamoun", "Nabil El", ""], ["Berrada", "Ismail", ""], ["Khoumsi", "Ahmed", ""]]}, {"id": "2106.12495", "submitter": "Abdellah El Mekki", "authors": "Abdellah El Mekki, Abdelkader El Mahdaouy, Kabil Essefar, Nabil El\n  Mamoun, Ismail Berrada, Ahmed Khoumsi", "title": "BERT-based Multi-Task Model for Country and Province Level Modern\n  Standard Arabic and Dialectal Arabic Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialect and standard language identification are crucial tasks for many\nArabic natural language processing applications. In this paper, we present our\ndeep learning-based system, submitted to the second NADI shared task for\ncountry-level and province-level identification of Modern Standard Arabic (MSA)\nand Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task\nLearning (MTL) model to tackle both country-level and province-level MSA/DA\nidentification. The latter MTL model consists of a shared Bidirectional Encoder\nRepresentation Transformers (BERT) encoder, two task-specific attention layers,\nand two classifiers. Our key idea is to leverage both the task-discriminative\nand the inter-task shared features for country and province MSA/DA\nidentification. The obtained results show that our MTL model outperforms\nsingle-task models on most subtasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:07:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mekki", "Abdellah El", ""], ["Mahdaouy", "Abdelkader El", ""], ["Essefar", "Kabil", ""], ["Mamoun", "Nabil El", ""], ["Berrada", "Ismail", ""], ["Khoumsi", "Ahmed", ""]]}, {"id": "2106.12566", "submitter": "Shengjie Luo", "authors": "Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin\n  Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional\n  Encoding", "comments": "Preprint. Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:51:26 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Luo", "Shengjie", ""], ["Li", "Shanda", ""], ["Cai", "Tianle", ""], ["He", "Di", ""], ["Peng", "Dinglan", ""], ["Zheng", "Shuxin", ""], ["Ke", "Guolin", ""], ["Wang", "Liwei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.12607", "submitter": "Sara Papi", "authors": "Sara Papi, Marco Gaido, Matteo Negri, Marco Turchi", "title": "Dealing with training and test segmentation mismatch: FBK@IWSLT2021", "comments": "Accepted at IWSLT2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes FBK's system submission to the IWSLT 2021 Offline Speech\nTranslation task. We participated with a direct model, which is a\nTransformer-based architecture trained to translate English speech audio data\ninto German texts. The training pipeline is characterized by knowledge\ndistillation and a two-step fine-tuning procedure. Both knowledge distillation\nand the first fine-tuning step are carried out on manually segmented real and\nsynthetic data, the latter being generated with an MT system trained on the\navailable corpora. Differently, the second fine-tuning step is carried out on a\nrandom segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce\nthe performance drops occurring when a speech translation model trained on\nmanually segmented data (i.e. an ideal, sentence-like segmentation) is\nevaluated on automatically segmented audio (i.e. actual, more realistic testing\nconditions). For the same purpose, a custom hybrid segmentation procedure that\naccounts for both audio content (pauses) and for the length of the produced\nsegments is applied to the test data before passing them to the system. At\ninference time, we compared this procedure with a baseline segmentation method\nbased on Voice Activity Detection (VAD). Our results indicate the effectiveness\nof the proposed hybrid approach, shown by a reduction of the gap with manual\nsegmentation from 8.3 to 1.4 BLEU points.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:11:32 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 09:41:24 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Papi", "Sara", ""], ["Gaido", "Marco", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2106.12608", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Chelsea Ju, J. Harry Caufield, Kevin Shih, Calvin Chen,\n  Yizhou Sun, Kai-Wei Chang, Peipei Ping, Wei Wang", "title": "Clinical Named Entity Recognition using Contextualized Token\n  Representations", "comments": "1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The clinical named entity recognition (CNER) task seeks to locate and\nclassify clinical terminologies into predefined categories, such as diagnostic\nprocedure, disease disorder, severity, medication, medication dosage, and sign\nsymptom. CNER facilitates the study of side-effect on medications including\nidentification of novel phenomena and human-focused information extraction.\nExisting approaches in extracting the entities of interests focus on using\nstatic word embeddings to represent each word. However, one word can have\ndifferent interpretations that depend on the context of the sentences.\nEvidently, static word embeddings are insufficient to integrate the diverse\ninterpretation of a word. To overcome this challenge, the technique of\ncontextualized word embedding has been introduced to better capture the\nsemantic meaning of each word based on its context. Two of these language\nmodels, ELMo and Flair, have been widely used in the field of Natural Language\nProcessing to generate the contextualized word embeddings on domain-generic\ndocuments. However, these embeddings are usually too general to capture the\nproximity among vocabularies of specific domains. To facilitate various\ndownstream applications using clinical case reports (CCRs), we pre-train two\ndeep contextualized language models, Clinical Embeddings from Language Model\n(C-ELMo) and Clinical Contextual String Embeddings (C-Flair) using the\nclinical-related corpus from the PubMed Central. Explicit experiments show that\nour models gain dramatic improvements compared to both static word embeddings\nand domain-generic language models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:12:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhou", "Yichao", ""], ["Ju", "Chelsea", ""], ["Caufield", "J. Harry", ""], ["Shih", "Kevin", ""], ["Chen", "Calvin", ""], ["Sun", "Yizhou", ""], ["Chang", "Kai-Wei", ""], ["Ping", "Peipei", ""], ["Wang", "Wei", ""]]}, {"id": "2106.12672", "submitter": "Yi Tay", "authors": "Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung,\n  Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, Donald Metzler", "title": "Charformer: Fast Character Transformers via Gradient-based Subword\n  Tokenization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:24:14 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 16:30:28 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Tay", "Yi", ""], ["Tran", "Vinh Q.", ""], ["Ruder", "Sebastian", ""], ["Gupta", "Jai", ""], ["Chung", "Hyung Won", ""], ["Bahri", "Dara", ""], ["Qin", "Zhen", ""], ["Baumgartner", "Simon", ""], ["Yu", "Cong", ""], ["Metzler", "Donald", ""]]}, {"id": "2106.12698", "submitter": "Maria Ryskina", "authors": "Maria Ryskina, Eduard Hovy, Taylor Berg-Kirkpatrick, Matthew R.\n  Gormley", "title": "Comparative Error Analysis in Neural and Finite-state Models for\n  Unsupervised Character-level Transduction", "comments": "Accepted to SIGMORPHON 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, character-level transduction problems have been solved with\nfinite-state models designed to encode structural and linguistic knowledge of\nthe underlying process, whereas recent approaches rely on the power and\nflexibility of sequence-to-sequence models with attention. Focusing on the less\nexplored unsupervised learning scenario, we compare the two model classes side\nby side and find that they tend to make different types of errors even when\nachieving comparable performance. We analyze the distributions of different\nerror classes using two unsupervised tasks as testbeds: converting informally\nromanized text into the native script of its language (for Russian, Arabic, and\nKannada) and translating between a pair of closely related languages (Serbian\nand Bosnian). Finally, we investigate how combining finite-state and\nsequence-to-sequence models at decoding time affects the output quantitatively\nand qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 00:09:24 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ryskina", "Maria", ""], ["Hovy", "Eduard", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Gormley", "Matthew R.", ""]]}, {"id": "2106.12700", "submitter": "Cheng Jie", "authors": "Cheng Jie, Da Xu, Zigeng Wang, Lu Wang, Wei Shen", "title": "An Efficient Group-based Search Engine Marketing System for E-Commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 00:12:07 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 01:27:47 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 22:39:56 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jie", "Cheng", ""], ["Xu", "Da", ""], ["Wang", "Zigeng", ""], ["Wang", "Lu", ""], ["Shen", "Wei", ""]]}, {"id": "2106.12741", "submitter": "Dalton Schutte", "authors": "Dalton Schutte, Jake Vasilakes, Anu Bompelli, Yuqi Zhou, Marcelo\n  Fiszman, Hua Xu, Halil Kilicoglu, Jeffrey R. Bishop, Terrence Adam, Rui Zhang", "title": "Discovering novel drug-supplement interactions using a dietary\n  supplements knowledge graph generated from the biomedical literature", "comments": "14 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology\nto produce a novel and comprehensive knowledge graph containing dietary\nsupplement (DS) information for discovering interactions between DS and drugs,\nor Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created\nSemRepDS (an extension of SemRep), capable of extracting semantic relations\nfrom abstracts by leveraging a DS-specific terminology (iDISK) containing\n28,884 DS terms not found in the UMLS. PubMed abstracts were processed using\nSemRepDS to generate semantic relations, which were then filtered using a\nPubMedBERT-based model to remove incorrect relations before generating our\nknowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug\ninteractions which are then evaluated by medical professionals for mechanistic\nplausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%\nmore DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT\nmodel obtained an F1 score of 0.8605 and removed 43.86% of the relations,\nimproving the precision of the relations by 26.4% compared to pre-filtering.\nSuppKG consists of 2,928 DS-specific nodes. Manual review of findings\nidentified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed\nDS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.\nDISCUSSION: The additional relations extracted using SemRepDS generated SuppKG\nthat was used to find plausible DSI not found in the current literature. By the\nnature of the SuppKG, these interactions are unlikely to have been found using\nSemRep without the expanded DS terminology. CONCLUSION: We successfully extend\nSemRep to include DS information and produce SuppKG which can be used to find\npotential DS-Drug interactions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:57:24 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Schutte", "Dalton", ""], ["Vasilakes", "Jake", ""], ["Bompelli", "Anu", ""], ["Zhou", "Yuqi", ""], ["Fiszman", "Marcelo", ""], ["Xu", "Hua", ""], ["Kilicoglu", "Halil", ""], ["Bishop", "Jeffrey R.", ""], ["Adam", "Terrence", ""], ["Zhang", "Rui", ""]]}, {"id": "2106.12744", "submitter": "Zhiyuan Chen Dr", "authors": "Jia Wei Chong, Zhiyuan Chen and Mei Shin Oh", "title": "An Automated Knowledge Mining and Document Classification System with\n  Multi-model Transfer Learning", "comments": "This paper has been submitted to journal of System and Management\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:03:46 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chong", "Jia Wei", ""], ["Chen", "Zhiyuan", ""], ["Oh", "Mei Shin", ""]]}, {"id": "2106.12767", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Dongjin Choi and Sara Evensen and \\c{C}a\\u{g}atay Demiralp and Estevam\n  Hruschka", "title": "TagRuler: Interactive Tool for Span-Level Data Programming by\n  Demonstration", "comments": "WWW'21 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 04:49:42 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Choi", "Dongjin", ""], ["Evensen", "Sara", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Hruschka", "Estevam", ""]]}, {"id": "2106.12797", "submitter": "Nawshad Farruque", "authors": "Nawshad Farruque, Randy Goebel and Osmar Zaiane", "title": "A comprehensive empirical analysis on cross-domain semantic enrichment\n  for detection of depressive language", "comments": "This is an extension over ECML-PKDD, 2019 paper \"Augmenting Semantic\n  Representation of Depressive Language: from Forums to Microblogs\", with more\n  embedding mapping/augmentation methods and data ablation tests. These\n  experiments were done in the year 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the process of creating word embedding feature representations\ndesigned for a learning task when annotated data is scarce, for example, in\ndepressive language detection from Tweets. We start with a rich word embedding\npre-trained from a large general dataset, which is then augmented with\nembeddings learned from a much smaller and more specific domain dataset through\na simple non-linear mapping mechanism. We also experimented with several other\nmore sophisticated methods of such mapping including, several auto-encoder\nbased and custom loss-function based methods that learn embedding\nrepresentations through gradually learning to be close to the words of similar\nsemantics and distant to dissimilar semantics. Our strengthened representations\nbetter capture the semantics of the depression domain, as it combines the\nsemantics learned from the specific domain coupled with word coverage from the\ngeneral language. We also present a comparative performance analyses of our\nword embedding representations with a simple bag-of-words model, well known\nsentiment and psycholinguistic lexicons, and a general pre-trained word\nembedding. When used as feature representations for several different machine\nlearning methods, including deep learning models in a depressive Tweets\nidentification task, we show that our augmented word embedding representations\nachieve a significantly better F1 score than the others, specially when applied\nto a high quality dataset. Also, we present several data ablation tests which\nconfirm the efficacy of our augmentation techniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:15:09 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Farruque", "Nawshad", ""], ["Goebel", "Randy", ""], ["Zaiane", "Osmar", ""]]}, {"id": "2106.12798", "submitter": "Marc Hanussek", "authors": "Sebastian Br\\\"andle, Marc Hanussek, Matthias Blohm, and Maximilien\n  Kintz", "title": "Evaluation of Representation Models for Text Classification with AutoML\n  Tools", "comments": "Accecpted for Future Technologies Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:19:44 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:33:15 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Br\u00e4ndle", "Sebastian", ""], ["Hanussek", "Marc", ""], ["Blohm", "Matthias", ""], ["Kintz", "Maximilien", ""]]}, {"id": "2106.12800", "submitter": "Chao-Wei Huang", "authors": "Shang-Chi Tsai, Chao-Wei Huang, Yun-Nung Chen", "title": "Modeling Diagnostic Label Correlation for Automatic ICD Coding", "comments": "NAACL 2021 Long Paper. Code available at\n  https://github.com/MiuLab/ICD-Correlation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the clinical notes written in electronic health records (EHRs), it is\nchallenging to predict the diagnostic codes which is formulated as a\nmulti-label classification task. The large set of labels, the hierarchical\ndependency, and the imbalanced data make this prediction task extremely hard.\nMost existing work built a binary prediction for each label independently,\nignoring the dependencies between labels. To address this problem, we propose a\ntwo-stage framework to improve automatic ICD coding by capturing the label\ncorrelation. Specifically, we train a label set distribution estimator to\nrescore the probability of each label set candidate generated by a base\npredictor. This paper is the first attempt at learning the label set\ndistribution as a reranking module for medical code prediction. In the\nexperiments, our proposed framework is able to improve upon best-performing\npredictors on the benchmark MIMIC datasets. The source code of this project is\navailable at https://github.com/MiuLab/ICD-Correlation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:26:30 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Tsai", "Shang-Chi", ""], ["Huang", "Chao-Wei", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2106.12806", "submitter": "Chandrahas .", "authors": "Chandrahas, Partha Pratim Talukdar", "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types", "comments": "Findings of the ACL: ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation\nphrase, tail noun phrase) triples such as (tesla, return to, new york)\nextracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap\nfor a domain, they are very sparse and far from being directly usable in an end\ntask. Therefore, the task of predicting new facts, i.e., link prediction,\nbecomes an important step while using these graphs in downstream tasks such as\ntext comprehension, question answering, and web search query recommendation.\nLearning embeddings for OpenKGs is one approach for link prediction that has\nreceived some attention lately. However, on careful examination, we found that\ncurrent OpenKG link prediction algorithms often predict noun phrases (NPs) with\nincompatible types for given noun and relation phrases. We address this problem\nin this work and propose OKGIT that improves OpenKG link prediction using novel\ntype compatibility score and type regularization. With extensive experiments on\nmultiple datasets, we show that the proposed method achieves state-of-the-art\nperformance while producing type compatible NPs in the link prediction task.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:48:05 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chandrahas", "", ""], ["Talukdar", "Partha Pratim", ""]]}, {"id": "2106.12830", "submitter": "Benjamin Murauer", "authors": "Benjamin Murauer, Michael Tschuggnall, G\\\"unther Specht", "title": "On the Influence of Machine Translation on Language Origin Obfuscation", "comments": "This was peer-reviewed, accepted and presented at\n  https://www.cicling.org/2018/, but the organizer somehow failed to publish\n  the proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the last decade, machine translation has become a popular means to deal\nwith multilingual digital content. By providing higher quality translations,\nobfuscating the source language of a text becomes more attractive. In this\npaper, we analyze the ability to detect the source language from the translated\noutput of two widely used commercial machine translation systems by utilizing\nmachine-learning algorithms with basic textual features like n-grams.\nEvaluations show that the source language can be reconstructed with high\naccuracy for documents that contain a sufficient amount of translated text. In\naddition, we analyze how the document size influences the performance of the\nprediction, as well as how limiting the set of possible source languages\nimproves the classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:33:24 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Murauer", "Benjamin", ""], ["Tschuggnall", "Michael", ""], ["Specht", "G\u00fcnther", ""]]}, {"id": "2106.12834", "submitter": "Christiaan Jacobs", "authors": "Christiaan Jacobs and Herman Kamper", "title": "Multilingual transfer of acoustic word embeddings improves when training\n  on languages related to the target zero-resource language", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic word embedding models map variable duration speech segments to fixed\ndimensional vectors, enabling efficient speech search and discovery. Previous\nwork explored how embeddings can be obtained in zero-resource settings where no\nlabelled data is available in the target language. The current best approach\nuses transfer learning: a single supervised multilingual model is trained using\nlabelled data from multiple well-resourced languages and then applied to a\ntarget zero-resource language (without fine-tuning). However, it is still\nunclear how the specific choice of training languages affect downstream\nperformance. Concretely, here we ask whether it is beneficial to use training\nlanguages related to the target. Using data from eleven languages spoken in\nSouthern Africa, we experiment with adding data from different language\nfamilies while controlling for the amount of data per language. In word\ndiscrimination and query-by-example search evaluations, we show that training\non languages from the same family gives large improvements. Through\nfiner-grained analysis, we show that training on even just a single related\nlanguage gives the largest gain. We also find that adding data from unrelated\nlanguages generally doesn't hurt performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:37:05 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jacobs", "Christiaan", ""], ["Kamper", "Herman", ""]]}, {"id": "2106.12944", "submitter": "Saneem Ahmed Chemmengath", "authors": "Yannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth\n  Bharadwaj, Mustafa Canim, Michael Glass, Alfio Gliozzo, Feifei Pan, Jaydeep\n  Sen, Karthik Sankaranarayanan, Soumen Chakrabarti", "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline\n  Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in transformers have enabled Table Question Answering (Table\nQA) systems to achieve high accuracy and SOTA results on open domain datasets\nlike WikiTableQuestions and WikiSQL. Such transformers are frequently\npre-trained on open-domain content such as Wikipedia, where they effectively\nencode questions and corresponding tables from Wikipedia as seen in Table QA\ndataset. However, web tables in Wikipedia are notably flat in their layout,\nwith the first row as the sole column header. The layout lends to a relational\nview of tables where each row is a tuple. Whereas, tables in domain-specific\nbusiness or scientific documents often have a much more complex layout,\nincluding hierarchical row and column headers, in addition to having\nspecialized vocabulary terms from that domain.\n  To address this problem, we introduce the domain-specific Table QA dataset\nAIT-QA (Airline Industry Table QA). The dataset consists of 515 questions\nauthored by human annotators on 116 tables extracted from public U.S. SEC\nfilings (publicly available at: https://www.sec.gov/edgar.shtml) of major\nairline companies for the fiscal years 2017-2019. We also provide annotations\npertaining to the nature of questions, marking those that require hierarchical\nheaders, domain-specific terminology, and paraphrased forms. Our zero-shot\nbaseline evaluation of three transformer-based SOTA Table QA methods - TaPAS\n(end-to-end), TaBERT (semantic parsing-based), and RCI (row-column\nencoding-based) - clearly exposes the limitation of these methods in this\npractical setting, with the best accuracy at just 51.8\\% (RCI). We also present\npragmatic table preprocessing steps used to pivot and project these complex\ntables into a layout suitable for the SOTA Table QA models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:14:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Katsis", "Yannis", ""], ["Chemmengath", "Saneem", ""], ["Kumar", "Vishwajeet", ""], ["Bharadwaj", "Samarth", ""], ["Canim", "Mustafa", ""], ["Glass", "Michael", ""], ["Gliozzo", "Alfio", ""], ["Pan", "Feifei", ""], ["Sen", "Jaydeep", ""], ["Sankaranarayanan", "Karthik", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2106.12976", "submitter": "Allison Lahnala", "authors": "Allison Lahnala, Yuntian Zhao, Charles Welch, Jonathan K. Kummerfeld,\n  Lawrence An, Kenneth Resnicow, Rada Mihalcea, Ver\\'onica P\\'erez-Rosas", "title": "Exploring Self-Identified Counseling Expertise in Online Support Forums", "comments": "Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of people engage in online health forums, making it\nimportant to understand the quality of the advice they receive. In this paper,\nwe explore the role of expertise in responses provided to help-seeking posts\nregarding mental health. We study the differences between (1) interactions with\npeers; and (2) interactions with self-identified mental health professionals.\nFirst, we show that a classifier can distinguish between these two groups,\nindicating that their language use does in fact differ. To understand this\ndifference, we perform several analyses addressing engagement aspects,\nincluding whether their comments engage the support-seeker further as well as\nlinguistic aspects, such as dominant language and linguistic style matching.\nOur work contributes toward the developing efforts of understanding how health\nexperts engage with health information- and support-seekers in social networks.\nMore broadly, it is a step toward a deeper understanding of the styles of\ninteractions that cultivate supportive engagement in online communities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:53:07 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lahnala", "Allison", ""], ["Zhao", "Yuntian", ""], ["Welch", "Charles", ""], ["Kummerfeld", "Jonathan K.", ""], ["An", "Lawrence", ""], ["Resnicow", "Kenneth", ""], ["Mihalcea", "Rada", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""]]}, {"id": "2106.12978", "submitter": "Georgios Damaskinos", "authors": "Alessandro Solbiati, Kevin Heffernan, Georgios Damaskinos, Shivani\n  Poddar, Shubham Modi, Jacques Cali", "title": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic segmentation of meetings is the task of dividing multi-person meeting\ntranscripts into topic blocks. Supervised approaches to the problem have proven\nintractable due to the difficulties in collecting and accurately annotating\nlarge datasets. In this paper we show how previous unsupervised topic\nsegmentation methods can be improved using pre-trained neural architectures. We\nintroduce an unsupervised approach based on BERT embeddings that achieves a\n15.5% reduction in error rate over existing unsupervised approaches applied to\ntwo popular datasets for meeting transcripts.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:54:43 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Solbiati", "Alessandro", ""], ["Heffernan", "Kevin", ""], ["Damaskinos", "Georgios", ""], ["Poddar", "Shivani", ""], ["Modi", "Shubham", ""], ["Cali", "Jacques", ""]]}, {"id": "2106.13000", "submitter": "Shammur Absar Chowdhury", "authors": "Hamdy Mubarak, Amir Hussein, Shammur Absar Chowdhury, Ahmed Ali", "title": "QASR: QCRI Aljazeera Speech Resource -- A Large Scale Annotated Arabic\n  Speech Corpus", "comments": "Speech Corpus, Spoken Conversation, ASR, Dialect Identification,\n  Punctuation Restoration, Speaker Verification, NER, Named Entity, Arabic,\n  Speaker gender, Turn-taking Accepted in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the largest transcribed Arabic speech corpus, QASR, collected\nfrom the broadcast domain. This multi-dialect speech dataset contains 2,000\nhours of speech sampled at 16kHz crawled from Aljazeera news channel. The\ndataset is released with lightly supervised transcriptions, aligned with the\naudio segments. Unlike previous datasets, QASR contains linguistically\nmotivated segmentation, punctuation, speaker information among others. QASR is\nsuitable for training and evaluating speech recognition systems, acoustics-\nand/or linguistics- based Arabic dialect identification, punctuation\nrestoration, speaker identification, speaker linking, and potentially other NLP\nmodules for spoken data. In addition to QASR transcription, we release a\ndataset of 130M words to aid in designing and training a better language model.\nWe show that end-to-end automatic speech recognition trained on QASR reports a\ncompetitive word error rate compared to the previous MGB-2 corpus. We report\nbaseline results for downstream natural language processing tasks such as named\nentity recognition using speech transcript. We also report the first baseline\nfor Arabic punctuation restoration. We make the corpus available for the\nresearch community.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:20:40 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Mubarak", "Hamdy", ""], ["Hussein", "Amir", ""], ["Chowdhury", "Shammur Absar", ""], ["Ali", "Ahmed", ""]]}, {"id": "2106.13033", "submitter": "Kuan-Yu Chen", "authors": "Ke-Han Lu, Bo-Han Fang, Kuan-Yu Chen", "title": "A Transformer-based Cross-modal Fusion Model with Adversarial Training\n  for VQA Challenge 2021", "comments": "CVPR 2021 Workshop: Visual Question Answering (VQA) Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:09:57 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Ke-Han", ""], ["Fang", "Bo-Han", ""], ["Chen", "Kuan-Yu", ""]]}, {"id": "2106.13045", "submitter": "Yannick Est\\`eve", "authors": "Sahar Ghannay, Antoine Caubri\\`ere, Salima Mdhaffar, Ga\\\"elle\n  Laperri\\`ere, Bassam Jabaian, Yannick Est\\`eve", "title": "Where are we in semantic concept extraction for Spoken Language\n  Understanding?", "comments": "Submitted to the SPECOM 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spoken language understanding (SLU) topic has seen a lot of progress these\nlast three years, with the emergence of end-to-end neural approaches. Spoken\nlanguage understanding refers to natural language processing tasks related to\nsemantic extraction from speech signal, like named entity recognition from\nspeech or slot filling task in a context of human-machine dialogue.\nClassically, SLU tasks were processed through a cascade approach that consists\nin applying, firstly, an automatic speech recognition process, followed by a\nnatural language processing module applied to the automatic transcriptions.\nThese three last years, end-to-end neural approaches, based on deep neural\nnetworks, have been proposed in order to directly extract the semantics from\nspeech signal, by using a single neural model. More recent works on\nself-supervised training with unlabeled data open new perspectives in term of\nperformance for automatic speech recognition and natural language processing.\nIn this paper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU, with or without the use of additional data. We\nalso present our last results that significantly outperform the current\nstate-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for\nthe last state-of-the-art system presented this year.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:18:32 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ghannay", "Sahar", ""], ["Caubri\u00e8re", "Antoine", ""], ["Mdhaffar", "Salima", ""], ["Laperri\u00e8re", "Ga\u00eblle", ""], ["Jabaian", "Bassam", ""], ["Est\u00e8ve", "Yannick", ""]]}, {"id": "2106.13155", "submitter": "Carlos G\\'omez-Rodr\\'iguez", "authors": "Mark Anderson and Carlos G\\'omez-Rodr\\'iguez", "title": "Splitting EUD graphs into trees: A quick and clatty approach", "comments": "IWPT 2021 Shared Task system description. To be published in\n  proceedings of the 17th International Conference on Parsing Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2021. We engaged in the task last year by focusing on efficiency.\nThis year we have focused on experimenting with new ideas on a limited time\nbudget. Our system is based on splitting the EUD graph into several trees,\nbased on linguistic criteria. We predict these trees using a sequence-labelling\nparser and combine them into an EUD graph. The results were relatively poor,\nalthough not a total disaster and could probably be improved with some\npolishing of the system's rough edges.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:28:55 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 17:01:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Anderson", "Mark", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2106.13213", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Terrance Liu, Anna Cai, Michal Muszynski, Ryo Ishii,\n  Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov,\n  Louis-Philippe Morency", "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from\n  Mobile Data", "comments": "ACL 2021. arXiv admin note: substantial text overlap with\n  arXiv:2012.02359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health conditions remain underdiagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\nfor the early detection, intervention, and treatment of mental health\ndisorders. One promising data source to help monitor human behavior is daily\nsmartphone usage. However, care must be taken to summarize behaviors without\nidentifying the user through personal (e.g., personally identifiable\ninformation) or protected (e.g., race, gender) attributes. In this paper, we\nstudy behavioral markers of daily mood using a recent dataset of mobile\nbehaviors from adolescent populations at high risk of suicidal behaviors. Using\ncomputational models, we find that language and multimodal representations of\nmobile typed text (spanning typed characters, words, keystroke timings, and app\nusage) are predictive of daily mood. However, we find that models trained to\npredict mood often also capture private user identities in their intermediate\nrepresentations. To tackle this problem, we evaluate approaches that obfuscate\nuser identity while remaining predictive. By combining multimodal\nrepresentations with privacy-preserving learning, we are able to push forward\nthe performance-privacy frontier.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:46:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liang", "Paul Pu", ""], ["Liu", "Terrance", ""], ["Cai", "Anna", ""], ["Muszynski", "Michal", ""], ["Ishii", "Ryo", ""], ["Allen", "Nicholas", ""], ["Auerbach", "Randy", ""], ["Brent", "David", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2106.13219", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov", "title": "Towards Understanding and Mitigating Social Biases in Language Models", "comments": "ICML 2021, code available at https://github.com/pliang279/LM_bias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:52:43 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liang", "Paul Pu", ""], ["Wu", "Chiyu", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2106.13302", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Alexandre Drouin, Raymond Li", "title": "byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces byteSteady -- a fast model for classification using\nbyte-level n-gram embeddings. byteSteady assumes that each input comes as a\nsequence of bytes. A representation vector is produced using the averaged\nembedding vectors of byte-level n-grams, with a pre-defined set of n. The\nhashing trick is used to reduce the number of embedding vectors. This input\nrepresentation vector is then fed into a linear classifier. A straightforward\napplication of byteSteady is text classification. We also apply byteSteady to\none type of non-language data -- DNA sequences for gene classification. For\nboth problems we achieved competitive classification results against strong\nbaselines, suggesting that byteSteady can be applied to both language and\nnon-language data. Furthermore, we find that simple compression using Huffman\ncoding does not significantly impact the results, which offers an\naccuracy-speed trade-off previously unexplored in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:14:48 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Xiang", ""], ["Drouin", "Alexandre", ""], ["Li", "Raymond", ""]]}, {"id": "2106.13316", "submitter": "Endri Kacupaj", "authors": "Endri Kacupaj, Shyamnath Premnadh, Kuldeep Singh, Jens Lehmann, Maria\n  Maleshkova", "title": "VOGUE: Answer Verbalization through Multi-Task Learning", "comments": "Machine Learning and Knowledge Discovery in Databases - European\n  Conference, ECML PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there have been significant developments in Question\nAnswering over Knowledge Graphs (KGQA). Despite all the notable advancements,\ncurrent KGQA systems only focus on answer generation techniques and not on\nanswer verbalization. However, in real-world scenarios (e.g., voice assistants\nsuch as Alexa, Siri, etc.), users prefer verbalized answers instead of a\ngenerated response. This paper addresses the task of answer verbalization for\n(complex) question answering over knowledge graphs. In this context, we propose\na multi-task-based answer verbalization framework: VOGUE (Verbalization thrOuGh\nmUlti-task lEarning). The VOGUE framework attempts to generate a verbalized\nanswer using a hybrid approach through a multi-task learning paradigm. Our\nframework can generate results based on using questions and queries as inputs\nconcurrently. VOGUE comprises four modules that are trained simultaneously\nthrough multi-task learning. We evaluate our framework on existing datasets for\nanswer verbalization, and it outperforms all current baselines on both BLEU and\nMETEOR scores.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 21:05:32 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 16:12:34 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kacupaj", "Endri", ""], ["Premnadh", "Shyamnath", ""], ["Singh", "Kuldeep", ""], ["Lehmann", "Jens", ""], ["Maleshkova", "Maria", ""]]}, {"id": "2106.13353", "submitter": "Eric Wallace", "authors": "Robert L. Logan IV, Ivana Bala\\v{z}evi\\'c, Eric Wallace, Fabio\n  Petroni, Sameer Singh, Sebastian Riedel", "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 23:38:10 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 06:30:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Logan", "Robert L.", "IV"], ["Bala\u017eevi\u0107", "Ivana", ""], ["Wallace", "Eric", ""], ["Petroni", "Fabio", ""], ["Singh", "Sameer", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2106.13375", "submitter": "Tristan Naumann", "authors": "Yu Wang, Jinchao Li, Tristan Naumann, Chenyan Xiong, Hao Cheng, Robert\n  Tinn, Cliff Wong, Naoto Usuyama, Richard Rogahn, Zhihong Shen, Yang Qin, Eric\n  Horvitz, Paul N. Bennett, Jianfeng Gao, Hoifung Poon", "title": "Domain-Specific Pretraining for Vertical Search: Case Study on\n  Biomedical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 01:02:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Yu", ""], ["Li", "Jinchao", ""], ["Naumann", "Tristan", ""], ["Xiong", "Chenyan", ""], ["Cheng", "Hao", ""], ["Tinn", "Robert", ""], ["Wong", "Cliff", ""], ["Usuyama", "Naoto", ""], ["Rogahn", "Richard", ""], ["Shen", "Zhihong", ""], ["Qin", "Yang", ""], ["Horvitz", "Eric", ""], ["Bennett", "Paul N.", ""], ["Gao", "Jianfeng", ""], ["Poon", "Hoifung", ""]]}, {"id": "2106.13382", "submitter": "Hope McGovern", "authors": "Hope McGovern", "title": "A Source-Criticism Debiasing Method for GloVe Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 01:31:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["McGovern", "Hope", ""]]}, {"id": "2106.13403", "submitter": "Ha Thanh Nguyen", "authors": "Ha-Thanh Nguyen, Vu Tran, Phuong Minh Nguyen, Thi-Hai-Yen Vuong, Quan\n  Minh Bui, Chau Minh Nguyen, Binh Tran Dang, Minh Le Nguyen, Ken Satoh", "title": "ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text\n  Processing", "comments": "Also published in COLIEE 2021's Proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity is a characteristic of natural language, which makes expression\nideas flexible. However, in a domain that requires accurate statements, it\nbecomes a barrier. Specifically, a single word can have many meanings and\nmultiple words can have the same meaning. When translating a text into a\nforeign language, the translator needs to determine the exact meaning of each\nelement in the original sentence to produce the correct translation sentence.\nFrom that observation, in this paper, we propose ParaLaw Nets, a pretrained\nmodel family using sentence-level cross-lingual information to reduce ambiguity\nand increase the performance in legal text processing. This approach achieved\nthe best result in the Question Answering task of COLIEE-2021.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 03:21:57 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Nguyen", "Ha-Thanh", ""], ["Tran", "Vu", ""], ["Nguyen", "Phuong Minh", ""], ["Vuong", "Thi-Hai-Yen", ""], ["Bui", "Quan Minh", ""], ["Nguyen", "Chau Minh", ""], ["Dang", "Binh Tran", ""], ["Nguyen", "Minh Le", ""], ["Satoh", "Ken", ""]]}, {"id": "2106.13405", "submitter": "Ha Thanh Nguyen", "authors": "Ha-Thanh Nguyen, Phuong Minh Nguyen, Thi-Hai-Yen Vuong, Quan Minh Bui,\n  Chau Minh Nguyen, Binh Tran Dang, Vu Tran, Minh Le Nguyen, Ken Satoh", "title": "JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE\n  2021", "comments": "Also published in COLIEE 2021's proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 03:31:12 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Nguyen", "Ha-Thanh", ""], ["Nguyen", "Phuong Minh", ""], ["Vuong", "Thi-Hai-Yen", ""], ["Bui", "Quan Minh", ""], ["Nguyen", "Chau Minh", ""], ["Dang", "Binh Tran", ""], ["Tran", "Vu", ""], ["Nguyen", "Minh Le", ""], ["Satoh", "Ken", ""]]}, {"id": "2106.13411", "submitter": "Subhajit Das", "authors": "Florina Dutt and Subhajit Das", "title": "Fine-grained Geolocation Prediction of Tweets with Human Machine\n  Collaboration", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is a useful resource to analyze peoples' opinions on various topics.\nOften these topics are correlated or associated with locations from where these\nTweet posts are made. For example, restaurant owners may need to know where\ntheir target customers eat with respect to the sentiment of the posts made\nrelated to food, policy planners may need to analyze citizens' opinion on\nrelevant issues such as crime, safety, congestion, etc. with respect to\nspecific parts of the city, or county or state. As promising as this is, less\nthan $1\\%$ of the crawled Tweet posts come with geolocation tags. That makes\naccurate prediction of Tweet posts for the non geo-tagged tweets very critical\nto analyze data in various domains. In this research, we utilized millions of\nTwitter posts and end-users domain expertise to build a set of deep neural\nnetwork models using natural language processing (NLP) techniques, that\npredicts the geolocation of non geo-tagged Tweet posts at various level of\ngranularities such as neighborhood, zipcode, and longitude with latitudes. With\nmultiple neural architecture experiments, and a collaborative human-machine\nworkflow design, our ongoing work on geolocation detection shows promising\nresults that empower end-users to correlate relationship between variables of\nchoice with the location information.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 03:51:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dutt", "Florina", ""], ["Das", "Subhajit", ""]]}, {"id": "2106.13474", "submitter": "Yunzhi Yao", "authors": "Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, Furu Wei", "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained\n  Language Models for Domains", "comments": "accepted as ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large pre-trained models have achieved great success in many natural language\nprocessing tasks. However, when they are applied in specific domains, these\nmodels suffer from domain shift and bring challenges in fine-tuning and online\nserving for latency and capacity constraints. In this paper, we present a\ngeneral approach to developing small, fast and effective pre-trained models for\nspecific domains. This is achieved by adapting the off-the-shelf general\npre-trained models and performing task-agnostic knowledge distillation in\ntarget domains. Specifically, we propose domain-specific vocabulary expansion\nin the adaptation stage and employ corpus level occurrence probability to\nchoose the size of incremental vocabulary automatically. Then we systematically\nexplore different strategies to compress the large pre-trained models for\nspecific domains. We conduct our experiments in the biomedical and computer\nscience domain. The experimental results demonstrate that our approach achieves\nbetter performance over the BERT BASE model in domain-specific tasks while 3.3x\nsmaller and 5.1x faster than BERT BASE. The code and pre-trained models are\navailable at https://aka.ms/adalm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 07:37:05 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 05:42:13 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yao", "Yunzhi", ""], ["Huang", "Shaohan", ""], ["Wang", "Wenhui", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""]]}, {"id": "2106.13479", "submitter": "Hieu-Thi Luong", "authors": "Hieu-Thi Luong and Junichi Yamagishi", "title": "Preliminary study on using vector quantization latent spaces for TTS/VC\n  systems with consistent performance", "comments": "to be presented at SSW11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally speaking, the main objective when training a neural speech\nsynthesis system is to synthesize natural and expressive speech from the output\nlayer of the neural network without much attention given to the hidden layers.\nHowever, by learning useful latent representation, the system can be used for\nmany more practical scenarios. In this paper, we investigate the use of\nquantized vectors to model the latent linguistic embedding and compare it with\nthe continuous counterpart. By enforcing different policies over the latent\nspaces in the training, we are able to obtain a latent linguistic embedding\nthat takes on different properties while having a similar performance in terms\nof quality and speaker similarity. Our experiments show that the voice cloning\nsystem built with vector quantization has only a small degradation in terms of\nperceptive evaluations, but has a discrete latent space that is useful for\nreducing the representation bit-rate, which is desirable for data transferring,\nor limiting the information leaking, which is important for speaker\nanonymization and other tasks of that nature.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 07:51:35 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Luong", "Hieu-Thi", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "2106.13521", "submitter": "Andargachew Mekonnen Gezmu", "authors": "Andargachew Mekonnen Gezmu, Tirufat Tesifaye Lema, Binyam Ephrem\n  Seyoum, Andreas N\\\"urnberger", "title": "Manually Annotated Spelling Error Corpus for Amharic", "comments": "Accepted to 2nd AfricaNLP Workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a manually annotated spelling error corpus for Amharic,\nlingua franca in Ethiopia. The corpus is designed to be used for the evaluation\nof spelling error detection and correction. The misspellings are tagged as\nnon-word and real-word errors. In addition, the contextual information\navailable in the corpus makes it useful in dealing with both types of spelling\nerrors.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:27:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gezmu", "Andargachew Mekonnen", ""], ["Lema", "Tirufat Tesifaye", ""], ["Seyoum", "Binyam Ephrem", ""], ["N\u00fcrnberger", "Andreas", ""]]}, {"id": "2106.13553", "submitter": "Marcos Garcia", "authors": "Marcos Garcia", "title": "Exploring the Representation of Word Meanings in Context: A Case Study\n  on Homonymy and Synonymy", "comments": "16 pages, 4 figures", "journal-ref": "ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a multilingual study of word meaning representations in\ncontext. We assess the ability of both static and contextualized models to\nadequately represent different lexical-semantic relations, such as homonymy and\nsynonymy. To do so, we created a new multilingual dataset that allows us to\nperform a controlled evaluation of several factors such as the impact of the\nsurrounding context or the overlap between words, conveying the same or\ndifferent senses. A systematic assessment on four scenarios shows that the best\nmonolingual models based on Transformers can adequately disambiguate homonyms\nin context. However, as they rely heavily on context, these models fail at\nrepresenting words with different senses when occurring in similar sentences.\nExperiments are performed in Galician, Portuguese, English, and Spanish, and\nboth the dataset (with more than 3,000 evaluation items) and new models are\nfreely released with this study.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:54:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 07:33:40 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Garcia", "Marcos", ""]]}, {"id": "2106.13627", "submitter": "Shuo Wang", "authors": "Shuo Wang, Zhaopeng Tu, Zhixing Tan, Wenxuan Wang, Maosong Sun, Yang\n  Liu", "title": "Language Models are Good Translators", "comments": "12 pages. Work in progress. An earlier verison of this manuscript is\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed the rapid advance in neural machine translation\n(NMT), the core of which lies in the encoder-decoder architecture. Inspired by\nthe recent progress of large-scale pre-trained language models on machine\ntranslation in a limited scenario, we firstly demonstrate that a single\nlanguage model (LM4MT) can achieve comparable performance with strong\nencoder-decoder NMT models on standard machine translation benchmarks, using\nthe same training data and similar amount of model parameters. LM4MT can also\neasily utilize source-side texts as additional supervision. Though modeling the\nsource- and target-language texts with the same mechanism, LM4MT can provide\nunified representations for both source and target sentences, which can better\ntransfer knowledge across languages. Extensive experiments on pivot-based and\nzero-shot translation tasks show that LM4MT can outperform the encoder-decoder\nNMT model by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:30:29 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Shuo", ""], ["Tu", "Zhaopeng", ""], ["Tan", "Zhixing", ""], ["Wang", "Wenxuan", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "2106.13711", "submitter": "Yaqing Wang", "authors": "Yaqing Wang, Fenglong Ma, Haoyu Wang, Kishlay Jha and Jing Gao", "title": "Multimodal Emergent Fake News Detection via Meta Neural Process Networks", "comments": "accepted by KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467153", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news travels at unprecedented speeds, reaches global audiences and puts\nusers and communities at great risk via social media platforms. Deep learning\nbased models show good performance when trained on large amounts of labeled\ndata on events of interest, whereas the performance of models tends to degrade\non other events due to domain shift. Therefore, significant challenges are\nposed for existing detection approaches to detect fake news on emergent events,\nwhere large-scale labeled datasets are difficult to obtain. Moreover, adding\nthe knowledge from newly emergent events requires to build a new model from\nscratch or continue to fine-tune the model, which can be challenging,\nexpensive, and unrealistic for real-world settings. In order to address those\nchallenges, we propose an end-to-end fake news detection framework named\nMetaFEND, which is able to learn quickly to detect fake news on emergent events\nwith a few verified posts. Specifically, the proposed model integrates\nmeta-learning and neural process methods together to enjoy the benefits of\nthese approaches. In particular, a label embedding module and a hard attention\nmechanism are proposed to enhance the effectiveness by handling categorical\ninformation and trimming irrelevant posts. Extensive experiments are conducted\non multimedia datasets collected from Twitter and Weibo. The experimental\nresults show our proposed MetaFEND model can detect fake news on never-seen\nevents effectively and outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:21:29 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Yaqing", ""], ["Ma", "Fenglong", ""], ["Wang", "Haoyu", ""], ["Jha", "Kishlay", ""], ["Gao", "Jing", ""]]}, {"id": "2106.13715", "submitter": "Li Dong", "authors": "Yaru Hao, Li Dong, Hangbo Bao, Ke Xu, Furu Wei", "title": "Learning to Sample Replacements for ELECTRA Pre-Training", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ELECTRA pretrains a discriminator to detect replaced tokens, where the\nreplacements are sampled from a generator trained with masked language\nmodeling. Despite the compelling performance, ELECTRA suffers from the\nfollowing two issues. First, there is no direct feedback loop from\ndiscriminator to generator, which renders replacement sampling inefficient.\nSecond, the generator's prediction tends to be over-confident along with\ntraining, making replacements biased to correct tokens. In this paper, we\npropose two methods to improve replacement sampling for ELECTRA pre-training.\nSpecifically, we augment sampling with a hardness prediction mechanism, so that\nthe generator can encourage the discriminator to learn what it has not\nacquired. We also prove that efficient sampling reduces the training variance\nof the discriminator. Moreover, we propose to use a focal loss for the\ngenerator in order to relieve oversampling of correct tokens as replacements.\nExperimental results show that our method improves ELECTRA pre-training on\nvarious downstream tasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:51:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hao", "Yaru", ""], ["Dong", "Li", ""], ["Bao", "Hangbo", ""], ["Xu", "Ke", ""], ["Wei", "Furu", ""]]}, {"id": "2106.13736", "submitter": "Shuming Ma", "authors": "Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio,\n  Saksham Singhal, Hany Hassan Awadalla, Xia Song, Furu Wei", "title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and\n  Translation by Augmenting Pretrained Multilingual Encoders", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:12:10 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Ma", "Shuming", ""], ["Dong", "Li", ""], ["Huang", "Shaohan", ""], ["Zhang", "Dongdong", ""], ["Muzio", "Alexandre", ""], ["Singhal", "Saksham", ""], ["Awadalla", "Hany Hassan", ""], ["Song", "Xia", ""], ["Wei", "Furu", ""]]}, {"id": "2106.13743", "submitter": "Nikhil Singh", "authors": "Nikhil Singh, Brandon Kates, Jeff Mentch, Anant Kharkar, Madeleine\n  Udell, Iddo Drori", "title": "Privileged Zero-Shot AutoML", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:31:05 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Singh", "Nikhil", ""], ["Kates", "Brandon", ""], ["Mentch", "Jeff", ""], ["Kharkar", "Anant", ""], ["Udell", "Madeleine", ""], ["Drori", "Iddo", ""]]}, {"id": "2106.13767", "submitter": "Hrishikesh Kulkarni", "authors": "Hrishikesh Kulkarni and Bradly Alicea", "title": "Sentiment Progression based Searching and Indexing of Literary Textual\n  Artefacts", "comments": "12 pages, 2 figures, accepted at NLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nhttp://www.cs.cmu.edu/~dbamman/booksummaries.html with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:49:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kulkarni", "Hrishikesh", ""], ["Alicea", "Bradly", ""]]}, {"id": "2106.13822", "submitter": "Rifat Shahriyar", "authors": "Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Samin,\n  Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, Rifat Shahriyar", "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44\n  Languages", "comments": "Findings of the Association for Computational Linguistics, ACL 2021\n  (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary works on abstractive text summarization have focused primarily\non high-resource languages like English, mostly due to the limited availability\nof datasets for low/mid-resource ones. In this work, we present XL-Sum, a\ncomprehensive and diverse dataset comprising 1 million professionally annotated\narticle-summary pairs from BBC, extracted using a set of carefully designed\nheuristics. The dataset covers 44 languages ranging from low to high-resource,\nfor many of which no public dataset is currently available. XL-Sum is highly\nabstractive, concise, and of high quality, as indicated by human and intrinsic\nevaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,\nwith XL-Sum and experiment on multilingual and low-resource summarization\ntasks. XL-Sum induces competitive results compared to the ones obtained using\nsimilar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10\nlanguages we benchmark on, with some of them exceeding 15, as obtained by\nmultilingual training. Additionally, training on low-resource languages\nindividually also provides competitive performance. To the best of our\nknowledge, XL-Sum is the largest abstractive summarization dataset in terms of\nthe number of samples collected from a single source and the number of\nlanguages covered. We are releasing our dataset and models to encourage future\nresearch on multilingual abstractive summarization. The resources can be found\nat \\url{https://github.com/csebuetnlp/xl-sum}.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:00:24 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hasan", "Tahmid", ""], ["Bhattacharjee", "Abhik", ""], ["Islam", "Md Saiful", ""], ["Samin", "Kazi", ""], ["Li", "Yuan-Fang", ""], ["Kang", "Yong-Bin", ""], ["Rahman", "M. Sohel", ""], ["Shahriyar", "Rifat", ""]]}, {"id": "2106.13833", "submitter": "Sara Shahmohammadi", "authors": "Sara Shahmohammadi, Hadi Veisi, Ali Darzi", "title": "Persian Rhetorical Structure Theory", "comments": "19 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past years, interest in discourse analysis and discourse parsing has\nsteadily grown, and many discourse-annotated corpora and, as a result,\ndiscourse parsers have been built. In this paper, we present a\ndiscourse-annotated corpus for the Persian language built in the framework of\nRhetorical Structure Theory as well as a discourse parser built upon the DPLP\nparser, an open-source discourse parser. Our corpus consists of 150\njournalistic texts, each text having an average of around 400 words. Corpus\ntexts were annotated using 18 discourse relations and based on the annotation\nguideline of the English RST Discourse Treebank corpus. Our text-level\ndiscourse parser is trained using gold segmentation and is built upon the DPLP\ndiscourse parser, which uses a large-margin transition-based approach to solve\nthe problem of discourse parsing. The performance of our discourse parser in\nspan (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%\nrespectively, in terms of F1 measure.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:15:47 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Shahmohammadi", "Sara", ""], ["Veisi", "Hadi", ""], ["Darzi", "Ali", ""]]}, {"id": "2106.13858", "submitter": "Ayush Singh", "authors": "Ruiyang Xu, Ayush Singh", "title": "Semantic Parsing Natural Language into Relational Algebra", "comments": "Semester project report for NLP course", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural interface to database (NLIDB) has been researched a lot during the\npast decades. In the core of NLIDB, is a semantic parser used to convert\nnatural language into SQL. Solutions from traditional NLP methodology focuses\non grammar rule pattern learning and pairing via intermediate logic forms.\nAlthough those methods give an acceptable performance on certain specific\ndatabase and parsing tasks, they are hard to generalize and scale. On the other\nhand, recent progress in neural deep learning seems to provide a promising\ndirection towards building a general NLIDB system. Unlike the traditional\napproach, those neural methodologies treat the parsing problem as a\nsequence-to-sequence learning problem. In this paper, we experimented on\nseveral sequence-to-sequence learning models and evaluate their performance on\ngeneral database parsing task.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 19:36:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Xu", "Ruiyang", ""], ["Singh", "Ayush", ""]]}, {"id": "2106.13876", "submitter": "Bodhisattwa Prasad Majumder", "authors": "Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz,\n  Julian McAuley", "title": "Rationale-Inspired Natural Language Explanations with Commonsense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 20:31:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Majumder", "Bodhisattwa Prasad", ""], ["Camburu", "Oana-Maria", ""], ["Lukasiewicz", "Thomas", ""], ["McAuley", "Julian", ""]]}, {"id": "2106.13884", "submitter": "Jacob Menick", "authors": "Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol\n  Vinyals, Felix Hill", "title": "Multimodal Few-Shot Learning with Frozen Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 21:07:09 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 10:04:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tsimpoukelli", "Maria", ""], ["Menick", "Jacob", ""], ["Cabi", "Serkan", ""], ["Eslami", "S. M. Ali", ""], ["Vinyals", "Oriol", ""], ["Hill", "Felix", ""]]}, {"id": "2106.13945", "submitter": "Wang Chen", "authors": "Wang Chen, Piji Li, Irwin King", "title": "A Training-free and Reference-free Summarization Evaluation Metric via\n  Centrality-weighted Relevance and Self-referenced Redundancy", "comments": "ACL 2021 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, reference-based and supervised summarization evaluation\nmetrics have been widely explored. However, collecting human-annotated\nreferences and ratings are costly and time-consuming. To avoid these\nlimitations, we propose a training-free and reference-free summarization\nevaluation metric. Our metric consists of a centrality-weighted relevance score\nand a self-referenced redundancy score. The relevance score is computed between\nthe pseudo reference built from the source document and the given summary,\nwhere the pseudo reference content is weighted by the sentence centrality to\nprovide importance guidance. Besides an $F_1$-based relevance score, we also\ndesign an $F_\\beta$-based variant that pays more attention to the recall score.\nAs for the redundancy score of the summary, we compute a self-masked similarity\nscore with the summary itself to evaluate the redundant information in the\nsummary. Finally, we combine the relevance and redundancy scores to produce the\nfinal evaluation score of the given summary. Extensive experiments show that\nour methods can significantly outperform existing methods on both\nmulti-document and single-document summarization evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 05:11:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Wang", ""], ["Li", "Piji", ""], ["King", "Irwin", ""]]}, {"id": "2106.13948", "submitter": "Xiaopeng Lu", "authors": "Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopeng Lu, Ingrid\n  Navarro, Jean Oh", "title": "Core Challenges in Embodied Vision-Language Planning", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 05:18:58 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 06:04:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Francis", "Jonathan", ""], ["Kitamura", "Nariaki", ""], ["Labelle", "Felix", ""], ["Lu", "Xiaopeng", ""], ["Navarro", "Ingrid", ""], ["Oh", "Jean", ""]]}, {"id": "2106.13973", "submitter": "Rakshit Naidu", "authors": "Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib\n  Singh, Fatemehsadat Mireshghallah", "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models", "comments": "4 pages, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:52:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Basu", "Priyam", ""], ["Roy", "Tiasa Singha", ""], ["Naidu", "Rakshit", ""], ["Muftuoglu", "Zumrut", ""], ["Singh", "Sahib", ""], ["Mireshghallah", "Fatemehsadat", ""]]}, {"id": "2106.14019", "submitter": "Hwanhee Lee", "authors": "Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Kyomin\n  Jung", "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive\n  Learning", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 13:27:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lee", "Hwanhee", ""], ["Yoon", "Seunghyun", ""], ["Dernoncourt", "Franck", ""], ["Bui", "Trung", ""], ["Jung", "Kyomin", ""]]}, {"id": "2106.14103", "submitter": "Hao Sun", "authors": "Pu Ren, Chengping Rao, Yang Liu, Jianxun Wang, Hao Sun", "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving\n  Spatiotemporal PDEs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 22:22:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ren", "Pu", ""], ["Rao", "Chengping", ""], ["Liu", "Yang", ""], ["Wang", "Jianxun", ""], ["Sun", "Hao", ""]]}, {"id": "2106.14127", "submitter": "Songwei Ge", "authors": "Songwei Ge and Devi Parikh", "title": "Visual Conceptual Blending with Large-scale Language and Vision Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 02:48:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ge", "Songwei", ""], ["Parikh", "Devi", ""]]}, {"id": "2106.14131", "submitter": "Mojtaba Valipour", "authors": "Mojtaba Valipour, Bowen You, Maysum Panju, Ali Ghodsi", "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 03:26:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Valipour", "Mojtaba", ""], ["You", "Bowen", ""], ["Panju", "Maysum", ""], ["Ghodsi", "Ali", ""]]}, {"id": "2106.14157", "submitter": "Fusataka Kuniyoshi", "authors": "Fusataka Kuniyoshi and Jun Ozawa and Makoto Miwa", "title": "Analyzing Research Trends in Inorganic Materials Literature Using NLP", "comments": "Accepted to ECML-PKDD2021. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of inorganic materials science, there is a growing demand to\nextract knowledge such as physical properties and synthesis processes of\nmaterials by machine-reading a large number of papers. This is because\nmaterials researchers refer to many papers in order to come up with promising\nterms of experiments for material synthesis. However, there are only a few\nsystems that can extract material names and their properties. This study\nproposes a large-scale natural language processing (NLP) pipeline for\nextracting material names and properties from materials science literature to\nenable the search and retrieval of results in materials science. Therefore, we\npropose a label definition for extracting material names and properties and\naccordingly build a corpus containing 836 annotated paragraphs extracted from\n301 papers for training a named entity recognition (NER) model. Experimental\nresults demonstrate the utility of this NER model; it achieves successful\nextraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our\napproach, we present a thorough evaluation on a real-world automatically\nannotated corpus by applying our trained NER model to 12,895 materials science\npapers. We analyze the trend in materials science by visualizing the outputs of\nthe NLP pipeline. For example, the country-by-year analysis indicates that in\nrecent years, the number of papers on \"MoS2,\" a material used in perovskite\nsolar cells, has been increasing rapidly in China but decreasing in the United\nStates. Further, according to the conditions-by-year analysis, the processing\ntemperature of the catalyst material \"PEDOT:PSS\" is shifting below 200 degree,\nand the number of reports with a processing time exceeding 5 h is increasing\nslightly.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 06:29:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kuniyoshi", "Fusataka", ""], ["Ozawa", "Jun", ""], ["Miwa", "Makoto", ""]]}, {"id": "2106.14163", "submitter": "Lianbo Ma", "authors": "Lianbo Ma, Huimin Ren, Xiliang Zhang", "title": "Effective Cascade Dual-Decoder Model for Joint Entity and Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting relational triples from texts is a fundamental task in knowledge\ngraph construction. The popular way of existing methods is to jointly extract\nentities and relations using a single model, which often suffers from the\noverlapping triple problem. That is, there are multiple relational triples that\nshare the same entities within one sentence. In this work, we propose an\neffective cascade dual-decoder approach to extract overlapping relational\ntriples, which includes a text-specific relation decoder and a\nrelation-corresponded entity decoder. Our approach is straightforward: the\ntext-specific relation decoder detects relations from a sentence according to\nits text semantics and treats them as extra features to guide the entity\nextraction; for each extracted relation, which is with trainable embedding, the\nrelation-corresponded entity decoder detects the corresponding head and tail\nentities using a span-based tagging scheme. In this way, the overlapping triple\nproblem is tackled naturally. Experiments on two public datasets demonstrate\nthat our proposed approach outperforms state-of-the-art methods and achieves\nbetter F1 scores under the strict evaluation metric. Our implementation is\navailable at https://github.com/prastunlp/DualDec.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 07:42:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ma", "Lianbo", ""], ["Ren", "Huimin", ""], ["Zhang", "Xiliang", ""]]}, {"id": "2106.14165", "submitter": "Mehrnoush Shamsfard", "authors": "Zeinab Rahimi, Mehrnoush ShamsFard", "title": "Persian Causality Corpus (PerCause) and the Causality Detection\n  Benchmark", "comments": "20 pages, 6 figures and 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing causal elements and causal relations in text is one of the\nchallenging issues in natural language processing; specifically, in low\nresource languages such as Persian. In this research we prepare a causality\nhuman annotated corpus for the Persian language which consists of 4446\nsentences and 5128 causal relations and three labels of cause, effect and\ncausal mark -- if possibl -- are specified for each relation. We have used this\ncorpus to train a system for detecting causal elements boundaries. Also, we\npresent a causality detection benchmark for three machine learning methods and\ntwo deep learning systems based on this corpus. Performance evaluations\nindicate that our best total result is obtained through CRF classifier which\nhas F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep\nlearning method with Accuracy equal to %91.4.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 07:54:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rahimi", "Zeinab", ""], ["ShamsFard", "Mehrnoush", ""]]}, {"id": "2106.14167", "submitter": "Romina Etezadi", "authors": "Romina Etezadi, Mehrnoush Shamsfard", "title": "PeCoQ: A Dataset for Persian Complex Question Answering over Knowledge\n  Graph", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.1109/IKT51791.2020.9345610", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question answering systems may find the answers to users' questions from\neither unstructured texts or structured data such as knowledge graphs.\nAnswering questions using supervised learning approaches including deep\nlearning models need large training datasets. In recent years, some datasets\nhave been presented for the task of Question answering over knowledge graphs,\nwhich is the focus of this paper. Although many datasets in English were\nproposed, there have been a few question-answering datasets in Persian. This\npaper introduces \\textit{PeCoQ}, a dataset for Persian question answering. This\ndataset contains 10,000 complex questions and answers extracted from the\nPersian knowledge graph, FarsBase. For each question, the SPARQL query and two\nparaphrases that were written by linguists are provided as well. There are\ndifferent types of complexities in the dataset, such as multi-relation,\nmulti-entity, ordinal, and temporal constraints. In this paper, we discuss the\ndataset's characteristics and describe our methodology for building it.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 08:21:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Etezadi", "Romina", ""], ["Shamsfard", "Mehrnoush", ""]]}, {"id": "2106.14233", "submitter": "Mohammad Javad Saeedizade", "authors": "Mohammad Javad Saeedizade, Najmeh Torabian, Behrouz Minaei-Bidgoli", "title": "KGRefiner: Knowledge Graph Refinement for Improving Accuracy of\n  Translational Link Prediction Methods", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Link prediction is the task of predicting missing relations between entities\nof the knowledge graph by inferring from the facts contained in it. Recent work\nin link prediction has attempted to provide a model for increasing link\nprediction accuracy by using more layers in neural network architecture or\nmethods that add to the computational complexity of models. This paper we\nproposed a method for refining the knowledge graph, which makes the knowledge\ngraph more informative, and link prediction operations can be performed more\naccurately using relatively fast translational models. Translational link\nprediction models, such as TransE, TransH, TransD, etc., have much less\ncomplexity than deep learning approaches. This method uses the hierarchy of\nrelationships and also the hierarchy of entities in the knowledge graph to add\nthe entity information as a new entity to the graph and connect it to the nodes\nwhich contain this information in their hierarchy. Our experiments show that\nour method can significantly increase the performance of translational link\nprediction methods in H@10, MR, MRR.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 13:32:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Saeedizade", "Mohammad Javad", ""], ["Torabian", "Najmeh", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2106.14282", "submitter": "Yichu Zhou", "authors": "Yichu Zhou and Vivek Srikumar", "title": "A Closer Look at How Fine-tuning Changes BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been several efforts to understand what information such\nrepresentations contain. A common strategy to use such representations is to\nfine-tune them for an end task. However, how fine-tuning for a task changes the\nunderlying space is less studied. In this work, we study the English BERT\nfamily and use two probing techniques to analyze how fine-tuning changes the\nspace. Our experiments reveal that fine-tuning improves performance because it\npushes points associated with a label away from other labels. By comparing the\nrepresentations before and after fine-tuning, we also discover that fine-tuning\ndoes not change the representations arbitrarily; instead, it adjusts the\nrepresentations to downstream tasks while preserving the original structure.\nFinally, using carefully constructed experiments, we show that fine-tuning can\nencode training sets in a representation, suggesting an overfitting problem of\na new kind.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 17:01:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Yichu", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2106.14321", "submitter": "Royi Lachmy", "authors": "Royi Lachmy, Valentina Pyatkin, Reut Tsarfaty", "title": "Draw Me a Flower: Grounding Formal Abstract Structures Stated in\n  Informal Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forming and interpreting abstraction is a core process in human\ncommunication. In particular, when giving and performing complex instructions\nstated in natural language (NL), people may naturally evoke abstract constructs\nsuch as objects, loops, conditions and functions to convey their intentions in\nan efficient and precise way. Yet, interpreting and grounding abstraction\nstated in NL has not been systematically studied in NLP/AI. To elicit\nnaturally-occurring abstractions in NL we develop the Hexagons referential\ngame, where players describe increasingly complex images on a two-dimensional\nHexagons board, and other players need to follow these instructions to recreate\nthe images. Using this game we collected the Hexagons dataset, which consists\nof 164 images and over 3000 naturally-occurring instructions, rich with diverse\nabstractions. Results of our baseline models on an instruction-to-execution\ntask derived from the Hexagons dataset confirm that higher-level abstractions\nin NL are indeed more challenging for current systems to process. Thus, this\ndataset exposes a new and challenging dimension for grounded semantic parsing,\nand we propose it for the community as a future benchmark to explore more\nsophisticated and high-level communication within NLP applications.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 21:11:16 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lachmy", "Royi", ""], ["Pyatkin", "Valentina", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2106.14332", "submitter": "Weile Wei", "authors": "Joseph Huber, Weile Wei, Giorgis Georgakoudis, Johannes Doerfert,\n  Oscar Hernandez", "title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cs.AR cs.CL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a methodology for using LLVM-based tools to tune the\nDCA++ (dynamical clusterapproximation) application that targets the new ARM\nA64FX processor. The goal is to describethe changes required for the new\narchitecture and generate efficient single instruction/multiple data(SIMD)\ninstructions that target the new Scalable Vector Extension instruction set.\nDuring manualtuning, the authors used the LLVM tools to improve code\nparallelization by using OpenMP SIMD,refactored the code and applied\ntransformation that enabled SIMD optimizations, and ensured thatthe correct\nlibraries were used to achieve optimal performance. By applying these code\nchanges, codespeed was increased by 1.98X and 78 GFlops were achieved on the\nA64FX processor. The authorsaim to automatize parts of the efforts in the\nOpenMP Advisor tool, which is built on top of existingand newly introduced LLVM\ntooling.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 22:38:16 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Huber", "Joseph", ""], ["Wei", "Weile", ""], ["Georgakoudis", "Giorgis", ""], ["Doerfert", "Johannes", ""], ["Hernandez", "Oscar", ""]]}, {"id": "2106.14361", "submitter": "Shib Sankar Dasgupta", "authors": "Shib Sankar Dasgupta, Michael Boratko, Shriya Atmakuri, Xiang Lorraine\n  Li, Dhruvesh Patel, Andrew McCallum", "title": "Word2Box: Learning Word Representation Using Box Embeddings", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning vector representations for words is one of the most fundamental\ntopics in NLP, capable of capturing syntactic and semantic relationships useful\nin a variety of downstream NLP tasks. Vector representations can be limiting,\nhowever, in that typical scoring such as dot product similarity intertwines\nposition and magnitude of the vector in space. Exciting innovations in the\nspace of representation learning have proposed alternative fundamental\nrepresentations, such as distributions, hyperbolic vectors, or regions. Our\nmodel, Word2Box, takes a region-based approach to the problem of word\nrepresentation, representing words as $n$-dimensional rectangles. These\nrepresentations encode position and breadth independently and provide\nadditional geometric operations such as intersection and containment which\nallow them to model co-occurrence patterns vectors struggle with. We\ndemonstrate improved performance on various word similarity tasks, particularly\non less common words, and perform a qualitative analysis exploring the\nadditional unique expressivity provided by Word2Box.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:17:11 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dasgupta", "Shib Sankar", ""], ["Boratko", "Michael", ""], ["Atmakuri", "Shriya", ""], ["Li", "Xiang Lorraine", ""], ["Patel", "Dhruvesh", ""], ["McCallum", "Andrew", ""]]}, {"id": "2106.14365", "submitter": "Alina Arseniev-Koehler", "authors": "Alina Arseniev-Koehler, Susan D. Cochran, Vickie M. Mays, Kai-Wei\n  Chang, Jacob Gates Foster", "title": "Integrating topic modeling and word embedding to characterize violent\n  deaths", "comments": null, "journal-ref": null, "doi": "10.31235/osf.io/nkyaq", "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:53:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Arseniev-Koehler", "Alina", ""], ["Cochran", "Susan D.", ""], ["Mays", "Vickie M.", ""], ["Chang", "Kai-Wei", ""], ["Foster", "Jacob Gates", ""]]}, {"id": "2106.14371", "submitter": "Qingjian Lin", "authors": "Qingjian Lin, Lin Yang, Xuyang Wang, Luyuan Xie, Chen Jia, Junjie Wang", "title": "Sparsely Overlapped Speech Training in the Time Domain: Joint Learning\n  of Target Speech Separation and Personal VAD Benefits", "comments": "Rejected by Interspeech 2021. Plan to commit to ICASSP 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Target speech separation is the process of filtering a certain speaker's\nvoice out of speech mixtures according to the additional speaker identity\ninformation provided. Recent works have made considerable improvement by\nprocessing signals in the time domain directly. The majority of them take fully\noverlapped speech mixtures for training. However, since most real-life\nconversations occur randomly and are sparsely overlapped, we argue that\ntraining with different overlap ratio data benefits. To do so, an unavoidable\nproblem is that the popularly used SI-SNR loss has no definition for silent\nsources. This paper proposes the weighted SI-SNR loss, together with the joint\nlearning of target speech separation and personal VAD. The weighted SI-SNR loss\nimposes a weight factor that is proportional to the target speaker's duration\nand returns zero when the target speaker is absent. Meanwhile, the personal VAD\ngenerates masks and sets non-target speech to silence. Experiments show that\nour proposed method outperforms the baseline by 1.73 dB in terms of SDR on\nfully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely\noverlapped speech of clean and noisy conditions. Besides, with slight\ndegradation in performance, our model could reduce the time costs in inference.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 02:35:03 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lin", "Qingjian", ""], ["Yang", "Lin", ""], ["Wang", "Xuyang", ""], ["Xie", "Luyuan", ""], ["Jia", "Chen", ""], ["Wang", "Junjie", ""]]}, {"id": "2106.14373", "submitter": "Fei Li", "authors": "Fei Li, Zhichao Lin, Meishan Zhang, Donghong Ji", "title": "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity\n  Recognition", "comments": "Accepted in the main conference of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Research on overlapped and discontinuous named entity recognition (NER) has\nreceived increasing attention. The majority of previous work focuses on either\noverlapped or discontinuous entities. In this paper, we propose a novel\nspan-based model that can recognize both overlapped and discontinuous entities\njointly. The model includes two major steps. First, entity fragments are\nrecognized by traversing over all possible text spans, thus, overlapped\nentities can be recognized. Second, we perform relation classification to judge\nwhether a given pair of entity fragments to be overlapping or succession. In\nthis way, we can recognize not only discontinuous entities, and meanwhile\ndoubly check the overlapped entities. As a whole, our model can be regarded as\na relation extraction paradigm essentially. Experimental results on multiple\nbenchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly\ncompetitive for overlapped and discontinuous NER.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 02:37:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Li", "Fei", ""], ["Lin", "Zhichao", ""], ["Zhang", "Meishan", ""], ["Ji", "Donghong", ""]]}, {"id": "2106.14387", "submitter": "Junyi Jessy Li", "authors": "Barea Sinno, Bernardo Oviedo, Katherine Atwell, Malihe Alikhani, Junyi\n  Jessy Li", "title": "Political Ideology and Polarization of Policy Positions: A\n  Multi-dimensional Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing political ideology and polarization is of critical importance in\nadvancing our understanding of the political context in society. Recent\nresearch has made great strides towards understanding the ideological bias\n(i.e., stance) of news media along a left-right spectrum. In this work, we take\na novel approach and study the ideology of the policy under discussion teasing\napart the nuanced co-existence of stance and ideology. Aligned with the\ntheoretical accounts in political science, we treat ideology as a\nmulti-dimensional construct, and introduce the first diachronic dataset of news\narticles whose political ideology under discussion is annotated by trained\npolitical scientists and linguists at the paragraph-level. We showcase that\nthis framework enables quantitative analysis of polarization, a temporal,\nmultifaceted measure of ideological distance. We further present baseline\nmodels for ideology prediction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 04:03:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sinno", "Barea", ""], ["Oviedo", "Bernardo", ""], ["Atwell", "Katherine", ""], ["Alikhani", "Malihe", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2106.14433", "submitter": "Sanhui Won", "authors": "Min Mao, Jiasheng Liu, Jingyao Zhou, Haipang Wu", "title": "Efficient Dialogue State Tracking by Masked Hierarchical Transformer", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our approach to DSTC 9 Track 2: Cross-lingual\nMulti-domain Dialog State Tracking, the task goal is to build a Cross-lingual\ndialog state tracker with a training set in rich resource language and a\ntesting set in low resource language. We formulate a method for joint learning\nof slot operation classification task and state tracking task respectively.\nFurthermore, we design a novel mask mechanism for fusing contextual information\nabout dialogue, the results show the proposed model achieves excellent\nperformance on DSTC Challenge II with a joint accuracy of 62.37% and 23.96% in\nMultiWOZ(en - zh) dataset and CrossWOZ(zh - en) dataset, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:35:49 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Mao", "Min", ""], ["Liu", "Jiasheng", ""], ["Zhou", "Jingyao", ""], ["Wu", "Haipang", ""]]}, {"id": "2106.14434", "submitter": "Evgeny Kotelnikov", "authors": "Evgeny Kotelnikov", "title": "Current Landscape of the Russian Sentiment Corpora", "comments": "12 pages, 5 tables, 2 figures. Accepted to Dialogue-2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Currently, there are more than a dozen Russian-language corpora for sentiment\nanalysis, differing in the source of the texts, domain, size, number and ratio\nof sentiment classes, and annotation method. This work examines publicly\navailable Russian-language corpora, presents their qualitative and quantitative\ncharacteristics, which make it possible to get an idea of the current landscape\nof the corpora for sentiment analysis. The ranking of corpora by annotation\nquality is proposed, which can be useful when choosing corpora for training and\ntesting. The influence of the training dataset on the performance of sentiment\nanalysis is investigated based on the use of the deep neural network model\nBERT. The experiments with review corpora allow us to conclude that on average\nthe quality of models increases with an increase in the number of training\ncorpora. For the first time, quality scores were obtained for the corpus of\nreviews of ROMIP seminars based on the BERT model. Also, the study proposes the\ntask of the building a universal model for sentiment analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:36:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kotelnikov", "Evgeny", ""]]}, {"id": "2106.14438", "submitter": "Evgeny Kotelnikov", "authors": "Irina Fishcheva, Valeriya Goloviznina, Evgeny Kotelnikov", "title": "Traditional Machine Learning and Deep Learning Models for Argumentation\n  Mining in Russian Texts", "comments": "13 pages, 6 tables, 4 figures. Accepted to Dialogue-2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Argumentation mining is a field of computational linguistics that is devoted\nto extracting from texts and classifying arguments and relations between them,\nas well as constructing an argumentative structure. A significant obstacle to\nresearch in this area for the Russian language is the lack of annotated\nRussian-language text corpora. This article explores the possibility of\nimproving the quality of argumentation mining using the extension of the\nRussian-language version of the Argumentative Microtext Corpus (ArgMicro) based\non the machine translation of the Persuasive Essays Corpus (PersEssays). To\nmake it possible to use these two corpora combined, we propose a Joint Argument\nAnnotation Scheme based on the schemes used in ArgMicro and PersEssays. We\nsolve the problem of classifying argumentative discourse units (ADUs) into two\nclasses - \"pro\" (\"for\") and \"opp\" (\"against\") using traditional machine\nlearning techniques (SVM, Bagging and XGBoost) and a deep neural network (BERT\nmodel). An ensemble of XGBoost and BERT models was proposed, which showed the\nhighest performance of ADUs classification for both corpora.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:44:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fishcheva", "Irina", ""], ["Goloviznina", "Valeriya", ""], ["Kotelnikov", "Evgeny", ""]]}, {"id": "2106.14444", "submitter": "Sanhui Won", "authors": "Weijie Zhang, Jiaoxuan Chen, Haipang Wu, Sanhui Wan, Gongfeng Li", "title": "A Knowledge-Grounded Dialog System Based on Pre-Trained Language Models", "comments": "7 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a knowledge-grounded dialog system developed for the ninth Dialog\nSystem Technology Challenge (DSTC9) Track 1 - Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access. We leverage\ntransfer learning with existing language models to accomplish the tasks in this\nchallenge track. Specifically, we divided the task into four sub-tasks and\nfine-tuned several Transformer models on each of the sub-tasks. We made\nadditional changes that yielded gains in both performance and efficiency,\nincluding the combination of the model with traditional entity-matching\ntechniques, and the addition of a pointer network to the output layer of the\nlanguage model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:56:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Weijie", ""], ["Chen", "Jiaoxuan", ""], ["Wu", "Haipang", ""], ["Wan", "Sanhui", ""], ["Li", "Gongfeng", ""]]}, {"id": "2106.14463", "submitter": "Saahil Jain", "authors": "Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du\n  Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P. Lungren,\n  Andrew Y. Ng, Curtis P. Langlotz, Pranav Rajpurkar", "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology\n  Reports", "comments": null, "journal-ref": null, "doi": "10.13026/hm87-5p47", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:24:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jain", "Saahil", ""], ["Agrawal", "Ashwin", ""], ["Saporta", "Adriel", ""], ["Truong", "Steven QH", ""], ["Duong", "Du Nguyen", ""], ["Bui", "Tan", ""], ["Chambon", "Pierre", ""], ["Zhang", "Yuhao", ""], ["Lungren", "Matthew P.", ""], ["Ng", "Andrew Y.", ""], ["Langlotz", "Curtis P.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2106.14464", "submitter": "Yilin Shen", "authors": "Yilin Shen, Yen-Chang Hsu, Avik Ray, Hongxia Jin", "title": "Enhancing the Generalization for Intent Classification and Out-of-Domain\n  Detection in SLU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intent classification is a major task in spoken language understanding (SLU).\nSince most models are built with pre-collected in-domain (IND) training\nutterances, their ability to detect unsupported out-of-domain (OOD) utterances\nhas a critical effect in practical use. Recent works have shown that using\nextra data and labels can improve the OOD detection performance, yet it could\nbe costly to collect such data. This paper proposes to train a model with only\nIND data while supporting both IND intent classification and OOD detection. Our\nmethod designs a novel domain-regularized module (DRM) to reduce the\noverconfident phenomenon of a vanilla classifier, achieving a better\ngeneralization in both cases. Besides, DRM can be used as a drop-in replacement\nfor the last layer in any neural network-based intent classifier, providing a\nlow-cost strategy for a significant improvement. The evaluation on four\ndatasets shows that our method built on BERT and RoBERTa models achieves\nstate-of-the-art performance against existing approaches and the strong\nbaselines we created for the comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:27:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Shen", "Yilin", ""], ["Hsu", "Yen-Chang", ""], ["Ray", "Avik", ""], ["Jin", "Hongxia", ""]]}, {"id": "2106.14574", "submitter": "Paula Czarnowska", "authors": "Paula Czarnowska, Yogarshi Vyas, Kashif Shah", "title": "Quantifying Social Biases in NLP: A Generalization and Empirical\n  Comparison of Extrinsic Fairness Metrics", "comments": "Accepted for publication in Transaction of the Association for\n  Computational Linguistics (TACL), 2021. The arXiv version is a pre-MIT Press\n  publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring bias is key for better understanding and addressing unfairness in\nNLP/ML models. This is often done via fairness metrics which quantify the\ndifferences in a model's behaviour across a range of demographic groups. In\nthis work, we shed more light on the differences and similarities between the\nfairness metrics used in NLP. First, we unify a broad range of existing metrics\nunder three generalized fairness metrics, revealing the connections between\nthem. Next, we carry out an extensive empirical comparison of existing metrics\nand demonstrate that the observed differences in bias measurement can be\nsystematically explained via differences in parameter choices for our\ngeneralized metrics.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 11:02:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Czarnowska", "Paula", ""], ["Vyas", "Yogarshi", ""], ["Shah", "Kashif", ""]]}, {"id": "2106.14609", "submitter": "Ehab Hamdy", "authors": "Ehab Hamdy", "title": "Neural Models for Offensive Language Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offensive language detection is an ever-growing natural language processing\n(NLP) application. This growth is mainly because of the widespread usage of\nsocial networks, which becomes a mainstream channel for people to communicate,\nwork, and enjoy entertainment content. Many incidents of sharing aggressive and\noffensive content negatively impacted society to a great extend. We believe\ncontributing to improving and comparing different machine learning models to\nfight such harmful contents is an important and challenging goal for this\nthesis. We targeted the problem of offensive language detection for building\nefficient automated models for offensive language detection. With the recent\nadvancements of NLP models, specifically, the Transformer model, which tackled\nmany shortcomings of the standard seq-to-seq techniques. The BERT model has\nshown state-of-the-art results on many NLP tasks. Although the literature still\nexploring the reasons for the BERT achievements in the NLP field. Other\nefficient variants have been developed to improve upon the standard BERT, such\nas RoBERTa and ALBERT. Moreover, due to the multilingual nature of text on\nsocial media that could affect the model decision on a given tween, it is\nbecoming essential to examine multilingual models such as XLM-RoBERTa trained\non 100 languages and how did it compare to unilingual models. The RoBERTa based\nmodel proved to be the most capable model and achieved the highest F1 score for\nthe tasks. Another critical aspect of a well-rounded offensive language\ndetection system is the speed at which a model can be trained and make\ninferences. In that respect, we have considered the model run-time and\nfine-tuned the very efficient implementation of FastText called BlazingText\nthat achieved good results, which is much faster than BERT-based models.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:02:45 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hamdy", "Ehab", ""]]}, {"id": "2106.14610", "submitter": "Henrique Ferraz de Arruda", "authors": "Henrique Ferraz de Arruda, Luciano da Fontoura Costa", "title": "A keyword-driven approach to science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To a good extent, words can be understood as corresponding to patterns or\ncategories that appeared in order to represent concepts and structures that are\nparticularly important or useful in a given time and space. Words are\ncharacterized by not being completely general nor specific, in the sense that\nthe same word can be instantiated or related to several different contexts,\ndepending on specific situations. Indeed, the way in which words are\ninstantiated and associated represents a particularly interesting aspect that\ncan substantially help to better understand the context in which they are\nemployed. Scientific words are no exception to that. In the present work, we\napproach the associations between a set of particularly relevant words in the\nsense of being not only frequently used in several areas, but also representing\nconcepts that are currently related to some of the main standing challenges in\nscience. More specifically, the study reported here takes into account the\nwords \"prediction\", \"model\", \"optimization\", \"complex\", \"entropy\", \"random\",\n\"deterministic\", \"pattern\", and \"database\". In order to complement the\nanalysis, we also obtain a network representing the relationship between the\nadopted areas. Many interesting results were found. First and foremost, several\nof the words were observed to have markedly distinct associations in different\nareas. Biology was found to be related to computer science, sharing\nassociations with databases. Furthermore, for most of the cases, the words\n\"complex\", \"model\", and \"prediction\" were observed to have several strong\nassociations.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 22:06:20 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 21:35:07 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["de Arruda", "Henrique Ferraz", ""], ["Costa", "Luciano da Fontoura", ""]]}, {"id": "2106.14611", "submitter": "Yu Wang", "authors": "Yu Wang, Yilin Shen, Hongxia Jin", "title": "An Adversarial Learning based Multi-Step Spoken Language Understanding\n  System through Human-Computer Interaction", "comments": "5 Pages, original work published at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing spoken language understanding systems can perform only\nsemantic frame parsing based on a single-round user query. They cannot take\nusers' feedback to update/add/remove slot values through multiround\ninteractions with users. In this paper, we introduce a novel multi-step spoken\nlanguage understanding system based on adversarial learning that can leverage\nthe multiround user's feedback to update slot values. We perform two\nexperiments on the benchmark ATIS dataset and demonstrate that the new system\ncan improve parsing performance by at least $2.5\\%$ in terms of F1, with only\none round of feedback. The improvement becomes even larger when the number of\nfeedback rounds increases. Furthermore, we also compare the new system with\nstate-of-the-art dialogue state tracking systems and demonstrate that the new\ninteractive system can perform better on multiround spoken language\nunderstanding tasks in terms of slot- and sentence-level accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 03:46:53 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Yu", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""]]}, {"id": "2106.14612", "submitter": "Robert Worden", "authors": "Robert Worden", "title": "A Theory of Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A theory of language learning is described, which uses Bayesian induction of\nfeature structures (scripts) and script functions. Each word sense in a\nlanguage is mentally represented by an m-script, a script function which\nembodies all the syntax and semantics of the word. M-scripts form a\nfully-lexicalised unification grammar, which can support adult language. Each\nword m-script can be learnt robustly from about six learning examples. The\ntheory has been implemented as a computer model, which can bootstrap-learn a\nlanguage from zero vocabulary. The Bayesian learning mechanism is (1) Capable:\nto learn arbitrarily complex meanings and syntactic structures; (2) Fast:\nlearning these structures from a few examples each; (3) Robust: learning in the\npresence of much irrelevant noise, and (4) Self-repairing: able to acquire\nimplicit negative evidence, using it to learn exceptions. Children learning\nlanguage are clearly all of (1) - (4), whereas connectionist theories fail on\n(1) and (2), and symbolic theories fail on (3) and (4). The theory is in good\nagreement with many key facts of language acquisition, including facts which\nare problematic for other theories. It is compared with over 100 key\ncross-linguistic findings about acquisition of the lexicon, phrase structure,\nmorphology, complementation and control, auxiliaries, verb argument structures,\ngaps and movement - in nearly all cases giving unforced agreement without extra\nassumptions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 11:06:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Worden", "Robert", ""]]}, {"id": "2106.14613", "submitter": "C. Maria Keet", "authors": "Zola Mahlaza and C. Maria Keet and Jarryd Dunn and Matthew Poulter", "title": "An evaluation of template and ML-based generation of user-readable text\n  from a knowledge graph", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical user-friendly renderings of knowledge graphs are visualisations and\nnatural language text. Within the latter HCI solution approach, data-driven\nnatural language generation systems receive increased attention, but they are\noften outperformed by template-based systems due to suffering from errors such\nas content dropping, hallucination, or repetition. It is unknown which of those\nerrors are associated significantly with low quality judgements by humans who\nthe text is aimed for, which hampers addressing errors based on their impact on\nimproving human evaluations. We assessed their possible association with an\nexperiment availing of expert and crowdsourced evaluations of human authored\ntext, template generated text, and sequence-to-sequence model generated text.\nThe results showed that there was no significant association between human\nauthored texts with errors and the low human judgements of naturalness and\nquality. There was also no significant association between machine learning\ngenerated texts with dropped or hallucinated slots and the low human judgements\nof naturalness and quality. Thus, both approaches appear to be viable options\nfor designing a natural language interface for knowledge graphs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:47:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mahlaza", "Zola", ""], ["Keet", "C. Maria", ""], ["Dunn", "Jarryd", ""], ["Poulter", "Matthew", ""]]}, {"id": "2106.14614", "submitter": "Haiqin Yang", "authors": "Haiqin Yang, Xiaoyuan Yao, Yiqun Duan, Jianping Shen, Jie Zhong, Kun\n  Zhang", "title": "Progressive Open-Domain Response Generation with Multiple Controllable\n  Attributes", "comments": "7 pages, 2 figures, 3 tables, in IJCAI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is desirable to include more controllable attributes to enhance the\ndiversity of generated responses in open-domain dialogue systems. However,\nexisting methods can generate responses with only one controllable attribute or\nlack a flexible way to generate them with multiple controllable attributes. In\nthis paper, we propose a Progressively trained Hierarchical Encoder-Decoder\n(PHED) to tackle this task. More specifically, PHED deploys Conditional\nVariational AutoEncoder (CVAE) on Transformer to include one aspect of\nattributes at one stage. A vital characteristic of the CVAE is to separate the\nlatent variables at each stage into two types: a global variable capturing the\ncommon semantic features and a specific variable absorbing the attribute\ninformation at that stage. PHED then couples the CVAE latent variables with the\nTransformer encoder and is trained by minimizing a newly derived ELBO and\ncontrolled losses to produce the next stage's input and produce responses as\nrequired. Finally, we conduct extensive evaluations to show that PHED\nsignificantly outperforms the state-of-the-art neural generation models and\nproduces more diverse responses as expected.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:48:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yang", "Haiqin", ""], ["Yao", "Xiaoyuan", ""], ["Duan", "Yiqun", ""], ["Shen", "Jianping", ""], ["Zhong", "Jie", ""], ["Zhang", "Kun", ""]]}, {"id": "2106.14618", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Anastasia Krithara, Konstantinos Bougiatiotis,\n  Martin Krallinger, Carlos Rodriguez-Penagos, Marta Villegas, Georgios\n  Paliouras", "title": "Overview of BioASQ 2020: The eighth BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering", "comments": "21 pages, 10 tables, 3 figures", "journal-ref": "Arampatzis A. et al. (eds) Experimental IR Meets Multilinguality,\n  Multimodality, and Interaction. CLEF 2020. Lecture Notes in Computer Science,\n  vol 12260. Springer, Cham", "doi": "10.1007/978-3-030-58219-7_16", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an overview of the eighth edition of the BioASQ\nchallenge, which ran as a lab in the Conference and Labs of the Evaluation\nForum (CLEF) 2020. BioASQ is a series of challenges aiming at the promotion of\nsystems and methodologies for large-scale biomedical semantic indexing and\nquestion answering. To this end, shared tasks are organized yearly since 2012,\nwhere different teams develop systems that compete on the same demanding\nbenchmark datasets that represent the real information needs of experts in the\nbiomedical domain. This year, the challenge has been extended with the\nintroduction of a new task on medical semantic indexing in Spanish. In total,\n34 teams with more than 100 systems participated in the three tasks of the\nchallenge. As in previous years, the results of the evaluation reveal that the\ntop-performing systems managed to outperform the strong baselines, which\nsuggests that state-of-the-art systems keep pushing the frontier of research\nthrough continuous improvements.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:24:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Krithara", "Anastasia", ""], ["Bougiatiotis", "Konstantinos", ""], ["Krallinger", "Martin", ""], ["Rodriguez-Penagos", "Carlos", ""], ["Villegas", "Marta", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2106.14619", "submitter": "Fuqi Song", "authors": "Fuqi Song", "title": "Classification of Contract-Amendment Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In Contract Life-cycle Management (CLM), managing and tracking the master\nagreements and their associated amendments is essential, in order to be kept\ninformed with different due dates and obligations. An automatic solution can\nfacilitate the daily jobs and improve the efficiency of legal practitioners. In\nthis paper, we propose an approach based on machine learning (ML) and Natural\nLanguage Processing (NLP) to detect the amendment relationship between two\ndocuments. The algorithm takes two PDF documents preprocessed by OCR (Optical\nCharacter Recognition) and NER (Named Entity Recognition) as input, and then it\nbuilds the features of each document pair and classifies the relationship. We\nexperimented with different configurations on a dataset consisting of 1124\npairs of contract-amendment documents in English and French. The best result\nobtained a F1-score of 91%, which outperformed 23% compared to a\nheuristic-based baseline.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:57:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Song", "Fuqi", ""]]}, {"id": "2106.14622", "submitter": "Swayambhu Nath Ray", "authors": "Swayambhu Nath Ray", "title": "Timestamping Documents and Beliefs", "comments": "Master's Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most of the textual information available to us are temporally variable. In a\nworld where information is dynamic, time-stamping them is a very important\ntask. Documents are a good source of information and are used for many tasks\nlike, sentiment analysis, classification of reviews etc. The knowledge of\ncreation date of documents facilitates several tasks like summarization, event\nextraction, temporally focused information extraction etc. Unfortunately, for\nmost of the documents on the web, the time-stamp meta-data is either erroneous\nor missing. Thus document dating is a challenging problem which requires\ninference over the temporal structure of the document alongside the contextual\ninformation of the document. Prior document dating systems have largely relied\non handcrafted features while ignoring such document-internal structures. In\nthis paper we propose NeuralDater, a Graph Convolutional Network (GCN) based\ndocument dating approach which jointly exploits syntactic and temporal graph\nstructures of document in a principled way. We also pointed out some\nlimitations of NeuralDater and tried to utilize both context and temporal\ninformation in documents in a more flexible and intuitive manner proposing AD3:\nAttentive Deep Document Dater, an attention-based document dating system. To\nthe best of our knowledge these are the first application of deep learning\nmethods for the task. Through extensive experiments on real-world datasets, we\nfind that our models significantly outperforms state-of-the-art baselines by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 02:12:18 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ray", "Swayambhu Nath", ""]]}, {"id": "2106.14624", "submitter": "Oliver Bensch", "authors": "Oliver Bensch, Mirela Popa and Constantin Spille", "title": "Key Information Extraction From Documents: Evaluation And Generator", "comments": "7 pages, 1 figure, accepted at the 2nd International Deep Learning\n  meets Ontologies and Natural Language Processing workshop at ESWC 2021,\n  Hersonissos, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting information from documents usually relies on natural language\nprocessing methods working on one-dimensional sequences of text. In some cases,\nfor example, for the extraction of key information from semi-structured\ndocuments, such as invoice-documents, spatial and formatting information of\ntext are crucial to understand the contextual meaning. Convolutional neural\nnetworks are already common in computer vision models to process and extract\nrelationships in multidimensional data. Therefore, natural language processing\nmodels have already been combined with computer vision models in the past, to\nbenefit from e.g. positional information and to improve performance of these\nkey information extraction models. Existing models were either trained on\nunpublished data sets or on an annotated collection of receipts, which did not\nfocus on PDF-like documents. Hence, in this research project a template-based\ndocument generator was created to compare state-of-the-art models for\ninformation extraction. An existing information extraction model \"Chargrid\"\n(Katti et al., 2019) was reconstructed and the impact of a bounding box\nregression decoder, as well as the impact of an NLP pre-processing step was\nevaluated for information extraction from documents. The results have shown\nthat NLP based pre-processing is beneficial for model performance. However, the\nuse of a bounding box regression decoder increases the model performance only\nfor fields that do not follow a rectangular shape.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:12:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bensch", "Oliver", ""], ["Popa", "Mirela", ""], ["Spille", "Constantin", ""]]}, {"id": "2106.14625", "submitter": "Leo Bouscarrat", "authors": "L\\'eo Bouscarrat (LIS, TALEP, QARMA), Antoine Bonnefoy, C\\'ecile\n  Capponi (LIS, QARMA), Carlos Ramisch (LIS, TALEP)", "title": "AMU-EURANOVA at CASE 2021 Task 1: Assessing the stability of\n  multilingual BERT", "comments": null, "journal-ref": "Proceedings of the 4th Workshop on Challenges and Applications of\n  Automated Extraction of Socio-political Events from Text (CASE 2021), Aug\n  2021, Online, Unknown Region", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explains our participation in task 1 of the CASE 2021 shared task.\nThis task is about multilingual event extraction from news. We focused on\nsub-task 4, event information extraction. This sub-task has a small training\ndataset and we fine-tuned a multilingual BERT to solve this sub-task. We\nstudied the instability problem on the dataset and tried to mitigate it.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:54:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bouscarrat", "L\u00e9o", "", "LIS, TALEP, QARMA"], ["Bonnefoy", "Antoine", "", "LIS, QARMA"], ["Capponi", "C\u00e9cile", "", "LIS, QARMA"], ["Ramisch", "Carlos", "", "LIS, TALEP"]]}, {"id": "2106.14657", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon, F. Grippa, L. Segneri", "title": "A new system for evaluating brand importance: A use case from the\n  fashion industry", "comments": null, "journal-ref": "13th ACM Web Science Conference 2021 (WebSci '21 Companion) (pp.\n  132-136). ACM, New York, NY, USA (2021)", "doi": "10.1145/3462741.3466678", "report-no": null, "categories": "cs.IR cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today brand managers and marketing specialists can leverage huge amount of\ndata to reveal patterns and trends in consumer perceptions, monitoring positive\nor negative associations of brands with respect to desired topics. In this\nstudy, we apply the Semantic Brand Score (SBS) indicator to assess brand\nimportance in the fashion industry. To this purpose, we measure and visualize\ntext data using the SBS Business Intelligence App (SBS BI), which relies on\nmethods and tools of text mining and social network analysis. We collected and\nanalyzed about 206,000 tweets that mentioned the fashion brands Fendi, Gucci\nand Prada, during the period from March 5 to March 12, 2021. From the analysis\nof the three SBS dimensions - prevalence, diversity and connectivity - we found\nthat Gucci dominated the discourse, with high values of SBS. We use this case\nstudy as an example to present a new system for evaluating brand importance and\nimage, through the analysis of (big) textual data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:04:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Segneri", "L.", ""]]}, {"id": "2106.14720", "submitter": "Curt Kohler", "authors": "Curt Kohler and Ron Daniel Jr", "title": "What's in a Measurement? Using GPT-3 on SemEval 2021 Task 8 -- MeasEval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the summer of 2020 OpenAI released its GPT-3 autoregressive language model\nto much fanfare. While the model has shown promise on tasks in several areas,\nit has not always been clear when the results were cherry-picked or when they\nwere the unvarnished output. We were particularly interested in what benefits\nGPT-3 could bring to the SemEval 2021 MeasEval task - identifying measurements\nand their associated attributes in scientific literature. We had already\nexperimented with multi-turn questions answering as a solution to this task. We\nwanted to see if we could use GPT-3's few-shot learning capabilities to more\neasily develop a solution that would have better performance than our prior\nwork. Unfortunately, we have not been successful in that effort. This paper\ndiscusses the approach we used, challenges we encountered, and results we\nobserved. Some of the problems we encountered were simply due to the state of\nthe art. For example, the limits on the size of the prompt and answer limited\nthe amount of the training signal that could be offered. Others are more\nfundamental. We are unaware of generative models that excel in retaining\nfactual information. Also, the impact of changes in the prompts is\nunpredictable, making it hard to reliably improve performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:48:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kohler", "Curt", ""], ["Daniel", "Ron", "Jr"]]}, {"id": "2106.14726", "submitter": "Florian Boudin", "authors": "Florian Boudin, Ygor Gallina, Akiko Aizawa", "title": "Keyphrase Generation for Scientific Document Retrieval", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.105", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence models have lead to significant progress in keyphrase\ngeneration, but it remains unknown whether they are reliable enough to be\nbeneficial for document retrieval. This study provides empirical evidence that\nsuch models can significantly improve retrieval performance, and introduces a\nnew extrinsic evaluation framework that allows for a better understanding of\nthe limitations of keyphrase generation models. Using this framework, we point\nout and discuss the difficulties encountered with supplementing documents with\n-- not present in text -- keyphrases, and generalizing models across domains.\nOur code is available at https://github.com/boudinfl/ir-using-kg\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:55:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Boudin", "Florian", ""], ["Gallina", "Ygor", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2106.14807", "submitter": "Jimmy Lin", "authors": "Jimmy Lin and Xueguang Ma", "title": "A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for\n  Information Retrieval Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in representational learning for information retrieval\ncan be organized in a conceptual framework that establishes two pairs of\ncontrasts: sparse vs. dense representations and unsupervised vs. learned\nrepresentations. Sparse learned representations can further be decomposed into\nexpansion and term weighting components. This framework allows us to understand\nthe relationship between recently proposed techniques such as DPR, ANCE,\nDeepCT, DeepImpact, and COIL, and furthermore, gaps revealed by our analysis\npoint to \"low hanging fruit\" in terms of techniques that have yet to be\nexplored. We present a novel technique dubbed \"uniCOIL\", a simple extension of\nCOIL that achieves to our knowledge the current state-of-the-art in sparse\nretrieval on the popular MS MARCO passage ranking dataset. Our implementation\nusing the Anserini IR toolkit is built on the Lucene search library and thus\nfully compatible with standard inverted indexes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:30:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lin", "Jimmy", ""], ["Ma", "Xueguang", ""]]}, {"id": "2106.14885", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Georgios Katsimpras, Eirini Vandorou, Anastasia\n  Krithara, Luis Gasco, Martin Krallinger, Georgios Paliouras", "title": "Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering", "comments": "25 pages, 15 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:2106.14618", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancing the state-of-the-art in large-scale biomedical semantic indexing\nand question answering is the main focus of the BioASQ challenge. BioASQ\norganizes respective tasks where different teams develop systems that are\nevaluated on the same benchmark datasets that represent the real information\nneeds of experts in the biomedical domain. This paper presents an overview of\nthe ninth edition of the BioASQ challenge in the context of the Conference and\nLabs of the Evaluation Forum (CLEF) 2021. In this year, a new question\nanswering task, named Synergy, is introduced to support researchers studying\nthe COVID-19 disease and measure the ability of the participating teams to\ndiscern information while the problem is still developing. In total, 42 teams\nwith more than 170 systems were registered to participate in the four tasks of\nthe challenge. The evaluation results, similarly to previous years, show a\nperformance gain against the baselines which indicates the continuous\nimprovement of the state-of-the-art in this field.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:03:11 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Katsimpras", "Georgios", ""], ["Vandorou", "Eirini", ""], ["Krithara", "Anastasia", ""], ["Gasco", "Luis", ""], ["Krallinger", "Martin", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2106.15065", "submitter": "Siddhant Arora", "authors": "Siddhant Arora, Alissa Ostapenko, Vijay Viswanathan, Siddharth Dalmia,\n  Florian Metze, Shinji Watanabe, Alan W Black", "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on\n  Spoken Language Understanding", "comments": "INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposable tasks are complex and comprise of a hierarchy of sub-tasks.\nSpoken intent prediction, for example, combines automatic speech recognition\nand natural language understanding. Existing benchmarks, however, typically\nhold out examples for only the surface-level sub-task. As a result, models with\nsimilar performance on these benchmarks may have unobserved performance\ndifferences on the other sub-tasks. To allow insightful comparisons between\ncompetitive end-to-end architectures, we propose a framework to construct\nrobust test sets using coordinate ascent over sub-task specific utility\nfunctions. Given a dataset for a decomposable task, our method optimally\ncreates a test set for each sub-task to individually assess sub-components of\nthe end-to-end model. Using spoken language understanding as a case study, we\ngenerate new splits for the Fluent Speech Commands and Snips SmartLights\ndatasets. Each split has two test sets: one with held-out utterances assessing\nnatural language understanding abilities, and one with held-out speakers to\ntest speech processing skills. Our splits identify performance gaps up to 10%\nbetween end-to-end systems that were within 1% of each other on the original\ntest sets. These performance gaps allow more realistic and actionable\ncomparisons between different architectures, driving future model development.\nWe release our splits and tools for the community.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 02:53:59 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Arora", "Siddhant", ""], ["Ostapenko", "Alissa", ""], ["Viswanathan", "Vijay", ""], ["Dalmia", "Siddharth", ""], ["Metze", "Florian", ""], ["Watanabe", "Shinji", ""], ["Black", "Alan W", ""]]}, {"id": "2106.15078", "submitter": "Guangyi Liu", "authors": "Guangyi Liu, Zichao Yang, Tianhua Tao, Xiaodan Liang, Zhen Li, Bowen\n  Zhou, Shuguang Cui, Zhiting Hu", "title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text\n  Generation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:59:21 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 02:34:03 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Liu", "Guangyi", ""], ["Yang", "Zichao", ""], ["Tao", "Tianhua", ""], ["Liang", "Xiaodan", ""], ["Li", "Zhen", ""], ["Zhou", "Bowen", ""], ["Cui", "Shuguang", ""], ["Hu", "Zhiting", ""]]}, {"id": "2106.15085", "submitter": "Junyi Chai", "authors": "Junyi Chai, Yujie He, Homa Hashemi, Bing Li, Daraksha Parveen,\n  Ranganath Kondapally, Wenjin Xu", "title": "Automatic Construction of Enterprise Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an automatic knowledge base construction system\nfrom large scale enterprise documents with minimal efforts of human\nintervention. In the design and deployment of such a knowledge mining system\nfor enterprise, we faced several challenges including data distributional\nshift, performance evaluation, compliance requirements and other practical\nissues. We leveraged state-of-the-art deep learning models to extract\ninformation (named entities and definitions) at per document level, then\nfurther applied classical machine learning techniques to process global\nstatistical information to improve the knowledge base. Experimental results are\nreported on actual enterprise documents. This system is currently serving as\npart of a Microsoft 365 service.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:29:02 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chai", "Junyi", ""], ["He", "Yujie", ""], ["Hashemi", "Homa", ""], ["Li", "Bing", ""], ["Parveen", "Daraksha", ""], ["Kondapally", "Ranganath", ""], ["Xu", "Wenjin", ""]]}, {"id": "2106.15102", "submitter": "Mohd Zeeshan Ansari", "authors": "M Zeeshan Ansari, Tanvir Ahmad, M M Sufyan Beg, Asma Ikram", "title": "A Simple and Efficient Probabilistic Language model for Code-Mixed Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The conventional natural language processing approaches are not accustomed to\nthe social media text due to colloquial discourse and non-homogeneous\ncharacteristics. Significantly, the language identification in a multilingual\ndocument is ascertained to be a preceding subtask in several information\nextraction applications such as information retrieval, named entity\nrecognition, relation extraction, etc. The problem is often more challenging in\ncode-mixed documents wherein foreign languages words are drawn into base\nlanguage while framing the text. The word embeddings are powerful language\nmodeling tools for representation of text documents useful in obtaining\nsimilarity between words or documents. We present a simple probabilistic\napproach for building efficient word embedding for code-mixed text and\nexemplifying it over language identification of Hindi-English short test\nmessages scrapped from Twitter. We examine its efficacy for the classification\ntask using bidirectional LSTMs and SVMs and observe its improved scores over\nvarious existing code-mixed embeddings\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 05:37:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ansari", "M Zeeshan", ""], ["Ahmad", "Tanvir", ""], ["Beg", "M M Sufyan", ""], ["Ikram", "Asma", ""]]}, {"id": "2106.15103", "submitter": "Noa Baker Gillis", "authors": "Noa Baker Gillis", "title": "Sexism in the Judiciary", "comments": "To be published in GeBNLP 2021 conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze 6.7 million case law documents to determine the presence of gender\nbias within our judicial system. We find that current bias detectino methods in\nNLP are insufficient to determine gender bias in our case law database and\npropose an alternative approach. We show that existing algorithms' inconsistent\nresults are consequences of prior research's definition of biases themselves.\nBias detection algorithms rely on groups of words to represent bias (e.g.,\n'salary,' 'job,' and 'boss' to represent employment as a potentially biased\ntheme against women in text). However, the methods to build these groups of\nwords have several weaknesses, primarily that the word lists are based on the\nresearchers' own intuitions. We suggest two new methods of automating the\ncreation of word lists to represent biases. We find that our methods outperform\ncurrent NLP bias detection methods. Our research improves the capabilities of\nNLP technology to detect bias and highlights gender biases present in\ninfluential case law. In order test our NLP bias detection method's\nperformance, we regress our results of bias in case law against U.S census data\nof women's participation in the workforce in the last 100 years.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 05:38:53 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gillis", "Noa Baker", ""]]}, {"id": "2106.15105", "submitter": "Mohd Zeeshan Ansari", "authors": "Mohd Zeeshan Ansari, Tanvir Ahmad and Noaima Bari", "title": "Language Lexicons for Hindi-English Multilingual Text Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Language Identification in textual documents is the process of automatically\ndetecting the language contained in a document based on its content. The\npresent Language Identification techniques presume that a document contains\ntext in one of the fixed set of languages, however, this presumption is\nincorrect when dealing with multilingual document which includes content in\nmore than one possible language. Due to the unavailability of large standard\ncorpora for Hindi-English mixed lingual language processing tasks we propose\nthe language lexicons, a novel kind of lexical database that supports several\nmultilingual language processing tasks. These lexicons are built by learning\nclassifiers over transliterated Hindi and English vocabulary. The designed\nlexicons possess richer quantitative characteristic than its primary source of\ncollection which is revealed using the visualization techniques.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 05:42:54 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ansari", "Mohd Zeeshan", ""], ["Ahmad", "Tanvir", ""], ["Bari", "Noaima", ""]]}, {"id": "2106.15110", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel\n  Gillick, Jacob Eisenstein, William W. Cohen", "title": "Time-Aware Language Models as Temporal Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many facts come with an expiration date, from the name of the President to\nthe basketball team Lebron James plays for. But language models (LMs) are\ntrained on snapshots of data collected at a specific moment in time, and this\ncan limit their utility, especially in the closed-book setting where the\npretraining corpus must contain the facts the model should memorize. We\nintroduce a diagnostic dataset aimed at probing LMs for factual knowledge that\nchanges over time and highlight problems with LMs at either end of the spectrum\n-- those trained on specific slices of temporal data, as well as those trained\non a wide range of temporal data. To mitigate these problems, we propose a\nsimple technique for jointly modeling text with its timestamp. This improves\nmemorization of seen facts from the training time period, as well as\ncalibration on predictions about unseen facts from future time periods. We also\nshow that models trained with temporal context can be efficiently ``refreshed''\nas new data arrives, without the need for retraining from scratch.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 06:18:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Cole", "Jeremy R.", ""], ["Eisenschlos", "Julian Martin", ""], ["Gillick", "Daniel", ""], ["Eisenstein", "Jacob", ""], ["Cohen", "William W.", ""]]}, {"id": "2106.15115", "submitter": "Rishemjit Kaur", "authors": "Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli,\n  Ravi Shekhar, Mehreen Alam and Rishemjit Kaur", "title": "Neural Machine Translation for Low-Resource Languages: A Survey", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) has seen a tremendous spurt of growth in\nless than ten years, and has already entered a mature phase. While considered\nas the most widely used solution for Machine Translation, its performance on\nlow-resource language pairs still remains sub-optimal compared to the\nhigh-resource counterparts, due to the unavailability of large parallel\ncorpora. Therefore, the implementation of NMT techniques for low-resource\nlanguage pairs has been receiving the spotlight in the recent NMT research\narena, thus leading to a substantial amount of research reported on this topic.\nThis paper presents a detailed survey of research advancements in low-resource\nlanguage NMT (LRL-NMT), along with a quantitative analysis aimed at identifying\nthe most popular solutions. Based on our findings from reviewing previous work,\nthis survey paper provides a set of guidelines to select the possible NMT\ntechnique for a given LRL data setting. It also presents a holistic view of the\nLRL-NMT research landscape and provides a list of recommendations to further\nenhance the research efforts on LRL-NMT.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 06:31:58 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ranathunga", "Surangika", ""], ["Lee", "En-Shiun Annie", ""], ["Skenduli", "Marjana Prifti", ""], ["Shekhar", "Ravi", ""], ["Alam", "Mehreen", ""], ["Kaur", "Rishemjit", ""]]}, {"id": "2106.15135", "submitter": "Fangwei Zhu", "authors": "Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui", "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wikipedia abstract generation aims to distill a Wikipedia abstract from web\nsources and has met significant success by adopting multi-document\nsummarization techniques. However, previous works generally view the abstract\nas plain text, ignoring the fact that it is a description of a certain entity\nand can be decomposed into different topics. In this paper, we propose a\ntwo-stage model TWAG that guides the abstract generation with topical\ninformation. First, we detect the topic of each input paragraph with a\nclassifier trained on existing Wikipedia articles to divide input documents\ninto different topics. Then, we predict the topic distribution of each abstract\nsentence, and decode the sentence from topic-aware representations with a\nPointer-Generator network. We evaluate our model on the WikiCatSum dataset, and\nthe results show that \\modelnames outperforms various existing baselines and is\ncapable of generating comprehensive abstracts. Our code and dataset can be\naccessed at \\url{https://github.com/THU-KEG/TWAG}\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 07:42:08 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhu", "Fangwei", ""], ["Tu", "Shangqing", ""], ["Shi", "Jiaxin", ""], ["Li", "Juanzi", ""], ["Hou", "Lei", ""], ["Cui", "Tong", ""]]}, {"id": "2106.15142", "submitter": "Zhiyue Liu", "authors": "Zhiyue Liu, Jiahai Wang, Zhenghong Li", "title": "Topic-to-Essay Generation with Comprehensive Knowledge Enhancement", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating high-quality and diverse essays with a set of topics is a\nchallenging task in natural language generation. Since several given topics\nonly provide limited source information, utilizing various topic-related\nknowledge is essential for improving essay generation performance. However,\nprevious works cannot sufficiently use that knowledge to facilitate the\ngeneration procedure. This paper aims to improve essay generation by extracting\ninformation from both internal and external knowledge. Thus, a topic-to-essay\ngeneration model with comprehensive knowledge enhancement, named TEGKE, is\nproposed. For internal knowledge enhancement, both topics and related essays\nare fed to a teacher network as source information. Then, informative features\nwould be obtained from the teacher network and transferred to a student network\nwhich only takes topics as input but provides comparable information compared\nwith the teacher network. For external knowledge enhancement, a topic knowledge\ngraph encoder is proposed. Unlike the previous works only using the nearest\nneighbors of topics in the commonsense base, our topic knowledge graph encoder\ncould exploit more structural and semantic information of the commonsense\nknowledge graph to facilitate essay generation. Moreover, the adversarial\ntraining based on the Wasserstein distance is proposed to improve generation\nquality. Experimental results demonstrate that TEGKE could achieve\nstate-of-the-art performance on both automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:01:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Liu", "Zhiyue", ""], ["Wang", "Jiahai", ""], ["Li", "Zhenghong", ""]]}, {"id": "2106.15153", "submitter": "Jinhyeok Yang", "authors": "Jinhyeok Yang, Jae-Sung Bae, Taejun Bak, Youngik Kim, Hoon-Young Cho", "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech\n  Synthesis", "comments": "Accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural multi-speaker text-to-speech (TTS) models have\nenabled the generation of reasonably good speech quality with a single model\nand made it possible to synthesize the speech of a speaker with limited\ntraining data. Fine-tuning to the target speaker data with the multi-speaker\nmodel can achieve better quality, however, there still exists a gap compared to\nthe real speech sample and the model depends on the speaker. In this work, we\npropose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts\nthe adversarial training method to a non-autoregressive multi-speaker TTS\nmodel. In addition, we propose simple but efficient automatic scaling methods\nfor feature matching loss used in adversarial training. In the subjective\nlistening tests, GANSpeech significantly outperformed the baseline\nmulti-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score\nthan the speaker-specific fine-tuned FastSpeech2.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:15:30 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yang", "Jinhyeok", ""], ["Bae", "Jae-Sung", ""], ["Bak", "Taejun", ""], ["Kim", "Youngik", ""], ["Cho", "Hoon-Young", ""]]}, {"id": "2106.15167", "submitter": "MeiHan Tong", "authors": "Meihan Tong, Shuai Wang, Bin Xu, Yixin Cao, Minghui Liu, Lei Hou,\n  Juanzi Li", "title": "Learning from Miscellaneous Other-Class Words for Few-shot Named Entity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot Named Entity Recognition (NER) exploits only a handful of\nannotations to identify and classify named entity mentions. Prototypical\nnetwork shows superior performance on few-shot NER. However, existing\nprototypical methods fail to differentiate rich semantics in other-class words,\nwhich will aggravate overfitting under few shot scenario. To address the issue,\nwe propose a novel model, Mining Undefined Classes from Other-class (MUCO),\nthat can automatically induce different undefined classes from the other class\nto improve few-shot NER. With these extra-labeled undefined classes, our method\nwill improve the discriminative ability of NER classifier and enhance the\nunderstanding of predefined classes with stand-by semantic knowledge.\nExperimental results demonstrate that our model outperforms five\nstate-of-the-art models in both 1-shot and 5-shots settings on four NER\nbenchmarks. We will release the code upon acceptance. The source code is\nreleased on https: //github.com/shuaiwa16/OtherClassNER.git.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:28:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Tong", "Meihan", ""], ["Wang", "Shuai", ""], ["Xu", "Bin", ""], ["Cao", "Yixin", ""], ["Liu", "Minghui", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""]]}, {"id": "2106.15195", "submitter": "Benjamin Marie", "authors": "Benjamin Marie, Atsushi Fujita, Raphael Rubino", "title": "Scientific Credibility of Machine Translation Research: A\n  Meta-Evaluation of 769 Papers", "comments": "Camera-ready for ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first large-scale meta-evaluation of machine\ntranslation (MT). We annotated MT evaluations conducted in 769 research papers\npublished from 2010 to 2020. Our study shows that practices for automatic MT\nevaluation have dramatically changed during the past decade and follow\nconcerning trends. An increasing number of MT evaluations exclusively rely on\ndifferences between BLEU scores to draw conclusions, without performing any\nkind of statistical significance testing nor human evaluation, while at least\n108 metrics claiming to be better than BLEU have been proposed. MT evaluations\nin recent papers tend to copy and compare automatic metric scores from previous\nwork to claim the superiority of a method or an algorithm without confirming\nneither exactly the same training, validating, and testing data have been used\nnor the metric scores are comparable. Furthermore, tools for reporting\nstandardized metric scores are still far from being widely adopted by the MT\ncommunity. After showing how the accumulation of these pitfalls leads to\ndubious evaluation, we propose a guideline to encourage better automatic MT\nevaluation along with a simple meta-evaluation scoring method to assess its\ncredibility.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:30:17 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Marie", "Benjamin", ""], ["Fujita", "Atsushi", ""], ["Rubino", "Raphael", ""]]}, {"id": "2106.15217", "submitter": "Jianhao Yan", "authors": "Jianhao Yan, Chenming Wu, Fandong Meng, Jie Zhou", "title": "Rethinking the Evaluation of Neural Machine Translation", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The evaluation of neural machine translation systems is usually built upon\ngenerated translation of a certain decoding method (e.g., beam search) with\nevaluation metrics over the generated translation (e.g., BLEU). However, this\nevaluation framework suffers from high search errors brought by heuristic\nsearch algorithms and is limited by its nature of evaluation over one best\ncandidate. In this paper, we propose a novel evaluation protocol, which not\nonly avoids the effect of search errors but provides a system-level evaluation\nin the perspective of model ranking. In particular, our method is based on our\nnewly proposed exact top-$k$ decoding instead of beam search. Our approach\nevaluates model errors by the distance between the candidate spaces scored by\nthe references and the model respectively. Extensive experiments on WMT'14\nEnglish-German demonstrate that bad ranking ability is connected to the\nwell-known beam search curse, and state-of-the-art Transformer models are\nfacing serious ranking errors. By evaluating various model architectures and\ntechniques, we provide several interesting findings. Finally, to effectively\napproximate the exact search algorithm with same time cost as original beam\nsearch, we present a minimum heap augmented beam search algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:59:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yan", "Jianhao", ""], ["Wu", "Chenming", ""], ["Meng", "Fandong", ""], ["Zhou", "Jie", ""]]}, {"id": "2106.15223", "submitter": "Wessel Radstok", "authors": "Wessel Radstok and Mel Chekol", "title": "Leveraging Static Models for Link Prediction in Temporal Knowledge\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)\npresents significant opportunities for improving the resulting embeddings, and\nconsequently for increased performance in downstream applications. Yet, little\nresearch effort has focussed on this area and much of the carried out research\nreports only marginally improved results compared to models trained without\ntemporal scopes (static models). Furthermore, rather than leveraging existing\nwork on static models, they introduce new models specific to temporal knowledge\ngraphs. We propose a novel perspective that takes advantage of the power of\nexisting static embedding models by focussing effort on manipulating the data\ninstead. Our method, SpliMe, draws inspiration from the field of signal\nprocessing and early work in graph embedding. We show that SpliMe competes with\nor outperforms the current state of the art in temporal KGE. Additionally, we\nuncover issues with the procedure currently used to assess the performance of\nstatic models on temporal graphs and introduce two ways to counteract them.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:15:17 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Radstok", "Wessel", ""], ["Chekol", "Mel", ""]]}, {"id": "2106.15231", "submitter": "Linyi Yang", "authors": "Linyi Yang, Jiazheng Li, P\\'adraig Cunningham, Yue Zhang, Barry Smyth,\n  Ruihai Dong", "title": "Exploring the Efficacy of Automatically Generated Counterfactuals for\n  Sentiment Analysis", "comments": "ACL-21, Main Conference, Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While state-of-the-art NLP models have been achieving the excellent\nperformance of a wide range of tasks in recent years, important questions are\nbeing raised about their robustness and their underlying sensitivity to\nsystematic biases that may exist in their training and test data. Such issues\ncome to be manifest in performance problems when faced with out-of-distribution\ndata in the field. One recent solution has been to use counterfactually\naugmented datasets in order to reduce any reliance on spurious patterns that\nmay exist in the original data. Producing high-quality augmented data can be\ncostly and time-consuming as it usually needs to involve human feedback and\ncrowdsourcing efforts. In this work, we propose an alternative by describing\nand evaluating an approach to automatically generating counterfactual data for\ndata augmentation and explanation. A comprehensive evaluation on several\ndifferent datasets and using a variety of state-of-the-art benchmarks\ndemonstrate how our approach can achieve significant improvements in model\nperformance when compared to models training on the original data and even when\ncompared to models trained with the benefit of human-generated augmented data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:27:01 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 04:56:27 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Yang", "Linyi", ""], ["Li", "Jiazheng", ""], ["Cunningham", "P\u00e1draig", ""], ["Zhang", "Yue", ""], ["Smyth", "Barry", ""], ["Dong", "Ruihai", ""]]}, {"id": "2106.15236", "submitter": "Jaafar Hammoud", "authors": "Jaafar Hammoud, Aleksandra Vatian, Natalia Dobrenko, Nikolai\n  Vedernikov, Anatoly Shalyto, Natalia Gusarova", "title": "New Arabic Medical Dataset for Diseases Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Arabic language suffers from a great shortage of datasets suitable for\ntraining deep learning models, and the existing ones include general\nnon-specialized classifications. In this work, we introduce a new Arab medical\ndataset, which includes two thousand medical documents collected from several\nArabic medical websites, in addition to the Arab Medical Encyclopedia. The\ndataset was built for the task of classifying texts and includes 10 classes\n(Blood, Bone, Cardiovascular, Ear, Endocrine, Eye, Gastrointestinal, Immune,\nLiver and Nephrological) diseases. Experiments on the dataset were performed by\nfine-tuning three pre-trained models: BERT from Google, Arabert that based on\nBERT with large Arabic corpus, and AraBioNER that based on Arabert with Arabic\nmedical corpus.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:42:53 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 10:45:54 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 12:41:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hammoud", "Jaafar", ""], ["Vatian", "Aleksandra", ""], ["Dobrenko", "Natalia", ""], ["Vedernikov", "Nikolai", ""], ["Shalyto", "Anatoly", ""], ["Gusarova", "Natalia", ""]]}, {"id": "2106.15238", "submitter": "Ashish Mittal", "authors": "Ashish Mittal, Samarth Bharadwaj, Shreya Khare, Saneem Chemmengath,\n  Karthik Sankaranarayanan, Brian Kingsbury", "title": "Representation based meta-learning for few-shot spoken intent\n  recognition", "comments": "Accepted paper at Interspeech 2020, 21st Annual Conference of the\n  International Speech Communication Association, Virtual Event, Shanghai,\n  China, 25-29 October, 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-3208", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spoken intent detection has become a popular approach to interface with\nvarious smart devices with ease. However, such systems are limited to the\npreset list of intents-terms or commands, which restricts the quick\ncustomization of personal devices to new intents. This paper presents a\nfew-shot spoken intent classification approach with task-agnostic\nrepresentations via meta-learning paradigm. Specifically, we leverage the\npopular representation-based meta-learning learning to build a task-agnostic\nrepresentation of utterances, that then use a linear classifier for prediction.\nWe evaluate three such approaches on our novel experimental protocol developed\non two popular spoken intent classification datasets: Google Commands and the\nFluent Speech Commands dataset. For a 5-shot (1-shot) classification of novel\nclasses, the proposed framework provides an average classification accuracy of\n88.6% (76.3%) on the Google Commands dataset, and 78.5% (64.2%) on the Fluent\nSpeech Commands dataset. The performance is comparable to traditionally\nsupervised classification models with abundant training samples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:46:35 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Mittal", "Ashish", ""], ["Bharadwaj", "Samarth", ""], ["Khare", "Shreya", ""], ["Chemmengath", "Saneem", ""], ["Sankaranarayanan", "Karthik", ""], ["Kingsbury", "Brian", ""]]}, {"id": "2106.15247", "submitter": "Peter Ochieng", "authors": "Peter Ochieng and Dennis Mugambi", "title": "Unsupervised Technique To Conversational Machine Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Conversational machine reading (CMR) tools have seen a rapid progress in the\nrecent past. The current existing tools rely on the supervised learning\ntechnique which require labeled dataset for their training. The supervised\ntechnique necessitates that for every new rule text, a manually labeled dataset\nmust be created. This is tedious and error prone. This paper introduces and\ndemonstrates how unsupervised learning technique can be applied in the\ndevelopment of CMR. Specifically, we demonstrate how unsupervised learning can\nbe used in rule extraction and entailment modules of CMR. Compared to the\ncurrent best CMR tool, our developed framework reports 3.3% improvement in\nmicro averaged accuracy and 1.4 % improvement in macro averaged accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:59:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ochieng", "Peter", ""], ["Mugambi", "Dennis", ""]]}, {"id": "2106.15313", "submitter": "Shivam Patel", "authors": "Kalliath Abdul Rasheed Issam, Shivam Patel, Subalalitha C. N", "title": "Topic Modeling Based Extractive Text Summarization", "comments": "10 pages, 13 figures, 3 tables", "journal-ref": "International Journal of Innovative Technology and Exploring\n  Engineering, Volume-9 Issue-6, April 2020, Page No. 1710-1719", "doi": "10.35940/ijitee.F4611.049620", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Text summarization is an approach for identifying important information\npresent within text documents. This computational technique aims to generate\nshorter versions of the source text, by including only the relevant and salient\ninformation present within the source text. In this paper, we propose a novel\nmethod to summarize a text document by clustering its contents based on latent\ntopics produced using topic modeling techniques and by generating extractive\nsummaries for each of the identified text clusters. All extractive\nsub-summaries are later combined to generate a summary for any given source\ndocument. We utilize the lesser used and challenging WikiHow dataset in our\napproach to text summarization. This dataset is unlike the commonly used news\ndatasets which are available for text summarization. The well-known news\ndatasets present their most important information in the first few lines of\ntheir source texts, which make their summarization a lesser challenging task\nwhen compared to summarizing the WikiHow dataset. Contrary to these news\ndatasets, the documents in the WikiHow dataset are written using a generalized\napproach and have lesser abstractedness and higher compression ratio, thus\nproposing a greater challenge to generate summaries. A lot of the current\nstate-of-the-art text summarization techniques tend to eliminate important\ninformation present in source documents in the favor of brevity. Our proposed\ntechnique aims to capture all the varied information present in source\ndocuments. Although the dataset proved challenging, after performing extensive\ntests within our experimental setup, we have discovered that our model produces\nencouraging ROUGE results and summaries when compared to the other published\nextractive and abstractive text summarization models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:28:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Issam", "Kalliath Abdul Rasheed", ""], ["Patel", "Shivam", ""], ["N", "Subalalitha C.", ""]]}, {"id": "2106.15343", "submitter": "Alekhya Akkinepally", "authors": "Tabish Maniar, Alekhya Akkinepally, Anantha Sharma", "title": "Differential Privacy for Credit Risk Model", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of machine learning algorithms to model user behavior and drive\nbusiness decisions has become increasingly commonplace, specifically providing\nintelligent recommendations to automated decision making. This has led to an\nincrease in the use of customers personal data to analyze customer behavior and\npredict their interests in a companys products. Increased use of this customer\npersonal data can lead to better models but also to the potential of customer\ndata being leaked, reverse engineered, and mishandled. In this paper, we assess\ndifferential privacy as a solution to address these privacy problems by\nbuilding privacy protections into the data engineering and model training\nstages of predictive model development. Our interest is a pragmatic\nimplementation in an operational environment, which necessitates a general\npurpose differentially private modeling framework, and we evaluate one such\ntool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk\nModel is a major modeling methodology in banking and finance where user data is\nanalyzed to determine the total Expected Loss to the bank. We examine the\napplication of differential privacy on the credit risk model and evaluate the\nperformance of a Differentially Private Model with a Non Differentially Private\nModel. Credit Risk Model is a major modeling methodology in banking and finance\nwhere users data is analyzed to determine the total Expected Loss to the bank.\nIn this paper, we explore the application of differential privacy on the credit\nrisk model and evaluate the performance of a Non Differentially Private Model\nwith Differentially Private Model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:58:49 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Maniar", "Tabish", ""], ["Akkinepally", "Alekhya", ""], ["Sharma", "Anantha", ""]]}, {"id": "2106.15355", "submitter": "Ana Valeria Gonzalez", "authors": "Ana Valeria Gonzalez, Anna Rogers, Anders S{\\o}gaard", "title": "On the Interaction of Belief Bias and Explanations", "comments": "accepted at findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A myriad of explainability methods have been proposed in recent years, but\nthere is little consensus on how to evaluate them. While automatic metrics\nallow for quick benchmarking, it isn't clear how such metrics reflect human\ninteraction with explanations. Human evaluation is of paramount importance, but\nprevious protocols fail to account for belief biases affecting human\nperformance, which may lead to misleading conclusions. We provide an overview\nof belief bias, its role in human evaluation, and ideas for NLP practitioners\non how to account for it. For two experimental paradigms, we present a case\nstudy of gradient-based explainability introducing simple ways to account for\nhumans' prior beliefs: models of varying quality and adversarial examples. We\nshow that conclusions about the highest performing methods change when\nintroducing such controls, pointing to the importance of accounting for belief\nbias in evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:49:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gonzalez", "Ana Valeria", ""], ["Rogers", "Anna", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2106.15467", "submitter": "Shanshan Wang", "authors": "Shanshan Wang, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Huasheng Liang,\n  Qiang Yan, Evangelos Kanoulas, Maarten de Rijke", "title": "Few-Shot Electronic Health Record Coding through Graph Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electronic health record (EHR) coding is the task of assigning ICD codes to\neach EHR. Most previous studies either only focus on the frequent ICD codes or\ntreat rare and frequent ICD codes in the same way. These methods perform well\non frequent ICD codes but due to the extremely unbalanced distribution of ICD\ncodes, the performance on rare ones is far from satisfactory. We seek to\nimprove the performance for both frequent and rare ICD codes by using a\ncontrastive graph-based EHR coding framework, CoGraph, which re-casts EHR\ncoding as a few-shot learning task. First, we construct a heterogeneous EHR\nword-entity (HEWE) graph for each EHR, where the words and entities extracted\nfrom an EHR serve as nodes and the relations between them serve as edges. Then,\nCoGraph learns similarities and dissimilarities between HEWE graphs from\ndifferent ICD codes so that information can be transferred among them. In a\nfew-shot learning scenario, the model only has access to frequent ICD codes\nduring training, which might force it to encode features that are useful for\nfrequent ICD codes only. To mitigate this risk, CoGraph devises two graph\ncontrastive learning schemes, GSCL and GECL, that exploit the HEWE graph\nstructures so as to encode transferable features. GSCL utilizes the\nintra-correlation of different sub-graphs sampled from HEWE graphs while GECL\nexploits the inter-correlation among HEWE graphs at different clinical stages.\nExperiments on the MIMIC-III benchmark dataset show that CoGraph significantly\noutperforms state-of-the-art methods on EHR coding, not only on frequent ICD\ncodes, but also on rare codes, in terms of several evaluation indicators. On\nfrequent ICD codes, GSCL and GECL improve the classification accuracy and F1 by\n1.31% and 0.61%, respectively, and on rare ICD codes CoGraph has more obvious\nimprovements by 2.12% and 2.95%.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 14:53:17 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Shanshan", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Liang", "Huasheng", ""], ["Yan", "Qiang", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2106.15498", "submitter": "Gerhard Hagerer", "authors": "Gerhard Hagerer and Wenbin Le and Hannah Danner and Georg Groh", "title": "Classification of Consumer Belief Statements From Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:25:33 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hagerer", "Gerhard", ""], ["Le", "Wenbin", ""], ["Danner", "Hannah", ""], ["Groh", "Georg", ""]]}, {"id": "2106.15537", "submitter": "Narinder Punn", "authors": "Gaurav Rajput, Narinder Singh punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "Hate speech detection using static BERT embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing popularity of social media platforms hate speech is emerging\nas a major concern, where it expresses abusive speech that targets specific\ngroup characteristics, such as gender, religion or ethnicity to spread\nviolence. Earlier people use to verbally deliver hate speeches but now with the\nexpansion of technology, some people are deliberately using social media\nplatforms to spread hate by posting, sharing, commenting, etc. Whether it is\nChristchurch mosque shootings or hate crimes against Asians in west, it has\nbeen observed that the convicts are very much influenced from hate text present\nonline. Even though AI systems are in place to flag such text but one of the\nkey challenges is to reduce the false positive rate (marking non hate as hate),\nso that these systems can detect hate speech without undermining the freedom of\nexpression. In this paper, we use ETHOS hate speech detection dataset and\nanalyze the performance of hate speech detection classifier by replacing or\nintegrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with\nstatic BERT embeddings (BE). With the extensive experimental trails it is\nobserved that the neural network performed better with static BE compared to\nusing FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,\none metric that significantly improved is specificity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:17:10 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Rajput", "Gaurav", ""], ["punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2106.15561", "submitter": "Xu Tan", "authors": "Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu", "title": "A Survey on Neural Speech Synthesis", "comments": "A comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:50:51 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 15:58:54 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 12:32:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Soong", "Frank", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.15674", "submitter": "Seyyed Ehsan Mahmoudi", "authors": "Seyyed Ehsan Mahmoudi and Mehrnoush Shamsfard", "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 18:43:06 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Mahmoudi", "Seyyed Ehsan", ""], ["Shamsfard", "Mehrnoush", ""]]}, {"id": "2106.15684", "submitter": "Morteza Rohanian", "authors": "Morteza Rohanian, Julian Hough, Matthew Purver", "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and\n  Speech Pause Features Robust to Noisy Inputs", "comments": "INTERSPEECH 2021. arXiv admin note: substantial text overlap with\n  arXiv:2106.09668", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present two multimodal fusion-based deep learning models that consume ASR\ntranscribed speech and acoustic data simultaneously to classify whether a\nspeaker in a structured diagnostic task has Alzheimer's Disease and to what\ndegree, evaluating the ADReSSo challenge 2021 data. Our best model, a BiLSTM\nwith highway layers using words, word probabilities, disfluency features, pause\ninformation, and a variety of acoustic features, achieves an accuracy of 84%\nand RSME error prediction of 4.26 on MMSE cognitive scores. While predicting\ncognitive decline is more challenging, our models show improvement using the\nmultimodal approach and word probabilities, disfluency and pause information\nover word-only models. We show considerable gains for AD classification using\nmultimodal fusion and gating, which can effectively deal with noisy inputs from\nacoustic features and ASR hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:24:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rohanian", "Morteza", ""], ["Hough", "Julian", ""], ["Purver", "Matthew", ""]]}, {"id": "2106.15760", "submitter": "Tung Nguyen Thanh", "authors": "Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li", "title": "A Conditional Splitting Framework for Efficient Constituency Parsing", "comments": "Accepted to ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generic seq2seq parsing framework that casts constituency\nparsing problems (syntactic and discourse parsing) into a series of conditional\nsplitting decisions. Our parsing model estimates the conditional probability\ndistribution of possible splitting points in a given text span and supports\nefficient top-down decoding, which is linear in number of nodes. The\nconditional splitting formulation together with efficient beam search inference\nfacilitate structural consistency without relying on expensive structured\ninference. Crucially, for discourse analysis we show that in our formulation,\ndiscourse segmentation can be framed as a special case of parsing which allows\nus to perform discourse parsing without requiring segmentation as a\npre-requisite. Experiments show that our model achieves good results on the\nstandard syntactic parsing tasks under settings with/without pre-trained\nrepresentations and rivals state-of-the-art (SoTA) methods that are more\ncomputationally expensive than ours. In discourse parsing, our method\noutperforms SoTA by a good margin.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 00:36:34 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Nguyen", "Thanh-Tung", ""], ["Nguyen", "Xuan-Phi", ""], ["Joty", "Shafiq", ""], ["Li", "Xiaoli", ""]]}, {"id": "2106.15772", "submitter": "Chao-Chun Liang", "authors": "Shen-Yun Miao, Chao-Chun Liang, Keh-Yih Su", "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem\n  Solvers", "comments": "ACL-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms\nof both language patterns and problem types) English math word problem (MWP)\ncorpus for evaluating the capability of various MWP solvers. Existing MWP\ncorpora for studying AI progress remain limited either in language usage\npatterns or in problem types. We thus present a new English MWP corpus with\n2,305 MWPs that cover more text patterns and most problem types taught in\nelementary school. Each MWP is annotated with its problem type and grade level\n(for indicating the level of difficulty). Furthermore, we propose a metric to\nmeasure the lexicon usage diversity of a given MWP corpus, and demonstrate that\nASDiv is more diverse than existing corpora. Experiments show that our proposed\ncorpus reflects the true capability of MWP solvers more faithfully.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 01:54:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Miao", "Shen-Yun", ""], ["Liang", "Chao-Chun", ""], ["Su", "Keh-Yih", ""]]}, {"id": "2106.15818", "submitter": "Kelly Marchisio", "authors": "Kelly Marchisio, Markus Freitag, David Grangier", "title": "What Can Unsupervised Machine Translation Contribute to High-Resource\n  Language Pairs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas existing literature on unsupervised machine translation (MT) focuses\non exploiting unsupervised techniques for low-resource language pairs where\nbilingual training data is scare or unavailable, we investigate whether\nunsupervised MT can also improve translation quality of high-resource language\npairs where sufficient bitext does exist. We compare the style of correct\ntranslations generated by either supervised or unsupervised MT and find that\nthe unsupervised output is less monotonic and more natural than supervised\noutput. We demonstrate a way to combine the benefits of unsupervised and\nsupervised MT into a single system, resulting in better human evaluation of\nquality and fluency. Our results open the door to discussions about the\npotential contributions of unsupervised MT in high-resource settings, and how\nsupervised and unsupervised systems might be mutually-beneficial.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 05:44:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Marchisio", "Kelly", ""], ["Freitag", "Markus", ""], ["Grangier", "David", ""]]}, {"id": "2106.15825", "submitter": "Benedikt Boenninghoff", "authors": "Benedikt Boenninghoff, Robert M. Nickel, Dorothea Kolossa", "title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in\n  Authorship Verification", "comments": "PAN@CLEF 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set AV task to a\ncross-topic/open-set AV task over a collection of fanfiction texts. In this\nwork, we present a novel hybrid neural-probabilistic framework that is designed\nto tackle the challenges of the 2021 task. Our system is based on our 2020\nwinning submission, with updates to significantly reduce sensitivities to\ntopical variations and to further improve the system's calibration by means of\nan uncertainty-adaptation layer. Our framework additionally includes an\nout-of-distribution detector (O2D2) for defining non-responses. Our proposed\nsystem outperformed all other systems that participated in the PAN 2021 AV\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 06:10:43 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 05:45:03 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Boenninghoff", "Benedikt", ""], ["Nickel", "Robert M.", ""], ["Kolossa", "Dorothea", ""]]}, {"id": "2106.15838", "submitter": "Liliang Ren", "authors": "Liliang Ren, Chenkai Sun, Heng Ji, Julia Hockenmaier", "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction", "comments": "Accepted by ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-Graph extraction aims to automatically extract information graphs\nconsisting of mentions and types from natural language texts. Existing\napproaches, such as table filling and pairwise scoring, have shown impressive\nperformance on various information extraction tasks, but they are difficult to\nscale to datasets with longer input texts because of their second-order\nspace/time complexities with respect to the input length. In this work, we\npropose a Hybrid Span Generator (HySPA) that invertibly maps the information\ngraph to an alternating sequence of nodes and edge types, and directly\ngenerates such sequences via a hybrid span decoder which can decode both the\nspans and the types recurrently in linear time and space complexities.\nExtensive experiments on the ACE05 dataset show that our approach also\nsignificantly outperforms state-of-the-art on the joint entity and relation\nextraction task.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 06:44:22 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ren", "Liliang", ""], ["Sun", "Chenkai", ""], ["Ji", "Heng", ""], ["Hockenmaier", "Julia", ""]]}, {"id": "2106.15846", "submitter": "Zhiyuan Wen", "authors": "Wen Zhiyuan, Cao Jiannong, Yang Ruosong, Liu Shuaiqi, Shen Jiaxing", "title": "Automatically Select Emotion for Response via Personality-affected\n  Emotion Transition", "comments": "Accepted by Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide consistent emotional interaction with users, dialog systems should\nbe capable to automatically select appropriate emotions for responses like\nhumans. However, most existing works focus on rendering specified emotions in\nresponses or empathetically respond to the emotion of users, yet the individual\ndifference in emotion expression is overlooked. This may lead to inconsistent\nemotional expressions and disinterest users. To tackle this issue, we propose\nto equip the dialog system with personality and enable it to automatically\nselect emotions in responses by simulating the emotion transition of humans in\nconversation. In detail, the emotion of the dialog system is transitioned from\nits preceding emotion in context. The transition is triggered by the preceding\ndialog context and affected by the specified personality trait. To achieve\nthis, we first model the emotion transition in the dialog system as the\nvariation between the preceding emotion and the response emotion in the\nValence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks\nto encode the preceding dialog context and the specified personality traits to\ncompose the variation. Finally, the emotion for response is selected from the\nsum of the preceding emotion and the variation. We construct a dialog dataset\nwith emotion and personality labels and conduct emotion prediction tasks for\nevaluation. Experimental results validate the effectiveness of the\npersonality-affected emotion transition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 07:00:42 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhiyuan", "Wen", ""], ["Jiannong", "Cao", ""], ["Ruosong", "Yang", ""], ["Shuaiqi", "Liu", ""], ["Jiaxing", "Shen", ""]]}, {"id": "2106.15872", "submitter": "Jakob Egetenmeyer", "authors": "Jakob Egetenmeyer", "title": "Genre determining prediction: Non-standard TAM marking in football\n  language", "comments": "23 pages, submitted to Frontiers in Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  German and French football language display tense-aspect-mood (TAM) forms\nwhich differ from the TAM use in other genres. In German football talk, the\npresent indicative may replace the pluperfect subjunctive. In French reports of\nfootball matches, the imperfective past may occur instead of a perfective past\ntense-aspect form. We argue that the two phenomena share a functional core and\nare licensed in the same way, which is a direct result of the genre they occur\nin. More precisely, football match reports adhere to a precise script and\nspecific events are temporally determined in terms of objective time. This\nallows speakers to exploit a secondary function of TAM forms, namely, they\nshift the temporal perspective. We argue that it is on the grounds of the genre\nthat comprehenders predict the deviating forms and are also able to decode\nthem. We present various corpus studies where we explore the functioning of\nthese phenomena in order to gain insights into their distribution,\ngrammaticalization and their functioning in discourse. Relevant factors are\nAktionsart properties, rhetorical relations and their interaction with other\nTAM forms. This allows us to discuss coping mechanisms on the part of the\ncomprehender. We broaden our understanding of the phenomena, which have only\nbeen partly covered for French and up to now seem to have been ignored in\nGerman.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:01:57 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Egetenmeyer", "Jakob", ""]]}, {"id": "2106.15876", "submitter": "Paheli Bhattacharya", "authors": "Paheli Bhattacharya and Soham Poddar and Koustav Rudra and Kripabandhu\n  Ghosh and Saptarshi Ghosh", "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal\n  Case Documents", "comments": "Accepted at the 18th International Conference on Artificial\n  Intelligence and Law (ICAIL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:06:15 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Poddar", "Soham", ""], ["Rudra", "Koustav", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2106.15880", "submitter": "Haoran Li", "authors": "Haoran Li, Wei Lu", "title": "Mixed Cross Entropy Loss for Neural Machine Translation", "comments": null, "journal-ref": "ICML2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural machine translation, cross entropy (CE) is the standard loss\nfunction in two training methods of auto-regressive models, i.e., teacher\nforcing and scheduled sampling. In this paper, we propose mixed cross entropy\nloss (mixed CE) as a substitute for CE in both training approaches. In teacher\nforcing, the model trained with CE regards the translation problem as a\none-to-one mapping process, while in mixed CE this process can be relaxed to\none-to-many. In scheduled sampling, we show that mixed CE has the potential to\nencourage the training and testing behaviours to be similar to each other, more\neffectively mitigating the exposure bias problem. We demonstrate the\nsuperiority of mixed CE over CE on several machine translation datasets, WMT'16\nRo-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled\nsampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE\nconsistently outperforms CE on a multi-reference set as well as a challenging\nparaphrased reference set. We also found the model trained with mixed CE is\nable to provide a better probability distribution defined over the translation\noutput space. Our code is available at https://github.com/haorannlp/mix.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:15:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Haoran", ""], ["Lu", "Wei", ""]]}, {"id": "2106.15896", "submitter": "Valerio Basile", "authors": "Sohail Akhtar, Valerio Basile, Viviana Patti", "title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of\n  Hate Speech Victims in Abusive Language Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social media platforms provide users the freedom of expression and a medium\nto exchange information and express diverse opinions. Unfortunately, this has\nalso resulted in the growth of abusive content with the purpose of\ndiscriminating people and targeting the most vulnerable communities such as\nimmigrants, LGBT, Muslims, Jews and women. Because abusive language is\nsubjective in nature, there might be highly polarizing topics or events\ninvolved in the annotation of abusive contents such as hate speech (HS).\nTherefore, we need novel approaches to model conflicting perspectives and\nopinions coming from people with different personal and demographic\nbackgrounds. In this paper, we present an in-depth study to model polarized\nopinions coming from different communities under the hypothesis that similar\ncharacteristics (ethnicity, social background, culture etc.) can influence the\nperspectives of annotators on a certain phenomenon. We believe that by relying\non this information, we can divide the annotators into groups sharing similar\nperspectives. We can create separate gold standards, one for each group, to\ntrain state-of-the-art deep learning models. We can employ an ensemble approach\nto combine the perspective-aware classifiers from different groups to an\ninclusive model. We also propose a novel resource, a multi-perspective English\nlanguage dataset annotated according to different sub-categories relevant for\ncharacterising online abuse: hate speech, aggressiveness, offensiveness and\nstereotype. By training state-of-the-art deep learning models on this novel\nresource, we show how our approach improves the prediction performance of a\nstate-of-the-art supervised classifier.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:35:49 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Akhtar", "Sohail", ""], ["Basile", "Valerio", ""], ["Patti", "Viviana", ""]]}, {"id": "2106.15903", "submitter": "Zhongkun Liu", "authors": "Zhongkun Liu, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Maarten de\n  Rijke, Ming Zhou", "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein\n  Distance", "comments": "13 pages, 4 figures, Published in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:44:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Liu", "Zhongkun", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["de Rijke", "Maarten", ""], ["Zhou", "Ming", ""]]}, {"id": "2106.15919", "submitter": "Anirudh Raju", "authors": "Anirudh Raju, Gautam Tiwari, Milind Rao, Pranav Dheram, Bryan\n  Anderson, Zhe Zhang, Bach Bui, Ariya Rastrow", "title": "End-to-End Spoken Language Understanding using RNN-Transducer ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end trained spoken language understanding (SLU) system\nthat extracts transcripts, intents and slots from an input speech utterance. It\nconsists of a streaming recurrent neural network transducer (RNNT) based\nautomatic speech recognition (ASR) model connected to a neural natural language\nunderstanding (NLU) model through a neural interface. This interface allows for\nend-to-end training using multi-task RNNT and NLU losses. Additionally, we\nintroduce semantic sequence loss training for the joint RNNT-NLU system that\nallows direct optimization of non-differentiable SLU metrics. This end-to-end\nSLU model paradigm can leverage state-of-the-art advancements and pretrained\nmodels in both ASR and NLU research communities, outperforming recently\nproposed direct speech-to-semantics models, and conventional pipelined ASR and\nNLU systems. We show that this method improves both ASR and NLU metrics on both\npublic SLU datasets and large proprietary datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:20:32 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 03:19:22 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Raju", "Anirudh", ""], ["Tiwari", "Gautam", ""], ["Rao", "Milind", ""], ["Dheram", "Pranav", ""], ["Anderson", "Bryan", ""], ["Zhang", "Zhe", ""], ["Bui", "Bach", ""], ["Rastrow", "Ariya", ""]]}, {"id": "2106.15971", "submitter": "Iman Bilal", "authors": "Iman Munire Bilal, Bo Wang, Maria Liakata, Rob Procter, Adam\n  Tsakalidis", "title": "Evaluation of Thematic Coherence in Microblogs", "comments": "ACL 2021 - Long Paper - Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting together microblogs representing opinions about the same topics\nwithin the same timeframe is useful to a number of different tasks and\npractitioners. A major question is how to evaluate the quality of such thematic\nclusters. Here we create a corpus of microblog clusters from three different\ndomains and time windows and define the task of evaluating thematic coherence.\nWe provide annotation guidelines and human annotations of thematic coherence by\njournalist experts. We subsequently investigate the efficacy of different\nautomated evaluation metrics for the task. We consider a range of metrics\nincluding surface level metrics, ones for topic model coherence and text\ngeneration metrics (TGMs). While surface level metrics perform well,\noutperforming topic coherence metrics, they are not as consistent as TGMs. TGMs\nare more reliable than all other metrics considered for capturing thematic\ncoherence in microblog clusters due to being less sensitive to the effect of\ntime windows.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 10:32:59 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bilal", "Iman Munire", ""], ["Wang", "Bo", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""], ["Tsakalidis", "Adam", ""]]}, {"id": "2106.15986", "submitter": "Matej Ul\\v{c}ar", "authors": "Matej Ul\\v{c}ar and Marko Robnik-\\v{S}ikonja", "title": "Cross-lingual alignments of ELMo contextual embeddings", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for less-resourced\nlanguages. Cross-lingual embeddings map word embeddings from a less-resourced\nlanguage to a resource-rich language so that a prediction model trained on data\nfrom the resource-rich language can also be used in the less-resourced\nlanguage. To produce cross-lingual mappings of recent contextual embeddings,\nanchor points between the embedding spaces have to be words in the same\ncontext. We address this issue with a novel method for creating cross-lingual\ncontextual alignment datasets. Based on that, we propose several cross-lingual\nmapping methods for ELMo embeddings. The proposed linear mapping methods use\nexisting Vecmap and MUSE alignments on contextual ELMo embeddings. Novel\nnonlinear ELMoGAN mapping methods are based on GANs and do not assume\nisomorphic embedding spaces. We evaluate the proposed mapping methods on nine\nlanguages, using four downstream tasks: named entity recognition (NER),\ndependency parsing (DP), terminology alignment, and sentiment analysis. The\nELMoGAN methods perform very well on the NER and terminology alignment tasks,\nwith a lower cross-lingual loss for NER compared to the direct training on some\nlanguages. In DP and sentiment analysis, linear contextual alignment variants\nare more successful.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:26:43 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 15:49:25 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ul\u010dar", "Matej", ""], ["Robnik-\u0160ikonja", "Marko", ""]]}, {"id": "2106.16013", "submitter": "Razieh Baradaran", "authors": "Razieh Baradaran and Hossein Amirkhani", "title": "Zero-Shot Estimation of Base Models' Weights in Ensemble of Machine\n  Reading Comprehension Systems for Robust Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main challenges of the machine reading comprehension (MRC) models\nis their fragile out-of-domain generalization, which makes these models not\nproperly applicable to real-world general-purpose question answering problems.\nIn this paper, we leverage a zero-shot weighted ensemble method for improving\nthe robustness of out-of-domain generalization in MRC models. In the proposed\nmethod, a weight estimation module is used to estimate out-of-domain weights,\nand an ensemble module aggregate several base models' predictions based on\ntheir weights. The experiments indicate that the proposed method not only\nimproves the final accuracy, but also is robust against domain changes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:22:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Baradaran", "Razieh", ""], ["Amirkhani", "Hossein", ""]]}, {"id": "2106.16034", "submitter": "Robert Mahari", "authors": "Robert Zev Mahari", "title": "AutoLAW: Augmented Legal Reasoning through Legal Precedent Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrate how NLP can be used to address an unmet need of the\nlegal community and increase access to justice. The paper introduces Legal\nPrecedent Prediction (LPP), the task of predicting relevant passages from\nprecedential court decisions given the context of a legal argument. To this\nend, the paper showcases a BERT model, trained on 530,000 examples of legal\narguments made by U.S. federal judges, to predict relevant passages from\nprecedential court decisions given the context of a legal argument. In 96% of\nunseen test examples the correct target passage is among the top-10 predicted\npassages. The same model is able to predict relevant precedent given a short\nsummary of a complex and unseen legal brief, predicting the precedent that was\nactually cited by the brief's co-author, former U.S. Solicitor General and\ncurrent U.S. Supreme Court Justice Elena Kagan.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:01:33 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Mahari", "Robert Zev", ""]]}, {"id": "2106.16038", "submitter": "Jiwei Li", "authors": "Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei\n  Wu, Jiwei Li", "title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin\n  Information", "comments": "To appear at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:06:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Sun", "Zijun", ""], ["Li", "Xiaoya", ""], ["Sun", "Xiaofei", ""], ["Meng", "Yuxian", ""], ["Ao", "Xiang", ""], ["He", "Qing", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""]]}, {"id": "2106.16053", "submitter": "Nikos Voskarides", "authors": "Nikos Voskarides, Edgar Meij, Sabrina Sauer, Maarten de Rijke", "title": "News Article Retrieval in Context for Event-centric Narrative Creation", "comments": "ICTIR 2021", "journal-ref": null, "doi": "10.1145/3471158.3472247", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:27:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Voskarides", "Nikos", ""], ["Meij", "Edgar", ""], ["Sauer", "Sabrina", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2106.16055", "submitter": "Pavel Denisov", "authors": "Pavel Denisov, Manuel Mager, Ngoc Thang Vu", "title": "IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task", "comments": "IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper describes the submission to the IWSLT 2021 Low-Resource Speech\nTranslation Shared Task by IMS team. We utilize state-of-the-art models\ncombined with several data augmentation, multi-task and transfer learning\napproaches for the automatic speech recognition (ASR) and machine translation\n(MT) steps of our cascaded system. Moreover, we also explore the feasibility of\na full end-to-end speech translation (ST) model in the case of very constrained\namount of ground truth labeled data. Our best system achieves the best\nperformance among all submitted systems for Congolese Swahili to English and\nFrench with BLEU scores 7.7 and 13.7 respectively, and the second best result\nfor Coastal Swahili to English with BLEU score 14.9.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:29:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Denisov", "Pavel", ""], ["Mager", "Manuel", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2106.16138", "submitter": "Li Dong", "authors": "Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Saksham Singhal, Payal\n  Bajaj, Xia Song, Furu Wei", "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:45:07 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chi", "Zewen", ""], ["Huang", "Shaohan", ""], ["Dong", "Li", ""], ["Ma", "Shuming", ""], ["Singhal", "Saksham", ""], ["Bajaj", "Payal", ""], ["Song", "Xia", ""], ["Wei", "Furu", ""]]}, {"id": "2106.16163", "submitter": "Thibault Sellam", "authors": "Thibault Sellam, Steve Yadlowsky, Jason Wei, Naomi Saphra, Alexander\n  D'Amour, Tal Linzen, Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipanjan\n  Das, Ian Tenney, Ellie Pavlick", "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis", "comments": "Checkpoints and example analyses: http://goo.gle/multiberts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments with pretrained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact (i.e., the\nparticular instance of the model), it is not always clear whether they hold for\nthe more general procedure (which includes the model architecture, training\ndata, initialization scheme, and loss function). Recent work has shown that\nre-running pretraining can lead to substantially different conclusions about\nperformance, suggesting that alternative evaluations are needed to make\nprincipled statements about procedures. To address this question, we introduce\nMultiBERTs: a set of 25 BERT-base checkpoints, trained with similar\nhyper-parameters as the original BERT model but differing in random\ninitialization and data shuffling. The aim is to enable researchers to draw\nrobust and statistically justified conclusions about pretraining procedures.\nThe full release includes 25 fully trained checkpoints, as well as statistical\nguidelines and a code library implementing our recommended hypothesis testing\nmethods. Finally, for five of these models we release a set of 28 intermediate\ncheckpoints in order to support research on learning dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:56:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Sellam", "Thibault", ""], ["Yadlowsky", "Steve", ""], ["Wei", "Jason", ""], ["Saphra", "Naomi", ""], ["D'Amour", "Alexander", ""], ["Linzen", "Tal", ""], ["Bastings", "Jasmijn", ""], ["Turc", "Iulia", ""], ["Eisenstein", "Jacob", ""], ["Das", "Dipanjan", ""], ["Tenney", "Ian", ""], ["Pavlick", "Ellie", ""]]}, {"id": "2106.16171", "submitter": "Iulia Turc", "authors": "Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, Kristina\n  Toutanova", "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their success, large pre-trained multilingual models have not\ncompletely alleviated the need for labeled data, which is cumbersome to collect\nfor all target languages. Zero-shot cross-lingual transfer is emerging as a\npractical solution: pre-trained models later fine-tuned on one transfer\nlanguage exhibit surprising performance when tested on many target languages.\nEnglish is the dominant source language for transfer, as reinforced by popular\nzero-shot benchmarks. However, this default choice has not been systematically\nvetted. In our study, we compare English against other transfer languages for\nfine-tuning, on two pre-trained multilingual models (mBERT and mT5) and\nmultiple classification and question answering tasks. We find that other\nhigh-resource languages such as German and Russian often transfer more\neffectively, especially when the set of target languages is diverse or unknown\na priori. Unexpectedly, this can be true even when the training sets were\nautomatically translated from English. This finding can have immediate impact\non multilingual zero-shot systems, and should inform future benchmark designs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:05:57 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Turc", "Iulia", ""], ["Lee", "Kenton", ""], ["Eisenstein", "Jacob", ""], ["Chang", "Ming-Wei", ""], ["Toutanova", "Kristina", ""]]}, {"id": "2106.16175", "submitter": "Ana-Maria Bucur", "authors": "Ana-Maria Bucur, Adrian Cosma and Liviu P. Dinu", "title": "Early Risk Detection of Pathological Gambling, Self-Harm and Depression\n  Using BERT", "comments": "Accepted to Early Risk Prediction on the Internet Workshop,\n  Conference and Labs of the Evaluation Forum (CLEF 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early risk detection of mental illnesses has a massive positive impact upon\nthe well-being of people. The eRisk workshop has been at the forefront of\nenabling interdisciplinary research in developing computational methods to\nautomatically estimate early risk factors for mental issues such as depression,\nself-harm, anorexia and pathological gambling. In this paper, we present the\ncontributions of the BLUE team in the 2021 edition of the workshop, in which we\ntackle the problems of early detection of gambling addiction, self-harm and\nestimating depression severity from social media posts. We employ pre-trained\nBERT transformers and data crawled automatically from mental health subreddits\nand obtain reasonable results on all three tasks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:12:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bucur", "Ana-Maria", ""], ["Cosma", "Adrian", ""], ["Dinu", "Liviu P.", ""]]}, {"id": "2106.16188", "submitter": "Yang Liu", "authors": "Yang Liu, Yifei Sun, Vincent Gao", "title": "Improving Factual Consistency of Abstractive Summarization on Customer\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:34:36 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Liu", "Yang", ""], ["Sun", "Yifei", ""], ["Gao", "Vincent", ""]]}, {"id": "2106.16196", "submitter": "Casey Kennington", "authors": "Casey Kennington and McKenzie Steenson", "title": "An Analysis of the Recent Visibility of the SigDial Conference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated speech and text interfaces are continuing to improve, resulting in\nincreased research in the area of dialogue systems. Moreover, conferences and\nworkshops from various fields are focusing more on language through speech and\ntext mediums as candidates for interaction with applications such as search\ninterfaces and robots. In this paper, we explore how visible the SigDial\nconference is to outside conferences by analysing papers from top Natural\nLangauge Processing conferences since 2015 to determine the popularity of\ncertain SigDial-related topics, as well as analysing what SigDial papers are\nbeing cited by others outside of SigDial. We find that despite a dramatic\nincrease in dialogue-related research, SigDial visibility has not increased. We\nconclude by offering some suggestions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:47:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kennington", "Casey", ""], ["Steenson", "McKenzie", ""]]}, {"id": "2106.16213", "submitter": "William Merrill", "authors": "William Merrill and Yoav Goldberg and Roy Schwartz and Noah A. Smith", "title": "On the Power of Saturated Transformers: A View from Circuit Complexity", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 17:09:47 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Merrill", "William", ""], ["Goldberg", "Yoav", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah A.", ""]]}]