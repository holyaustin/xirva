[{"id": "1104.2034", "submitter": "Yavor Parvanov", "authors": "Yavor Angelov Parvanov", "title": "Materials to the Russian-Bulgarian Comparative Dictionary \"EAD\"", "comments": "Bulgarian Rusistics; Vol. 1 (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This article presents a fragment of a new comparative dictionary \"A\ncomparative dictionary of names of expansive action in Russian and Bulgarian\nlanguages\". Main features of the new web-based comparative dictionary are\nplaced, the principles of its formation are shown, primary links between the\nword-matches are classified. The principal difference between translation\ndictionaries and the model of double comparison is also shown. The\nclassification scheme of the pages is proposed. New concepts and keywords have\nbeen introduced. The real prototype of the dictionary with a few key pages is\npublished. The broad debate about the possibility of this prototype to become a\nversion of Russian-Bulgarian comparative dictionary of a new generation is\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 19:50:50 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Parvanov", "Yavor Angelov", ""]]}, {"id": "1104.2086", "submitter": "Slav Petrov", "authors": "Slav Petrov, Dipanjan Das and Ryan McDonald", "title": "A Universal Part-of-Speech Tagset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate future research in unsupervised induction of syntactic\nstructure and to standardize best-practices, we propose a tagset that consists\nof twelve universal part-of-speech categories. In addition to the tagset, we\ndevelop a mapping from 25 different treebank tagsets to this universal set. As\na result, when combined with the original treebank data, this universal tagset\nand mapping produce a dataset consisting of common parts-of-speech for 22\ndifferent languages. We highlight the use of this resource via two experiments,\nincluding one that reports competitive accuracies for unsupervised grammar\ninduction without gold standard part-of-speech tags.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 23:06:54 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Petrov", "Slav", ""], ["Das", "Dipanjan", ""], ["McDonald", "Ryan", ""]]}, {"id": "1104.4321", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous", "title": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and\n  Compounds", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese characters can be compared to a molecular structure: a character is\nanalogous to a molecule, radicals are like atoms, calligraphic strokes\ncorrespond to elementary particles, and when characters form compounds, they\nare like molecular structures. In chemistry the conjunction of all of these\nstructural levels produces what we perceive as matter. In language, the\nconjunction of strokes, radicals, characters, and compounds produces meaning.\nBut when does meaning arise? We all know that radicals are, in some sense, the\nbasic semantic components of Chinese script, but what about strokes?\nConsidering the fact that many characters are made by adding individual strokes\nto (combinations of) radicals, we can legitimately ask the question whether\nstrokes carry meaning, or not. In this talk I will present my project of\nextending traditional NLP techniques to radicals and strokes, aiming to obtain\na deeper understanding of the way ideographic languages model the world.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 17:56:50 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Haralambous", "Yannis", ""]]}, {"id": "1104.4426", "submitter": "Maurizio Serva", "authors": "Maurizio Serva", "title": "Phylogeny and geometry of languages from normalized Levenshtein distance", "comments": "Review paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea that the distance among pairs of languages can be evaluated from\nlexical differences seems to have its roots in the work of the French explorer\nDumont D'Urville. He collected comparative words lists of various languages\nduring his voyages aboard the Astrolabe from 1826 to 1829 and, in his work\nabout the geographical division of the Pacific, he proposed a method to measure\nthe degree of relation between languages.\n  The method used by the modern lexicostatistics, developed by Morris Swadesh\nin the 1950s, measures distances from the percentage of shared cognates, which\nare words with a common historical origin. The weak point of this method is\nthat subjective judgment plays a relevant role.\n  Recently, we have proposed a new automated method which is motivated by the\nanalogy with genetics. The new approach avoids any subjectivity and results can\nbe easily replicated by other scholars. The distance between two languages is\ndefined by considering a renormalized Levenshtein distance between pair of\nwords with the same meaning and averaging on the words contained in a list. The\nrenormalization, which takes into account the length of the words, plays a\ncrucial role, and no sensible results can be found without it.\n  In this paper we give a short review of our automated method and we\nillustrate it by considering the cluster of Malagasy dialects. We show that it\nsheds new light on their kinship relation and also that it furnishes a lot of\nnew information concerning the modalities of the settlement of Madagascar.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 12:20:56 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2011 18:07:21 GMT"}, {"version": "v3", "created": "Fri, 29 Apr 2011 06:26:36 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Serva", "Maurizio", ""]]}, {"id": "1104.4681", "submitter": "Gopalakrishnan Tr Nair", "authors": "R. Rajeswara Rao, V. Kamakshi Prasad, A. Nagesh", "title": "Performance Evaluation of Statistical Approaches for Text Independent\n  Speaker Recognition Using Source Feature", "comments": "8 pages, 7 figures, International Journal", "journal-ref": "InterJRI Computer Science and Networking, Volume 2, Issue 1 August\n  2010", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the performance evaluation of statistical approaches\nfor TextIndependent speaker recognition system using source feature. Linear\nprediction LP residual is used as a representation of excitation information in\nspeech. The speaker-specific information in the excitation of voiced speech is\ncaptured using statistical approaches such as Gaussian Mixture Models GMMs and\nHidden Markov Models HMMs. The decrease in the error during training and\nrecognizing speakers during testing phase close to 100 percent accuracy\ndemonstrates that the excitation component of speech contains speaker-specific\ninformation and is indeed being effectively captured by continuous Ergodic HMM\nthan GMM. The performance of the speaker recognition system is evaluated on GMM\nand 2 state ergodic HMM with different mixture components and test speech\nduration. We demonstrate the speaker recognition studies on TIMIT database for\nboth GMM and Ergodic HMM.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 05:00:25 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Rao", "R. Rajeswara", ""], ["Prasad", "V. Kamakshi", ""], ["Nagesh", "A.", ""]]}, {"id": "1104.4950", "submitter": "Hamed Hassanzadeh", "authors": "Hamed Hassanzadeh and MohammadReza Keyvanpour", "title": "A Machine Learning Based Analytical Framework for Semantic Annotation\n  Requirements", "comments": null, "journal-ref": "International Journal of Web & Semantic Technology (IJWesT), Vol.\n  2, No. 2, pp. 27-38, Aprill 2011", "doi": "10.5121/ijwest.2011.2203", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Semantic Web is an extension of the current web in which information is\ngiven well-defined meaning. The perspective of Semantic Web is to promote the\nquality and intelligence of the current web by changing its contents into\nmachine understandable form. Therefore, semantic level information is one of\nthe cornerstones of the Semantic Web. The process of adding semantic metadata\nto web resources is called Semantic Annotation. There are many obstacles\nagainst the Semantic Annotation, such as multilinguality, scalability, and\nissues which are related to diversity and inconsistency in content of different\nweb pages. Due to the wide range of domains and the dynamic environments that\nthe Semantic Annotation systems must be performed on, the problem of automating\nannotation process is one of the significant challenges in this domain. To\novercome this problem, different machine learning approaches such as supervised\nlearning, unsupervised learning and more recent ones like, semi-supervised\nlearning and active learning have been utilized. In this paper we present an\ninclusive layered classification of Semantic Annotation challenges and discuss\nthe most important issues in this field. Also, we review and analyze machine\nlearning applications for solving semantic annotation problems. For this goal,\nthe article tries to closely study and categorize related researches for better\nunderstanding and to reach a framework that can map machine learning techniques\ninto the Semantic Annotation challenges and requirements.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 15:36:47 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Hassanzadeh", "Hamed", ""], ["Keyvanpour", "MohammadReza", ""]]}, {"id": "1104.5362", "submitter": "Andr\\'e Kempe", "authors": "Andr\\'e Kempe", "title": "Selected Operations, Algorithms, and Applications of n-Tape Weighted\n  Finite-State Machines", "comments": "15 pages, 7 figures, LaTeX (+ .eps)", "journal-ref": "Proc. FSMNLP 2009, Pretoria, South Africa. July 21-24. (invited\n  talk)", "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted finite-state machine with n tapes (n-WFSM) defines a rational\nrelation on n strings. It is a generalization of weighted acceptors (one tape)\nand transducers (two tapes).\n  After recalling some basic definitions about n-ary weighted rational\nrelations and n-WFSMs, we summarize some central operations on these relations\nand machines, such as join and auto-intersection. Unfortunately, due to Post's\nCorrespondence Problem, a fully general join or auto-intersection algorithm\ncannot exist. We recall a restricted algorithm for a class of n-WFSMs.\n  Through a series of practical applications, we finally investigate the\naugmented descriptive power of n-WFSMs and their join, compared to classical\ntransducers and their composition. Some applications are not feasible with the\nlatter. The series includes: the morphological analysis of Semitic languages,\nthe preservation of intermediate results in transducer cascades, the induction\nof morphological rules from corpora, the alignment of lexicon entries, the\nautomatic extraction of acronyms and their meaning from corpora, and the search\nfor cognates in a bilingual lexicon.\n  All described operations and applications have been implemented with Xerox's\nWFSC tool.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 11:44:33 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2011 09:38:16 GMT"}], "update_date": "2011-05-02", "authors_parsed": [["Kempe", "Andr\u00e9", ""]]}]