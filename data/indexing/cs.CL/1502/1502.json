[{"id": "1502.00512", "submitter": "Will Williams", "authors": "Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson", "title": "Scaling Recurrent Neural Network Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the scaling properties of Recurrent Neural Network\nLanguage Models (RNNLMs). We discuss how to train very large RNNs on GPUs and\naddress the questions of how RNNLMs scale with respect to model size,\ntraining-set size, computational costs and memory. Our analysis shows that\ndespite being more costly to train, RNNLMs obtain much lower perplexities on\nstandard benchmarks than n-gram models. We train the largest known RNNs and\npresent relative word error rates gains of 18% on an ASR task. We also present\nthe new lowest perplexities on the recently released billion word language\nmodelling benchmark, 1 BLEU point gain on machine translation and a 17%\nrelative hit rate gain in word prediction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:27:37 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Williams", "Will", ""], ["Prasad", "Niranjani", ""], ["Mrva", "David", ""], ["Ash", "Tom", ""], ["Robinson", "Tony", ""]]}, {"id": "1502.00731", "submitter": "Jaeho Shin", "authors": "Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang,\n  Christopher R\\'e", "title": "Incremental Knowledge Base Construction Using DeepDive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Populating a database with unstructured information is a long-standing\nproblem in industry and research that encompasses problems of extraction,\ncleaning, and integration. Recent names used for this problem include dealing\nwith dark data and knowledge base construction (KBC). In this work, we describe\nDeepDive, a system that combines database and machine learning ideas to help\ndevelop KBC systems, and we present techniques to make the KBC process more\nefficient. We observe that the KBC process is iterative, and we develop\ntechniques to incrementally produce inference results for KBC systems. We\npropose two methods for incremental inference, based respectively on sampling\nand variational techniques. We also study the tradeoff space of these methods\nand develop a simple rule-based optimizer. DeepDive includes all of these\ncontributions, and we evaluate DeepDive on five KBC systems, showing that it\ncan speed up KBC inference tasks by up to two orders of magnitude with\nnegligible impact on quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 04:16:24 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 21:59:15 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2015 06:13:32 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2015 22:24:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Shin", "Jaeho", ""], ["Wu", "Sen", ""], ["Wang", "Feiran", ""], ["De Sa", "Christopher", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1502.00831", "submitter": "Robin Piedeleu", "authors": "Robin Piedeleu, Dimitri Kartsaklis, Bob Coecke and Mehrnoosh Sadrzadeh", "title": "Open System Categorical Quantum Semantics in Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO math.CT math.QA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originally inspired by categorical quantum mechanics (Abramsky and Coecke,\nLiCS'04), the categorical compositional distributional model of natural\nlanguage meaning of Coecke, Sadrzadeh and Clark provides a conceptually\nmotivated procedure to compute the meaning of a sentence, given its grammatical\nstructure within a Lambek pregroup and a vectorial representation of the\nmeaning of its parts. The predictions of this first model have outperformed\nthat of other models in mainstream empirical language processing tasks on large\nscale data. Moreover, just like CQM allows for varying the model in which we\ninterpret quantum axioms, one can also vary the model in which we interpret\nword meaning.\n  In this paper we show that further developments in categorical quantum\nmechanics are relevant to natural language processing too. Firstly, Selinger's\nCPM-construction allows for explicitly taking into account lexical ambiguity\nand distinguishing between the two inherently different notions of homonymy and\npolysemy. In terms of the model in which we interpret word meaning, this means\na passage from the vector space model to density matrices. Despite this change\nof model, standard empirical methods for comparing meanings can be easily\nadopted, which we demonstrate by a small-scale experiment on real-world data.\nThis experiment moreover provides preliminary evidence of the validity of our\nproposed new model for word meaning.\n  Secondly, commutative classical structures as well as their non-commutative\ncounterparts that arise in the image of the CPM-construction allow for encoding\nrelative pronouns, verbs and adjectives, and finally, iteration of the\nCPM-construction, something that has no counterpart in the quantum realm,\nenables one to accommodate both entailment and ambiguity.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 12:16:19 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 19:06:34 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Piedeleu", "Robin", ""], ["Kartsaklis", "Dimitri", ""], ["Coecke", "Bob", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1502.01245", "submitter": "Diego Amancio", "authors": "Diego R. Amancio", "title": "Authorship recognition via fluctuation analysis of network topology and\n  word intermittency", "comments": null, "journal-ref": "J. Stat. Mech. (2015) P03005", "doi": "10.1088/1742-5468/2015/03/P03005", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods have been widely employed in many practical natural\nlanguage processing applications. More specifically, complex networks concepts\nand methods from dynamical systems theory have been successfully applied to\nrecognize stylistic patterns in written texts. Despite the large amount of\nstudies devoted to represent texts with physical models, only a few studies\nhave assessed the relevance of attributes derived from the analysis of\nstylistic fluctuations. Because fluctuations represent a pivotal factor for\ncharacterizing a myriad of real systems, this study focused on the analysis of\nthe properties of stylistic fluctuations in texts via topological analysis of\ncomplex networks and intermittency measurements. The results showed that\ndifferent authors display distinct fluctuation patterns. In particular, it was\nfound that it is possible to identify the authorship of books using the\nintermittency of specific words. Taken together, the results described here\nsuggest that the patterns found in stylistic fluctuations could be used to\nanalyze other related complex systems. Furthermore, the discovery of novel\npatterns related to textual stylistic fluctuations indicates that these\npatterns could be useful to improve the state of the art of many\nstylistic-based natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 16:12:45 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Amancio", "Diego R.", ""]]}, {"id": "1502.01271", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO)", "title": "INRIASAC: Simple Hypernym Extraction Methods", "comments": "SemEval 2015, Jun 2015, Denver, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of terms from a given domain, how can we structure them into a\ntaxonomy without manual intervention? This is the task 17 of SemEval 2015. Here\nwe present our simple taxonomy structuring techniques which, despite their\nsimplicity, ranked first in this 2015 benchmark. We use large quantities of\ntext (English Wikipedia) and simple heuristics such as term overlap and\ndocument and sentence co-occurrence to produce hypernym lists. We describe\nthese techniques and pre-sent an initial evaluation of results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 17:53:01 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 09:05:18 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"]]}, {"id": "1502.01446", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, Chengqing Zong", "title": "Beyond Word-based Language Model in Statistical Machine Translation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language model is one of the most important modules in statistical machine\ntranslation and currently the word-based language model dominants this\ncommunity. However, many translation models (e.g. phrase-based models) generate\nthe target language sentences by rendering and compositing the phrases rather\nthan the words. Thus, it is much more reasonable to model dependency between\nphrases, but few research work succeed in solving this problem. In this paper,\nwe tackle this problem by designing a novel phrase-based language model which\nattempts to solve three key sub-problems: 1, how to define a phrase in language\nmodel; 2, how to determine the phrase boundary in the large-scale monolingual\ndata in order to enlarge the training set; 3, how to alleviate the data\nsparsity problem due to the huge vocabulary size of phrases. By carefully\nhandling these issues, the extensive experiments on Chinese-to-English\ntranslation show that our phrase-based language model can significantly improve\nthe translation quality by up to +1.47 absolute BLEU score.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 07:42:18 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Zhang", "Jiajun", ""], ["Liu", "Shujie", ""], ["Li", "Mu", ""], ["Zhou", "Ming", ""], ["Zong", "Chengqing", ""]]}, {"id": "1502.01682", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris\n  Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin and Scott\n  Miller", "title": "Use of Modality and Negation in Semantically-Informed Syntactic MT", "comments": "28 pages, 13 figures, 2 tables; appeared in Computational\n  Linguistics, 38(2):411-438, 2012", "journal-ref": "Computational Linguistics, 38(2):411-438, 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the resource- and system-building efforts of an\neight-week Johns Hopkins University Human Language Technology Center of\nExcellence Summer Camp for Applied Language Exploration (SCALE-2009) on\nSemantically-Informed Machine Translation (SIMT). We describe a new\nmodality/negation (MN) annotation scheme, the creation of a (publicly\navailable) MN lexicon, and two automated MN taggers that we built using the\nannotation scheme and lexicon. Our annotation scheme isolates three components\nof modality and negation: a trigger (a word that conveys modality or negation),\na target (an action associated with modality or negation) and a holder (an\nexperiencer of modality). We describe how our MN lexicon was semi-automatically\nproduced and we demonstrate that a structure-based MN tagger results in\nprecision around 86% (depending on genre) for tagging of a standard LDC data\nset.\n  We apply our MN annotation scheme to statistical machine translation using a\nsyntactic framework that supports the inclusion of semantic annotations.\nSyntactic tags enriched with semantic annotations are assigned to parse trees\nin the target-language training texts through a process of tree grafting. While\nthe focus of our work is modality and negation, the tree grafting procedure is\ngeneral and supports other types of semantic information. We exploit this\ncapability by including named entities, produced by a pre-existing tagger, in\naddition to the MN elements produced by the taggers described in this paper.\nThe resulting system significantly outperformed a linguistically naive baseline\nmodel (Hiero), and reached the highest scores yet reported on the NIST 2009\nUrdu-English test set. This finding supports the hypothesis that both syntactic\nand semantic information can improve translation quality.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 19:10:26 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Dorr", "Bonnie J.", ""], ["Callison-Burch", "Chris", ""], ["Filardo", "Nathaniel W.", ""], ["Piatko", "Christine", ""], ["Levin", "Lori", ""], ["Miller", "Scott", ""]]}, {"id": "1502.01710", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Yann LeCun", "title": "Text Understanding from Scratch", "comments": "This technical report is superseded by a paper entitled\n  \"Character-level Convolutional Networks for Text Classification\",\n  arXiv:1509.01626. It has considerably more experimental results and a\n  rewritten introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article demontrates that we can apply deep learning to text\nunderstanding from character-level inputs all the way up to abstract text\nconcepts, using temporal convolutional networks (ConvNets). We apply ConvNets\nto various large-scale datasets, including ontology classification, sentiment\nanalysis, and text categorization. We show that temporal ConvNets can achieve\nastonishing performance without the knowledge of words, phrases, sentences and\nany other syntactic or semantic structures with regards to a human language.\nEvidence shows that our models can work for both English and Chinese.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 20:45:19 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 21:32:01 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2015 03:45:02 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 04:42:29 GMT"}, {"version": "v5", "created": "Mon, 4 Apr 2016 02:40:48 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Zhang", "Xiang", ""], ["LeCun", "Yann", ""]]}, {"id": "1502.01753", "submitter": "Peter Wittek", "authors": "Peter Wittek, S\\'andor Dar\\'anyi, Efstratios Kontopoulos, Theodoros\n  Moysiadis, Ioannis Kompatsiaris", "title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving\n  Vector Field", "comments": "8 pages, 1 figure. Code used to conduct the experiments is available\n  at https://github.com/peterwittek/concept_drifts", "journal-ref": "Proceedings of IJCNN-15, International Joint Conference on Neural\n  Networks, pages 1--8, 2015", "doi": "10.1109/IJCNN.2015.7280766", "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the Aristotelian concept of potentiality vs. actuality allowing for\nthe study of energy and dynamics in language, we propose a field approach to\nlexical analysis. Falling back on the distributional hypothesis to\nstatistically model word meaning, we used evolving fields as a metaphor to\nexpress time-dependent changes in a vector space model by a combination of\nrandom indexing and evolving self-organizing maps (ESOM). To monitor semantic\ndrifts within the observation period, an experiment was carried out on the term\nspace of a collection of 12.8 million Amazon book reviews. For evaluation, the\nsemantic consistency of ESOM term clusters was compared with their respective\nneighbourhoods in WordNet, and contrasted with distances among term vectors by\nrandom indexing. We found that at 0.05 level of significance, the terms in the\nclusters showed a high level of semantic consistency. Tracking the drift of\ndistributional patterns in the term space across time periods, we found that\nconsistency decreased, but not at a statistically significant level. Our method\nis highly scalable, with interpretations in philosophy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 22:51:45 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wittek", "Peter", ""], ["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Kontopoulos", "Efstratios", ""], ["Moysiadis", "Theodoros", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1502.02233", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh", "title": "Hierarchical Dirichlet process for tracking complex topical structure\n  evolution and its application to autism research literature", "comments": "In Proc. Pacific-Asia Conference on Knowledge Discovery and Data\n  Mining (PAKDD), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel framework for the discovery of the topical\ncontent of a data corpus, and the tracking of its complex structural changes\nacross the temporal dimension. In contrast to previous work our model does not\nimpose a prior on the rate at which documents are added to the corpus nor does\nit adopt the Markovian assumption which overly restricts the type of changes\nthat the model can capture. Our key technical contribution is a framework based\non (i) discretization of time into epochs, (ii) epoch-wise topic discovery\nusing a hierarchical Dirichlet process-based model, and (iii) a temporal\nsimilarity graph which allows for the modelling of complex topic changes:\nemergence and disappearance, evolution, and splitting and merging. The power of\nthe proposed framework is demonstrated on the medical literature corpus\nconcerned with the autism spectrum disorder (ASD) - an increasingly important\nresearch subject of significant social and healthcare importance. In addition\nto the collected ASD literature corpus which we will make freely available, our\ncontributions also include two free online tools we built as aids to ASD\nresearchers. These can be used for semantically meaningful navigation and\nsearching, as well as knowledge discovery from this large and rapidly growing\ncorpus of literature.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 10:25:20 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Beykikhoshk", "Adham", ""], ["Arandjelovic", "Ognjen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1502.02277", "submitter": "Seung-Hoon Na", "authors": "Seung-Hoon Na and In-Su Kang and Jong-Hyeok Lee", "title": "Improving Term Frequency Normalization for Multi-topical Documents, and\n  Application to Language Modeling Approaches", "comments": "8 pages, conference paper, published in ECIR '08", "journal-ref": "Advances in Information Retrieval Lecture Notes in Computer\n  Science Volume 4956, 2008, pp 382-393", "doi": "10.1007/978-3-540-78646-7_35", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term frequency normalization is a serious issue since lengths of documents\nare various. Generally, documents become long due to two different reasons -\nverbosity and multi-topicality. First, verbosity means that the same topic is\nrepeatedly mentioned by terms related to the topic, so that term frequency is\nmore increased than the well-summarized one. Second, multi-topicality indicates\nthat a document has a broad discussion of multi-topics, rather than single\ntopic. Although these document characteristics should be differently handled,\nall previous methods of term frequency normalization have ignored these\ndifferences and have used a simplified length-driven approach which decreases\nthe term frequency by only the length of a document, causing an unreasonable\npenalization. To attack this problem, we propose a novel TF normalization\nmethod which is a type of partially-axiomatic approach. We first formulate two\nformal constraints that the retrieval model should satisfy for documents having\nverbose and multi-topicality characteristic, respectively. Then, we modify\nlanguage modeling approaches to better satisfy these two constraints, and\nderive novel smoothing methods. Experimental results show that the proposed\nmethod increases significantly the precision for keyword queries, and\nsubstantially improves MAP (Mean Average Precision) for verbose queries.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 17:32:44 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Na", "Seung-Hoon", ""], ["Kang", "In-Su", ""], ["Lee", "Jong-Hyeok", ""]]}, {"id": "1502.02655", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster", "title": "An investigation into language complexity of World-of-Warcraft\n  game-external texts", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a language complexity analysis of World of Warcraft (WoW)\ncommunity texts, which we compare to texts from a general corpus of web\nEnglish. Results from several complexity types are presented, including lexical\ndiversity, density, readability and syntactic complexity. The language of WoW\ntexts is found to be comparable to the general corpus on some complexity\nmeasures, yet more specialized on other measures. Our findings can be used by\neducators willing to include game-related activities into school curricula.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 21:59:21 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["\u0160uster", "Simon", ""]]}, {"id": "1502.03322", "submitter": "Yongfeng  Zhang", "authors": "Yongfeng Zhang, Min Zhang, Yiqun Liu, and Shaoping Ma", "title": "Boost Phrase-level Polarity Labelling with Review-level Sentiment\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis on user reviews helps to keep track of user reactions\ntowards products, and make advices to users about what to buy. State-of-the-art\nreview-level sentiment classification techniques could give pretty good\nprecisions of above 90%. However, current phrase-level sentiment analysis\napproaches might only give sentiment polarity labelling precisions of around\n70%~80%, which is far from satisfaction and restricts its application in many\npractical tasks. In this paper, we focus on the problem of phrase-level\nsentiment polarity labelling and attempt to bridge the gap between phrase-level\nand review-level sentiment analysis. We investigate the inconsistency between\nthe numerical star ratings and the sentiment orientation of textual user\nreviews. Although they have long been treated as identical, which serves as a\nbasic assumption in previous work, we find that this assumption is not\nnecessarily true. We further propose to leverage the results of review-level\nsentiment classification to boost the performance of phrase-level polarity\nlabelling using a novel constrained convex optimization framework. Besides, the\nframework is capable of integrating various kinds of information sources and\nheuristics, while giving the global optimal solution due to its convexity.\nExperimental results on both English and Chinese reviews show that our\nframework achieves high labelling precisions of up to 89%, which is a\nsignificant improvement from current approaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 14:45:41 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Zhang", "Yongfeng", ""], ["Zhang", "Min", ""], ["Liu", "Yiqun", ""], ["Ma", "Shaoping", ""]]}, {"id": "1502.03520", "submitter": "Yingyu Liang", "authors": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski", "title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "comments": "Appear in Transactions of the Association for Computational\n  Linguistics (TACL), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic word embeddings represent the meaning of a word via a vector, and\nare created by diverse methods. Many use nonlinear operations on co-occurrence\nstatistics, and have hand-tuned hyperparameters and reweighting methods.\n  This paper proposes a new generative model, a dynamic version of the\nlog-linear topic model of~\\citet{mnih2007three}. The methodological novelty is\nto use the prior to compute closed form expressions for word statistics. This\nprovides a theoretical justification for nonlinear models like PMI, word2vec,\nand GloVe, as well as some hyperparameter choices. It also helps explain why\nlow-dimensional semantic embeddings contain linear algebraic structure that\nallows solution of word analogies, as shown by~\\citet{mikolov2013efficient} and\nmany subsequent papers.\n  Experimental support is provided for the generative model assumptions, the\nmost important of which is that latent word vectors are fairly uniformly\ndispersed in space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:50:08 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 00:49:42 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 21:14:43 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2015 23:51:24 GMT"}, {"version": "v5", "created": "Wed, 14 Oct 2015 04:27:00 GMT"}, {"version": "v6", "created": "Wed, 24 Feb 2016 01:28:03 GMT"}, {"version": "v7", "created": "Fri, 22 Jul 2016 20:09:25 GMT"}, {"version": "v8", "created": "Wed, 19 Jun 2019 21:54:20 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Arora", "Sanjeev", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1502.03630", "submitter": "Min Yang", "authors": "Min Yang, Tianyi Cui, Wenting Tu", "title": "Ordering-sensitive and Semantic-aware Topic Modeling", "comments": "To appear in proceedings of AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 12:32:39 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Yang", "Min", ""], ["Cui", "Tianyi", ""], ["Tu", "Wenting", ""]]}, {"id": "1502.03671", "submitter": "R\\'emi Lebret", "authors": "R\\'emi Lebret, Pedro O. Pinheiro, Ronan Collobert", "title": "Phrase-based Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\nin two popular datasets for the task: Flickr30k and the recently proposed\nMicrosoft COCO.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:17:15 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 09:48:52 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Lebret", "R\u00e9mi", ""], ["Pinheiro", "Pedro O.", ""], ["Collobert", "Ronan", ""]]}, {"id": "1502.03682", "submitter": "Matthias Samwald", "authors": "Jose Antonio Mi\\~narro-Gim\\'enez, Oscar Mar\\'in-Alonso, Matthias\n  Samwald", "title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:44:15 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Mi\u00f1arro-Gim\u00e9nez", "Jose Antonio", ""], ["Mar\u00edn-Alonso", "Oscar", ""], ["Samwald", "Matthias", ""]]}, {"id": "1502.03752", "submitter": "Saad Alkahtani", "authors": "Saad Alkahtani, Wei Liu, and William J. Teahan", "title": "A new hybrid metric for verifying parallel corpora of Arabic-English", "comments": "in CCSEA-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a new metric that has been applied to verify the quality\nin translation between sentence pairs in parallel corpora of Arabic-English.\nThis metric combines two techniques, one based on sentence length and the other\nbased on compression code length. Experiments on sample test parallel\nArabic-English corpora indicate the combination of these two techniques\nimproves accuracy of the identification of satisfactory and unsatisfactory\nsentence pairs compared to sentence length and compression code length alone.\nThe new method proposed in this research is effective at filtering noise and\nreducing mis-translations resulting in greatly improved quality.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 17:49:45 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Alkahtani", "Saad", ""], ["Liu", "Wei", ""], ["Teahan", "William J.", ""]]}, {"id": "1502.04049", "submitter": "Preethi Raghavan", "authors": "Preethi Raghavan, James L. Chen, Eric Fosler-Lussier, Albert M. Lai", "title": "How essential are unstructured clinical narratives and information\n  fusion to clinical trial recruitment?", "comments": "AMIA TBI 2014, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Electronic health records capture patient information using structured\ncontrolled vocabularies and unstructured narrative text. While structured data\ntypically encodes lab values, encounters and medication lists, unstructured\ndata captures the physician's interpretation of the patient's condition,\nprognosis, and response to therapeutic intervention. In this paper, we\ndemonstrate that information extraction from unstructured clinical narratives\nis essential to most clinical applications. We perform an empirical study to\nvalidate the argument and show that structured data alone is insufficient in\nresolving eligibility criteria for recruiting patients onto clinical trials for\nchronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is\nessential to solving 59% of the CLL trial criteria and 77% of the prostate\ncancer trial criteria. More specifically, for resolving eligibility criteria\nwith temporal constraints, we show the need for temporal reasoning and\ninformation integration with medical events within and across unstructured\nclinical narratives and structured data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 16:28:40 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Raghavan", "Preethi", ""], ["Chen", "James L.", ""], ["Fosler-Lussier", "Eric", ""], ["Lai", "Albert M.", ""]]}, {"id": "1502.04081", "submitter": "David Belanger", "authors": "David Belanger and Sham Kakade", "title": "A Linear Dynamical System Model for Text", "comments": "Accepted at International Conference of Machine Learning 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low dimensional representations of words allow accurate NLP models to be\ntrained on limited annotated data. While most representations ignore words'\nlocal context, a natural way to induce context-dependent representations is to\nperform inference in a probabilistic latent-variable sequence model. Given the\nrecent success of continuous vector space word representations, we provide such\nan inference procedure for continuous states, where words' representations are\ngiven by the posterior mean of a linear dynamical system. Here, efficient\ninference can be performed using Kalman filtering. Our learning algorithm is\nextremely scalable, operating on simple cooccurrence counts for both parameter\ninitialization using the method of moments and subsequent iterations of EM. In\nour experiments, we employ our inferred word embeddings as features in standard\ntagging tasks, obtaining significant accuracy improvements. Finally, the Kalman\nfilter updates can be seen as a linear recurrent neural network. We demonstrate\nthat using the parameters of our model to initialize a non-linear recurrent\nneural network language model reduces its training time by a day and yields\nlower perplexity.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 18:39:29 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 20:04:53 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Belanger", "David", ""], ["Kakade", "Sham", ""]]}, {"id": "1502.04174", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Hai Zhao", "title": "Probabilistic Models for High-Order Projective Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents generalized probabilistic models for high-order\nprojective dependency parsing and an algorithmic framework for learning these\nstatistical models involving dependency trees. Partition functions and\nmarginals for high-order dependency trees can be computed efficiently, by\nadapting our algorithms which extend the inside-outside algorithm to\nhigher-order cases. To show the effectiveness of our algorithms, we perform\nexperiments on three languages---English, Chinese and Czech, using maximum\nconditional likelihood estimation for model training and L-BFGS for parameter\nestimation. Our methods achieve competitive performance for English, and\noutperform all previously reported dependency parsers for Chinese and Czech.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 06:47:34 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Ma", "Xuezhe", ""], ["Zhao", "Hai", ""]]}, {"id": "1502.04938", "submitter": "Arianna Bisazza", "authors": "Arianna Bisazza and Marcello Federico", "title": "A Survey of Word Reordering in Statistical Machine Translation:\n  Computational Models and Language Phenomena", "comments": "44 pages, to appear in Computational Linguistics", "journal-ref": "Computational Linguistics, Vol. 42, No. 2: 163-205, MIT Press\n  (June 2016)", "doi": "10.1162/COLI_a_00245", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word reordering is one of the most difficult aspects of statistical machine\ntranslation (SMT), and an important factor of its quality and efficiency.\nDespite the vast amount of research published to date, the interest of the\ncommunity in this problem has not decreased, and no single method appears to be\nstrongly dominant across language pairs. Instead, the choice of the optimal\napproach for a new translation task still seems to be mostly driven by\nempirical trials. To orientate the reader in this vast and complex research\narea, we present a comprehensive survey of word reordering viewed as a\nstatistical modeling challenge and as a natural language phenomenon. The survey\ndescribes in detail how word reordering is modeled within different\nstring-based and tree-based SMT frameworks and as a stand-alone task, including\nsystematic overviews of the literature in advanced reordering modeling. We then\nquestion why some approaches are more successful than others in different\nlanguage pairs. We argue that, besides measuring the amount of reordering, it\nis important to understand which kinds of reordering occur in a given language\npair. To this end, we conduct a qualitative analysis of word reordering\nphenomena in a diverse sample of language pairs, based on a large collection of\nlinguistic knowledge. Empirical results in the SMT literature are shown to\nsupport the hypothesis that a few linguistic facts can be very useful to\nanticipate the reordering characteristics of a language pair and to select the\nSMT framework that best suits them.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 15:59:09 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 22:52:23 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Bisazza", "Arianna", ""], ["Federico", "Marcello", ""]]}, {"id": "1502.05441", "submitter": "Ahmad Hassanat", "authors": "Ahmad B.A. Hassanat and Ghada Awad Altarawneh", "title": "Rule-and Dictionary-based Solution for Variations in Written Arabic\n  Names in Social Networks, Big Data, Accounting Systems and Large Databases", "comments": null, "journal-ref": "Research Journal of Applied Sciences, Engineering and Technology,\n  2014, 8(14): 1630-1638", "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem that some Arabic names can be written in\nmultiple ways. When someone searches for only one form of a name, neither exact\nnor approximate matching is appropriate for returning the multiple variants of\nthe name. Exact matching requires the user to enter all forms of the name for\nthe search, and approximate matching yields names not among the variations of\nthe one being sought. In this paper, we attempt to solve the problem with a\ndictionary of all Arabic names mapped to their different (alternative) writing\nforms. We generated alternatives based on rules we derived from reviewing the\nfirst names of 9.9 million citizens and former citizens of Jordan. This\ndictionary can be used for both standardizing the written form when inserting a\nnew name into a database and for searching for the name and all its alternative\nwritten forms. Creating the dictionary automatically based on rules resulted in\nat least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. We\naddressed the errors by manually editing the dictionary. The dictionary can be\nof help to real world-databases, with the qualification that manual editing\ndoes not guarantee 100% correctness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 23:16:00 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Hassanat", "Ahmad B. A.", ""], ["Altarawneh", "Ghada Awad", ""]]}, {"id": "1502.05472", "submitter": "Fabrizio Sebastiani", "authors": "Diego Marcheggiani and Fabrizio Sebastiani", "title": "On the Effects of Low-Quality Training Data on Information Extraction\n  from Clinical Reports", "comments": "Submitted for publication", "journal-ref": null, "doi": "10.1145/3106235", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the last five years there has been a flurry of work on information\nextraction from clinical documents, i.e., on algorithms capable of extracting,\nfrom the informal and unstructured texts that are generated during everyday\nclinical practice, mentions of concepts relevant to such practice. Most of this\nliterature is about methods based on supervised learning, i.e., methods for\ntraining an information extraction system from manually annotated examples.\nWhile a lot of work has been devoted to devising learning methods that generate\nmore and more accurate information extractors, no work has been devoted to\ninvestigating the effect of the quality of training data on the learning\nprocess. Low quality in training data often derives from the fact that the\nperson who has annotated the data is different from the one against whose\njudgment the automatically annotated data must be evaluated. In this paper we\ntest the impact of such data quality issues on the accuracy of information\nextraction systems as applied to the clinical domain. We do this by comparing\nthe accuracy deriving from training data annotated by the authoritative coder\n(i.e., the one who has also annotated the test data, and by whose judgment we\nmust abide), with the accuracy deriving from training data annotated by a\ndifferent coder. The results indicate that, although the disagreement between\nthe two coders (as measured on the training set) is substantial, the difference\nis (surprisingly enough) not always statistically significant.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 06:04:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 08:08:49 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Marcheggiani", "Diego", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1502.05698", "submitter": "Jason  Weston", "authors": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart\n  van Merri\\\"enboer, Armand Joulin, Tomas Mikolov", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One long-term goal of machine learning research is to produce methods that\nare applicable to reasoning and natural language, in particular building an\nintelligent dialogue agent. To measure progress towards that goal, we argue for\nthe usefulness of a set of proxy tasks that evaluate reading comprehension via\nquestion answering. Our tasks measure understanding in several ways: whether a\nsystem is able to answer questions via chaining facts, simple induction,\ndeduction and many more. The tasks are designed to be prerequisites for any\nsystem that aims to be capable of conversing with a human. We believe many\nexisting learning systems can currently not solve them, and hence our aim is to\nclassify these tasks into skill sets, so that researchers can identify (and\nthen rectify) the failings of their systems. We also extend and improve the\nrecently introduced Memory Networks model, and show it is able to solve some,\nbut not all, of the tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:46:10 GMT"}, {"version": "v10", "created": "Thu, 31 Dec 2015 13:08:14 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:35:12 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 17:50:21 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2015 14:03:33 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 14:51:46 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2015 21:58:20 GMT"}, {"version": "v7", "created": "Wed, 7 Oct 2015 19:36:24 GMT"}, {"version": "v8", "created": "Sat, 21 Nov 2015 23:23:02 GMT"}, {"version": "v9", "created": "Sun, 29 Nov 2015 06:24:27 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Weston", "Jason", ""], ["Bordes", "Antoine", ""], ["Chopra", "Sumit", ""], ["Rush", "Alexander M.", ""], ["van Merri\u00ebnboer", "Bart", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1502.05957", "submitter": "Paul Vitanyi", "authors": "Andrew R. Cohen (Dept Electri. Comput. Eng., Drexel Univ.), Paul M.B.\n  Vitanyi (CWI and University of Amsterdam)", "title": "Web Similarity in Sets of Search Terms using Database Queries", "comments": "LaTeX 18 pages, 3 tables. A precursor is arXiv:1308.3177", "journal-ref": "SN COMPUT. SCI. 1, 161(2020)", "doi": "10.1007/s42979-020-00148-5", "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized web distance (NWD) is a similarity or normalized semantic distance\nbased on the World Wide Web or another large electronic database, for instance\nWikipedia, and a search engine that returns reliable aggregate page counts. For\nsets of search terms the NWD gives a common similarity (common semantics) on a\nscale from 0 (identical) to 1 (completely different). The NWD approximates the\nsimilarity of members of a set according to all (upper semi)computable\nproperties. We develop the theory and give applications of classifying using\nAmazon, Wikipedia, and the NCBI website from the National Institutes of Health.\nThe last gives new correlations between health hazards. A restriction of the\nNWD to a set of two yields the earlier normalized google distance (NGD) but no\ncombination of the NGD's of pairs in a set can extract the information the NWD\nextracts from the set. The NWD enables a new contextual (different databases)\nlearning approachbased on Kolmogorov complexity theory that incorporates\nknowledge from these databases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:55:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 16:27:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Cohen", "Andrew R.", "", "Dept Electri. Comput. Eng., Drexel Univ."], ["Vitanyi", "Paul M. B.", "", "CWI and University of Amsterdam"]]}, {"id": "1502.06161", "submitter": "Thiago Marzag\\~ao", "authors": "Thiago Marzag\\~ao", "title": "Using NLP to measure democracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses natural language processing to create the first machine-coded\ndemocracy index, which I call Automated Democracy Scores (ADS). The ADS are\nbased on 42 million news articles from 6,043 different sources and cover all\nindependent countries in the 1993-2012 period. Unlike the democracy indices we\nhave today the ADS are replicable and have standard errors small enough to\nactually distinguish between cases.\n  The ADS are produced with supervised learning. Three approaches are tried: a)\na combination of Latent Semantic Analysis and tree-based regression methods; b)\na combination of Latent Dirichlet Allocation and tree-based regression methods;\nand c) the Wordscores algorithm. The Wordscores algorithm outperforms the\nalternatives, so it is the one on which the ADS are based.\n  There is a web application where anyone can change the training set and see\nhow the results change: democracy-scores.org\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 01:30:32 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Marzag\u00e3o", "Thiago", ""]]}, {"id": "1502.06922", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He,\n  Jianshu Chen, Xinying Song, Rabab Ward", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval", "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": "10.1109/TASLP.2016.2520371", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 19:39:27 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2015 06:11:19 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 06:35:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Palangi", "Hamid", ""], ["Deng", "Li", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Song", "Xinying", ""], ["Ward", "Rabab", ""]]}, {"id": "1502.07038", "submitter": "Dominick Ng", "authors": "Dominick Ng and Mohit Bansal and James R. Curran", "title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop novel first- and second-order features for dependency parsing\nbased on the Google Syntactic Ngrams corpus, a collection of subtree counts of\nparsed sentences from scanned books. We also extend previous work on surface\n$n$-gram features from Web1T to the Google Books corpus and from first-order to\nsecond-order, comparing and analysing performance over newswire and web\ntreebanks.\n  Surface and syntactic $n$-grams both produce substantial and complementary\ngains in parsing accuracy across domains. Our best system combines the two\nfeature sets, achieving up to 0.8% absolute UAS improvements on newswire and\n1.4% on web text.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 03:27:38 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Ng", "Dominick", ""], ["Bansal", "Mohit", ""], ["Curran", "James R.", ""]]}, {"id": "1502.07157", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA), Guiyao Ke (IRISA)", "title": "Exploiting a comparability mapping to improve bi-lingual data\n  categorization: a three-mode data analysis perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address in this paper the co-clustering and co-classification of bilingual\ndata laying in two linguistic similarity spaces when a comparability measure\ndefining a mapping between these two spaces is available. A new approach that\nwe can characterized as a three-mode analysis scheme, is proposed to mix the\ncomparability measure with the two similarity measures. Our aim is to improve\njointly the accuracy of classification and clustering tasks performed in each\nof the two linguistic spaces, as well as the quality of the final alignment of\ncomparable clusters that can be obtained. We used first some purely synthetic\nrandom data sets to assess our formal similarity-comparability mixing model. We\nthen propose two variants of the comparability measure that has been defined by\n(Li and Gaussier 2010) in the context of bilingual lexicon extraction to adapt\nit to clustering or categorizing tasks. These two variant measures are\nsubsequently used to evaluate our similarity-comparability mixing model in the\ncontext of the co-classification and co-clustering of comparable textual data\nsets collected from Wikipedia categories for the English and French languages.\nOur experiments show clear improvements in clustering and classification\naccuracies when mixing comparability with similarity measures, with, as\nexpected, a higher robustness obtained when the two comparability variant\nmeasures that we propose are used. We believe that this approach is\nparticularly well suited for the construction of thematic comparable corpora of\ncontrollable quality.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 13:07:41 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 19:30:15 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"], ["Ke", "Guiyao", "", "IRISA"]]}, {"id": "1502.07257", "submitter": "Sergey Bartunov", "authors": "Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, Dmitry Vetrov", "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed Skip-gram model is a powerful method for learning\nhigh-dimensional word representations that capture rich semantic relationships\nbetween words. However, Skip-gram as well as most prior work on learning word\nrepresentations does not take into account word ambiguity and maintain only\nsingle representation per word. Although a number of Skip-gram modifications\nwere proposed to overcome this limitation and learn multi-prototype word\nrepresentations, they either require a known number of word meanings or learn\nthem using greedy heuristic approaches. In this paper we propose the Adaptive\nSkip-gram model which is a nonparametric Bayesian extension of Skip-gram\ncapable to automatically learn the required number of representations for all\nwords at desired semantic resolution. We derive efficient online variational\nlearning algorithm for the model and empirically demonstrate its efficiency on\nword-sense induction task.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 17:15:56 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 10:36:49 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Bartunov", "Sergey", ""], ["Kondrashkin", "Dmitry", ""], ["Osokin", "Anton", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1502.07504", "submitter": "Attia Nehar", "authors": "Attia Nehar and Djelloul Ziadi and Hadda Cherroun", "title": "Rational Kernels for Arabic Stemming and Text Classification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problems of Arabic Text Classification and\nstemming using Transducers and Rational Kernels. We introduce a new stemming\ntechnique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns\nare modelled using transducers and stemming is done without depending on any\ndictionary. Using transducers for stemming, documents are transformed into\nfinite state transducers. This document representation allows us to use and\nexplore rational kernels as a framework for Arabic Text Classification.\nStemming experiments are conducted on three word collections and classification\nexperiments are done on the Saudi Press Agency dataset. Results show that our\napproach, when compared with other approaches, is promising specially in terms\nof Accuracy, Recall and F1.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 11:09:59 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Nehar", "Attia", ""], ["Ziadi", "Djelloul", ""], ["Cherroun", "Hadda", ""]]}, {"id": "1502.07920", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang", "title": "Local Translation Prediction with Global Sentence Representation", "comments": "7 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine translation models have made great progress in improving\nthe translation quality. However, the existing models predict the target\ntranslation with only the source- and target-side local context information. In\npractice, distinguishing good translations from bad ones does not only depend\non the local features, but also rely on the global sentence-level information.\nIn this paper, we explore the source-side global sentence-level features for\ntarget-side local translation prediction. We propose a novel\nbilingually-constrained chunk-based convolutional neural network to learn\nsentence semantic representations. With the sentence-level feature\nrepresentation, we further design a feed-forward neural network to better\npredict translations using both local and global information. The large-scale\nexperiments show that our method can obtain substantial improvements in\ntranslation quality over the strong baseline: the hierarchical phrase-based\ntranslation model augmented with the neural network joint model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 14:55:15 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Zhang", "Jiajun", ""]]}, {"id": "1502.08029", "submitter": "Li Yao", "authors": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,\n  Hugo Larochelle, Aaron Courville", "title": "Describing Videos by Exploiting Temporal Structure", "comments": "Accepted to ICCV15. This version comes with code release and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:30:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 17:24:47 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 15:27:08 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2015 20:32:27 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:12:46 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Yao", "Li", ""], ["Torabi", "Atousa", ""], ["Cho", "Kyunghyun", ""], ["Ballas", "Nicolas", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1502.08030", "submitter": "Hung Nghiep Tran", "authors": "Hung Nghiep Tran, Tin Huynh, Tien Do", "title": "Author Name Disambiguation by Using Deep Neural Network", "comments": null, "journal-ref": "Asian Conference on Intelligent Information and Database Systems\n  (ACIIDS 2014)", "doi": "10.1007/978-3-319-05476-6_13", "report-no": null, "categories": "cs.DL cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author name ambiguity decreases the quality and reliability of information\nretrieved from digital libraries. Existing methods have tried to solve this\nproblem by predefining a feature set based on expert's knowledge for a specific\ndataset. In this paper, we propose a new approach which uses deep neural\nnetwork to learn features automatically from data. Additionally, we propose the\ngeneral system architecture for author name disambiguation on any dataset. In\nthis research, we evaluate the proposed method on a dataset containing\nVietnamese author names. The results show that this method significantly\noutperforms other methods that use predefined feature set. The proposed method\nachieves 99.31% in terms of accuracy. Prediction error rate decreases from\n1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with\nother methods that use predefined feature set (Table 3).\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:34:42 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 01:32:31 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Tran", "Hung Nghiep", ""], ["Huynh", "Tin", ""], ["Do", "Tien", ""]]}, {"id": "1502.08033", "submitter": "Hung Nghiep Tran", "authors": "Vu Le Anh, Vo Hoang Hai, Hung Nghiep Tran, Jason J. Jung", "title": "SciRecSys: A Recommendation System for Scientific Publication by\n  Discovering Keyword Relationships", "comments": null, "journal-ref": "International Conference on Computational Collective Intelligence\n  (ICCCI 2014)", "doi": "10.1007/978-3-319-11289-3_8", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new approach for discovering various relationships\namong keywords over the scientific publications based on a Markov Chain model.\nIt is an important problem since keywords are the basic elements for\nrepresenting abstract objects such as documents, user profiles, topics and many\nthings else. Our model is very effective since it combines four important\nfactors in scientific publications: content, publicity, impact and randomness.\nParticularly, a recommendation system (called SciRecSys) has been presented to\nsupport users to efficiently find out relevant articles.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:35:24 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Anh", "Vu Le", ""], ["Hai", "Vo Hoang", ""], ["Tran", "Hung Nghiep", ""], ["Jung", "Jason J.", ""]]}]