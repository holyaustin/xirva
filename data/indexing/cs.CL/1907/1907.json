[{"id": "1907.00112", "submitter": "Vikramjit Mitra", "authors": "Vikramjit Mitra, Sue Booker, Erik Marchi, David Scott Farrar, Ute\n  Dorothea Peitz, Bridget Cheng, Ermine Teves, Anuj Mehta, Devang Naik", "title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect\n  Expression from Voice", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Millions of people reach out to digital assistants such as Siri every day,\nasking for information, making phone calls, seeking assistance, and much more.\nThe expectation is that such assistants should understand the intent of the\nusers query. Detecting the intent of a query from a short, isolated utterance\nis a difficult task. Intent cannot always be obtained from speech-recognized\ntranscriptions. A transcription driven approach can interpret what has been\nsaid but fails to acknowledge how it has been said, and as a consequence, may\nignore the expression present in the voice. Our work investigates whether a\nsystem can reliably detect vocal expression in queries using acoustic and\nparalinguistic embedding. Results show that the proposed method offers a\nrelative equal error rate (EER) decrease of 60% compared to a bag-of-word based\nsystem, corroborating that expression is significantly represented by vocal\nattributes, rather than being purely lexical. Addition of emotion embedding\nhelped to reduce the EER by 30% relative to the acoustic embedding,\ndemonstrating the relevance of emotion in expressive voice.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:57:36 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Mitra", "Vikramjit", ""], ["Booker", "Sue", ""], ["Marchi", "Erik", ""], ["Farrar", "David Scott", ""], ["Peitz", "Ute Dorothea", ""], ["Cheng", "Bridget", ""], ["Teves", "Ermine", ""], ["Mehta", "Anuj", ""], ["Naik", "Devang", ""]]}, {"id": "1907.00151", "submitter": "Yi Liao", "authors": "Yi Liao, Yasheng Wang, Qun Liu, Xin Jiang", "title": "GPT-based Generation for Classical Chinese Poetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective method for generating high quality\nclassical Chinese poetry with Generative Pre-trained Language Model (GPT). The\nmethod adopts a simple GPT model, without using any human crafted rules or\nfeatures, or designing any additional neural components. While the proposed\nmodel learns to generate various forms of classical Chinese poems, including\nJueju, L\\\"{u}shi, various Cipai and Couples, the generated poems are of very\nhigh quality. We also propose and implement a method to fine-tune the model to\ngenerate acrostic poetry. To the best of our knowledge, this is the first to\nemploy GPT in developing a poetry generation system. We have released an online\nmini demonstration program on Wechat to show the generation capability of the\nproposed method for classical Chinese poetry.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 06:04:48 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 14:18:55 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 12:55:06 GMT"}, {"version": "v4", "created": "Fri, 12 Jul 2019 05:44:58 GMT"}, {"version": "v5", "created": "Thu, 5 Sep 2019 02:34:36 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Liao", "Yi", ""], ["Wang", "Yasheng", ""], ["Liu", "Qun", ""], ["Jiang", "Xin", ""]]}, {"id": "1907.00157", "submitter": "Sandeep Singh Adhikari Mr", "authors": "Sandeep Singh Adhikari, Sukhneer Singh, Anoop Rajagopal, Aruna Rajan", "title": "Progressive Fashion Attribute Extraction", "comments": "6 pages, 6 figures, AI for fashion : KDD 2019 Workshop, August 2019,\n  Anchorage, Alaska - USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting fashion attributes from images of people wearing clothing/fashion\naccessories is a very hard multi-class classification problem. Most often, even\ncatalogues of fashion do not have all the fine-grained attributes tagged due to\nprohibitive cost of annotation. Using images of fashion articles, running\nmulti-class attribute extraction with a single model for all kinds of\nattributes (neck design detailing, sleeves detailing, etc) requires classifiers\nthat are robust to missing and ambiguously labelled data. In this work, we\npropose a progressive training approach for such multi-class classification,\nwhere weights learnt from an attribute are fine tuned for another attribute of\nthe same fashion article (say, dresses). We branch networks for each attributes\nfrom a base network progressively during training. While it may have many\nlabels, an image doesn't need to have all possible labels for fashion articles\npresent in it. We also compare our approach to multi-label classification, and\ndemonstrate improvements over overall classification accuracies using our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 06:51:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Adhikari", "Sandeep Singh", ""], ["Singh", "Sukhneer", ""], ["Rajagopal", "Anoop", ""], ["Rajan", "Aruna", ""]]}, {"id": "1907.00168", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg and Bill Byrne", "title": "The CUED's Grammatical Error Correction Systems for BEA-2019", "comments": "BEA-2019 (ACL2019 workshop) shared task system description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two entries from the Cambridge University Engineering Department\nto the BEA 2019 Shared Task on grammatical error correction. Our submission to\nthe low-resource track is based on prior work on using finite state transducers\ntogether with strong neural language models. Our system for the restricted\ntrack is a purely neural system consisting of neural language models and neural\nmachine translation models trained with back-translation and a combination of\ncheckpoint averaging and fine-tuning -- without the help of any additional\ntools like spell checkers. The latter system has been used inside a separate\nsystem combination entry in cooperation with the Cambridge University Computer\nLab.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 09:00:45 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Stahlberg", "Felix", ""], ["Byrne", "Bill", ""]]}, {"id": "1907.00181", "submitter": "Anders Edelbo Lillie", "authors": "Anders Edelbo Lillie and Emil Refsgaard Middelboe", "title": "Fake News Detection using Stance Classification: A Survey", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper surveys and presents recent academic work carried out within the\nfield of stance classification and fake news detection. Echo chambers and the\nmodel organism problem are examples that pose challenges to acquire data with\nhigh quality, due to opinions being polarised in microblogs. Nevertheless it is\nshown that several machine learning approaches achieve promising results in\nclassifying stance. Some use crowd stance for fake news detection, such as the\napproach in [Dungs et al., 2018] using Hidden Markov Models. Furthermore\nfeature engineering have significant importance in several approaches, which is\nshown in [Aker et al., 2017]. This paper additionally includes a proposal of a\nsystem implementation based on the presented survey.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 11:00:22 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lillie", "Anders Edelbo", ""], ["Middelboe", "Emil Refsgaard", ""]]}, {"id": "1907.00184", "submitter": "Marcely Zanon Boito", "authors": "Marcely Zanon Boito, Aline Villavicencio, Laurent Besacier", "title": "Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery\n  in Low-resource Settings", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Bahdanau et al. [1] first introduced attention for neural machine\ntranslation, most sequence-to-sequence models made use of attention mechanisms\n[2, 3, 4]. While they produce soft-alignment matrices that could be interpreted\nas alignment between target and source languages, we lack metrics to quantify\ntheir quality, being unclear which approach produces the best alignments. This\npaper presents an empirical evaluation of 3 main sequence-to-sequence models\n(CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme\nsequences. This task consists in aligning word sequences in a source language\nwith phoneme sequences in a target language, inferring from it word\nsegmentation on the target side [5]. Evaluating word segmentation quality can\nbe seen as an extrinsic evaluation of the soft-alignment matrices produced\nduring training. Our experiments in a low-resource scenario on Mboshi and\nEnglish languages (both aligned to French) show that RNNs surprisingly\noutperform CNNs and Transformer for this task. Our results are confirmed by an\nintrinsic evaluation of alignment quality through the use of Average Normalized\nEntropy (ANE). Lastly, we improve our best word discovery model by using an\nalignment entropy confidence measure that accumulates ANE over all the\noccurrences of a given alignment pair in the collection.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 11:47:22 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 12:35:35 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Boito", "Marcely Zanon", ""], ["Villavicencio", "Aline", ""], ["Besacier", "Laurent", ""]]}, {"id": "1907.00218", "submitter": "Liwen Zhang", "authors": "Liwen Zhang, Kewei Tu, Yue Zhang", "title": "Latent Variable Sentiment Grammar", "comments": "Accepted at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have been investigated for sentiment classification over\nconstituent trees. They learn phrase composition automatically by encoding tree\nstructures but do not explicitly model sentiment composition, which requires to\nencode sentiment class labels. To this end, we investigate two formalisms with\ndeep sentiment representations that capture sentiment subtype expressions by\nlatent variables and Gaussian mixture vectors, respectively. Experiments on\nStanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar\nover vanilla neural encoders. Using ELMo embeddings, our method gives the best\nresults on this benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 15:15:16 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:28:39 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Liwen", ""], ["Tu", "Kewei", ""], ["Zhang", "Yue", ""]]}, {"id": "1907.00321", "submitter": "Lonce Wyse", "authors": "Lonce Wyse", "title": "Mechanisms of Artistic Creativity in Deep Learning Neural Networks", "comments": "8 pages, International Conference on Computational Creativity,\n  Charlotte, NC, USA. June, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generative capabilities of deep learning neural networks (DNNs) have been\nattracting increasing attention for both the remarkable artifacts they produce,\nbut also because of the vast conceptual difference between how they are\nprogrammed and what they do. DNNs are 'black boxes' where high-level behavior\nis not explicitly programmed, but emerges from the complex interactions of\nthousands or millions of simple computational elements. Their behavior is often\ndescribed in anthropomorphic terms that can be misleading, seem magical, or\nstoke fears of an imminent singularity in which machines become 'more' than\nhuman. In this paper, we examine 5 distinct behavioral characteristics\nassociated with creativity, and provide an example of a mechanisms from\ngenerative deep learning architectures that give rise to each these\ncharacteristics. All 5 emerge from machinery built for purposes other than the\ncreative characteristics they exhibit, mostly classification. These mechanisms\nof creative generative capabilities thus demonstrate a deep kinship to\ncomputational perceptual processes. By understanding how these different\nbehaviors arise, we hope to on one hand take the magic out of anthropomorphic\ndescriptions, but on the other, to build a deeper appreciation of machinic\nforms of creativity on their own terms that will allow us to nurture their\nfurther development.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 05:29:38 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wyse", "Lonce", ""]]}, {"id": "1907.00326", "submitter": "Jie Cao", "authors": "Jie Cao, Michael Tanana, Zac E. Imel, Eric Poitras, David C. Atkins,\n  Vivek Srikumar", "title": "Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral\n  Codes", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically analyzing dialogue can help understand and guide behavior in\ndomains such as counseling, where interactions are largely mediated by\nconversation. In this paper, we study modeling behavioral codes used to asses a\npsychotherapy treatment style called Motivational Interviewing (MI), which is\neffective for addressing substance abuse and related problems. Specifically, we\naddress the problem of providing real-time guidance to therapists with a\ndialogue observer that (1) categorizes therapist and client MI behavioral codes\nand, (2) forecasts codes for upcoming utterances to help guide the conversation\nand potentially alert the therapist. For both tasks, we define neural network\nmodels that build upon recent successes in dialogue modeling. Our experiments\ndemonstrate that our models can outperform several baselines for both tasks. We\nalso report the results of a careful analysis that reveals the impact of the\nvarious network design tradeoffs for modeling therapy dialogue.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 06:03:32 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Cao", "Jie", ""], ["Tanana", "Michael", ""], ["Imel", "Zac E.", ""], ["Poitras", "Eric", ""], ["Atkins", "David C.", ""], ["Srikumar", "Vivek", ""]]}, {"id": "1907.00390", "submitter": "Zhonfu Chen", "authors": "Haihong E, Peiqing Niu, Zhongfu Chen, Meina Song", "title": "A Novel Bi-directional Interrelated Model for Joint Intent Detection and\n  Slot Filling", "comments": "Accepted paper of ACL 2019 (short paper) with 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spoken language understanding (SLU) system includes two main tasks, slot\nfilling (SF) and intent detection (ID). The joint model for the two tasks is\nbecoming a tendency in SLU. But the bi-directional interrelated connections\nbetween the intent and slots are not established in the existing joint models.\nIn this paper, we propose a novel bi-directional interrelated model for joint\nintent detection and slot filling. We introduce an SF-ID network to establish\ndirect connections for the two tasks to help them promote each other mutually.\nBesides, we design an entirely new iteration mechanism inside the SF-ID network\nto enhance the bi-directional interrelated connections. The experimental\nresults show that the relative improvement in the sentence-level semantic frame\naccuracy of our model is 3.79% and 5.42% on ATIS and Snips datasets,\nrespectively, compared to the state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 14:54:01 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["E", "Haihong", ""], ["Niu", "Peiqing", ""], ["Chen", "Zhongfu", ""], ["Song", "Meina", ""]]}, {"id": "1907.00409", "submitter": "Jan Christian Blaise Cruz", "authors": "Jan Christian Blaise Cruz and Charibeth Cheng", "title": "Evaluating Language Model Finetuning Techniques for Low-resource\n  Languages", "comments": "Pretrained models and datasets available at\n  https://github.com/jcblaisecruz02/Tagalog-BERT", "journal-ref": null, "doi": "10.13140/RG.2.2.23028.40322", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike mainstream languages (such as English and French), low-resource\nlanguages often suffer from a lack of expert-annotated corpora and benchmark\nresources that make it hard to apply state-of-the-art techniques directly. In\nthis paper, we alleviate this scarcity problem for the low-resourced Filipino\nlanguage in two ways. First, we introduce a new benchmark language modeling\ndataset in Filipino which we call WikiText-TL-39. Second, we show that language\nmodel finetuning techniques such as BERT and ULMFiT can be used to consistently\ntrain robust classifiers in low-resource settings, experiencing at most a\n0.0782 increase in validation error when the number of training examples is\ndecreased from 10K to 1K while finetuning using a privately-held sentiment\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 16:32:28 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Cruz", "Jan Christian Blaise", ""], ["Cheng", "Charibeth", ""]]}, {"id": "1907.00443", "submitter": "Dhananjay Ram", "authors": "Dhananjay Ram, Lesly Miculicich, Herv\\'e Bourlard", "title": "Multilingual Bottleneck Features for Query by Example Spoken Term\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art solutions to query by example spoken term detection\n(QbE-STD) usually rely on bottleneck feature representation of the query and\naudio document to perform dynamic time warping (DTW) based template matching.\nHere, we present a study on QbE-STD performance using several monolingual as\nwell as multilingual bottleneck features extracted from feed forward networks.\nThen, we propose to employ residual networks (ResNet) to estimate the\nbottleneck features and show significant improvements over the corresponding\nfeed forward network based features. The neural networks are trained on\nGlobalPhone corpus and QbE-STD experiments are performed on a very challenging\nQUESST 2014 database.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 20:14:19 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ram", "Dhananjay", ""], ["Miculicich", "Lesly", ""], ["Bourlard", "Herv\u00e9", ""]]}, {"id": "1907.00448", "submitter": "Jiawei Wu", "authors": "Jiawei Wu, Xin Wang, William Yang Wang", "title": "Self-Supervised Dialogue Learning", "comments": "11pages, 2 figures, accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sequential order of utterances is often meaningful in coherent dialogues,\nand the order changes of utterances could lead to low-quality and incoherent\nconversations. We consider the order information as a crucial supervised signal\nfor dialogue learning, which, however, has been neglected by many previous\ndialogue systems. Therefore, in this paper, we introduce a self-supervised\nlearning task, inconsistent order detection, to explicitly capture the flow of\nconversation in dialogues. Given a sampled utterance pair triple, the task is\nto predict whether it is ordered or misordered. Then we propose a\nsampling-based self-supervised network SSN to perform the prediction with\nsampled triple references from previous dialogue history. Furthermore, we\ndesign a joint learning framework where SSN can guide the dialogue systems\ntowards more coherent and relevant dialogue learning through adversarial\ntraining. We demonstrate that the proposed methods can be applied to both\nopen-domain and task-oriented dialogue scenarios, and achieve the new\nstate-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 20:17:52 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wu", "Jiawei", ""], ["Wang", "Xin", ""], ["Wang", "William Yang", ""]]}, {"id": "1907.00455", "submitter": "Marie-Jean Meurs", "authors": "Diego Maupom\\'e and Marie-Jean Meurs", "title": "Multiplicative Models for Recurrent Language Modeling", "comments": "10 pages, pre-print from Proceedings of CICLing 2019: 20th\n  International Conference on Computational Linguistics and Intelligent Text\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been interest in multiplicative recurrent neural networks\nfor language modeling. Indeed, simple Recurrent Neural Networks (RNNs)\nencounter difficulties recovering from past mistakes when generating sequences\ndue to high correlation between hidden states. These challenges can be\nmitigated by integrating second-order terms in the hidden-state update. One\nsuch model, multiplicative Long Short-Term Memory (mLSTM) is particularly\ninteresting in its original formulation because of the sharing of its\nsecond-order term, referred to as the intermediate state. We explore these\narchitectural improvements by introducing new models and testing them on\ncharacter-level language modeling tasks. This allows us to establish the\nrelevance of shared parametrization in recurrent language modeling.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 20:51:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Maupom\u00e9", "Diego", ""], ["Meurs", "Marie-Jean", ""]]}, {"id": "1907.00457", "submitter": "Julian Salazar", "authors": "Shaoshi Ling, Julian Salazar, Katrin Kirchhoff", "title": "Contextual Phonetic Pretraining for End-to-end Utterance-level Language\n  and Speaker Recognition", "comments": "submitted to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained contextual word representations in NLP have greatly improved\nperformance on various downstream tasks. For speech, we propose contextual\nframe representations that capture phonetic information at the acoustic frame\nlevel and can be used for utterance-level language, speaker, and speech\nrecognition. These representations come from the frame-wise intermediate\nrepresentations of an end-to-end, self-attentive ASR model (SAN-CTC) on spoken\nutterances. We first train the model on the Fisher English corpus with\ncontext-independent phoneme labels, then use its representations at inference\ntime as features for task-specific models on the NIST LRE07 closed-set language\nrecognition task and a Fisher speaker recognition task, giving significant\nimprovements over the state-of-the-art on both (e.g., language EER of 4.68% on\n3sec utterances, 23% relative reduction in speaker EER). Results remain\ncompetitive when using a novel dilated convolutional model for language\nrecognition, or when ASR pretraining is done with character labels only.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 20:54:21 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ling", "Shaoshi", ""], ["Salazar", "Julian", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "1907.00462", "submitter": "Marie-Jean Meurs", "authors": "Diego Maupom\\'e, Marc Queudot, Marie-Jean Meurs", "title": "Inter and Intra Document Attention for Depression Risk Assessment", "comments": "9 pages, in Proceedings of The 32nd Canadian Conference on Artificial\n  Intelligence (Canadian AI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take interest in the early assessment of risk for depression in social\nmedia users. We focus on the eRisk 2018 dataset, which represents users as a\nsequence of their written online contributions. We implement four RNN-based\nsystems to classify the users. We explore several aggregations methods to\ncombine predictions on individual posts. Our best model reads through all\nwritings of a user in parallel but uses an attention mechanism to prioritize\nthe most important ones at each timestep.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:09:48 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Maupom\u00e9", "Diego", ""], ["Queudot", "Marc", ""], ["Meurs", "Marie-Jean", ""]]}, {"id": "1907.00464", "submitter": "Joseph Fisher", "authors": "Joseph Fisher, Andreas Vlachos", "title": "Merge and Label: A novel neural network architecture for nested NER", "comments": "Accepted at ACL 2019. Code available at\n  https://github.com/fishjh2/merge_label", "journal-ref": "ACL 2019 P19-1585", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is one of the best studied tasks in natural\nlanguage processing. However, most approaches are not capable of handling\nnested structures which are common in many applications. In this paper we\nintroduce a novel neural network architecture that first merges tokens and/or\nentities into entities forming nested structures, and then labels each of them\nindependently. Unlike previous work, our merge and label approach predicts\nreal-valued instead of discrete segmentation structures, which allow it to\ncombine word and nested entity embeddings while maintaining differentiability.\n%which smoothly groups entities into single vectors across multiple levels. We\nevaluate our approach using the ACE 2005 Corpus, where it achieves\nstate-of-the-art F1 of 74.6, further improved with contextual embeddings (BERT)\nto 82.4, an overall improvement of close to 8 F1 points over previous\napproaches trained on the same data. Additionally we compare it against\nBiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that\nits ability to predict nested structures does not impact performance in simpler\ncases.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:12:14 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Fisher", "Joseph", ""], ["Vlachos", "Andreas", ""]]}, {"id": "1907.00477", "submitter": "Tejas Srinivasan", "authors": "Tejas Srinivasan, Ramon Sanabria, Florian Metze", "title": "Analyzing Utility of Visual Context in Multimodal Speech Recognition\n  Under Noisy Conditions", "comments": "Accepted to How2 Workshop, ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning allows us to leverage information from multiple sources\n(visual, acoustic and text), similar to our experience of the real world.\nHowever, it is currently unclear to what extent auxiliary modalities improve\nperformance over unimodal models, and under what circumstances the auxiliary\nmodalities are useful. We examine the utility of the auxiliary visual context\nin Multimodal Automatic Speech Recognition in adversarial settings, where we\ndeprive the models from partial audio signal during inference time. Our\nexperiments show that while MMASR models show significant gains over\ntraditional speech-to-text architectures (upto 4.2% WER improvements), they do\nnot incorporate visual information when the audio signal has been corrupted.\nThis shows that current methods of integrating the visual modality do not\nimprove model robustness to noise, and we need better visually grounded\nadaptation techniques.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 21:49:07 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 14:39:48 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Srinivasan", "Tejas", ""], ["Sanabria", "Ramon", ""], ["Metze", "Florian", ""]]}, {"id": "1907.00488", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock", "title": "Topic Modeling the Reading and Writing Behavior of Information Foragers", "comments": "Accepted Ph.D. dissertation, Indiana University, Informatics (Complex\n  Systems) and Cognitive Science, June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general problem of \"information foraging\" in an environment about which\nagents have incomplete information has been explored in many fields, including\ncognitive psychology, neuroscience, economics, finance, ecology, and computer\nscience. In all of these areas, the searcher aims to enhance future performance\nby surveying enough of existing knowledge to orient themselves in the\ninformation space. Individuals can be viewed as conducting a cognitive search\nin which they must balance exploration of ideas that are novel to them against\nexploitation of knowledge in domains in which they are already expert.\n  In this dissertation, I present several case studies that demonstrate how\nreading and writing behaviors interact to construct personal knowledge bases.\nThese studies use LDA topic modeling to represent the information environment\nof the texts each author read and wrote. Three studies revolve around Charles\nDarwin. Darwin left detailed records of every book he read for 23 years, from\ndisembarking from the H.M.S. Beagle to just after publication of The Origin of\nSpecies. Additionally, he left copies of his drafts before publication. I\ncharacterize his reading behavior, then show how that reading behavior\ninteracted with the drafts and subsequent revisions of The Origin of Species,\nand expand the dataset to include later readings and writings. Then, through a\nstudy of Thomas Jefferson's correspondence, I expand the study to non-book\ndata. Finally, through an examination of neuroscience citation data, I move\nfrom individual behavior to collective behavior in constructing an information\nenvironment. Together, these studies reveal \"the interplay between individual\nand collective phenomena where innovation takes place\" (Tria et al. 2014).\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:40:37 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Murdock", "Jaimie", ""]]}, {"id": "1907.00494", "submitter": "Liang Ding", "authors": "Liang Ding and Dacheng Tao", "title": "The University of Sydney's Machine Translation System for WMT19", "comments": "To appear in WMT2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the University of Sydney's submission of the WMT 2019\nshared news translation task. We participated in the\nFinnish$\\rightarrow$English direction and got the best BLEU(33.0) score among\nall the participants. Our system is based on the self-attentional Transformer\nnetworks, into which we integrated the most recent effective strategies from\nacademic research (e.g., BPE, back translation, multi-features data selection,\ndata augmentation, greedy model ensemble, reranking, ConMBR system combination,\nand post-processing). Furthermore, we propose a novel augmentation method\n$Cycle Translation$ and a data mixture strategy $Big$/$Small$ parallel\nconstruction to entirely exploit the synthetic corpus. Extensive experiments\nshow that adding the above techniques can make continuous improvements of the\nBLEU scores, and the best result outperforms the baseline (Transformer ensemble\nmodel trained with the original parallel corpus) by approximately 5.3 BLEU\nscore, achieving the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:55:27 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ding", "Liang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1907.00505", "submitter": "Ziniu Hu", "authors": "Ziniu Hu and Ting Chen and Kai-Wei Chang and Yizhou Sun", "title": "Few-Shot Representation Learning for Out-Of-Vocabulary Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for learning word embeddings often assume there are\nsufficient occurrences for each word in the corpus, such that the\nrepresentation of words can be accurately estimated from their contexts.\nHowever, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do\nnot appear in training corpus emerge frequently. It is challenging to learn\naccurate representations of these words with only a few observations. In this\npaper, we formulate the learning of OOV embeddings as a few-shot regression\nproblem, and address it by training a representation function to predict the\noracle embedding vector (defined as embedding trained with abundant\nobservations) based on limited observations. Specifically, we propose a novel\nhierarchical attention-based architecture to serve as the neural regression\nfunction, with which the context information of a word is encoded and\naggregated from K observations. Furthermore, our approach can leverage\nModel-Agnostic Meta-Learning (MAML) for adapting the learned model to the new\ncorpus fast and robustly. Experiments show that the proposed approach\nsignificantly outperforms existing methods in constructing accurate embeddings\nfor OOV words, and improves downstream tasks where these embeddings are\nutilized.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 00:43:45 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hu", "Ziniu", ""], ["Chen", "Ting", ""], ["Chang", "Kai-Wei", ""], ["Sun", "Yizhou", ""]]}, {"id": "1907.00510", "submitter": "Amir Karami", "authors": "Amir Karami, Suzanne C. Swan, Cynthia Nicole White, Kayla Ford", "title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to\n  Shine a Light on Workplace Sexism and Sexual Harassment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The goal of this study is to understand how people experience\nsexism and sexual harassment in the workplace by discovering themes in 2,362\nexperiences posted on the Everyday Sexism Project's website everydaysexism.com.\nMethod: This study used both quantitative and qualitative methods. The\nquantitative method was a computational framework to collect and analyze a\nlarge number of workplace sexual harassment experiences. The qualitative method\nwas the analysis of the topics generated by a text mining method. Results:\nTwenty-three topics were coded and then grouped into three overarching themes\nfrom the sex discrimination and sexual harassment literature. The Sex\nDiscrimination theme included experiences in which women were treated\nunfavorably due to their sex, such as being passed over for promotion, denied\nopportunities, paid less than men, and ignored or talked over in meetings. The\nSex Discrimination and Gender harassment theme included stories about sex\ndiscrimination and gender harassment, such as sexist hostility behaviors\nranging from insults and jokes invoking misogynistic stereotypes to bullying\nbehavior. The last theme, Unwanted Sexual Attention, contained stories\ndescribing sexual comments and behaviors used to degrade women. Unwanted\ntouching was the highest weighted topic, indicating how common it was for\nwebsite users to endure being touched, hugged or kissed, groped, and grabbed.\nConclusions: This study illustrates how researchers can use automatic processes\nto go beyond the limits of traditional research methods and investigate\nnaturally occurring large scale datasets on the internet to achieve a better\nunderstanding of everyday workplace sexism experiences.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 01:48:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Karami", "Amir", ""], ["Swan", "Suzanne C.", ""], ["White", "Cynthia Nicole", ""], ["Ford", "Kayla", ""]]}, {"id": "1907.00570", "submitter": "Joris Baan", "authors": "Joris Baan, Maartje ter Hoeve, Marlies van der Wees, Anne Schuth,\n  Maarten de Rijke", "title": "Do Transformer Attention Heads Provide Transparency in Abstractive\n  Summarization?", "comments": "To appear at FACTS-IR 2019, SIGIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms become more powerful, often at the cost of increased\ncomplexity. In response, the demand for algorithms to be transparent is\ngrowing. In NLP tasks, attention distributions learned by attention-based deep\nlearning models are used to gain insights in the models' behavior. To which\nextent is this perspective valid for all NLP tasks? We investigate whether\ndistributions calculated by different attention heads in a transformer\narchitecture can be used to improve transparency in the task of abstractive\nsummarization. To this end, we present both a qualitative and quantitative\nanalysis to investigate the behavior of the attention heads. We show that some\nattention heads indeed specialize towards syntactically and semantically\ndistinct input. We propose an approach to evaluate to which extent the\nTransformer model relies on specifically learned attention distributions. We\nalso discuss what this implies for using attention distributions as a means of\ntransparency.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 06:46:43 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:57:07 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Baan", "Joris", ""], ["ter Hoeve", "Maartje", ""], ["van der Wees", "Marlies", ""], ["Schuth", "Anne", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1907.00607", "submitter": "Jiyuan Zheng", "authors": "Yutong Wang, Jiyuan Zheng, Qijiong Liu, Zhou Zhao, Jun Xiao, Yueting\n  Zhuang", "title": "Weak Supervision Enhanced Generative Network for Question Generation", "comments": "Published as a conference paper at IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic question generation according to an answer within the given passage\nis useful for many applications, such as question answering system, dialogue\nsystem, etc. Current neural-based methods mostly take two steps which extract\nseveral important sentences based on the candidate answer through manual rules\nor supervised neural networks and then use an encoder-decoder framework to\ngenerate questions about these sentences. These approaches neglect the semantic\nrelations between the answer and the context of the whole passage which is\nsometimes necessary for answering the question. To address this problem, we\npropose the Weak Supervision Enhanced Generative Network (WeGen) which\nautomatically discovers relevant features of the passage given the answer span\nin a weakly supervised manner to improve the quality of generated questions.\nMore specifically, we devise a discriminator, Relation Guider, to capture the\nrelations between the whole passage and the associated answer and then the\nMulti-Interaction mechanism is deployed to transfer the knowledge dynamically\nfor our question generation system. Experiments show the effectiveness of our\nmethod in both automatic evaluations and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 08:44:30 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Yutong", ""], ["Zheng", "Jiyuan", ""], ["Liu", "Qijiong", ""], ["Zhao", "Zhou", ""], ["Xiao", "Jun", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1907.00620", "submitter": "Huilin Gao", "authors": "Tong Guo, Huilin Gao", "title": "Using Database Rule for Weak Supervised Text-to-SQL Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple way to do the task of text-to-SQL problem with weak\nsupervision. We call it Rule-SQL. Given the question and the answer from the\ndatabase table without the SQL logic form, Rule-SQL use the rules based on\ntable column names and question string for the SQL exploration first and then\nuse the explored SQL for supervised training. We design several rules for\nreducing the exploration search space. For the deep model, we leverage BERT for\nthe representation layer and separate the model to SELECT, AGG and WHERE parts.\nThe experiment result on WikiSQL outperforms the strong baseline of full\nsupervision and is comparable to the start-of-the-art weak supervised mothods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 09:14:45 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 03:35:00 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 08:55:56 GMT"}, {"version": "v4", "created": "Thu, 25 Jul 2019 03:13:29 GMT"}, {"version": "v5", "created": "Wed, 31 Jul 2019 03:08:45 GMT"}, {"version": "v6", "created": "Mon, 9 Sep 2019 07:48:37 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Guo", "Tong", ""], ["Gao", "Huilin", ""]]}, {"id": "1907.00659", "submitter": "Miguel Domingo", "authors": "Miguel Domingo and Francisco Casacuberta", "title": "Modernizing Historical Documents: a User Study", "comments": "Accepted for publication at Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2020.02.027", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessibility to historical documents is mostly limited to scholars. This is\ndue to the language barrier inherent in human language and the linguistic\nproperties of these documents. Given a historical document, modernization aims\nto generate a new version of it, written in the modern version of the\ndocument's language. Its goal is to tackle the language barrier, decreasing the\ncomprehension difficulty and making historical documents accessible to a\nbroader audience. In this work, we proposed a new neural machine translation\napproach that profits from modern documents to enrich its systems. We tested\nthis approach with both automatic and human evaluation, and conducted a user\nstudy. Results showed that modernization is successfully reaching its goal,\nalthough it still has room for improvement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:13:19 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 08:39:04 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Domingo", "Miguel", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "1907.00692", "submitter": "Sihem Sahnoun", "authors": "Sihem Sahnoun", "title": "Event extraction based on open information extraction and ontology", "comments": "arXiv admin note: text overlap with arXiv:1607.02784 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented in this master thesis consists of extracting a set of\nevents from texts written in natural language. For this purpose, we have based\nourselves on the basic notions of the information extraction as well as the\nopen information extraction. First, we applied an open information\nextraction(OIE) system for the relationship extraction, to highlight the\nimportance of OIEs in event extraction, and we used the ontology to the event\nmodeling. We tested the results of our approach with test metrics. As a result,\nthe two-level event extraction approach has shown good performance results but\nrequires a lot of expert intervention in the construction of classifiers and\nthis will take time. In this context we have proposed an approach that reduces\nthe expert intervention in the relation extraction, the recognition of entities\nand the reasoning which are automatic and based on techniques of adaptation and\ncorrespondence. Finally, to prove the relevance of the extracted results, we\nconducted a set of experiments using different test metrics as well as a\ncomparative study.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:24:46 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Sahnoun", "Sihem", ""]]}, {"id": "1907.00708", "submitter": "Dominic Danks", "authors": "Fran\\c{c}ois-Xavier Aubet, Dominic Danks, Yuchen Zhu", "title": "EQuANt (Enhanced Question Answer Network)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Reading Comprehension (MRC) is an important topic in the domain of\nautomated question answering and in natural language processing more generally.\nSince the release of the SQuAD 1.1 and SQuAD 2 datasets, progress in the field\nhas been particularly significant, with current state-of-the-art models now\nexhibiting near-human performance at both answering well-posed questions and\ndetecting questions which are unanswerable given a corresponding context. In\nthis work, we present Enhanced Question Answer Network (EQuANt), an MRC model\nwhich extends the successful QANet architecture of Yu et al. to cope with\nunanswerable questions. By training and evaluating EQuANt on SQuAD 2, we show\nthat it is indeed possible to extend QANet to the unanswerable domain. We\nachieve results which are close to 2 times better than our chosen baseline\nobtained by evaluating a lightweight version of the original QANet architecture\non SQuAD 2. In addition, we report that the performance of EQuANt on SQuAD 1.1\nafter being trained on SQuAD2 exceeds that of our lightweight QANet\narchitecture trained and evaluated on SQuAD 1.1, demonstrating the utility of\nmulti-task learning in the MRC context.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 08:13:45 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 21:03:37 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Aubet", "Fran\u00e7ois-Xavier", ""], ["Danks", "Dominic", ""], ["Zhu", "Yuchen", ""]]}, {"id": "1907.00710", "submitter": "Lizi Liao Ms", "authors": "Lizi Liao, Ryuichi Takanobu, Yunshan Ma, Xun Yang, Minlie Huang and\n  Tat-Seng Chua", "title": "Deep Conversational Recommender in Travel", "comments": "12 pages, 7 figures, submitted to TKDE. arXiv admin note: text\n  overlap with arXiv:1809.07070 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When traveling to a foreign country, we are often in dire need of an\nintelligent conversational agent to provide instant and informative responses\nto our various queries. However, to build such a travel agent is non-trivial.\nFirst of all, travel naturally involves several sub-tasks such as hotel\nreservation, restaurant recommendation and taxi booking etc, which invokes the\nneed for global topic control. Secondly, the agent should consider various\nconstraints like price or distance given by the user to recommend an\nappropriate venue. In this paper, we present a Deep Conversational Recommender\n(DCR) and apply to travel. It augments the sequence-to-sequence (seq2seq)\nmodels with a neural latent topic component to better guide response generation\nand make the training easier. To consider the various constraints for venue\nrecommendation, we leverage a graph convolutional network (GCN) based approach\nto capture the relationships between different venues and the match between\nvenue and dialog context. For response generation, we combine the topic-based\ncomponent with the idea of pointer networks, which allows us to effectively\nincorporate recommendation results. We perform extensive evaluation on a\nmulti-turn task-oriented dialog dataset in travel domain and the results show\nthat our method achieves superior performance as compared to a wide range of\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 04:39:26 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Liao", "Lizi", ""], ["Takanobu", "Ryuichi", ""], ["Ma", "Yunshan", ""], ["Yang", "Xun", ""], ["Huang", "Minlie", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1907.00720", "submitter": "Tianwen Jiang", "authors": "Tianwen Jiang, Tong Zhao, Bing Qin, Ting Liu, Nitesh V. Chawla, Meng\n  Jiang", "title": "Constructing Information-Lossless Biological Knowledge Graphs from\n  Conditional Statements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditions are essential in the statements of biological literature. Without\nthe conditions (e.g., environment, equipment) that were precisely specified,\nthe facts (e.g., observations) in the statements may no longer be valid. One\nbiological statement has one or multiple fact(s) and/or condition(s). Their\nsubject and object can be either a concept or a concept's attribute. Existing\ninformation extraction methods do not consider the role of condition in the\nbiological statement nor the role of attribute in the subject/object. In this\nwork, we design a new tag schema and propose a deep sequence tagging framework\nto structure conditional statement into fact and condition tuples from\nbiological text. Experiments demonstrate that our method yields a\ninformation-lossless structure of the literature.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:50:39 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Jiang", "Tianwen", ""], ["Zhao", "Tong", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""], ["Chawla", "Nitesh V.", ""], ["Jiang", "Meng", ""]]}, {"id": "1907.00735", "submitter": "Carlos Escolano", "authors": "Carlos Escolano, Marta R. Costa-Juss\\`a, Jos\\'e A. R. Fonollosa", "title": "From Bilingual to Multilingual Neural Machine Translation by Incremental\n  Training", "comments": "Accepted paper at ACL 2019 Student Research Workshop. arXiv admin\n  note: substantial text overlap with arXiv:1905.06831", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual Neural Machine Translation approaches are based on the use of\ntask-specific models and the addition of one more language can only be done by\nretraining the whole system. In this work, we propose a new training schedule\nthat allows the system to scale to more languages without modification of the\nprevious components based on joint training and language-independent\nencoder/decoder modules allowing for zero-shot translation. This work in\nprogress shows close results to the state-of-the-art in the WMT task.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:28:00 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 10:18:43 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Escolano", "Carlos", ""], ["Costa-Juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1907.00758", "submitter": "Aciel Eshky", "authors": "Aciel Eshky, Manuel Sam Ribeiro, Korin Richmond, Steve Renals", "title": "Synchronising audio and ultrasound by learning cross-modal embeddings", "comments": "5 pages, 1 figure, 4 tables; Interspeech 2019 with the following\n  edits: 1) Loss and accuracy upon convergence were accidentally reported from\n  an older model. Now updated with model described throughout the paper. All\n  other results remain unchanged. 2) Max true offset in the training data\n  corrected from 179ms to 1789ms. 3) Detectability \"boundary/range\" renamed to\n  detectability \"thresholds\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audiovisual synchronisation is the task of determining the time offset\nbetween speech audio and a video recording of the articulators. In child speech\ntherapy, audio and ultrasound videos of the tongue are captured using\ninstruments which rely on hardware to synchronise the two modalities at\nrecording time. Hardware synchronisation can fail in practice, and no mechanism\nexists to synchronise the signals post hoc. To address this problem, we employ\na two-stream neural network which exploits the correlation between the two\nmodalities to find the offset. We train our model on recordings from 69\nspeakers, and show that it correctly synchronises 82.9% of test utterances from\nunseen therapy sessions and unseen speakers, thus considerably reducing the\nnumber of utterances to be manually synchronised. An analysis of model\nperformance on the test utterances shows that directed phone articulations are\nmore difficult to automatically synchronise compared to utterances containing\nnatural variation in speech such as words, sentences, or conversations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 13:22:48 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 11:24:26 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Eshky", "Aciel", ""], ["Ribeiro", "Manuel Sam", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "1907.00810", "submitter": "Carlos Escolano", "authors": "Carlos Escolano, Marta R. Costa-juss\\`a, Elora Lacroux, Pere-Pau\n  V\\'azquez", "title": "Multilingual, Multi-scale and Multi-layer Visualization of Intermediate\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main alternatives nowadays to deal with sequences are Recurrent Neural\nNetworks (RNN), Convolutional Neural Networks (CNN) architectures and the\nTransformer. In this context, RNN's, CNN's and Transformer have most commonly\nbeen used as an encoder-decoder architecture with multiple layers in each\nmodule. Far beyond this, these architectures are the basis for the contextual\nword embeddings which are revolutionizing most natural language downstream\napplications. However, intermediate layer representations in sequence-based\narchitectures can be difficult to interpret. To make each layer representation\nwithin these architectures more accessible and meaningful, we introduce a\nweb-based tool that visualizes them both at the sentence and token level. We\npresent three use cases. The first analyses gender issues in contextual word\nembeddings. The second and third are showing multilingual intermediate\nrepresentations for sentences and tokens and the evolution of these\nintermediate representations along the multiple layers of the decoder and in\nthe context of multilingual machine translation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:15:16 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Escolano", "Carlos", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Lacroux", "Elora", ""], ["V\u00e1zquez", "Pere-Pau", ""]]}, {"id": "1907.00818", "submitter": "Manuel Sam Ribeiro", "authors": "Manuel Sam Ribeiro, Aciel Eshky, Korin Richmond and Steve Renals", "title": "Ultrasound tongue imaging for diarization and alignment of child speech\n  therapy sessions", "comments": "5 pages, 3 figures, Accepted for publication at Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the automatic processing of child speech therapy sessions\nusing ultrasound visual biofeedback, with a specific focus on complementing\nacoustic features with ultrasound images of the tongue for the tasks of speaker\ndiarization and time-alignment of target words. For speaker diarization, we\npropose an ultrasound-based time-domain signal which we call estimated tongue\nactivity. For word-alignment, we augment an acoustic model with low-dimensional\nrepresentations of ultrasound images of the tongue, learned by a convolutional\nneural network. We conduct our experiments using the Ultrasuite repository of\nultrasound and speech recordings for child speech therapy sessions. For both\ntasks, we observe that systems augmented with ultrasound data outperform\ncorresponding systems using only the audio signal.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:25:51 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 09:40:39 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Ribeiro", "Manuel Sam", ""], ["Eshky", "Aciel", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "1907.00820", "submitter": "Kexin Wang", "authors": "Kexin Wang, Yu Zhou, Shaonan Wang, Jiajun Zhang and Chengqing Zong", "title": "Understanding Memory Modules on Learning Simple Algorithms", "comments": "Accepted at the XAI Workshop in IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that memory modules are crucial for the generalization\nability of neural networks on learning simple algorithms. However, we still\nhave little understanding of the working mechanism of memory modules. To\nalleviate this problem, we apply a two-step analysis pipeline consisting of\nfirst inferring hypothesis about what strategy the model has learned according\nto visualization and then verify it by a novel proposed qualitative analysis\nmethod based on dimension reduction. Using this method, we have analyzed two\npopular memory-augmented neural networks, neural Turing machine and\nstack-augmented neural network on two simple algorithm tasks including\nreversing a random sequence and evaluation of arithmetic expressions. Results\nhave shown that on the former task both models can learn to generalize and on\nthe latter task only the stack-augmented model can do so. We show that\ndifferent strategies are learned by the models, in which specific categories of\ninput are monitored and different policies are made based on that to change the\nmemory.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:27:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Kexin", ""], ["Zhou", "Yu", ""], ["Wang", "Shaonan", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1907.00835", "submitter": "Aciel Eshky", "authors": "Aciel Eshky, Manuel Sam Ribeiro, Joanne Cleland, Korin Richmond, Zoe\n  Roxburgh, James Scobbie, Alan Wrench", "title": "UltraSuite: A Repository of Ultrasound and Acoustic Data from Child\n  Speech Therapy Sessions", "comments": "5 pages, 1 figure, 3 tables; accepted to Interspeech 2018: 19th\n  Annual Conference of the International Speech Communication Association\n  (ISCA)", "journal-ref": null, "doi": "10.21437/Interspeech.2018-1736", "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce UltraSuite, a curated repository of ultrasound and acoustic\ndata, collected from recordings of child speech therapy sessions. This release\nincludes three data collections, one from typically developing children and two\nfrom children with speech sound disorders. In addition, it includes a set of\nannotations, some manual and some automatically produced, and software tools to\nprocess, transform and visualise the data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:54:31 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Eshky", "Aciel", ""], ["Ribeiro", "Manuel Sam", ""], ["Cleland", "Joanne", ""], ["Richmond", "Korin", ""], ["Roxburgh", "Zoe", ""], ["Scobbie", "James", ""], ["Wrench", "Alan", ""]]}, {"id": "1907.00852", "submitter": "Eugene Kharitonov", "authors": "Eugene Kharitonov and Rahma Chaabouni and Diane Bouchacourt and Marco\n  Baroni", "title": "EGG: a toolkit for research on Emergence of lanGuage in Games", "comments": "EMNLP 2019 Demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is renewed interest in simulating language emergence among deep neural\nagents that communicate to jointly solve a task, spurred by the practical aim\nto develop language-enabled interactive AIs, as well as by theoretical\nquestions about the evolution of human language. However, optimizing deep\narchitectures connected by a discrete communication channel (such as that in\nwhich language emerges) is technically challenging. We introduce EGG, a toolkit\nthat greatly simplifies the implementation of emergent-language communication\ngames. EGG's modular design provides a set of building blocks that the user can\ncombine to create new games, easily navigating the optimization and\narchitecture space. We hope that the tool will lower the technical barrier, and\nencourage researchers from various backgrounds to do original work in this\nexciting area.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:20:01 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 11:13:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Kharitonov", "Eugene", ""], ["Chaabouni", "Rahma", ""], ["Bouchacourt", "Diane", ""], ["Baroni", "Marco", ""]]}, {"id": "1907.00854", "submitter": "Daniel Whitenack", "authors": "Shirish Hirekodi, Seban Sunny, Leonard Topno, Alwin Daniel, Daniel\n  Whitenack, Reuben Skewes, Stuart Cranney", "title": "Katecheo: A Portable and Modular System for Multi-Topic Question\n  Answering", "comments": "ACL 2020 system demo submission, 7 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a modular system that can be deployed on any Kubernetes cluster\nfor question answering via REST API. This system, called Katecheo, includes\nthree configurable modules that collectively enable identification of\nquestions, classification of those questions into topics, document search, and\nreading comprehension. We demonstrate the system using publicly available\nknowledge base articles extracted from Stack Exchange sites. However, users can\nextend the system to any number of topics, or domains, without the need to\nmodify any of the model serving code or train their own models. All components\nof the system are open source and available under a permissive Apache 2\nLicense.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:20:10 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 19:01:40 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Hirekodi", "Shirish", ""], ["Sunny", "Seban", ""], ["Topno", "Leonard", ""], ["Daniel", "Alwin", ""], ["Whitenack", "Daniel", ""], ["Skewes", "Reuben", ""], ["Cranney", "Stuart", ""]]}, {"id": "1907.00883", "submitter": "Rahul Goel", "authors": "Rahul Goel, Shachi Paul, Dilek Hakkani-T\\\"ur", "title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State\n  Tracking", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works on end-to-end trainable neural network based approaches have\ndemonstrated state-of-the-art results on dialogue state tracking. The best\nperforming approaches estimate a probability distribution over all possible\nslot values. However, these approaches do not scale for large value sets\ncommonly present in real-life applications and are not ideal for tracking slot\nvalues that were not observed in the training set. To tackle these issues,\ncandidate-generation-based approaches have been proposed. These approaches\nestimate a set of values that are possible at each turn based on the\nconversation history and/or language understanding outputs, and hence enable\nstate tracking over unseen values and large value sets however, they fall short\nin terms of performance in comparison to the first group. In this work, we\nanalyze the performance of these two alternative dialogue state tracking\nmethods, and present a hybrid approach (HyST) which learns the appropriate\nmethod for each slot type. To demonstrate the effectiveness of HyST on a\nrich-set of slot types, we experiment with the recently released MultiWOZ-2.0\nmulti-domain, task-oriented dialogue-dataset. Our experiments show that HyST\nscales to multi-domain applications. Our best performing model results in a\nrelative improvement of 24% and 10% over the previous SOTA and our best\nbaseline respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:55:36 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Goel", "Rahul", ""], ["Paul", "Shachi", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "1907.00900", "submitter": "Antonio Toral", "authors": "Antonio Toral", "title": "Post-editese: an Exacerbated Translationese", "comments": "Accepted at the 17th Machine Translation Summit. v2: two references\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-editing (PE) machine translation (MT) is widely used for dissemination\nbecause it leads to higher productivity than human translation from scratch\n(HT). In addition, PE translations are found to be of equal or better quality\nthan HTs. However, most such studies measure quality solely as the number of\nerrors. We conduct a set of computational analyses in which we compare PE\nagainst HT on three different datasets that cover five translation directions\nwith measures that address different translation universals and laws of\ntranslation: simplification, normalisation and interference. We find out that\nPEs are simpler and more normalised and have a higher degree of interference\nfrom the source language than HTs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 16:16:39 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 12:37:40 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Toral", "Antonio", ""]]}, {"id": "1907.00937", "submitter": "Priyanka Nigam", "authors": "Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian\n  (Allen) Ding, Ankit Shingavi, Choon Hui Teo, Hao Gu, Bing Yin", "title": "Semantic Product Search", "comments": "10 pages, 7 figures, KDD 2019 (Applied Data Science Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of semantic matching in product search, that is, given a\ncustomer query, retrieve all semantically related products from the catalog.\nPure lexical matching via an inverted index falls short in this respect due to\nseveral factors: a) lack of understanding of hypernyms, synonyms, and antonyms,\nb) fragility to morphological variants (e.g. \"woman\" vs. \"women\"), and c)\nsensitivity to spelling errors. To address these issues, we train a deep\nlearning model for semantic matching using customer behavior data. Much of the\nrecent work on large-scale semantic search using deep learning focuses on\nranking for web search. In contrast, semantic matching for product search\npresents several novel challenges, which we elucidate in this paper. We address\nthese challenges by a) developing a new loss function that has an inbuilt\nthreshold to differentiate between random negative examples, impressed but not\npurchased examples, and positive examples (purchased items), b) using average\npooling in conjunction with n-grams to capture short-range linguistic patterns,\nc) using hashing to handle out of vocabulary tokens, and d) using a model\nparallel training architecture to scale across 8 GPUs. We present compelling\noffline results that demonstrate at least 4.7% improvement in Recall@100 and\n14.5% improvement in mean average precision (MAP) over baseline\nstate-of-the-art semantic search methods using the same tokenization method.\nMoreover, we present results and discuss learnings from online A/B tests which\ndemonstrate the efficacy of our method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:20:02 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Nigam", "Priyanka", "", "Allen"], ["Song", "Yiwei", "", "Allen"], ["Mohan", "Vijai", "", "Allen"], ["Lakshman", "Vihan", "", "Allen"], ["Weitian", "", "", "Allen"], ["Ding", "", ""], ["Shingavi", "Ankit", ""], ["Teo", "Choon Hui", ""], ["Gu", "Hao", ""], ["Yin", "Bing", ""]]}, {"id": "1907.00962", "submitter": "Titipat Achakulvisut", "authors": "Titipat Achakulvisut, Chandra Bhagavatula, Daniel Acuna, Konrad\n  Kording", "title": "Claim Extraction in Biomedical Publications using Deep Discourse Model\n  and Transfer Learning", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Claims are a fundamental unit of scientific discourse. The exponential growth\nin the number of scientific publications makes automatic claim extraction an\nimportant problem for researchers who are overwhelmed by this information\noverload. Such an automated claim extraction system is useful for both manual\nand programmatic exploration of scientific knowledge. In this paper, we\nintroduce a new dataset of 1,500 scientific abstracts from the biomedical\ndomain with expert annotations for each sentence indicating whether the\nsentence presents a scientific claim. We introduce a new model for claim\nextraction and compare it to several baseline models including rule-based and\ndeep learning techniques. Moreover, we show that using a transfer learning\napproach with a fine-tuning step allows us to improve performance from a large\ndiscourse-annotated dataset. Our final model increases F1-score by over 14\npercent points compared to a baseline model without transfer learning. We\nrelease a publicly accessible tool for discourse and claims prediction along\nwith an annotation tool. We discuss further applications beyond biomedical\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:56:41 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 22:34:08 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Achakulvisut", "Titipat", ""], ["Bhagavatula", "Chandra", ""], ["Acuna", "Daniel", ""], ["Kording", "Konrad", ""]]}, {"id": "1907.01011", "submitter": "Zhun Liu", "authors": "Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan\n  Salakhutdinov, Louis-Philippe Morency", "title": "Learning Representations from Imperfect Time Series Data via Tensor Rank\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increased interest in multimodal language processing\nincluding multimodal dialog, question answering, sentiment analysis, and speech\nrecognition. However, naturally occurring multimodal data is often imperfect as\na result of imperfect modalities, missing entries or noise corruption. To\naddress these concerns, we present a regularization method based on tensor rank\nminimization. Our method is based on the observation that high-dimensional\nmultimodal time series data often exhibit correlations across time and\nmodalities which leads to low-rank tensor representations. However, the\npresence of noise or incomplete values breaks these correlations and results in\ntensor representations of higher rank. We design a model to learn such tensor\nrepresentations and effectively regularize their rank. Experiments on\nmultimodal language data show that our model achieves good results across\nvarious levels of imperfection.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:40:52 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Liang", "Paul Pu", ""], ["Liu", "Zhun", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Zhao", "Qibin", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1907.01041", "submitter": "Lakshay Sharma", "authors": "Lakshay Sharma, Laura Graesser, Nikita Nangia, Utku Evci", "title": "Natural Language Understanding with the Quora Question Pairs Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the task Natural Language Understanding (NLU) by looking\nat duplicate question detection in the Quora dataset. We conducted extensive\nexploration of the dataset and used various machine learning models, including\nlinear and tree-based models. Our final finding was that a simple Continuous\nBag of Words neural network model had the best performance, outdoing more\ncomplicated recurrent and attention based models. We also conducted error\nanalysis and found some subjectivity in the labeling of the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:48:34 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Sharma", "Lakshay", ""], ["Graesser", "Laura", ""], ["Nangia", "Nikita", ""], ["Evci", "Utku", ""]]}, {"id": "1907.01055", "submitter": "Zixu Wang", "authors": "Zixu Wang, Julia Ive, Sumithra Velupillai, Lucia Specia", "title": "Is artificial data useful for biomedical Natural Language Processing\n  algorithms?", "comments": "BioNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle to the development of Natural Language Processing (NLP)\nmethods in the biomedical domain is data accessibility. This problem can be\naddressed by generating medical data artificially. Most previous studies have\nfocused on the generation of short clinical text, and evaluation of the data\nutility has been limited. We propose a generic methodology to guide the\ngeneration of clinical text with key phrases. We use the artificial data as\nadditional training data in two key biomedical NLP tasks: text classification\nand temporal relation extraction. We show that artificially generated training\ndata used in conjunction with real training data can lead to performance boosts\nfor data-greedy neural network algorithms. We also demonstrate the usefulness\nof the generated data for NLP setups where it fully replaces real training\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:17:59 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 19:08:19 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wang", "Zixu", ""], ["Ive", "Julia", ""], ["Velupillai", "Sumithra", ""], ["Specia", "Lucia", ""]]}, {"id": "1907.01098", "submitter": "Piyush Papreja", "authors": "Piyush Papreja and Hemanth Venkateswara and Sethuraman Panchanathan", "title": "Representation, Exploration and Recommendation of Music Playlists", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-43887-6_50", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Playlists have become a significant part of our listening experience because\nof the digital cloud-based services such as Spotify, Pandora, Apple Music.\nOwing to the meteoric rise in the usage of playlists, recommending playlists is\ncrucial to music services today. Although there has been a lot of work done in\nplaylist prediction, the area of playlist representation hasn't received that\nlevel of attention. Over the last few years, sequence-to-sequence models,\nespecially in the field of natural language processing, have shown the\neffectiveness of learned embeddings in capturing the semantic characteristics\nof sequences. We can apply similar concepts to music to learn fixed length\nrepresentations for playlists and use those representations for downstream\ntasks such as playlist discovery, browsing, and recommendation. In this work,\nwe formulate the problem of learning a fixed-length playlist representation in\nan unsupervised manner, using Sequence-to-sequence (Seq2seq) models,\ninterpreting playlists as sentences and songs as words. We compare our model\nwith two other encoding architectures for baseline comparison. We evaluate our\nwork using the suite of tasks commonly used for assessing sentence embeddings,\nalong with a few additional tasks pertaining to music, and a recommendation\ntask to study the traits captured by the playlist embeddings and their\neffectiveness for the purpose of music recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 23:20:45 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Papreja", "Piyush", ""], ["Venkateswara", "Hemanth", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1907.01108", "submitter": "Chaitanya Ahuja", "authors": "Chaitanya Ahuja and Louis-Philippe Morency", "title": "Language2Pose: Natural Language Grounded Pose Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating animations from natural language sentences finds its applications\nin a a number of domains such as movie script visualization, virtual human\nanimation and, robot motion planning. These sentences can describe different\nkinds of actions, speeds and direction of these actions, and possibly a target\ndestination. The core modeling challenge in this language-to-pose application\nis how to map linguistic concepts to motion animations.\n  In this paper, we address this multimodal problem by introducing a neural\narchitecture called Joint Language to Pose (or JL2P), which learns a joint\nembedding of language and pose. This joint embedding space is learned\nend-to-end using a curriculum learning approach which emphasizes shorter and\neasier sequences first before moving to longer and harder ones. We evaluate our\nproposed model on a publicly available corpus of 3D pose data and\nhuman-annotated sentences. Both objective metrics and human judgment evaluation\nconfirm that our proposed approach is able to generate more accurate animations\nand are deemed visually more representative by humans than other data driven\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 00:38:44 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 19:06:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ahuja", "Chaitanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1907.01115", "submitter": "Nick Walker", "authors": "Nick Walker, Yu-Tang Peng, Maya Cakmak", "title": "Neural Semantic Parsing with Anonymization for Command Understanding in\n  General-Purpose Service Robots", "comments": "To appear in RoboCup 2019: Robot World Cup XXIII, Springer", "journal-ref": null, "doi": "10.1007/978-3-030-35699-6_26", "report-no": null, "categories": "cs.RO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service robots are envisioned to undertake a wide range of tasks at the\nrequest of users. Semantic parsing is one way to convert natural language\ncommands given to these robots into executable representations. Methods for\ncreating semantic parsers, however, rely either on large amounts of data or on\nengineered lexical features and parsing rules, which has limited their\napplication in robotics. To address this challenge, we propose an approach that\nleverages neural semantic parsing methods in combination with contextual word\nembeddings to enable the training of a semantic parser with little data and\nwithout domain specific parser engineering. Key to our approach is the use of\nan anonymized target representation which is more easily learned by the parser.\nIn most cases, this simplified representation can trivially be transformed into\nan executable format, and in others the parse can be completed through further\ninteraction with the user. We evaluate this approach in the context of the\nRoboCup@Home General Purpose Service Robot task, where we have collected a\ncorpus of paraphrased versions of commands from the standardized command\ngenerator. Our results show that neural semantic parsers can predict the\nlogical form of unseen commands with 89% accuracy. We release our data and the\ndetails of our models to encourage further development from the RoboCup and\nservice robotics communities.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 01:09:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Walker", "Nick", ""], ["Peng", "Yu-Tang", ""], ["Cakmak", "Maya", ""]]}, {"id": "1907.01118", "submitter": "Shanshan Liu", "authors": "Shanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, Weiming Zhang", "title": "Neural Machine Reading Comprehension: Methods and Trends", "comments": "46 pages", "journal-ref": "J. Applied Sciences, 2019, 9(18): 3698", "doi": "10.1016/j.apsusc.2018.11.214", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension (MRC), which requires a machine to answer\nquestions based on a given context, has attracted increasing attention with the\nincorporation of various deep-learning techniques over the past few years.\nAlthough research on MRC based on deep learning is flourishing, there remains a\nlack of a comprehensive survey summarizing existing approaches and recent\ntrends, which motivated the work presented in this article. Specifically, we\ngive a thorough review of this research field, covering different aspects\nincluding (1) typical MRC tasks: their definitions, differences, and\nrepresentative datasets; (2) the general architecture of neural MRC: the main\nmodules and prevalent approaches to each; and (3) new trends: some emerging\nareas in neural MRC as well as the corresponding challenges. Finally,\nconsidering what has been achieved so far, the survey also envisages what the\nfuture may hold by discussing the open issues left to be addressed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 01:14:13 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 03:51:57 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 14:28:17 GMT"}, {"version": "v4", "created": "Mon, 15 Jul 2019 14:10:10 GMT"}, {"version": "v5", "created": "Wed, 30 Oct 2019 03:35:15 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Liu", "Shanshan", ""], ["Zhang", "Xin", ""], ["Zhang", "Sheng", ""], ["Wang", "Hui", ""], ["Zhang", "Weiming", ""]]}, {"id": "1907.01160", "submitter": "Jonathan Le Roux", "authors": "Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu,\n  Emmett McQuinn, Dwight Crow, Ethan Manilow, Jonathan Le Roux", "title": "WHAM!: Extending Speech Separation to Noisy Environments", "comments": "Accepted for publication at Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in separating the speech signals from multiple overlapping\nspeakers using a single audio channel has brought us closer to solving the\ncocktail party problem. However, most studies in this area use a constrained\nproblem setup, comparing performance when speakers overlap almost completely,\nat artificially low sampling rates, and with no external background noise. In\nthis paper, we strive to move the field towards more realistic and challenging\nscenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!)\ndataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined\nwith real ambient noise samples. The samples were collected in coffee shops,\nrestaurants, and bars in the San Francisco Bay Area, and are made publicly\navailable. We benchmark various speech separation architectures and objective\nfunctions to evaluate their robustness to noise. While separation performance\ndecreases as a result of noise, we still observe substantial gains relative to\nthe noisy signals for most approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 04:27:55 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Wichern", "Gordon", ""], ["Antognini", "Joe", ""], ["Flynn", "Michael", ""], ["Zhu", "Licheng Richard", ""], ["McQuinn", "Emmett", ""], ["Crow", "Dwight", ""], ["Manilow", "Ethan", ""], ["Roux", "Jonathan Le", ""]]}, {"id": "1907.01166", "submitter": "Hung Le", "authors": "Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C.H. Hoi", "title": "Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue\n  Systems", "comments": "Accepted at ACL 2019 (Long Paper)", "journal-ref": "Association for Computational Linguistics (2019) 5612-5623", "doi": "10.18653/v1/P19-1564", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is\nconducted based on visual and audio aspects of a given video, is significantly\nmore challenging than traditional image or text-grounded dialogue systems\nbecause (1) feature space of videos span across multiple picture frames, making\nit difficult to obtain semantic information; and (2) a dialogue agent must\nperceive and process information from different modalities (audio, video,\ncaption, etc.) to obtain a comprehensive understanding. Most existing work is\nbased on RNNs and sequence-to-sequence architectures, which are not very\neffective for capturing complex long-term dependencies (like in videos). To\novercome this, we propose Multimodal Transformer Networks (MTN) to encode\nvideos and incorporate information from different modalities. We also propose\nquery-aware attention through an auto-encoder to extract query-aware features\nfrom non-text modalities. We develop a training procedure to simulate\ntoken-level decoding to improve the quality of generated responses during\ninference. We get state of the art performance on Dialogue System Technology\nChallenge 7 (DSTC7). Our model also generalizes to another multimodal\nvisual-grounded dialogue task, and obtains promising performance. We\nimplemented our models using PyTorch and the code is released at\nhttps://github.com/henryhungle/MTN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 04:54:17 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Le", "Hung", ""], ["Sahoo", "Doyen", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1907.01256", "submitter": "Yo Joong Choe", "authors": "Yo Joong Choe, Jiyeon Ham, Kyubyong Park and Yeoil Yoon", "title": "A Neural Grammatical Error Correction System Built On Better\n  Pre-training and Sequential Transfer Learning", "comments": "Accepted to ACL 2019 Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical error correction can be viewed as a low-resource\nsequence-to-sequence task, because publicly available parallel corpora are\nlimited. To tackle this challenge, we first generate erroneous versions of\nlarge unannotated corpora using a realistic noising function. The resulting\nparallel corpora are subsequently used to pre-train Transformer models. Then,\nby sequentially applying transfer learning, we adapt these models to the domain\nand style of the test set. Combined with a context-aware neural spellchecker,\nour system achieves competitive results in both restricted and low resource\ntracks in ACL 2019 BEA Shared Task. We release all of our code and materials\nfor reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:33:36 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Choe", "Yo Joong", ""], ["Ham", "Jiyeon", ""], ["Park", "Kyubyong", ""], ["Yoon", "Yeoil", ""]]}, {"id": "1907.01272", "submitter": "Saadia Gabriel", "authors": "Saadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys,\n  Kyle Lo, Asli Celikyilmaz, Yejin Choi", "title": "Discourse Understanding and Factual Consistency in Abstractive\n  Summarization", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for abstractive summarization with factual\nconsistency and distinct modeling of the narrative flow in an output summary.\nOur work addresses current limitations of models for abstractive summarization\nthat often hallucinate information or generate summaries with coherence issues.\n  To generate abstractive summaries with factual consistency and narrative\nflow, we propose Cooperative Generator -- Discriminator Networks (Co-opNet), a\nnovel transformer-based framework where a generator works with a discriminator\narchitecture to compose coherent long-form summaries. We explore four different\ndiscriminator objectives which each capture a different aspect of coherence,\nincluding whether salient spans of generated abstracts are hallucinated or\nappear in the input context, and the likelihood of sentence adjacency in\ngenerated abstracts. We measure the ability of Co-opNet to learn these\nobjectives with arXiv scientific papers, using the abstracts as a proxy for\ngold long-form scientific article summaries. Empirical results from automatic\nand human evaluations demonstrate that Co-opNet learns to summarize with\nconsiderably improved global coherence compared to competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:06:33 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 01:54:41 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Gabriel", "Saadia", ""], ["Bosselut", "Antoine", ""], ["Da", "Jeff", ""], ["Holtzman", "Ari", ""], ["Buys", "Jan", ""], ["Lo", "Kyle", ""], ["Celikyilmaz", "Asli", ""], ["Choi", "Yejin", ""]]}, {"id": "1907.01279", "submitter": "Raj Nath Patel", "authors": "Rohit Gupta, Patrik Lambert, Raj Nath Patel, and John Tinsley", "title": "Improving Robustness in Real-World Neural Machine Translation Engines", "comments": "6 Pages, Accepted in Machine Translation Summit 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a commercial provider of machine translation, we are constantly training\nengines for a variety of uses, languages, and content types. In each case,\nthere can be many variables, such as the amount of training data available, and\nthe quality requirements of the end user. These variables can have an impact on\nthe robustness of Neural MT engines. On the whole, Neural MT cures many ills of\nother MT paradigms, but at the same time, it has introduced a new set of\nchallenges to address. In this paper, we describe some of the specific issues\nwith practical NMT and the approaches we take to improve model robustness in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:11:23 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Gupta", "Rohit", ""], ["Lambert", "Patrik", ""], ["Patel", "Raj Nath", ""], ["Tinsley", "John", ""]]}, {"id": "1907.01302", "submitter": "Mortaza Doulaty", "authors": "Mortaza (Morrie) Doulaty and Thomas Hain", "title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic\n  Speech Recognition", "comments": null, "journal-ref": "Proc. of Interspeech (2019), Graz, Austria", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting in-domain data from a large pool of diverse and out-of-domain data\nis a non-trivial problem. In most cases simply using all of the available data\nwill lead to sub-optimal and in some cases even worse performance compared to\ncarefully selecting a matching set. This is true even for data-inefficient\nneural models. Acoustic Latent Dirichlet Allocation (aLDA) is shown to be\nuseful in a variety of speech technology related tasks, including domain\nadaptation of acoustic models for automatic speech recognition and entity\nlabeling for information retrieval. In this paper we propose to use aLDA as a\ndata similarity criterion in a data selection framework. Given a large pool of\nout-of-domain and potentially mismatched data, the task is to select the\nbest-matching training data to a set of representative utterances sampled from\na target domain. Our target data consists of around 32 hours of meeting data\n(both far-field and close-talk) and the pool contains 2k hours of meeting,\ntalks, voice search, dictation, command-and-control, audio books, lectures,\ngeneric media and telephony speech data. The proposed technique for training\ndata selection, significantly outperforms random selection, posterior-based\nselection as well as using all of the available data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:33:52 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Mortaza", "", "", "Morrie"], ["Doulaty", "", ""], ["Hain", "Thomas", ""]]}, {"id": "1907.01304", "submitter": "Anders Edelbo Lillie", "authors": "Anders Edelbo Lillie and Emil Refsgaard Middelboe", "title": "Danish Stance Classification and Rumour Resolution", "comments": "97(92) pages, 22 figures, 39 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet is rife with flourishing rumours that spread through microblogs\nand social media. Recent work has shown that analysing the stance of the crowd\ntowards a rumour is a good indicator for its veracity. One state-of-the-art\nsystem uses an LSTM neural network to automatically classify stance for posts\non Twitter by considering the context of a whole branch, while another, more\nsimple Decision Tree classifier, performs at least as well by performing\ncareful feature engineering. One approach to predict the veracity of a rumour\nis to use stance as the only feature for a Hidden Markov Model (HMM). This\nthesis generates a stance-annotated Reddit dataset for the Danish language, and\nimplements various models for stance classification. Out of these, a Linear\nSupport Vector Machine provides the best results with an accuracy of 0.76 and\nmacro F1 score of 0.42. Furthermore, experiments show that stance labels can be\nused across languages and platforms with a HMM to predict the veracity of\nrumours, achieving an accuracy of 0.82 and F1 score of 0.67. Even higher scores\nare achieved by relying only on the Danish dataset. In this case veracity\nprediction scores an accuracy of 0.83 and an F1 of 0.68. Finally, when using\nautomatic stance labels for the HMM, only a small drop in performance is\nobserved, showing that the implemented system can have practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:44:31 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Lillie", "Anders Edelbo", ""], ["Middelboe", "Emil Refsgaard", ""]]}, {"id": "1907.01339", "submitter": "David Vilares", "authors": "Michalina Strzyz, David Vilares, Carlos G\\'omez-Rodr\\'iguez", "title": "Sequence Labeling Parsing by Learning Across Representations", "comments": "Proc. of the 57th Annual Meeting of the Association for Computational\n  Linguistics (ACL 2019). Revised version after fixing evaluation bug", "journal-ref": null, "doi": "10.18653/v1/P19-1531", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use parsing as sequence labeling as a common framework to learn across\nconstituency and dependency syntactic abstractions. To do so, we cast the\nproblem as multitask learning (MTL). First, we show that adding a parsing\nparadigm as an auxiliary loss consistently improves the performance on the\nother paradigm. Secondly, we explore an MTL sequence labeling model that parses\nboth representations, at almost no cost in terms of performance and speed. The\nresults across the board show that on average MTL models with auxiliary losses\nfor constituency parsing outperform single-task ones by 1.14 F1 points, and for\ndependency parsing by 0.62 UAS points.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:13:13 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 14:57:14 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 08:02:44 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Strzyz", "Michalina", ""], ["Vilares", "David", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1907.01356", "submitter": "Shuangjia Zheng", "authors": "Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, Yuedong Yang", "title": "Predicting Retrosynthetic Reaction using Self-Corrected Transformer\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesis planning is the process of recursively decomposing target molecules\ninto available precursors. Computer-aided retrosynthesis can potentially assist\nchemists in designing synthetic routes, but at present it is cumbersome and\nprovides results of dissatisfactory quality. In this study, we develop a\ntemplate-free self-corrected retrosynthesis predictor (SCROP) to perform a\nretrosynthesis prediction task trained by using the Transformer neural network\narchitecture. In the method, the retrosynthesis planning is converted as a\nmachine translation problem between molecular linear notations of reactants and\nthe products. Coupled with a neural network-based syntax corrector, our method\nachieves an accuracy of 59.0% on a standard benchmark dataset, which increases\n>21% over other deep learning methods, and >6% over template-based methods.\nMore importantly, our method shows an accuracy 1.7 times higher than other\nstate-of-the-art methods for compounds not appearing in the training set.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:35:37 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 03:48:46 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zheng", "Shuangjia", ""], ["Rao", "Jiahua", ""], ["Zhang", "Zhongyue", ""], ["Xu", "Jun", ""], ["Yang", "Yuedong", ""]]}, {"id": "1907.01369", "submitter": "Gabriel Murray", "authors": "Uliyana Kubasova and Gabriel Murray and McKenzie Braley", "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance", "comments": "Accepted to INTERSPEECH 2019 (Graz, Austria)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyzes the efficacy of verbal and nonverbal features of group\nconversation for the task of automatic prediction of group task performance. We\ndescribe a new publicly available survival task dataset that was collected and\nannotated to facilitate this prediction task. In these experiments, the new\ndataset is merged with an existing survival task dataset, allowing us to\ncompare feature sets on a much larger amount of data than has been used in\nrecent related work. This work is also distinct from related research on social\nsignal processing (SSP) in that we compare verbal and nonverbal features,\nwhereas SSP is almost exclusively concerned with nonverbal aspects of social\ninteraction. A key finding is that nonverbal features from the speech signal\nare extremely effective for this task, even on their own. However, the most\neffective individual features are verbal features, and we highlight the most\nimportant ones.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 17:07:03 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 20:53:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Kubasova", "Uliyana", ""], ["Murray", "Gabriel", ""], ["Braley", "McKenzie", ""]]}, {"id": "1907.01409", "submitter": "Wilfried Michel", "authors": "Wilfried Michel, Ralf Schl\\\"uter, Hermann Ney", "title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative\n  Training Criteria for LVCSR", "comments": "Submitted to Interspeech 2019", "journal-ref": "Interspeech 2019, 20th Annual Conference of the International\n  Speech Communication Association, Graz, Austria, 15-19 September 2019, pp.\n  1601--1605", "doi": "10.21437/Interspeech.2019-2254", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence discriminative training criteria have long been a standard tool in\nautomatic speech recognition for improving the performance of acoustic models\nover their maximum likelihood / cross entropy trained counterparts. While\npreviously a lattice approximation of the search space has been necessary to\nreduce computational complexity, recently proposed methods use other\napproximations to dispense of the need for the computationally expensive step\nof separate lattice creation.\n  In this work we present a memory efficient implementation of the\nforward-backward computation that allows us to use uni-gram word-level language\nmodels in the denominator calculation while still doing a full summation on\nGPU. This allows for a direct comparison of lattice-based and lattice-free\nsequence discriminative training criteria such as MMI and sMBR, both using the\nsame language model during training.\n  We compared performance, speed of convergence, and stability on large\nvocabulary continuous speech recognition tasks like Switchboard and Quaero. We\nfound that silence modeling seriously impacts the performance in the\nlattice-free case and needs special treatment. In our experiments lattice-free\nMMI comes on par with its lattice-based counterpart. Lattice-based sMBR still\noutperforms all lattice-free training criteria.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:16:04 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Michel", "Wilfried", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1907.01413", "submitter": "Sam Ribeiro", "authors": "Manuel Sam Ribeiro, Aciel Eshky, Korin Richmond and Steve Renals", "title": "Speaker-independent classification of phonetic segments from raw\n  ultrasound in child speech", "comments": "5 pages, 4 figures, published in ICASSP2019 (IEEE International\n  Conference on Acoustics, Speech and Signal Processing, 2019)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683564", "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound tongue imaging (UTI) provides a convenient way to visualize the\nvocal tract during speech production. UTI is increasingly being used for speech\ntherapy, making it important to develop automatic methods to assist various\ntime-consuming manual tasks currently performed by speech therapists. A key\nchallenge is to generalize the automatic processing of ultrasound tongue images\nto previously unseen speakers. In this work, we investigate the classification\nof phonetic segments (tongue shapes) from raw ultrasound recordings under\nseveral training scenarios: speaker-dependent, multi-speaker,\nspeaker-independent, and speaker-adapted. We observe that models underperform\nwhen applied to data from speakers not seen at training time. However, when\nprovided with minimal additional speaker information, such as the mean\nultrasound frame, the models generalize better to unseen speakers.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:04:13 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Ribeiro", "Manuel Sam", ""], ["Eshky", "Aciel", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "1907.01417", "submitter": "Julien Fauqueur", "authors": "Julien Fauqueur, Ashok Thillaisundaram, Theodosia Togia", "title": "Constructing large scale biomedical knowledge bases from scratch with\n  rapid annotation of interpretable patterns", "comments": "BioNLP 2019: 18th ACL Workshop on Biomedical Natural Language\n  Processing, fixed author's name typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge base construction is crucial for summarising, understanding and\ninferring relationships between biomedical entities. However, for many\npractical applications such as drug discovery, the scarcity of relevant facts\n(e.g. gene X is therapeutic target for disease Y) severely limits a domain\nexpert's ability to create a usable knowledge base, either directly or by\ntraining a relation extraction model.\n  In this paper, we present a simple and effective method of extracting new\nfacts with a pre-specified binary relationship type from the biomedical\nliterature, without requiring any training data or hand-crafted rules. Our\nsystem discovers, ranks and presents the most salient patterns to domain\nexperts in an interpretable form. By marking patterns as compatible with the\ndesired relationship type, experts indirectly batch-annotate candidate pairs\nwhose relationship is expressed with such patterns in the literature. Even with\na complete absence of seed data, experts are able to discover thousands of\nhigh-quality pairs with the desired relationship within minutes. When a small\nnumber of relevant pairs do exist - even when their relationship is more\ngeneral (e.g. gene X is biologically associated with disease Y) than the\nrelationship of interest - our system leverages them in order to i) learn a\nbetter ranking of the patterns to be annotated or ii) generate weakly labelled\npairs in a fully automated manner.\n  We evaluate our method both intrinsically and via a downstream knowledge base\ncompletion task, and show that it is an effective way of constructing knowledge\nbases when few or no relevant facts are already available.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 14:53:30 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 09:52:05 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Fauqueur", "Julien", ""], ["Thillaisundaram", "Ashok", ""], ["Togia", "Theodosia", ""]]}, {"id": "1907.01468", "submitter": "Dong Nguyen", "authors": "Dong Nguyen, Maria Liakata, Simon DeDeo, Jacob Eisenstein, David\n  Mimno, Rebekah Tromble, Jane Winters", "title": "How we do things with words: Analyzing text as social and cultural data", "comments": null, "journal-ref": "Front. Artif. Intell. 3:62 (2020)", "doi": "10.3389/frai.2020.00062", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we describe our experiences with computational text analysis.\nWe hope to achieve three primary goals. First, we aim to shed light on thorny\nissues not always at the forefront of discussions about computational text\nanalysis methods. Second, we hope to provide a set of best practices for\nworking with thick social and cultural concepts. Our guidance is based on our\nown experiences and is therefore inherently imperfect. Still, given our\ndiversity of disciplinary backgrounds and research practices, we hope to\ncapture a range of ideas and identify commonalities that will resonate for\nmany. And this leads to our final goal: to help promote interdisciplinary\ncollaborations. Interdisciplinary insights and partnerships are essential for\nrealizing the full potential of any computational text analysis that involves\nsocial and cultural concepts, and the more we are able to bridge these divides,\nthe more fruitful we believe our work will be.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:55:42 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Nguyen", "Dong", ""], ["Liakata", "Maria", ""], ["DeDeo", "Simon", ""], ["Eisenstein", "Jacob", ""], ["Mimno", "David", ""], ["Tromble", "Rebekah", ""], ["Winters", "Jane", ""]]}, {"id": "1907.01470", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou,\n  Armand Joulin", "title": "Augmenting Self-attention with Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer networks have lead to important progress in language modeling and\nmachine translation. These models include two consecutive modules, a\nfeed-forward layer and a self-attention layer. The latter allows the network to\ncapture long term dependencies and are often regarded as the key ingredient in\nthe success of Transformers. Building upon this intuition, we propose a new\nmodel that solely consists of attention layers. More precisely, we augment the\nself-attention layers with persistent memory vectors that play a similar role\nas the feed-forward layer. Thanks to these vectors, we can remove the\nfeed-forward layer without degrading the performance of a transformer. Our\nevaluation shows the benefits brought by our model on standard character and\nword level language modeling benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:56:20 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Grave", "Edouard", ""], ["Lample", "Guillaume", ""], ["Jegou", "Herve", ""], ["Joulin", "Armand", ""]]}, {"id": "1907.01549", "submitter": "Siddhartha Devapujula", "authors": "Siddhartha Devapujula, Sagar Arora, Sumit Borar", "title": "Learning to Rank Broad and Narrow Queries in E-Commerce", "comments": "7+1 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search is a prominent channel for discovering products on an e-commerce\nplatform. Ranking products retrieved from search becomes crucial to address\ncustomer's need and optimize for business metrics. While learning to Rank\n(LETOR) models have been extensively studied and have demonstrated efficacy in\nthe context of web search; it is a relatively new research area to be explored\nin the e-commerce. In this paper, we present a framework for building LETOR\nmodel for an e-commerce platform. We analyze user queries and propose a\nmechanism to segment queries between broad and narrow based on user's intent.\nWe discuss different types of features - query, product and query-product and\ndiscuss challenges in using them. We show that sparsity in product features can\nbe tackled through a denoising auto-encoder while skip-gram based word\nembeddings help solve the query-product sparsity issues. We also present\nvarious target metrics that can be employed for evaluating search results and\ncompare their robustness. Further, we build and compare performances of both\npointwise and pairwise LETOR models on fashion category data set. We also build\nand compare distinct models for broad and narrow queries, analyze feature\nimportance across these and show that these specialized models perform better\nthan a combined model in the fashion world.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:30:38 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 09:17:51 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Devapujula", "Siddhartha", ""], ["Arora", "Sagar", ""], ["Borar", "Sumit", ""]]}, {"id": "1907.01611", "submitter": "Yannis Tzitzikas", "authors": "Katerina Papantoniou and Yannis Tzitzikas", "title": "CS563-QA: A Collection for Evaluating Question Answering Systems", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) is a challenging topic since it requires tackling the\nvarious difficulties of natural language understanding. Since evaluation is\nimportant not only for identifying the strong and weak points of the various\ntechniques for QA, but also for facilitating the inception of new methods and\ntechniques, in this paper we present a collection for evaluating QA methods\nover free text that we have created. Although it is a small collection, it\ncontains cases of increasing difficulty, therefore it has an educational value\nand it can be used for rapid evaluation of QA systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:00:29 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 19:07:42 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Papantoniou", "Katerina", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "1907.01636", "submitter": "Clint Pazhayidam George", "authors": "Clint P. George, Wei Xia, George Michailidis", "title": "Analyses of Multi-collection Corpora via Compound Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As electronically stored data grow in daily life, obtaining novel and\nrelevant information becomes challenging in text mining. Thus people have\nsought statistical methods based on term frequency, matrix algebra, or topic\nmodeling for text mining. Popular topic models have centered on one single text\ncollection, which is deficient for comparative text analyses. We consider a\nsetting where one can partition the corpus into subcollections. Each\nsubcollection shares a common set of topics, but there exists relative\nvariation in topic proportions among collections. Including any prior knowledge\nabout the corpus (e.g. organization structure), we propose the compound latent\nDirichlet allocation (cLDA) model, improving on previous work, encouraging\ngeneralizability, and depending less on user-input parameters. To identify the\nparameters of interest in cLDA, we study Markov chain Monte Carlo (MCMC) and\nvariational inference approaches extensively, and suggest an efficient MCMC\nmethod. We evaluate cLDA qualitatively and quantitatively using both synthetic\nand real-world corpora. The usability study on some real-world corpora\nillustrates the superiority of cLDA to explore the underlying topics\nautomatically but also model their connections and variations across multiple\ncollections.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 06:59:25 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["George", "Clint P.", ""], ["Xia", "Wei", ""], ["Michailidis", "George", ""]]}, {"id": "1907.01638", "submitter": "Fenglei Jin", "authors": "Fenglei Jin, Cuiyun Gao, Michael R. Lyu", "title": "An Online Topic Modeling Framework with Topics Automatically Labeled", "comments": "5 pages, 3 figures, ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel online topic tracking framework, named\nIEDL, for tracking the topic changes related to deep learning techniques on\nStack Exchange and automatically interpreting each identified topic. The\nproposed framework combines the prior topic distributions in a time window\nduring inferring the topics in current time slice, and introduces a new ranking\nscheme to select most representative phrases and sentences for the inferred\ntopics in each time slice. Experiments on 7,076 Stack Exchange posts show the\neffectiveness of IEDL in tracking topic changes and labeling topics.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 02:42:44 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Jin", "Fenglei", ""], ["Gao", "Cuiyun", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1907.01642", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz and Philipp Scharpf and Kaushal Dudhat and Yash Nagar\n  and Felix Hamborg and Bela Gipp", "title": "Introducing MathQA -- A Math-Aware Question Answering System", "comments": "Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries\n  (JCDL), Workshop on Knowledge Discovery (2018)", "journal-ref": null, "doi": "10.1108/IDD-06-2018-0022", "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open source math-aware Question Answering System based on Ask\nPlatypus. Our system returns as a single mathematical formula for a natural\nlanguage question in English or Hindi. This formulae originate from the\nknowledge-base Wikidata. We translate these formulae to computable data by\nintegrating the calculation engine sympy into our system. This way, users can\nenter numeric values for the variables occurring in the formula. Moreover, the\nsystem loads numeric values for constants occurring in the formula from\nWikidata. In a user study, our system outperformed a commercial computational\nmathematical knowledge engine by 13%. However, the performance of our system\nheavily depends on the size and quality of the formula data available in\nWikidata. Since only a few items in Wikidata contained formulae when we started\nthe project, we facilitated the import process by suggesting formula edits to\nWikidata editors. With the simple heuristic that the first formula is\nsignificant for the article, 80% of the suggestions were correct.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 08:27:53 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Schubotz", "Moritz", ""], ["Scharpf", "Philipp", ""], ["Dudhat", "Kaushal", ""], ["Nagar", "Yash", ""], ["Hamborg", "Felix", ""], ["Gipp", "Bela", ""]]}, {"id": "1907.01643", "submitter": "Hemant Pugaliya", "authors": "Hemant Pugaliya, Karan Saxena, Shefali Garg, Sheetal Shalini, Prashant\n  Gupta, Eric Nyberg, Teruko Mitamura", "title": "Pentagon at MEDIQA 2019: Multi-task Learning for Filtering and\n  Re-ranking Answers using Language Inference and Question Entailment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have\nquickly become the state of the art, bypassing previous deep and shallow\nlearning methods by a large margin. More recently, pre-trained models from\nlarge related datasets have been able to perform well on many downstream tasks\nby just fine-tuning on domain-specific datasets . However, using powerful\nmodels on non-trivial tasks, such as ranking and large document classification,\nstill remains a challenge due to input size limitations of parallel\narchitecture and extremely small datasets (insufficient for fine-tuning). In\nthis work, we introduce an end-to-end system, trained in a multi-task setting,\nto filter and re-rank answers in the medical domain. We use task-specific\npre-trained models as deep feature extractors. Our model achieves the highest\nSpearman's Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on\nthe ACL-BioNLP workshop MediQA Question Answering shared-task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:48:40 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Pugaliya", "Hemant", ""], ["Saxena", "Karan", ""], ["Garg", "Shefali", ""], ["Shalini", "Sheetal", ""], ["Gupta", "Prashant", ""], ["Nyberg", "Eric", ""], ["Mitamura", "Teruko", ""]]}, {"id": "1907.01668", "submitter": "Shuo Zhang", "authors": "Shuo Zhang", "title": "Data mining Mandarin tone contour shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spontaneous speech, Mandarin tones that belong to the same tone category\nmay exhibit many different contour shapes. We explore the use of data mining\nand NLP techniques for understanding the variability of tones in a large corpus\nof Mandarin newscast speech. First, we adapt a graph-based approach to\ncharacterize the clusters (fuzzy types) of tone contour shapes observed in each\ntone n-gram category. Second, we show correlations between these realized\ncontour shape types and a bag of automatically extracted linguistic features.\nWe discuss the implications of the current study within the context of\nphonological and information theory.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:29:53 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zhang", "Shuo", ""]]}, {"id": "1907.01669", "submitter": "Rahul Goel", "authors": "Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi,\n  Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, Dilek Hakkani-Tu\\\"r", "title": "MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State\n  Corrections and State Tracking Baselines", "comments": "Data release writeup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain\ndialogue dataset spanning 7 distinct domains and containing over 10,000\ndialogues. Though immensely useful and one of the largest resources of its kind\nto-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there is substantial\nnoise in the dialogue state annotations and dialogue utterances which\nnegatively impact the performance of state-tracking models. Secondly, follow-up\nwork (Lee et al., 2019) has augmented the original dataset with user dialogue\nacts. This leads to multiple co-existent versions of the same dataset with\nminor modifications. In this work we tackle the aforementioned issues by\nintroducing MultiWOZ 2.1. To fix the noisy state annotations, we use\ncrowdsourced workers to re-annotate state and utterances based on the original\nutterances in the dataset. This correction process results in changes to over\n32% of state annotations across 40% of the dialogue turns. In addition, we fix\n146 dialogue utterances by canonicalizing slot values in the utterances to the\nvalues in the dataset ontology. To address the second problem, we combined the\ncontributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also\nincludes user dialogue acts as well as multiple slot descriptions per dialogue\nstate slot. We then benchmark a number of state-of-the-art dialogue state\ntracking models on the MultiWOZ 2.1 dataset and show the joint state tracking\nperformance on the corrected state annotations. We are publicly releasing\nMultiWOZ 2.1 to the community, hoping that this dataset resource will allow for\nmore effective models across various dialogue subproblems to be built in the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:30:31 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 21:06:30 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 18:34:38 GMT"}, {"version": "v4", "created": "Tue, 3 Dec 2019 22:41:37 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Eric", "Mihail", ""], ["Goel", "Rahul", ""], ["Paul", "Shachi", ""], ["Kumar", "Adarsh", ""], ["Sethi", "Abhishek", ""], ["Ku", "Peter", ""], ["Goyal", "Anuj Kumar", ""], ["Agarwal", "Sanchit", ""], ["Gao", "Shuyang", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1907.01677", "submitter": "Anirudh Raju", "authors": "Anirudh Raju, Denis Filimonov, Gautam Tiwari, Guitang Lan, Ariya\n  Rastrow", "title": "Scalable Multi Corpora Neural Language Models for ASR", "comments": "Interspeech 2019 (accepted: oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models (NLM) have been shown to outperform conventional\nn-gram language models by a substantial margin in Automatic Speech Recognition\n(ASR) and other tasks. There are, however, a number of challenges that need to\nbe addressed for an NLM to be used in a practical large-scale ASR system. In\nthis paper, we present solutions to some of the challenges, including training\nNLM from heterogenous corpora, limiting latency impact and handling\npersonalized bias in the second-pass rescorer. Overall, we show that we can\nachieve a 6.2% relative WER reduction using neural LM in a second-pass n-best\nrescoring framework with a minimal increase in latency.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 23:28:52 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Raju", "Anirudh", ""], ["Filimonov", "Denis", ""], ["Tiwari", "Gautam", ""], ["Lan", "Guitang", ""], ["Rastrow", "Ariya", ""]]}, {"id": "1907.01686", "submitter": "An Yang", "authors": "Xin Zhang, An Yang, Sujian Li, Yizhong Wang", "title": "Machine Reading Comprehension: a Literature Review", "comments": "46 pages, preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine reading comprehension aims to teach machines to understand a text\nlike a human and is a new challenging direction in Artificial Intelligence.\nThis article summarizes recent advances in MRC, mainly focusing on two aspects\n(i.e., corpus and techniques). The specific characteristics of various MRC\ncorpus are listed and compared. The main ideas of some typical MRC techniques\nare also described.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 09:18:31 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zhang", "Xin", ""], ["Yang", "An", ""], ["Li", "Sujian", ""], ["Wang", "Yizhong", ""]]}, {"id": "1907.01749", "submitter": "Zexin Cai", "authors": "Zexin Cai, Yaogen Yang, Chuxiong Zhang, Xiaoyi Qin, Ming Li", "title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural\n  Network with Multi-level Embedding Features", "comments": "5 pages, 1 figure, 2 tables, submit to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a conditional neural network architecture for Mandarin\nChinese polyphone disambiguation. The system is composed of a bidirectional\nrecurrent neural network component acting as a sentence encoder to accumulate\nthe context correlations, followed by a prediction network that maps the\npolyphonic character embeddings along with the conditions to corresponding\npronunciations. We obtain the word-level condition from a pre-trained\nword-to-vector lookup table. One goal of polyphone disambiguation is to address\nthe homograph problem existing in the front-end processing of Mandarin Chinese\ntext-to-speech system. Our system achieves an accuracy of 94.69\\% on a publicly\navailable polyphonic character dataset. To further validate our choices on the\nconditional feature, we investigate polyphone disambiguation systems with\nmulti-level conditions respectively. The experimental results show that both\nthe sentence-level and the word-level conditional embedding features are able\nto attain good performance for Mandarin Chinese polyphone disambiguation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 05:50:34 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Cai", "Zexin", ""], ["Yang", "Yaogen", ""], ["Zhang", "Chuxiong", ""], ["Qin", "Xiaoyi", ""], ["Li", "Ming", ""]]}, {"id": "1907.01752", "submitter": "Leshem Choshen", "authors": "Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend", "title": "On the Weaknesses of Reinforcement Learning for Neural Machine\n  Translation", "comments": "Accepted to ICLR 2020 (matching content, different style)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is frequently used to increase performance in\ntext generation tasks, including machine translation (MT), notably through the\nuse of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).\nHowever, little is known about what and how these methods learn in the context\nof MT. We prove that one of the most common RL methods for MT does not optimize\nthe expected reward, as well as show that other methods take an infeasibly long\ntime to converge. In fact, our results suggest that RL practices in MT are\nlikely to improve performance only where the pre-trained parameters are already\nclose to yielding the correct translation. Our findings further suggest that\nobserved gains may be due to effects unrelated to the training signal, but\nrather from changes in the shape of the distribution curve.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 06:15:14 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 13:57:23 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 10:31:04 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2020 07:19:09 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Choshen", "Leshem", ""], ["Fox", "Lior", ""], ["Aizenbud", "Zohar", ""], ["Abend", "Omri", ""]]}, {"id": "1907.01791", "submitter": "Shiva P", "authors": "Shiva Pentyala, Mengwen Liu, Markus Dreyer", "title": "Multi-Task Networks With Universe, Group, and Task Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for multi-task learning that take advantage of natural\ngroupings of related tasks. Task groups may be defined along known properties\nof the tasks, such as task domain or language. Such task groups represent\nsupervised information at the inter-task level and can be encoded into the\nmodel. We investigate two variants of neural network architectures that\naccomplish this, learning different feature spaces at the levels of individual\ntasks, task groups, as well as the universe of all tasks: (1) parallel\narchitectures encode each input simultaneously into feature spaces at different\nlevels; (2) serial architectures encode each input successively into feature\nspaces at different levels in the task hierarchy. We demonstrate the methods on\nnatural language understanding (NLU) tasks, where a grouping of tasks into\ndifferent task domains leads to improved performance on ATIS, Snips, and a\nlarge inhouse dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 08:39:14 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Pentyala", "Shiva", ""], ["Liu", "Mengwen", ""], ["Dreyer", "Markus", ""]]}, {"id": "1907.01914", "submitter": "Dmytro Tkanov", "authors": "Ievgen Karaulov, Dmytro Tkanov", "title": "Attention model for articulatory features detection", "comments": "Interspeech 2019, 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulatory distinctive features, as well as phonetic transcription, play\nimportant role in speech-related tasks: computer-assisted pronunciation\ntraining, text-to-speech conversion (TTS), studying speech production\nmechanisms, speech recognition for low-resourced languages. End-to-end\napproaches to speech-related tasks got a lot of traction in recent years. We\napply Listen, Attend and Spell~(LAS)~\\cite{Chan-LAS2016} architecture to phones\nrecognition on a small small training set, like TIMIT~\\cite{TIMIT-1992}. Also,\nwe introduce a novel decoding technique that allows to train manners and places\nof articulation detectors end-to-end using attention models. We also explore\njoint phones recognition and articulatory features detection in multitask\nlearning setting.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:30:27 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Karaulov", "Ievgen", ""], ["Tkanov", "Dmytro", ""]]}, {"id": "1907.01957", "submitter": "Cong-Thanh Do", "authors": "Cong-Thanh Do", "title": "End-to-End Speech Recognition with High-Frame-Rate Features Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art end-to-end automatic speech recognition (ASR) extracts\nacoustic features from input speech signal every 10 ms which corresponds to a\nframe rate of 100 frames/second. In this report, we investigate the use of\nhigh-frame-rate features extraction in end-to-end ASR. High frame rates of 200\nand 400 frames/second are used in the features extraction and provide\nadditional information for end-to-end ASR. The effectiveness of high-frame-rate\nfeatures extraction is evaluated independently and in combination with speed\nperturbation based data augmentation. Experiments performed on two speech\ncorpora, Wall Street Journal (WSJ) and CHiME-5, show that using high-frame-rate\nfeatures extraction yields improved performance for end-to-end ASR, both\nindependently and in combination with speed perturbation. On WSJ corpus, the\nrelative reduction of word error rate (WER) yielded by high-frame-rate features\nextraction independently and in combination with speed perturbation are up to\n21.3% and 24.1%, respectively. On CHiME-5 corpus, the corresponding relative\nWER reductions are up to 2.8% and 7.9%, respectively, on the test data recorded\nby microphone arrays and up to 11.8% and 21.2%, respectively, on the test data\nrecorded by binaural microphones.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 14:14:27 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 15:48:37 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Do", "Cong-Thanh", ""]]}, {"id": "1907.01968", "submitter": "Lijun Wu", "authors": "Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao, Tao Qin,\n  Jianhuang Lai, Tie-Yan Liu", "title": "Depth Growing for Neural Machine Translation", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While very deep neural networks have shown effectiveness for computer vision\nand text classification applications, how to increase the network depth of\nneural machine translation (NMT) models for better translation quality remains\na challenging problem. Directly stacking more blocks to the NMT model results\nin no improvement and even reduces performance. In this work, we propose an\neffective two-stage approach with three specially designed components to\nconstruct deeper NMT models, which result in significant improvements over the\nstrong Transformer baselines on WMT$14$ English$\\to$German and\nEnglish$\\to$French translation tasks\\footnote{Our code is available at\n\\url{https://github.com/apeterswu/Depth_Growing_NMT}}.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 14:31:45 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Wu", "Lijun", ""], ["Wang", "Yiren", ""], ["Xia", "Yingce", ""], ["Tian", "Fei", ""], ["Gao", "Fei", ""], ["Qin", "Tao", ""], ["Lai", "Jianhuang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1907.02022", "submitter": "Peter Anderson", "authors": "Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra and Stefan\n  Lee", "title": "Chasing Ghosts: Instruction Following as Bayesian State Tracking", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visually-grounded navigation instruction can be interpreted as a sequence\nof expected observations and actions an agent following the correct trajectory\nwould encounter and perform. Based on this intuition, we formulate the problem\nof finding the goal location in Vision-and-Language Navigation (VLN) within the\nframework of Bayesian state tracking - learning observation and motion models\nconditioned on these expectable events. Together with a mapper that constructs\na semantic spatial map on-the-fly during navigation, we formulate an end-to-end\ndifferentiable Bayes filter and train it to identify the goal by predicting the\nmost likely trajectory through the map according to the instructions. The\nresulting navigation policy constitutes a new approach to instruction following\nthat explicitly models a probability distribution over states, encoding strong\ngeometric and algorithmic priors while enabling greater explainability. Our\nexperiments show that our approach outperforms a strong LingUNet baseline when\npredicting the goal location on the map. On the full VLN task, i.e. navigating\nto the goal location, our approach achieves promising results with less\nreliance on navigation constraints.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:39:05 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 18:52:33 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Anderson", "Peter", ""], ["Shrivastava", "Ayush", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "1907.02030", "submitter": "Ben Adler", "authors": "Ben Adler and Giacomo Boscaini-Gilroy", "title": "Real-time Claim Detection from News Articles and Retrieval of\n  Semantically-Similar Factchecks", "comments": "NewsIR19 Workshop at SIGIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factchecking has always been a part of the journalistic process. However with\nnewsroom budgets shrinking it is coming under increasing pressure just as the\namount of false information circulating is on the rise. We therefore propose a\nmethod to increase the efficiency of the factchecking process, using the latest\ndevelopments in Natural Language Processing (NLP). This method allows us to\ncompare incoming claims to an existing corpus and return similar, factchecked,\nclaims in a live system-allowing factcheckers to work simultaneously without\nduplicating their work.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:50:50 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Adler", "Ben", ""], ["Boscaini-Gilroy", "Giacomo", ""]]}, {"id": "1907.02031", "submitter": "Lin Li", "authors": "Dong Li, Lin Li", "title": "Combining Q&A Pair Quality and Question Relevance Features on\n  Community-based Question Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Q&A community has become an important way for people to access knowledge\nand information from the Internet. However, the existing translation based on\nmodels does not consider the query specific semantics when assigning weights to\nquery terms in question retrieval. So we improve the term weighting model based\non the traditional topic translation model and further considering the quality\ncharacteristics of question and answer pairs, this paper proposes a\ncommunitybased question retrieval method that combines question and answer on\nquality and question relevance (T2LM+). We have also proposed a question\nretrieval method based on convolutional neural networks. The results show that\nCompared with the relatively advanced methods, the two methods proposed in this\npaper increase MAP by 4.91% and 6.31%.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:53:28 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Li", "Dong", ""], ["Li", "Lin", ""]]}, {"id": "1907.02046", "submitter": "Lin Li", "authors": "Donghang Pan, Jingling Yuan, Lin Li, Deming Sheng", "title": "Deep neural network-based classification model for Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing prosperity of social networks has brought great challenges to the\nsentimental tendency mining of users. As more and more researchers pay\nattention to the sentimental tendency of online users, rich research results\nhave been obtained based on the sentiment classification of explicit texts.\nHowever, research on the implicit sentiment of users is still in its infancy.\nAiming at the difficulty of implicit sentiment classification, a research on\nimplicit sentiment classification model based on deep neural network is carried\nout. Classification models based on DNN, LSTM, Bi-LSTM and CNN were established\nto judge the tendency of the user's implicit sentiment text. Based on the\nBi-LSTM model, the classification model of word-level attention mechanism is\nstudied. The experimental results on the public dataset show that the\nestablished LSTM series classification model and CNN classification model can\nachieve good sentiment classification effect, and the classification effect is\nsignificantly better than the DNN model. The Bi-LSTM based attention mechanism\nclassification model obtained the optimal R value in the positive category\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:24:14 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Pan", "Donghang", ""], ["Yuan", "Jingling", ""], ["Li", "Lin", ""], ["Sheng", "Deming", ""]]}, {"id": "1907.02052", "submitter": "Jieh-Sheng Lee", "authors": "Jieh-Sheng Lee and Jieh Hsiang", "title": "Patent Claim Generation by Fine-Tuning OpenAI GPT-2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model for\ngenerating patent claims. GPT-2 has demonstrated impressive efficacy of\npre-trained language models on various tasks, particularly coherent text\ngeneration. Patent claim language itself has rarely been explored in the past\nand poses a unique challenge. We are motivated to generate coherent patent\nclaims automatically so that augmented inventing might be viable someday. In\nour implementation, we identified a unique language structure in patent claims\nand leveraged its implicit human annotations. We investigated the fine-tuning\nprocess by probing the first 100 steps and observing the generated text at each\nstep. Based on both conditional and unconditional random sampling, we analyze\nthe overall quality of generated patent claims. Our contributions include: (1)\nbeing the first to generate patent claims by machines and being the first to\napply GPT-2 to patent claim generation, (2) providing various experiment\nresults for qualitative analysis and future research, (3) proposing a new\nsampling approach for text generation, and (4) building an e-mail bot for\nfuture researchers to explore the fine-tuned GPT-2 model further.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 00:02:59 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Lee", "Jieh-Sheng", ""], ["Hsiang", "Jieh", ""]]}, {"id": "1907.02065", "submitter": "Lakshay Sharma", "authors": "Elaina Tan, Lakshay Sharma", "title": "Neural Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the biggest advances in major Computer Vision tasks, such as\nobject recognition, handwritten-digit identification, facial recognition, and\nmany others., have all come through the use of Convolutional Neural Networks\n(CNNs). Similarly, in the domain of Natural Language Processing, Recurrent\nNeural Networks (RNNs), and Long Short Term Memory networks (LSTMs) in\nparticular, have been crucial to some of the biggest breakthroughs in\nperformance for tasks such as machine translation, part-of-speech tagging,\nsentiment analysis, and many others. These individual advances have greatly\nbenefited tasks even at the intersection of NLP and Computer Vision, and\ninspired by this success, we studied some existing neural image captioning\nmodels that have proven to work well. In this work, we study some existing\ncaptioning models that provide near state-of-the-art performances, and try to\nenhance one such model. We also present a simple image captioning model that\nmakes use of a CNN, an LSTM, and the beam search1 algorithm, and study its\nperformance based on various qualitative and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:49:25 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Tan", "Elaina", ""], ["Sharma", "Lakshay", ""]]}, {"id": "1907.02090", "submitter": "Maira Gatti de Bayser", "authors": "Maira Gatti de Bayser, Paulo Cavalin, Claudio Pinhanez, Bianca\n  Zadrozny", "title": "Learning Multi-Party Turn-Taking Models from Dialogue Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the application of machine learning (ML) techniques\nto enable intelligent systems to learn multi-party turn-taking models from\ndialogue logs. The specific ML task consists of determining who speaks next,\nafter each utterance of a dialogue, given who has spoken and what was said in\nthe previous utterances. With this goal, this paper presents comparisons of the\naccuracy of different ML techniques such as Maximum Likelihood Estimation\n(MLE), Support Vector Machines (SVM), and Convolutional Neural Networks (CNN)\narchitectures, with and without utterance data. We present three corpora: the\nfirst with dialogues from an American TV situated comedy (chit-chat), the\nsecond with logs from a financial advice multi-bot system and the third with a\ncorpus created from the Multi-Domain Wizard-of-Oz dataset (both are\ntopic-oriented). The results show: (i) the size of the corpus has a very\npositive impact on the accuracy for the content-based deep learning approaches\nand those models perform best in the larger datasets; and (ii) if the dialogue\ndataset is small and topic-oriented (but with few topics), it is sufficient to\nuse an agent-only MLE or SVM models, although slightly higher accuracies can be\nachieved with the use of the content of the utterances with a CNN model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:08:16 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["de Bayser", "Maira Gatti", ""], ["Cavalin", "Paulo", ""], ["Pinhanez", "Claudio", ""], ["Zadrozny", "Bianca", ""]]}, {"id": "1907.02106", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves, Matthew Horridge, Rui Li, Yu Liu, Mark A.\n  Musen, Csongor I. Nyulas, Evelyn Obamos, Dhananjay Shrouty, and David Temple", "title": "Use of OWL and Semantic Web Technologies at Pinterest", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30796-7_26", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pinterest is a popular Web application that has over 250 million active\nusers. It is a visual discovery engine for finding ideas for recipes, fashion,\nweddings, home decoration, and much more. In the last year, the company adopted\nSemantic Web technologies to create a knowledge graph that aims to represent\nthe vast amount of content and users on Pinterest, to help both content\nrecommendation and ads targeting. In this paper, we present the engineering of\nan OWL ontology---the Pinterest Taxonomy---that forms the core of Pinterest's\nknowledge graph, the Pinterest Taste Graph. We describe modeling choices and\nenhancements to WebProt\\'eg\\'e that we used for the creation of the ontology.\nIn two months, eight Pinterest engineers, without prior experience of OWL and\nWebProt\\'eg\\'e, revamped an existing taxonomy of noisy terms into an OWL\nontology. We share our experience and present the key aspects of our work that\nwe believe will be useful for others working in this area.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:58:49 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["Horridge", "Matthew", ""], ["Li", "Rui", ""], ["Liu", "Yu", ""], ["Musen", "Mark A.", ""], ["Nyulas", "Csongor I.", ""], ["Obamos", "Evelyn", ""], ["Shrouty", "Dhananjay", ""], ["Temple", "David", ""]]}, {"id": "1907.02202", "submitter": "Zhenpeng Chen", "authors": "Zhenpeng Chen and Yanbin Cao and Xuan Lu and Qiaozhu Mei and Xuanzhe\n  Liu", "title": "SEntiMoji: An Emoji-Powered Learning Approach for Sentiment Analysis in\n  Software Engineering", "comments": "Accepted by the 2019 ACM Joint European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE\n  2019). Please include ESEC/FSE in any citations", "journal-ref": null, "doi": "10.1145/3338906.3338977", "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis has various application scenarios in software engineering\n(SE), such as detecting developers' emotions in commit messages and identifying\ntheir opinions on Q&A forums. However, commonly used out-of-the-box sentiment\nanalysis tools cannot obtain reliable results on SE tasks and the\nmisunderstanding of technical jargon is demonstrated to be the main reason.\nThen, researchers have to utilize labeled SE-related texts to customize\nsentiment analysis for SE tasks via a variety of algorithms. However, the\nscarce labeled data can cover only very limited expressions and thus cannot\nguarantee the analysis quality. To address such a problem, we turn to the\neasily available emoji usage data for help. More specifically, we employ\nemotional emojis as noisy labels of sentiments and propose a representation\nlearning approach that uses both Tweets and GitHub posts containing emojis to\nlearn sentiment-aware representations for SE-related texts. These emoji-labeled\nposts can not only supply the technical jargon, but also incorporate more\ngeneral sentiment patterns shared across domains. They as well as labeled data\nare used to learn the final sentiment classifier. Compared to the existing\nsentiment analysis methods used in SE, the proposed approach can achieve\nsignificant improvement on representative benchmark datasets. By further\ncontrast experiments, we find that the Tweets make a key contribution to the\npower of our approach. This finding informs future research not to unilaterally\npursue the domain-specific resource, but try to transform knowledge from the\nopen domain through ubiquitous signals such as emojis.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 03:38:41 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Chen", "Zhenpeng", ""], ["Cao", "Yanbin", ""], ["Lu", "Xuan", ""], ["Mei", "Qiaozhu", ""], ["Liu", "Xuanzhe", ""]]}, {"id": "1907.02205", "submitter": "Lin Li", "authors": "Duan Wei, Li Lin", "title": "An External Knowledge Enhanced Multi-label Charge Prediction Approach\n  with Label Number Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label charge prediction is a task to predict the corresponding\naccusations for legal cases, and recently becomes a hot topic. However, current\nstudies use rough methods to deal with the label number. These methods manually\nset parameters to select label numbers, which has an effect in final prediction\nquality. We propose an external knowledge enhanced multi-label charge\nprediction approach that has two phases. One is charge label prediction phase\nwith external knowledge from law provisions, the other one is number learning\nphase with a number learning network (NLN) designed. Our approach enhanced by\nexternal knowledge can automatically adjust the threshold to get label number\nof law cases. It combines the output probabilities of samples and their\ncorresponding label numbers to get final prediction results. In experiments,\nour approach is connected to some state of-the art deep learning models. By\ntesting on the biggest published Chinese law dataset, we find that our approach\nhas improvements on these models. We future conduct experiments on multi-label\nsamples from the dataset. In items of macro-F1, the improvement of baselines\nwith our approach is 3%-5%; In items of micro-F1, the significant improvement\nof our approach is 5%-15%. The experiment results show the effectiveness our\napproach for multi-label charge prediction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 03:50:21 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Wei", "Duan", ""], ["Lin", "Li", ""]]}, {"id": "1907.02298", "submitter": "Zi Lin", "authors": "Junjie Cao, Zi Lin, Weiwei Sun, Xiaojun Wan", "title": "A Comparative Analysis of Knowledge-Intensive and Data-Intensive\n  Semantic Parsers", "comments": "submitted to the journal Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a phenomenon-oriented comparative analysis of the two dominant\napproaches in task-independent semantic parsing: classic, knowledge-intensive\nand neural, data-intensive models. To reflect state-of-the-art neural NLP\ntechnologies, we introduce a new target structure-centric parser that can\nproduce semantic graphs much more accurately than previous data-driven parsers.\nWe then show that, in spite of comparable performance overall, knowledge- and\ndata-intensive models produce different types of errors, in a way that can be\nexplained by their theoretical properties. This analysis leads to new\ndirections for parser development.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 09:40:27 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 10:36:59 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 08:39:30 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cao", "Junjie", ""], ["Lin", "Zi", ""], ["Sun", "Weiwei", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1907.02326", "submitter": "Shigehiko Schamoni", "authors": "Tsz Kin Lam, Shigehiko Schamoni, Stefan Riezler", "title": "Interactive-Predictive Neural Machine Translation through Reinforcement\n  and Imitation", "comments": "Machine Translation Summit 2019 (MTSUMMIT XVII), Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an interactive-predictive neural machine translation framework for\neasier model personalization using reinforcement and imitation learning. During\nthe interactive translation process, the user is asked for feedback on\nuncertain locations identified by the system. Responses are weak feedback in\nthe form of \"keep\" and \"delete\" edits, and expert demonstrations in the form of\n\"substitute\" edits. Conditioning on the collected feedback, the system creates\nalternative translations via constrained beam search. In simulation experiments\non two language pairs our systems get close to the performance of supervised\ntraining with much less human effort.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 10:58:58 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 10:38:36 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Lam", "Tsz Kin", ""], ["Schamoni", "Shigehiko", ""], ["Riezler", "Stefan", ""]]}, {"id": "1907.02423", "submitter": "Ryan D. Cotterell", "authors": "Ryan Cotterell and Hinrich Sch\\\"utze", "title": "Morphological Word Embeddings", "comments": "Published at NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic similarity is multi-faceted. For instance, two words may be\nsimilar with respect to semantics, syntax, or morphology inter alia. Continuous\nword-embeddings have been shown to capture most of these shades of similarity\nto some degree. This work considers guiding word-embeddings with\nmorphologically annotated data, a form of semi-supervised learning, encouraging\nthe vectors to encode a word's morphology, i.e., words close in the embedded\nspace share morphological features. We extend the log-bilinear model to this\nend and show that indeed our learned embeddings achieve this, using German as a\ncase study.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:32:39 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Cotterell", "Ryan", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1907.02427", "submitter": "Youmna Farag", "authors": "Youmna Farag and Helen Yannakoudakis", "title": "Multi-Task Learning for Coherence Modeling", "comments": "11 pages, 3 figures, Accepted at ACL 2019", "journal-ref": "THE 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL\n  LINGUISTICS (ACL 2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of assessing discourse coherence, an aspect of text\nquality that is essential for many NLP tasks, such as summarization and\nlanguage assessment. We propose a hierarchical neural network trained in a\nmulti-task fashion that learns to predict a document-level coherence score (at\nthe network's top layers) along with word-level grammatical roles (at the\nbottom layers), taking advantage of inductive transfer between the two tasks.\nWe assess the extent to which our framework generalizes to different domains\nand prediction tasks, and demonstrate its effectiveness not only on standard\nbinary evaluation coherence tasks, but also on real-world tasks involving the\nprediction of varying degrees of coherence, achieving a new state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:40:22 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 17:30:15 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Farag", "Youmna", ""], ["Yannakoudakis", "Helen", ""]]}, {"id": "1907.02479", "submitter": "Srikanth Ronanki", "authors": "Viacheslav Klimkov, Srikanth Ronanki, Jonas Rohnke, Thomas Drugman", "title": "Fine-grained robust prosody transfer for single-speaker neural\n  text-to-speech", "comments": "5 pages, 7 figures, Accepted for Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural text-to-speech system for fine-grained prosody transfer\nfrom one speaker to another. Conventional approaches for end-to-end prosody\ntransfer typically use either fixed-dimensional or variable-length prosody\nembedding via a secondary attention to encode the reference signal. However,\nwhen trained on a single-speaker dataset, the conventional prosody transfer\nsystems are not robust enough to speaker variability, especially in the case of\na reference signal coming from an unseen speaker. Therefore, we propose\ndecoupling of the reference signal alignment from the overall system. For this\npurpose, we pre-compute phoneme-level time stamps and use them to aggregate\nprosodic features per phoneme, injecting them into a sequence-to-sequence\ntext-to-speech system. We incorporate a variational auto-encoder to further\nenhance the latent representation of prosody embeddings. We show that our\nproposed approach is significantly more stable and achieves reliable prosody\ntransplantation from an unseen speaker. We also propose a solution to the use\ncase in which the transcription of the reference signal is absent. We evaluate\nall our proposed methods using both objective and subjective listening tests.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 16:20:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Klimkov", "Viacheslav", ""], ["Ronanki", "Srikanth", ""], ["Rohnke", "Jonas", ""], ["Drugman", "Thomas", ""]]}, {"id": "1907.02581", "submitter": "Derek Howard", "authors": "Derek Howard, Marta Maslej, Justin Lee, Jacob Ritchie, Geoffrey\n  Woollard, Leon French", "title": "Transfer Learning for Risk Classification of Social Media Posts: Model\n  Evaluation Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mental illness affects a significant portion of the worldwide population.\nOnline mental health forums can provide a supportive environment for those\nafflicted and also generate a large amount of data which can be mined to\npredict mental health states using machine learning methods. We benchmark\nmultiple methods of text feature representation for social media posts and\ncompare their downstream use with automated machine learning (AutoML) tools to\ntriage content for moderator attention. We used 1588 labeled posts from the\nCLPsych 2017 shared task collected from the Reachout.com forum (Milne et al.,\n2019). Posts were represented using lexicon based tools including VADER,\nEmpath, LIWC and also used pre-trained artificial neural network models\nincluding DeepMoji, Universal Sentence Encoder, and GPT-1. We used TPOT and\nauto-sklearn as AutoML tools to generate classifiers to triage the posts. The\ntop-performing system used features derived from the GPT-1 model, which was\nfinetuned on over 150,000 unlabeled posts from Reachout.com. Our top system had\na macro averaged F1 score of 0.572, providing a new state-of-the-art result on\nthe CLPsych 2017 task. This was achieved without additional information from\nmeta-data or preceding posts. Error analyses revealed that this top system\noften misses expressions of hopelessness. We additionally present\nvisualizations that aid understanding of the learned classifiers. We show that\ntransfer learning is an effective strategy for predicting risk with relatively\nlittle labeled data. We note that finetuning of pretrained language models\nprovides further gains when large amounts of unlabeled text is available.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 20:37:44 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 21:09:43 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Howard", "Derek", ""], ["Maslej", "Marta", ""], ["Lee", "Justin", ""], ["Ritchie", "Jacob", ""], ["Woollard", "Geoffrey", ""], ["French", "Leon", ""]]}, {"id": "1907.02606", "submitter": "Saeedeh Shekarpour", "authors": "Saeedeh Shekarpour and Faisal Alshargi", "title": "A Road-map Towards Explainable Question Answering A Solution for\n  Information Pollution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing rate of information pollution on the Web requires novel\nsolutions to tackle that. Question Answering (QA) interfaces are simplified and\nuser-friendly interfaces to access information on the Web. However, similar to\nother AI applications, they are black boxes which do not manifest the details\nof the learning or reasoning steps for augmenting an answer. The Explainable\nQuestion Answering (XQA) system can alleviate the pain of information pollution\nwhere it provides transparency to the underlying computational model and\nexposes an interface enabling the end-user to access and validate provenance,\nvalidity, context, circulation, interpretation, and feedbacks of information.\nThis position paper sheds light on the core concepts, expectations, and\nchallenges in favor of the following questions (i) What is an XQA system?, (ii)\nWhy do we need XQA?, (iii) When do we need XQA? (iv) How to represent the\nexplanations? (iv) How to evaluate XQA systems?\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 21:42:29 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Shekarpour", "Saeedeh", ""], ["Alshargi", "Faisal", ""]]}, {"id": "1907.02636", "submitter": "Zi Long", "authors": "Zi Long, Lianzhi Tan, Shengping Zhou, Chaoyang He, Xin Liu", "title": "Collecting Indicators of Compromise from Unstructured Text of\n  Cybersecurity Articles using Neural-Based Sequence Labelling", "comments": "IJCNN 2019. arXiv admin note: substantial text overlap with\n  arXiv:1810.10156", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indicators of Compromise (IOCs) are artifacts observed on a network or in an\noperating system that can be utilized to indicate a computer intrusion and\ndetect cyber-attacks in an early stage. Thus, they exert an important role in\nthe field of cybersecurity. However, state-of-the-art IOCs detection systems\nrely heavily on hand-crafted features with expert knowledge of cybersecurity,\nand require large-scale manually annotated corpora to train an IOC classifier.\nIn this paper, we propose using an end-to-end neural-based sequence labelling\nmodel to identify IOCs automatically from cybersecurity articles without expert\nknowledge of cybersecurity. By using a multi-head self-attention module and\ncontextual features, we find that the proposed model is capable of gathering\ncontextual information from texts of cybersecurity articles and performs better\nin the task of IOC identification. Experiments show that the proposed model\noutperforms other sequence labelling models, achieving the average F1-score of\n89.0% on English cybersecurity article test set, and approximately the average\nF1-score of 81.8% on Chinese test set.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 02:54:02 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 02:35:04 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Long", "Zi", ""], ["Tan", "Lianzhi", ""], ["Zhou", "Shengping", ""], ["He", "Chaoyang", ""], ["Liu", "Xin", ""]]}, {"id": "1907.02679", "submitter": "Zenan Zhai", "authors": "Zenan Zhai, Dat Quoc Nguyen, Saber A. Akhondi, Camilo Thorne,\n  Christian Druckenbrodt, Trevor Cohn, Michelle Gregory, Karin Verspoor", "title": "Improving Chemical Named Entity Recognition in Patents with\n  Contextualized Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical patents are an important resource for chemical information. However,\nfew chemical Named Entity Recognition (NER) systems have been evaluated on\npatent documents, due in part to their structural and linguistic complexity. In\nthis paper, we explore the NER performance of a BiLSTM-CRF model utilising\npre-trained word embeddings, character-level word representations and\ncontextualized ELMo word representations for chemical patents. We compare word\nembeddings pre-trained on biomedical and chemical patent corpora. The effect of\ntokenizers optimized for the chemical domain on NER performance in chemical\npatents is also explored. The results on two patent corpora show that\ncontextualized word representations generated from ELMo substantially improve\nchemical NER performance w.r.t. the current state-of-the-art. We also show that\ndomain-specific resources such as word embeddings trained on chemical patents\nand chemical-specific tokenizers have a positive impact on NER performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 05:19:46 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Zhai", "Zenan", ""], ["Nguyen", "Dat Quoc", ""], ["Akhondi", "Saber A.", ""], ["Thorne", "Camilo", ""], ["Druckenbrodt", "Christian", ""], ["Cohn", "Trevor", ""], ["Gregory", "Michelle", ""], ["Verspoor", "Karin", ""]]}, {"id": "1907.02684", "submitter": "Junru Zhou", "authors": "Junru Zhou and Hai Zhao", "title": "Head-Driven Phrase Structure Grammar Parsing on Penn Treebank", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism\nrepresenting rich contextual syntactic and even semantic meanings. This paper\nmakes the first attempt to formulate a simplified HPSG by integrating\nconstituent and dependency formal representations into head-driven phrase\nstructure. Then two parsing algorithms are respectively proposed for two\nconverted tree representations, division span and joint span. As HPSG encodes\nboth constituent and dependency structure information, the proposed HPSG\nparsers may be regarded as a sort of joint decoder for both types of structures\nand thus are evaluated in terms of extracted or converted constituent and\ndependency parsing trees. Our parser achieves new state-of-the-art performance\nfor both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank,\nverifying the effectiveness of joint learning constituent and dependency\nstructures. In details, we report 96.33 F1 of constituent parsing and 97.20\\%\nUAS of dependency parsing on PTB.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 05:44:21 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 08:23:27 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 05:17:32 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 04:34:54 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhou", "Junru", ""], ["Zhao", "Hai", ""]]}, {"id": "1907.02704", "submitter": "Vincent Labatut", "authors": "Vincent Labatut (LIA), Xavier Bost (LIA)", "title": "Extraction and Analysis of Fictional Character Networks: A Survey", "comments": null, "journal-ref": "ACM Computing Surveys, Association for Computing Machinery, 2019,\n  52 (5), pp.89", "doi": "10.1145/3344548", "report-no": null, "categories": "cs.SI cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:27:31 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 09:00:13 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 12:51:45 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 13:01:30 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Labatut", "Vincent", "", "LIA"], ["Bost", "Xavier", "", "LIA"]]}, {"id": "1907.02784", "submitter": "No\\'e Tits", "authors": "No\\'e Tits", "title": "A Methodology for Controlling the Emotional Expressiveness in Synthetic\n  Speech -- a Deep Learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we aim to build a Text-to-Speech system able to produce\nspeech with a controllable emotional expressiveness. We propose a methodology\nfor solving this problem in three main steps. The first is the collection of\nemotional speech data. We discuss the various formats of existing datasets and\ntheir usability in speech generation. The second step is the development of a\nsystem to automatically annotate data with emotion/expressiveness features. We\ncompare several techniques using transfer learning to extract such a\nrepresentation through other tasks and propose a method to visualize and\ninterpret the correlation between vocal and emotional features. The third step\nis the development of a deep learning-based system taking text and\nemotion/expressiveness as input and producing speech as output. We study the\nimpact of fine tuning from a neutral TTS towards an emotional TTS in terms of\nintelligibility and perception of the emotion.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:00:53 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Tits", "No\u00e9", ""]]}, {"id": "1907.02848", "submitter": "Chinnadhurai Sankar", "authors": "Chinnadhurai Sankar, Sujith Ravi", "title": "Deep Reinforcement Learning For Modeling Chit-Chat Dialog With Discrete\n  Attributes", "comments": "SIGDIAL 2019 - BEST PAPER AWARD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open domain dialog systems face the challenge of being repetitive and\nproducing generic responses. In this paper, we demonstrate that by conditioning\nthe response generation on interpretable discrete dialog attributes and\ncomposed attributes, it helps improve the model perplexity and results in\ndiverse and interesting non-redundant responses. We propose to formulate the\ndialog attribute prediction as a reinforcement learning (RL) problem and use\npolicy gradients methods to optimize utterance generation using long-term\nrewards. Unlike existing RL approaches which formulate the token prediction as\na policy, our method reduces the complexity of the policy optimization by\nlimiting the action space to dialog attributes, thereby making the policy\noptimization more practical and sample efficient. We demonstrate this with\nexperimental and human evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:19:15 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 17:03:04 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Sankar", "Chinnadhurai", ""], ["Ravi", "Sujith", ""]]}, {"id": "1907.02864", "submitter": "Robert M\\\"uller", "authors": "Daniel Elsner, Stefan Langer, Fabian Ritz, Robert M\\\"uller, Steffen\n  Illium", "title": "Deep Neural Baselines for Computational Paralinguistics", "comments": "5 pages, 3 figures; This paper was accepted at INTERSPEECH 2019,\n  Graz, 15-19th September 2019. DOI will be added after publishment of the\n  accepted paper", "journal-ref": "Proc. Interspeech 2019, 2388-2392", "doi": "10.21437/Interspeech.2019-2478", "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting sleepiness from spoken language is an ambitious task, which is\naddressed by the Interspeech 2019 Computational Paralinguistics Challenge\n(ComParE). We propose an end-to-end deep learning approach to detect and\nclassify patterns reflecting sleepiness in the human voice. Our approach is\nbased solely on a moderately complex deep neural network architecture. It may\nbe applied directly on the audio data without requiring any specific feature\nengineering, thus remaining transferable to other audio classification tasks.\nNevertheless, our approach performs similar to state-of-the-art machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:43:55 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Elsner", "Daniel", ""], ["Langer", "Stefan", ""], ["Ritz", "Fabian", ""], ["M\u00fcller", "Robert", ""], ["Illium", "Steffen", ""]]}, {"id": "1907.02884", "submitter": "Giuseppe Castellucci", "authors": "Giuseppe Castellucci, Valentina Bellomaria, Andrea Favalli, Raniero\n  Romagnoli", "title": "Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intent Detection and Slot Filling are two pillar tasks in Spoken Natural\nLanguage Understanding. Common approaches adopt joint Deep Learning\narchitectures in attention-based recurrent frameworks. In this work, we aim at\nexploiting the success of \"recurrence-less\" models for these tasks. We\nintroduce Bert-Joint, i.e., a multi-lingual joint text classification and\nsequence labeling framework. The experimental evaluation over two well-known\nEnglish benchmarks demonstrates the strong performances that can be obtained\nwith this model, even when few annotated data is available. Moreover, we\nannotated a new dataset for the Italian language, and we observed similar\nperformances without the need for changing the model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:11:29 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Castellucci", "Giuseppe", ""], ["Bellomaria", "Valentina", ""], ["Favalli", "Andrea", ""], ["Romagnoli", "Raniero", ""]]}, {"id": "1907.02964", "submitter": "Carlos Brys Mg.", "authors": "Carlos Roberto Brys, Jos\\'e F. Aldana-Montes, David Luis La Red\n  Mart\\'inez", "title": "Un Modelo Ontol\\'ogico para el Gobierno Electr\\'onico", "comments": "10 pages, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision making often requires information that must be Provided with the\nrich data format. Addressing these new requirements appropriately makes it\nnecessary for government agencies to orchestrate large amounts of information\nfrom different sources and formats, to be efficiently delivered through the\ndevices commonly used by people, such as computers, netbooks, tablets and\nsmartphones. To overcome these problems, a model is proposed for the conceptual\nrepresentation of the State's organizational units, seen as georeferenced\nentities of Electronic Government, based on ontologies designed under the\nprinciples of Linked Open Data, which allows the automatic extraction of\ninformation through the machines, which supports the process of governmental\ndecision making and gives citizens full access to find and process through\nmobile technologies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 21:42:26 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Brys", "Carlos Roberto", ""], ["Aldana-Montes", "Jos\u00e9 F.", ""], ["Mart\u00ednez", "David Luis La Red", ""]]}, {"id": "1907.02997", "submitter": "Hussein Alrubaye", "authors": "Hussein Alrubaye, Mohamed Wiem Mkaouer, Ali Ouni", "title": "MigrationMiner: An Automated Detection Tool of Third-Party Java Library\n  Migration at the Method Level", "comments": "Accepted at ICSME 2019. arXiv admin note: text overlap with\n  arXiv:1906.02591", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce, MigrationMiner, an automated tool that detects\ncode migrations performed between Java third-party library. Given a list of\nopen source projects, the tool detects potential library migration code changes\nand collects the specific code fragments in which the developer replaces\nmethods from the retired library with methods from the new library. To support\nthe migration process, MigrationMiner collects the library documentation that\nis associated with every method involved in the migration. We evaluate our tool\non a benchmark of manually validated library migrations. Results show that\nMigrationMiner achieves an accuracy of 100%. A demo video of MigrationMiner is\navailable at https://youtu.be/sAlR1HNetXc.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 18:53:20 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 11:22:51 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Alrubaye", "Hussein", ""], ["Mkaouer", "Mohamed Wiem", ""], ["Ouni", "Ali", ""]]}, {"id": "1907.03007", "submitter": "Dar\\'io Garigliotti", "authors": "Jon Arne B{\\o} Hovda and Dar\\'io Garigliotti and Krisztian Balog", "title": "NeuType: A Simple and Effective Neural Network Approach for Predicting\n  Missing Entity Type Information in Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases store information about the semantic types of entities, which\ncan be utilized in a range of information access tasks. This information,\nhowever, is often incomplete, due to new entities emerging on a daily basis. We\naddress the task of automatically assigning types to entities in a knowledge\nbase from a type taxonomy. Specifically, we present two neural network\narchitectures, which take short entity descriptions and, optionally,\ninformation about related entities as input. Using the DBpedia knowledge base\nfor experimental evaluation, we demonstrate that these simple architectures\nyield significant improvements over the current state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 19:47:10 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Hovda", "Jon Arne B\u00f8", ""], ["Garigliotti", "Dar\u00edo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1907.03020", "submitter": "Shachi Paul", "authors": "Shachi Paul, Rahul Goel, Dilek Hakkani-T\\\"ur", "title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning approaches for building task-oriented dialogue systems\nrequire large conversational datasets with labels to train on. We are\ninterested in building task-oriented dialogue systems from human-human\nconversations, which may be available in ample amounts in existing customer\ncare center logs or can be collected from crowd workers. Annotating these\ndatasets can be prohibitively expensive. Recently multiple annotated\ntask-oriented human-machine dialogue datasets have been released, however their\nannotation schema varies across different collections, even for well-defined\ncategories such as dialogue acts (DAs). We propose a Universal DA schema for\ntask-oriented dialogues and align existing annotated datasets with our schema.\nOur aim is to train a Universal DA tagger (U-DAT) for task-oriented dialogues\nand use it for tagging human-human conversations. We investigate multiple\ndatasets, propose manual and automated approaches for aligning the different\nschema, and present results on a target corpus of human-human dialogues. In\nunsupervised learning experiments we achieve an F1 score of 54.1% on system\nturns in human-human dialogues. In a semi-supervised setup, the F1 score\nincreases to 57.7% which would otherwise require at least 1.7K manually\nannotated turns. For new domains, we show further improvements when unlabeled\nor labeled target domain data is available.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 20:43:30 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Paul", "Shachi", ""], ["Goel", "Rahul", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "1907.03040", "submitter": "Guan-Lin Chao", "authors": "Guan-Lin Chao, Ian Lane", "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional\n  Encoder Representations from Transformer", "comments": "Published in Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important yet rarely tackled problem in dialogue state tracking (DST) is\nscalability for dynamic ontology (e.g., movie, restaurant) and unseen slot\nvalues. We focus on a specific condition, where the ontology is unknown to the\nstate tracker, but the target slot value (except for none and dontcare),\npossibly unseen during training, can be found as word segment in the dialogue\ncontext. Prior approaches often rely on candidate generation from n-gram\nenumeration or slot tagger outputs, which can be inefficient or suffer from\nerror propagation. We propose BERT-DST, an end-to-end dialogue state tracker\nwhich directly extracts slot values from the dialogue context. We use BERT as\ndialogue context encoder whose contextualized language representations are\nsuitable for scalable DST to identify slot values from their semantic context.\nFurthermore, we employ encoder parameter sharing across all slots with two\nadvantages: (1) Number of parameters does not grow linearly with the ontology.\n(2) Language representation knowledge can be transferred among slots. Empirical\nevaluation shows BERT-DST with cross-slot parameter sharing outperforms prior\nwork on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves\ncompetitive performance on the standard DSTC2 and WOZ 2.0 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 22:41:02 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Chao", "Guan-Lin", ""], ["Lane", "Ian", ""]]}, {"id": "1907.03049", "submitter": "Yu-Siang Wang", "authors": "Yu-Siang Wang, Hung-Ting Su, Chen-Hsi Chang, Zhe-Yu Liu, Winston H.\n  Hsu", "title": "Video Question Generation via Cross-Modal Self-Attention Networks\n  Learning", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel task, Video Question Generation (Video QG). A Video QG\nmodel automatically generates questions given a video clip and its\ncorresponding dialogues. Video QG requires a range of skills -- sentence\ncomprehension, temporal relation, the interplay between vision and language,\nand the ability to ask meaningful questions. To address this, we propose a\nnovel semantic rich cross-modal self-attention (SRCMSA) network to aggregate\nthe multi-modal and diverse features. To be more precise, we enhance the video\nframes semantic by integrating the object-level information, and we jointly\nconsider the cross-modal attention for the video question generation task.\nExcitingly, our proposed model remarkably improves the baseline from 7.58 to\n14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a\nnovel path toward understanding the challenging video input and we provide\ndetailed analysis in terms of diversity, which ushers the avenues for future\ninvestigations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 23:47:04 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 19:45:54 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 21:11:03 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wang", "Yu-Siang", ""], ["Su", "Hung-Ting", ""], ["Chang", "Chen-Hsi", ""], ["Liu", "Zhe-Yu", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1907.03060", "submitter": "Atsushi Fujita", "authors": "Aizhan Imankulova, Raj Dabre, Atsushi Fujita, Kenji Imamura", "title": "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer\n  Learning for Low-Resource Neural Machine Translation", "comments": "Accepted at the 17th Machine Translation Summit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper proposes a novel multilingual multistage fine-tuning approach for\nlow-resource neural machine translation (NMT), taking a challenging\nJapanese--Russian pair for benchmarking. Although there are many solutions for\nlow-resource scenarios, such as multilingual NMT and back-translation, we have\nempirically confirmed their limited success when restricted to in-domain data.\nWe therefore propose to exploit out-of-domain data through transfer learning,\nby using it to first train a multilingual NMT model followed by multistage\nfine-tuning on in-domain parallel and back-translated pseudo-parallel data. Our\napproach, which combines domain adaptation, multilingualism, and\nback-translation, helps improve the translation quality by more than 3.7 BLEU\npoints, over a strong baseline, for this extremely low-resource scenario.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 02:14:30 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Imankulova", "Aizhan", ""], ["Dabre", "Raj", ""], ["Fujita", "Atsushi", ""], ["Imamura", "Kenji", ""]]}, {"id": "1907.03064", "submitter": "Raghav Menon", "authors": "Astik Biswas, Raghav Menon, Ewald van der Westhuizen, Thomas Niesler", "title": "Improved low-resource Somali speech recognition by semi-supervised\n  acoustic and language model training", "comments": "5 pages, 6 Tables, 3 figures, 22 references (Accepted at Interspeech\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improvements in automatic speech recognition (ASR) for Somali, a\ncurrently extremely under-resourced language. This forms part of a continuing\nUnited Nations (UN) effort to employ ASR-based keyword spotting systems to\nsupport humanitarian relief programmes in rural Africa. Using just 1.57 hours\nof annotated speech data as a seed corpus, we increase the pool of training\ndata by applying semi-supervised training to 17.55 hours of untranscribed\nspeech. We make use of factorised time-delay neural networks (TDNN-F) for\nacoustic modelling, since these have recently been shown to be effective in\nresource-scarce situations. Three semi-supervised training passes were\nperformed, where the decoded output from each pass was used for acoustic model\ntraining in the subsequent pass. The automatic transcriptions from the best\nperforming pass were used for language model augmentation. To ensure the\nquality of automatic transcriptions, decoder confidence is used as a threshold.\nThe acoustic and language models obtained from the semi-supervised approach\nshow significant improvement in terms of WER and perplexity compared to the\nbaseline. Incorporating the automatically generated transcriptions yields a\n6.55\\% improvement in language model perplexity. The use of 17.55 hour of\nSomali acoustic data in semi-supervised training shows an improvement of 7.74\\%\nrelative over the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 02:53:10 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Biswas", "Astik", ""], ["Menon", "Raghav", ""], ["van der Westhuizen", "Ewald", ""], ["Niesler", "Thomas", ""]]}, {"id": "1907.03070", "submitter": "Hsiang-En Cherng", "authors": "Hsiang-En Cherng, Chia-Hui Chang", "title": "Short Text Conversation Based on Deep Neural Network and Analysis on\n  Evaluation Measures", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Natural Language Processing, Automatic\nquestion-answering system such as Waston, Siri, Alexa, has become one of the\nmost important NLP applications. Nowadays, enterprises try to build automatic\ncustom service chatbots to save human resources and provide a 24-hour customer\nservice. Evaluation of chatbots currently relied greatly on human annotation\nwhich cost a plenty of time. Thus, has initiated a new Short Text Conversation\nsubtask called Dialogue Quality (DQ) and Nugget Detection (ND) which aim to\nautomatically evaluate dialogues generated by chatbots. In this paper, we solve\nthe DQ and ND subtasks by deep neural network. We proposed two models for both\nDQ and ND subtasks which is constructed by hierarchical structure: embedding\nlayer, utterance layer, context layer and memory layer, to hierarchical learn\ndialogue representation from word level, sentence level, context level to long\nrange context level. Furthermore, we apply gating and attention mechanism at\nutterance layer and context layer to improve the performance. We also tried\nBERT to replace embedding layer and utterance layer as sentence representation.\nThe result shows that BERT produced a better utterance representation than\nmulti-stack CNN for both DQ and ND subtasks and outperform other models\nproposed by other researches. The evaluation measures are proposed by , that\nis, NMD, RSNOD for DQ and JSD, RNSS for ND, which is not traditional evaluation\nmeasures such as accuracy, precision, recall and f1-score. Thus, we have done a\nseries of experiments by using traditional evaluation measures and analyze the\nperformance and error.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 03:58:04 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Cherng", "Hsiang-En", ""], ["Chang", "Chia-Hui", ""]]}, {"id": "1907.03110", "submitter": "Mohamed Seghir Hadj Ameur", "authors": "Mohamed Seghir Hadj Ameur, Farid Meziane, Ahmed Guessoum", "title": "ANETAC: Arabic Named Entity Transliteration and Classification Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we make freely accessible ANETAC our English-Arabic named\nentity transliteration and classification dataset that we built from freely\navailable parallel translation corpora. The dataset contains 79,924 instances,\neach instance is a triplet (e, a, c), where e is the English named entity, a is\nits Arabic transliteration and c is its class that can be either a Person, a\nLocation, or an Organization. The ANETAC dataset is mainly aimed for the\nresearchers that are working on Arabic named entity transliteration, but it can\nalso be used for named entity classification purposes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 10:37:18 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ameur", "Mohamed Seghir Hadj", ""], ["Meziane", "Farid", ""], ["Guessoum", "Ahmed", ""]]}, {"id": "1907.03112", "submitter": "Lena Shakurova", "authors": "Lena Shakurova, Beata Nyari, Chao Li, Mihai Rotaru", "title": "Best Practices for Learning Domain-Specific Cross-Lingual Embeddings", "comments": "Proceedings of the 4th Workshop on Representation Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual embeddings aim to represent words in multiple languages in a\nshared vector space by capturing semantic similarities across languages. They\nare a crucial component for scaling tasks to multiple languages by transferring\nknowledge from languages with rich resources to low-resource languages. A\ncommon approach to learning cross-lingual embeddings is to train monolingual\nembeddings separately for each language and learn a linear projection from the\nmonolingual spaces into a shared space, where the mapping relies on a small\nseed dictionary. While there are high-quality generic seed dictionaries and\npre-trained cross-lingual embeddings available for many language pairs, there\nis little research on how they perform on specialised tasks. In this paper, we\ninvestigate the best practices for constructing the seed dictionary for a\nspecific domain. We evaluate the embeddings on the sequence labelling task of\nCurriculum Vitae parsing and show that the size of a bilingual dictionary, the\nfrequency of the dictionary words in the domain corpora and the source of data\n(task-specific vs generic) influence the performance. We also show that the\nless training data is available in the low-resource language, the more the\nconstruction of the bilingual dictionary matters, and demonstrate that some of\nthe choices are crucial in the zero-shot transfer learning case.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 10:45:45 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Shakurova", "Lena", ""], ["Nyari", "Beata", ""], ["Li", "Chao", ""], ["Rotaru", "Mihai", ""]]}, {"id": "1907.03167", "submitter": "Jingcheng Du", "authors": "Jingcheng Du, Chongliang Luo, Qiang Wei, Yong Chen, Cui Tao", "title": "Exploring difference in public perceptions on HPV vaccine between gender\n  groups from Twitter using deep learning", "comments": "This manuscript has been accepted by 2019 KDD Workshop on Applied\n  Data Science for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we proposed a convolutional neural network model for gender\nprediction using English Twitter text as input. Ensemble of proposed model\nachieved an accuracy at 0.8237 on gender prediction and compared favorably with\nthe state-of-the-art performance in a recent author profiling task. We further\nleveraged the trained models to predict the gender labels from an HPV vaccine\nrelated corpus and identified gender difference in public perceptions regarding\nHPV vaccine. The findings are largely consistent with previous survey-based\nstudies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 18:58:54 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Du", "Jingcheng", ""], ["Luo", "Chongliang", ""], ["Wei", "Qiang", ""], ["Chen", "Yong", ""], ["Tao", "Cui", ""]]}, {"id": "1907.03187", "submitter": "Bobak Farzin", "authors": "Bobak Farzin, Piotr Czapla, Jeremy Howard", "title": "Applying a Pre-trained Language Model to Spanish Twitter Humor\n  Prediction", "comments": "IberLEF 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our entry into the HAHA 2019 Challenge placed $3^{rd}$ in the classification\ntask and $2^{nd}$ in the regression task. We describe our system and\ninnovations, as well as comparing our results to a Naive Bayes baseline. A\nlarge Twitter based corpus allowed us to train a language model from scratch\nfocused on Spanish and transfer that knowledge to our competition model. To\novercome the inherent errors in some labels we reduce our class confidence with\nlabel smoothing in the loss function. All the code for our project is included\nin a GitHub repository for easy reference and to enable replication by others.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 21:05:54 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Farzin", "Bobak", ""], ["Czapla", "Piotr", ""], ["Howard", "Jeremy", ""]]}, {"id": "1907.03202", "submitter": "Anupiya Nugaliyadde Mr", "authors": "J.K. Joseph, W.M.T. Chathurika, A. Nugaliyadde, Y. Mallawarachchi", "title": "Evolutionary Algorithm for Sinhala to English Translation", "comments": "The paper was submitted to National Information Technology Conference\n  (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation (MT) is an area in natural language processing, which\nfocus on translating from one language to another. Many approaches ranging from\nstatistical methods to deep learning approaches are used in order to achieve\nMT. However, these methods either require a large number of data or a clear\nunderstanding about the language. Sinhala language has less digital text which\ncould be used to train a deep neural network. Furthermore, Sinhala has complex\nrules therefore, it is harder to create statistical rules in order to apply\nstatistical methods in MT. This research focuses on Sinhala to English\ntranslation using an Evolutionary Algorithm (EA). EA is used to identifying the\ncorrect meaning of Sinhala text and to translate it to English. The Sinhala\ntext is passed to identify the meaning in order to get the correct meaning of\nthe sentence. With the use of the EA the translation is carried out. The\ntranslated text is passed on to grammatically correct the sentence. This has\nshown to achieve accurate results.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:51:28 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Joseph", "J. K.", ""], ["Chathurika", "W. M. T.", ""], ["Nugaliyadde", "A.", ""], ["Mallawarachchi", "Y.", ""]]}, {"id": "1907.03224", "submitter": "Jianying Lin", "authors": "Jianying Lin, Rui Liu, Quanye Jia", "title": "Joint Lifelong Topic Model and Manifold Ranking for Document\n  Summarization", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the manifold ranking method has a significant effect on the ranking of\nunknown data based on known data by using a weighted network, many researchers\nuse the manifold ranking method to solve the document summarization task.\nHowever, their models only consider the original features but ignore the\nsemantic features of sentences when they construct the weighted networks for\nthe manifold ranking method. To solve this problem, we proposed two improved\nmodels based on the manifold ranking method. One is combining the topic model\nand manifold ranking method (JTMMR) to solve the document summarization task.\nThis model not only uses the original feature, but also uses the semantic\nfeature to represent the document, which can improve the accuracy of the\nmanifold ranking method. The other one is combining the lifelong topic model\nand manifold ranking method (JLTMMR). On the basis of the JTMMR, this model\nadds the constraint of knowledge to improve the quality of the topic. At the\nsame time, we also add the constraint of the relationship between documents to\ndig out a better document semantic features. The JTMMR model can improve the\neffect of the manifold ranking method by using the better semantic feature.\nExperiments show that our models can achieve a better result than other\nbaseline models for multi-document summarization task. At the same time, our\nmodels also have a good performance on the single document summarization task.\nAfter combining with a few basic surface features, our model significantly\noutperforms some model based on deep learning in recent years. After that, we\nalso do an exploring work for lifelong machine learning by analyzing the effect\nof adding feedback. Experiments show that the effect of adding feedback to our\nmodel is significant.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 05:41:55 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lin", "Jianying", ""], ["Liu", "Rui", ""], ["Jia", "Quanye", ""]]}, {"id": "1907.03227", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Thien Huu Nguyen, Dejing Dou", "title": "Graph based Neural Networks for Event Factuality Prediction using\n  Syntactic and Semantic Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event factuality prediction (EFP) is the task of assessing the degree to\nwhich an event mentioned in a sentence has happened. For this task, both\nsyntactic and semantic information are crucial to identify the important\ncontext words. The previous work for EFP has only combined these information in\na simple way that cannot fully exploit their coordination. In this work, we\nintroduce a novel graph-based neural network for EFP that can integrate the\nsemantic and syntactic information more effectively. Our experiments\ndemonstrate the advantage of the proposed model for EFP.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 06:02:05 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Nguyen", "Thien Huu", ""], ["Dou", "Dejing", ""]]}, {"id": "1907.03228", "submitter": "Ben Zhou", "authors": "Ben Zhou, Daniel Khashabi, Chen-Tse Tsai, Dan Roth", "title": "Zero-Shot Open Entity Typing as Type-Compatible Grounding", "comments": "16 pages, 5 figures, Accepted at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of entity-typing has been studied predominantly in supervised\nlearning fashion, mostly with task-specific annotations (for coarse types) and\nsometimes with distant supervision (for fine types). While such approaches have\nstrong performance within datasets, they often lack the flexibility to transfer\nacross text genres and to generalize to new type taxonomies. In this work we\npropose a zero-shot entity typing approach that requires no annotated data and\ncan flexibly identify newly defined types. Given a type taxonomy defined as\nBoolean functions of FREEBASE \"types\", we ground a given mention to a set of\ntype-compatible Wikipedia entries and then infer the target mention's types\nusing an inference algorithm that makes use of the types of these entries. We\nevaluate our system on a broad range of datasets, including standard\nfine-grained and coarse-grained entity typing datasets, and also a dataset in\nthe biological domain. Our system is shown to be competitive with\nstate-of-the-art supervised NER systems and outperforms them on out-of-domain\ndatasets. We also show that our system significantly outperforms other\nzero-shot fine typing systems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 06:05:56 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhou", "Ben", ""], ["Khashabi", "Daniel", ""], ["Tsai", "Chen-Tse", ""], ["Roth", "Dan", ""]]}, {"id": "1907.03230", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Thien Huu Nguyen, Dejing Dou", "title": "Improving Cross-Domain Performance for Relation Extraction via\n  Dependency Prediction and Information Flow Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation Extraction (RE) is one of the fundamental tasks in Information\nExtraction and Natural Language Processing. Dependency trees have been shown to\nbe a very useful source of information for this task. The current deep learning\nmodels for relation extraction has mainly exploited this dependency information\nby guiding their computation along the structures of the dependency trees. One\npotential problem with this approach is it might prevent the models from\ncapturing important context information beyond syntactic structures and cause\nthe poor cross-domain generalization. This paper introduces a novel method to\nuse dependency trees in RE for deep learning models that jointly predicts\ndependency and semantics relations. We also propose a new mechanism to control\nthe information flow in the model based on the input entity mentions. Our\nextensive experiments on benchmark datasets show that the proposed model\noutperforms the existing methods for RE significantly.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 06:21:47 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Nguyen", "Thien Huu", ""], ["Dou", "Dejing", ""]]}, {"id": "1907.03233", "submitter": "I-Hung Hsu", "authors": "I-Hung Hsu, Ayush Jaiswal, Premkumar Natarajan", "title": "NIESR: Nuisance Invariant End-to-end Speech Recognition", "comments": "To appear in Proceedings of Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models for speech recognition have achieved great success\nrecently, but they can learn incorrect associations between the target and\nnuisance factors of speech (e.g., speaker identities, background noise, etc.),\nwhich can lead to overfitting. While several methods have been proposed to\ntackle this problem, existing methods incorporate additional information about\nnuisance factors during training to develop invariant models. However,\nenumeration of all possible nuisance factors in speech data and the collection\nof their annotations is difficult and expensive. We present a robust training\nscheme for end-to-end speech recognition that adopts an unsupervised\nadversarial invariance induction framework to separate out essential factors\nfor speech-recognition from nuisances without using any supplementary labels\nbesides the transcriptions. Experiments show that the speech recognition model\ntrained with the proposed training scheme achieves relative improvements of\n5.48% on WSJ0, 6.16% on CHiME3, and 6.61% on TIMIT dataset over the base model.\nAdditionally, the proposed method achieves a relative improvement of 14.44% on\nthe combined WSJ0+CHiME3 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 07:03:09 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Hsu", "I-Hung", ""], ["Jaiswal", "Ayush", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1907.03240", "submitter": "Li Jiacheng", "authors": "Jiacheng Li, Haizhou Shi, Siliang Tang, Fei Wu, Yueting Zhuang", "title": "Informative Visual Storytelling with Cross-modal Rules", "comments": "9 pages, to appear in ACM Multimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3350918", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods in the Visual Storytelling field often suffer from the\nproblem of generating general descriptions, while the image contains a lot of\nmeaningful contents remaining unnoticed. The failure of informative story\ngeneration can be concluded to the model's incompetence of capturing enough\nmeaningful concepts. The categories of these concepts include entities,\nattributes, actions, and events, which are in some cases crucial to grounded\nstorytelling. To solve this problem, we propose a method to mine the\ncross-modal rules to help the model infer these informative concepts given\ncertain visual input. We first build the multimodal transactions by\nconcatenating the CNN activations and the word indices. Then we use the\nassociation rule mining algorithm to mine the cross-modal rules, which will be\nused for the concept inference. With the help of the cross-modal rules, the\ngenerated stories are more grounded and informative. Besides, our proposed\nmethod holds the advantages of interpretation, expandability, and\ntransferability, indicating potential for wider application. Finally, we\nleverage these concepts in our encoder-decoder framework with the attention\nmechanism. We conduct several experiments on the VIsual StoryTelling~(VIST)\ndataset, the results of which demonstrate the effectiveness of our approach in\nterms of both automatic metrics and human evaluation. Additional experiments\nare also conducted showing that our mined cross-modal rules as additional\nknowledge helps the model gain better performance when trained on a small\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 07:41:59 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 05:24:15 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Li", "Jiacheng", ""], ["Shi", "Haizhou", ""], ["Tang", "Siliang", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1907.03399", "submitter": "Takuma Udagawa", "authors": "Takuma Udagawa, Akiko Aizawa", "title": "A Natural Language Corpus of Common Grounding under Continuous and\n  Partially-Observable Context", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common grounding is the process of creating, repairing and updating mutual\nunderstandings, which is a critical aspect of sophisticated human\ncommunication. However, traditional dialogue systems have limited capability of\nestablishing common ground, and we also lack task formulations which introduce\nnatural difficulty in terms of common grounding while enabling easy evaluation\nand analysis of complex models. In this paper, we propose a minimal dialogue\ntask which requires advanced skills of common grounding under continuous and\npartially-observable context. Based on this task formulation, we collected a\nlargescale dataset of 6,760 dialogues which fulfills essential requirements of\nnatural language corpora. Our analysis of the dataset revealed important\nphenomena related to common grounding that need to be considered. Finally, we\nevaluate and analyze baseline neural models on a simple subtask that requires\nrecognition of the created common ground. We show that simple baseline models\nperform decently but leave room for further improvement. Overall, we show that\nour proposed task will be a fundamental testbed where we can train, evaluate,\nand analyze dialogue system's ability for sophisticated common grounding.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 04:19:17 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Udagawa", "Takuma", ""], ["Aizawa", "Akiko", ""]]}, {"id": "1907.03468", "submitter": "Rongxiang Weng", "authors": "Rongxiang Weng, Hao Zhou, Shujian Huang, Lei Li, Yifan Xia and Jiajun\n  Chen", "title": "Correct-and-Memorize: Learning to Translate from Interactive Revisions", "comments": "Accepted at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine translation models are still not on par with human\ntranslators. Previous work takes human interactions into the neural machine\ntranslation process to obtain improved results in target languages. However,\nnot all model-translation errors are equal -- some are critical while others\nare minor. In the meanwhile, the same translation mistakes occur repeatedly in\na similar context. To solve both issues, we propose CAMIT, a novel method for\ntranslating in an interactive environment. Our proposed method works with\ncritical revision instructions, therefore allows human to correct arbitrary\nwords in model-translated sentences. In addition, CAMIT learns from and softly\nmemorizes revision actions based on the context, alleviating the issue of\nrepeating mistakes. Experiments in both ideal and real interactive translation\nsettings demonstrate that our proposed \\method enhances machine translation\nresults significantly while requires fewer revision instructions from human\ncompared to previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 09:09:45 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 05:30:33 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Weng", "Rongxiang", ""], ["Zhou", "Hao", ""], ["Huang", "Shujian", ""], ["Li", "Lei", ""], ["Xia", "Yifan", ""], ["Chen", "Jiajun", ""]]}, {"id": "1907.03491", "submitter": "Pengfei Liu", "authors": "Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, Xuanjing Huang", "title": "Searching for Effective Neural Extractive Summarization: What Works and\n  What's Next", "comments": "Accepted by ACL 2019 (oral); Project homepage:\n  http://pfliu.com/InterpretSum/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have seen remarkable success in the use of deep neural\nnetworks on text summarization.\n  However, there is no clear understanding of \\textit{why} they perform so\nwell, or \\textit{how} they might be improved.\n  In this paper, we seek to better understand how neural extractive\nsummarization systems could benefit from different types of model\narchitectures, transferable knowledge and\n  learning schemas. Additionally, we find an effective way to improve current\nframeworks and achieve the state-of-the-art result on CNN/DailyMail by a large\nmargin based on our\n  observations and analyses. Hopefully, our work could provide more clues for\nfuture research on extractive summarization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 10:17:28 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhong", "Ming", ""], ["Liu", "Pengfei", ""], ["Wang", "Danqing", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1907.03513", "submitter": "Satoshi Akasaki", "authors": "Satoshi Akasaki, Naoki Yoshinaga, Masashi Toyoda", "title": "Early Discovery of Emerging Entities in Microblogs", "comments": "Fixed errata in IJCAI paper. Dataset is available\n  here:http://www.tkl.iis.u-tokyo.ac.jp/~akasaki/ijcai19.html", "journal-ref": "IJCAI2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:13:42 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Akasaki", "Satoshi", ""], ["Yoshinaga", "Naoki", ""], ["Toyoda", "Masashi", ""]]}, {"id": "1907.03533", "submitter": "Rasoul Ramezanian", "authors": "Rasoul Ramezanian", "title": "A Formal Axiomatization of Computation", "comments": "13 page. arXiv admin note: substantial text overlap with\n  arXiv:1906.09873, arXiv:1205.5994", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an axiomatization for the notion of computation. Based on the\nidea of Brouwer choice sequences, we construct a model, denoted by $E$, which\nsatisfies our axioms and $E \\models \\mathrm{ P \\neq NP}$. In other words,\nregarding \"effective computability\" in Brouwer intuitionism viewpoint, we show\n$\\mathrm{ P \\neq NP}$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 04:52:45 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 07:47:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ramezanian", "Rasoul", ""]]}, {"id": "1907.03590", "submitter": "Zelin Dai", "authors": "Zelin Dai, Weitang Liu, Guanhua Zhan", "title": "Multiple Generative Models Ensemble for Knowledge-Driven Proactive\n  Human-Computer Dialogue Agent", "comments": "7 pages, 3 figures submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sequence to sequence models were used to establish an end-to-end\nmulti-turns proactive dialogue generation agent, with the aid of data\naugmentation techniques and variant encoder-decoder structure designs. A\nrank-based ensemble approach was developed for boosting performance. Results\nindicate that our single model, in average, makes an obvious improvement in the\nterms of F1-score and BLEU over the baseline by 18.67% on the DuConv dataset.\nIn particular, the ensemble methods further significantly outperform the\nbaseline by 35.85%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 13:16:07 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 13:37:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Dai", "Zelin", ""], ["Liu", "Weitang", ""], ["Zhan", "Guanhua", ""]]}, {"id": "1907.03663", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Yan Song, Yangqiu Song, Dong Yu", "title": "Knowledge-aware Pronoun Coreference Resolution", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolving pronoun coreference requires knowledge support, especially for\nparticular domains (e.g., medicine). In this paper, we explore how to leverage\ndifferent types of knowledge to better resolve pronoun coreference with a\nneural model. To ensure the generalization ability of our model, we directly\nincorporate knowledge in the format of triplets, which is the most common\nformat of modern knowledge graphs, instead of encoding it with features or\nrules as that in conventional approaches. Moreover, since not all knowledge is\nhelpful in certain contexts, to selectively use them, we propose a knowledge\nattention module, which learns to select and use informative knowledge based on\ncontexts, to enhance our model. Experimental results on two datasets from\ndifferent domains prove the validity and effectiveness of our model, where it\noutperforms state-of-the-art baselines by a large margin. Moreover, since our\nmodel learns to use external knowledge rather than only fitting the training\ndata, it also demonstrates superior performance to baselines in the\ncross-domain setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:01:17 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Hongming", ""], ["Song", "Yan", ""], ["Song", "Yangqiu", ""], ["Yu", "Dong", ""]]}, {"id": "1907.03715", "submitter": "Junmei Zhong", "authors": "Junmei Zhong, William Li", "title": "Predicting Customer Call Intent by Analyzing Phone Call Transcripts\n  based on CNN for Multi-Class Classification", "comments": "12 pages, 4 figures. 8th International Conference on Soft Computing,\n  Artificial Intelligence and Applications (SAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto dealerships receive thousands of calls daily from customers who are\ninterested in sales, service, vendors and jobseekers. With so many calls, it is\nvery important for auto dealers to understand the intent of these calls to\nprovide positive customer experiences that ensure customer satisfaction, deep\ncustomer engagement to boost sales and revenue, and optimum allocation of\nagents or customer service representatives across the business. In this paper,\nwe define the problem of customer phone call intent as a multi-class\nclassification problem stemming from the large database of recorded phone call\ntranscripts. To solve this problem, we develop a convolutional neural network\n(CNN)-based supervised learning model to classify the customer calls into four\nintent categories: sales, service, vendor and jobseeker. Experimental results\nshow that with the thrust of our scalable data labeling method to provide\nsufficient training data, the CNN-based predictive model performs very well on\nlong text classification according to the quantitative metrics of F1-Score,\nprecision, recall, and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 16:39:23 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhong", "Junmei", ""], ["Li", "William", ""]]}, {"id": "1907.03748", "submitter": "Carolin Lawrence", "authors": "Laura Jehl, Carolin Lawrence, Stefan Riezler", "title": "Learning Neural Sequence-to-Sequence Models from Weak Feedback with\n  Bipolar Ramp Loss", "comments": "Transactions of the Association for Computational Linguistics 2019\n  Vol. 7, 233-248. Presented at ACL, Florence, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many machine learning scenarios, supervision by gold labels is not\navailable and consequently neural models cannot be trained directly by maximum\nlikelihood estimation (MLE). In a weak supervision scenario, metric-augmented\nobjectives can be employed to assign feedback to model outputs, which can be\nused to extract a supervision signal for training. We present several\nobjectives for two separate weakly supervised tasks, machine translation and\nsemantic parsing. We show that objectives should actively discourage negative\noutputs in addition to promoting a surrogate gold structure. This notion of\nbipolarity is naturally present in ramp loss objectives, which we adapt to\nneural models. We show that bipolar ramp loss objectives outperform other\nnon-bipolar ramp loss objectives and minimum risk training (MRT) on both weakly\nsupervised tasks, as well as on a supervised machine translation task.\nAdditionally, we introduce a novel token-level ramp loss objective, which is\nable to outperform even the best sequence-level ramp loss on both weakly\nsupervised tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 10:04:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Jehl", "Laura", ""], ["Lawrence", "Carolin", ""], ["Riezler", "Stefan", ""]]}, {"id": "1907.03750", "submitter": "Hongliang Dai", "authors": "Hongliang Dai and Yangqiu Song", "title": "Neural Aspect and Opinion Term Extraction with Mined Rules as Weak\n  Supervision", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of labeled training data is a major bottleneck for neural network based\naspect and opinion term extraction on product reviews. To alleviate this\nproblem, we first propose an algorithm to automatically mine extraction rules\nfrom existing training examples based on dependency parsing results. The mined\nrules are then applied to label a large amount of auxiliary data. Finally, we\nstudy training procedures to train a neural model which can learn from both the\ndata automatically labeled by the rules and a small amount of data accurately\nannotated by human. Experimental results show that although the mined rules\nthemselves do not perform well due to their limited flexibility, the\ncombination of human annotated data and rule labeled auxiliary data can improve\nthe neural model and allow it to achieve performance better than or comparable\nwith the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 12:59:04 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Dai", "Hongliang", ""], ["Song", "Yangqiu", ""]]}, {"id": "1907.03752", "submitter": "Vukosi Marivate", "authors": "Vukosi Marivate, Tshephisho Sefara", "title": "Improving short text classification through global augmentation methods", "comments": "Final version published in CD-MAKE 2020: Machine Learning and\n  Knowledge Extraction pp 385-399", "journal-ref": "Machine Learning and Knowledge Extraction. CD-MAKE 2020. Lecture\n  Notes in Computer Science, vol 12279 (2020)", "doi": "10.1007/978-3-030-57321-8_21", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of different approaches to text augmentation. To do this\nwe use 3 datasets that include social media and formal text in the form of news\narticles. Our goal is to provide insights for practitioners and researchers on\nmaking choices for augmentation for classification use cases. We observe that\nWord2vec-based augmentation is a viable option when one does not have access to\na formal synonym model (like WordNet-based augmentation). The use of\n\\emph{mixup} further improves performance of all text based augmentations and\nreduces the effects of overfitting on a tested deep learning model. Round-trip\ntranslation with a translation service proves to be harder to use due to cost\nand as such is less accessible for both normal and low resource use-cases.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 18:05:12 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 14:41:38 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Marivate", "Vukosi", ""], ["Sefara", "Tshephisho", ""]]}, {"id": "1907.03871", "submitter": "Fatima Alkhawaldeh", "authors": "Fatima T. AL-Khawaldeh", "title": "A Study of the Effect of Resolving Negation and Sentiment Analysis in\n  Recognizing Text Entailment for Arabic", "comments": "5 pages", "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT) ISSN: 2221-0741 Vol. 5, No. 7, 124-128, 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing the entailment relation showed that its influence to extract the\nsemantic inferences in wide-ranging natural language processing domains (text\nsummarization, question answering, etc.) and enhanced the results of their\noutput. For Arabic language, few attempts concerns with Arabic entailment\nproblem. This paper aims to increase the entailment accuracy for Arabic texts\nby resolving negation of the text-hypothesis pair and determining the polarity\nof the text-hypothesis pair whether it is Positive, Negative or Neutral. It is\nnoticed that the absence of negation detection feature gives inaccurate results\nwhen detecting the entailment relation since the negation revers the truth. The\nnegation words are considered stop words and removed from the text-hypothesis\npair which may lead wrong entailment decision. Another case not solved\npreviously, it is impossible that the positive text entails negative text and\nvice versa. In this paper, in order to classify the text-hypothesis pair\npolarity, a sentiment analysis tool is used. We show that analyzing the\npolarity of the text-hypothesis pair increases the entailment accuracy. to\nevaluate our approach we used a dataset for Arabic textual entailment (ArbTEDS)\nconsisted of 618 text-hypothesis pairs and showed that the Arabic entailment\naccuracy is increased by resolving negation for entailment relation and\nanalyzing the polarity of the text-hypothesis pair.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:29:46 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["AL-Khawaldeh", "Fatima T.", ""]]}, {"id": "1907.03885", "submitter": "Hamidreza Ghader", "authors": "Hamidreza Ghader, Christof Monz", "title": "An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation\n  Architectures", "comments": "To be presented at Machine Translation Summit 2019 (MTSUMMIT XVII),\n  Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Earlier approaches indirectly studied the information captured by the hidden\nstates of recurrent and non-recurrent neural machine translation models by\nfeeding them into different classifiers. In this paper, we look at the encoder\nhidden states of both transformer and recurrent machine translation models from\nthe nearest neighbors perspective. We investigate to what extent the nearest\nneighbors share information with the underlying word embeddings as well as\nrelated WordNet entries. Additionally, we study the underlying syntactic\nstructure of the nearest neighbors to shed light on the role of syntactic\nsimilarities in bringing the neighbors together. We compare transformer and\nrecurrent models in a more intrinsic way in terms of capturing lexical\nsemantics and syntactic structures, in contrast to extrinsic approaches used by\nprevious works. In agreement with the extrinsic evaluations in the earlier\nworks, our experimental results show that transformers are superior in\ncapturing lexical semantics, but not necessarily better in capturing the\nunderlying syntax. Additionally, we show that the backward recurrent layer in a\nrecurrent model learns more about the semantics of words, whereas the forward\nrecurrent layer encodes more context.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 21:39:29 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ghader", "Hamidreza", ""], ["Monz", "Christof", ""]]}, {"id": "1907.03920", "submitter": "Tyler Gray", "authors": "Tyler J. Gray, Christopher M. Danforth, and Peter Sheridan Dodds", "title": "Hahahahaha, Duuuuude, Yeeessss!: A two-parameter characterization of\n  stretchable words and the dynamics of mistypings and misspellings", "comments": "18 pages, 18 figures, and 9 tables. Online appendices at\n  http://compstorylab.org/stretchablewords/", "journal-ref": null, "doi": "10.1371/journal.pone.0232938", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stretched words like `heellllp' or `heyyyyy' are a regular feature of spoken\nlanguage, often used to emphasize or exaggerate the underlying meaning of the\nroot word. While stretched words are rarely found in formal written language\nand dictionaries, they are prevalent within social media. In this paper, we\nexamine the frequency distributions of `stretchable words' found in roughly 100\nbillion tweets authored over an 8 year period. We introduce two central\nparameters, `balance' and `stretch', that capture their main characteristics,\nand explore their dynamics by creating visual tools we call `balance plots' and\n`spelling trees'. We discuss how the tools and methods we develop here could be\nused to study the statistical patterns of mistypings and misspellings, along\nwith the potential applications in augmenting dictionaries, improving language\nprocessing, and in any area where sequence construction matters, such as\ngenetics.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 00:44:23 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gray", "Tyler J.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1907.03927", "submitter": "Makoto Morishita", "authors": "Soichiro Murakami, Makoto Morishita, Tsutomu Hirao, Masaaki Nagata", "title": "NTT's Machine Translation Systems for WMT19 Robustness Task", "comments": "submitted to WMT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes NTT's submission to the WMT19 robustness task. This task\nmainly focuses on translating noisy text (e.g., posts on Twitter), which\npresents different difficulties from typical translation tasks such as news.\nOur submission combined techniques including utilization of a synthetic corpus,\ndomain adaptation, and a placeholder mechanism, which significantly improved\nover the previous baseline. Experimental results revealed the placeholder\nmechanism, which temporarily replaces the non-standard tokens including emojis\nand emoticons with special placeholder tokens during translation, improves\ntranslation accuracy even with noisy texts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:11:23 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Murakami", "Soichiro", ""], ["Morishita", "Makoto", ""], ["Hirao", "Tsutomu", ""], ["Nagata", "Masaaki", ""]]}, {"id": "1907.03950", "submitter": "Drew A. Hudson", "authors": "Drew A. Hudson and Christopher D. Manning", "title": "Learning by Abstraction: The Neural State Machine", "comments": "Published as a conference paper at NeurIPS 2019 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model's strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:08:41 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 17:14:03 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 09:33:51 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 10:02:05 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Hudson", "Drew A.", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1907.03969", "submitter": "Yo Nakawake", "authors": "Yo Nakawake, Kosuke Sato", "title": "Systematic quantitative analyses reveal the folk-zoological knowledge\n  embedded in folktales", "comments": "This document is a preprint. Accepted version of the paper is\n  available at https://www.nature.com/articles/s41599-019-0375-x", "journal-ref": "Palgrave Commun 5, 161 (2019)", "doi": "10.1057/s41599-019-0375-x", "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultural learning is a unique human capacity essential for a wide range of\nadaptations. Researchers have argued that folktales have the pedagogical\nfunction of transmitting the essential information for the environment. The\nmost important knowledge for foraging and pastoral society is folk-zoological\nknowledge, such as the predator-prey relationship among wild animals, or\nbetween wild and domesticated animals. Here, we analysed the descriptions of\nthe 382 animal folktales using the natural language processing method and\ndescriptive statistics listed in a worldwide tale-type index\n(Aarne-Thompson-Uther type index). Our analyses suggested that first, the\npredator-prey relationship frequently appeared in a co-occurrent animal pair\nwithin a folktale (e.g., cat and mouse or wolf and pig), and second, the motif\nof 'deception', describing the antagonistic behaviour among animals, appeared\nrelatively higher in 'wild and domestic animals' and 'wild animals' than other\ntypes. Furthermore, the motif of 'deception' appeared more frequently in pairs,\ncorresponding to the predator-prey relationship. These results corresponded\nwith the hypothesis that the combination of animal characters and what happens\nin stories represented relationships in the real world. The present study\ndemonstrated that the combination of quantitative methods and qualitative data\nbroaden our understanding of the evolutionary aspects of human cultures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:40:13 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 15:27:00 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Nakawake", "Yo", ""], ["Sato", "Kosuke", ""]]}, {"id": "1907.03975", "submitter": "Mingyu Ma", "authors": "Mingyu Derek Ma, Kevin K. Bowden, Jiaqi Wu, Wen Cui, Marilyn Walker", "title": "Implicit Discourse Relation Identification for Open-domain Dialogues", "comments": "To appear in Proceedings of the 57th Annual Meeting of the\n  Association for Computational Linguistics (ACL2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relation identification has been an active area of research for\nmany years, and the challenge of identifying implicit relations remains largely\nan unsolved task, especially in the context of an open-domain dialogue system.\nPrevious work primarily relies on a corpora of formal text which is inherently\nnon-dialogic, i.e., news and journals. This data however is not suitable to\nhandle the nuances of informal dialogue nor is it capable of navigating the\nplethora of valid topics present in open-domain dialogue. In this paper, we\ndesigned a novel discourse relation identification pipeline specifically tuned\nfor open-domain dialogue systems. We firstly propose a method to automatically\nextract the implicit discourse relation argument pairs and labels from a\ndataset of dialogic turns, resulting in a novel corpus of discourse relation\npairs; the first of its kind to attempt to identify the discourse relations\nconnecting the dialogic turns in open-domain discourse. Moreover, we have taken\nthe first steps to leverage the dialogue features unique to our task to further\nimprove the identification of such relations by performing feature ablation and\nincorporating dialogue features to enhance the state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:58:46 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ma", "Mingyu Derek", ""], ["Bowden", "Kevin K.", ""], ["Wu", "Jiaqi", ""], ["Cui", "Wen", ""], ["Walker", "Marilyn", ""]]}, {"id": "1907.04072", "submitter": "Udit Arora", "authors": "Udit Arora, William Scott Paka, Tanmoy Chakraborty", "title": "Multitask Learning for Blackmarket Tweet Detection", "comments": "4 pages, IEEE/ACM International Conference on Social Networks\n  Analysis and Mining (ASONAM) 2019", "journal-ref": null, "doi": "10.1145/3341161.3342934", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social media platforms have made the world more connected than ever\nbefore, thereby making it easier for everyone to spread their content across a\nwide variety of audiences. Twitter is one such popular platform where people\npublish tweets to spread their messages to everyone. Twitter allows users to\nRetweet other users' tweets in order to broadcast it to their network. The more\nretweets a particular tweet gets, the faster it spreads. This creates\nincentives for people to obtain artificial growth in the reach of their tweets\nby using certain blackmarket services to gain inorganic appraisals for their\ncontent.\n  In this paper, we attempt to detect such tweets that have been posted on\nthese blackmarket services in order to gain artificially boosted retweets. We\nuse a multitask learning framework to leverage soft parameter sharing between a\nclassification and a regression based task on separate inputs. This allows us\nto effectively detect tweets that have been posted to these blackmarket\nservices, achieving an F1-score of 0.89 when classifying tweets as blackmarket\nor genuine.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 10:42:33 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Arora", "Udit", ""], ["Paka", "William Scott", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "1907.04105", "submitter": "Vivian Silva", "authors": "Vivian S. Silva, Andr\\'e Freitas, Siegfried Handschuh", "title": "On the Semantic Interpretability of Artificial Intelligence Models", "comments": "17 pages, 4 figures. Submitted to AI Magazine on August, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence models are becoming increasingly more powerful and\naccurate, supporting or even replacing humans' decision making. But with\nincreased power and accuracy also comes higher complexity, making it hard for\nusers to understand how the model works and what the reasons behind its\npredictions are. Humans must explain and justify their decisions, and so do the\nAI models supporting them in this process, making semantic interpretability an\nemerging field of study. In this work, we look at interpretability from a\nbroader point of view, going beyond the machine learning scope and covering\ndifferent AI fields such as distributional semantics and fuzzy logic, among\nothers. We examine and classify the models according to their nature and also\nbased on how they introduce interpretability features, analyzing how each\napproach affects the final users and pointing to gaps that still need to be\naddressed to provide more human-centered interpretability solutions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 12:01:35 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Silva", "Vivian S.", ""], ["Freitas", "Andr\u00e9", ""], ["Handschuh", "Siegfried", ""]]}, {"id": "1907.04126", "submitter": "Gavin Abercrombie", "authors": "Gavin Abercrombie and Riza Batista-Navarro", "title": "Sentiment and position-taking analysis of parliamentary debates: A\n  systematic literature review", "comments": "Journal of Computational Social Science (2020)", "journal-ref": null, "doi": "10.1007/s42001-019-00060-w", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parliamentary and legislative debate transcripts provide access to\ninformation concerning the opinions, positions and policy preferences of\nelected politicians. They attract attention from researchers from a wide\nvariety of backgrounds, from political and social sciences to computer science.\nAs a result, the problem of automatic sentiment and position-taking analysis\nhas been tackled from different perspectives, using varying approaches and\nmethods, and with relatively little collaboration or cross-pollination of\nideas. The existing research is scattered across publications from various\nfields and venues. In this article we present the results of a systematic\nliterature review of 61 studies, all of which address the automatic analysis of\nthe sentiment and opinions expressed and positions taken by speakers in\nparliamentary (and other legislative) debates. In this review, we discuss the\navailable research with regard to the aims and objectives of the researchers\nwho work on these problems, the automatic analysis tasks they undertake, and\nthe approaches and methods they use. We conclude by summarizing their findings,\ndiscussing the challenges of applying computational analysis to parliamentary\ndebates, and suggesting possible avenues for further research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:06:31 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 10:10:32 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Abercrombie", "Gavin", ""], ["Batista-Navarro", "Riza", ""]]}, {"id": "1907.04149", "submitter": "Fatima Alkhawaldeh", "authors": "Fatima T. AL-Khawaldeh", "title": "Answer Extraction for Why Arabic Questions Answering Systems: EWAQ", "comments": "5 pages", "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT) ISSN: 2221-0741 Vol. 5, No. 5, 82-86, 2015", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing amount of web information, questions answering systems\nbecomes very important to allow users to access to direct answers for their\nrequests. This paper presents an Arabic Questions Answering Systems based on\nentailment metrics. The type of questions which this paper focuses on is why\nquestions. There are many reasons lead us to develop this system: generally,\nthe lack of Arabic Questions Answering Systems and scarcity Arabic Questions\nAnswering Systems which focus on why questions. The goal of the proposed system\nin this research is to extract answers from re-ranked retrieved passages which\nare retrieved by search engines. This system extracts the answer only to why\nquestions. This system is called by EWAQ: Entailment based Why Arabic Questions\nAnswering. Each answer is scored with entailment metrics and ranked according\nto their scores in order to determine the most possible correct answer. EWAQ is\ncompared with search engines: yahoo, google and ask.com, the well-established\nweb-based Questions Answering systems, using manual test set. In EWAQ\nexperiments, it is showed that the accuracy is increased by implementing the\ntextual entailment in re-raking the retrieved relevant passages by search\nengines and deciding the correct answer. The obtained results show that using\nentailment based similarity can help significantly to tackle the why Answer\nExtraction module in Arabic language.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:25:28 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["AL-Khawaldeh", "Fatima T.", ""]]}, {"id": "1907.04152", "submitter": "Adam Dobrakowski", "authors": "Adam Gabriel Dobrakowski, Agnieszka Mykowiecka, Ma{\\l}gorzata\n  Marciniak, Wojciech Jaworski, Przemys{\\l}aw Biecek", "title": "Interpretable Segmentation of Medical Free-Text Records Based on Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it true that patients with similar conditions get similar diagnoses? In\nthis paper we show NLP methods and a unique corpus of documents to validate\nthis claim. We (1) introduce a method for representation of medical visits\nbased on free-text descriptions recorded by doctors, (2) introduce a new method\nfor clustering of patients' visits and (3) present an~application of the\nproposed method on a corpus of 100,000 visits. With the proposed method we\nobtained stable and separated segments of visits which were positively\nvalidated against final medical diagnoses. We show how the presented algorithm\nmay be used to aid doctors during their practice.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:22:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 11:13:32 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 10:09:48 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Dobrakowski", "Adam Gabriel", ""], ["Mykowiecka", "Agnieszka", ""], ["Marciniak", "Ma\u0142gorzata", ""], ["Jaworski", "Wojciech", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "1907.04197", "submitter": "Desmond Ong", "authors": "Zhengxuan Wu, Xiyu Zhang, Tan Zhi-Xuan, Jamil Zaki, Desmond C. Ong", "title": "Attending to Emotional Narratives", "comments": "Accepted at IEEE Affective Computing and Intelligent Interaction\n  (ACII) 2019; 6 pages + 1 page ref; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms in deep neural networks have achieved excellent\nperformance on sequence-prediction tasks. Here, we show that these\nrecently-proposed attention-based mechanisms---in particular, the Transformer\nwith its parallelizable self-attention layers, and the Memory Fusion Network\nwith attention across modalities and time---also generalize well to multimodal\ntime-series emotion recognition. Using a recently-introduced dataset of\nemotional autobiographical narratives, we adapt and apply these two attention\nmechanisms to predict emotional valence over time. Our models perform extremely\nwell, in some cases reaching a performance comparable with human raters. We end\nwith a discussion of the implications of attention mechanisms to affective\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 03:50:43 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Zhang", "Xiyu", ""], ["Zhi-Xuan", "Tan", ""], ["Zaki", "Jamil", ""], ["Ong", "Desmond C.", ""]]}, {"id": "1907.04198", "submitter": "Jennifer Gago", "authors": "Jennifer J. Gago, Valentina Vasco, Bartek {\\L}ukawski, Ugo Pattacini,\n  Vadim Tikhanoff, Juan G. Victores, Carlos Balaguer", "title": "Sequence-to-Sequence Natural Language to Humanoid Robot Sign Language", "comments": "13 pages, 8 figures, conference", "journal-ref": null, "doi": "10.11128/arep.58", "report-no": null, "categories": "cs.RO cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a study on natural language to sign language translation\nwith human-robot interaction application purposes. By means of the presented\nmethodology, the humanoid robot TEO is expected to represent Spanish sign\nlanguage automatically by converting text into movements, thanks to the\nperformance of neural networks. Natural language to sign language translation\npresents several challenges to developers, such as the discordance between the\nlength of input and output data and the use of non-manual markers. Therefore,\nneural networks and, consequently, sequence-to-sequence models, are selected as\na data-driven system to avoid traditional expert system approaches or temporal\ndependencies limitations that lead to limited or too complex translation\nsystems. To achieve these objectives, it is necessary to find a way to perform\nhuman skeleton acquisition in order to collect the signing input data. OpenPose\nand skeletonRetriever are proposed for this purpose and a 3D sensor\nspecification study is developed to select the best acquisition hardware.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 14:41:50 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Gago", "Jennifer J.", ""], ["Vasco", "Valentina", ""], ["\u0141ukawski", "Bartek", ""], ["Pattacini", "Ugo", ""], ["Tikhanoff", "Vadim", ""], ["Victores", "Juan G.", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1907.04211", "submitter": "Genaro J. Martinez", "authors": "Sergio J. Martinez, Ivan M. Mendoza, Genaro J. Martinez, Shigeru\n  Ninagawa", "title": "Universal One-Dimensional Cellular Automata Derived for Turing Machines\n  and its Dynamical Behaviour", "comments": "18 pages, 8 tables, 3 figures.\n  https://www.oldcitypublishing.com/journals/ijuc-home/ijuc-issue-contents/ijuc-volume-14-number-2-2019/ijuc-14-2-p-121-138/", "journal-ref": "International Journal of Unconventional Computing 14(2) pages\n  121-138, 2019", "doi": null, "report-no": null, "categories": "nlin.CG cs.CL cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universality in cellular automata theory is a central problem studied and\ndeveloped from their origins by John von Neumann. In this paper, we present an\nalgorithm where any Turing machine can be converted to one-dimensional cellular\nautomaton with a 2-linear time and display its spatial dynamics. Three\nparticular Turing machines are converted in three universal one-dimensional\ncellular automata, they are: binary sum, rule 110 and a universal reversible\nTuring machine.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:12:13 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Martinez", "Sergio J.", ""], ["Mendoza", "Ivan M.", ""], ["Martinez", "Genaro J.", ""], ["Ninagawa", "Shigeru", ""]]}, {"id": "1907.04224", "submitter": "Yonatan Belinkov", "authors": "Yonatan Belinkov, Ahmed Ali, James Glass", "title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic\n  Speech Recognition", "comments": "Corrected dataset statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end neural network systems for automatic speech recognition (ASR) are\ntrained from acoustic features to text transcriptions. In contrast to modular\nASR systems, which contain separately-trained components for acoustic modeling,\npronunciation lexicon, and language modeling, the end-to-end paradigm is both\nconceptually simpler and has the potential benefit of training the entire\nsystem on the end task. However, such neural network models are more opaque: it\nis not clear how to interpret the role of different parts of the network and\nwhat information it learns during training. In this paper, we analyze the\nlearned internal representations in an end-to-end ASR model. We evaluate the\nrepresentation quality in terms of several classification tasks, comparing\nphonemes and graphemes, as well as different articulatory features. We study\ntwo languages (English and Arabic) and three datasets, finding remarkable\nconsistency in how different properties are represented in different layers of\nthe deep neural network.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 14:59:16 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 20:05:34 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Belinkov", "Yonatan", ""], ["Ali", "Ahmed", ""], ["Glass", "James", ""]]}, {"id": "1907.04265", "submitter": "Janaki Sheth", "authors": "Janaki Sheth, Ariel Tankus, Michelle Tran, Nader Pouratian, Itzhak\n  Fried and William Speier", "title": "Translating neural signals to text using a Brain-Machine Interface", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interfaces (BCI) help patients with faltering communication\nabilities due to neurodegenerative diseases produce text or speech output by\ndirect neural processing. However, practical implementation of such a system\nhas proven difficult due to limitations in speed, accuracy, and\ngeneralizability of the existing interfaces. To this end, we aim to create a\nBCI system that decodes text directly from neural signals. We implement a\nframework that initially isolates frequency bands in the input signal\nencapsulating differential information regarding production of various phonemic\nclasses. These bands then form a feature set that feeds into an LSTM which\ndiscerns at each time point probability distributions across all phonemes\nuttered by a subject. Finally, these probabilities are fed into a particle\nfiltering algorithm which incorporates prior knowledge of the English language\nto output text corresponding to the decoded word. Performance of this model on\ndata obtained from six patients shows encouragingly high levels of accuracy at\nspeeds and bit rates significantly higher than existing BCI communication\nsystems. Further, in producing an output, our network abstains from\nconstraining the reconstructed word to be from a given bag-of-words, unlike\nprevious studies. The success of our proposed approach, offers promise for the\nemployment of a BCI interface by patients in unfettered, naturalistic\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:07:38 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Sheth", "Janaki", ""], ["Tankus", "Ariel", ""], ["Tran", "Michelle", ""], ["Pouratian", "Nader", ""], ["Fried", "Itzhak", ""], ["Speier", "William", ""]]}, {"id": "1907.04286", "submitter": "William Kearns", "authors": "William R. Kearns, Wilson Lau, Jason A. Thomas", "title": "UW-BHI at MEDIQA 2019: An Analysis of Representation Methods for Medical\n  Natural Language Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in distributed language modeling have led to large\nperformance increases on a variety of natural language processing (NLP) tasks.\nHowever, it is not well understood how these methods may be augmented by\nknowledge-based approaches. This paper compares the performance and internal\nrepresentation of an Enhanced Sequential Inference Model (ESIM) between three\nexperimental conditions based on the representation method: Bidirectional\nEncoder Representations from Transformers (BERT), Embeddings of Semantic\nPredications (ESP), or Cui2Vec. The methods were evaluated on the Medical\nNatural Language Inference (MedNLI) subtask of the MEDIQA 2019 shared task.\nThis task relied heavily on semantic understanding and thus served as a\nsuitable evaluation set for the comparison of these representation methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:47:50 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kearns", "William R.", ""], ["Lau", "Wilson", ""], ["Thomas", "Jason A.", ""]]}, {"id": "1907.04307", "submitter": "Daniel Cer", "authors": "Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah\n  Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung,\n  Brian Strope and Ray Kurzweil", "title": "Multilingual Universal Sentence Encoder for Semantic Retrieval", "comments": "6 pages, 6 tables, 2 listings, and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two pre-trained retrieval focused multilingual sentence encoding\nmodels, respectively based on the Transformer and CNN model architectures. The\nmodels embed text from 16 languages into a single semantic space using a\nmulti-task trained dual-encoder that learns tied representations using\ntranslation based bridge tasks (Chidambaram al., 2018). The models provide\nperformance that is competitive with the state-of-the-art on: semantic\nretrieval (SR), translation pair bitext retrieval (BR) and retrieval question\nanswering (ReQA). On English transfer learning tasks, our sentence-level\nembeddings approach, and in some cases exceed, the performance of monolingual,\nEnglish only, sentence embedding models. Our models are made available for\ndownload on TensorFlow Hub.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:46:17 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yang", "Yinfei", ""], ["Cer", "Daniel", ""], ["Ahmad", "Amin", ""], ["Guo", "Mandy", ""], ["Law", "Jax", ""], ["Constant", "Noah", ""], ["Abrego", "Gustavo Hernandez", ""], ["Yuan", "Steve", ""], ["Tar", "Chris", ""], ["Sung", "Yun-Hsuan", ""], ["Strope", "Brian", ""], ["Kurzweil", "Ray", ""]]}, {"id": "1907.04347", "submitter": "Daniel Fried", "authors": "Daniel Fried, Nikita Kitaev, Dan Klein", "title": "Cross-Domain Generalization of Neural Constituency Parsers", "comments": "ACL 2019. DF and NK contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural parsers obtain state-of-the-art results on benchmark treebanks for\nconstituency parsing -- but to what degree do they generalize to other domains?\nWe present three results about the generalization of neural parsers in a\nzero-shot setting: training on trees from one corpus and evaluating on\nout-of-domain corpora. First, neural and non-neural parsers generalize\ncomparably to new domains. Second, incorporating pre-trained encoder\nrepresentations into neural parsers substantially improves their performance\nacross all domains, but does not give a larger relative improvement for\nout-of-domain treebanks. Finally, despite the rich input representations they\nlearn, neural parsers still benefit from structured output prediction of output\ntrees, yielding higher exact match accuracy and stronger generalization both to\nlarger text spans and to out-of-domain corpora. We analyze generalization on\nEnglish and Chinese corpora, and in the process obtain state-of-the-art parsing\nresults for the Brown, Genia, and English Web treebanks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:05:59 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Fried", "Daniel", ""], ["Kitaev", "Nikita", ""], ["Klein", "Dan", ""]]}, {"id": "1907.04355", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, David Harwath, James Glass", "title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition", "comments": "Accepted to Interspeech 2019. 4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning aims to reduce the amount of data required to excel at a\nnew task by re-using the knowledge acquired from learning other related tasks.\nThis paper proposes a novel transfer learning scenario, which distills robust\nphonetic features from grounding models that are trained to tell whether a pair\nof image and speech are semantically correlated, without using any textual\ntranscripts. As semantics of speech are largely determined by its lexical\ncontent, grounding models learn to preserve phonetic information while\ndisregarding uncorrelated factors, such as speaker and channel. To study the\nproperties of features distilled from different layers, we use them as input\nseparately to train multiple speech recognition models. Empirical results\ndemonstrate that layers closer to input retain more phonetic information, while\nfollowing layers exhibit greater invariance to domain shift. Moreover, while\nmost previous studies include training data for speech recognition for feature\nextractor training, our grounding models are not trained on any of those data,\nindicating more universal applicability to new domains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:23:32 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Harwath", "David", ""], ["Glass", "James", ""]]}, {"id": "1907.04378", "submitter": "Shuang Ma", "authors": "Shuang Ma, Daniel McDuff, Yale Song", "title": "M3D-GAN: Multi-Modal Multi-Domain Translation with Universal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks have led to significant advances in\ncross-modal/domain translation. However, typically these networks are designed\nfor a specific task (e.g., dialogue generation or image synthesis, but not\nboth). We present a unified model, M3D-GAN, that can translate across a wide\nrange of modalities (e.g., text, image, and speech) and domains (e.g.,\nattributes in images or emotions in speech). Our model consists of modality\nsubnets that convert data from different modalities into unified\nrepresentations, and a unified computing body where data from different\nmodalities share the same network architecture. We introduce a universal\nattention module that is jointly trained with the whole network and learns to\nencode a large range of domain information into a highly structured latent\nspace. We use this to control synthesis in novel ways, such as producing\ndiverse realistic pictures from a sketch or varying the emotion of synthesized\nspeech. We evaluate our approach on extensive benchmark tasks, including\nimage-to-image, text-to-image, image captioning, text-to-speech, speech\nrecognition, and machine translation. Our results show state-of-the-art\nperformance on some of the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:33:01 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Ma", "Shuang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "1907.04380", "submitter": "Adam Poliak", "authors": "Yonatan Belinkov, Adam Poliak, Stuart M. Shieber, Benjamin Van Durme,\n  Alexander M. Rush", "title": "Don't Take the Premise for Granted: Mitigating Artifacts in Natural\n  Language Inference", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI) datasets often contain hypothesis-only\nbiases---artifacts that allow models to achieve non-trivial performance without\nlearning whether a premise entails a hypothesis. We propose two probabilistic\nmethods to build models that are more robust to such biases and better transfer\nacross datasets. In contrast to standard approaches to NLI, our methods predict\nthe probability of a premise given a hypothesis and NLI label, discouraging\nmodels from ignoring the premise. We evaluate our methods on synthetic and\nexisting NLI datasets by training on datasets containing biases and testing on\ndatasets containing no (or different) hypothesis-only biases. Our results\nindicate that these methods can make NLI models more robust to dataset-specific\nartifacts, transferring better than a baseline architecture in 9 out of 12 NLI\ndatasets. Additionally, we provide an extensive analysis of the interplay of\nour methods with known biases in NLI datasets, as well as the effects of\nencouraging models to ignore biases and fine-tuning on target datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:40:47 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Belinkov", "Yonatan", ""], ["Poliak", "Adam", ""], ["Shieber", "Stuart M.", ""], ["Van Durme", "Benjamin", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1907.04389", "submitter": "Adam Poliak", "authors": "Yonatan Belinkov, Adam Poliak, Stuart M. Shieber, Benjamin Van Durme,\n  Alexander M. Rush", "title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language\n  Inference", "comments": "StarSem 2019 - The Eighth Joint Conference on Lexical and\n  Computational Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular Natural Language Inference (NLI) datasets have been shown to be\ntainted by hypothesis-only biases. Adversarial learning may help models ignore\nsensitive biases and spurious correlations in data. We evaluate whether\nadversarial learning can be used in NLI to encourage models to learn\nrepresentations free of hypothesis-only biases. Our analyses indicate that the\nrepresentations learned via adversarial learning may be less biased, with only\nsmall drops in NLI accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:04:24 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Belinkov", "Yonatan", ""], ["Poliak", "Adam", ""], ["Shieber", "Stuart M.", ""], ["Van Durme", "Benjamin", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1907.04407", "submitter": "Mohammad Heydari", "authors": "Mohammad Heydari", "title": "Sentiment Analysis Challenges in Persian Language", "comments": "the paper structure must be completely modify from scratch", "journal-ref": null, "doi": "10.13140/RG.2.2.29169.43363", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth in data on the internet requires a data mining process to\nreach a decision to support insight. The Persian language has strong potential\nfor deep research in any aspect of natural language processing, especially\nsentimental analysis approach. Thousands of websites and blogs updates and\nmodifies by Persian users around the world that contains millions of Persian\ncontext. This range of application requires a comprehensive structured\nframework to extract beneficial information for helping enterprises to enhance\ntheir business and initiate a customer-centric management process by producing\neffective recommender systems. Sentimental analysis is an intelligent approach\nfor extracting useful information from huge amounts of data to help an\nenterprise for smart management process. In this road, machine learning and\ndeep learning techniques will become very helpful but there is the number of\nchallenges which are face to them. This paper tried to present and assert the\nmost important challenges of sentimental analysis in the Persian language. This\nlanguage is an Indo-European language which spoken by over 110 million people\naround the world and is an official language in Iran, Tajikistan, and\nAfghanistan. Its also widely used in Uzbekistan, Pakistan and Turkish by order.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:46:37 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 20:50:05 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Heydari", "Mohammad", ""]]}, {"id": "1907.04433", "submitter": "Aston Zhang", "authors": "Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin, Xingjian\n  Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang, Zhi\n  Zhang, Zhongyue Zhang, Shuai Zheng, Yi Zhu", "title": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural\n  Language Processing", "comments": null, "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-7", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GluonCV and GluonNLP, the deep learning toolkits for computer\nvision and natural language processing based on Apache MXNet (incubating).\nThese toolkits provide state-of-the-art pre-trained models, training scripts,\nand training logs, to facilitate rapid prototyping and promote reproducible\nresearch. We also provide modular APIs with flexible building blocks to enable\nefficient customization. Leveraging the MXNet ecosystem, the deep learning\nmodels in GluonCV and GluonNLP can be deployed onto a variety of platforms with\ndifferent programming languages. The Apache 2.0 license has been adopted by\nGluonCV and GluonNLP to allow for software distribution, modification, and\nusage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 21:59:44 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 00:54:42 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Guo", "Jian", ""], ["He", "He", ""], ["He", "Tong", ""], ["Lausen", "Leonard", ""], ["Li", "Mu", ""], ["Lin", "Haibin", ""], ["Shi", "Xingjian", ""], ["Wang", "Chenguang", ""], ["Xie", "Junyuan", ""], ["Zha", "Sheng", ""], ["Zhang", "Aston", ""], ["Zhang", "Hang", ""], ["Zhang", "Zhi", ""], ["Zhang", "Zhongyue", ""], ["Zheng", "Shuai", ""], ["Zhu", "Yi", ""]]}, {"id": "1907.04448", "submitter": "Ron J Weiss", "authors": "Yu Zhang, Ron J. Weiss, Heiga Zen, Yonghui Wu, Zhifeng Chen, RJ\n  Skerry-Ryan, Ye Jia, Andrew Rosenberg, Bhuvana Ramabhadran", "title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech\n  Synthesis and Cross-Language Voice Cloning", "comments": "5 pages, submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multispeaker, multilingual text-to-speech (TTS) synthesis model\nbased on Tacotron that is able to produce high quality speech in multiple\nlanguages. Moreover, the model is able to transfer voices across languages,\ne.g. synthesize fluent Spanish speech using an English speaker's voice, without\ntraining on any bilingual or parallel examples. Such transfer works across\ndistantly related languages, e.g. English and Mandarin.\n  Critical to achieving this result are: 1. using a phonemic input\nrepresentation to encourage sharing of model capacity across languages, and 2.\nincorporating an adversarial loss term to encourage the model to disentangle\nits representation of speaker identity (which is perfectly correlated with\nlanguage in the training data) from the speech content. Further scaling up the\nmodel by training on multiple speakers of each language, and incorporating an\nautoencoding input to help stabilize attention during training, results in a\nmodel which can be used to consistently synthesize intelligible speech for\ntraining speakers in all languages seen during training, and in native or\nforeign accents.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:43:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 15:03:15 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhang", "Yu", ""], ["Weiss", "Ron J.", ""], ["Zen", "Heiga", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""], ["Skerry-Ryan", "RJ", ""], ["Jia", "Ye", ""], ["Rosenberg", "Andrew", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1907.04462", "submitter": "Kexin Zhao", "authors": "Jihyun Park, Kexin Zhao, Kainan Peng, Wei Ping", "title": "Multi-Speaker End-to-End Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we extend ClariNet (Ping et al., 2019), a fully end-to-end\nspeech synthesis model (i.e., text-to-wave), to generate high-fidelity speech\nfrom multiple speakers. To model the unique characteristic of different voices,\nlow dimensional trainable speaker embeddings are shared across each component\nof ClariNet and trained together with the rest of the model. We demonstrate\nthat the multi-speaker ClariNet outperforms state-of-the-art systems in terms\nof naturalness, because the whole model is jointly optimized in an end-to-end\nmanner.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 23:53:39 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Park", "Jihyun", ""], ["Zhao", "Kexin", ""], ["Peng", "Kainan", ""], ["Ping", "Wei", ""]]}, {"id": "1907.04492", "submitter": "Juan Manuel P\\'erez", "authors": "Juan Manuel P\\'erez, Dami\\'an E. Aleman, Santiago N. Kalinowski,\n  Agust\\'in Gravano", "title": "Exploiting user-frequency information for mining regionalisms from\n  Social Media texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of detecting regionalisms (expressions or words used in certain\nregions) has traditionally relied on the use of questionnaires and surveys, and\nhas also heavily depended on the expertise and intuition of the surveyor. The\nirruption of Social Media and its microblogging services has produced an\nunprecedented wealth of content, mainly informal text generated by users,\nopening new opportunities for linguists to extend their studies of language\nvariation. Previous work on automatic detection of regionalisms depended mostly\non word frequencies. In this work, we present a novel metric based on\nInformation Theory that incorporates user frequency. We tested this metric on a\ncorpus of Argentinian Spanish tweets in two ways: via manual annotation of the\nrelevance of the retrieved terms, and also as a feature selection method for\ngeolocation of users. In either case, our metric outperformed other techniques\nbased solely in word frequency, suggesting that measuring the amount of users\nthat produce a word is informative. This tool has helped lexicographers\ndiscover several unregistered words of Argentinian Spanish, as well as\ndifferent meanings assigned to registered words.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 02:43:37 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["P\u00e9rez", "Juan Manuel", ""], ["Aleman", "Dami\u00e1n E.", ""], ["Kalinowski", "Santiago N.", ""], ["Gravano", "Agust\u00edn", ""]]}, {"id": "1907.04613", "submitter": "Jind\\v{r}ich Libovick\\'y", "authors": "Jind\\v{r}ich Libovick\\'y", "title": "Neural Networks as Explicit Word-Based Rules", "comments": "3 pages; extended abstract at BlackboxNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters of convolutional networks used in computer vision are often\nvisualized as image patches that maximize the response of the filter. We use\nthe same approach to interpret weight matrices in simple architectures for\nnatural language processing tasks. We interpret a convolutional network for\nsentiment classification as word-based rules. Using the rule, we recover the\nperformance of the original model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 10:50:22 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""]]}, {"id": "1907.04618", "submitter": "Franck Burlot", "authors": "Franck Burlot", "title": "Lingua Custodia at WMT'19: Attempts to Control Terminology", "comments": "Proceedings of the Fourth Conference on Machine Translation (WMT),\n  pages72-79, Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Lingua Custodia's submission to the WMT'19 news shared\ntask for German-to-French on the topic of the EU elections. We report\nexperiments on the adaptation of the terminology of a machine translation\nsystem to a specific topic, aimed at providing more accurate translations of\nspecific entities like political parties and person names, given that the\nshared task provided no in-domain training parallel data dealing with the\nrestricted topic. Our primary submission to the shared task uses\nbacktranslation generated with a type of decoding allowing the insertion of\nconstraints in the output in order to guarantee the correct translation of\nspecific terms that are not necessarily observed in the data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:14:53 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Burlot", "Franck", ""]]}, {"id": "1907.04670", "submitter": "Larkin Liu", "authors": "Larkin Liu, Yu-Chung Lin, Joshua Reid", "title": "Improving the Performance of the LSTM and HMM Model via Hybridization", "comments": "Working Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Language models based on deep neural networks and traditional stochastic\nmodelling have become both highly functional and effective in recent times. In\nthis work, a general survey into the two types of language modelling is\nconducted. We investigate the effectiveness of the Hidden Markov Model (HMM),\nand the Long Short-Term Memory Model (LSTM). We analyze the hidden state\nstructures common to both models, and present an analysis on structural\nsimilarity of the hidden states, common to both HMM's and LSTM's. We compare\nthe LSTM's predictive accuracy and hidden state output with respect to the HMM\nfor a varying number of hidden states. In this work, we justify that the less\ncomplex HMM can serve as an appropriate approximation of the LSTM model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:12:51 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 21:05:24 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 10:56:06 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 13:16:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Larkin", ""], ["Lin", "Yu-Chung", ""], ["Reid", "Joshua", ""]]}, {"id": "1907.04743", "submitter": "Daniel Korzekwa", "authors": "Daniel Korzekwa, Roberto Barra-Chicote, Bozena Kostek, Thomas Drugman,\n  Mateusz Lajszczak", "title": "Interpretable Deep Learning Model for the Detection and Reconstruction\n  of Dysarthric Speech", "comments": "5 pages, 5 figures, Accepted for Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a novel approach for the detection and reconstruction of\ndysarthric speech. The encoder-decoder model factorizes speech into a\nlow-dimensional latent space and encoding of the input text. We showed that the\nlatent space conveys interpretable characteristics of dysarthria, such as\nintelligibility and fluency of speech. MUSHRA perceptual test demonstrated that\nthe adaptation of the latent space let the model generate speech of improved\nfluency. The multi-task supervised approach for predicting both the probability\nof dysarthric speech and the mel-spectrogram helps improve the detection of\ndysarthria with higher accuracy. This is thanks to a low-dimensional latent\nspace of the auto-encoder as opposed to directly predicting dysarthria from a\nhighly dimensional mel-spectrogram.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:20:02 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Korzekwa", "Daniel", ""], ["Barra-Chicote", "Roberto", ""], ["Kostek", "Bozena", ""], ["Drugman", "Thomas", ""], ["Lajszczak", "Mateusz", ""]]}, {"id": "1907.04744", "submitter": "Junjie Huang", "authors": "Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun\n  Liu, Maosong Sun", "title": "Modeling Semantic Compositionality with Sememe Knowledge", "comments": "To appear at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic compositionality (SC) refers to the phenomenon that the meaning of a\ncomplex linguistic unit can be composed of the meanings of its constituents.\nMost related works focus on using complicated compositionality functions to\nmodel SC while few works consider external knowledge in models. In this paper,\nwe verify the effectiveness of sememes, the minimum semantic units of human\nlanguages, in modeling SC by a confirmatory experiment. Furthermore, we make\nthe first attempt to incorporate sememe knowledge into SC models, and employ\nthe sememeincorporated models in learning representations of multiword\nexpressions, a typical task of SC. In experiments, we implement our models by\nincorporating knowledge from a famous sememe knowledge base HowNet and perform\nboth intrinsic and extrinsic evaluations. Experimental results show that our\nmodels achieve significant performance boost as compared to the baseline\nmethods without considering sememe knowledge. We further conduct quantitative\nanalysis and case studies to demonstrate the effectiveness of applying sememe\nknowledge in modeling SC. All the code and data of this paper can be obtained\non https://github.com/thunlp/Sememe-SC.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:20:43 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Qi", "Fanchao", ""], ["Huang", "Junjie", ""], ["Yang", "Chenghao", ""], ["Liu", "Zhiyuan", ""], ["Chen", "Xiao", ""], ["Liu", "Qun", ""], ["Sun", "Maosong", ""]]}, {"id": "1907.04780", "submitter": "Noah Constant", "authors": "Amin Ahmad, Noah Constant, Yinfei Yang, Daniel Cer", "title": "ReQA: An Evaluation for End-to-End Answer Retrieval Models", "comments": null, "journal-ref": null, "doi": "10.18653/v1/D19-5819", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular QA benchmarks like SQuAD have driven progress on the task of\nidentifying answer spans within a specific passage, with models now surpassing\nhuman performance. However, retrieving relevant answers from a huge corpus of\ndocuments is still a challenging problem, and places different requirements on\nthe model architecture. There is growing interest in developing scalable answer\nretrieval models trained end-to-end, bypassing the typical document retrieval\nstep. In this paper, we introduce Retrieval Question-Answering (ReQA), a\nbenchmark for evaluating large-scale sentence-level answer retrieval models. We\nestablish baselines using both neural encoding models as well as classical\ninformation retrieval techniques. We release our evaluation code to encourage\nfurther work on this challenging task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:16:36 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 03:14:32 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Ahmad", "Amin", ""], ["Constant", "Noah", ""], ["Yang", "Yinfei", ""], ["Cer", "Daniel", ""]]}, {"id": "1907.04829", "submitter": "Kevin Clark", "authors": "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D.\n  Manning, Quoc V. Le", "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It can be challenging to train multi-task neural networks that outperform or\neven match their single-task counterparts. To help address this, we propose\nusing knowledge distillation where single-task models teach a multi-task model.\nWe enhance this training with teacher annealing, a novel method that gradually\ntransitions the model from distillation to supervised learning, helping the\nmulti-task model surpass its single-task teachers. We evaluate our approach by\nmulti-task fine-tuning BERT on the GLUE benchmark. Our method consistently\nimproves over standard single-task and multi-task training.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:14:47 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Clark", "Kevin", ""], ["Luong", "Minh-Thang", ""], ["Khandelwal", "Urvashi", ""], ["Manning", "Christopher D.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1907.04882", "submitter": "Xiaodong Cui", "authors": "Xiaodong Cui and Michael Picheny", "title": "Acoustic Model Optimization Based On Evolutionary Stochastic Gradient\n  Descent with Anchors for Automatic Speech Recognition", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary stochastic gradient descent (ESGD) was proposed as a\npopulation-based approach that combines the merits of gradient-aware and\ngradient-free optimization algorithms for superior overall optimization\nperformance. In this paper we investigate a variant of ESGD for optimization of\nacoustic models for automatic speech recognition (ASR). In this variant, we\nassume the existence of a well-trained acoustic model and use it as an anchor\nin the parent population whose good \"gene\" will propagate in the evolution to\nthe offsprings. We propose an ESGD algorithm leveraging the anchor models such\nthat it guarantees the best fitness of the population will never degrade from\nthe anchor model. Experiments on 50-hour Broadcast News (BN50) and 300-hour\nSwitchboard (SWB300) show that the ESGD with anchors can further improve the\nloss and ASR performance over the existing well-trained acoustic models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 18:38:44 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Cui", "Xiaodong", ""], ["Picheny", "Michael", ""]]}, {"id": "1907.04887", "submitter": "Xiaodong Cui", "authors": "Khoi-Nguyen C. Mac, Xiaodong Cui, Wei Zhang and Michael Picheny", "title": "Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for\n  Automatic Speech Recognition", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic speech recognition (ASR), wideband (WB) and narrowband (NB)\nspeech signals with different sampling rates typically use separate acoustic\nmodels. Therefore mixed-bandwidth (MB) acoustic modeling has important\npractical values for ASR system deployment. In this paper, we extensively\ninvestigate large-scale MB deep neural network acoustic modeling for ASR using\n1,150 hours of WB data and 2,300 hours of NB data. We study various MB\nstrategies including downsampling, upsampling and bandwidth extension for MB\nacoustic modeling and evaluate their performance on 8 diverse WB and NB test\nsets from various application domains. To deal with the large amounts of\ntraining data, distributed training is carried out on multiple GPUs using\nsynchronous data parallelism.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 18:52:11 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Mac", "Khoi-Nguyen C.", ""], ["Cui", "Xiaodong", ""], ["Zhang", "Wei", ""], ["Picheny", "Michael", ""]]}, {"id": "1907.04905", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti and Arthur Gola de Paula", "title": "Development of email classifier in Brazilian Portuguese using feature\n  selection for automatic response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic email categorization is an important application of text\nclassification. We study the automatic reply of email business messages in\nBrazilian Portuguese. We present a novel corpus containing messages from a real\napplication, and baseline categorization experiments using Naive Bayes and\nsupport Vector Machines. We then discuss the effect of lemmatization and the\nrole of part-of-speech tagging filtering on precision and recall. Support\nVector Machines classification coupled with nonlemmatized selection of verbs,\nnouns and adjectives was the best approach, with 87.3% maximum accuracy.\nStraightforward lemmatization in Portuguese led to the lowest classification\nresults in the group, with 85.3% and 81.7% precision in SVM and Naive Bayes\nrespectively. Thus, while lemmatization reduced precision and recall,\npart-of-speech filtering improved overall results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 03:24:53 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Bonatti", "Rogerio", ""], ["de Paula", "Arthur Gola", ""]]}, {"id": "1907.04907", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Francisco J. R. Ruiz, and David M. Blei", "title": "Topic Modeling in Embedding Spaces", "comments": "Code can be found at https://github.com/adjidieng/ETM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling analyzes documents to learn meaningful patterns of words.\nHowever, existing topic models fail to learn interpretable topics when working\nwith large and heavy-tailed vocabularies. To this end, we develop the Embedded\nTopic Model (ETM), a generative model of documents that marries traditional\ntopic models with word embeddings. In particular, it models each word with a\ncategorical distribution whose natural parameter is the inner product between a\nword embedding and an embedding of its assigned topic. To fit the ETM, we\ndevelop an efficient amortized variational inference algorithm. The ETM\ndiscovers interpretable topics even with large vocabularies that include rare\nwords and stop words. It outperforms existing document models, such as latent\nDirichlet allocation (LDA), in terms of both topic quality and predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 03:50:57 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Dieng", "Adji B.", ""], ["Ruiz", "Francisco J. R.", ""], ["Blei", "David M.", ""]]}, {"id": "1907.04916", "submitter": "Felix Weninger", "authors": "Felix Weninger, Jes\\'us Andr\\'es-Ferrer, Xinwei Li, Puming Zhan", "title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence\n  ASR", "comments": "To appear in INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence (seq2seq) based ASR systems have shown state-of-the-art\nperformances while having clear advantages in terms of simplicity. However,\ncomparisons are mostly done on speaker independent (SI) ASR systems, though\nspeaker adapted conventional systems are commonly used in practice for\nimproving robustness to speaker and environment variations. In this paper, we\napply speaker adaptation to seq2seq models with the goal of matching the\nperformance of conventional ASR adaptation. Specifically, we investigate\nKullback-Leibler divergence (KLD) as well as Linear Hidden Network (LHN) based\nadaptation for seq2seq ASR, using different amounts (up to 20 hours) of\nadaptation data per speaker. Our SI models are trained on large amounts of\ndictation data and achieve state-of-the-art results. We obtained 25% relative\nword error rate (WER) improvement with KLD adaptation of the seq2seq model vs.\n18.7% gain from acoustic model adaptation in the conventional system. We also\nshow that the WER of the seq2seq model decreases log-linearly with the amount\nof adaptation data. Finally, we analyze adaptation based on the minimum WER\ncriterion and adapting the language model (LM) for score fusion with the\nspeaker adapted seq2seq model, which result in further improvements of the\nseq2seq system performance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:09:40 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Weninger", "Felix", ""], ["Andr\u00e9s-Ferrer", "Jes\u00fas", ""], ["Li", "Xinwei", ""], ["Zhan", "Puming", ""]]}, {"id": "1907.04944", "submitter": "Nishant Subramani", "authors": "Nishant Subramani, Samuel R. Bowman, Kyunghyun Cho", "title": "Can Unconditional Language Models Recover Arbitrary Sentences?", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based generative language models like ELMo and BERT can work\neffectively as general purpose sentence encoders in text classification without\nfurther fine-tuning. Is it possible to adapt them in a similar way for use as\ngeneral-purpose decoders? For this to be possible, it would need to be the case\nthat for any target sentence of interest, there is some continuous\nrepresentation that can be passed to the language model to cause it to\nreproduce that sentence. We set aside the difficult problem of designing an\nencoder that can produce such representations and, instead, ask directly\nwhether such representations exist at all. To do this, we introduce a pair of\neffective, complementary methods for feeding representations into pretrained\nunconditional language models and a corresponding set of methods to map\nsentences into and out of this representation space, the reparametrized\nsentence space. We then investigate the conditions under which a language model\ncan be made to generate a sentence through the identification of a point in\nsuch a space and find that it is possible to recover arbitrary sentences nearly\nperfectly with language models and representations of moderate size without\nmodifying any model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 22:13:48 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 23:03:30 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Subramani", "Nishant", ""], ["Bowman", "Samuel R.", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1907.04954", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen, Khalid Alnajjar", "title": "Modelling the Socialization of Creative Agents in a Master-Apprentice\n  Setting: The Case of Movie Title Puns", "comments": null, "journal-ref": "Proceedings of the 10th International Conference on Computational\n  Creativity. Grace, K., Cook, M., Ventura, D. & Maher, M. L. (eds.).\n  Association for Computational Creativity, p. 266-273 (2019)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents work on modelling the social psychological aspect of\nsocialization in the case of a computationally creative master-apprentice\nsystem. In each master-apprentice pair, the master, a genetic algorithm, is\nseen as a parent for its apprentice, which is an NMT based sequence-to-sequence\nmodel. The effect of different parenting styles on the creative output of each\npair is in the focus of this study. This approach brings a novel view point to\ncomputational social creativity, which has mainly focused in the past on\ncomputationally creative agents being on a socially equal level, whereas our\napproach studies the phenomenon in the context of a social hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 23:12:17 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Alnajjar", "Khalid", ""]]}, {"id": "1907.04957", "submitter": "Jesse Thomason", "authors": "Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer", "title": "Vision-and-Dialog Navigation", "comments": "Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots navigating in human environments should use language to ask for\nassistance and be able to understand human responses. To study this challenge,\nwe introduce Cooperative Vision-and-Dialog Navigation, a dataset of over 2k\nembodied, human-human dialogs situated in simulated, photorealistic home\nenvironments. The Navigator asks questions to their partner, the Oracle, who\nhas privileged access to the best next steps the Navigator should take\naccording to a shortest path planner. To train agents that search an\nenvironment for a goal location, we define the Navigation from Dialog History\ntask. An agent, given a target object and a dialog history between humans\ncooperating to find that object, must infer navigation actions towards the goal\nin unexplored environments. We establish an initial, multi-modal\nsequence-to-sequence model and demonstrate that looking farther back in the\ndialog history improves performance. Sourcecode and a live interface demo can\nbe found at https://cvdn.dev/\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 23:41:46 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 04:07:17 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 02:09:00 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Thomason", "Jesse", ""], ["Murray", "Michael", ""], ["Cakmak", "Maya", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1907.05019", "submitter": "Orhan Firat", "authors": "Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin\n  Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry,\n  Wolfgang Macherey, Zhifeng Chen, Yonghui Wu", "title": "Massively Multilingual Neural Machine Translation in the Wild: Findings\n  and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce our efforts towards building a universal neural machine\ntranslation (NMT) system capable of translating between any language pair. We\nset a milestone towards this goal by building a single massively multilingual\nNMT model handling 103 languages trained on over 25 billion examples. Our\nsystem demonstrates effective transfer learning ability, significantly\nimproving translation quality of low-resource languages, while keeping\nhigh-resource language translation quality on-par with competitive bilingual\nbaselines. We provide in-depth analysis of various aspects of model building\nthat are crucial to achieving quality and practicality in universal NMT. While\nwe prototype a high-quality universal translation system, our extensive\nempirical analysis exposes issues that need to be further addressed, and we\nsuggest directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 06:47:30 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Arivazhagan", "Naveen", ""], ["Bapna", "Ankur", ""], ["Firat", "Orhan", ""], ["Lepikhin", "Dmitry", ""], ["Johnson", "Melvin", ""], ["Krikun", "Maxim", ""], ["Chen", "Mia Xu", ""], ["Cao", "Yuan", ""], ["Foster", "George", ""], ["Cherry", "Colin", ""], ["Macherey", "Wolfgang", ""], ["Chen", "Zhifeng", ""], ["Wu", "Yonghui", ""]]}, {"id": "1907.05048", "submitter": "Corina Dima", "authors": "Corina Dima, Dani\\\"el de Kok, Neele Witte, Erhard Hinrichs", "title": "No Word is an Island -- A Transformation Weighting Model for Semantic\n  Composition", "comments": "Author final version of article accepted for publication in TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composition models of distributional semantics are used to construct phrase\nrepresentations from the representations of their words. Composition models are\ntypically situated on two ends of a spectrum. They either have a small number\nof parameters but compose all phrases in the same way, or they perform\nword-specific compositions at the cost of a far larger number of parameters. In\nthis paper we propose transformation weighting (TransWeight), a composition\nmodel that consistently outperforms existing models on nominal compounds,\nadjective-noun phrases and adverb-adjective phrases in English, German and\nDutch. TransWeight drastically reduces the number of parameters needed compared\nto the best model in the literature by composing similar words in the same way.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:41:53 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Dima", "Corina", ""], ["de Kok", "Dani\u00ebl", ""], ["Witte", "Neele", ""], ["Hinrichs", "Erhard", ""]]}, {"id": "1907.05084", "submitter": "David Schlangen", "authors": "Nikolai Ilinykh, Sina Zarrie{\\ss}, David Schlangen", "title": "MeetUp! A Corpus of Joint Activity Dialogues in a Visual Environment", "comments": "In Proceedings of the 23rd Workshop on the Semantics and Pragmatics\n  of Dialogue (semdial / LondonLogue), London, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building computer systems that can converse about their visual environment is\none of the oldest concerns of research in Artificial Intelligence and\nComputational Linguistics (see, for example, Winograd's 1972 SHRDLU system).\nOnly recently, however, have methods from computer vision and natural language\nprocessing become powerful enough to make this vision seem more attainable.\nPushed especially by developments in computer vision, many data sets and\ncollection environments have recently been published that bring together verbal\ninteraction and visual processing. Here, we argue that these datasets tend to\noversimplify the dialogue part, and we propose a task---MeetUp!---that requires\nboth visual and conversational grounding, and that makes stronger demands on\nrepresentations of the discourse. MeetUp! is a two-player coordination game\nwhere players move in a visual environment, with the objective of finding each\nother. To do so, they must talk about what they see, and achieve mutual\nunderstanding. We describe a data collection and show that the resulting\ndialogues indeed exhibit the dialogue phenomena of interest, while also\nchallenging the language & vision aspect.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:06:20 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ilinykh", "Nikolai", ""], ["Zarrie\u00df", "Sina", ""], ["Schlangen", "David", ""]]}, {"id": "1907.05092", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin, Zhaoyang Zeng, Bei Liu,\n  Jianlong Fu, and Alexander Hauptmann", "title": "Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events\n  in Videos", "comments": "Winner solution in CVPR 2019 Activitynet Dense Video Captioning\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual reasoning is essential to understand events in long untrimmed\nvideos. In this work, we systematically explore different captioning models\nwith various contexts for the dense-captioning events in video task, which aims\nto generate captions for different events in the untrimmed video. We propose\nfive types of contexts as well as two categories of event captioning models,\nand evaluate their contributions for event captioning from both accuracy and\ndiversity aspects. The proposed captioning models are plugged into our pipeline\nsystem for the dense video captioning challenge. The overall system achieves\nthe state-of-the-art performance on the dense-captioning events in video task\nwith 9.91 METEOR score on the challenge testing set.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:29:04 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Chen", "Shizhe", ""], ["Song", "Yuqing", ""], ["Zhao", "Yida", ""], ["Jin", "Qin", ""], ["Zeng", "Zhaoyang", ""], ["Liu", "Bei", ""], ["Fu", "Jianlong", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1907.05190", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, Stefan Riezler", "title": "Self-Regulated Interactive Sequence-to-Sequence Learning", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all types of supervision signals are created equal: Different types of\nfeedback have different costs and effects on learning. We show how\nself-regulation strategies that decide when to ask for which kind of feedback\nfrom a teacher (or from oneself) can be cast as a learning-to-learn problem\nleading to improved cost-aware sequence-to-sequence learning. In experiments on\ninteractive neural machine translation, we find that the self-regulator\ndiscovers an $\\epsilon$-greedy strategy for the optimal cost-quality trade-off\nby mixing different feedback types including corrections, error markups, and\nself-supervision. Furthermore, we demonstrate its robustness under domain shift\nand identify it as a promising alternative to active learning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 13:42:46 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 15:15:13 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kreutzer", "Julia", ""], ["Riezler", "Stefan", ""]]}, {"id": "1907.05242", "submitter": "Guillaume Lample", "authors": "Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato,\n  Ludovic Denoyer, Herv\\'e J\\'egou", "title": "Large Memory Layers with Product Keys", "comments": "Advances in Neural Information Processing Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a structured memory which can be easily integrated into\na neural network. The memory is very large by design and significantly\nincreases the capacity of the architecture, by up to a billion parameters with\na negligible computational overhead. Its design and access pattern is based on\nproduct keys, which enable fast and exact nearest neighbor search. The ability\nto increase the number of parameters while keeping the same computational\nbudget lets the overall system strike a better trade-off between prediction\naccuracy and computation efficiency both at training and test time. This memory\nlayer allows us to tackle very large scale language modeling tasks. In our\nexperiments we consider a dataset with up to 30 billion words, and we plug our\nmemory layer in a state-of-the-art transformer-based architecture. In\nparticular, we found that a memory augmented model with only 12 layers\noutperforms a baseline transformer model with 24 layers, while being twice\nfaster at inference time. We release our code for reproducibility purposes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:52:12 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 03:46:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lample", "Guillaume", ""], ["Sablayrolles", "Alexandre", ""], ["Ranzato", "Marc'Aurelio", ""], ["Denoyer", "Ludovic", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1907.05333", "submitter": "Jun Kuang", "authors": "Jun Kuang, Yixin Cao, Jianbing Zheng, Xiangnan He, Ming Gao, Aoying\n  Zhou", "title": "Improving Neural Relation Extraction with Implicit Mutual Relations", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/ICDE48307.2020.00093", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) aims at extracting the relation between two entities\nfrom the text corpora. It is a crucial task for Knowledge Graph (KG)\nconstruction. Most existing methods predict the relation between an entity pair\nby learning the relation from the training sentences, which contain the\ntargeted entity pair. In contrast to existing distant supervision approaches\nthat suffer from insufficient training corpora to extract relations, our\nproposal of mining implicit mutual relation from the massive unlabeled corpora\ntransfers the semantic information of entity pairs into the RE model, which is\nmore expressive and semantically plausible. After constructing an entity\nproximity graph based on the implicit mutual relations, we preserve the\nsemantic relations of entity pairs via embedding each vertex of the graph into\na low-dimensional space. As a result, we can easily and flexibly integrate the\nimplicit mutual relations and other entity information, such as entity types,\ninto the existing RE methods.\n  Our experimental results on a New York Times and another Google Distant\nSupervision datasets suggest that our proposed neural RE framework provides a\npromising improvement for the RE task, and significantly outperforms the\nstate-of-the-art methods. Moreover, the component for mining implicit mutual\nrelations is so flexible that can help to improve the performance of both\nCNN-based and RNN-based RE models significant.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 02:16:11 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kuang", "Jun", ""], ["Cao", "Yixin", ""], ["Zheng", "Jianbing", ""], ["He", "Xiangnan", ""], ["Gao", "Ming", ""], ["Zhou", "Aoying", ""]]}, {"id": "1907.05336", "submitter": "Mojtaba Nayyeri", "authors": "Mojtaba Nayyeri, Xiaotian Zhou, Sahar Vahdati, Hamed Shariat Yazdi,\n  Jens Lehmann", "title": "Adaptive Margin Ranking Loss for Knowledge Graph Embeddings via a\n  Correntropy Objective Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Translation-based embedding models have gained significant attention in link\nprediction tasks for knowledge graphs. TransE is the primary model among\ntranslation-based embeddings and is well-known for its low complexity and high\nefficiency. Therefore, most of the earlier works have modified the score\nfunction of the TransE approach in order to improve the performance of link\nprediction tasks. Nevertheless, proven theoretically and experimentally, the\nperformance of TransE strongly depends on the loss function. Margin Ranking\nLoss (MRL) has been one of the earlier loss functions which is widely used for\ntraining TransE. However, the scores of positive triples are not necessarily\nenforced to be sufficiently small to fulfill the translation from head to tail\nby using relation vector (original assumption of TransE). To tackle this\nproblem, several loss functions have been proposed recently by adding upper\nbounds and lower bounds to the scores of positive and negative samples.\nAlthough highly effective, previously developed models suffer from an expansion\nin search space for a selection of the hyperparameters (in particular the upper\nand lower bounds of scores) on which the performance of the translation-based\nmodels is highly dependent. In this paper, we propose a new loss function\ndubbed Adaptive Margin Loss (AML) for training translation-based embedding\nmodels. The formulation of the proposed loss function enables an adaptive and\nautomated adjustment of the margin during the learning process. Therefore,\ninstead of obtaining two values (upper bound and lower bound), only the center\nof a margin needs to be determined. During learning, the margin is expanded\nautomatically until it converges. In our experiments on a set of standard\nbenchmark datasets including Freebase and WordNet, the effectiveness of AML is\nconfirmed for training TransE on link prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 12:32:40 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Nayyeri", "Mojtaba", ""], ["Zhou", "Xiaotian", ""], ["Vahdati", "Sahar", ""], ["Yazdi", "Hamed Shariat", ""], ["Lehmann", "Jens", ""]]}, {"id": "1907.05337", "submitter": "Izhak Shafran", "authors": "Laurent El Shafey, Hagen Soltau, Izhak Shafran", "title": "Joint Speech Recognition and Speaker Diarization via Sequence\n  Transduction", "comments": null, "journal-ref": "Proc. Interspeech 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech applications dealing with conversations require not only recognizing\nthe spoken words, but also determining who spoke when. The task of assigning\nwords to speakers is typically addressed by merging the outputs of two separate\nsystems, namely, an automatic speech recognition (ASR) system and a speaker\ndiarization (SD) system. The two systems are trained independently with\ndifferent objective functions. Often the SD systems operate directly on the\nacoustics and are not constrained to respect word boundaries and this\ndeficiency is overcome in an ad hoc manner. Motivated by recent advances in\nsequence to sequence learning, we propose a novel approach to tackle the two\ntasks by a joint ASR and SD system using a recurrent neural network transducer.\nOur approach utilizes both linguistic and acoustic cues to infer speaker roles,\nas opposed to typical SD systems, which only use acoustic cues. We evaluated\nthe performance of our approach on a large corpus of medical conversations\nbetween physicians and patients. Compared to a competitive conventional\nbaseline, our approach improves word-level diarization error rate from 15.8% to\n2.2%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 00:23:22 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Shafey", "Laurent El", ""], ["Soltau", "Hagen", ""], ["Shafran", "Izhak", ""]]}, {"id": "1907.05338", "submitter": "Ran Wang", "authors": "Ran Wang, Haibo Su, Chunye Wang, Kailin Ji, Jupeng Ding", "title": "To Tune or Not To Tune? How About the Best of Both Worlds?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The introduction of pre-trained language models has revolutionized natural\nlanguage research communities. However, researchers still know relatively\nlittle regarding their theoretical and empirical properties. In this regard,\nPeters et al. perform several experiments which demonstrate that it is better\nto adapt BERT with a light-weight task-specific head, rather than building a\ncomplex one on top of the pre-trained language model, and freeze the parameters\nin the said language model. However, there is another option to adopt. In this\npaper, we propose a new adaptation method which we first train the task model\nwith the BERT parameters frozen and then fine-tune the entire model together.\nOur experimental results show that our model adaptation method can achieve 4.7%\naccuracy improvement in semantic similarity task, 0.99% accuracy improvement in\nsequence labeling task and 0.72% accuracy improvement in the text\nclassification task.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 04:46:31 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Ran", ""], ["Su", "Haibo", ""], ["Wang", "Chunye", ""], ["Ji", "Kailin", ""], ["Ding", "Jupeng", ""]]}, {"id": "1907.05339", "submitter": "Hainan Zhang", "authors": "Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo and Xueqi Cheng", "title": "ReCoSa: Detecting the Relevant Contexts with Self-Attention for\n  Multi-turn Dialogue Generation", "comments": null, "journal-ref": "ACL2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-turn dialogue generation, response is usually related with only a\nfew contexts. Therefore, an ideal model should be able to detect these relevant\ncontexts and produce a suitable response accordingly. However, the widely used\nhierarchical recurrent encoderdecoder models just treat all the contexts\nindiscriminately, which may hurt the following response generation process.\nSome researchers try to use the cosine similarity or the traditional attention\nmechanism to find the relevant contexts, but they suffer from either\ninsufficient relevance assumption or position bias problem. In this paper, we\npropose a new model, named ReCoSa, to tackle this problem. Firstly, a word\nlevel LSTM encoder is conducted to obtain the initial representation of each\ncontext. Then, the self-attention mechanism is utilized to update both the\ncontext and masked response representation. Finally, the attention weights\nbetween each context and response representations are computed and used in the\nfurther decoding process. Experimental results on both Chinese customer\nservices dataset and English Ubuntu dialogue dataset show that ReCoSa\nsignificantly outperforms baseline models, in terms of both metric-based and\nhuman evaluations. Further analysis on attention shows that the detected\nrelevant contexts by ReCoSa are highly coherent with human's understanding,\nvalidating the correctness and interpretability of ReCoSa.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 04:11:15 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zhang", "Hainan", ""], ["Lan", "Yanyan", ""], ["Pang", "Liang", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1907.05340", "submitter": "Hainan Zhang", "authors": "Hainan Zhang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng", "title": "Neural or Statistical: An Empirical Study on Language Models for Chinese\n  Input Recommendation on Mobile", "comments": null, "journal-ref": "LNCS2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese input recommendation plays an important role in alleviating human\ncost in typing Chinese words, especially in the scenario of mobile\napplications. The fundamental problem is to predict the conditional probability\nof the next word given the sequence of previous words. Therefore, statistical\nlanguage models, i.e.~n-grams based models, have been extensively used on this\ntask in real application. However, the characteristics of extremely different\ntyping behaviors usually lead to serious sparsity problem, even n-gram with\nsmoothing will fail. A reasonable approach to tackle this problem is to use the\nrecently proposed neural models, such as probabilistic neural language model,\nrecurrent neural network and word2vec. They can leverage more semantically\nsimilar words for estimating the probability. However, there is no conclusion\non which approach of the two will work better in real application. In this\npaper, we conduct an extensive empirical study to show the differences between\nstatistical and neural language models. The experimental results show that the\ntwo different approach have individual advantages, and a hybrid approach will\nbring a significant improvement.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 04:39:24 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zhang", "Hainan", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1907.05343", "submitter": "Ruisheng Cao", "authors": "Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li and Kai Yu", "title": "Semantic Parsing with Dual Learning", "comments": "Accepted by ACL 2019 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing converts natural language queries into structured logical\nforms. The paucity of annotated training samples is a fundamental challenge in\nthis field. In this work, we develop a semantic parsing framework with the dual\nlearning algorithm, which enables a semantic parser to make full use of data\n(labeled and even unlabeled) through a dual-learning game. This game between a\nprimal model (semantic parsing) and a dual model (logical form to query) forces\nthem to regularize each other, and can achieve feedback signals from some\nprior-knowledge. By utilizing the prior-knowledge of logical form structures,\nwe propose a novel reward signal at the surface and semantic levels which tends\nto generate complete and reasonable logical forms. Experimental results show\nthat our approach achieves new state-of-the-art performance on ATIS dataset and\ngets competitive performance on Overnight dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 04:51:44 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 15:57:19 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Cao", "Ruisheng", ""], ["Zhu", "Su", ""], ["Liu", "Chen", ""], ["Li", "Jieyu", ""], ["Yu", "Kai", ""]]}, {"id": "1907.05346", "submitter": "Jiahuan Pei", "authors": "Jiahuan Pei, Pengjie Ren, Maarten de Rijke", "title": "A Modular Task-oriented Dialogue System Using a Neural\n  Mixture-of-Experts", "comments": "Proceedings of the 2019 SIGIR Workshop WCIS: Workshop on\n  Conversational Interaction Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end Task-oriented Dialogue Systems (TDSs) have attracted a lot of\nattention for their superiority (e.g., in terms of global optimization) over\npipeline modularized TDSs. Previous studies on end-to-end TDSs use a\nsingle-module model to generate responses for complex dialogue contexts.\nHowever, no model consistently outperforms the others in all cases. We propose\na neural Modular Task-oriented Dialogue System(MTDS) framework, in which a few\nexpert bots are combined to generate the response for a given dialogue context.\nMTDS consists of a chair bot and several expert bots. Each expert bot is\nspecialized for a particular situation, e.g., one domain, one type of action of\na system, etc. The chair bot coordinates multiple expert bots and adaptively\nselects an expert bot to generate the appropriate response. We further propose\na Token-level Mixture-of-Expert (TokenMoE) model to implement MTDS, where the\nexpert bots predict multiple tokens at each timestamp and the chair bot\ndetermines the final generated token by fully taking into consideration the\noutputs of all expert bots. Both the chair bot and the expert bots are jointly\ntrained in an end-to-end fashion. To verify the effectiveness of TokenMoE, we\ncarry out extensive experiments on a benchmark dataset. Compared with the\nbaseline using a single-module model, our TokenMoE improves the performance by\n8.1% of inform rate and 0.8% of success rate.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:25:50 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Pei", "Jiahuan", ""], ["Ren", "Pengjie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1907.05403", "submitter": "Casey Kennington", "authors": "Andrew Rafla and Casey Kennington", "title": "Incrementalizing RASA's Open-Source Natural Language Understanding\n  Pipeline", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As spoken dialogue systems and chatbots are gaining more widespread adoption,\ncommercial and open-sourced services for natural language understanding are\nemerging. In this paper, we explain how we altered the open-source RASA natural\nlanguage understanding pipeline to process incrementally (i.e., word-by-word),\nfollowing the incremental unit framework proposed by Schlangen and Skantze. To\ndo so, we altered existing RASA components to process incrementally, and added\nan update-incremental intent recognition model as a component to RASA. Our\nevaluations on the Snips dataset show that our changes allow RASA to function\nas an effective incremental natural language understanding service.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:35:20 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Rafla", "Andrew", ""], ["Kennington", "Casey", ""]]}, {"id": "1907.05446", "submitter": "Gabriel Ilharco", "authors": "Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, Jason Baldridge", "title": "General Evaluation for Instruction Conditioned Navigation using Dynamic\n  Time Warping", "comments": null, "journal-ref": "Thirty-third Conference on Neural Information Processing Systems\n  (NeurIPS 2019)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In instruction conditioned navigation, agents interpret natural language and\ntheir surroundings to navigate through an environment. Datasets for studying\nthis task typically contain pairs of these instructions and reference\ntrajectories. Yet, most evaluation metrics used thus far fail to properly\naccount for the latter, relying instead on insufficient similarity comparisons.\nWe address fundamental flaws in previously used metrics and show how Dynamic\nTime Warping (DTW), a long known method of measuring similarity between two\ntime series, can be used for evaluation of navigation agents. For such, we\ndefine the normalized Dynamic Time Warping (nDTW) metric, that softly penalizes\ndeviations from the reference path, is naturally sensitive to the order of the\nnodes composing each path, is suited for both continuous and graph-based\nevaluations, and can be efficiently calculated. Further, we define SDTW, which\nconstrains nDTW to only successful paths. We collect human similarity judgments\nfor simulated paths and find nDTW correlates better with human rankings than\nall other metrics. We also demonstrate that using nDTW as a reward signal for\nReinforcement Learning navigation agents improves their performance on both the\nRoom-to-Room (R2R) and Room-for-Room (R4R) datasets. The R4R results in\nparticular highlight the superiority of SDTW over previous success-constrained\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 18:42:03 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 16:59:52 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ilharco", "Gabriel", ""], ["Jain", "Vihan", ""], ["Ku", "Alexander", ""], ["Ie", "Eugene", ""], ["Baldridge", "Jason", ""]]}, {"id": "1907.05507", "submitter": "Alexandros Papangelis", "authors": "Alexandros Papangelis, Yi-Chia Wang, Piero Molino, Gokhan Tur", "title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement\n  Learning", "comments": "SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first complete attempt at concurrently training conversational\nagents that communicate only via self-generated language. Using DSTC2 as seed\ndata, we trained natural language understanding (NLU) and generation (NLG)\nnetworks for each agent and let the agents interact online. We model the\ninteraction as a stochastic collaborative game where each agent (player) has a\nrole (\"assistant\", \"tourist\", \"eater\", etc.) and their own objectives, and can\nonly interact via natural language they generate. Each agent, therefore, needs\nto learn to operate optimally in an environment with multiple sources of\nuncertainty (its own NLU and NLG, the other agent's NLU, Policy, and NLG). In\nour evaluation, we show that the stochastic-game agents outperform deep\nlearning based supervised baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 22:05:48 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 11:28:40 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Wang", "Yi-Chia", ""], ["Molino", "Piero", ""], ["Tur", "Gokhan", ""]]}, {"id": "1907.05524", "submitter": "Haoruo Peng", "authors": "Haoruo Peng, Daniel Khashabi, Dan Roth", "title": "Solving Hard Coreference Problems", "comments": "NAACL 2015. Proceedings of the 2015 Conference of the North American\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference resolution is a key problem in natural language understanding\nthat still escapes reliable solutions. One fundamental difficulty has been that\nof resolving instances involving pronouns since they often require deep\nlanguage understanding and use of background knowledge. In this paper, we\npropose an algorithmic solution that involves a new representation for the\nknowledge required to address hard coreference problems, along with a\nconstrained optimization framework that uses this knowledge in coreference\ndecision making. Our representation, Predicate Schemas, is instantiated with\nknowledge acquired in an unsupervised way, and is compiled automatically into\nconstraints that impact the coreference decision. We present a general\ncoreference resolution system that significantly improves state-of-the-art\nperformance on hard, Winograd-style, pronoun resolution cases, while still\nperforming at the state-of-the-art level on standard coreference resolution\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 23:40:56 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Peng", "Haoruo", ""], ["Khashabi", "Daniel", ""], ["Roth", "Dan", ""]]}, {"id": "1907.05545", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Francisco J. R. Ruiz, and David M. Blei", "title": "The Dynamic Embedded Topic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling analyzes documents to learn meaningful patterns of words. For\ndocuments collected in sequence, dynamic topic models capture how these\npatterns vary over time. We develop the dynamic embedded topic model (D-ETM), a\ngenerative model of documents that combines dynamic latent Dirichlet allocation\n(D-LDA) and word embeddings. The D-ETM models each word with a categorical\ndistribution parameterized by the inner product between the word embedding and\na per-time-step embedding representation of its assigned topic. The D-ETM\nlearns smooth topic trajectories by defining a random walk prior over the\nembedding representations of the topics. We fit the D-ETM using structured\namortized variational inference with a recurrent neural network. On three\ndifferent corpora---a collection of United Nations debates, a set of ACL\nabstracts, and a dataset of Science Magazine articles---we found that the D-ETM\noutperforms D-LDA on a document completion task. We further found that the\nD-ETM learns more diverse and coherent topics than D-LDA while requiring\nsignificantly less time to fit.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 01:55:36 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 22:44:28 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Dieng", "Adji B.", ""], ["Ruiz", "Francisco J. R.", ""], ["Blei", "David M.", ""]]}, {"id": "1907.05559", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,\n  Xing Xie", "title": "NPA: Neural News Recommendation with Personalized Attention", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330665", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommendation is very important to help users find interested news and\nalleviate information overload. Different users usually have different\ninterests and the same user may have various interests. Thus, different users\nmay click the same news article with attention on different aspects. In this\npaper, we propose a neural news recommendation model with personalized\nattention (NPA). The core of our approach is a news representation model and a\nuser representation model. In the news representation model we use a CNN\nnetwork to learn hidden representations of news articles based on their titles.\nIn the user representation model we learn the representations of users based on\nthe representations of their clicked news articles. Since different words and\ndifferent news articles may have different informativeness for representing\nnews and users, we propose to apply both word- and news-level attention\nmechanism to help our model attend to important words and news articles. In\naddition, the same news article and the same word may have different\ninformativeness for different users. Thus, we propose a personalized attention\nnetwork which exploits the embedding of user ID to generate the query vector\nfor the word- and news-level attentions. Extensive experiments are conducted on\na real-world news recommendation dataset collected from MSN news, and the\nresults validate the effectiveness of our approach on news recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 03:11:14 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["An", "Mingxiao", ""], ["Huang", "Jianqiang", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "1907.05562", "submitter": "Yazhou Zhang", "authors": "Yazhou Zhang, Lingling Song, Dawei Song, Peng Guo, Junwei Zhang and\n  Peng Zhang", "title": "ScenarioSA: A Large Scale Conversational Database for Interactive\n  Sentiment Analysis", "comments": "Withdrawn by arXiv administration due to policy violation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive sentiment analysis is an emerging, yet challenging, subtask of\nthe sentiment analysis problem. It aims to discover the affective state and\nsentimental change of each person in a conversation. Existing sentiment\nanalysis approaches are insufficient in modelling the interactions among\npeople. However, the development of new approaches are critically limited by\nthe lack of labelled interactive sentiment datasets. In this paper, we present\na new conversational emotion database that we have created and made publically\navailable, namely ScenarioSA. We manually label 2,214 multi-turn English\nconversations collected from natural contexts. In comparison with existing\nsentiment datasets, ScenarioSA (1) covers a wide range of scenarios; (2)\ndescribes the interactions between two speakers; and (3) reflects the\nsentimental evolution of each speaker over the course of a conversation.\nFinally, we evaluate various state-of-the-art algorithms on ScenarioSA,\ndemonstrating the need of novel interactive sentiment analysis models and the\npotential of ScenarioSA to facilitate the development of such models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 03:34:06 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 13:02:44 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhang", "Yazhou", ""], ["Song", "Lingling", ""], ["Song", "Dawei", ""], ["Guo", "Peng", ""], ["Zhang", "Junwei", ""], ["Zhang", "Peng", ""]]}, {"id": "1907.05572", "submitter": "Zhiwei Wang", "authors": "Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang", "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks have long been the dominating choice for sequence\nmodeling. However, it severely suffers from two issues: impotent in capturing\nvery long-term dependencies and unable to parallelize the sequential\ncomputation procedure. Therefore, many non-recurrent sequence models that are\nbuilt on convolution and attention operations have been proposed recently.\nNotably, models with multi-head attention such as Transformer have demonstrated\nextreme effectiveness in capturing long-term dependencies in a variety of\nsequence modeling tasks. Despite their success, however, these models lack\nnecessary components to model local structures in sequences and heavily rely on\nposition embeddings that have limited effects and require a considerable amount\nof design efforts. In this paper, we propose the R-Transformer which enjoys the\nadvantages of both RNNs and the multi-head attention mechanism while avoids\ntheir respective drawbacks. The proposed model can effectively capture both\nlocal structures and global long-term dependencies in sequences without any use\nof position embeddings. We evaluate R-Transformer through extensive experiments\nwith data from a wide range of domains and the empirical results show that\nR-Transformer outperforms the state-of-the-art methods by a large margin in\nmost of the tasks. We have made the code publicly available at\n\\url{https://github.com/DSE-MSU/R-transformer}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 04:01:57 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Wang", "Zhiwei", ""], ["Ma", "Yao", ""], ["Liu", "Zitao", ""], ["Tang", "Jiliang", ""]]}, {"id": "1907.05576", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,\n  Xing Xie", "title": "Neural News Recommendation with Attentive Multi-View Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized news recommendation is very important for online news platforms\nto help users find interested news and improve user experience. News and user\nrepresentation learning is critical for news recommendation. Existing news\nrecommendation methods usually learn these representations based on single news\ninformation, e.g., title, which may be insufficient. In this paper we propose a\nneural news recommendation approach which can learn informative representations\nof users and news by exploiting different kinds of news information. The core\nof our approach is a news encoder and a user encoder. In the news encoder we\npropose an attentive multi-view learning model to learn unified news\nrepresentations from titles, bodies and topic categories by regarding them as\ndifferent views of news. In addition, we apply both word-level and view-level\nattention mechanism to news encoder to select important words and views for\nlearning informative news representations. In the user encoder we learn the\nrepresentations of users based on their browsed news and apply attention\nmechanism to select informative news for user representation learning.\nExtensive experiments on a real-world dataset show our approach can effectively\nimprove the performance of news recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 04:50:33 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["An", "Mingxiao", ""], ["Huang", "Jianqiang", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "1907.05599", "submitter": "Tianyu Zhao", "authors": "Tianyu Zhao and Tatsuya Kawahara", "title": "Effective Incorporation of Speaker Information in Utterance Encoding in\n  Dialog", "comments": "8+1 pages, 3 figures, and 5 tables. Rejected by SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dialog studies, we often encode a dialog using a hierarchical encoder\nwhere each utterance is converted into an utterance vector, and then a sequence\nof utterance vectors is converted into a dialog vector. Since knowing who\nproduced which utterance is essential to understanding a dialog, conventional\nmethods tried integrating speaker labels into utterance vectors. We found the\nmethod problematic in some cases where speaker annotations are inconsistent\namong different dialogs. A relative speaker modeling method is proposed to\naddress the problem. Experimental evaluations on dialog act recognition and\nresponse generation show that the proposed method yields superior and more\nconsistent performances.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 07:37:00 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Zhao", "Tianyu", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "1907.05611", "submitter": "Hui Chen", "authors": "Hui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou, Yusen Zhang, Borje\n  Karlsson", "title": "GRN: Gated Relation Network to Enhance Convolutional Neural Network for\n  Named Entity Recognition", "comments": "This paper is accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approaches for named entity recognition (NER) mostly adopt\ncomplex recurrent neural networks (RNN), e.g., long-short-term-memory (LSTM).\nHowever, RNNs are limited by their recurrent nature in terms of computational\nefficiency. In contrast, convolutional neural networks (CNN) can fully exploit\nthe GPU parallelism with their feedforward architectures. However, little\nattention has been paid to performing NER with CNNs, mainly owing to their\ndifficulties in capturing the long-term context information in a sequence. In\nthis paper, we propose a simple but effective CNN-based network for NER, i.e.,\ngated relation network (GRN), which is more capable than common CNNs in\ncapturing long-term context. Specifically, in GRN we firstly employ CNNs to\nexplore the local context features of each word. Then we model the relations\nbetween words and use them as gates to fuse local context features into global\nones for predicting labels. Without using recurrent layers that process a\nsentence in a sequential manner, our GRN allows computations to be performed in\nparallel across the entire sentence. Experiments on two benchmark NER datasets\n(i.e., CoNLL2003 and Ontonotes 5.0) show that, our proposed GRN can achieve\nstate-of-the-art performance with or without external knowledge. It also enjoys\nlower time costs to train and test.We have made the code publicly available at\nhttps://github.com/HuiChen24/NER-GRN.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:16:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 08:40:24 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Chen", "Hui", ""], ["Lin", "Zijia", ""], ["Ding", "Guiguang", ""], ["Lou", "Jianguang", ""], ["Zhang", "Yusen", ""], ["Karlsson", "Borje", ""]]}, {"id": "1907.05664", "submitter": "David Tuckey", "authors": "David Tuckey, Krysia Broda, Alessandra Russo", "title": "Saliency Maps Generation for Automatic Text Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency map generation techniques are at the forefront of explainable AI\nliterature for a broad range of machine learning applications. Our goal is to\nquestion the limits of these approaches on more complex tasks. In this paper we\napply Layer-Wise Relevance Propagation (LRP) to a sequence-to-sequence\nattention model trained on a text summarization dataset. We obtain unexpected\nsaliency maps and discuss the rightfulness of these \"explanations\". We argue\nthat we need a quantitative way of testing the counterfactual case to judge the\ntruthfulness of the saliency maps. We suggest a protocol to check the validity\nof the importance attributed to the input and show that the saliency maps\nobtained sometimes capture the real use of the input features by the network,\nand sometimes do not. We use this example to discuss how careful we need to be\nwhen accepting them as explanation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 10:28:00 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Tuckey", "David", ""], ["Broda", "Krysia", ""], ["Russo", "Alessandra", ""]]}, {"id": "1907.05671", "submitter": "Graham Spinks", "authors": "Graham Spinks, Marie-Francine Moens", "title": "Justifying Diagnosis Decisions by Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.jbi.2019.103248", "report-no": null, "categories": "cs.LG cs.CL cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An integrated approach is proposed across visual and textual data to both\ndetermine and justify a medical diagnosis by a neural network. As deep learning\ntechniques improve, interest grows to apply them in medical applications. To\nenable a transition to workflows in a medical context that are aided by machine\nlearning, the need exists for such algorithms to help justify the obtained\noutcome so human clinicians can judge their validity. In this work, deep\nlearning methods are used to map a frontal X-Ray image to a continuous textual\nrepresentation. This textual representation is decoded into a diagnosis and the\nassociated textual justification that will help a clinician evaluate the\noutcome. Additionally, more explanatory data is provided for the diagnosis by\ngenerating a realistic X-Ray that belongs to the nearest alternative diagnosis.\nWith a clinical expert opinion study on a subset of the X-Ray data set from the\nIndiana University hospital network, we demonstrate that our justification\nmechanism significantly outperforms existing methods that use saliency maps.\nWhile performing multi-task training with multiple loss functions, our method\nachieves excellent diagnosis accuracy and captioning quality when compared to\ncurrent state-of-the-art single-task methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 10:51:48 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Spinks", "Graham", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1907.05698", "submitter": "Zhao You", "authors": "Zhao You, Dan Su, Dong Yu", "title": "Teach an all-rounder with experts in different domains", "comments": "5 pages and 2 figures; accepted by 2019 IEEE International Conference\n  on Acoustics, Speech and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many automatic speech recognition (ASR) tasks, an ideal model has to be\napplicable over multiple domains. In this paper, we propose to teach an\nall-rounder with experts in different domains. Concretely, we build a\nmulti-domain acoustic model by applying the teacher-student training framework.\nFirst, for each domain, a teacher model (domain-dependent model) is trained by\nfine-tuning a multi-condition model with domain-specific subset. Then all these\nteacher models are used to teach one single student model simultaneously. We\nperform experiments on two predefined domain setups. One is domains with\ndifferent speaking styles, the other is nearfield, far-field and far-field with\nnoise. Moreover, two types of models are examined: deep feedforward sequential\nmemory network (DFSMN) and long short term memory (LSTM). Experimental results\nshow that the model trained with this framework outperforms not only\nmulti-condition model but also domain-dependent model. Specially, our training\nmethod provides up to 10.4% relative character error rate improvement over\nbaseline model (multi-condition model).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:26:25 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["You", "Zhao", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "1907.05757", "submitter": "Maria Ponomareva", "authors": "Maria Ponomareva and Kirill Milintsevich and Ekaterina Chernyak and\n  Anatoly Starostin", "title": "Automated Word Stress Detection in Russian", "comments": "SCLeM 2017", "journal-ref": "Published in Proceedings of the First Workshop on Subword and\n  Character Level Models in NLP, pages 31 35, Copenhagen, Denmark, September 7,\n  2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we address the problem of automated word stress detection in\nRussian using character level models and no part-speech-taggers. We use a\nsimple bidirectional RNN with LSTM nodes and achieve the accuracy of 90% or\nhigher. We experiment with two training datasets and show that using the data\nfrom an annotated corpus is much more efficient than using a dictionary, since\nit allows us to take into account word frequencies and the morphological\ncontext of the word.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 14:13:53 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Ponomareva", "Maria", ""], ["Milintsevich", "Kirill", ""], ["Chernyak", "Ekaterina", ""], ["Starostin", "Anatoly", ""]]}, {"id": "1907.05774", "submitter": "Pawe{\\l} Budzianowski", "authors": "Pawe{\\l} Budzianowski and Ivan Vuli\\'c", "title": "Hello, It's GPT-2 -- How Can I Help You? Towards the Use of Pretrained\n  Language Models for Task-Oriented Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scarcity is a long-standing and crucial challenge that hinders quick\ndevelopment of task-oriented dialogue systems across multiple domains:\ntask-oriented dialogue models are expected to learn grammar, syntax, dialogue\nreasoning, decision making, and language generation from absurdly small amounts\nof task-specific data. In this paper, we demonstrate that recent progress in\nlanguage modeling pre-training and transfer learning shows promise to overcome\nthis problem. We propose a task-oriented dialogue model that operates solely on\ntext input: it effectively bypasses explicit policy and language generation\nmodules. Building on top of the TransferTransfo framework (Wolf et al., 2019)\nand generative model pre-training (Radford et al., 2019), we validate the\napproach on complex multi-domain task-oriented dialogues from the MultiWOZ\ndataset. Our automatic and human evaluations show that the proposed model is on\npar with a strong task-specific neural baseline. In the long run, our approach\nholds promise to mitigate the data scarcity problem, and to support the\nconstruction of more engaging and more eloquent task-oriented conversational\nagents.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 14:55:06 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 10:44:47 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Budzianowski", "Pawe\u0142", ""], ["Vuli\u0107", "Ivan", ""]]}, {"id": "1907.05789", "submitter": "Yu Bao", "authors": "Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova,\n  Xinyu Dai, Jiajun Chen", "title": "Generating Sentences from Disentangled Syntactic and Semantic Spaces", "comments": "11 pages, accepted in ACL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoders (VAEs) are widely used in natural language\ngeneration due to the regularization of the latent space. However, generating\nsentences from the continuous latent space does not explicitly model the\nsyntactic information. In this paper, we propose to generate sentences from\ndisentangled syntactic and semantic spaces. Our proposed method explicitly\nmodels syntactic information in the VAE's latent space by using the linearized\ntree sequence, leading to better performance of language generation.\nAdditionally, the advantage of sampling in the disentangled syntactic and\nsemantic latent spaces enables us to perform novel applications, such as the\nunsupervised paraphrase generation and syntax-transfer generation. Experimental\nresults show that our proposed model achieves similar or better performance in\nvarious tasks, compared with state-of-the-art related work.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 04:40:48 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Bao", "Yu", ""], ["Zhou", "Hao", ""], ["Huang", "Shujian", ""], ["Li", "Lei", ""], ["Mou", "Lili", ""], ["Vechtomova", "Olga", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1907.05790", "submitter": "Christophe Servan", "authors": "Estelle Maudet, Oralie Cattan, Maureen de Seyssel, Christophe Servan", "title": "Qwant Research @DEFT 2019: Document matching and information retrieval\n  using clinical cases", "comments": "Article accepted at the workshop DEfi fouille de Texte (DEFT 2019).\n  Article in French", "journal-ref": "DEFT 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on Qwant Research contribution to tasks 2 and 3 of the\nDEFT 2019's challenge, focusing on French clinical cases analysis. Task 2 is a\ntask on semantic similarity between clinical cases and discussions. For this\ntask, we propose an approach based on language models and evaluate the impact\non the results of different preprocessings and matching techniques. For task 3,\nwe have developed an information extraction system yielding very encouraging\nresults accuracy-wise. We have experimented two different approaches, one based\non the exclusive use of neural networks, the other based on a linguistic\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 08:29:21 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Maudet", "Estelle", ""], ["Cattan", "Oralie", ""], ["de Seyssel", "Maureen", ""], ["Servan", "Christophe", ""]]}, {"id": "1907.05791", "submitter": "Holger Schwenk", "authors": "Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, Francisco\n  Guzm\\'an", "title": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from\n  Wikipedia", "comments": "13 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach based on multilingual sentence embeddings to\nautomatically extract parallel sentences from the content of Wikipedia articles\nin 85 languages, including several dialects or low-resource languages. We do\nnot limit the the extraction process to alignments with English, but\nsystematically consider all possible language pairs. In total, we are able to\nextract 135M parallel sentences for 1620 different language pairs, out of which\nonly 34M are aligned with English. This corpus of parallel sentences is freely\navailable at\nhttps://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix. To get\nan indication on the quality of the extracted bitexts, we train neural MT\nbaseline systems on the mined data only for 1886 languages pairs, and evaluate\nthem on the TED corpus, achieving strong BLEU scores for many language pairs.\nThe WikiMatrix bitexts seem to be particularly interesting to train MT systems\nbetween distant languages without the need to pivot through English.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 23:57:30 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 00:14:48 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Schwenk", "Holger", ""], ["Chaudhary", "Vishrav", ""], ["Sun", "Shuo", ""], ["Gong", "Hongyu", ""], ["Guzm\u00e1n", "Francisco", ""]]}, {"id": "1907.05792", "submitter": "Jatin Ganhotra", "authors": "Jatin Ganhotra, Siva Sankalp Patel, Kshitij Fadnis", "title": "Knowledge-incorporating ESIM models for Response Selection in\n  Retrieval-based Dialog Systems", "comments": "Ranked 2nd on Ubuntu and 4th on Advising task in DSTC-7 Track 1.\n  Accepted for an oral presentation at the DSTC-7 workshop at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Goal-oriented dialog systems, which can be trained end-to-end without\nmanually encoding domain-specific features, show tremendous promise in the\ncustomer support use-case e.g. flight booking, hotel reservation, technical\nsupport, student advising etc. These dialog systems must learn to interact with\nexternal domain knowledge to achieve the desired goal e.g. recommending courses\nto a student, booking a table at a restaurant etc. This paper presents extended\nEnhanced Sequential Inference Model (ESIM) models: a) K-ESIM (Knowledge-ESIM),\nwhich incorporates the external domain knowledge and b) T-ESIM (Targeted-ESIM),\nwhich leverages information from similar conversations to improve the\nprediction accuracy. Our proposed models and the baseline ESIM model are\nevaluated on the Ubuntu and Advising datasets in the Sentence Selection track\nof the latest Dialog System Technology Challenge (DSTC7), where the goal is to\nfind the correct next utterance, given a partial conversation, from a set of\ncandidates. Our preliminary results suggest that incorporating external\nknowledge sources and leveraging information from similar dialogs leads to\nperformance improvements for predicting the next utterance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:55:24 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Ganhotra", "Jatin", ""], ["Patel", "Siva Sankalp", ""], ["Fadnis", "Kshitij", ""]]}, {"id": "1907.05839", "submitter": "Giorgio Magri", "authors": "Arto Anttila, Scott Borgeson, Giorgio Magri", "title": "Equiprobable mappings in weighted constraint grammars", "comments": "10 pages; Proceedings of ACL Sigmorphon 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that MaxEnt is so rich that it can distinguish between any two\ndifferent mappings: there always exists a nonnegative weight vector which\nassigns them different MaxEnt probabilities. Stochastic HG instead does admit\nequiprobable mappings and we give a complete formal characterization of them.\nWe compare these different predictions of the two frameworks on a test case of\nFinnish stress.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 17:02:14 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Anttila", "Arto", ""], ["Borgeson", "Scott", ""], ["Magri", "Giorgio", ""]]}, {"id": "1907.05854", "submitter": "Rachel Bawden", "authors": "Rachel Bawden, Nikolay Bogoychev, Ulrich Germann, Roman Grundkiewicz,\n  Faheem Kirefu, Antonio Valerio Miceli Barone and Alexandra Birch", "title": "The University of Edinburgh's Submissions to the WMT19 News Translation\n  Task", "comments": "To appear in the Proceedings of WMT19: Shared Task Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The University of Edinburgh participated in the WMT19 Shared Task on News\nTranslation in six language directions: English-to-Gujarati,\nGujarati-to-English, English-to-Chinese, Chinese-to-English, German-to-English,\nand English-to-Czech. For all translation directions, we created or used\nback-translations of monolingual data in the target language as additional\nsynthetic training data. For English-Gujarati, we also explored semi-supervised\nMT with cross-lingual language model pre-training, and translation pivoting\nthrough Hindi. For translation to and from Chinese, we investigated\ncharacter-based tokenisation vs. sub-word segmentation of Chinese text. For\nGerman-to-English, we studied the impact of vast amounts of back-translated\ntraining data on translation quality, gaining a few additional insights over\nEdunov et al. (2018). For English-to-Czech, we compared different\npre-processing and tokenisation regimes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 17:23:30 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Bawden", "Rachel", ""], ["Bogoychev", "Nikolay", ""], ["Germann", "Ulrich", ""], ["Grundkiewicz", "Roman", ""], ["Kirefu", "Faheem", ""], ["Barone", "Antonio Valerio Miceli", ""], ["Birch", "Alexandra", ""]]}, {"id": "1907.05955", "submitter": "Liang Lu", "authors": "Liang Lu, Xiong Xiao, Zhuo Chen, Yifan Gong", "title": "PyKaldi2: Yet another speech toolkit based on Kaldi and PyTorch", "comments": "5 pages, 2 figures, submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PyKaldi2 speech recognition toolkit implemented based on Kaldi\nand PyTorch. While similar toolkits are available built on top of the two, a\nkey feature of PyKaldi2 is sequence training with criteria such as MMI, sMBR\nand MPE. In particular, we implemented the sequence training module with\non-the-fly lattice generation during model training in order to simplify the\ntraining pipeline. To address the challenging acoustic environments in real\napplications, PyKaldi2 also supports on-the-fly noise and reverberation\nsimulation to improve the model robustness. With this feature, it is possible\nto backpropogate the gradients from the sequence-level loss to the front-end\nfeature extraction module, which, hopefully, can foster more research in the\ndirection of joint front-end and backend learning. We performed benchmark\nexperiments on Librispeech, and show that PyKaldi2 can achieve reasonable\nrecognition accuracy. The toolkit is released under the MIT license.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 20:54:59 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 23:07:54 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 22:48:32 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Lu", "Liang", ""], ["Xiao", "Xiong", ""], ["Chen", "Zhuo", ""], ["Gong", "Yifan", ""]]}, {"id": "1907.06017", "submitter": "Ye Bai", "authors": "Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian and Zhengqi Wen", "title": "Learn Spelling from Teachers: Transferring Knowledge from Language\n  Models to Sequence-to-Sequence Speech Recognition", "comments": "5 pages, 3 figures, accepted by INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating an external language model into a sequence-to-sequence speech\nrecognition system is non-trivial. Previous works utilize linear interpolation\nor a fusion network to integrate external language models. However, these\napproaches introduce external components, and increase decoding computation. In\nthis paper, we instead propose a knowledge distillation based training approach\nto integrating external language models into a sequence-to-sequence model. A\nrecurrent neural network language model, which is trained on large scale\nexternal text, generates soft labels to guide the sequence-to-sequence model\ntraining. Thus, the language model plays the role of the teacher. This approach\ndoes not add any external component to the sequence-to-sequence model during\ntesting. And this approach is flexible to be combined with shallow fusion\ntechnique together for decoding. The experiments are conducted on public\nChinese datasets AISHELL-1 and CLMAD. Our approach achieves a character error\nrate of 9.3%, which is relatively reduced by 18.42% compared with the vanilla\nsequence-to-sequence model.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 06:27:24 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bai", "Ye", ""], ["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Tian", "Zhengkun", ""], ["Wen", "Zhengqi", ""]]}, {"id": "1907.06042", "submitter": "Chia-Hsuan Lee", "authors": "Chia-Hsuan Lee, Hung-Yi Lee", "title": "Cross-Lingual Transfer Learning for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based question answering (QA) on English documents has achieved\nsuccess because there is a large amount of English training examples. However,\nfor most languages, training examples for high-quality QA models are not\navailable. In this paper, we explore the problem of cross-lingual transfer\nlearning for QA, where a source language task with plentiful annotations is\nutilized to improve the performance of a QA model on a target language task\nwith limited available annotations. We examine two different approaches. A\nmachine translation (MT) based approach translates the source language into the\ntarget language, or vice versa. Although the MT-based approach brings\nimprovement, it assumes the availability of a sentence-level translation\nsystem. A GAN-based approach incorporates a language discriminator to learn\nlanguage-universal feature representations, and consequentially transfer\nknowledge from the source language. The GAN-based approach rivals the\nperformance of the MT-based approach with fewer linguistic resources. Applying\nboth approaches simultaneously yield the best results. We use two English\nbenchmark datasets, SQuAD and NewsQA, as source language data, and show\nsignificant improvements over a number of established baselines on a Chinese QA\ntask. We achieve the new state-of-the-art on the Chinese QA dataset.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 10:04:37 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Lee", "Chia-Hsuan", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1907.06080", "submitter": "Dai Quoc Nguyen", "authors": "Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung", "title": "A Relational Memory-based Embedding Model for Triple Classification and\n  Search Personalization", "comments": "To appear in Proceedings of ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding methods often suffer from a limitation of\nmemorizing valid triples to predict new ones for triple classification and\nsearch personalization problems. To this end, we introduce a novel embedding\nmodel, named R-MeN, that explores a relational memory network to encode\npotential dependencies in relationship triples. R-MeN considers each triple as\na sequence of 3 input vectors that recurrently interact with a memory using a\ntransformer self-attention mechanism. Thus R-MeN encodes new information from\ninteractions between the memory and each input vector to return a corresponding\nvector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional\nneural network-based decoder to produce a scalar score for the triple.\nExperimental results show that our proposed R-MeN obtains state-of-the-art\nresults on SEARCH17 for the search personalization task, and on WN11 and FB13\nfor the triple classification task.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 14:01:27 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 05:12:37 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nguyen", "Dai Quoc", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1907.06111", "submitter": "Hossein Zeinali", "authors": "Nooshin Maghsoodi, Hossein Sameti, Hossein Zeinali, Themos~Stafylakis", "title": "Speaker Recognition with Random Digit Strings Using Uncertainty\n  Normalized HMM-based i-vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we combine Hidden Markov Models (HMMs) with i-vector\nextractors to address the problem of text-dependent speaker recognition with\nrandom digit strings. We employ digit-specific HMMs to segment the utterances\ninto digits, to perform frame alignment to HMM states and to extract Baum-Welch\nstatistics. By making use of the natural partition of input features into\ndigits, we train digit-specific i-vector extractors on top of each HMM and we\nextract well-localized i-vectors, each modelling merely the phonetic content\ncorresponding to a single digit. We then examine ways to perform channel and\nuncertainty compensation, and we propose a novel method for using the\nuncertainty in the i-vector estimates. The experiments on RSR2015 part III show\nthat the proposed method attains 1.52\\% and 1.77\\% Equal Error Rate (EER) for\nmale and female respectively, outperforming state-of-the-art methods such as\nx-vectors, trained on vast amounts of data. Furthermore, these results are\nattained by a single system trained entirely on RSR2015, and by a simple\nscore-normalized cosine distance. Moreover, we show that the omission of\nchannel compensation yields only a minor degradation in performance, meaning\nthat the system attains state-of-the-art results even without recordings from\nmultiple handsets per speaker for training or enrolment. Similar conclusions\nare drawn from our experiments on the RedDots corpus, where the same method is\nevaluated on phrases. Finally, we report results with bottleneck features and\nshow that further improvement is attained when fusing them with spectral\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 17:52:17 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Maghsoodi", "Nooshin", ""], ["Sameti", "Hossein", ""], ["Zeinali", "Hossein", ""], ["Themos~Stafylakis", "", ""]]}, {"id": "1907.06112", "submitter": "Hossein Zeinali", "authors": "Hossein Zeinali, Pavel Mat\\v{e}jka, Ladislav Mo\\v{s}ner, Old\\v{r}ich\n  Plchot, Anna Silnova, Ond\\v{r}ej Novotn\\'y, J\\'an Profant, Ond\\v{r}ej\n  Glembek, Luk\\'a\\v{s} Burget", "title": "BUT VOiCES 2019 System Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a description of our effort in VOiCES 2019 Speaker Recognition\nchallenge. All systems in the fixed condition are based on the x-vector\nparadigm with different features and DNN topologies. The single best system\nreaches 1.2% EER and a fusion of 3 systems yields 1.0% EER, which is 15%\nrelative improvement. The open condition allowed us to use external data which\nwe did for the PLDA adaptation and achieved less than ~10% relative\nimprovement. In the submission to open condition, we used 3 x-vector systems\nand also one i-vector based system.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 17:57:55 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Zeinali", "Hossein", ""], ["Mat\u011bjka", "Pavel", ""], ["Mo\u0161ner", "Ladislav", ""], ["Plchot", "Old\u0159ich", ""], ["Silnova", "Anna", ""], ["Novotn\u00fd", "Ond\u0159ej", ""], ["Profant", "J\u00e1n", ""], ["Glembek", "Ond\u0159ej", ""], ["Burget", "Luk\u00e1\u0161", ""]]}, {"id": "1907.06142", "submitter": "Linfeng Song", "authors": "Linfeng Song", "title": "Tackling Graphical NLP problems with Graph Recurrent Networks", "comments": "Ph.D. thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to properly model graphs is a long-existing and important problem in NLP\narea, where several popular types of graphs are knowledge graphs, semantic\ngraphs and dependency graphs. Comparing with other data structures, such as\nsequences and trees, graphs are generally more powerful in representing complex\ncorrelations among entities. For example, a knowledge graph stores real-word\nentities (such as \"Barack_Obama\" and \"U.S.\") and their relations (such as\n\"live_in\" and \"lead_by\"). Properly encoding a knowledge graph is beneficial to\nuser applications, such as question answering and knowledge discovery. Modeling\ngraphs is also very challenging, probably because graphs usually contain\nmassive and cyclic relations.\n  Recent years have witnessed the success of deep learning, especially\nRNN-based models, on many NLP problems. Besides, RNNs and their variations have\nbeen extensively studied on several graph problems and showed preliminary\nsuccesses. Despite the successes that have been achieved, RNN-based models\nsuffer from several major drawbacks on graphs. First, they can only consume\nsequential data, thus linearization is required to serialize input graphs,\nresulting in the loss of important structural information. Second, the\nserialization results are usually very long, so it takes a long time for RNNs\nto encode them.\n  In this thesis, we propose a novel graph neural network, named graph\nrecurrent network (GRN). We study our GRN model on 4 very different tasks, such\nas machine reading comprehension, relation extraction and machine translation.\nSome take undirected graphs without edge labels, while the others have directed\nones with edge labels. To consider these important differences, we gradually\nenhance our GRN model, such as further considering edge labels and adding an\nRNN decoder. Carefully designed experiments show the effectiveness of GRN on\nall these tasks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 22:48:31 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Song", "Linfeng", ""]]}, {"id": "1907.06170", "submitter": "Marcin Junczys-Dowmunt", "authors": "Marcin Junczys-Dowmunt", "title": "Microsoft Translator at WMT 2019: Towards Large-Scale Document-Level\n  Neural Machine Translation", "comments": "WMT 2019 Shared Task submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Microsoft Translator submissions to the WMT19 news\ntranslation shared task for English-German. Our main focus is document-level\nneural machine translation with deep transformer models. We start with strong\nsentence-level baselines, trained on large-scale data created via\ndata-filtering and noisy back-translation and find that back-translation seems\nto mainly help with translationese input. We explore fine-tuning techniques,\ndeeper models and different ensembling strategies to counter these effects.\nUsing document boundaries present in the authentic and synthetic parallel data,\nwe create sequences of up to 1000 subword segments and train transformer\ntranslation models. We experiment with data augmentation techniques for the\nsmaller authentic data with document-boundaries and for larger authentic data\nwithout boundaries. We further explore multi-task training for the\nincorporation of document-level source language monolingual data via the\nBERT-objective on the encoder and two-pass decoding for combinations of\nsentence-level and document-level systems. Based on preliminary human\nevaluation results, evaluators strongly prefer the document-level systems over\nour comparable sentence-level system. The document-level systems also seem to\nscore higher than the human references in source-based direct assessment.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 05:47:53 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Junczys-Dowmunt", "Marcin", ""]]}, {"id": "1907.06205", "submitter": "Venkatesh Theru Mohan", "authors": "Venkatesh Theru Mohan and Ali Jannesari", "title": "Automatic Repair and Type Binding of Undeclared Variables using Neural\n  Networks", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.FL cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning had been used in program analysis for the prediction of hidden\nsoftware defects using software defect datasets, security vulnerabilities using\ngenerative adversarial networks as well as identifying syntax errors by\nlearning a trained neural machine translation on program codes. However, all\nthese approaches either require defect datasets or bug-free source codes that\nare executable for training the deep learning model. Our neural network model\nis neither trained with any defect datasets nor bug-free programming source\ncodes, instead it is trained using structural semantic details of Abstract\nSyntax Tree (AST) where each node represents a construct appearing in the\nsource code. This model is implemented to fix one of the most common semantic\nerrors, such as undeclared variable errors as well as infer their type\ninformation before program compilation. By this approach, the model has\nachieved in correctly locating and identifying 81% of the programs on prutor\ndataset of 1059 programs with only undeclared variable errors and also\ninferring their types correctly in 80% of the programs.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 11:14:14 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mohan", "Venkatesh Theru", ""], ["Jannesari", "Ali", ""]]}, {"id": "1907.06210", "submitter": "Mai Oudah", "authors": "Ella Noll, Mai Oudah, Nizar Habash", "title": "Simple Automatic Post-editing for Arabic-Japanese Machine Translation", "comments": "Machine translation, Automatic Post editing, Arabic, Japanese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common bottleneck for developing machine translation (MT) systems for some\nlanguage pairs is the lack of direct parallel translation data sets, in general\nand in certain domains. Alternative solutions such as zero-shot models or\npivoting techniques are successful in getting a strong baseline, but are often\nbelow the more supported language-pair systems. In this paper, we focus on\nArabic-Japanese machine translation, a less studied language pair; and we work\nwith a unique parallel corpus of Arabic news articles that were manually\ntranslated to Japanese. We use this parallel corpus to adapt a state-of-the-art\ndomain/genre agnostic neural MT system via a simple automatic post-editing\ntechnique. Our results and detailed analysis suggest that this approach is\nquite viable for less supported language pairs in specific domains.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 11:56:20 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Noll", "Ella", ""], ["Oudah", "Mai", ""], ["Habash", "Nizar", ""]]}, {"id": "1907.06226", "submitter": "Jipeng Qiang", "authors": "Jipeng Qiang and Yun Li and Yi Zhu and Yunhao Yuan and Xindong Wu", "title": "Lexical Simplification with Pretrained Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical simplification (LS) aims to replace complex words in a given sentence\nwith their simpler alternatives of equivalent meaning. Recently unsupervised\nlexical simplification approaches only rely on the complex word itself\nregardless of the given sentence to generate candidate substitutions, which\nwill inevitably produce a large number of spurious candidates. We present a\nsimple LS approach that makes use of the Bidirectional Encoder Representations\nfrom Transformers (BERT) which can consider both the given sentence and the\ncomplex word during generating candidate substitutions for the complex word.\nSpecifically, we mask the complex word of the original sentence for feeding\ninto the BERT to predict the masked token. The predicted results will be used\nas candidate substitutions. Despite being entirely unsupervised, experimental\nresults show that our approach obtains obvious improvement compared with these\nbaselines leveraging linguistic databases and parallel corpus, outperforming\nthe state-of-the-art by more than 12 Accuracy points on three well-known\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 14:19:22 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:36:41 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 03:36:12 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 01:48:46 GMT"}, {"version": "v5", "created": "Thu, 29 Oct 2020 03:21:25 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Qiang", "Jipeng", ""], ["Li", "Yun", ""], ["Zhu", "Yi", ""], ["Yuan", "Yunhao", ""], ["Wu", "Xindong", ""]]}, {"id": "1907.06292", "submitter": "Wenhan Xiong", "authors": "Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo Yu, Shiyu\n  Chang, Xiaoxiao Guo, William Yang Wang", "title": "TWEETQA: A Social Media Focused Question Answering Dataset", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With social media becoming increasingly pop-ular on which lots of news and\nreal-time eventsare reported, developing automated questionanswering systems is\ncritical to the effective-ness of many applications that rely on real-time\nknowledge. While previous datasets haveconcentrated on question answering (QA)\nforformal text like news and Wikipedia, wepresent the first large-scale dataset\nfor QA oversocial media data. To ensure that the tweetswe collected are useful,\nwe only gather tweetsused by journalists to write news articles. Wethen ask\nhuman annotators to write questionsand answers upon these tweets. Unlike\notherQA datasets like SQuAD in which the answersare extractive, we allow the\nanswers to be ab-stractive. We show that two recently proposedneural models\nthat perform well on formaltexts are limited in their performance when ap-plied\nto our dataset. In addition, even the fine-tuned BERT model is still lagging\nbehind hu-man performance with a large margin. Our re-sults thus point to the\nneed of improved QAsystems targeting social media text.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 22:20:59 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Xiong", "Wenhan", ""], ["Wu", "Jiawei", ""], ["Wang", "Hong", ""], ["Kulkarni", "Vivek", ""], ["Yu", "Mo", ""], ["Chang", "Shiyu", ""], ["Guo", "Xiaoxiao", ""], ["Wang", "William Yang", ""]]}, {"id": "1907.06330", "submitter": "Prateek Verma", "authors": "Prateek Verma, Aliasgar Kutiyanawala, Ke Shen", "title": "Ranking sentences from product description & bullets for better search", "comments": "Accepted at SIGIR eCom'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Products in an ecommerce catalog contain information-rich fields like\ndescription and bullets that can be useful to extract entities (attributes)\nusing NER based systems. However, these fields are often verbose and contain\nlot of information that is not relevant from a search perspective. Treating\neach sentence within these fields equally can lead to poor full text match and\nintroduce problems in extracting attributes to develop ontologies, semantic\nsearch etc. To address this issue, we describe two methods based on extractive\nsummarization with reinforcement learning by leveraging information in product\ntitles and search click through logs to rank sentences from bullets,\ndescription, etc. Finally, we compare the accuracy of these two models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 04:48:34 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Verma", "Prateek", ""], ["Kutiyanawala", "Aliasgar", ""], ["Shen", "Ke", ""]]}, {"id": "1907.06342", "submitter": "Sreeram Ganji Mr.", "authors": "Sreeram Ganji, Kunal Dhawan, Kumar Priyadarshi and Rohit Sinha", "title": "Joint Language Identification of Code-Switching Speech using Attention\n  based E2E Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language identification (LID) has relevance in many speech processing\napplications. For the automatic recognition of code-switching speech, the\nconventional approaches often employ an LID system for detecting the languages\npresent within an utterance. In the existing works, the LID on code-switching\nspeech involves modelling of the underlying languages separately. In this work,\nwe propose a joint modelling based LID system for code-switching speech. To\nachieve the same, an attention-based end-to-end (E2E) network has been\nexplored. For the development and evaluation of the proposed approach, a\nrecently created Hindi-English code-switching corpus has been used. For the\ncontrast purpose, an LID system employing the connectionist temporal\nclassification-based E2E network is also developed. On comparing both the LID\nsystems, the attention based approach is noted to result in better LID\naccuracy. The effective location of code-switching boundaries within the\nutterance by the proposed approach has been demonstrated by plotting the\nattention weights of E2E network.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 06:30:15 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ganji", "Sreeram", ""], ["Dhawan", "Kunal", ""], ["Priyadarshi", "Kumar", ""], ["Sinha", "Rohit", ""]]}, {"id": "1907.06385", "submitter": "Angela Fan", "authors": "Sidak Pal Singh, Angela Fan, Michael Auli", "title": "GLOSS: Generative Latent Optimization of Sentence Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn unsupervised sentence representations in a\nnon-compositional manner based on Generative Latent Optimization. Our approach\ndoes not impose any assumptions on how words are to be combined into a sentence\nrepresentation. We discuss a simple Bag of Words model as well as a variant\nthat models word positions. Both are trained to reconstruct the sentence based\non a latent code and our model can be used to generate text. Experiments show\nlarge improvements over the related Paragraph Vectors. Compared to uSIF, we\nachieve a relative improvement of 5% when trained on the same data and our\nmethod performs competitively to Sent2vec while trained on 30 times less data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 09:23:49 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Singh", "Sidak Pal", ""], ["Fan", "Angela", ""], ["Auli", "Michael", ""]]}, {"id": "1907.06407", "submitter": "Bal\\'azs Tarj\\'an", "authors": "Bal\\'azs Tarj\\'an, Gy\\\"orgy Szasz\\'ak, Tibor Fegy\\'o, P\\'eter Mihajlik", "title": "Investigation on N-gram Approximated RNNLMs for Recognition of\n  Morphologically Rich Speech", "comments": "12 pages, 2 figures, accepted for publication at SLSP 2019", "journal-ref": null, "doi": "10.1007/978-3-030-31372-2_19", "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of Hungarian conversational telephone speech is challenging due\nto the informal style and morphological richness of the language. Recurrent\nNeural Network Language Model (RNNLM) can provide remedy for the high\nperplexity of the task; however, two-pass decoding introduces a considerable\nprocessing delay. In order to eliminate this delay we investigate approaches\naiming at the complexity reduction of RNNLM, while preserving its accuracy. We\ncompare the performance of conventional back-off n-gram language models (BNLM),\nBNLM approximation of RNNLMs (RNN-BNLM) and RNN n-grams in terms of perplexity\nand word error rate (WER). Morphological richness is often addressed by using\nstatistically derived subwords - morphs - in the language models, hence our\ninvestigations are extended to morph-based models, as well. We found that using\nRNN-BNLMs 40% of the RNNLM perplexity reduction can be recovered, which is\nroughly equal to the performance of a RNN 4-gram model. Combining morph-based\nmodeling and approximation of RNNLM, we were able to achieve 8% relative WER\nreduction and preserve real-time operation of our conversational telephone\nspeech recognition system.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 10:07:07 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 08:29:32 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 11:47:18 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Tarj\u00e1n", "Bal\u00e1zs", ""], ["Szasz\u00e1k", "Gy\u00f6rgy", ""], ["Fegy\u00f3", "Tibor", ""], ["Mihajlik", "P\u00e9ter", ""]]}, {"id": "1907.06458", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Bla\\v{z} \\v{S}krlj, Andra\\v{z} Repar and Senja Pollak", "title": "RaKUn: Rank-based Keyword extraction via Unsupervised learning and Meta\n  vertex aggregation", "comments": "The final authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-31372-2_26", "journal-ref": "Statistical Language and Speech Processing 2019 Proceedings", "doi": "10.1007/978-3-030-31372-2_26", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyword extraction is used for summarizing the content of a document and\nsupports efficient document retrieval, and is as such an indispensable part of\nmodern text-based systems. We explore how load centrality, a graph-theoretic\nmeasure applied to graphs derived from a given text can be used to efficiently\nidentify and rank keywords. Introducing meta vertices (aggregates of existing\nvertices) and systematic redundancy filters, the proposed method performs on\npar with state-of-the-art for the keyword extraction task on 14 diverse\ndatasets. The proposed method is unsupervised, interpretable and can also be\nused for document visualization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 12:10:24 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 19:02:55 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 12:13:37 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["\u0160krlj", "Bla\u017e", ""], ["Repar", "Andra\u017e", ""], ["Pollak", "Senja", ""]]}, {"id": "1907.06488", "submitter": "Alexandre B\\'erard", "authors": "Alexandre B\\'erard, Ioan Calapodescu, Claude Roux", "title": "Naver Labs Europe's Systems for the WMT19 Machine Translation Robustness\n  Task", "comments": "WMT 2019 - Shared Task Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the systems that we submitted to the WMT19 Machine\nTranslation robustness task. This task aims to improve MT's robustness to noise\nfound on social media, like informal language, spelling mistakes and other\northographic variations. The organizers provide parallel data extracted from a\nsocial media website in two language pairs: French-English and Japanese-English\n(in both translation directions). The goal is to obtain the best scores on\nunseen test sets from the same source, according to automatic metrics (BLEU)\nand human evaluation. We proposed one single and one ensemble system for each\ntranslation direction. Our ensemble models ranked first in all language pairs,\naccording to BLEU evaluation. We discuss the pre-processing choices that we\nmade, and present our solutions for robustness to noise and domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 13:19:44 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["B\u00e9rard", "Alexandre", ""], ["Calapodescu", "Ioan", ""], ["Roux", "Claude", ""]]}, {"id": "1907.06554", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi and Hamed Zamani and Fabio Crestani and W. Bruce\n  Croft", "title": "Asking Clarifying Questions in Open-Domain Information-Seeking\n  Conversations", "comments": "To appear in SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331265", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users often fail to formulate their complex information needs in a single\nquery. As a consequence, they may need to scan multiple result pages or\nreformulate their queries, which may be a frustrating experience.\nAlternatively, systems can improve user satisfaction by proactively asking\nquestions of the users to clarify their information needs. Asking clarifying\nquestions is especially important in conversational systems since they can only\nreturn a limited number of (often only one) result(s). In this paper, we\nformulate the task of asking clarifying questions in open-domain\ninformation-seeking conversational systems. To this end, we propose an offline\nevaluation methodology for the task and collect a dataset, called Qulac,\nthrough crowdsourcing. Our dataset is built on top of the TREC Web Track\n2009-2012 data and consists of over 10K question-answer pairs for 198 TREC\ntopics with 762 facets. Our experiments on an oracle model demonstrate that\nasking only one good question leads to over 170% retrieval performance\nimprovement in terms of P@1, which clearly demonstrates the potential impact of\nthe task. We further propose a retrieval framework consisting of three\ncomponents: question retrieval, question selection, and document retrieval. In\nparticular, our question selection model takes into account the original query\nand previous question-answer interactions while selecting the next question.\nOur model significantly outperforms competitive baselines. To foster research\nin this area, we have made Qulac publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 15:45:37 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Zamani", "Hamed", ""], ["Crestani", "Fabio", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1907.06585", "submitter": "Thierry Boy de la Tour", "authors": "Thierry Boy de la Tour", "title": "Parallelism Theorem and Derived Rules for Parallel Coherent\n  Transformations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Independent Parallelism Theorem is proven in the theory of adhesive HLR\ncategories. It shows the bijective correspondence between sequential\nindependent and parallel independent direct derivations in the Weak\nDouble-Pushout framework, see [2]. The parallel derivations are expressed by\nmeans of Parallel Coherent Transformations (PCTs), hence without assuming the\nexistence of coproducts compatible with M as in the standard Parallelism\nTheorem. It is aslo shown that a derived rule can be extracted from any PCT, in\nthe sense that to any direct derivation of this rule corresponds a valid PCT.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 13:08:12 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["de la Tour", "Thierry Boy", ""]]}, {"id": "1907.06616", "submitter": "Sergey Edunov", "authors": "Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey\n  Edunov", "title": "Facebook FAIR's WMT19 News Translation Task Submission", "comments": "7 pages; WMT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Facebook FAIR's submission to the WMT19 shared news\ntranslation task. We participate in two language pairs and four language\ndirections, English <-> German and English <-> Russian. Following our\nsubmission from last year, our baseline systems are large BPE-based transformer\nmodels trained with the Fairseq sequence modeling toolkit which rely on sampled\nback-translations. This year we experiment with different bitext data filtering\nschemes, as well as with adding filtered back-translated data. We also ensemble\nand fine-tune our models on domain-specific data, then decode using noisy\nchannel model reranking. Our submissions are ranked first in all four\ndirections of the human evaluation campaign. On En->De, our system\nsignificantly outperforms other systems as well as human translations. This\nsystem improves upon our WMT'18 submission by 4.5 BLEU points.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:22:54 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ng", "Nathan", ""], ["Yee", "Kyra", ""], ["Baevski", "Alexei", ""], ["Ott", "Myle", ""], ["Auli", "Michael", ""], ["Edunov", "Sergey", ""]]}, {"id": "1907.06679", "submitter": "Falcon Dai", "authors": "Falcon Z. Dai, Zheng Cai", "title": "Towards Near-imperceptible Steganographic Text", "comments": "To appear at ACL 2019. Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the imperceptibility of several existing linguistic\nsteganographic systems (Fang et al., 2017; Yang et al., 2018) relies on\nimplicit assumptions on statistical behaviors of fluent text. We formally\nanalyze them and empirically evaluate these assumptions. Furthermore, based on\nthese observations, we propose an encoding algorithm called patient-Huffman\nwith improved near-imperceptible guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 18:17:13 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 22:28:16 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Dai", "Falcon Z.", ""], ["Cai", "Zheng", ""]]}, {"id": "1907.06745", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal and Peilin Zhou", "title": "Low-supervision urgency detection and transfer in short crisis messages", "comments": "8 pages, short version published in ASONAM 2019", "journal-ref": null, "doi": "10.1145/3341161.3342936", "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanitarian disasters have been on the rise in recent years due to the\neffects of climate change and socio-political situations such as the refugee\ncrisis. Technology can be used to best mobilize resources such as food and\nwater in the event of a natural disaster, by semi-automatically flagging tweets\nand short messages as indicating an urgent need. The problem is challenging not\njust because of the sparseness of data in the immediate aftermath of a\ndisaster, but because of the varying characteristics of disasters in developing\ncountries (making it difficult to train just one system) and the noise and\nquirks in social media. In this paper, we present a robust, low-supervision\nsocial media urgency system that adapts to arbitrary crises by leveraging both\nlabeled and unlabeled data in an ensemble setting. The system is also able to\nadapt to new crises where an unlabeled background corpus may not be available\nyet by utilizing a simple and effective transfer learning methodology.\nExperimentally, our transfer learning and low-supervision approaches are found\nto outperform viable baselines with high significance on myriad disaster\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:43:53 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Zhou", "Peilin", ""]]}, {"id": "1907.06773", "submitter": "Giovanni Sileno", "authors": "Giovanni Sileno", "title": "Logic Conditionals, Supervenience, and Selection Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principles of cognitive economy would require that concepts about objects,\nproperties and relations should be introduced only if they simplify the\nconceptualisation of a domain. Unexpectedly, classic logic conditionals,\nspecifying structures holding within elements of a formal conceptualisation, do\nnot always satisfy this crucial principle. The paper argues that this\nrequirement is captured by supervenience, hereby further identified as a\nproperty necessary for compression. The resulting theory suggests an\nalternative explanation of the empirical experiences observable in Wason's\nselection tasks, associating human performance with conditionals on the ability\nof dealing with compression, rather than with logic necessity.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 22:16:00 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 13:17:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Sileno", "Giovanni", ""]]}, {"id": "1907.06848", "submitter": "Zejie Zhou", "authors": "Zejie Zhou, Boleslaw K. Szymanski, Jianxi Gao", "title": "Modeling competitive evolution of multiple languages", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0232888", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing evidence demonstrates that in many places language coexistence has\nbecome ubiquitous and essential for supporting language and cultural diversity\nand associated with its financial and economic benefits. The competitive\nevolution among multiple languages determines the evolution outcome, either\ncoexistence, decline, or extinction. Here, we extend the Abrams-Strogatz model\nof language competition to multiple languages and then validate it by analyzing\nthe behavioral transitions of language usage over the recent several decades in\nSingapore and Hong Kong. In each case, we estimate from data the model\nparameters that measure each language utility for its speakers and the strength\nof two biases, the majority preference for their language, and the minority\naversion to it. The values of these two biases decide which language is the\nfastest growing in the competition and what would be the stable state of the\nsystem. We also study the system convergence time to stable states and discover\nthe existence of tipping points with multiple attractors. Moreover, the\ncritical slowdown of convergence to the stable fractions of language users\nappears near and peaks at the tipping points, signaling when the system\napproaches them. Our analysis furthers our understanding of multiple language\nevolution and the role of tipping points in behavioral transitions. These\ninsights may help to protect languages from extinction and retain the language\nand cultural diversity.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 05:31:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhou", "Zejie", ""], ["Szymanski", "Boleslaw K.", ""], ["Gao", "Jianxi", ""]]}, {"id": "1907.06860", "submitter": "Jianlin Shi", "authors": "Jianlin Shi, Kevin Graves, John F. Hurdle", "title": "A generic rule-based system for clinical trial patient selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The n2c2 2018 Challenge task 1 aimed to identify patients who meet lists of\nheterogeneous inclusion/exclusion criteria for a hypothetical clinical trial.\nWe demonstrate a generic rule-based natural language pipeline can support this\ntask with decent performance (the average F1 score on the test set is 0.89,\nranked the 8th out of 45 teams ).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 06:43:34 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Shi", "Jianlin", ""], ["Graves", "Kevin", ""], ["Hurdle", "John F.", ""]]}, {"id": "1907.06944", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Bla\\v{z} \\v{S}krlj and Senja Pollak", "title": "Language comparison via network topology", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-31372-2_10", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling relations between languages can offer understanding of language\ncharacteristics and uncover similarities and differences between languages.\nAutomated methods applied to large textual corpora can be seen as opportunities\nfor novel statistical studies of language development over time, as well as for\nimproving cross-lingual natural language processing techniques. In this work,\nwe first propose how to represent textual data as a directed, weighted network\nby the text2net algorithm. We next explore how various fast,\nnetwork-topological metrics, such as network community structure, can be used\nfor cross-lingual comparisons. In our experiments, we employ eight different\nnetwork topology metrics, and empirically showcase on a parallel corpus, how\nthe methods can be used for modeling the relations between nine selected\nlanguages. We demonstrate that the proposed method scales to large corpora\nconsisting of hundreds of thousands of aligned sentences on an of-the-shelf\nlaptop. We observe that on the one hand properties such as communities, capture\nsome of the known differences between the languages, while others can be seen\nas novel opportunities for linguistic studies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 11:33:04 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 14:19:19 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["\u0160krlj", "Bla\u017e", ""], ["Pollak", "Senja", ""]]}, {"id": "1907.06950", "submitter": "Sergey A. Slavnov", "authors": "Sergey Slavnov", "title": "Abstract categorial grammars with island constraints and effective\n  decidability", "comments": "This was a premature attempt, sorry", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known approach to treating syntactic island constraints in the setting\nof Lambek grammars consists in adding specific bracket modalities to the logic.\nWe adapt this approach to abstract categorial grammars (ACG). Thus we define\nbracketed (implicational) linear logic, bracketed lambda-calculus, and,\neventually, bracketed ACG based on bracketed $\\lambda$-calculus. This allows us\nmodeling at least simplest island constraints, typically, in the context of\nrelativization. Next we identify specific safely bracketed ACG which, just like\nordinary (bracket-free) second order ACG generate effectively decidable\nlanguages, but are sufficiently flexible to model some higher order phenomena\nlike relativization and correctly deal with syntactic islands, at least in\nsimple toy examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 11:52:46 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:10:04 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Slavnov", "Sergey", ""]]}, {"id": "1907.07033", "submitter": "Sooji Han", "authors": "Sooji Han, Jie Gao, Fabio Ciravegna", "title": "Neural Language Model Based Training Data Augmentation for Weakly\n  Supervised Early Rumor Detection", "comments": "8 pages", "journal-ref": "The 2019 IEEE/ACM International Conference on Advances in Social\n  Networks Analysis and Mining", "doi": "10.1145/3341161.3342892", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity and class imbalance of training data are known issues in current\nrumor detection tasks. We propose a straight-forward and general-purpose data\naugmentation technique which is beneficial to early rumor detection relying on\nevent propagation patterns. The key idea is to exploit massive unlabeled event\ndata sets on social media to augment limited labeled rumor source tweets. This\nwork is based on rumor spreading patterns revealed by recent rumor studies and\nsemantic relatedness between labeled and unlabeled data. A state-of-the-art\nneural language model (NLM) and large credibility-focused Twitter corpora are\nemployed to learn context-sensitive representations of rumor tweets. Six\ndifferent real-world events based on three publicly available rumor datasets\nare employed in our experiments to provide a comparative evaluation of the\neffectiveness of the method. The results show that our method can expand the\nsize of an existing rumor data set nearly by 200% and corresponding social\ncontext (i.e., conversational threads) by 100% with reasonable quality.\nPreliminary experiments with a state-of-the-art deep learning-based rumor\ndetection model show that augmented data can alleviate over-fitting and class\nimbalance caused by limited train data and can help to train complex neural\nnetworks (NNs). With augmented data, the performance of rumor detection can be\nimproved by 12.1% in terms of F-score. Our experiments also indicate that\naugmented training data can help to generalize rumor detection models on unseen\nrumors.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:32:33 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Han", "Sooji", ""], ["Gao", "Jie", ""], ["Ciravegna", "Fabio", ""]]}, {"id": "1907.07073", "submitter": "William Brannon", "authors": "Doug Beeferman, William Brannon and Deb Roy", "title": "RadioTalk: a large-scale corpus of talk radio transcripts", "comments": "5 pages, 4 figures, accepted by Interspeech 2019", "journal-ref": "Proc. Interspeech 2019, 564-568 (2019)", "doi": "10.21437/Interspeech.2019-2714", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RadioTalk, a corpus of speech recognition transcripts sampled\nfrom talk radio broadcasts in the United States between October of 2018 and\nMarch of 2019. The corpus is intended for use by researchers in the fields of\nnatural language processing, conversational analysis, and the social sciences.\nThe corpus encompasses approximately 2.8 billion words of automatically\ntranscribed speech from 284,000 hours of radio, together with metadata about\nthe speech, such as geographical location, speaker turn boundaries, gender, and\nradio program information. In this paper we summarize why and how we prepared\nthe corpus, give some descriptive statistics on stations, shows and speakers,\nand carry out a few high-level analyses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:34:58 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Beeferman", "Doug", ""], ["Brannon", "William", ""], ["Roy", "Deb", ""]]}, {"id": "1907.07265", "submitter": "Angelo Basile", "authors": "Angelo Basile, Albert Gatt, Malvina Nissim", "title": "You Write Like You Eat: Stylistic variation as a predictor of social\n  stratification", "comments": "11 pages, 5 figures, ACL Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by Labov's seminal work on stylistic variation as a function of\nsocial stratification, we develop and compare neural models that predict a\nperson's presumed socio-economic status, obtained through distant\nsupervision,from their writing style on social media. The focus of our work is\non identifying the most important stylistic parameters to predict\nsocio-economic group. In particular, we show the effectiveness of\nmorpho-syntactic features as stylistic predictors of socio-economic group,in\ncontrast to lexical features, which are good predictors of topic.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 21:20:49 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Basile", "Angelo", ""], ["Gatt", "Albert", ""], ["Nissim", "Malvina", ""]]}, {"id": "1907.07322", "submitter": "Thomas Searle", "authors": "Thomas Searle, Zeljko Kraljevic, Rebecca Bendayan, Daniel Bean,\n  Richard Dobson", "title": "MedCATTrainer: A Biomedical Free Text Annotation Interface with Active\n  Learning and Research Use Case Specific Customisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MedCATTrainer an interface for building, improving and customising\na given Named Entity Recognition and Linking (NER+L) model for biomedical\ndomain text. NER+L is often used as a first step in deriving value from\nclinical text. Collecting labelled data for training models is difficult due to\nthe need for specialist domain knowledge. MedCATTrainer offers an interactive\nweb-interface to inspect and improve recognised entities from an underlying\nNER+L model via active learning. Secondary use of data for clinical research\noften has task and context specific criteria. MedCATTrainer provides a further\ninterface to define and collect supervised learning training data for\nresearcher specific use cases. Initial results suggest our approach allows for\nefficient and accurate collection of research use case specific training data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:32:04 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Searle", "Thomas", ""], ["Kraljevic", "Zeljko", ""], ["Bendayan", "Rebecca", ""], ["Bean", "Daniel", ""], ["Dobson", "Richard", ""]]}, {"id": "1907.07323", "submitter": "L\\'eo Bouscarrat", "authors": "L\\'eo Bouscarrat, Antoine Bonnefoy, Thomas Peel, C\\'ecile Pereira", "title": "STRASS: A Light and Effective Method for Extractive Summarization Based\n  on Sentence Embeddings", "comments": "To appear in 2019 ACL Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces STRASS: Summarization by TRAnsformation Selection and\nScoring. It is an extractive text summarization method which leverages the\nsemantic information in existing sentence embedding spaces. Our method creates\nan extractive summary by selecting the sentences with the closest embeddings to\nthe document embedding. The model learns a transformation of the document\nembedding to minimize the similarity between the extractive summary and the\nground truth summary. As the transformation is only composed of a dense layer,\nthe training can be done on CPU, therefore, inexpensive. Moreover, inference\ntime is short and linear according to the number of sentences. As a second\ncontribution, we introduce the French CASS dataset, composed of judgments from\nthe French Court of cassation and their corresponding summaries. On this\ndataset, our results show that our method performs similarly to the state of\nthe art extractive methods with effective training and inferring time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 16:14:09 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Bouscarrat", "L\u00e9o", ""], ["Bonnefoy", "Antoine", ""], ["Peel", "Thomas", ""], ["Pereira", "C\u00e9cile", ""]]}, {"id": "1907.07328", "submitter": "Peng Wu", "authors": "Peng Wu, Shujian Huang, Rongxiang Weng, Zaixiang Zheng, Jianbing\n  Zhang, Xiaohui Yan and Jiajun Chen", "title": "Learning Representation Mapping for Relation Detection in Knowledge Base\n  Question Answering", "comments": "10 pages, 5 figures, accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation detection is a core step in many natural language process\napplications including knowledge base question answering. Previous efforts show\nthat single-fact questions could be answered with high accuracy. However, one\ncritical problem is that current approaches only get high accuracy for\nquestions whose relations have been seen in the training data. But for unseen\nrelations, the performance will drop rapidly. The main reason for this problem\nis that the representations for unseen relations are missing. In this paper, we\npropose a simple mapping method, named representation adapter, to learn the\nrepresentation mapping for both seen and unseen relations based on previously\nlearned relation embedding. We employ the adversarial objective and the\nreconstruction objective to improve the mapping performance. We re-organize the\npopular SimpleQuestion dataset to reveal and evaluate the problem of detecting\nunseen relations. Experiments show that our method can greatly improve the\nperformance of unseen relations while the performance for those seen part is\nkept comparable to the state-of-the-art. Our code and data are available at\nhttps://github.com/wudapeng268/KBQA-Adapter.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 04:29:27 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Wu", "Peng", ""], ["Huang", "Shujian", ""], ["Weng", "Rongxiang", ""], ["Zheng", "Zaixiang", ""], ["Zhang", "Jianbing", ""], ["Yan", "Xiaohui", ""], ["Chen", "Jiajun", ""]]}, {"id": "1907.07347", "submitter": "Timothy Niven", "authors": "Kai-Chou Yang, Timothy Niven, Hung-Yu Kao", "title": "Fake News Detection as Natural Language Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the entry by the Intelligent Knowledge Management (IKM)\nLab in the WSDM 2019 Fake News Classification challenge. We treat the task as\nnatural language inference (NLI). We individually train a number of the\nstrongest NLI models as well as BERT. We ensemble these results and retrain\nwith noisy labels in two stages. We analyze transitivity relations in the train\nand test sets and determine a set of test cases that can be reliably classified\non this basis. The remainder of test cases are classified by our ensemble. Our\nentry achieves test set accuracy of 88.063% for 3rd place in the competition.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 06:03:17 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Yang", "Kai-Chou", ""], ["Niven", "Timothy", ""], ["Kao", "Hung-Yu", ""]]}, {"id": "1907.07355", "submitter": "Timothy Niven", "authors": "Timothy Niven, Hung-Yu Kao", "title": "Probing Neural Network Comprehension of Natural Language Arguments", "comments": "ACL 2019 (Updated Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are surprised to find that BERT's peak performance of 77% on the Argument\nReasoning Comprehension Task reaches just three points below the average\nuntrained human baseline. However, we show that this result is entirely\naccounted for by exploitation of spurious statistical cues in the dataset. We\nanalyze the nature of these cues and demonstrate that a range of models all\nexploit them. This analysis informs the construction of an adversarial dataset\non which all models achieve random accuracy. Our adversarial dataset provides a\nmore robust assessment of argument comprehension and should be adopted as the\nstandard in future work.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 06:26:20 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 04:07:54 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Niven", "Timothy", ""], ["Kao", "Hung-Yu", ""]]}, {"id": "1907.07366", "submitter": "Yen Hao Huang", "authors": "Yen-Hao Huang, Yi-Hsin Chen, Fernando Henrique Calderon Alvarado,\n  Ssu-Rui Lee, Shu-I Wu, Yuwen Lai and Yi-Shin Chen", "title": "Leveraging Linguistic Characteristics for Bipolar Disorder Recognition\n  with Gender Differences", "comments": "Accepted by DSHealth '19: 2019 KDD Workshop on Applied Data Science\n  for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous studies on automatic recognition model for bipolar disorder\n(BD) were based on both social media and linguistic features. The present study\ninvestigates the possibility of adopting only language-based features, namely\nthe syntax and morpheme collocation. We also examine the effect of gender on\nthe results considering gender has long been recognized as an important\nmodulating factor for mental disorders, yet it received little attention in\nprevious linguistic models. The present study collects Twitter posts 3 months\nprior to the self-disclosure by 349 BD users (231 female, 118 male). We\nconstruct a set of syntactic patterns in terms of the word usage based on graph\npattern construction and pattern attention mechanism. The factors examined are\ngender differences, syntactic patterns, and bipolar recognition performance.\nThe performance indicates our F1 scores reach over 91% and outperform several\nbaselines, including those using TF-IDF, LIWC and pre-trained language models\n(ELMO and BERT). The contributions of the present study are: (1) The features\nare contextualized, domain-agnostic, and purely linguistic. (2) The performance\nof BD recognition is improved by gender-enriched linguistic pattern features,\nwhich are constructed with gender differences in language usage.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 07:37:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Huang", "Yen-Hao", ""], ["Chen", "Yi-Hsin", ""], ["Alvarado", "Fernando Henrique Calderon", ""], ["Lee", "Ssu-Rui", ""], ["Wu", "Shu-I", ""], ["Lai", "Yuwen", ""], ["Chen", "Yi-Shin", ""]]}, {"id": "1907.07421", "submitter": "Hwaran Lee", "authors": "Hwaran Lee, Jinsik Lee, Tae-Yoon Kim", "title": "SUMBT: Slot-Utterance Matching for Universal and Scalable Belief\n  Tracking", "comments": "6 pages, 2 figures, The 57th Annual Meeting of the Association for\n  Computational Linguistics (ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In goal-oriented dialog systems, belief trackers estimate the probability\ndistribution of slot-values at every dialog turn. Previous neural approaches\nhave modeled domain- and slot-dependent belief trackers, and have difficulty in\nadding new slot-values, resulting in lack of flexibility of domain ontology\nconfigurations. In this paper, we propose a new approach to universal and\nscalable belief tracker, called slot-utterance matching belief tracker (SUMBT).\nThe model learns the relations between domain-slot-types and slot-values\nappearing in utterances through attention mechanisms based on contextual\nsemantic vectors. Furthermore, the model predicts slot-value labels in a\nnon-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and\nMultiWOZ, the proposed model showed performance improvement in comparison with\nslot-dependent methods and achieved the state-of-the-art joint accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 10:03:38 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Lee", "Hwaran", ""], ["Lee", "Jinsik", ""], ["Kim", "Tae-Yoon", ""]]}, {"id": "1907.07507", "submitter": "Guntis Barzdins", "authors": "Guntis Barzdins and Eduards Sidorovics", "title": "Differentiable Disentanglement Filter: an Application Agnostic Core\n  Concept Discovery Probe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been speculated that deep neural networks function by discovering\na hierarchical set of domain-specific core concepts or patterns, which are\nfurther combined to recognize even more elaborate concepts for the\nclassification or other machine learning tasks. Meanwhile disentangling the\nactual core concepts engrained in the word embeddings (like word2vec or BERT)\nor deep convolutional image recognition neural networks (like PG-GAN) is\ndifficult and some success there has been achieved only recently. In this paper\nwe propose a novel neural network nonlinearity named Differentiable\nDisentanglement Filter (DDF) which can be transparently inserted into any\nexisting neural network layer to automatically disentangle the core concepts\nused by that layer. The DDF probe is inspired by the obscure properties of the\nhyper-dimensional computing theory. The DDF proof-of-concept implementation is\nshown to disentangle concepts within the neural 3D scene representation - a\ntask vital for visual grounding of natural language narratives.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:31:31 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 08:06:31 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Barzdins", "Guntis", ""], ["Sidorovics", "Eduards", ""]]}, {"id": "1907.07526", "submitter": "Valentina Bellomaria", "authors": "Valentina Bellomaria, Giuseppe Castellucci, Andrea Favalli and Raniero\n  Romagnoli", "title": "Almawave-SLU: A new dataset for SLU in Italian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The widespread use of conversational and question answering systems made it\nnecessary to improve the performances of speaker intent detection and\nunderstanding of related semantic slots, i.e., Spoken Language Understanding\n(SLU). Often, these tasks are approached with supervised learning methods,\nwhich needs considerable labeled datasets. This paper presents the first\nItalian dataset for SLU. It is derived through a semi-automatic procedure and\nis used as a benchmark of various open source and commercial systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:52:54 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Bellomaria", "Valentina", ""], ["Castellucci", "Giuseppe", ""], ["Favalli", "Andrea", ""], ["Romagnoli", "Raniero", ""]]}, {"id": "1907.07564", "submitter": "Nishchay Sharma", "authors": "Madan Gopal Jhawar, Vipindeep Vangala, Nishchay Sharma, Ankur\n  Hayatnagarkar, Mansi Saxena, Swati Valecha", "title": "Conversational Help for Task Completion and Feature Discovery in\n  Personal Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intelligent Personal Assistants (IPAs) have become widely popular in recent\ntimes. Most of the commercial IPAs today support a wide range of skills\nincluding Alarms, Reminders, Weather Updates, Music, News, Factual\nQuestioning-Answering, etc. The list grows every day, making it difficult to\nremember the command structures needed to execute various tasks. An IPA must\nhave the ability to communicate information about supported skills and direct\nusers towards the right commands needed to execute them. Users interact with\npersonal assistants in natural language. A query is defined to be a Help Query\nif it seeks information about a personal assistant's capabilities, or asks for\ninstructions to execute a task. In this paper, we propose an interactive system\nwhich identifies help queries and retrieves appropriate responses. Our system\ncomprises of a C-BiLSTM based classifier, which is a fusion of Convolutional\nNeural Networks (CNN) and Bidirectional LSTM (BiLSTM) architectures, to detect\nhelp queries and a semantic Approximate Nearest Neighbours (ANN) module to map\nthe query to an appropriate predefined response. Evaluation of our system on\nreal-world queries from a commercial IPA and a detailed comparison with popular\ntraditional machine learning and deep learning based models reveal that our\nsystem outperforms other approaches and returns relevant responses for help\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:25:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Jhawar", "Madan Gopal", ""], ["Vangala", "Vipindeep", ""], ["Sharma", "Nishchay", ""], ["Hayatnagarkar", "Ankur", ""], ["Saxena", "Mansi", ""], ["Valecha", "Swati", ""]]}, {"id": "1907.07626", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang and Liming Song", "title": "AP19-OLR Challenge: Three Tasks and Their Baselines", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.00616,\n  arXiv:1706.09742, arXiv:1609.08445", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the fourth oriental language recognition (OLR)\nchallenge AP19-OLR, including the data profile, the tasks and the evaluation\nprinciples. The OLR challenge has been held successfully for three consecutive\nyears, along with APSIPA Annual Summit and Conference (APSIPA ASC). The\nchallenge this year still focuses on practical and challenging tasks, precisely\n(1) short-utterance LID, (2) cross-channel LID and (3) zero-resource LID. The\nevent this year includes more languages and more real-life data provided by\nSpeechOcean and the NSFC M2ASR project. All the data is free for participants.\nRecipes for x-vector system and back-end evaluation are also conducted as\nbaselines for the three tasks. The participants can refer to these\nonline-published recipes to deploy LID systems for convenience. We report the\nbaseline results on the three tasks and demonstrate that the three tasks are\nworth some efforts to achieve better performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 03:55:21 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 09:46:14 GMT"}, {"version": "v3", "created": "Sun, 1 Sep 2019 10:25:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Song", "Liming", ""]]}, {"id": "1907.07638", "submitter": "Jatin Ganhotra", "authors": "Janarthanan Rajendran, Jatin Ganhotra, Lazaros Polymenakos", "title": "Learning End-to-End Goal-Oriented Dialog with Maximal User Task Success\n  and Minimal Human Agent Use", "comments": "Author final version of article accepted for publication in TACL -\n  https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00274 and oral\n  presentation at ACL 2019", "journal-ref": "Transactions of the Association for Computational Linguistics 2019\n  Vol. 7, 375-386", "doi": "10.1162/tacl_a_00274", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural end-to-end goal-oriented dialog systems showed promise to reduce the\nworkload of human agents for customer service, as well as reduce wait time for\nusers. However, their inability to handle new user behavior at deployment has\nlimited their usage in real world. In this work, we propose an end-to-end\ntrainable method for neural goal-oriented dialog systems which handles new user\nbehaviors at deployment by transferring the dialog to a human agent\nintelligently. The proposed method has three goals: 1) maximize user's task\nsuccess by transferring to human agents, 2) minimize the load on the human\nagents by transferring to them only when it is essential and 3) learn online\nfrom the human agent's responses to reduce human agents load further. We\nevaluate our proposed method on a modified-bAbI dialog task that simulates the\nscenario of new user behaviors occurring at test time. Experimental results\nshow that our proposed method is effective in achieving the desired goals.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 16:50:16 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Rajendran", "Janarthanan", ""], ["Ganhotra", "Jatin", ""], ["Polymenakos", "Lazaros", ""]]}, {"id": "1907.07653", "submitter": "Prabod Rathnayaka", "authors": "Prabod Rathnayaka, Supun Abeysinghe, Chamod Samarajeewa, Isura\n  Manchanayake, Malaka J.Walpola, Rashmika Nawaratne, Tharindu Bandaragoda and\n  Damminda Alahakoon", "title": "Gated Recurrent Neural Network Approach for Multilabel Emotion Detection\n  in Microblogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People express their opinions and emotions freely in social media posts and\nonline reviews that contain valuable feedback for multiple stakeholders such as\nbusinesses and political campaigns. Manually extracting opinions and emotions\nfrom large volumes of such posts is an impossible task. Therefore, automated\nprocessing of these posts to extract opinions and emotions is an important\nresearch problem. However, human emotion detection is a challenging task due to\nthe complexity and nuanced nature. To overcome these barriers, researchers have\nextensively used techniques such as deep learning, distant supervision, and\ntransfer learning. In this paper, we propose a novel Pyramid Attention Network\n(PAN) based model for emotion detection in microblogs. The main advantage of\nour approach is that PAN has the capability to evaluate sentences in different\nperspectives to capture multiple emotions existing in a single text. The\nproposed model was evaluated on a recently released dataset and the results\nachieved the state-of-the-art accuracy of 58.9%.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 17:33:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Rathnayaka", "Prabod", ""], ["Abeysinghe", "Supun", ""], ["Samarajeewa", "Chamod", ""], ["Manchanayake", "Isura", ""], ["Walpola", "Malaka J.", ""], ["Nawaratne", "Rashmika", ""], ["Bandaragoda", "Tharindu", ""], ["Alahakoon", "Damminda", ""]]}, {"id": "1907.07672", "submitter": "Shahin Atakishiyev", "authors": "Shahin Atakishiyev, Marek Z. Reformat", "title": "Analysis of Word Embeddings Using Fuzzy Clustering", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data dominated systems and applications, a concept of representing words\nin a numerical format has gained a lot of attention. There are a few approaches\nused to generate such a representation. An interesting issue that should be\nconsidered is the ability of such representations - called embeddings - to\nimitate human-based semantic similarity between words. In this study, we\nperform a fuzzy-based analysis of vector representations of words, i.e., word\nembeddings. We use two popular fuzzy clustering algorithms on count-based word\nembeddings, known as GloVe, of different dimensionality. Words from\nWordSim-353, called the gold standard, are represented as vectors and\nclustered. The results indicate that fuzzy clustering algorithms are very\nsensitive to high-dimensional data, and parameter tuning can dramatically\nchange their performance. We show that by adjusting the value of the fuzzifier\nparameter, fuzzy clustering can be successfully applied to vectors of high - up\nto one hundred - dimensions. Additionally, we illustrate that fuzzy clustering\nallows to provide interesting results regarding membership of words to\ndifferent clusters.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:40:46 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 07:48:52 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 07:56:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Atakishiyev", "Shahin", ""], ["Reformat", "Marek Z.", ""]]}, {"id": "1907.07757", "submitter": "Fan Yang", "authors": "Fan Yang, Shiva K. Pentyala, Sina Mohseni, Mengnan Du, Hao Yuan, Rhema\n  Linder, Eric D. Ragan, Shuiwang Ji, Xia Hu", "title": "XFake: Explainable Fake News Detector with Visualizations", "comments": "4 pages, WebConf'2019 Demo", "journal-ref": null, "doi": "10.1145/3308558.3314119", "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo paper, we present the XFake system, an explainable fake news\ndetector that assists end-users to identify news credibility. To effectively\ndetect and interpret the fakeness of news items, we jointly consider both\nattributes (e.g., speaker) and statements. Specifically, MIMIC, ATTN and PERT\nframeworks are designed, where MIMIC is built for attribute analysis, ATTN is\nfor statement semantic analysis and PERT is for statement linguistic analysis.\nBeyond the explanations extracted from the designed frameworks, relevant\nsupporting examples as well as visualization are further provided to facilitate\nthe interpretation. Our implemented system is demonstrated on a real-world\ndataset crawled from PolitiFact, where thousands of verified political news\nhave been collected.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:29:58 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Yang", "Fan", ""], ["Pentyala", "Shiva K.", ""], ["Mohseni", "Sina", ""], ["Du", "Mengnan", ""], ["Yuan", "Hao", ""], ["Linder", "Rhema", ""], ["Ragan", "Eric D.", ""], ["Ji", "Shuiwang", ""], ["Hu", "Xia", ""]]}, {"id": "1907.07804", "submitter": "Subhojeet Pramanik", "authors": "Subhojeet Pramanik, Priyanka Agrawal, Aman Hussain", "title": "OmniNet: A unified architecture for multi-modal multi-task learning", "comments": "Source code available at: https://github.com/subho406/OmniNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is a popularly used neural network architecture, especially for\nlanguage understanding. We introduce an extended and unified architecture that\ncan be used for tasks involving a variety of modalities like image, text,\nvideos, etc. We propose a spatio-temporal cache mechanism that enables learning\nspatial dimension of the input in addition to the hidden states corresponding\nto the temporal input sequence. The proposed architecture further enables a\nsingle model to support tasks with multiple input modalities as well as\nasynchronous multi-task learning, thus we refer to it as OmniNet. For example,\na single instance of OmniNet can concurrently learn to perform the tasks of\npart-of-speech tagging, image captioning, visual question answering and video\nactivity recognition. We demonstrate that training these four tasks together\nresults in about three times compressed model while retaining the performance\nin comparison to training them individually. We also show that using this\nneural network pre-trained on some modalities assists in learning unseen tasks\nsuch as video captioning and video question answering. This illustrates the\ngeneralization capacity of the self-attention mechanism on the spatio-temporal\ncache present in OmniNet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:59:56 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 09:59:06 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Pramanik", "Subhojeet", ""], ["Agrawal", "Priyanka", ""], ["Hussain", "Aman", ""]]}, {"id": "1907.07818", "submitter": "Manash Pratim Barman", "authors": "Manash Pratim Barman, Amit Awekar, Sambhav Kothari", "title": "Decoding the Style and Bias of Song Lyrics", "comments": "Accepted for ACM SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central idea of this paper is to gain a deeper understanding of song\nlyrics computationally. We focus on two aspects: style and biases of song\nlyrics. All prior works to understand these two aspects are limited to manual\nanalysis of a small corpus of song lyrics. In contrast, we analyzed more than\nhalf a million songs spread over five decades. We characterize the lyrics style\nin terms of vocabulary, length, repetitiveness, speed, and readability. We have\nobserved that the style of popular songs significantly differs from other\nsongs. We have used distributed representation methods and WEAT test to measure\nvarious gender and racial biases in the song lyrics. We have observed that\nbiases in song lyrics correlate with prior results on human subjects. This\ncorrelation indicates that song lyrics reflect the biases that exist in\nsociety. Increasing consumption of music and the effect of lyrics on human\nemotions makes this analysis important.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:57:46 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Barman", "Manash Pratim", ""], ["Awekar", "Amit", ""], ["Kothari", "Sambhav", ""]]}, {"id": "1907.07826", "submitter": "Md. Ataur Rahman", "authors": "Md. Ataur Rahman and Md. Hanif Seddiqui", "title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 01:00:42 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Rahman", "Md. Ataur", ""], ["Seddiqui", "Md. Hanif", ""]]}, {"id": "1907.07950", "submitter": "Miryam de Lhoneux", "authors": "Miryam de Lhoneux, Sara Stymne, Joakim Nivre", "title": "What Should/Do/Can LSTMs Learn When Parsing Auxiliary Verb\n  Constructions?", "comments": "Accepted by the Computational Linguistics journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in investigating what neural NLP models learn\nabout language. A prominent open question is the question of whether or not it\nis necessary to model hierarchical structure. We present a linguistic\ninvestigation of a neural parser adding insights to this question. We look at\ntransitivity and agreement information of auxiliary verb constructions (AVCs)\nin comparison to finite main verbs (FMVs). This comparison is motivated by\ntheoretical work in dependency grammar and in particular the work of Tesni\\`ere\n(1959) where AVCs and FMVs are both instances of a nucleus, the basic unit of\nsyntax. An AVC is a dissociated nucleus, it consists of at least two words, and\nan FMV is its non-dissociated counterpart, consisting of exactly one word. We\nsuggest that the representation of AVCs and FMVs should capture similar\ninformation. We use diagnostic classifiers to probe agreement and transitivity\ninformation in vectors learned by a transition-based neural parser in four\ntypologically different languages. We find that the parser learns different\ninformation about AVCs and FMVs if only sequential models (BiLSTMs) are used in\nthe architecture but similar information when a recursive layer is used. We\nfind explanations for why this is the case by looking closely at how\ninformation is learned in the network and looking at what happens with\ndifferent dependency representations of AVCs. We conclude that there may be\nbenefits to using a recursive layer in dependency parsing and that we have not\nyet found the best way to integrate it in our parsers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:37:38 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 07:31:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["de Lhoneux", "Miryam", ""], ["Stymne", "Sara", ""], ["Nivre", "Joakim", ""]]}, {"id": "1907.07972", "submitter": "Zulfat Miftahutdinov", "authors": "Zulfat Miftahutdinov and Elena Tutubalina", "title": "Deep Neural Models for Medical Concept Normalization in User-Generated\n  Texts", "comments": "This is preprint of the paper \"Deep Neural Models for Medical Concept\n  Normalization in User-Generated Texts\" to be published at ACL 2019 - 57th\n  Annual Meeting of the Association for Computational Linguistics, Proceedings\n  of the Student Research Workshop", "journal-ref": null, "doi": "10.18653/v1/P19-2055", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the medical concept normalization problem, i.e.,\nthe problem of mapping a health-related entity mention in a free-form text to a\nconcept in a controlled vocabulary, usually to the standard thesaurus in the\nUnified Medical Language System (UMLS). This is a challenging task since\nmedical terminology is very different when coming from health care\nprofessionals or from the general public in the form of social media texts. We\napproach it as a sequence learning problem with powerful neural networks such\nas recurrent neural networks and contextualized word representation models\ntrained to obtain semantic representations of social media expressions. Our\nexperimental evaluation over three different benchmarks shows that neural\narchitectures leverage the semantic meaning of the entity mention and\nsignificantly outperform an existing state of the art models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:36:03 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Miftahutdinov", "Zulfat", ""], ["Tutubalina", "Elena", ""]]}, {"id": "1907.08015", "submitter": "Zhongyang Li", "authors": "Xiao Ding, Zhongyang Li, Ting Liu and Kuo Liao", "title": "ELG: An Event Logic Graph", "comments": "arXiv admin note: text overlap with arXiv:1805.05081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution and development of events have their own basic principles,\nwhich make events happen sequentially. Therefore, the discovery of such\nevolutionary patterns among events are of great value for event prediction,\ndecision-making and scenario design of dialog systems. However, conventional\nknowledge graph mainly focuses on the entities and their relations, which\nneglects the real world events. In this paper, we present a novel type of\nknowledge base - Event Logic Graph (ELG), which can reveal evolutionary\npatterns and development logics of real world events. Specifically, ELG is a\ndirected cyclic graph, whose nodes are events, and edges stand for the\nsequential, causal, conditional or hypernym-hyponym (is-a) relations between\nevents. We constructed two domain ELG: financial domain ELG, which consists of\nmore than 1.5 million of event nodes and more than 1.8 million of directed\nedges, and travel domain ELG, which consists of about 30 thousand of event\nnodes and more than 234 thousand of directed edges. Experimental results show\nthat ELG is effective for the task of script event prediction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:39:12 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 17:44:58 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Ding", "Xiao", ""], ["Li", "Zhongyang", ""], ["Liu", "Ting", ""], ["Liao", "Kuo", ""]]}, {"id": "1907.08158", "submitter": "Gongbo Tang", "authors": "Gongbo Tang, Rico Sennrich, Joakim Nivre", "title": "Understanding Neural Machine Translation by Simplification: The Case of\n  Encoder-free Models", "comments": "Accepted by RANLP 2019, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to understand neural machine translation (NMT) via\nsimplifying NMT architectures and training encoder-free NMT models. In an\nencoder-free model, the sums of word embeddings and positional embeddings\nrepresent the source. The decoder is a standard Transformer or recurrent neural\nnetwork that directly attends to embeddings via attention mechanisms.\nExperimental results show (1) that the attention mechanism in encoder-free\nmodels acts as a strong feature extractor, (2) that the word embeddings in\nencoder-free models are competitive to those in conventional models, (3) that\nnon-contextualized source representations lead to a big performance drop, and\n(4) that encoder-free models have different effects on alignment quality for\nGerman-English and Chinese-English.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 16:59:40 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tang", "Gongbo", ""], ["Sennrich", "Rico", ""], ["Nivre", "Joakim", ""]]}, {"id": "1907.08167", "submitter": "Huaixiu Zheng", "authors": "Yue Weng, Huaixiu Zheng, Franziska Bell, Gokhan Tur", "title": "OCC: A Smart Reply System for Efficient In-App Communications", "comments": "link to demo: https://www.youtube.com/watch?v=nOffUT7rS0A&t=32s", "journal-ref": "KDD 19, August 4-8, 2019, Anchorage, AK, USA", "doi": "10.1145/3292500.3330694", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart reply systems have been developed for various messaging platforms. In\nthis paper, we introduce Uber's smart reply system: one-click-chat (OCC), which\nis a key enhanced feature on top of the Uber in-app chat system. It enables\ndriver-partners to quickly respond to rider messages using smart replies. The\nsmart replies are dynamically selected according to conversation content using\nmachine learning algorithms. Our system consists of two major components:\nintent detection and reply retrieval, which are very different from standard\nsmart reply systems where the task is to directly predict a reply. It is\ndesigned specifically for mobile applications with short and non-canonical\nmessages. Reply retrieval utilizes pairings between intent and reply based on\ntheir popularity in chat messages as derived from historical data. For intent\ndetection, a set of embedding and classification techniques are experimented\nwith, and we choose to deploy a solution using unsupervised distributed\nembedding and nearest-neighbor classifier. It has the advantage of only\nrequiring a small amount of labeled training data, simplicity in developing and\ndeploying to production, and fast inference during serving and hence highly\nscalable. At the same time, it performs comparably with deep learning\narchitectures such as word-level convolutional neural network. Overall, the\nsystem achieves a high accuracy of 76% on intent detection. Currently, the\nsystem is deployed in production for English-speaking countries and 71% of\nin-app communications between riders and driver-partners adopted the smart\nreplies to speedup the communication process.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 17:19:30 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Weng", "Yue", ""], ["Zheng", "Huaixiu", ""], ["Bell", "Franziska", ""], ["Tur", "Gokhan", ""]]}, {"id": "1907.08176", "submitter": "Tiantian Gao", "authors": "Tiantian Gao, Paul Fodor, Michael Kifer", "title": "Querying Knowledge via Multi-Hop English Questions", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 636-653", "doi": "10.1017/S1471068419000103", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inherent difficulty of knowledge specification and the lack of trained\nspecialists are some of the key obstacles on the way to making intelligent\nsystems based on the knowledge representation and reasoning (KRR) paradigm\ncommonplace. Knowledge and query authoring using natural language, especially\ncontrolled natural language (CNL), is one of the promising approaches that\ncould enable domain experts, who are not trained logicians, to both create\nformal knowledge and query it. In previous work, we introduced the KALM system\n(Knowledge Authoring Logic Machine) that supports knowledge authoring (and\nsimple querying) with very high accuracy that at present is unachievable via\nmachine learning approaches. The present paper expands on the question\nanswering aspect of KALM and introduces KALM-QA (KALM for Question Answering)\nthat is capable of answering much more complex English questions. We show that\nKALM-QA achieves 100% accuracy on an extensive suite of movie-related\nquestions, called MetaQA, which contains almost 29,000 test questions and over\n260,000 training questions. We contrast this with a published machine learning\napproach, which falls far short of this high mark.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 17:37:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gao", "Tiantian", ""], ["Fodor", "Paul", ""], ["Kifer", "Michael", ""]]}, {"id": "1907.08184", "submitter": "Jingyuan Zhang", "authors": "Jingyuan Zhang, Timothy Baldwin", "title": "Evaluating the Utility of Document Embedding Vector Difference for\n  Relation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that vector offsets obtained by subtracting\npretrained word embedding vectors can be used to predict lexical relations with\nsurprising accuracy. Inspired by this finding, in this paper, we extend the\nidea to the document level, in generating document-level embeddings,\ncalculating the distance between them, and using a linear classifier to\nclassify the relation between the documents. In the context of duplicate\ndetection and dialogue act tagging tasks, we show that document-level\ndifference vectors have utility in assessing document-level similarity, but\nperform less well in multi-relational classification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 17:47:22 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Zhang", "Jingyuan", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1907.08236", "submitter": "Jonathan K Kummerfeld", "authors": "Jonathan K. Kummerfeld", "title": "SLATE: A Super-Lightweight Annotation Tool for Experts", "comments": "To appear at ACL as a demo", "journal-ref": "ACL: Demonstrations (2019) 7-12", "doi": "10.18653/v1/P19-3002", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many annotation tools have been developed, covering a wide variety of tasks\nand providing features like user management, pre-processing, and automatic\nlabeling. However, all of these tools use Graphical User Interfaces, and often\nrequire substantial effort to install and configure. This paper presents a new\nannotation tool that is designed to fill the niche of a lightweight interface\nfor users with a terminal-based workflow. Slate supports annotation at\ndifferent scales (spans of characters, tokens, and lines, or a document) and of\ndifferent types (free text, labels, and links), with easily customisable\nkeybindings, and unicode support. In a user study comparing with other tools it\nwas consistently the easiest to install and use. Slate fills a need not met by\nexisting systems, and has already been used to annotate two corpora, one of\nwhich involved over 250 hours of annotation effort.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:32:16 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kummerfeld", "Jonathan K.", ""]]}, {"id": "1907.08243", "submitter": "Pedro Henrique Alves Martins", "authors": "Pedro Henrique Martins, Zita Marinho, Andr\\'e F. T. Martins", "title": "Joint Learning of Named Entity Recognition and Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) and entity linking (EL) are two fundamentally\nrelated tasks, since in order to perform EL, first the mentions to entities\nhave to be detected. However, most entity linking approaches disregard the\nmention detection part, assuming that the correct mentions have been previously\ndetected. In this paper, we perform joint learning of NER and EL to leverage\ntheir relatedness and obtain a more robust and generalisable system. For that,\nwe introduce a model inspired by the Stack-LSTM approach (Dyer et al., 2015).\nWe observe that, in fact, doing multi-task learning of NER and EL improves the\nperformance in both tasks when comparing with models trained with individual\nobjectives. Furthermore, we achieve results competitive with the\nstate-of-the-art in both NER and EL.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:47:33 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Martins", "Pedro Henrique", ""], ["Marinho", "Zita", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "1907.08259", "submitter": "Mukul Bhutani", "authors": "Prakhar Gupta, Vinayshekhar Bannihatti Kumar, Mukul Bhutani, Alan W\n  Black", "title": "WriterForcing: Generating more interesting story endings", "comments": "Accepted in ACL workshop on Storytelling 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of generating interesting endings for stories. Neural\ngenerative models have shown promising results for various text generation\nproblems. Sequence to Sequence (Seq2Seq) models are typically trained to\ngenerate a single output sequence for a given input sequence. However, in the\ncontext of a story, multiple endings are possible. Seq2Seq models tend to\nignore the context and generate generic and dull responses. Very few works have\nstudied generating diverse and interesting story endings for a given story\ncontext. In this paper, we propose models which generate more diverse and\ninteresting outputs by 1) training models to focus attention on important\nkeyphrases of the story, and 2) promoting generation of non-generic words. We\nshow that the combination of the two leads to more diverse and interesting\nendings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 19:29:29 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Gupta", "Prakhar", ""], ["Kumar", "Vinayshekhar Bannihatti", ""], ["Bhutani", "Mukul", ""], ["Black", "Alan W", ""]]}, {"id": "1907.08293", "submitter": "Sreeram Ganji Mr.", "authors": "Kunal Dhawan, Ganji Sreeram, Kumar Priyadarshi and Rohit Sinha", "title": "Investigating Target Set Reduction for End-to-End Speech Recognition of\n  Hindi-English Code-Switching Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) systems are fast replacing the conventional systems in the\ndomain of automatic speech recognition. As the target labels are learned\ndirectly from speech data, the E2E systems need a bigger corpus for effective\ntraining. In the context of code-switching task, the E2E systems face two\nchallenges: (i) the expansion of the target set due to multiple languages\ninvolved, and (ii) the lack of availability of sufficiently large\ndomain-specific corpus. Towards addressing those challenges, we propose an\napproach for reducing the number of target labels for reliable training of the\nE2E systems on limited data. The efficacy of the proposed approach has been\ndemonstrated on two prominent architectures, namely CTC-based and\nattention-based E2E networks. The experimental validations are performed on a\nrecently created Hindi-English code-switching corpus. For contrast purpose, the\nresults for the full target set based E2E system and a hybrid DNN-HMM system\nare also reported.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 06:34:28 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Dhawan", "Kunal", ""], ["Sreeram", "Ganji", ""], ["Priyadarshi", "Kumar", ""], ["Sinha", "Rohit", ""]]}, {"id": "1907.08321", "submitter": "Isaac Kamlish", "authors": "Isaac Kamlish, Isaac Bentata Chocron, Nicholas McCarthy", "title": "SentiMATE: Learning to play Chess through Natural Language Processing", "comments": "Accepted for Oral at the AIIDE-19 Workshop on Artificial Intelligence\n  for Strategy Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present SentiMATE, a novel end-to-end Deep Learning model for Chess,\nemploying Natural Language Processing that aims to learn an effective\nevaluation function assessing move quality. This function is pre-trained on the\nsentiment of commentary associated with the training moves and is used to guide\nand optimize the agent's game-playing decision making. The contributions of\nthis research are three-fold: we build and put forward both a classifier which\nextracts commentary describing the quality of Chess moves in vast commentary\ndatasets, and a Sentiment Analysis model trained on Chess commentary to\naccurately predict the quality of said moves, to then use those predictions to\nevaluate the optimal next move of a Chess agent. Both classifiers achieve over\n90 % classification accuracy. Lastly, we present a Chess engine, SentiMATE,\nwhich evaluates Chess moves based on a pre-trained sentiment evaluation\nfunction. Our results exhibit strong evidence to support our initial hypothesis\n- \"Can Natural Language Processing be used to train a novel and sample\nefficient evaluation function in Chess Engines?\" - as we integrate our\nevaluation function into modern Chess engines and play against agents with\ntraditional Chess move evaluation functions, beating both random agents and a\nDeepChess implementation at a level-one search depth - representing the number\nof moves a traditional Chess agent (employing the alpha-beta search algorithm)\nlooks ahead in order to evaluate a given chess state.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 23:48:21 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 09:08:47 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 18:57:47 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kamlish", "Isaac", ""], ["Chocron", "Isaac Bentata", ""], ["McCarthy", "Nicholas", ""]]}, {"id": "1907.08326", "submitter": "Sashank Santhanam", "authors": "Sashank Santhanam, Vidhushini Srinivasan, Shaina Glass, Samira Shaikh", "title": "I Stand With You: Using Emojis to Study Solidarity in Crisis Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how emojis are used to express solidarity in social media in the\ncontext of two major crisis events - a natural disaster, Hurricane Irma in 2017\nand terrorist attacks that occurred on November 2015 in Paris. Using annotated\ncorpora, we first train a recurrent neural network model to classify\nexpressions of solidarity in text. Next, we use these expressions of solidarity\nto characterize human behavior in online social networks, through the temporal\nand geospatial diffusion of emojis. Our analysis reveals that emojis are a\npowerful indicator of sociolinguistic behaviors (solidarity) that are exhibited\non social media as the crisis events unfold.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 00:40:28 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Santhanam", "Sashank", ""], ["Srinivasan", "Vidhushini", ""], ["Glass", "Shaina", ""], ["Shaikh", "Samira", ""]]}, {"id": "1907.08469", "submitter": "Syrielle Montariol", "authors": "Syrielle Montariol, Aina Gar\\'i Soler, Alexandre Allauzen", "title": "Exploring sentence informativeness", "comments": "Published at TALN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is a preliminary exploration of the concept of informativeness\n-how much information a sentence gives about a word it contains- and its\npotential benefits to building quality word representations from scarce data.\nWe propose several sentence-level classifiers to predict informativeness, and\nwe perform a manual annotation on a set of sentences. We conclude that these\ntwo measures correspond to different notions of informativeness. However, our\nexperiments show that using the classifiers' predictions to train word\nembeddings has an impact on embedding quality.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 11:39:41 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 10:19:21 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Montariol", "Syrielle", ""], ["Soler", "Aina Gar\u00ed", ""], ["Allauzen", "Alexandre", ""]]}, {"id": "1907.08501", "submitter": "Gerhard Wohlgenannt Dr.", "authors": "Gerhard Wohlgenannt, Dmitry Mouromtsev, Dmitry Pavlov, Yury Emelyanov\n  and Alexey Morozov", "title": "A Comparative Evaluation of Visual and Natural Language Question\n  Answering Over Linked Data", "comments": "KEOD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing number and size of Linked Data datasets, it is crucial to\nmake the data accessible and useful for users without knowledge of formal query\nlanguages. Two approaches towards this goal are knowledge graph visualization\nand natural language interfaces. Here, we investigate specifically question\nanswering (QA) over Linked Data by comparing a diagrammatic visual approach\nwith existing natural language-based systems. Given a QA benchmark (QALD7), we\nevaluate a visual method which is based on iteratively creating diagrams until\nthe answer is found, against four QA systems that have natural language queries\nas input. Besides other benefits, the visual approach provides higher\nperformance, but also requires more manual input. The results indicate that the\nmethods can be used complementary, and that such a combination has a large\npositive impact on QA performance, and also facilitates additional features\nsuch as data exploration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 13:09:32 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wohlgenannt", "Gerhard", ""], ["Mouromtsev", "Dmitry", ""], ["Pavlov", "Dmitry", ""], ["Emelyanov", "Yury", ""], ["Morozov", "Alexey", ""]]}, {"id": "1907.08532", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Yue Zhang, Mo Yu, Wei Zhang, Lin Pan, Linfeng Song, Kun\n  Xu, Yousef El-Kurdi", "title": "Multi-Granular Text Encoding for Self-Explaining Categorization", "comments": "Accepted by BlackboxNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Self-explaining text categorization requires a classifier to make a\nprediction along with supporting evidence. A popular type of evidence is\nsub-sequences extracted from the input text which are sufficient for the\nclassifier to make the prediction. In this work, we define multi-granular\nngrams as basic units for explanation, and organize all ngrams into a\nhierarchical structure, so that shorter ngrams can be reused while computing\nlonger ngrams. We leverage a tree-structured LSTM to learn a\ncontext-independent representation for each unit via parameter sharing.\nExperiments on medical disease classification show that our model is more\naccurate, efficient and compact than BiLSTM and CNN baselines. More\nimportantly, our model can extract intuitive multi-granular evidence to support\nits predictions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 14:43:51 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wang", "Zhiguo", ""], ["Zhang", "Yue", ""], ["Yu", "Mo", ""], ["Zhang", "Wei", ""], ["Pan", "Lin", ""], ["Song", "Linfeng", ""], ["Xu", "Kun", ""], ["El-Kurdi", "Yousef", ""]]}, {"id": "1907.08540", "submitter": "Steven Wilson", "authors": "Steven R. Wilson and Rada Mihalcea", "title": "Predicting Human Activities from User-Generated Content", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The activities we do are linked to our interests, personality, political\npreferences, and decisions we make about the future. In this paper, we explore\nthe task of predicting human activities from user-generated content. We collect\na dataset containing instances of social media users writing about a range of\neveryday activities. We then use a state-of-the-art sentence embedding\nframework tailored to recognize the semantics of human activities and perform\nan automatic clustering of these activities. We train a neural network model to\nmake predictions about which clusters contain activities that were performed by\na given user based on the text of their previous posts and self-description.\nAdditionally, we explore the degree to which incorporating inferred user traits\ninto our model helps with this prediction task.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 15:02:16 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wilson", "Steven R.", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1907.08672", "submitter": "Damien Sileo", "authors": "Damien Sileo, Tim Van-de-Cruys, Camille Pradel, Philippe Muller", "title": "Discourse-Based Evaluation of Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce DiscEval, a compilation of $11$ evaluation datasets with a focus\non discourse, that can be used for evaluation of English Natural Language\nUnderstanding when considering meaning as use. We make the case that evaluation\nwith discourse tasks is overlooked and that Natural Language Inference (NLI)\npretraining may not lead to the learning really universal representations.\nDiscEval can also be used as supplementary training data for multi-task\nlearning-based systems, and is publicly available, alongside the code for\ngathering and preprocessing the datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 20:09:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Sileo", "Damien", ""], ["Van-de-Cruys", "Tim", ""], ["Pradel", "Camille", ""], ["Muller", "Philippe", ""]]}, {"id": "1907.08696", "submitter": "Zhongkai Sun", "authors": "Zhongkai Sun, Prathusha K Sarma, William Sethares, Erik P. Bucy", "title": "Multi-modal Sentiment Analysis using Deep Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper learns multi-modal embeddings from text, audio, and video\nviews/modes of data in order to improve upon down-stream sentiment\nclassification. The experimental framework also allows investigation of the\nrelative contributions of the individual views in the final multi-modal\nembedding. Individual features derived from the three views are combined into a\nmulti-modal embedding using Deep Canonical Correlation Analysis (DCCA) in two\nways i) One-Step DCCA and ii) Two-Step DCCA. This paper learns text embeddings\nusing BERT, the current state-of-the-art in text encoders. We posit that this\nhighly optimized algorithm dominates over the contribution of other views,\nthough each view does contribute to the final result. Classification tasks are\ncarried out on two benchmark datasets and on a new Debate Emotion data set, and\ntogether these demonstrate that the one-Step DCCA outperforms the current\nstate-of-the-art in learning multi-modal embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 21:48:28 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Sun", "Zhongkai", ""], ["Sarma", "Prathusha K", ""], ["Sethares", "William", ""], ["Bucy", "Erik P.", ""]]}, {"id": "1907.08710", "submitter": "Pinjia He", "authors": "Pinjia He, Clara Meister, Zhendong Su", "title": "Structure-Invariant Testing for Machine Translation", "comments": "Accepted at ICSE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine translation software has increasingly been\nintegrated into our daily lives. People routinely use machine translation for\nvarious applications, such as describing symptoms to a foreign doctor and\nreading political news in a foreign language. However, the complexity and\nintractability of neural machine translation (NMT) models that power modern\nmachine translation make the robustness of these systems difficult to even\nassess, much less guarantee. Machine translation systems can return inferior\nresults that lead to misunderstanding, medical misdiagnoses, threats to\npersonal safety, or political conflicts. Despite its apparent importance,\nvalidating the robustness of machine translation systems is very difficult and\nhas, therefore, been much under-explored.\n  To tackle this challenge, we introduce structure-invariant testing (SIT), a\nnovel metamorphic testing approach for validating machine translation software.\nOur key insight is that the translation results of \"similar\" source sentences\nshould typically exhibit similar sentence structures. Specifically, SIT (1)\ngenerates similar source sentences by substituting one word in a given sentence\nwith semantically similar, syntactically equivalent words; (2) represents\nsentence structure by syntax parse trees (obtained via constituency or\ndependency parsing); (3) reports sentence pairs whose structures differ\nquantitatively by more than some threshold. To evaluate SIT, we use it to test\nGoogle Translate and Bing Microsoft Translator with 200 source sentences as\ninput, which led to 64 and 70 buggy issues with 69.5\\% and 70\\% top-1 accuracy,\nrespectively. The translation errors are diverse, including under-translation,\nover-translation, incorrect modification, word/phrase mistranslation, and\nunclear logic.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 22:20:01 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 17:37:36 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 10:30:36 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["He", "Pinjia", ""], ["Meister", "Clara", ""], ["Su", "Zhendong", ""]]}, {"id": "1907.08722", "submitter": "Shashi Narayan", "authors": "Shashi Narayan and Shay B. Cohen and Mirella Lapata", "title": "What is this Article about? Extreme Summarization with Topic-aware\n  Convolutional Neural Networks", "comments": "Accepted to appear in Journal of Artificial Intelligence Research\n  (JAIR), 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce 'extreme summarization', a new single-document summarization\ntask which aims at creating a short, one-sentence news summary answering the\nquestion ``What is the article about?''. We argue that extreme summarization,\nby nature, is not amenable to extractive strategies and requires an abstractive\nmodeling approach. In the hope of driving research on this task further: (a) we\ncollect a real-world, large scale dataset by harvesting online articles from\nthe British Broadcasting Corporation (BBC); and (b) propose a novel abstractive\nmodel which is conditioned on the article's topics and based entirely on\nconvolutional neural networks. We demonstrate experimentally that this\narchitecture captures long-range dependencies in a document and recognizes\npertinent content, outperforming an oracle extractive system and\nstate-of-the-art abstractive approaches when evaluated automatically and by\nhumans on the extreme summarization dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 22:57:21 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Narayan", "Shashi", ""], ["Cohen", "Shay B.", ""], ["Lapata", "Mirella", ""]]}, {"id": "1907.08736", "submitter": "Haohan Bo", "authors": "Haohan Bo, Steven H. H. Ding, Benjamin C. M. Fung, Farkhund Iqbal", "title": "ER-AE: Differentially Private Text Generation for Authorship\n  Anonymization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of privacy protection studies for textual data focus on removing\nexplicit sensitive identifiers. However, personal writing style, as a strong\nindicator of the authorship, is often neglected. Recent studies, such as SynTF,\nhave shown promising results on privacy-preserving text mining. However, their\nanonymization algorithm can only output numeric term vectors which are\ndifficult for the recipients to interpret. We propose a novel text generation\nmodel with a two-set exponential mechanism for authorship anonymization. By\naugmenting the semantic information through a REINFORCE training reward\nfunction, the model can generate differentially private text that has a close\nsemantic and similar grammatical structure to the original text while removing\npersonal traits of the writing style. It does not assume any conditioned labels\nor paralleled text data for training. We evaluate the performance of the\nproposed model on the real-life peer reviews dataset and the Yelp review\ndataset. The result suggests that our model outperforms the state-of-the-art on\nsemantic preservation, authorship obfuscation, and stylometric transformation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 02:07:02 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 14:45:01 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 17:48:09 GMT"}, {"version": "v4", "created": "Thu, 13 May 2021 07:55:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bo", "Haohan", ""], ["Ding", "Steven H. H.", ""], ["Fung", "Benjamin C. M.", ""], ["Iqbal", "Farkhund", ""]]}, {"id": "1907.08854", "submitter": "Zekang Li", "authors": "Zekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li, Jie Zhou", "title": "Incremental Transformer with Deliberation Decoder for Document Grounded\n  Conversations", "comments": "Accepted as a long paper at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 18:49:36 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 03:48:56 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 22:09:11 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Li", "Zekang", ""], ["Niu", "Cheng", ""], ["Meng", "Fandong", ""], ["Feng", "Yang", ""], ["Li", "Qian", ""], ["Zhou", "Jie", ""]]}, {"id": "1907.08889", "submitter": "Phu Mon Htut", "authors": "Phu Mon Htut, Joel Tetreault", "title": "The Unbearable Weight of Generating Artificial Errors for Grammatical\n  Error Correction", "comments": "To appear at ACL-BEA workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sequence-to-sequence models have been very effective for\nend-to-end grammatical error correction (GEC). As creating human-annotated\nparallel corpus for GEC is expensive and time-consuming, there has been work on\nartificial corpus generation with the aim of creating sentences that contain\nrealistic grammatical errors from grammatically correct sentences. In this\npaper, we investigate the impact of using recent neural models for generating\nerrors to help neural models to correct errors. We conduct a battery of\nexperiments on the effect of data size, models, and comparison with a\nrule-based approach.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 01:20:07 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Htut", "Phu Mon", ""], ["Tetreault", "Joel", ""]]}, {"id": "1907.08922", "submitter": "Melvin Wevers", "authors": "Melvin Wevers", "title": "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers,\n  1950-1990", "comments": "6 pages with appendix. Published in Proceedings of the 1st\n  International Workshop on Computational Approaches to Historical Language\n  Change 2019 co-organized with ACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary debates on filter bubbles and polarization in public and social\nmedia raise the question to what extent news media of the past exhibited\nbiases. This paper specifically examines bias related to gender in six Dutch\nnational newspapers between 1950 and 1990. We measure bias related to gender by\ncomparing local changes in word embedding models trained on newspapers with\ndivergent ideological backgrounds. We demonstrate clear differences in gender\nbias and changes within and between newspapers over time. In relation to themes\nsuch as sexuality and leisure, we see the bias moving toward women, whereas,\ngenerally, the bias shifts in the direction of men, despite growing female\nemployment number and feminist movements. Even though Dutch society became less\nstratified ideologically (depillarization), we found an increasing divergence\nin gender bias between religious and social-democratic on the one hand and\nliberal newspapers on the other. Methodologically, this paper illustrates how\nword embeddings can be used to examine historical language change. Future work\nwill investigate how fine-tuning deep contextualized embedding models, such as\nELMO, might be used for similar tasks with greater contextual information.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 06:58:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wevers", "Melvin", ""]]}, {"id": "1907.08937", "submitter": "Hao Zhu", "authors": "Weize Chen, Hao Zhu, Xu Han, Zhiyuan Liu, Maosong Sun", "title": "Quantifying Similarity between Relations with Fact Distribution", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a conceptually simple and effective method to quantify the\nsimilarity between relations in knowledge bases. Specifically, our approach is\nbased on the divergence between the conditional probability distributions over\nentity pairs. In this paper, these distributions are parameterized by a very\nsimple neural network. Although computing the exact similarity is in-tractable,\nwe provide a sampling-based method to get a good approximation. We empirically\nshow the outputs of our approach significantly correlate with human judgments.\nBy applying our method to various tasks, we also find that (1) our approach\ncould effectively detect redundant relations extracted by open information\nextraction (Open IE) models, that (2) even the most competitive models for\nrelational classification still make mistakes among very similar relations, and\nthat (3) our approach could be incorporated into negative sampling and softmax\nclassification to alleviate these mistakes. The source code and experiment\ndetails of this paper can be obtained from\nhttps://github.com/thunlp/relation-similarity.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 09:22:50 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Weize", ""], ["Zhu", "Hao", ""], ["Han", "Xu", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "1907.08948", "submitter": "Shantipriya Parida", "authors": "Shantipriya Parida, Ond\\v{r}ej Bojar, Satya Ranjan Dash", "title": "Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine\n  Translation", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Visual Genome is a dataset connecting structured image information with\nEnglish language. We present ``Hindi Visual Genome'', a multimodal dataset\nconsisting of text and images suitable for English-Hindi multimodal machine\ntranslation task and multimodal research. We have selected short English\nsegments (captions) from Visual Genome along with associated images and\nautomatically translated them to Hindi with manual post-editing which took the\nassociated images into account. We prepared a set of 31525 segments,\naccompanied by a challenge test set of 1400 segments. This challenge test set\nwas created by searching for (particularly) ambiguous English words based on\nthe embedding similarity and manually selecting those where the image helps to\nresolve the ambiguity.\n  Our dataset is the first for multimodal English-Hindi machine translation,\nfreely available for non-commercial research purposes. Our Hindi version of\nVisual Genome also allows to create Hindi image labelers or other practical\ntools.\n  Hindi Visual Genome also serves in Workshop on Asian Translation (WAT) 2019\nMulti-Modal Translation Task.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 10:00:28 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Parida", "Shantipriya", ""], ["Bojar", "Ond\u0159ej", ""], ["Dash", "Satya Ranjan", ""]]}, {"id": "1907.08971", "submitter": "Eyal Shnarch", "authors": "Martin Gleize, Eyal Shnarch, Leshem Choshen, Lena Dankin, Guy\n  Moshkowich, Ranit Aharonov, Noam Slonim", "title": "Are You Convinced? Choosing the More Convincing Evidence with a Siamese\n  Network", "comments": "accepted to ACL 2019 - long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advancement in argument detection, we suggest to pay more attention\nto the challenging task of identifying the more convincing arguments. Machines\ncapable of responding and interacting with humans in helpful ways have become\nubiquitous. We now expect them to discuss with us the more delicate questions\nin our world, and they should do so armed with effective arguments. But what\nmakes an argument more persuasive? What will convince you? In this paper, we\npresent a new data set, IBM-EviConv, of pairs of evidence labeled for\nconvincingness, designed to be more challenging than existing alternatives. We\nalso propose a Siamese neural network architecture shown to outperform several\nbaselines on both a prior convincingness data set and our own. Finally, we\nprovide insights into our experimental results and the various kinds of\nargumentative value our method is capable of detecting.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 13:05:45 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 09:14:32 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gleize", "Martin", ""], ["Shnarch", "Eyal", ""], ["Choshen", "Leshem", ""], ["Dankin", "Lena", ""], ["Moshkowich", "Guy", ""], ["Aharonov", "Ranit", ""], ["Slonim", "Noam", ""]]}, {"id": "1907.09006", "submitter": "Yibin Zheng", "authors": "Yibin Zheng, Xi Wang, Lei He, Shifeng Pan, Frank K. Soong, Zhengqi\n  Wen, Jianhua Tao", "title": "Forward-Backward Decoding for Regularizing End-to-End TTS", "comments": "Accepted by INTERSPEECH2019. arXiv admin note: text overlap with\n  arXiv:1808.04064, arXiv:1804.05374 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural end-to-end TTS can generate very high-quality synthesized speech, and\neven close to human recording within similar domain text. However, it performs\nunsatisfactory when scaling it to challenging test sets. One concern is that\nthe encoder-decoder with attention-based network adopts autoregressive\ngenerative sequence model with the limitation of \"exposure bias\" To address\nthis issue, we propose two novel methods, which learn to predict future by\nimproving agreement between forward and backward decoding sequence. The first\none is achieved by introducing divergence regularization terms into model\ntraining objective to reduce the mismatch between two directional models,\nnamely L2R and R2L (which generates targets from left-to-right and\nright-to-left, respectively). While the second one operates on decoder-level\nand exploits the future information during decoding. In addition, we employ a\njoint training strategy to allow forward and backward decoding to improve each\nother in an interactive process. Experimental results show our proposed methods\nespecially the second one (bidirectional decoder regularization), leads a\nsignificantly improvement on both robustness and overall naturalness, as\noutperforming baseline (the revised version of Tacotron2) with a MOS gap of\n0.14 in a challenging test, and achieving close to human quality (4.42 vs. 4.49\nin MOS) on general test.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:24:30 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zheng", "Yibin", ""], ["Wang", "Xi", ""], ["He", "Lei", ""], ["Pan", "Shifeng", ""], ["Soong", "Frank K.", ""], ["Wen", "Zhengqi", ""], ["Tao", "Jianhua", ""]]}, {"id": "1907.09007", "submitter": "Issa Annamoradnejad", "authors": "Issa Annamoradnejad, Jafar Habibi", "title": "A Comprehensive Analysis of Twitter Trending Topics", "comments": "6 pages, 8 figures, 3 tables, conference paper", "journal-ref": "2019 5th International Conference on Web Research (ICWR), Tehran,\n  Iran, 2019, pp. 22-27", "doi": "10.1109/ICWR.2019.8765252", "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Twitter, a name, phrase, or topic that is mentioned at a greater rate than\nothers is called a \"trending topic\" or simply \"trend\". Twitter trends list has\na powerful ability to promote public events such as natural events, political\nscandals, market changes and other types of breaking news. Nevertheless, there\nhave been very few works focused on the dynamics of these trending topics. In\nthis article, we thoroughly examined the Twitter's trending topics of 2018. To\nthis end, we automatically accessed Twitter's trends API and stored the\nresulting 50 top trending topics in a novel dataset. We propose and analyze our\ndataset according to six criteria: lexical analysis, time to reach, trend\nreoccurrence, trending time, tweets count, and language analysis. Based on our\nresults, 77.6% of the topics that reached the Top-10 list were trending with\nless than 100k tweets. More than 50% of the topics could not hold the position\nfor more than an hour. English and Arabic languages comprised close to 40% and\n20% of the first rank topics, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 17:07:07 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 12:31:30 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Annamoradnejad", "Issa", ""], ["Habibi", "Jafar", ""]]}, {"id": "1907.09038", "submitter": "Stein{\\th}\\'or Steingr\\'imsson", "authors": "Stein{\\th}\\'or Steingr\\'imsson and \\\"Orvar K\\'arason and Hrafn\n  Loftsson", "title": "Augmenting a BiLSTM tagger with a Morphological Lexicon and a Lexical\n  Category Identification Step", "comments": "Accepted by RANLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work on using BiLSTM models for PoS tagging has primarily focused on\nsmall tagsets. We evaluate BiLSTM models for tagging Icelandic, a\nmorphologically rich language, using a relatively large tagset. Our baseline\nBiLSTM model achieves higher accuracy than any previously published tagger not\ntaking advantage of a morphological lexicon. When we extend the model by\nincorporating such data, we outperform previous state-of-the-art results by a\nsignificant margin. We also report on work in progress that attempts to address\nthe problem of data sparsity inherent in morphologically detailed, fine-grained\ntagsets. We experiment with training a separate model on only the lexical\ncategory and using the coarse-grained output tag as an input for the main\nmodel. This method further increases the accuracy and reduces the tagging\nerrors by 21.3% compared to previous state-of-the-art results. Finally, we\ntrain and test our tagger on a new gold standard for Icelandic.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 21:27:44 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Steingr\u00edmsson", "Stein\u00fe\u00f3r", ""], ["K\u00e1rason", "\u00d6rvar", ""], ["Loftsson", "Hrafn", ""]]}, {"id": "1907.09169", "submitter": "Syrielle Montariol", "authors": "Syrielle Montariol, Alexandre Allauzen", "title": "Learning dynamic word embeddings with drift regularisation", "comments": "Published at TALN 2019. in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word usage, meaning and connotation change throughout time. Diachronic word\nembeddings are used to grasp these changes in an unsupervised way. In this\npaper, we use variants of the Dynamic Bernoulli Embeddings model to learn\ndynamic word embeddings, in order to identify notable properties of the model.\nThe comparison is made on the New York Times Annotated Corpus in English and a\nset of articles from the French newspaper Le Monde covering the same period.\nThis allows us to define a pipeline to analyse the evolution of words use\nacross two languages.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:44:09 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Montariol", "Syrielle", ""], ["Allauzen", "Alexandre", ""]]}, {"id": "1907.09177", "submitter": "Fuming Fang", "authors": "David Ifeoluwa Adelani, Haotian Mai, Fuming Fang, Huy H. Nguyen,\n  Junichi Yamagishi, Isao Echizen", "title": "Generating Sentiment-Preserving Fake Online Reviews Using Neural\n  Language Models and Their Human- and Machine-based Detection", "comments": "The 34-th International Conference on Advanced Information Networking\n  and Applications (AINA-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced neural language models (NLMs) are widely used in sequence generation\ntasks because they are able to produce fluent and meaningful sentences. They\ncan also be used to generate fake reviews, which can then be used to attack\nonline review systems and influence the buying decisions of online shoppers. To\nperform such attacks, it is necessary for experts to train a tailored LM for a\nspecific topic. In this work, we show that a low-skilled threat model can be\nbuilt just by combining publicly available LMs and show that the produced fake\nreviews can fool both humans and machines. In particular, we use the GPT-2 NLM\nto generate a large number of high-quality reviews based on a review with the\ndesired sentiment and then using a BERT based text classifier (with accuracy of\n96%) to filter out reviews with undesired sentiments. Because none of the words\nin the review are modified, fluent samples like the training data can be\ngenerated from the learned distribution. A subjective evaluation with 80\nparticipants demonstrated that this simple method can produce reviews that are\nas fluent as those written by people. It also showed that the participants\ntended to distinguish fake reviews randomly. Three countermeasures, Grover,\nGLTR, and OpenAI GPT-2 detector, were found to be difficult to accurately\ndetect fake review.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 08:22:08 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 07:46:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Adelani", "David Ifeoluwa", ""], ["Mai", "Haotian", ""], ["Fang", "Fuming", ""], ["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1907.09190", "submitter": "Angela Fan", "authors": "Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston,\n  Michael Auli", "title": "ELI5: Long Form Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first large-scale corpus for long-form question answering, a\ntask requiring elaborate and in-depth answers to open-ended questions. The\ndataset comprises 270K threads from the Reddit forum ``Explain Like I'm Five''\n(ELI5) where an online community provides answers to questions which are\ncomprehensible by five year olds. Compared to existing datasets, ELI5 comprises\ndiverse questions requiring multi-sentence answers. We provide a large set of\nweb documents to help answer the question. Automatic and human evaluations show\nthat an abstractive model trained with a multi-task objective outperforms\nconventional Seq2Seq, language modeling, as well as a strong extractive\nbaseline. However, our best model is still far from human performance since\nraters prefer gold responses in over 86% of cases, leaving ample opportunity\nfor future improvement.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 09:01:35 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fan", "Angela", ""], ["Jernite", "Yacine", ""], ["Perez", "Ethan", ""], ["Grangier", "David", ""], ["Weston", "Jason", ""], ["Auli", "Michael", ""]]}, {"id": "1907.09273", "submitter": "Arthur Szlam", "authors": "Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite, Armand\n  Joulin, Gabriel Synnaeve, Douwe Kiela, Haonan Yu, Zhuoyuan Chen, Siddharth\n  Goyal, Demi Guo, Danielle Rothermel, C. Lawrence Zitnick, Jason Weston", "title": "Why Build an Assistant in Minecraft?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document we describe a rationale for a research program aimed at\nbuilding an open \"assistant\" in the game Minecraft, in order to make progress\non the problems of natural language understanding and learning from dialogue.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:32:15 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 21:52:08 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Szlam", "Arthur", ""], ["Gray", "Jonathan", ""], ["Srinet", "Kavya", ""], ["Jernite", "Yacine", ""], ["Joulin", "Armand", ""], ["Synnaeve", "Gabriel", ""], ["Kiela", "Douwe", ""], ["Yu", "Haonan", ""], ["Chen", "Zhuoyuan", ""], ["Goyal", "Siddharth", ""], ["Guo", "Demi", ""], ["Rothermel", "Danielle", ""], ["Zitnick", "C. Lawrence", ""], ["Weston", "Jason", ""]]}, {"id": "1907.09293", "submitter": "David Powers", "authors": "David M W Powers", "title": "DREAMT -- Embodied Motivational Conversational Storytelling", "comments": "12 pages; to be presented as lightning talk plus poster at StoryNLP\n  on 1 August 2019 at ACL in Florence - poster pdf and powerpoint available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storytelling is fundamental to language, including culture, conversation and\ncommunication in their broadest senses. It thus emerges as an essential\ncomponent of intelligent systems, including systems where natural language is\nnot a primary focus or where we do not usually think of a story being involved.\nIn this paper we explore the emergence of storytelling as a requirement in\nembodied conversational agents, including its role in educational and health\ninterventions, as well as in a general-purpose computer interface for people\nwith disabilities or other constraints that prevent the use of traditional\nkeyboard and speech interfaces. We further present a characterization of\nstorytelling as an inventive fleshing out of detail according to a particular\npersonal perspective, and propose the DREAMT model to focus attention on the\ndifferent layers that need to be present in a character-driven storytelling\nsystem. Most if not all aspects of the DREAMT model have arisen from or been\nexplored in some aspect of our implemented research systems, but currently only\nat a primitive and relatively unintegrated level. However, this experience\nleads us to formalize and elaborate the DREAMT model mnemonically as follows: -\nDescription/Dialogue/Definition/Denotation - Realization/Representation/Role -\nExplanation/Education/Entertainment - Actualization/Activation -\nMotivation/Modelling - Topicalization/Transformation\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:49:37 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Powers", "David M W", ""]]}, {"id": "1907.09312", "submitter": "Qingrong Xia", "authors": "Qingrong Xia, Zhenghua Li, Min Zhang, Meishan Zhang, Guohong Fu, Rui\n  Wang, Luo Si", "title": "Syntax-aware Neural Semantic Role Labeling", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic role labeling (SRL), also known as shallow semantic parsing, is an\nimportant yet challenging task in NLP. Motivated by the close correlation\nbetween syntactic and semantic structures, traditional discrete-feature-based\nSRL approaches make heavy use of syntactic features. In contrast,\ndeep-neural-network-based approaches usually encode the input sentence as a\nword sequence without considering the syntactic structures. In this work, we\ninvestigate several previous approaches for encoding syntactic trees, and make\na thorough study on whether extra syntax-aware representations are beneficial\nfor neural SRL models. Experiments on the benchmark CoNLL-2005 dataset show\nthat syntax-aware SRL approaches can effectively improve performance over a\nstrong baseline with external word representations from ELMo. With the extra\nsyntax-aware representations, our approaches achieve new state-of-the-art 85.6\nF1 (single model) and 86.6 F1 (ensemble) on the test data, outperforming the\ncorresponding strong baselines with ELMo by 0.8 and 1.0, respectively. Detailed\nerror analysis are conducted to gain more insights on the investigated\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:25:27 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Xia", "Qingrong", ""], ["Li", "Zhenghua", ""], ["Zhang", "Min", ""], ["Zhang", "Meishan", ""], ["Fu", "Guohong", ""], ["Wang", "Rui", ""], ["Si", "Luo", ""]]}, {"id": "1907.09340", "submitter": "Pranava Madhyastha", "authors": "Pranava Madhyastha, Josiah Wang, Lucia Specia", "title": "VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions", "comments": "Accepted for publication at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the task of evaluating image description generation systems. We\npropose a novel image-aware metric for this task: VIFIDEL. It estimates the\nfaithfulness of a generated caption with respect to the content of the actual\nimage, based on the semantic similarity between labels of objects depicted in\nimages and words in the description. The metric is also able to take into\naccount the relative importance of objects mentioned in human reference\ndescriptions during evaluation. Even if these human reference descriptions are\nnot available, VIFIDEL can still reliably evaluate system descriptions. The\nmetric achieves high correlation with human judgments on two well-known\ndatasets and is competitive with metrics that depend on human references\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:33:43 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Madhyastha", "Pranava", ""], ["Wang", "Josiah", ""], ["Specia", "Lucia", ""]]}, {"id": "1907.09358", "submitter": "Aditya Mogadala", "authors": "Aditya Mogadala and Marimuthu Kalimuthu and Dietrich Klakow", "title": "Trends in Integration of Vision and Language Research: A Survey of\n  Tasks, Datasets, and Methods", "comments": "Accepted at Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. This success can be partly\nattributed to the advancements made in the sub-fields of AI such as Machine\nLearning (ML), Computer Vision (CV), and Natural Language Processing (NLP). The\nlargest of the growths in these fields has been made possible with deep\nlearning, a sub-area of machine learning, which uses the principles of\nartificial neural networks. This has created significant interest in the\nintegration of vision and language. The tasks are designed such that they\nperfectly embrace the ideas of deep learning. In this survey, we focus on ten\nprominent tasks that integrate language and vision by discussing their problem\nformulations, methods, existing datasets, evaluation measures, and compare the\nresults obtained with corresponding state-of-the-art methods. Our efforts go\nbeyond earlier surveys which are either task-specific or concentrate only on\none type of visual content, i.e., image or video. Furthermore, we also provide\nsome potential future directions in this field of research with an anticipation\nthat this survey brings in innovative thoughts and ideas to address the\nexisting challenges and build new applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:53:48 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 13:26:29 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mogadala", "Aditya", ""], ["Kalimuthu", "Marimuthu", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1907.09361", "submitter": "Nilesh Chakraborty", "authors": "Nilesh Chakraborty, Denis Lukovnikov, Gaurav Maheshwari, Priyansh\n  Trivedi, Jens Lehmann, Asja Fischer", "title": "Introduction to Neural Network based Approaches for Question Answering\n  over Knowledge Graphs", "comments": "Preprint, under review. The first four authors contributed equally to\n  this paper, and should be regarded as co-first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering has emerged as an intuitive way of querying structured\ndata sources, and has attracted significant advancements over the years. In\nthis article, we provide an overview over these recent advancements, focusing\non neural network based question answering systems over knowledge graphs. We\nintroduce readers to the challenges in the tasks, current paradigms of\napproaches, discuss notable advancements, and outline the emerging trends in\nthe field. Through this article, we aim to provide newcomers to the field with\na suitable entry point, and ease their process of making informed decisions\nwhile creating their own QA system.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:57:13 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chakraborty", "Nilesh", ""], ["Lukovnikov", "Denis", ""], ["Maheshwari", "Gaurav", ""], ["Trivedi", "Priyansh", ""], ["Lehmann", "Jens", ""], ["Fischer", "Asja", ""]]}, {"id": "1907.09369", "submitter": "Armin Seyeditabari", "authors": "Armin Seyeditabari, Narges Tabari, Shafie Gholizadeh, Wlodek Zadrozny", "title": "Emotion Detection in Text: Focusing on Latent Representation", "comments": "6 pages, 7 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, emotion detection in text has become more popular due to its\nvast potential applications in marketing, political science, psychology,\nhuman-computer interaction, artificial intelligence, etc. In this work, we\nargue that current methods which are based on conventional machine learning\nmodels cannot grasp the intricacy of emotional language by ignoring the\nsequential nature of the text, and the context. These methods, therefore, are\nnot sufficient to create an applicable and generalizable emotion detection\nmethodology. Understanding these limitations, we present a new network based on\na bidirectional GRU model to show that capturing more meaningful information\nfrom text can significantly improve the performance of these models. The\nresults show significant improvement with an average of 26.8 point increase in\nF-measure on our test data and 38.6 increase on the totally new dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:33:53 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Seyeditabari", "Armin", ""], ["Tabari", "Narges", ""], ["Gholizadeh", "Shafie", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1907.09527", "submitter": "Vrindavan Harrison", "authors": "Vrindavan Harrison, Lena Reed, Shereen Oraby, Marilyn Walker", "title": "Maximizing Stylistic Control and Semantic Accuracy in NLG: Personality\n  Variation and Discourse Contrast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural generation methods for task-oriented dialogue typically generate from\na meaning representation that is populated using a database of domain\ninformation, such as a table of data describing a restaurant. While earlier\nwork focused solely on the semantic fidelity of outputs, recent work has\nstarted to explore methods for controlling the style of the generated text\nwhile simultaneously achieving semantic accuracy. Here we experiment with two\nstylistic benchmark tasks, generating language that exhibits variation in\npersonality, and generating discourse contrast. We report a huge performance\nimprovement in both stylistic control and semantic accuracy over the state of\nthe art on both of these benchmarks. We test several different models and show\nthat putting stylistic conditioning in the decoder and eliminating the semantic\nre-ranker used in earlier models results in more than 15 points higher BLEU for\nPersonality, with a reduction of semantic error to near zero. We also report an\nimprovement from .75 to .81 in controlling contrast and a reduction in semantic\nerror from 16% to 2%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 18:57:14 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Harrison", "Vrindavan", ""], ["Reed", "Lena", ""], ["Oraby", "Shereen", ""], ["Walker", "Marilyn", ""]]}, {"id": "1907.09600", "submitter": "Lorenzo A. Rossi", "authors": "Lorenzo A. Rossi, Chad Shawber, Janet Munu and Finly Zachariah", "title": "Evaluation of Embeddings of Laboratory Test Codes for Patients at a\n  Cancer Center", "comments": "2019 KDD Workshop on Applied Data Science for Healthcare (DSHealth,\n  August 2019, Anchorage, AK). Make sure you have downloaded the latest version\n  with the link to the DSHealth2019_loinc_embeddings GitHub repository:\n  https://github.com/elleros/DSHealth2019_loinc_embeddings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Laboratory test results are an important and generally high dimensional\ncomponent of a patient's Electronic Health Record (EHR). We train embedding\nrepresentations (via Word2Vec and GloVe) for LOINC codes of laboratory tests\nfrom the EHRs of about 80,000 patients at a cancer center. To include\ninformation about lab test outcomes, we also train embeddings on the\nconcatenation of a LOINC code with a symbol indicating normality or abnormality\nof the result. We observe several clinically meaningful similarities among\nLOINC embeddings trained over our data. For the embeddings of the concatenation\nof LOINCs with abnormality codes, we evaluate the performance for mortality\nprediction tasks and the ability to preserve ordinality properties: i.e. a lab\ntest with normal outcome should be more similar to an abnormal one than to the\na very abnormal one.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:58:40 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 15:29:44 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Rossi", "Lorenzo A.", ""], ["Shawber", "Chad", ""], ["Munu", "Janet", ""], ["Zachariah", "Finly", ""]]}, {"id": "1907.09636", "submitter": "Woojay Jeon", "authors": "Woojay Jeon, Maxwell Jordan, Mahesh Krishnamoorthy", "title": "On Modeling ASR Word Confidence", "comments": "Presented at IEEE ICASSP 2020, May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for computing ASR word confidences that effectively\nmitigates the effect of ASR errors for diverse downstream applications,\nimproves the word error rate of the 1-best result, and allows better comparison\nof scores across different models. We propose 1) a new method for modeling word\nconfidence using a Heterogeneous Word Confusion Network (HWCN) that addresses\nsome key flaws in conventional Word Confusion Networks, and 2) a new score\ncalibration method for facilitating direct comparison of scores from different\nmodels. Using a bidirectional lattice recurrent neural network to compute the\nconfidence scores of each word in the HWCN, we show that the word sequence with\nthe best overall confidence is more accurate than the default 1-best result of\nthe recognizer, and that the calibration method can substantially improve the\nreliability of recognizer combination.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 23:53:41 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 00:43:25 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 01:48:54 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2020 04:59:37 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Jeon", "Woojay", ""], ["Jordan", "Maxwell", ""], ["Krishnamoorthy", "Mahesh", ""]]}, {"id": "1907.09669", "submitter": "Linkai Luo", "authors": "Linkai Luo and Yue Wang", "title": "EmotionX-HSU: Adopting Pre-trained BERT for Emotion Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach to the EmotionX-2019, the shared task of\nSocialNLP 2019. To detect emotion for each utterance of two datasets from the\nTV show Friends and Facebook chat log EmotionPush, we propose two-step deep\nlearning based methodology: (i) encode each of the utterance into a sequence of\nvectors that represent its meaning; and (ii) use a simply softmax classifier to\npredict one of the emotions amongst four candidates that an utterance may\ncarry. Notice that the source of labeled utterances is not rich, we utilise a\nwell-trained model, known as BERT, to transfer part of the knowledge learned\nfrom a large amount of corpus to our model. We then focus on fine-tuning our\nmodel until it well fits to the in-domain data. The performance of the proposed\nmodel is evaluated by micro-F1 scores, i.e., 79.1% and 86.2% for the testsets\nof Friends and EmotionPush, respectively. Our model ranks 3rd among 11\nsubmissions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 03:05:39 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Luo", "Linkai", ""], ["Wang", "Yue", ""]]}, {"id": "1907.09671", "submitter": "David Gaddy", "authors": "David Gaddy and Dan Klein", "title": "Pre-Learning Environment Representations for Data-Efficient Neural\n  Instruction Following", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to map from natural language instructions\nto state transitions (actions) in a data-efficient manner. Our method takes\ninspiration from the idea that it should be easier to ground language to\nconcepts that have already been formed through pre-linguistic observation. We\naugment a baseline instruction-following learner with an initial\nenvironment-learning phase that uses observations of language-free state\ntransitions to induce a suitable latent representation of actions before\nprocessing the instruction-following training data. We show that mapping to\npre-learned representations substantially improves performance over systems\nwhose representations are learned from limited instructional data alone.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 03:11:07 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gaddy", "David", ""], ["Klein", "Dan", ""]]}, {"id": "1907.09692", "submitter": "Boyuan Pan", "authors": "Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei\n  He", "title": "Discourse Marker Augmented Network with Reinforcement Learning for\n  Natural Language Inference", "comments": "Accepted in ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI), also known as Recognizing Textual\nEntailment (RTE), is one of the most important problems in natural language\nprocessing. It requires to infer the logical relationship between two given\nsentences. While current approaches mostly focus on the interaction\narchitectures of the sentences, in this paper, we propose to transfer knowledge\nfrom some important discourse markers to augment the quality of the NLI model.\nWe observe that people usually use some discourse markers such as \"so\" or \"but\"\nto represent the logical relationship between two sentences. These words\npotentially have deep connections with the meanings of the sentences, thus can\nbe utilized to help improve the representations of them. Moreover, we use\nreinforcement learning to optimize a new objective function with a reward\ndefined by the property of the NLI datasets to make full use of the labels\ninformation. Experiments show that our method achieves the state-of-the-art\nperformance on several large-scale datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 04:27:57 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Pan", "Boyuan", ""], ["Yang", "Yazheng", ""], ["Zhao", "Zhou", ""], ["Zhuang", "Yueting", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "1907.09699", "submitter": "Hayate Iso", "authors": "Hayate Iso, Yui Uehara, Tatsuya Ishigaki, Hiroshi Noji, Eiji Aramaki,\n  Ichiro Kobayashi, Yusuke Miyao, Naoaki Okazaki and Hiroya Takamura", "title": "Learning to Select, Track, and Generate for Data-to-Text", "comments": "ACL 2019", "journal-ref": null, "doi": "10.18653/v1/P19-1202", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-to-text generation model with two modules, one for tracking\nand the other for text generation. Our tracking module selects and keeps track\nof salient information and memorizes which record has been mentioned. Our\ngeneration module generates a summary conditioned on the state of tracking\nmodule. Our model is considered to simulate the human-like writing process that\ngradually selects the information by determining the intermediate variables\nwhile writing the summary. In addition, we also explore the effectiveness of\nthe writer information for generation. Experimental results show that our model\noutperforms existing models in all evaluation metrics even without writer\ninformation. Incorporating writer information further improves the performance,\ncontributing to content planning and surface realization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:31:54 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Iso", "Hayate", ""], ["Uehara", "Yui", ""], ["Ishigaki", "Tatsuya", ""], ["Noji", "Hiroshi", ""], ["Aramaki", "Eiji", ""], ["Kobayashi", "Ichiro", ""], ["Miyao", "Yusuke", ""], ["Okazaki", "Naoaki", ""], ["Takamura", "Hiroya", ""]]}, {"id": "1907.09705", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Fengming Xie, Yibo Liu, Xiang Bai, Cong Yao", "title": "2D-CTC for Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has been an important, active research topic in\ncomputer vision for years. Previous approaches mainly consider text as 1D\nsignals and cast scene text recognition as a sequence prediction problem, by\nfeat of CTC or attention based encoder-decoder framework, which is originally\ndesigned for speech recognition. However, different from speech voices, which\nare 1D signals, text instances are essentially distributed in 2D image spaces.\nTo adhere to and make use of the 2D nature of text for higher recognition\naccuracy, we extend the vanilla CTC model to a second dimension, thus creating\n2D-CTC. 2D-CTC can adaptively concentrate on most relevant features while\nexcluding the impact from clutters and noises in the background; It can also\nnaturally handle text instances with various forms (horizontal, oriented and\ncurved) while giving more interpretable intermediate predictions. The\nexperiments on standard benchmarks for scene text recognition, such as IIIT-5K,\nICDAR 2015, SVP-Perspective, and CUTE80, demonstrate that the proposed 2D-CTC\nmodel outperforms state-of-the-art methods on the text of both regular and\nirregular shapes. Moreover, 2D-CTC exhibits its superiority over prior art on\ntraining and testing speed. Our implementation and models of 2D-CTC will be\nmade publicly available soon later.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:55:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Xie", "Fengming", ""], ["Liu", "Yibo", ""], ["Bai", "Xiang", ""], ["Yao", "Cong", ""]]}, {"id": "1907.09724", "submitter": "Mamoru Komachi", "authors": "Satoru Katsumata and Mamoru Komachi", "title": "Towards Unsupervised Grammatical Error Correction using Statistical\n  Machine Translation with Synthetic Comparable Corpus", "comments": "7 pages; extended version of BEA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce unsupervised techniques based on phrase-based statistical\nmachine translation for grammatical error correction (GEC) trained on a pseudo\nlearner corpus created by Google Translation. We verified our GEC system\nthrough experiments on various GEC dataset, includi ng a low resource track of\nthe shared task at Building Educational Applications 2019 (BEA 2019). As a\nresult, we achieved an F_0.5 score of 28.31 points with the test data of the\nlow resource track.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:15:23 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Katsumata", "Satoru", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1907.09748", "submitter": "Yaxiong Wang", "authors": "Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li and Xin\n  Fan", "title": "Position Focused Attention Network for Image-Text Matching", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching tasks have recently attracted a lot of attention in the\ncomputer vision field. The key point of this cross-domain problem is how to\naccurately measure the similarity between the visual and the textual contents,\nwhich demands a fine understanding of both modalities. In this paper, we\npropose a novel position focused attention network (PFAN) to investigate the\nrelation between the visual and the textual views. In this work, we integrate\nthe object position clue to enhance the visual-text joint-embedding learning.\nWe first split the images into blocks, by which we infer the relative position\nof region in the image. Then, an attention mechanism is proposed to model the\nrelations between the image region and blocks and generate the valuable\nposition feature, which will be further utilized to enhance the region\nexpression and model a more reliable relationship between the visual image and\nthe textual sentence. Experiments on the popular datasets Flickr30K and MS-COCO\nshow the effectiveness of the proposed method. Besides the public datasets, we\nalso conduct experiments on our collected practical large-scale news dataset\n(Tencent-News) to validate the practical application value of proposed method.\nAs far as we know, this is the first attempt to test the performance on the\npractical application. Our method achieves the state-of-art performance on all\nof these three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:23:42 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wang", "Yaxiong", ""], ["Yang", "Hao", ""], ["Qian", "Xueming", ""], ["Ma", "Lin", ""], ["Lu", "Jing", ""], ["Li", "Biao", ""], ["Fan", "Xin", ""]]}, {"id": "1907.09854", "submitter": "Muthu Kumar Chandrasekaran", "authors": "Muthu Kumar Chandrasekaran and Michihiro Yasunaga and Dragomir Radev\n  and Dayne Freitag and Min-Yen Kan", "title": "Overview and Results: CL-SciSumm Shared Task 2019", "comments": "In Proceedings of BIRNDL 2019 at SIGIR 2019, Paris", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CL-SciSumm Shared Task is the first medium-scale shared task on\nscientific document summarization in the computational linguistics~(CL) domain.\nIn 2019, it comprised three tasks: (1A) identifying relationships between\nciting documents and the referred document, (1B) classifying the discourse\nfacets, and (2) generating the abstractive summary. The dataset comprised 40\nannotated sets of citing and reference papers of the CL-SciSumm 2018 corpus and\n1000 more from the SciSummNet dataset. All papers are from the open access\nresearch papers in the CL domain. This overview describes the participation and\nthe official results of the CL-SciSumm 2019 Shared Task, organized as a part of\nthe 42nd Annual Conference of the Special Interest Group in Information\nRetrieval (SIGIR), held in Paris, France in July 2019. We compare the\nparticipating systems in terms of two evaluation metrics and discuss the use of\nROUGE as an evaluation metric. The annotated dataset used for this shared task\nand the scripts used for evaluation can be accessed and used by the community\nat: https://github.com/WING-NUS/scisumm-corpus.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:06:01 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chandrasekaran", "Muthu Kumar", ""], ["Yasunaga", "Michihiro", ""], ["Radev", "Dragomir", ""], ["Freitag", "Dayne", ""], ["Kan", "Min-Yen", ""]]}, {"id": "1907.09899", "submitter": "Ziyun Wang", "authors": "Ziyun Wang, Brenden M. Lake", "title": "Modeling question asking using neural program generation", "comments": "Please cite as: Wang, Z. and Lake, B. M. (2021). Modeling question\n  asking using neural program generation. In Proceedings of the 43rd Annual\n  Conference of the Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People ask questions that are far richer, more informative, and more creative\nthan current AI systems. We propose a neuro-symbolic framework for modeling\nhuman question asking, which represents questions as formal programs and\ngenerates programs with an encoder-decoder based deep neural network. From\nextensive experiments using an information-search game, we show that our method\ncan predict which questions humans are likely to ask in unconstrained settings.\nWe also propose a novel grammar-based question generation framework trained\nwith reinforcement learning, which is able to generate creative questions\nwithout supervised human data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:20:21 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:18:55 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 13:27:11 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 09:04:41 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Ziyun", ""], ["Lake", "Brenden M.", ""]]}, {"id": "1907.10016", "submitter": "Shikib Mehri", "authors": "Shikib Mehri, Tejas Srinivasan and Maxine Eskenazi", "title": "Structured Fusion Networks for Dialog", "comments": "Accepted to SIGDial 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural dialog models have exhibited strong performance, however their\nend-to-end nature lacks a representation of the explicit structure of dialog.\nThis results in a loss of generalizability, controllability and a data-hungry\nnature. Conversely, more traditional dialog systems do have strong models of\nexplicit structure. This paper introduces several approaches for explicitly\nincorporating structure into neural models of dialog. Structured Fusion\nNetworks first learn neural dialog modules corresponding to the structured\ncomponents of traditional dialog systems and then incorporate these modules in\na higher-level generative model. Structured Fusion Networks obtain strong\nresults on the MultiWOZ dataset, both with and without reinforcement learning.\nStructured Fusion Networks are shown to have several valuable properties,\nincluding better domain generalizability, improved performance in reduced data\nscenarios and robustness to divergence during reinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:20:13 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Mehri", "Shikib", ""], ["Srinivasan", "Tejas", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1907.10036", "submitter": "Sara Evensen", "authors": "Sara Evensen, Yoshihiko Suhara, Alon Halevy, Vivian Li, Wang-Chiew\n  Tan, Saran Mumick", "title": "Happiness Entailment: Automating Suggestions for Well-Being", "comments": "ACII 2019, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding what makes people happy is a central topic in psychology. Prior\nwork has mostly focused on developing self-reporting assessment tools for\nindividuals and relies on experts to analyze the periodic reported assessments.\nOne of the goals of the analysis is to understand what actions are necessary to\nencourage modifications in the behaviors of the individuals to improve their\noverall well-being. In this paper, we outline a complementary approach; on the\nassumption that the user journals her happy moments as short texts, a system\ncan analyze these texts and propose sustainable suggestions for the user that\nmay lead to an overall improvement in her well-being. We prototype one\nnecessary component of such a system, the Happiness Entailment Recognition\n(HER) module, which takes as input a short text describing an event, a\ncandidate suggestion, and outputs a determination about whether the suggestion\nis more likely to be good for this user based on the event described. This\ncomponent is implemented as a neural network model with two encoders, one for\nthe user input and one for the candidate actionable suggestion, with additional\nlayers to capture psychologically significant features in the happy moment and\nsuggestion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:46:02 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Evensen", "Sara", ""], ["Suhara", "Yoshihiko", ""], ["Halevy", "Alon", ""], ["Li", "Vivian", ""], ["Tan", "Wang-Chiew", ""], ["Mumick", "Saran", ""]]}, {"id": "1907.10129", "submitter": "Aditi Chaudhary", "authors": "Aditi Chaudhary, Elizabeth Salesky, Gayatri Bhat, David R. Mortensen,\n  Jaime G. Carbonell, Yulia Tsvetkov", "title": "CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context\n  in Morphology", "comments": "In Proceedings of the ACL-SIGMORPHON 2019 Shared Task:\n  Crosslinguality and Context in Morphology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019\ntask 2 of Morphological Analysis and Lemmatization in Context. This task\nrequires us to produce the lemma and morpho-syntactic description of each token\nin a sequence, for 107 treebanks. We approach this task with a hierarchical\nneural conditional random field (CRF) model which predicts each coarse-grained\nfeature (eg. POS, Case, etc.) independently. However, most treebanks are\nunder-resourced, thus making it challenging to train deep neural models for\nthem. Hence, we propose a multi-lingual transfer training regime where we\ntransfer from multiple related languages that share similar typology.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:05:37 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Chaudhary", "Aditi", ""], ["Salesky", "Elizabeth", ""], ["Bhat", "Gayatri", ""], ["Mortensen", "David R.", ""], ["Carbonell", "Jaime G.", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "1907.10136", "submitter": "Vinayshekhar Bannihatti Kumar", "authors": "Vinayshekhar Bannihatti Kumar, Ashwin Srinivasan, Aditi Chaudhary,\n  James Route, Teruko Mitamura, Eric Nyberg", "title": "Dr.Quad at MEDIQA 2019: Towards Textual Inference and Question\n  Entailment using contextualized representations", "comments": "Accepted in ACL challenge MediQA as part of the BioNLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019\nshared task on Textual Inference and Question Entailment in the Medical Domain.\nOur system is based on the prior work Liu et al. (2019) which uses a multi-task\nobjective function for textual entailment. In this work, we explore different\nstrategies for generalizing state-of-the-art language understanding models to\nthe specialized medical domain. Our results on the shared task demonstrate that\nincorporating domain knowledge through data augmentation is a powerful strategy\nfor addressing challenges posed by specialized domains such as medicine.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:18:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Kumar", "Vinayshekhar Bannihatti", ""], ["Srinivasan", "Ashwin", ""], ["Chaudhary", "Aditi", ""], ["Route", "James", ""], ["Mitamura", "Teruko", ""], ["Nyberg", "Eric", ""]]}, {"id": "1907.10165", "submitter": "Derek Tam", "authors": "Derek Tam, Nicholas Monath, Ari Kobren, Aaron Traylor, Rajarshi Das,\n  Andrew McCallum", "title": "Optimal Transport-based Alignment of Learned Character Representations\n  for String Similarity", "comments": "ACL Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String similarity models are vital for record linkage, entity resolution, and\nsearch. In this work, we present STANCE --a learned model for computing the\nsimilarity of two strings. Our approach encodes the characters of each string,\naligns the encodings using Sinkhorn Iteration (alignment is posed as an\ninstance of optimal transport) and scores the alignment with a convolutional\nneural network. We evaluate STANCE's ability to detect whether two strings can\nrefer to the same entity--a task we term alias detection. We construct five new\nalias detection datasets (and make them publicly available). We show that\nSTANCE or one of its variants outperforms both state-of-the-art and classic,\nparameter-free similarity models on four of the five datasets. We also\ndemonstrate STANCE's ability to improve downstream tasks by applying it to an\ninstance of cross-document coreference and show that it leads to a 2.8 point\nimprovement in B^3 F1 over the previous state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 22:41:22 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Tam", "Derek", ""], ["Monath", "Nicholas", ""], ["Kobren", "Ari", ""], ["Traylor", "Aaron", ""], ["Das", "Rajarshi", ""], ["McCallum", "Andrew", ""]]}, {"id": "1907.10185", "submitter": "Patrick Lumbantobing", "authors": "Patrick Lumban Tobing, Yi-Chiao Wu, Tomoki Hayashi, Kazuhiro\n  Kobayashi, Tomoki Toda", "title": "Non-Parallel Voice Conversion with Cyclic Variational Autoencoder", "comments": "Accepted to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel technique for a non-parallel voice\nconversion (VC) with the use of cyclic variational autoencoder (CycleVAE)-based\nspectral modeling. In a variational autoencoder(VAE) framework, a latent space,\nusually with a Gaussian prior, is used to encode a set of input features. In a\nVAE-based VC, the encoded latent features are fed into a decoder, along with\nspeaker-coding features, to generate estimated spectra with either the original\nspeaker identity (reconstructed) or another speaker identity (converted). Due\nto the non-parallel modeling condition, the converted spectra can not be\ndirectly optimized, which heavily degrades the performance of a VAE-based VC.\nIn this work, to overcome this problem, we propose to use CycleVAE-based\nspectral model that indirectly optimizes the conversion flow by recycling the\nconverted features back into the system to obtain corresponding cyclic\nreconstructed spectra that can be directly optimized. The cyclic flow can be\ncontinued by using the cyclic reconstructed features as input for the next\ncycle. The experimental results demonstrate the effectiveness of the proposed\nCycleVAE-based VC, which yields higher accuracy of converted spectra, generates\nlatent features with higher correlation degree, and significantly improves the\nquality and conversion accuracy of the converted speech.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 00:37:20 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Tobing", "Patrick Lumban", ""], ["Wu", "Yi-Chiao", ""], ["Hayashi", "Tomoki", ""], ["Kobayashi", "Kazuhiro", ""], ["Toda", "Tomoki", ""]]}, {"id": "1907.10210", "submitter": "Jian Zhu", "authors": "Jian Zhu, Will Styler and Ian Calloway", "title": "A CNN-based tool for automatic tongue contour tracking in ultrasound\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For speech research, ultrasound tongue imaging provides a non-invasive means\nfor visualizing tongue position and movement during articulation. Extracting\ntongue contours from ultrasound images is a basic step in analyzing ultrasound\ndata but this task often requires non-trivial manual annotation. This study\npresents an open source tool for fully automatic tracking of tongue contours in\nultrasound frames using neural network based methods. We have implemented and\nsystematically compared two convolutional neural networks, U-Net and\nDenseU-Net, under different conditions. Though both models can perform\nautomatic contour tracking with comparable accuracy, Dense U-Net architecture\nseems more generalizable across test datasets while U-Net has faster extraction\nspeed. Our comparison also shows that the choice of loss function and data\naugmentation have a greater effect on tracking performance in this task. This\npublic available segmentation tool shows considerable promise for the automated\ntongue contour annotation of ultrasound images in speech research.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:34:58 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhu", "Jian", ""], ["Styler", "Will", ""], ["Calloway", "Ian", ""]]}, {"id": "1907.10302", "submitter": "Jun Gao", "authors": "Wei Bi, Jun Gao, Xiaojiang Liu, Shuming Shi", "title": "Fine-Grained Sentence Functions for Short-Text Conversation", "comments": "Here is a revised version of our paper accepted by ACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence function is an important linguistic feature referring to a user's\npurpose in uttering a specific sentence. The use of sentence function has shown\npromising results to improve the performance of conversation models. However,\nthere is no large conversation dataset annotated with sentence functions. In\nthis work, we collect a new Short-Text Conversation dataset with manually\nannotated SEntence FUNctions (STC-Sefun). Classification models are trained on\nthis dataset to (i) recognize the sentence function of new data in a large\ncorpus of short-text conversations; (ii) estimate a proper sentence function of\nthe response given a test query. We later train conversation models conditioned\non the sentence functions, including information retrieval-based and neural\ngenerative models. Experimental results demonstrate that the use of sentence\nfunctions can help improve the quality of the returned responses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:49:01 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 07:08:46 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 04:03:04 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bi", "Wei", ""], ["Gao", "Jun", ""], ["Liu", "Xiaojiang", ""], ["Shi", "Shuming", ""]]}, {"id": "1907.10352", "submitter": "Fabio Kepler", "authors": "Fabio Kepler and Jonay Tr\\'enous and Marcos Treviso and Miguel Vera\n  and Ant\\'onio G\\'ois and M. Amin Farajian and Ant\\'onio V. Lopes and Andr\\'e\n  F. T. Martins", "title": "Unbabel's Participation in the WMT19 Translation Quality Estimation\n  Shared Task", "comments": "In Proceedings of the Fourth Conference on Machine Translation (WMT)\n  2019: https://www.aclweb.org/anthology/W19-5406/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the contribution of the Unbabel team to the WMT 2019 Shared Task\non Quality Estimation. We participated on the word, sentence, and\ndocument-level tracks, encompassing 3 language pairs: English-German,\nEnglish-Russian, and English-French. Our submissions build upon the recent\nOpenKiwi framework: we combine linear, neural, and predictor-estimator systems\nwith new transfer learning approaches using BERT and XLM pre-trained models. We\ncompare systems individually and propose new ensemble techniques for word and\nsentence-level predictions. We also propose a simple technique for converting\nword labels into document-level predictions. Overall, our submitted systems\nachieve the best results on all tracks and language pairs by a considerable\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:30:39 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 21:28:17 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Kepler", "Fabio", ""], ["Tr\u00e9nous", "Jonay", ""], ["Treviso", "Marcos", ""], ["Vera", "Miguel", ""], ["G\u00f3is", "Ant\u00f3nio", ""], ["Farajian", "M. Amin", ""], ["Lopes", "Ant\u00f3nio V.", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "1907.10362", "submitter": "Ant\\'onio G\\'ois", "authors": "Ant\\'onio G\\'ois, Andr\\'e F. T. Martins", "title": "Translator2Vec: Understanding and Representing Human Post-Editors", "comments": "Accepted on MT Summit 2019; dataset available here:\n  https://www.github.com/Unbabel/translator2vec; please cite as:\n  @article{gois2019translator2vec, title={Translator2Vec: Understanding and\n  Representing Human Post-Editors}, author={G\\'ois, Ant\\'onio and F. T.\n  Martins, Andr\\'e}, year={2019}, publisher={European Association for Machine\n  Translation} }", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of machines and humans for translation is effective, with\nmany studies showing productivity gains when humans post-edit\nmachine-translated output instead of translating from scratch. To take full\nadvantage of this combination, we need a fine-grained understanding of how\nhuman translators work, and which post-editing styles are more effective than\nothers. In this paper, we release and analyze a new dataset with document-level\npost-editing action sequences, including edit operations from keystrokes, mouse\nactions, and waiting times. Our dataset comprises 66,268 full document sessions\npost-edited by 332 humans, the largest of the kind released to date. We show\nthat action sequences are informative enough to identify post-editors\naccurately, compared to baselines that only look at the initial and final text.\nWe build on this to learn and visualize continuous representations of\npost-editors, and we show that these representations improve the downstream\ntask of predicting post-editing time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 11:01:24 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["G\u00f3is", "Ant\u00f3nio", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "1907.10371", "submitter": "Lei Li", "authors": "Wenhuan Zeng, Abulikemu Abuduweili, Lei Li, Pengcheng Yang", "title": "Automatic Generation of Personalized Comment Based on User Profile", "comments": "ACL SRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comments on social media are very diverse, in terms of content, style and\nvocabulary, which make generating comments much more challenging than other\nexisting natural language generation~(NLG) tasks. Besides, since different user\nhas different expression habits, it is necessary to take the user's profile\ninto consideration when generating comments. In this paper, we introduce the\ntask of automatic generation of personalized comment~(AGPC) for social media.\nBased on tens of thousands of users' real comments and corresponding user\nprofiles on weibo, we propose Personalized Comment Generation Network~(PCGN)\nfor AGPC. The model utilizes user feature embedding with a gated memory and\nattends to user description to model personality of users. In addition,\nexternal user representation is taken into consideration during the decoding to\nenhance the comments generation. Experimental results show that our model can\ngenerate natural, human-like and personalized comments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 11:37:08 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zeng", "Wenhuan", ""], ["Abuduweili", "Abulikemu", ""], ["Li", "Lei", ""], ["Yang", "Pengcheng", ""]]}, {"id": "1907.10449", "submitter": "Sebastian Pado", "authors": "Sebastian Pado and Daniel Hole", "title": "Distributional Analysis of Polysemous Function Words", "comments": "Extended version of paper presented at TbiLLC 2019, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the phenomenon of function word\npolysemy. We adopt the framework of distributional semantics, which\ncharacterizes word meaning by observing occurrence contexts in large corpora\nand which is in principle well situated to model polysemy. Nevertheless,\nfunction words were traditionally considered as impossible to analyze\ndistributionally due to their highly flexible usage patterns. We establish that\ncontextualized word embeddings, the most recent generation of distributional\nmethods, offer hope in this regard. Using the German reflexive pronoun 'sich'\nas an example, we find that contextualized word embeddings capture\ntheoretically motivated word senses for 'sich' to the extent to which these\nsenses are mirrored systematically in linguistic usage.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:48:16 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 17:31:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Pado", "Sebastian", ""], ["Hole", "Daniel", ""]]}, {"id": "1907.10529", "submitter": "Mandar Joshi", "authors": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke\n  Zettlemoyer, Omer Levy", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "comments": "Accepted at TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SpanBERT, a pre-training method that is designed to better\nrepresent and predict spans of text. Our approach extends BERT by (1) masking\ncontiguous random spans, rather than random tokens, and (2) training the span\nboundary representations to predict the entire content of the masked span,\nwithout relying on the individual token representations within it. SpanBERT\nconsistently outperforms BERT and our better-tuned baselines, with substantial\ngains on span selection tasks such as question answering and coreference\nresolution. In particular, with the same training data and model size as\nBERT-large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0,\nrespectively. We also achieve a new state of the art on the OntoNotes\ncoreference resolution task (79.6\\% F1), strong performance on the TACRED\nrelation extraction benchmark, and even show gains on GLUE.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:43:40 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 17:13:57 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 03:53:04 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Joshi", "Mandar", ""], ["Chen", "Danqi", ""], ["Liu", "Yinhan", ""], ["Weld", "Daniel S.", ""], ["Zettlemoyer", "Luke", ""], ["Levy", "Omer", ""]]}, {"id": "1907.10568", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy Pavel, Maxine\n  Eskenazi, and Jeffrey P. Bigham", "title": "Investigating Evaluation of Open-Domain Dialogue Systems With Human\n  Generated Multiple References", "comments": "SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to mitigate the shortcomings of automatic evaluation\nof open-domain dialog systems through multi-reference evaluation. Existing\nmetrics have been shown to correlate poorly with human judgement, particularly\nin open-domain dialog. One alternative is to collect human annotations for\nevaluation, which can be expensive and time consuming. To demonstrate the\neffectiveness of multi-reference evaluation, we augment the test set of\nDailyDialog with multiple references. A series of experiments show that the use\nof multiple references results in improved correlation between several\nautomatic metrics and human judgement for both the quality and the diversity of\nsystem output.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 17:18:48 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 06:08:46 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Gupta", "Prakhar", ""], ["Mehri", "Shikib", ""], ["Zhao", "Tiancheng", ""], ["Pavel", "Amy", ""], ["Eskenazi", "Maxine", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1907.10597", "submitter": "Roy Schwartz", "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni", "title": "Green AI", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computations required for deep learning research have been doubling every\nfew months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].\nThese computations have a surprisingly large carbon footprint [38]. Ironically,\ndeep learning was inspired by the human brain, which is remarkably energy\nefficient. Moreover, the financial cost of the computations can make it\ndifficult for academics, students, and researchers, in particular those from\nemerging economies, to engage in deep learning research.\n  This position paper advocates a practical solution by making efficiency an\nevaluation criterion for research alongside accuracy and related measures. In\naddition, we propose reporting the financial cost or \"price tag\" of developing,\ntraining, and running models to provide baselines for the investigation of\nincreasingly efficient methods. Our goal is to make AI both greener and more\ninclusive---enabling any inspired undergraduate with a laptop to write\nhigh-quality research papers. Green AI is an emerging focus at the Allen\nInstitute for AI.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:36:18 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 02:54:44 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 20:09:57 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Schwartz", "Roy", ""], ["Dodge", "Jesse", ""], ["Smith", "Noah A.", ""], ["Etzioni", "Oren", ""]]}, {"id": "1907.10641", "submitter": "Keisuke Sakaguchi", "authors": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi", "title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011),\na benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun\nresolution problems originally designed to be unsolvable for statistical models\nthat rely on selectional preferences or word associations. However, recent\nadvances in neural language models have already reached around 90% accuracy on\nvariants of WSC. This raises an important question whether these models have\ntruly acquired robust commonsense capabilities or whether they rely on spurious\nbiases in the datasets that lead to an overestimation of the true capabilities\nof machine commonsense. To investigate this question, we introduce WinoGrande,\na large-scale dataset of 44k problems, inspired by the original WSC design, but\nadjusted to improve both the scale and the hardness of the dataset. The key\nsteps of the dataset construction consist of (1) a carefully designed\ncrowdsourcing procedure, followed by (2) systematic bias reduction using a\nnovel AfLite algorithm that generalizes human-detectable word associations to\nmachine-detectable embedding associations. The best state-of-the-art methods on\nWinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of\n94.0%, depending on the amount of the training data allowed. Furthermore, we\nestablish new state-of-the-art results on five related benchmarks - WSC\n(90.1%), DPR (93.1%), COPA (90.6%), KnowRef (85.6%), and Winogender (97.1%).\nThese results have dual implications: on one hand, they demonstrate the\neffectiveness of WinoGrande when used as a resource for transfer learning. On\nthe other hand, they raise a concern that we are likely to be overestimating\nthe true capabilities of machine commonsense across all these benchmarks. We\nemphasize the importance of algorithmic bias reduction in existing and future\nbenchmarks to mitigate such overestimation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 18:11:59 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 19:01:32 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Sakaguchi", "Keisuke", ""], ["Bras", "Ronan Le", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""]]}, {"id": "1907.10658", "submitter": "Kevin Bowden", "authors": "Kevin K.Bowden, Jiaqi Wu, Wen Cui, Juraj Juraska, Vrindavan Harrison,\n  Brian Schwarzmann, Nick Santer, Marilyn Walker", "title": "SlugBot: Developing a Computational Model andFramework of a Novel\n  Dialogue Genre", "comments": "arXiv admin note: text overlap with arXiv:1801.01531", "journal-ref": null, "doi": "10.13140/RG.2.2.33543.96166", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most interesting aspects of the Amazon Alexa Prize competition is\nthat the framing of the competition requires the development of new\ncomputational models of dialogue and its structure. Traditional computational\nmodels of dialogue are of two types: (1) task-oriented dialogue, supported by\nAI planning models,or simplified planning models consisting of frames with\nslots to be filled; or (2)search-oriented dialogue where every user turn is\ntreated as a search query that may elaborate and extend current search results.\nAlexa Prize dialogue systems such as SlugBot must support conversational\ncapabilities that go beyond what these traditional models can do. Moreover,\nwhile traditional dialogue systems rely on theoretical computational models,\nthere are no existing computational theories that circumscribe the expected\nsystem and user behaviors in the intended conversational genre of the Alexa\nPrize Bots. This paper describes how UCSC's SlugBot team has combined the\ndevelopment of a novel computational theoretical model, Discourse Relation\nDialogue Model, with its implementation in a modular system in order to test\nand refine it. We highlight how our novel dialogue model has led us to create a\nnovel ontological resource, UniSlug, and how the structure of UniSlug determine\nshow we curate and structure content so that our dialogue manager implements\nand tests our novel computational dialogue model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:16:58 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Wu", "Jiaqi", ""], ["Cui", "Wen", ""], ["Juraska", "Juraj", ""], ["Harrison", "Vrindavan", ""], ["Schwarzmann", "Brian", ""], ["Santer", "Nick", ""], ["Walker", "Marilyn", ""]]}, {"id": "1907.10676", "submitter": "Diego Moussallem", "authors": "Diego Moussallem and Matthias Wauer and Axel-Cyrille Ngonga Ngomo", "title": "Semantic Web for Machine Translation: Challenges and Directions", "comments": "Accepted at the Journal track of International Semantic Web\n  conference (ISWC) 2019. arXiv admin note: substantial text overlap with\n  arXiv:1711.09476", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of machine translation approaches have recently been developed\nto facilitate the fluid migration of content across languages. However, the\nliterature suggests that many obstacles must still be dealt with to achieve\nbetter automatic translations. One of these obstacles is lexical and syntactic\nambiguity. A promising way of overcoming this problem is using Semantic Web\ntechnologies. This article is an extended abstract of our systematic review on\nmachine translation approaches that rely on Semantic Web technologies for\nimproving the translation of texts. Overall, we present the challenges and\nopportunities in the use of Semantic Web technologies in Machine Translation.\nMoreover, our research suggests that while Semantic Web technologies can\nenhance the quality of machine translation outputs for various problems, the\ncombination of both is still in its infancy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 15:49:20 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Moussallem", "Diego", ""], ["Wauer", "Matthias", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1907.10710", "submitter": "Hongfei Zhang", "authors": "Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N. Bennett,\n  Nick Craswell, and Saurabh Tiwary", "title": "Generic Intent Representation in Web Search", "comments": null, "journal-ref": "SIGIR 2019: Proceedings of the 42nd International ACM SIGIR\n  Conference on Research and Development in Information Retrieval", "doi": "10.1145/3331184.3331198", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a\ndistributed representation space for user intent in search. Leveraging large\nscale user clicks from Bing search logs as weak supervision of user intent, GEN\nEncoder learns to map queries with shared clicks into similar embeddings\nend-to-end and then finetunes on multiple paraphrase tasks. Experimental\nresults on an intrinsic evaluation task - query intent similarity modeling -\ndemonstrate GEN Encoder's robust and significant advantages over previous\nrepresentation methods. Ablation studies reveal the crucial role of learning\nfrom implicit user feedback in representing user intent and the contributions\nof multi-task learning in representation generality. We also demonstrate that\nGEN Encoder alleviates the sparsity of tail search traffic and cuts down half\nof the unseen queries by using an efficient approximate nearest neighbor search\nto effectively identify previous queries with the same search intent. Finally,\nwe demonstrate distances between GEN encodings reflect certain information\nseeking behaviors in search sessions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:40:19 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Zhang", "Hongfei", ""], ["Song", "Xia", ""], ["Xiong", "Chenyan", ""], ["Rosset", "Corby", ""], ["Bennett", "Paul N.", ""], ["Craswell", "Nick", ""], ["Tiwary", "Saurabh", ""]]}, {"id": "1907.10717", "submitter": "Giuseppe Di Molfetta Prof.", "authors": "Quentin Aristote and Nathana\\\"el Eon and Giuseppe Di Molfetta", "title": "Dynamical Triangulation Induced by Quantum Walk", "comments": null, "journal-ref": null, "doi": "10.3390/sym12010128", "report-no": null, "categories": "quant-ph cs.CL cs.DM gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the single-particle sector of a quantum cellular automaton, namely\na quantum walk, on a simple dynamical triangulated $2-$manifold. The\ntriangulation is changed through Pachner moves, induced by the walker density\nitself, allowing the surface to transform into any topologically equivalent\none. This model extends the quantum walk over triangular grid, introduced in a\nprevious work, by one of the authors, whose space-time limit recovers the Dirac\nequation in (2+1)-dimensions. Numerical simulations show that the number of\ntriangles and the local curvature grow as $t^\\alpha e^{-\\beta t^2}$, where\n$\\alpha$ and $\\beta$ parametrize the way geometry changes upon the local\ndensity of the walker, and that, in the long run, flatness emerges. Finally, we\nalso prove that the global behavior of the walker, remains the same under\nspacetime random fluctuations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:50:33 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 05:42:52 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 15:14:56 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 16:47:20 GMT"}, {"version": "v5", "created": "Wed, 8 Jan 2020 13:15:54 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Aristote", "Quentin", ""], ["Eon", "Nathana\u00ebl", ""], ["Di Molfetta", "Giuseppe", ""]]}, {"id": "1907.10726", "submitter": "Suyoun Kim", "authors": "Suyoun Kim, Siddharth Dalmia, Florian Metze", "title": "Cross-Attention End-to-End ASR for Two-Party Conversations", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end speech recognition model that learns interaction\nbetween two speakers based on the turn-changing information. Unlike\nconventional speech recognition models, our model exploits two speakers'\nhistory of conversational-context information that spans across multiple turns\nwithin an end-to-end framework. Specifically, we propose a speaker-specific\ncross-attention mechanism that can look at the output of the other speaker side\nas well as the one of the current speaker for better at recognizing long\nconversations. We evaluated the models on the Switchboard conversational speech\ncorpus and show that our model outperforms standard end-to-end speech\nrecognition models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 21:18:39 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Kim", "Suyoun", ""], ["Dalmia", "Siddharth", ""], ["Metze", "Florian", ""]]}, {"id": "1907.10738", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, Chitta Baral", "title": "Careful Selection of Knowledge to solve Open Book Question Answering", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open book question answering is a type of natural language based QA (NLQA)\nwhere questions are expected to be answered with respect to a given set of open\nbook facts, and common knowledge about a topic. Recently a challenge involving\nsuch QA, OpenBookQA, has been proposed. Unlike most other NLQA tasks that focus\non linguistic understanding, OpenBookQA requires deeper reasoning involving\nlinguistic understanding as well as reasoning with common knowledge. In this\npaper we address QA with respect to the OpenBookQA dataset and combine state of\nthe art language models with abductive information retrieval (IR), information\ngain based re-ranking, passage selection and weighted scoring to achieve 72.0%\naccuracy, an 11.6% improvement over the current state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 21:37:16 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Pal", "Kuntal Kumar", ""], ["Mitra", "Arindam", ""], ["Baral", "Chitta", ""]]}, {"id": "1907.10739", "submitter": "Sebastian Gehrmann", "authors": "Sebastian Gehrmann, Hendrik Strobelt, Robert Kr\\\"uger, Hanspeter\n  Pfister, Alexander M. Rush", "title": "Visual Interaction with Deep Learning Models through Collaborative\n  Semantic Inference", "comments": "IEEE VIS 2019 (VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of tasks can have critical consequences when humans lose agency\nover decision processes. Deep learning models are particularly susceptible\nsince current black-box approaches lack explainable reasoning. We argue that\nboth the visual interface and model structure of deep learning systems need to\ntake into account interaction design. We propose a framework of collaborative\nsemantic inference (CSI) for the co-design of interactions and models to enable\nvisual collaboration between humans and algorithms. The approach exposes the\nintermediate reasoning process of models which allows semantic interactions\nwith the visual metaphors of a problem, which means that a user can both\nunderstand and control parts of the model reasoning process. We demonstrate the\nfeasibility of CSI with a co-designed case study of a document summarization\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 21:37:29 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Strobelt", "Hendrik", ""], ["Kr\u00fcger", "Robert", ""], ["Pfister", "Hanspeter", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1907.10761", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Gorka Labaka, Eneko Agirre", "title": "Bilingual Lexicon Induction through Unsupervised Machine Translation", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent research line has obtained strong results on bilingual lexicon\ninduction by aligning independently trained word embeddings in two languages\nand using the resulting cross-lingual embeddings to induce word translation\npairs through nearest neighbor or related retrieval methods. In this paper, we\npropose an alternative approach to this problem that builds on the recent work\non unsupervised machine translation. This way, instead of directly inducing a\nbilingual lexicon from cross-lingual embeddings, we use them to build a\nphrase-table, combine it with a language model, and use the resulting machine\ntranslation system to generate a synthetic parallel corpus, from which we\nextract the bilingual lexicon using statistical word alignment techniques. As\nsuch, our method can work with any word embedding and cross-lingual mapping\ntechnique, and it does not require any additional resource besides the\nmonolingual corpus used to train the embeddings. When evaluated on the exact\nsame cross-lingual embeddings, our proposed method obtains an average\nimprovement of 6 accuracy points over nearest neighbor and 4 points over CSLS\nretrieval, establishing a new state-of-the-art in the standard MUSE dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 22:30:04 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Artetxe", "Mikel", ""], ["Labaka", "Gorka", ""], ["Agirre", "Eneko", ""]]}, {"id": "1907.10781", "submitter": "Hui Liu", "authors": "Hui Liu, Wentao Qin and Xiaojun Wan", "title": "INS: An Interactive Chinese News Synthesis System", "comments": "6 pages, 1 figure", "journal-ref": "Proceedings of NAACL-HLT 2019: Demonstrations, pages 18-23", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, we are surrounded by more and more online news articles. Tens or\nhundreds of news articles need to be read if we wish to explore a hot news\nevent or topic. So it is of vital importance to automatically synthesize a\nbatch of news articles related to the event or topic into a new synthesis\narticle (or overview article) for reader's convenience. It is so challenging to\nmake news synthesis fully automatic that there is no successful solution by\nnow. In this paper, we put forward a novel Interactive News Synthesis system\n(i.e. INS), which can help generate news overview articles automatically or by\ninteracting with users. More importantly, INS can serve as a tool for editors\nto help them finish their jobs. In our experiments, INS performs well on both\ntopic representation and synthesis article generation. A user study also\ndemonstrates the usefulness and users' satisfaction with the INS tool. A demo\nvideo is available at \\url{https://youtu.be/7ItteKW3GEk}.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 01:01:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Liu", "Hui", ""], ["Qin", "Wentao", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1907.10873", "submitter": "Nikola Nikolov", "authors": "Nikola I. Nikolov, Alessandro Calmanovici, Richard H.R. Hahnloser", "title": "Summary Refinement through Denoising", "comments": "RANLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method for post-processing the outputs of a text\nsummarization system in order to refine its overall quality. Our approach is to\ntrain text-to-text rewriting models to correct information redundancy errors\nthat may arise during summarization. We train on synthetically generated noisy\nsummaries, testing three different types of noise that introduce out-of-context\ninformation within each summary. When applied on top of extractive and\nabstractive summarization baselines, our summary denoising models yield metric\nimprovements while reducing redundancy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:37:41 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Nikolov", "Nikola I.", ""], ["Calmanovici", "Alessandro", ""], ["Hahnloser", "Richard H. R.", ""]]}, {"id": "1907.10885", "submitter": "Rui Li", "authors": "Rui Li, Kai Shuang, Mengyu Gu, Sen Su", "title": "Adaptive Noise Injection: A Structure-Expanding Regularization for RNN", "comments": "Recently, we find the theory \"extending model can play the role of\n  regularization\"doesn't hold on other NLP tasks' datasets. Now, we are looking\n  for a new theory to explain the effectiveness of ANI.We don't have an\n  alternative version yet, so we choose to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vanilla LSTM has become one of the most potential architectures in\nword-level language modeling, like other recurrent neural networks, overfitting\nis always a key barrier for its effectiveness. The existing noise-injected\nregularizations introduce the random noises of fixation intensity, which\ninhibits the learning of the RNN throughout the training process. In this\npaper, we propose a new structure-expanding regularization method called\nAdjective Noise Injection (ANI), which considers the output of an extra RNN\nbranch as a kind of adaptive noises and injects it into the main-branch RNN\noutput. Due to the adaptive noises can be improved as the training processes,\nits negative effects can be weakened and even transformed into a positive\neffect to further improve the expressiveness of the main-branch RNN. As a\nresult, ANI can regularize the RNN in the early stage of training and further\npromoting its training performance in the later stage. We conduct experiments\non three widely-used corpora: PTB, WT2, and WT103, whose results verify both\nthe regularization and promoting the training performance functions of ANI.\nFurthermore, we design a series simulation experiments to explore the reasons\nthat may lead to the regularization effect of ANI, and we find that in training\nprocess, the robustness against the parameter update errors can be strengthened\nwhen the LSTM equipped with ANI.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:58:08 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:05:26 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Li", "Rui", ""], ["Shuang", "Kai", ""], ["Gu", "Mengyu", ""], ["Su", "Sen", ""]]}, {"id": "1907.11049", "submitter": "Chunyang Xiao", "authors": "Chunyang Xiao, Christoph Teichmann, and Konstantine Arkoudas", "title": "Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While sequence-to-sequence (seq2seq) models achieve state-of-the-art\nperformance in many natural language processing tasks, they can be too slow for\nreal-time applications. One performance bottleneck is predicting the most\nlikely next token over a large vocabulary; methods to circumvent this\nbottleneck are a current research topic. We focus specifically on using seq2seq\nmodels for semantic parsing, where we observe that grammars often exist which\nspecify valid formal representations of utterance semantics. By developing a\ngeneric approach for restricting the predictions of a seq2seq model to\ngrammatically permissible continuations, we arrive at a widely applicable\ntechnique for speeding up semantic parsing. The technique leads to a 74%\nspeed-up on an in-house dataset with a large vocabulary, compared to the same\nneural model without grammatical restrictions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:45:48 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Xiao", "Chunyang", ""], ["Teichmann", "Christoph", ""], ["Arkoudas", "Konstantine", ""]]}, {"id": "1907.11062", "submitter": "L\\'eo Hemamou", "authors": "L\\'eo Hemamou, Ghazi Felhi, Vincent Vandenbussche, Jean-Claude Martin,\n  Chlo\\'e Clavel", "title": "HireNet: a Hierarchical Attention Model for the Automatic Analysis of\n  Asynchronous Video Job Interviews", "comments": "AAAI 2019", "journal-ref": "Vol 33 (2019): Proceedings of the Thirty-Third AAAI Conference on\n  Artificial Intelligence", "doi": "10.1609/aaai.v33i01.3301573", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  New technologies drastically change recruitment techniques. Some research\nprojects aim at designing interactive systems that help candidates practice job\ninterviews. Other studies aim at the automatic detection of social signals\n(e.g. smile, turn of speech, etc...) in videos of job interviews. These studies\nare limited with respect to the number of interviews they process, but also by\nthe fact that they only analyze simulated job interviews (e.g. students\npretending to apply for a fake position). Asynchronous video interviewing tools\nhave become mature products on the human resources market, and thus, a popular\nstep in the recruitment process. As part of a project to help recruiters, we\ncollected a corpus of more than 7000 candidates having asynchronous video job\ninterviews for real positions and recording videos of themselves answering a\nset of questions. We propose a new hierarchical attention model called HireNet\nthat aims at predicting the hirability of the candidates as evaluated by\nrecruiters. In HireNet, an interview is considered as a sequence of questions\nand answers containing salient socials signals. Two contextual sources of\ninformation are modeled in HireNet: the words contained in the question and in\nthe job position. Our model achieves better F1-scores than previous approaches\nfor each modality (verbal content, audio and video). Results from early and\nlate multimodal fusion suggest that more sophisticated fusion schemes are\nneeded to improve on the monomodal results. Finally, some examples of moments\ncaptured by the attention mechanisms suggest our model could potentially be\nused to help finding key moments in an asynchronous job interview.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:00:18 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Hemamou", "L\u00e9o", ""], ["Felhi", "Ghazi", ""], ["Vandenbussche", "Vincent", ""], ["Martin", "Jean-Claude", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "1907.11065", "submitter": "Lin Zehui", "authors": "Lin Zehui, Pengfei Liu, Luyao Huang, Junkun Chen, Xipeng Qiu, Xuanjing\n  Huang", "title": "DropAttention: A Regularization Method for Fully-Connected\n  Self-Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variants dropout methods have been designed for the fully-connected layer,\nconvolutional layer and recurrent layer in neural networks, and shown to be\neffective to avoid overfitting. As an appealing alternative to recurrent and\nconvolutional layers, the fully-connected self-attention layer surprisingly\nlacks a specific dropout method. This paper explores the possibility of\nregularizing the attention weights in Transformers to prevent different\ncontextualized feature vectors from co-adaption. Experiments on a wide range of\ntasks show that DropAttention can improve performance and reduce overfitting.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:03:06 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 01:49:33 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zehui", "Lin", ""], ["Liu", "Pengfei", ""], ["Huang", "Luyao", ""], ["Chen", "Junkun", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1907.11112", "submitter": "Arpit Sharma", "authors": "Arpit Sharma", "title": "Using Answer Set Programming for Commonsense Reasoning in the Winograd\n  Schema Challenge", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Winograd Schema Challenge (WSC) is a natural language understanding task\nproposed as an alternative to the Turing test in 2011. In this work we attempt\nto solve WSC problems by reasoning with additional knowledge. By using an\napproach built on top of graph-subgraph isomorphism encoded using Answer Set\nProgramming (ASP) we were able to handle 240 out of 291 WSC problems. The ASP\nencoding allows us to add additional constraints in an elaboration tolerant\nmanner. In the process we present a graph based representation of WSC problems\nas well as relevant commonsense knowledge. This paper is under consideration\nfor acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:45:04 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Sharma", "Arpit", ""]]}, {"id": "1907.11158", "submitter": "Fariz Ikhwantri", "authors": "Fariz Ikhwantri", "title": "Cross-Lingual Transfer for Distantly Supervised and Low-resources\n  Indonesian NER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually annotated corpora for low-resource languages are usually small in\nquantity (gold), or large but distantly supervised (silver). Inspired by recent\nprogress of injecting pre-trained language model (LM) on many Natural Language\nProcessing (NLP) task, we proposed to fine-tune pre-trained language model from\nhigh-resources languages to low-resources languages to improve the performance\nof both scenarios. Our empirical experiment demonstrates significant\nimprovement when fine-tuning pre-trained language model in cross-lingual\ntransfer scenarios for small gold corpus and competitive results in large\nsilver compare to supervised cross-lingual transfer, which will be useful when\nthere is no parallel annotation in the same task to begin. We compare our\nproposed method of cross-lingual transfer using pre-trained LM to different\nsources of transfer such as mono-lingual LM and Part-of-Speech tagging (POS) in\nthe downstream task of both large silver and small gold NER dataset by\nexploiting character-level input of bi-directional language model task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 16:04:09 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Ikhwantri", "Fariz", ""]]}, {"id": "1907.11184", "submitter": "Prithviraj Sen", "authors": "Yiwei Yang, Eser Kandogan, Yunyao Li, Walter S. Lasecki, and\n  Prithviraj Sen", "title": "HEIDL: Learning Linguistic Expressions with Deep Learning and\n  Human-in-the-Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the role of humans is increasingly recognized in machine learning\ncommunity, representation of and interaction with models in current\nhuman-in-the-loop machine learning (HITL-ML) approaches are too low-level and\nfar-removed from human's conceptual models. We demonstrate HEIDL, a prototype\nHITL-ML system that exposes the machine-learned model through high-level,\nexplainable linguistic expressions formed of predicates representing semantic\nstructure of text. In HEIDL, human's role is elevated from simply evaluating\nmodel predictions to interpreting and even updating the model logic directly by\nenabling interaction with rule predicates themselves. Raising the currency of\ninteraction to such semantic levels calls for new interaction paradigms between\nhumans and machines that result in improved productivity for text analytics\nmodel development process. Moreover, by involving humans in the process, the\nhuman-machine co-created models generalize better to unseen data as domain\nexperts are able to instill their expertise by extrapolating from what has been\nlearned by automated algorithms from few labelled data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 16:45:06 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Yang", "Yiwei", ""], ["Kandogan", "Eser", ""], ["Li", "Yunyao", ""], ["Lasecki", "Walter S.", ""], ["Sen", "Prithviraj", ""]]}, {"id": "1907.11315", "submitter": "Rylan Conway", "authors": "Rylan Conway and Lambert Mathias", "title": "Time Masking: Leveraging Temporal Information in Spoken Dialogue Systems", "comments": "SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a spoken dialogue system, dialogue state tracker (DST) components track\nthe state of the conversation by updating a distribution of values associated\nwith each of the slots being tracked for the current user turn, using the\ninteractions until then. Much of the previous work has relied on modeling the\nnatural order of the conversation, using distance based offsets as an\napproximation of time. In this work, we hypothesize that leveraging the\nwall-clock temporal difference between turns is crucial for finer-grained\ncontrol of dialogue scenarios. We develop a novel approach that applies a {\\it\ntime mask}, based on the wall-clock time difference, to the associated slot\nembeddings and empirically demonstrate that our proposed approach outperforms\nexisting approaches that leverage distance offsets, on both an internal\nbenchmark dataset as well as DSTC2.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:33:29 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Conway", "Rylan", ""], ["Mathias", "Lambert", ""]]}, {"id": "1907.11438", "submitter": "Max Eichler", "authors": "Max Eichler, G\\\"ozde G\\\"ul \\c{S}ahin, Iryna Gurevych", "title": "LINSPECTOR WEB: A Multilingual Probing Suite for Word Representations", "comments": "Accepted at EMNLP 2019 System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present LINSPECTOR WEB, an open source multilingual inspector to analyze\nword representations. Our system provides researchers working in low-resource\nsettings with an easily accessible web based probing tool to gain quick\ninsights into their word embeddings especially outside of the English language.\nTo do this we employ 16 simple linguistic probing tasks such as gender, case\nmarking, and tense for a diverse set of 28 languages. We support probing of\nstatic word embeddings along with pretrained AllenNLP models that are commonly\nused for NLP downstream tasks such as named entity recognition, natural\nlanguage inference and dependency parsing. The results are visualized in a\npolar chart and also provided as a table. LINSPECTOR WEB is available as an\noffline tool or at https://linspector.ukp.informatik.tu-darmstadt.de.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:54:36 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 07:47:55 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 08:32:40 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Eichler", "Max", ""], ["\u015eahin", "G\u00f6zde G\u00fcl", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1907.11499", "submitter": "Yumo Xu", "authors": "Yumo Xu and Mirella Lapata", "title": "Weakly Supervised Domain Detection", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics (TACL); 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce domain detection as a new natural language\nprocessing task. We argue that the ability to detect textual segments which are\ndomain-heavy, i.e., sentences or phrases which are representative of and\nprovide evidence for a given domain could enhance the robustness and\nportability of various text classification applications. We propose an\nencoder-detector framework for domain detection and bootstrap classifiers with\nmultiple instance learning (MIL). The model is hierarchically organized and\nsuited to multilabel classification. We demonstrate that despite learning with\nminimal supervision, our model can be applied to text spans of different\ngranularities, languages, and genres. We also showcase the potential of domain\ndetection for text summarization.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:53:15 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Xu", "Yumo", ""], ["Lapata", "Mirella", ""]]}, {"id": "1907.11512", "submitter": "Leilei Gan", "authors": "Leilei Gan, Yue Zhang", "title": "Investigating Self-Attention Network for Chinese Word Segmentation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network has become the dominant method for Chinese word segmentation.\nMost existing models cast the task as sequence labeling, using BiLSTM-CRF for\nrepresenting the input and making output predictions. Recently, attention-based\nsequence models have emerged as a highly competitive alternative to LSTMs,\nwhich allow better running speed by parallelization of computation. We\ninvestigate self attention network for Chinese word segmentation, making\ncomparisons between BiLSTM-CRF models. In addition, the influence of\ncontextualized character embeddings is investigated using BERT, and a method is\nproposed for integrating word information into SAN segmentation. Results show\nthat SAN gives highly competitive results compared with BiLSTMs, with BERT and\nword information further improving segmentation for in-domain and cross-domain\nsegmentation. Our final models give the best results for 6 heterogenous domain\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 12:29:37 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Gan", "Leilei", ""], ["Zhang", "Yue", ""]]}, {"id": "1907.11521", "submitter": "Hai Ye", "authors": "Hai Ye and Zhunchen Luo", "title": "Deep Ranking Based Cost-sensitive Multi-label Learning for Distant\n  Supervision Relation Extraction", "comments": "Preprint submitted to Journal of Information Processing and\n  Management. arXiv admin note: text overlap with arXiv:1612.07602", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base provides a potential way to improve the intelligence of\ninformation retrieval (IR) systems, for that knowledge base has numerous\nrelations between entities which can help the IR systems to conduct inference\nfrom one entity to another entity. Relation extraction is one of the\nfundamental techniques to construct a knowledge base. Distant supervision is a\nsemi-supervised learning method for relation extraction which learns with\nlabeled and unlabeled data. However, this approach suffers the problem of\nrelation overlapping in which one entity tuple may have multiple relation\nfacts. We believe that relation types can have latent connections, which we\ncall class ties, and can be exploited to enhance relation extraction. However,\nthis property between relation classes has not been fully explored before. In\nthis paper, to exploit class ties between relations to improve relation\nextraction, we propose a general ranking based multi-label learning framework\ncombined with convolutional neural networks, in which ranking based loss\nfunctions with regularization technique are introduced to learn the latent\nconnections between relations. Furthermore, to deal with the problem of class\nimbalance in distant supervision relation extraction, we further adopt\ncost-sensitive learning to rescale the costs from the positive and negative\nlabels. Extensive experiments on a widely used dataset show the effectiveness\nof our model to exploit class ties and to relieve class imbalance problem.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:41:45 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Ye", "Hai", ""], ["Luo", "Zhunchen", ""]]}, {"id": "1907.11640", "submitter": "Roger Moore", "authors": "Roger K. Moore, Lucy Skidmore", "title": "On the Use/Misuse of the Term 'Phoneme'", "comments": "Accepted at INTERSPEECH-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term 'phoneme' lies at the heart of speech science and technology, and\nyet it is not clear that the research community fully appreciates its meaning\nand implications. In particular, it is suspected that many researchers use the\nterm in a casual sense to refer to the sounds of speech, rather than as a well\ndefined abstract concept. If true, this means that some sections of the\ncommunity may be missing an opportunity to understand and exploit the\nimplications of this important psychological phenomenon. Here we review the\ncorrect meaning of the term 'phoneme' and report the results of an\ninvestigation into its use/misuse in the accepted papers at INTERSPEECH-2018.\nIt is confirmed that a significant proportion of the community (i) may not be\naware of the critical difference between `phonetic' and 'phonemic' levels of\ndescription, (ii) may not fully understand the significance of 'phonemic\ncontrast', and as a consequence, (iii) consistently misuse the term 'phoneme'.\nThese findings are discussed, and recommendations are made as to how this\nsituation might be mitigated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:52:04 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Moore", "Roger K.", ""], ["Skidmore", "Lucy", ""]]}, {"id": "1907.11656", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "Vocal Interactivity in Crowds, Flocks and Swarms: Implications for Voice\n  User Interfaces", "comments": "Accepted at 2nd International Workshop on Vocal Interactivity\n  in-and-between Humans, Animals and Robots (VIHAR-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an explosion in the availability of Voice User\nInterfaces. However, user surveys suggest that there are issues with respect to\nusability, and it has been hypothesised that contemporary voice-enabled systems\nare missing crucial behaviours relating to user engagement and vocal\ninteractivity. However, it is well established that such ostensive behaviours\nare ubiquitous in the animal kingdom, and that vocalisation provides a means\nthrough which interaction may be coordinated and managed between individuals\nand within groups. Hence, this paper reports results from a study aimed at\nidentifying generic mechanisms that might underpin coordinated collective vocal\nbehaviour with a particular focus on closed-loop negative-feedback control as a\npowerful regulatory process. A computer-based real-time simulation of vocal\ninteractivity is described which has provided a number of insights, including\nthe enumeration of a number of key control variables that may be worthy of\nfurther investigation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:07:20 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1907.11692", "submitter": "Myle Ott", "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\n  Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:48:29 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Liu", "Yinhan", ""], ["Ott", "Myle", ""], ["Goyal", "Naman", ""], ["Du", "Jingfei", ""], ["Joshi", "Mandar", ""], ["Chen", "Danqi", ""], ["Levy", "Omer", ""], ["Lewis", "Mike", ""], ["Zettlemoyer", "Luke", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1907.11769", "submitter": "Antoine Tixier", "authors": "Henrietta Baker, Matthew R. Hallowell, Antoine J.-P. Tixier", "title": "Automatically Learning Construction Injury Precursors from Text", "comments": "Fixed typos, updated one figure, updated corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of the increasing availability of digitally recorded safety reports\nin the construction industry, it is important to develop methods to exploit\nthese data to improve our understanding of safety incidents and ability to\nlearn from them. In this study, we compare several approaches to automatically\nlearn injury precursors from raw construction accident reports. More precisely,\nwe experiment with two state-of-the-art deep learning architectures for Natural\nLanguage Processing (NLP), Convolutional Neural Networks (CNN) and Hierarchical\nAttention Networks (HAN), and with the established Term Frequency - Inverse\nDocument Frequency representation (TF-IDF) + Support Vector Machine (SVM)\napproach. For each model, we provide a method to identify (after training) the\ntextual patterns that are, on average, the most predictive of each safety\noutcome. We show that among those pieces of text, valid injury precursors can\nbe found. The proposed methods can also be used by the user to visualize and\nunderstand the models' predictions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 19:43:07 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 11:19:52 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 12:48:15 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Baker", "Henrietta", ""], ["Hallowell", "Matthew R.", ""], ["Tixier", "Antoine J. -P.", ""]]}, {"id": "1907.11779", "submitter": "Matej Martinc", "authors": "Matej Martinc, Senja Pollak, Marko Robnik-\\v{S}ikonja", "title": "Supervised and Unsupervised Neural Approaches to Text Readability", "comments": "39 pages, published in Computational Linguistic Journal", "journal-ref": null, "doi": "10.1162/coli_a_00398", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a set of novel neural supervised and unsupervised approaches for\ndetermining the readability of documents. In the unsupervised setting, we\nleverage neural language models, whereas in the supervised setting, three\ndifferent neural classification architectures are tested. We show that the\nproposed neural unsupervised approach is robust, transferable across languages\nand allows adaptation to a specific readability task and data set. By\nsystematic comparison of several neural architectures on a number of benchmark\nand new labelled readability datasets in two languages, this study also offers\na comprehensive analysis of different neural approaches to readability\nclassification. We expose their strengths and weaknesses, compare their\nperformance to current state-of-the-art classification approaches to\nreadability, which in most cases still rely on extensive feature engineering,\nand propose possibilities for improvements.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 20:04:57 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 08:32:27 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 14:18:21 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Martinc", "Matej", ""], ["Pollak", "Senja", ""], ["Robnik-\u0160ikonja", "Marko", ""]]}, {"id": "1907.11843", "submitter": "Chao Lu", "authors": "Chao Lu, Yi Bu, Xianlei Dong, Jie Wang, Ying Ding, Vincent\n  Larivi\\`ere, Cassidy R. Sugimoto, Logan Paul, Chengzhi Zhang", "title": "Analyzing Linguistic Complexity and Scientific Impact", "comments": "27pages, 6 figures, accepted by Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of publications and the number of citations received have become\nthe most common indicators of scholarly success. In this context, scientific\nwriting increasingly plays an important role in scholars' scientific careers.\nTo understand the relationship between scientific writing and scientific\nimpact, this paper selected 12 variables of linguistic complexity as a proxy\nfor depicting scientific writing. We then analyzed these features from 36,400\nfull-text Biology articles and 1,797 full-text Psychology articles. These\nfeatures were compared to the scientific impact of articles, grouped into high,\nmedium, and low categories. The results suggested no practical significant\nrelationship between linguistic complexity and citation strata in either\ndiscipline. This suggests that textual complexity plays little role in\nscientific impact in our data sets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 04:08:55 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lu", "Chao", ""], ["Bu", "Yi", ""], ["Dong", "Xianlei", ""], ["Wang", "Jie", ""], ["Ding", "Ying", ""], ["Larivi\u00e8re", "Vincent", ""], ["Sugimoto", "Cassidy R.", ""], ["Paul", "Logan", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "1907.11889", "submitter": "Matan Orbach", "authors": "Tamar Lavee, Matan Orbach, Lili Kotlerman, Yoav Kantor, Shai Gretz,\n  Lena Dankin, Shachar Mirkin, Michal Jacovi, Yonatan Bilu, Ranit Aharonov and\n  Noam Slonim", "title": "Towards Effective Rebuttal: Listening Comprehension using Corpus-Wide\n  Claim Mining", "comments": "6th Argument Mining Workshop @ ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engaging in a live debate requires, among other things, the ability to\neffectively rebut arguments claimed by your opponent. In particular, this\nrequires identifying these arguments. Here, we suggest doing so by\nautomatically mining claims from a corpus of news articles containing billions\nof sentences, and searching for them in a given speech. This raises the\nquestion of whether such claims indeed correspond to those made in spoken\nspeeches. To this end, we collected a large dataset of $400$ speeches in\nEnglish discussing $200$ controversial topics, mined claims for each topic, and\nasked annotators to identify the mined claims mentioned in each speech. Results\nshow that in the vast majority of speeches debaters indeed make use of such\nclaims. In addition, we present several baselines for the automatic detection\nof mined claims in speeches, forming the basis for future work. All collected\ndata is freely available for research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 10:19:19 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lavee", "Tamar", ""], ["Orbach", "Matan", ""], ["Kotlerman", "Lili", ""], ["Kantor", "Yoav", ""], ["Gretz", "Shai", ""], ["Dankin", "Lena", ""], ["Mirkin", "Shachar", ""], ["Jacovi", "Michal", ""], ["Bilu", "Yonatan", ""], ["Aharonov", "Ranit", ""], ["Slonim", "Noam", ""]]}, {"id": "1907.11907", "submitter": "Svanhv\\'it Ing\\'olfsd\\'ottir", "authors": "Svanhv\\'it Lilja Ing\\'olfsd\\'ottir, Hrafn Loftsson, J\\'on Fri{\\dh}rik\n  Da{\\dh}ason, Krist\\'in Bjarnad\\'ottir", "title": "Nefnir: A high accuracy lemmatizer for Icelandic", "comments": "Presented at NoDaLiDa 2019, Turku, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lemmatization, finding the basic morphological form of a word in a corpus, is\nan important step in many natural language processing tasks when working with\nmorphologically rich languages. We describe and evaluate Nefnir, a new open\nsource lemmatizer for Icelandic. Nefnir uses suffix substitution rules, derived\nfrom a large morphological database, to lemmatize tagged text. Evaluation shows\nthat for correctly tagged text, Nefnir obtains an accuracy of 99.55%, and for\ntext tagged with a PoS tagger, the accuracy obtained is 96.88%.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 13:30:56 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ing\u00f3lfsd\u00f3ttir", "Svanhv\u00edt Lilja", ""], ["Loftsson", "Hrafn", ""], ["Da\u00f0ason", "J\u00f3n Fri\u00f0rik", ""], ["Bjarnad\u00f3ttir", "Krist\u00edn", ""]]}, {"id": "1907.11932", "submitter": "Zhijing Jin", "authors": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits", "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on\n  Text Classification and Entailment", "comments": "AAAI 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms are often vulnerable to adversarial examples that\nhave imperceptible alterations from the original counterparts but can fool the\nstate-of-the-art models. It is helpful to evaluate or even improve the\nrobustness of these models by exposing the maliciously crafted adversarial\nexamples. In this paper, we present TextFooler, a simple but strong baseline to\ngenerate natural adversarial text. By applying it to two fundamental natural\nlanguage tasks, text classification and textual entailment, we successfully\nattacked three target models, including the powerful pre-trained BERT, and the\nwidely used convolutional and recurrent neural networks. We demonstrate the\nadvantages of this framework in three ways: (1) effective---it outperforms\nstate-of-the-art attacks in terms of success rate and perturbation rate, (2)\nutility-preserving---it preserves semantic content and grammaticality, and\nremains correctly classified by humans, and (3) efficient---it generates\nadversarial text with computational complexity linear to the text length. *The\ncode, pre-trained target models, and test examples are available at\nhttps://github.com/jind11/TextFooler.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 15:07:04 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 05:33:35 GMT"}, {"version": "v3", "created": "Sat, 23 Nov 2019 07:53:08 GMT"}, {"version": "v4", "created": "Thu, 23 Jan 2020 07:16:25 GMT"}, {"version": "v5", "created": "Sun, 5 Apr 2020 07:12:08 GMT"}, {"version": "v6", "created": "Wed, 8 Apr 2020 23:10:10 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Jin", "Di", ""], ["Jin", "Zhijing", ""], ["Zhou", "Joey Tianyi", ""], ["Szolovits", "Peter", ""]]}, {"id": "1907.11983", "submitter": "Xiaodong Liu", "authors": "Pengcheng He, Xiaodong Liu, Weizhu Chen and Jianfeng Gao", "title": "A Hybrid Neural Network Model for Commonsense Reasoning", "comments": "9 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hybrid neural network (HNN) model for commonsense\nreasoning. An HNN consists of two component models, a masked language model and\na semantic similarity model, which share a BERT-based contextual encoder but\nuse different model-specific input and output layers. HNN obtains new\nstate-of-the-art results on three classic commonsense reasoning tasks, pushing\nthe WNLI benchmark to 89%, the Winograd Schema Challenge (WSC) benchmark to\n75.1%, and the PDP60 benchmark to 90.0%. An ablation study shows that language\nmodels and semantic similarity models are complementary approaches to\ncommonsense reasoning, and HNN effectively combines the strengths of both. The\ncode and pre-trained models will be publicly available at\nhttps://github.com/namisan/mt-dnn.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 21:51:52 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["He", "Pengcheng", ""], ["Liu", "Xiaodong", ""], ["Chen", "Weizhu", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1907.12008", "submitter": "Chiung Ching Ho", "authors": "Wei Lun Lim, Chiung Ching Ho, Choo-Yee Ting", "title": "Fusing location and text features for sentiment classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-tagged Twitter data has been used recently to infer insights on the human\naspects of social media. Insights related to demographics, spatial distribution\nof cultural activities, space-time travel trajectories for humans as well as\nhappiness has been mined from geo-tagged twitter data in recent studies. To\ndate, not much study has been done on the impact of the geolocation features of\na Tweet on its sentiment. This observation has inspired us to propose the usage\nof geo-location features as a method to perform sentiment classification. In\nthis method, the sentiment classification of geo-tagged tweets is performed by\nconcatenating geo-location features and one-hot encoded word vectors as inputs\nfor convolutional neural networks (CNN) and long short-term memory (LSTM)\nnetworks. The addition of language-independent features in the form of\ngeo-location features has helped to enrich the tweet representation in order to\ncombat the sparse nature of short tweet message. The results achieved has\ndemonstrated that concatenating geo-location features to one-hot encoded word\nvectors can achieve higher accuracy as compared to the usage of word vectors\nalone for the purpose of sentiment classification.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 03:57:16 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lim", "Wei Lun", ""], ["Ho", "Chiung Ching", ""], ["Ting", "Choo-Yee", ""]]}, {"id": "1907.12009", "submitter": "Jun Gao", "authors": "Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang and Tie-Yan Liu", "title": "Representation Degeneration Problem in Training Natural Language\n  Generation Models", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study an interesting problem in training neural network-based models for\nnatural language generation tasks, which we call the \\emph{representation\ndegeneration problem}. We observe that when training a model for natural\nlanguage generation tasks through likelihood maximization with the weight tying\ntrick, especially with big training datasets, most of the learnt word\nembeddings tend to degenerate and be distributed into a narrow cone, which\nlargely limits the representation power of word embeddings. We analyze the\nconditions and causes of this problem and propose a novel regularization method\nto address it. Experiments on language modeling and machine translation show\nthat our method can largely mitigate the representation degeneration problem\nand achieve better performance than baseline algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 03:57:41 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Gao", "Jun", ""], ["He", "Di", ""], ["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Wang", "Liwei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1907.12021", "submitter": "William Yang Wang", "authors": "Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni,\n  Matthew Turk, William Yang Wang", "title": "What Should I Ask? Using Conversationally Informative Rewards for\n  Goal-Oriented Visual Dialog", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to engage in goal-oriented conversations has allowed humans to\ngain knowledge, reduce uncertainty, and perform tasks more efficiently.\nArtificial agents, however, are still far behind humans in having goal-driven\nconversations. In this work, we focus on the task of goal-oriented visual\ndialogue, aiming to automatically generate a series of questions about an image\nwith a single objective. This task is challenging since these questions must\nnot only be consistent with a strategy to achieve a goal, but also consider the\ncontextual information in the image. We propose an end-to-end goal-oriented\nvisual dialogue system, that combines reinforcement learning with regularized\ninformation gain. Unlike previous approaches that have been proposed for the\ntask, our work is motivated by the Rational Speech Act framework, which models\nthe process of human inquiry to reach a goal. We test the two versions of our\nmodel on the GuessWhat?! dataset, obtaining significant results that outperform\nthe current state-of-the-art models in the task of generating questions to find\nan undisclosed object in an image.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:15:35 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Shukla", "Pushkar", ""], ["Elmadjian", "Carlos", ""], ["Sharan", "Richika", ""], ["Kulkarni", "Vivek", ""], ["Turk", "Matthew", ""], ["Wang", "William Yang", ""]]}, {"id": "1907.12048", "submitter": "Xavier Holt Mr", "authors": "Xavier Holt", "title": "Probabilistic Models of Relational Implication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data in its most basic form is a static collection of known facts.\nHowever, by learning to infer and deduct additional information and structure,\nwe can massively increase the usefulness of the underlying data. One common\nform of inferential reasoning in knowledge bases is implication discovery.\nHere, by learning when one relation implies another, we can extend our\nknowledge representation. There are several existing models for relational\nimplication, however we argue they are motivated but not principled. To this\nend, we define a formal probabilistic model of relational implication. By using\nestimators based on the empirical distribution of our dataset, we demonstrate\nthat our model outperforms existing approaches. While previous work achieves a\nbest score of 0.7812 AUC on an evaluatory dataset, our ProbE model improves\nthis to 0.7915. Furthermore, we demonstrate that our model can be improved\nsubstantially through the use of link prediction models and dense latent\nrepresentations of the underlying argument and relations. This variant, denoted\nProbL, improves the state of the art on our evaluation dataset to 0.8143. In\naddition to developing a new framework and providing novel scores of relational\nimplication, we provide two pragmatic resources to assist future research.\nFirst, we motivate and develop an improved crowd framework for constructing\nlabelled datasets of relational implication. Using this, we reannotate and make\npublic a dataset comprised of 17,848 instances of labelled relational\nimplication. We demonstrate that precision (as evaluated by expert consensus\nwith the crowd labels) on the resulting dataset improves from 53% to 95%.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 08:56:06 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Holt", "Xavier", ""]]}, {"id": "1907.12108", "submitter": "Zhaojiang Lin", "authors": "Zhaojiang Lin, Peng Xu, Genta Indra Winata, Farhad Bin Siddique, Zihan\n  Liu, Jamin Shin, Pascale Fung", "title": "CAiRE: An Empathetic Neural Chatbot", "comments": "Extended version of AAAI-2020 demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end empathetic conversation agent CAiRE.\nOur system adapts TransferTransfo (Wolf et al., 2019) learning approach that\nfine-tunes a large-scale pre-trained language model with multi-task objectives:\nresponse language modeling, response prediction and dialogue emotion detection.\nWe evaluate our model on the recently proposed empathetic-dialogues dataset\n(Rashkin et al., 2019), the experiment results show that CAiRE achieves\nstate-of-the-art performance on dialogue emotion detection and empathetic\nresponse generation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 16:52:09 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 03:19:54 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 08:20:01 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2020 01:45:10 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lin", "Zhaojiang", ""], ["Xu", "Peng", ""], ["Winata", "Genta Indra", ""], ["Siddique", "Farhad Bin", ""], ["Liu", "Zihan", ""], ["Shin", "Jamin", ""], ["Fung", "Pascale", ""]]}, {"id": "1907.12133", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Wei-Lun Chao, Dong Xuan", "title": "An Empirical Study on Leveraging Scene Graphs for Visual Question\n  Answering", "comments": "Accepted as oral presentation at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (Visual QA) has attracted significant attention\nthese years. While a variety of algorithms have been proposed, most of them are\nbuilt upon different combinations of image and language features as well as\nmulti-modal attention and fusion. In this paper, we investigate an alternative\napproach inspired by conventional QA systems that operate on knowledge graphs.\nSpecifically, we investigate the use of scene graphs derived from images for\nVisual QA: an image is abstractly represented by a graph with nodes\ncorresponding to object entities and edges to object relationships. We adapt\nthe recently proposed graph network (GN) to encode the scene graph and perform\nstructured reasoning according to the input question. Our empirical studies\ndemonstrate that scene graphs can already capture essential information of\nimages and graph networks have the potential to outperform state-of-the-art\nVisual QA algorithms but with a much cleaner architecture. By analyzing the\nfeatures generated by GNs we can further interpret the reasoning process,\nsuggesting a promising direction towards explainable Visual QA.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 19:59:20 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Cheng", ""], ["Chao", "Wei-Lun", ""], ["Xuan", "Dong", ""]]}, {"id": "1907.12162", "submitter": "Petr Marek", "authors": "Petr Marek", "title": "Hybrid Code Networks using a convolutional neural network as an input\n  layer achieves higher turn accuracy", "comments": "Proceedings of the International Student Scientific Conference Poster\n  23/2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dialogue management is a task of conversational artificial intelligence.\nThe goal of the dialogue manager is to select the appropriate response to the\nconversational partner conditioned by the input message and recent dialogue\nstate. Hybrid Code Networks is one of the models of dialogue managers, which\nuses an average of word embeddings and bag-of-words as input features. We\nperform experiments on Dialogue bAbI Task 6 and Alquist Conversational Dataset.\nThe experiments show that the convolutional neural network used as an input\nlayer of the Hybrid Code Network improves the model's turn accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 23:41:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Marek", "Petr", ""]]}, {"id": "1907.12280", "submitter": "Gy\\\"orgy G\\\"or\\\"og", "authors": "Gy\\\"orgy G\\\"or\\\"og and P\\'eter Weisz", "title": "Legal entity recognition in an agglutinating language and document\n  connection network for EU Legislation and EU/Hungarian Case Law", "comments": "5 pages, 3 figures, 15 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an application aiming at federated search for EU and\nHungarian legislation and jurisdiction. It now contains above 1 million\ndocuments, with daily updates. The database holds documents downloaded from the\nEU sources EUR-Lex and Curia Online as well as public jurisdiction documents\nfrom the Constitutional Court of Hungary and The National Office for The\nJudiciary. The application is termed Justeus. Justeus provides comprehensible\nsearch possibilities. Besides free text and metadata (dropdown list) searches,\nit features hierarchical data structures (concept hierarchy trees) of directory\ncodes and classification as well as subject terms. Justeus collects all links\nof a particular document to other documents (court judgements citing other case\nlaw documents as well as legislation, national court decisions referring to EU\nregulation etc.) as tables and directed graph networks. Choosing a document,\nits relations to other documents are visualized in real time as a network.\nNetwork graphs help in identifying key documents influencing or referred by\nmany other documents (legislative and/or jurisdictive) and sets of documents\npredominantly referring to each other (citation networks).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:54:02 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["G\u00f6r\u00f6g", "Gy\u00f6rgy", ""], ["Weisz", "P\u00e9ter", ""]]}, {"id": "1907.12293", "submitter": "Yajun Zhou", "authors": "Weinan E and Yajun Zhou", "title": "A mathematical model for universal semantics", "comments": "Main text (12 pages, 7 figures); Software Manual (ii+262 pages, 16\n  figures, 12 tables, available as two ancillary files). Revised according to\n  reviewers' comments", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3022533", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the meaning of words with language-independent numerical\nfingerprints, through a mathematical analysis of recurring patterns in texts.\nApproximating texts by Markov processes on a long-range time scale, we are able\nto extract topics, discover synonyms, and sketch semantic fields from a\nparticular document of moderate length, without consulting external\nknowledge-base or thesaurus. Our Markov semantic model allows us to represent\neach topical concept by a low-dimensional vector, interpretable as algebraic\ninvariants in succinct statistical operations on the document, targeting local\nenvironments of individual words. These language-independent semantic\nrepresentations enable a robot reader to both understand short texts in a given\nlanguage (automated question-answering) and match medium-length texts across\ndifferent languages (automated word translation). Our semantic fingerprints\nquantify local meaning of words in 14 representative languages across 5 major\nlanguage families, suggesting a universal and cost-effective mechanism by which\nhuman languages are processed at the semantic level. Our protocols and source\ncodes are publicly available on\nhttps://github.com/yajun-zhou/linguae-naturalis-principia-mathematica\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 09:25:49 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 02:21:44 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 04:19:43 GMT"}, {"version": "v4", "created": "Sat, 23 Nov 2019 10:09:43 GMT"}, {"version": "v5", "created": "Thu, 16 Jan 2020 11:46:28 GMT"}, {"version": "v6", "created": "Sun, 15 Mar 2020 01:46:54 GMT"}, {"version": "v7", "created": "Sun, 12 Jul 2020 12:59:40 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["E", "Weinan", ""], ["Zhou", "Yajun", ""]]}, {"id": "1907.12316", "submitter": "Eug\\'enio Ribeiro", "authors": "Eug\\'enio Ribeiro, Ricardo Ribeiro, and David Martins de Matos", "title": "Hierarchical Multi-Label Dialog Act Recognition on Spanish Data", "comments": "21 pages, 4 figures, 17 tables, translated version of the article\n  published in Linguam\\'atica 11(1)", "journal-ref": "Linguam\\'atica 11(1) (2019) 17-40", "doi": "10.21814/lm.11.1.278", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog acts reveal the intention behind the uttered words. Thus, their\nautomatic recognition is important for a dialog system trying to understand its\nconversational partner. The study presented in this article approaches that\ntask on the DIHANA corpus, whose three-level dialog act annotation scheme poses\nproblems which have not been explored in recent studies. In addition to the\nhierarchical problem, the two lower levels pose multi-label classification\nproblems. Furthermore, each level in the hierarchy refers to a different aspect\nconcerning the intention of the speaker both in terms of the structure of the\ndialog and the task. Also, since its dialogs are in Spanish, it allows us to\nassess whether the state-of-the-art approaches on English data generalize to a\ndifferent language. More specifically, we compare the performance of different\nsegment representation approaches focusing on both sequences and patterns of\nwords and assess the importance of the dialog history and the relations between\nthe multiple levels of the hierarchy. Concerning the single-label\nclassification problem posed by the top level, we show that the conclusions\ndrawn on English data also hold on Spanish data. Furthermore, we show that the\napproaches can be adapted to multi-label scenarios. Finally, by hierarchically\ncombining the best classifiers for each level, we achieve the best results\nreported for this corpus.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:12:18 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ribeiro", "Eug\u00e9nio", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1907.12412", "submitter": "Yu Sun", "authors": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu,\n  Haifeng Wang", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "comments": "11 pages, 3 figures and 7 tables; Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained models have achieved state-of-the-art results in\nvarious language understanding tasks, which indicates that pre-training on\nlarge-scale corpora may play a crucial role in natural language processing.\nCurrent pre-training procedures usually focus on training the model with\nseveral simple tasks to grasp the co-occurrence of words or sentences. However,\nbesides co-occurring, there exists other valuable lexical, syntactic and\nsemantic information in training corpora, such as named entity, semantic\ncloseness and discourse relations. In order to extract to the fullest extent,\nthe lexical, syntactic and semantic information from training corpora, we\npropose a continual pre-training framework named ERNIE 2.0 which builds and\nlearns incrementally pre-training tasks through constant multi-task learning.\nExperimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on\n16 tasks including English tasks on GLUE benchmarks and several common tasks in\nChinese. The source codes and pre-trained models have been released at\nhttps://github.com/PaddlePaddle/ERNIE.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:25:37 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 02:09:16 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Sun", "Yu", ""], ["Wang", "Shuohuan", ""], ["Li", "Yukun", ""], ["Feng", "Shikun", ""], ["Tian", "Hao", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "1907.12413", "submitter": "Fabian Sperrle", "authors": "Fabian Sperrle, Rita Sevastjanova, Rebecca Kehlbeck and Mennatallah\n  El-Assady", "title": "VIANA: Visual Interactive Annotation of Argumentation", "comments": "Proceedings of IEEE Conference on Visual Analytics Science and\n  Technology (VAST), 2019", "journal-ref": "2019 IEEE Conference on Visual Analytics Science and Technology\n  (VAST)", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation Mining addresses the challenging tasks of identifying\nboundaries of argumentative text fragments and extracting their relationships.\nFully automated solutions do not reach satisfactory accuracy due to their\ninsufficient incorporation of semantics and domain knowledge. Therefore,\nexperts currently rely on time-consuming manual annotations. In this paper, we\npresent a visual analytics system that augments the manual annotation process\nby automatically suggesting which text fragments to annotate next. The accuracy\nof those suggestions is improved over time by incorporating linguistic\nknowledge and language modeling to learn a measure of argument similarity from\nuser interactions. Based on a long-term collaboration with domain experts, we\nidentify and model five high-level analysis tasks. We enable close reading and\nnote-taking, annotation of arguments, argument reconstruction, extraction of\nargument relations, and exploration of argument graphs. To avoid context\nswitches, we transition between all views through seamless morphing, visually\nanchoring all text- and graph-based layers. We evaluate our system with a\ntwo-stage expert user study based on a corpus of presidential debates. The\nresults show that experts prefer our system over existing solutions due to the\nspeedup provided by the automatic suggestions and the tight integration between\ntext and graph views.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:26:03 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Sperrle", "Fabian", ""], ["Sevastjanova", "Rita", ""], ["Kehlbeck", "Rebecca", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1907.12437", "submitter": "Jerin Philip", "authors": "Jerin Philip, Vinay P. Namboodiri, C.V. Jawahar", "title": "A Baseline Neural Machine Translation System for Indian Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, yet effective, Neural Machine Translation system for\nIndian languages. We demonstrate the feasibility for multiple language pairs,\nand establish a strong baseline for further research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:59:02 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Philip", "Jerin", ""], ["Namboodiri", "Vinay P.", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1907.12461", "submitter": "Sascha Rothe", "authors": "Sascha Rothe, Shashi Narayan, Aliaksei Severyn", "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks", "comments": "To be published in Transactions of the Association for Computational\n  Linguistics (TACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pre-training of large neural models has recently revolutionized\nNatural Language Processing. By warm-starting from the publicly released\ncheckpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus\nhas been mainly on the Natural Language Understanding tasks. In this paper, we\ndemonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible\nwith publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and\nconducted an extensive empirical study on the utility of initializing our\nmodel, both encoder and decoder, with these checkpoints. Our models result in\nnew state-of-the-art results on Machine Translation, Text Summarization,\nSentence Splitting, and Sentence Fusion.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 14:42:30 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 13:29:28 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Rothe", "Sascha", ""], ["Narayan", "Shashi", ""], ["Severyn", "Aliaksei", ""]]}, {"id": "1907.12477", "submitter": "Robert Tjarko Lange", "authors": "Robert Tjarko Lange, Aldo Faisal", "title": "Semantic RL with Action Grammars: Data-Efficient Learning of\n  Hierarchical Task Abstractions", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Reinforcement Learning algorithms have successfully been applied\nto temporal credit assignment problems with sparse reward signals. However,\nstate-of-the-art algorithms require manual specification of sub-task\nstructures, a sample inefficient exploration phase or lack semantic\ninterpretability. Humans, on the other hand, efficiently detect hierarchical\nsub-structures induced by their surroundings. It has been argued that this\ninference process universally applies to language, logical reasoning as well as\nmotor control. Therefore, we propose a cognitive-inspired Reinforcement\nLearning architecture which uses grammar induction to identify sub-goal\npolicies. By treating an on-policy trajectory as a sentence sampled from the\npolicy-conditioned language of the environment, we identify hierarchical\nconstituents with the help of unsupervised grammatical inference. The resulting\nset of temporal abstractions is called action grammar (Pastra & Aloimonos,\n2012) and unifies symbolic and connectionist approaches to Reinforcement\nLearning. It can be used to facilitate efficient imitation, transfer and online\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:27:50 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:26:35 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Lange", "Robert Tjarko", ""], ["Faisal", "Aldo", ""]]}, {"id": "1907.12484", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, Jasmijn Bastings, Stefan Riezler", "title": "Joey NMT: A Minimalist NMT Toolkit for Novices", "comments": null, "journal-ref": "EMNLP-IJCNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Joey NMT, a minimalist neural machine translation toolkit based on\nPyTorch that is specifically designed for novices. Joey NMT provides many\npopular NMT features in a small and simple code base, so that novices can\neasily and quickly learn to use it and adapt it to their needs. Despite its\nfocus on simplicity, Joey NMT supports classic architectures (RNNs,\ntransformers), fast beam search, weight tying, and more, and achieves\nperformance comparable to more complex toolkits on standard benchmarks. We\nevaluate the accessibility of our toolkit in a user study where novices with\ngeneral knowledge about Pytorch and NMT and experts work through a\nself-contained Joey NMT tutorial, showing that novices perform almost as well\nas experts in a subsequent code quiz. Joey NMT is available at\nhttps://github.com/joeynmt/joeynmt .\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:35:13 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 14:49:02 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 03:51:37 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kreutzer", "Julia", ""], ["Bastings", "Jasmijn", ""], ["Riezler", "Stefan", ""]]}, {"id": "1907.12524", "submitter": "Juntao Yu", "authors": "Juntao Yu, Bernd Bohnet, Massimo Poesio", "title": "Neural Mention Detection", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mention detection is an important preprocessing step for annotation and\ninterpretation in applications such as NER and coreference resolution, but few\nstand-alone neural models have been proposed able to handle the full range of\nmentions. In this work, we propose and compare three neural network-based\napproaches to mention detection. The first approach is based on the mention\ndetection part of a state of the art coreference resolution system; the second\nuses ELMO embeddings together with a bidirectional LSTM and a biaffine\nclassifier; the third approach uses the recently introduced BERT model. Our\nbest model (using a biaffine classifier) achieves gains of up to 1.8 percentage\npoints on mention recall when compared with a strong baseline in a HIGH RECALL\ncoreference annotation setting. The same model achieves improvements of up to\n5.3 and 6.2 p.p. when compared with the best-reported mention detection F1 on\nthe CONLL and CRAC coreference data sets respectively in a HIGH F1 annotation\nsetting. We then evaluate our models for coreference resolution by using\nmentions predicted by our best model in start-of-the-art coreference systems.\nThe enhanced model achieved absolute improvements of up to 1.7 and 0.7 p.p.\nwhen compared with our strong baseline systems (pipeline system and end-to-end\nsystem) respectively. For nested NER, the evaluation of our model on the GENIA\ncorpora shows that our model matches or outperforms state-of-the-art models\ndespite not being specifically designed for this task.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 16:59:53 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:39:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yu", "Juntao", ""], ["Bohnet", "Bernd", ""], ["Poesio", "Massimo", ""]]}, {"id": "1907.12664", "submitter": "Dominik Mach\\'a\\v{c}ek", "authors": "Ivana Kvapil\\'ikov\\'a, Dominik Mach\\'a\\v{c}ek, Ond\\v{r}ej Bojar", "title": "CUNI Systems for the Unsupervised News Translation Task in WMT 2019", "comments": "WMT19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the CUNI translation system used for the\nunsupervised news shared task of the ACL 2019 Fourth Conference on Machine\nTranslation (WMT19). We follow the strategy of Artexte et al. (2018b), creating\na seed phrase-based system where the phrase table is initialized from\ncross-lingual embedding mappings trained on monolingual data, followed by a\nneural machine translation system trained on synthetic parallel data. The\nsynthetic corpus was produced from a monolingual corpus by a tuned PBMT model\nrefined through iterative back-translation. We further focus on the handling of\nnamed entities, i.e. the part of vocabulary where the cross-lingual embedding\nmapping suffers most. Our system reaches a BLEU score of 15.3 on the\nGerman-Czech WMT19 shared task.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:44:50 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kvapil\u00edkov\u00e1", "Ivana", ""], ["Mach\u00e1\u010dek", "Dominik", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1907.12667", "submitter": "Boyuan Pan", "authors": "Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun", "title": "Reinforced Dynamic Reasoning for Conversational Question Generation", "comments": "Accepted in ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a new task named Conversational Question Generation\n(CQG) which is to generate a question based on a passage and a conversation\nhistory (i.e., previous turns of question-answer pairs). CQG is a crucial task\nfor developing intelligent agents that can drive question-answering style\nconversations or test user understanding of a given passage. Towards that end,\nwe propose a new approach named Reinforced Dynamic Reasoning (ReDR) network,\nwhich is based on the general encoder-decoder framework but incorporates a\nreasoning procedure in a dynamic manner to better understand what has been\nasked and what to ask next about the passage. To encourage producing meaningful\nquestions, we leverage a popular question answering (QA) model to provide\nfeedback and fine-tune the question generator using a reinforcement learning\nmechanism. Empirical results on the recently released CoQA dataset demonstrate\nthe effectiveness of our method in comparison with various baselines and model\nvariants. Moreover, to show the applicability of our method, we also apply it\nto create multi-turn question-answering conversations for passages in SQuAD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:56:35 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pan", "Boyuan", ""], ["Li", "Hao", ""], ["Yao", "Ziyu", ""], ["Cai", "Deng", ""], ["Sun", "Huan", ""]]}, {"id": "1907.12674", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov, Erik Velldal, Lilja {\\O}vrelid", "title": "One-to-X analogical reasoning on word embeddings: a case for diachronic\n  armed conflict prediction from news texts", "comments": "1st International Workshop on Computational Approaches to Historical\n  Language Change (ACL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We extend the well-known word analogy task to a one-to-X formulation,\nincluding one-to-none cases, when no correct answer exists. The task is cast as\na relation discovery problem and applied to historical armed conflicts\ndatasets, attempting to predict new relations of type `location:armed-group'\nbased on data about past events. As the source of semantic information, we use\ndiachronic word embedding models trained on English news texts. A simple\ntechnique to improve diachronic performance in such task is demonstrated, using\na threshold based on a function of cosine distance to decrease the number of\nfalse positives; this approach is shown to be beneficial on two different\ncorpora. Finally, we publish a ready-to-use test set for one-to-X analogy\nevaluation on historical armed conflicts data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 22:26:51 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Velldal", "Erik", ""], ["\u00d8vrelid", "Lilja", ""]]}, {"id": "1907.12679", "submitter": "Mamoru Komachi", "authors": "Hiroki Shimanaka, Tomoyuki Kajiwara, Mamoru Komachi", "title": "Machine Translation Evaluation with BERT Regressor", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the metric using BERT (Bidirectional Encoder Representations\nfrom Transformers) (Devlin et al., 2019) for automatic machine translation\nevaluation. The experimental results of the WMT-2017 Metrics Shared Task\ndataset show that our metric achieves state-of-the-art performance in\nsegment-level metrics task for all to-English language pairs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 22:53:59 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Shimanaka", "Hiroki", ""], ["Kajiwara", "Tomoyuki", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1907.12697", "submitter": "Feng Wei", "authors": "Feng Wei, Uyen Trang Nguyen, Hui Jiang", "title": "Dual-FOFE-net Neural Models for Entity Linking with PageRank", "comments": "ICANN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple and computationally efficient approach for\nentity linking (EL), compared with recurrent neural networks (RNNs) or\nconvolutional neural networks (CNNs), by making use of feedforward neural\nnetworks (FFNNs) and the recent dual fixed-size ordinally forgetting encoding\n(dual-FOFE) method to fully encode the sentence fragment and its left/right\ncontexts into a fixed-size representation. Furthermore, in this work, we\npropose to incorporate PageRank based distillation in our candidate generation\nmodule. Our neural linking models consist of three parts: a PageRank based\ncandidate generation module, a dual-FOFE-net neural ranking model and a simple\nNIL entity clustering system. Experimental results have shown that our proposed\nneural linking models achieved higher EL accuracy than state-of-the-art models\non the TAC2016 task dataset over the baseline system, without requiring any\nin-house data or complicated handcrafted features. Moreover, it achieves a\ncompetitive accuracy on the TAC2017 task dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 01:37:34 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Wei", "Feng", ""], ["Nguyen", "Uyen Trang", ""], ["Jiang", "Hui", ""]]}, {"id": "1907.12750", "submitter": "Dominik Mach\\'a\\v{c}ek", "authors": "Martin Popel, Dominik Mach\\'a\\v{c}ek, Michal Auersperger, Ond\\v{r}ej\n  Bojar and Pavel Pecina", "title": "English-Czech Systems in WMT19: Document-Level Transformer", "comments": "WMT19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our NMT systems submitted to the WMT19 shared task in\nEnglish-Czech news translation. Our systems are based on the Transformer model\nimplemented in either Tensor2Tensor (T2T) or Marian framework.\n  We aimed at improving the adequacy and coherence of translated documents by\nenlarging the context of the source and target. Instead of translating each\nsentence independently, we split the document into possibly overlapping\nmulti-sentence segments. In case of the T2T implementation, this\n\"document-level\"-trained system achieves a $+0.6$ BLEU improvement ($p<0.05$)\nrelative to the same system applied on isolated sentences. To assess the\npotential effect document-level models might have on lexical coherence, we\nperformed a semi-automatic analysis, which revealed only a few sentences\nimproved in this aspect. Thus, we cannot draw any conclusions from this weak\nevidence.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 06:17:19 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Popel", "Martin", ""], ["Mach\u00e1\u010dek", "Dominik", ""], ["Auersperger", "Michal", ""], ["Bojar", "Ond\u0159ej", ""], ["Pecina", "Pavel", ""]]}, {"id": "1907.12763", "submitter": "Victor Escorcia", "authors": "Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, Bryan\n  Russell", "title": "Temporal Localization of Moments in Video Collections with Natural\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce the task of retrieving relevant video moments\nfrom a large corpus of untrimmed, unsegmented videos given a natural language\nquery. Our task poses unique challenges as a system must efficiently identify\nboth the relevant videos and localize the relevant moments in the videos. This\ntask is in contrast to prior work that localizes relevant moments in a single\nvideo or searches a large collection of already-segmented videos. For our task,\nwe introduce Clip Alignment with Language (CAL), a model that aligns features\nfor a natural language query to a sequence of short video clips that compose a\ncandidate moment in a video. Our approach goes beyond prior work that\naggregates video features over a candidate moment by allowing for finer clip\nalignment. Moreover, our approach is amenable to efficient indexing of the\nresulting clip-level representations, which makes it suitable for moment\nlocalization in large video collections. We evaluate our approach on three\nrecently proposed datasets for temporal localization of moments in video with\nnatural language extended to our video corpus moment retrieval setting: DiDeMo,\nCharades-STA, and ActivityNet-captions. We show that our CAL model outperforms\nthe recently proposed Moment Context Network (MCN) on all criteria across all\ndatasets on our proposed task, obtaining an 8%-85% and 11%-47% boost for\naverage recall and median rank, respectively, and achieves 5x faster retrieval\nand 8x smaller index size with a 500K video corpus.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 07:31:02 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Escorcia", "Victor", ""], ["Soldan", "Mattia", ""], ["Sivic", "Josef", ""], ["Ghanem", "Bernard", ""], ["Russell", "Bryan", ""]]}, {"id": "1907.12801", "submitter": "Wenliang Chen", "authors": "Haitao Wang, Zhengqiu He, Jin Ma, Wenliang Chen, Min Zhang", "title": "IPRE: a Dataset for Inter-Personal Relationship Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-personal relationship is the basis of human society. In order to\nautomatically identify the relations between persons from texts, we need\nannotated data for training systems. However, there is a lack of a massive\namount of such data so far. To address this situation, we introduce IPRE, a new\ndataset for inter-personal relationship extraction which aims to facilitate\ninformation extraction and knowledge graph construction research. In total,\nIPRE has over 41,000 labeled sentences for 34 types of relations, including\nabout 9,000 sentences annotated by workers. Our data is the first dataset for\ninter-personal relationship extraction. Additionally, we define three\nevaluation tasks based on IPRE and provide the baseline systems for further\ncomparison in future work.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 09:33:13 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 14:01:22 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wang", "Haitao", ""], ["He", "Zhengqiu", ""], ["Ma", "Jin", ""], ["Chen", "Wenliang", ""], ["Zhang", "Min", ""]]}, {"id": "1907.12850", "submitter": "Jongho Im", "authors": "Jongho Im, Taikgun Song, Youngsu Lee, Jewoo Kim", "title": "Confirmatory Aspect-based Opinion Mining Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A new opinion extraction method is proposed to summarize unstructured,\nuser-generated content (i.e., online customer reviews) in the fixed topic\ndomains. To differentiate the current approach from other opinion extraction\napproaches, which are often exposed to a sparsity problem and lack of sentiment\nscores, a confirmatory aspect-based opinion mining framework is introduced\nalong with its practical algorithm called DiSSBUS. In this procedure, 1) each\ncustomer review is disintegrated into a set of clauses; 2) each clause is\nsummarized to bi-terms-a topic word and an evaluation word-using a\npart-of-speech (POS) tagger; and 3) each bi-term is matched to a pre-specified\ntopic relevant to a specific domain. The proposed processes have two primary\nadvantages over existing methods: 1) they can decompose a single review into a\nset of bi-terms related to pre-specified topics in the domain of interest and,\ntherefore, 2) allow identification of the reviewer's opinions on the topics via\nevaluation words within the set of bi-terms. The proposed aspect-based opinion\nmining is applied to customer reviews of restaurants in Hawaii obtained from\nTripAdvisor, and the empirical findings validate the effectiveness of the\nmethod.\n  Keywords: Clause-based sentiment analysis, Customer review, Opinion mining,\nTopic modeling, User-generate-contents.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:00:03 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Im", "Jongho", ""], ["Song", "Taikgun", ""], ["Lee", "Youngsu", ""], ["Kim", "Jewoo", ""]]}, {"id": "1907.12878", "submitter": "Basma El Amel Boussaha", "authors": "Basma El Amel Boussaha, Nicolas Hernandez, Christine Jacquin, Emmanuel\n  Morin", "title": "Deep Retrieval-Based Dialogue Systems: A Short Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building dialogue systems that naturally converse with humans is being an\nattractive and an active research domain. Multiple systems are being designed\neveryday and several datasets are being available. For this reason, it is being\nhard to keep an up-to-date state-of-the-art. In this work, we present the\nlatest and most relevant retrieval-based dialogue systems and the available\ndatasets used to build and evaluate them. We discuss their limitations and\nprovide insights and guidelines for future work.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:16:18 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Boussaha", "Basma El Amel", ""], ["Hernandez", "Nicolas", ""], ["Jacquin", "Christine", ""], ["Morin", "Emmanuel", ""]]}, {"id": "1907.12885", "submitter": "Murahtan Kurfal{\\i}", "authors": "Murathan Kurfal{\\i} and Robert \\\"Ostling", "title": "Zero-shot transfer for implicit discourse relation classification", "comments": "to be presented at SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically classifying the relation between sentences in a discourse is a\nchallenging task, in particular when there is no overt expression of the\nrelation. It becomes even more challenging by the fact that annotated training\ndata exists only for a small number of languages, such as English and Chinese.\nWe present a new system using zero-shot transfer learning for implicit\ndiscourse relation classification, where the only resource used for the target\nlanguage is unannotated parallel text. This system is evaluated on the\ndiscourse-annotated TED-MDB parallel corpus, where it obtains good results for\nall seven languages using only English training data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:23:01 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kurfal\u0131", "Murathan", ""], ["\u00d6stling", "Robert", ""]]}, {"id": "1907.12894", "submitter": "Yang Gao", "authors": "Yang Gao, Christian M. Meyer, Mohsen Mesgar, Iryna Gurevych", "title": "Reward Learning for Efficient Reinforcement Learning in Extractive\n  Document Summarisation", "comments": "Accepted to IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document summarisation can be formulated as a sequential decision-making\nproblem, which can be solved by Reinforcement Learning (RL) algorithms. The\npredominant RL paradigm for summarisation learns a cross-input policy, which\nrequires considerable time, data and parameter tuning due to the huge search\nspaces and the delayed rewards. Learning input-specific RL policies is a more\nefficient alternative but so far depends on handcrafted rewards, which are\ndifficult to design and yield poor performance. We propose RELIS, a novel RL\nparadigm that learns a reward function with Learning-to-Rank (L2R) algorithms\nat training time and uses this reward function to train an input-specific RL\npolicy at test time. We prove that RELIS guarantees to generate near-optimal\nsummaries with appropriate L2R and RL algorithms. Empirically, we evaluate our\napproach on extractive multi-document summarisation. We show that RELIS reduces\nthe training time by two orders of magnitude compared to the state-of-the-art\nmodels while performing on par with them.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:31:07 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Gao", "Yang", ""], ["Meyer", "Christian M.", ""], ["Mesgar", "Mohsen", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1907.12895", "submitter": "Marcely Zanon Boito", "authors": "Marcely Zanon Boito, William N. Havard, Mahault Garnerin, \\'Eric Le\n  Ferrand, Laurent Besacier", "title": "MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken\n  Utterances Extracted from the Bible", "comments": "Accepted to LREC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\npublished multilingual speech dataset based on recorded readings of the New\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\nthat the source content (the Bible) is the same for all the languages is not\nexploited to date.Therefore, this article proposes to add multilingual links\nbetween speech segments in different languages, and shares a large and clean\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\nalignment as well as on translation for typologically different language pairs.\nThe quality of the final corpus is attested by human evaluation performed on a\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\nusefulness of the final product on a bilingual speech retrieval task.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:31:49 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 12:36:33 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 14:00:13 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Boito", "Marcely Zanon", ""], ["Havard", "William N.", ""], ["Garnerin", "Mahault", ""], ["Ferrand", "\u00c9ric Le", ""], ["Besacier", "Laurent", ""]]}, {"id": "1907.12951", "submitter": "Nikola Nikolov", "authors": "Nikola I. Nikolov and Richard H.R. Hahnloser", "title": "Abstractive Document Summarization without Parallel Data", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization typically relies on large collections of paired\narticles and summaries. However, in many cases, parallel data is scarce and\ncostly to obtain. We develop an abstractive summarization system that relies\nonly on large collections of example summaries and non-matching articles. Our\napproach consists of an unsupervised sentence extractor that selects salient\nsentences to include in the final summary, as well as a sentence abstractor\nthat is trained on pseudo-parallel and synthetic data, that paraphrases each of\nthe extracted sentences. We perform an extensive evaluation of our method: on\nthe CNN/DailyMail benchmark, on which we compare our approach to fully\nsupervised baselines, as well as on the novel task of automatically generating\na press release from a scientific journal article, which is well suited for our\nsystem. We show promising performance on both tasks, without relying on any\narticle-summary pairs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 14:00:03 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 15:50:39 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Nikolov", "Nikola I.", ""], ["Hahnloser", "Richard H. R.", ""]]}, {"id": "1907.12984", "submitter": "Hao Xiong", "authors": "Hao Xiong, Ruiqing Zhang, Chuanqiang Zhang, Zhongjun He, Hua Wu and\n  Haifeng Wang", "title": "DuTongChuan: Context-aware Translation Model for Simultaneous\n  Interpreting", "comments": "Description for Baidu's Simultaneous Interpreting System", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present DuTongChuan, a novel context-aware translation\nmodel for simultaneous interpreting. This model allows to constantly read\nstreaming text from the Automatic Speech Recognition (ASR) model and\nsimultaneously determine the boundaries of Information Units (IUs) one after\nanother. The detected IU is then translated into a fluent translation with two\nsimple yet effective decoding strategies: partial decoding and context-aware\ndecoding. In practice, by controlling the granularity of IUs and the size of\nthe context, we can get a good trade-off between latency and translation\nquality easily. Elaborate evaluation from human translators reveals that our\nsystem achieves promising translation quality (85.71% for Chinese-English, and\n86.36% for English-Chinese), specially in the sense of surprisingly good\ndiscourse coherence. According to an End-to-End (speech-to-speech simultaneous\ninterpreting) evaluation, this model presents impressive performance in\nreducing latency (to less than 3 seconds at most times). Furthermore, we\nsuccessfully deploy this model in a variety of Baidu's products which have\nhundreds of millions of users, and we release it as a service in our AI\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 14:35:06 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 08:32:08 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Xiong", "Hao", ""], ["Zhang", "Ruiqing", ""], ["Zhang", "Chuanqiang", ""], ["He", "Zhongjun", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "1907.13121", "submitter": "Tom Sercu", "authors": "Tom Sercu, Neil Mallinar", "title": "Multi-Frame Cross-Entropy Training for Convolutional Neural Networks in\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Multi-Frame Cross-Entropy training (MFCE) for convolutional\nneural network acoustic models. Recognizing that similar to RNNs, CNNs are in\nnature sequence models that take variable length inputs, we propose to take as\ninput to the CNN a part of an utterance long enough that multiple labels are\npredicted at once, therefore getting cross-entropy loss signal from multiple\nadjacent frames. This increases the amount of label information drastically for\nsmall marginal computational cost. We show large WER improvements on hub5 and\nrt02 after training on the 2000-hour Switchboard benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 19:56:46 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Sercu", "Tom", ""], ["Mallinar", "Neil", ""]]}, {"id": "1907.13237", "submitter": "Sajawel Ahmed", "authors": "Manuel Stoeckel, Sajawel Ahmed, Alexander Mehler", "title": "SenseFitting: Sense Level Semantic Specialization of Word Embeddings for\n  Word Sense Disambiguation", "comments": "Sketch for LREC 2020 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a neural network-based system of Word Sense Disambiguation (WSD)\nfor German that is based on SenseFitting, a novel method for optimizing WSD. We\noutperform knowledge-based WSD methods by up to 25% F1-score and produce a new\nstate-of-the-art on the German sense-annotated dataset WebCAGe. Our method uses\nthree feature vectors consisting of a) sense, b) gloss, and c) relational\nvectors to represent target senses and to compare them with the vector\ncentroids of sample contexts. Utilizing widely available word embeddings and\nlexical resources, we are able to compensate for the lower resource\navailability of German. SenseFitting builds upon the recently introduced\nsemantic specialization procedure Attract-Repel, and leverages sense level\nsemantic constraints from lexical-semantic networks (e.g. GermaNet) or online\nsocial dictionaries (e.g. Wiktionary) to produce high-quality sense embeddings\nfrom pre-trained word embeddings. We evaluate our sense embeddings with a new\nSimLex-999 based similarity dataset, called SimSense, that we developed for\nthis work. We achieve results that outperform current lemma-based\nspecialization methods for German, making them comparable to results achieved\nfor English.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:38:16 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Stoeckel", "Manuel", ""], ["Ahmed", "Sajawel", ""], ["Mehler", "Alexander", ""]]}, {"id": "1907.13280", "submitter": "Guan-Lin Chao", "authors": "Guan-Lin Chao, Abhinav Rastogi, Semih Yavuz, Dilek Hakkani-T\\\"ur,\n  Jindong Chen, Ian Lane", "title": "Learning Question-Guided Video Representation for Multi-Turn Video\n  Question Answering", "comments": "Accepted at SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and conversing about dynamic scenes is one of the key\ncapabilities of AI agents that navigate the environment and convey useful\ninformation to humans. Video question answering is a specific scenario of such\nAI-human interaction where an agent generates a natural language response to a\nquestion regarding the video of a dynamic scene. Incorporating features from\nmultiple modalities, which often provide supplementary information, is one of\nthe challenging aspects of video question answering. Furthermore, a question\noften concerns only a small segment of the video, hence encoding the entire\nvideo sequence using a recurrent neural network is not computationally\nefficient. Our proposed question-guided video representation module efficiently\ngenerates the token-level video summary guided by each word in the question.\nThe learned representations are then fused with the question to generate the\nanswer. Through empirical evaluation on the Audio Visual Scene-aware Dialog\n(AVSD) dataset, our proposed models in single-turn and multi-turn question\nanswering achieve state-of-the-art performance on several automatic natural\nlanguage generation evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 01:37:58 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chao", "Guan-Lin", ""], ["Rastogi", "Abhinav", ""], ["Yavuz", "Semih", ""], ["Hakkani-T\u00fcr", "Dilek", ""], ["Chen", "Jindong", ""], ["Lane", "Ian", ""]]}, {"id": "1907.13295", "submitter": "Sahisnu Mazumder", "authors": "Sahisnu Mazumder, Bing Liu, Shuai Wang, Nianzu Ma", "title": "Lifelong and Interactive Learning of Factual Knowledge in Dialogues", "comments": "Published in SIGDIAL 2019", "journal-ref": null, "doi": "10.18653/v1/W19-5903", "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue systems are increasingly using knowledge bases (KBs) storing\nreal-world facts to help generate quality responses. However, as the KBs are\ninherently incomplete and remain fixed during conversation, it limits dialogue\nsystems' ability to answer questions and to handle questions involving entities\nor relations that are not in the KB. In this paper, we make an attempt to\npropose an engine for Continuous and Interactive Learning of Knowledge (CILK)\nfor dialogue systems to give them the ability to continuously and interactively\nlearn and infer new knowledge during conversations. With more knowledge\naccumulated over time, they will be able to learn better and answer more\nquestions. Our empirical evaluation shows that CILK is promising.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 03:11:33 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 02:25:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mazumder", "Sahisnu", ""], ["Liu", "Bing", ""], ["Wang", "Shuai", ""], ["Ma", "Nianzu", ""]]}, {"id": "1907.13337", "submitter": "Jiawei Zhou", "authors": "Jiawei Zhou and Alexander M. Rush", "title": "Simple Unsupervised Summarization by Contextual Matching", "comments": null, "journal-ref": "Proceedings of the 57th Annual Meeting of the Association for\n  Computational Linguistics, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an unsupervised method for sentence summarization using only\nlanguage modeling. The approach employs two language models, one that is\ngeneric (i.e. pretrained), and the other that is specific to the target domain.\nWe show that by using a product-of-experts criteria these are enough for\nmaintaining continuous contextual matching while maintaining output fluency.\nExperiments on both abstractive and extractive sentence summarization data sets\nshow promising results of our method without being exposed to any paired data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:11:59 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Zhou", "Jiawei", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1907.13356", "submitter": "Sainik Mahata", "authors": "Avishek Garain, Sainik Kumar Mahata, Subhabrata Dutta", "title": "Normalyzing Numeronyms -- A NLP approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to apply Natural Language Processing for\nnormalizing numeronyms to make them understandable by humans. We approach the\nproblem through a two-step mechanism. We make use of the state of the art\nLevenshtein distance of words. We then apply Cosine Similarity for selection of\nthe normalized text and reach greater accuracy in solving the problem. Our\napproach garners accuracy figures of 71\\% and 72\\% for Bengali and English\nlanguage, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:53:57 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 08:56:48 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Garain", "Avishek", ""], ["Mahata", "Sainik Kumar", ""], ["Dutta", "Subhabrata", ""]]}, {"id": "1907.13362", "submitter": "Johnny Wei", "authors": "Johnny Tian-Zheng Wei", "title": "On conducting better validation studies of automatic metrics in natural\n  language generation evaluation", "comments": "Rejected from the NAACL Workshop on Methods for Optimizing and\n  Evaluating Neural Language Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language generation (NLG) has received increasing attention, which\nhas highlighted evaluation as a central methodological concern. Since human\nevaluations for these systems are costly, automatic metrics have broad appeal\nin NLG. Research in language generation often finds situations where it is\nappropriate to apply existing metrics or propose new ones. The application of\nthese metrics are entirely dependent on validation studies - studies that\ndetermine a metric's correlation to human judgment. However, there are many\ndetails and considerations in conducting strong validation studies. This\ndocument is intended for those validating existing metrics or proposing new\nones in the broad context of NLG: we 1) begin with a write-up of best practices\nin validation studies, 2) outline how to adopt these practices, 3) conduct\nanalyses in the WMT'17 metrics shared task\\footnote{Our jupyter notebook\ncontaining the analyses is available at \\url{https://github.com}}, and 4)\nhighlight promising approaches to NLG metrics 5) conclude with our opinions on\nthe future of this area.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 08:40:26 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Wei", "Johnny Tian-Zheng", ""]]}, {"id": "1907.13511", "submitter": "Joel Shor", "authors": "Joel Shor, Dotan Emanuel, Oran Lang, Omry Tuval, Michael Brenner,\n  Julie Cattiau, Fernando Vieira, Maeve McNally, Taylor Charbonneau, Melissa\n  Nollstadt, Avinatan Hassidim, Yossi Matias", "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data", "comments": "5 pages", "journal-ref": null, "doi": "10.21437/Interspeech.2019-1427", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) systems have dramatically improved over\nthe last few years. ASR systems are most often trained from 'typical' speech,\nwhich means that underrepresented groups don't experience the same level of\nimprovement. In this paper, we present and evaluate finetuning techniques to\nimprove ASR for users with non-standard speech. We focus on two types of\nnon-standard speech: speech from people with amyotrophic lateral sclerosis\n(ALS) and accented speech. We train personalized models that achieve 62% and\n35% relative WER improvement on these two groups, bringing the absolute WER for\nALS speakers, on a test set of message bank phrases, down to 10% for mild\ndysarthria and 20% for more serious dysarthria. We show that 71% of the\nimprovement comes from only 5 minutes of training data. Finetuning a particular\nsubset of layers (with many fewer parameters) often gives better results than\nfinetuning the entire model. This is the first step towards building state of\nthe art ASR models for dysarthric speech.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:07:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shor", "Joel", ""], ["Emanuel", "Dotan", ""], ["Lang", "Oran", ""], ["Tuval", "Omry", ""], ["Brenner", "Michael", ""], ["Cattiau", "Julie", ""], ["Vieira", "Fernando", ""], ["McNally", "Maeve", ""], ["Charbonneau", "Taylor", ""], ["Nollstadt", "Melissa", ""], ["Hassidim", "Avinatan", ""], ["Matias", "Yossi", ""]]}, {"id": "1907.13528", "submitter": "Allyson Ettinger", "authors": "Allyson Ettinger", "title": "What BERT is not: Lessons from a new suite of psycholinguistic\n  diagnostics for language models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training by language modeling has become a popular and successful\napproach to NLP tasks, but we have yet to understand exactly what linguistic\ncapacities these pre-training processes confer upon models. In this paper we\nintroduce a suite of diagnostics drawn from human language experiments, which\nallow us to ask targeted questions about the information used by language\nmodels for generating predictions in context. As a case study, we apply these\ndiagnostics to the popular BERT model, finding that it can generally\ndistinguish good from bad completions involving shared category or role\nreversal, albeit with less sensitivity than humans, and it robustly retrieves\nnoun hypernyms, but it struggles with challenging inferences and role-based\nevent prediction -- and in particular, it shows clear insensitivity to the\ncontextual impacts of negation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:37:32 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 15:21:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ettinger", "Allyson", ""]]}]