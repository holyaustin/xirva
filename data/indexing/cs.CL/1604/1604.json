[{"id": "1604.00077", "submitter": "Sheng-Syun Shen", "authors": "Sheng-syun Shen, Hung-yi Lee", "title": "Neural Attention Models for Sequence Classification: Analysis and\n  Application to Key Term Extraction and Dialogue Act Detection", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network architectures combining with attention mechanism, or\nneural attention model, have shown promising performance recently for the tasks\nincluding speech recognition, image caption generation, visual question\nanswering and machine translation. In this paper, neural attention model is\napplied on two sequence classification tasks, dialogue act detection and key\nterm extraction. In the sequence labeling tasks, the model input is a sequence,\nand the output is the label of the input sequence. The major difficulty of\nsequence labeling is that when the input sequence is long, it can include many\nnoisy or irrelevant part. If the information in the whole sequence is treated\nequally, the noisy or irrelevant part may degrade the classification\nperformance. The attention mechanism is helpful for sequence classification\ntask because it is capable of highlighting important part among the entire\nsequence for the classification task. The experimental results show that with\nthe attention mechanism, discernible improvements were achieved in the sequence\nlabeling task considered here. The roles of the attention mechanism in the\ntasks are further analyzed and visualized in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 23:17:46 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Shen", "Sheng-syun", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1604.00100", "submitter": "Kushal Arora", "authors": "Kushal Arora and Anand Rangarajan", "title": "A Compositional Approach to Language Modeling", "comments": "submitted to ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional language models treat language as a finite state automaton on a\nprobability space over words. This is a very strong assumption when modeling\nsomething inherently complex such as language. In this paper, we challenge this\nby showing how the linear chain assumption inherent in previous work can be\ntranslated into a sequential composition tree. We then propose a new model that\nmarginalizes over all possible composition trees thereby removing any\nunderlying structural assumptions. As the partition function of this new model\nis intractable, we use a recently proposed sentence level evaluation metric\nContrastive Entropy to evaluate our model. Given this new evaluation metric, we\nreport more than 100% improvement across distortion levels over current state\nof the art recurrent neural network based language models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 01:51:34 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Arora", "Kushal", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1604.00117", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Larry Heck and Mari Ostendorf", "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language\n  Understanding", "comments": "Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to use multi-task learning to efficiently scale\nslot filling models for natural language understanding to handle multiple\ntarget tasks or domains. The key to scalability is reducing the amount of\ntraining data needed to learn a model for a new task. The proposed multi-task\nmodel delivers better performance with less data by leveraging patterns that it\nlearns from the other tasks. The approach supports an open vocabulary, which\nallows the models to generalize to unseen words, which is particularly\nimportant when very little training data is used. A newly collected\ncrowd-sourced data set, covering four different domains, is used to demonstrate\nthe effectiveness of the domain adaptation and open vocabulary techniques.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 03:24:32 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 02:53:00 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Jaech", "Aaron", ""], ["Heck", "Larry", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1604.00119", "submitter": "Krish Perumal", "authors": "Krish Perumal", "title": "Semi-supervised and Unsupervised Methods for Categorizing Posts in Web\n  Discussion Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web discussion forums are used by millions of people worldwide to share\ninformation belonging to a variety of domains such as automotive vehicles,\npets, sports, etc. They typically contain posts that fall into different\ncategories such as problem, solution, feedback, spam, etc. Automatic\nidentification of these categories can aid information retrieval that is\ntailored for specific user requirements. Previously, a number of supervised\nmethods have attempted to solve this problem; however, these depend on the\navailability of abundant training data. A few existing unsupervised and\nsemi-supervised approaches are either focused on identifying a single category\nor do not report category-specific performance. In contrast, this work proposes\nunsupervised and semi-supervised methods that require no or minimal training\ndata to achieve this objective without compromising on performance. A\nfine-grained analysis is also carried out to discuss their limitations. The\nproposed methods are based on sequence models (specifically, Hidden Markov\nModels) that can model language for each category using word and part-of-speech\nprobability distributions, and manually specified features. Empirical\nevaluations across domains demonstrate that the proposed methods are better\nsuited for this task than existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 03:32:03 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:06:13 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 21:27:17 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Perumal", "Krish", ""]]}, {"id": "1604.00125", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei and Yanran Li", "title": "AttSum: Joint Learning of Focusing and Summarization with Neural\n  Attention", "comments": "10 pages, 1 figure", "journal-ref": "COLING 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Query relevance ranking and sentence saliency ranking are the two main tasks\nin extractive query-focused summarization. Previous supervised summarization\nsystems often perform the two tasks in isolation. However, since reference\nsummaries are the trade-off between relevance and saliency, using them as\nsupervision, neither of the two rankers could be trained well. This paper\nproposes a novel summarization system called AttSum, which tackles the two\ntasks jointly. It automatically learns distributed representations for\nsentences as well as the document cluster. Meanwhile, it applies the attention\nmechanism to simulate the attentive reading of human behavior when a query is\ngiven. Extensive experiments are conducted on DUC query-focused summarization\nbenchmark datasets. Without using any hand-crafted features, AttSum achieves\ncompetitive performance. It is also observed that the sentences recognized to\nfocus on the query indeed meet the query need.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 04:18:39 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 02:22:33 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Cao", "Ziqiang", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""], ["Wei", "Furu", ""], ["Li", "Yanran", ""]]}, {"id": "1604.00126", "submitter": "Ardavan Saeedi", "authors": "Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, Sam Gershman", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional topic models do not account for semantic regularities in\nlanguage. Recent distributional representations of words exhibit semantic\nconsistency over directional metrics such as cosine similarity. However,\nneither categorical nor Gaussian observational distributions used in existing\ntopic models are appropriate to leverage such correlations. In this paper, we\npropose to use the von Mises-Fisher distribution to model the density of words\nover a unit sphere. Such a representation is well-suited for directional data.\nWe use a Hierarchical Dirichlet Process for our base topic model and propose an\nefficient inference algorithm based on Stochastic Variational Inference. This\nmodel enables us to naturally exploit the semantic structures of word\nembeddings while flexibly discovering the number of topics. Experiments\ndemonstrate that our method outperforms competitive approaches in terms of\ntopic coherence on two different text corpora while offering efficient\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 04:36:58 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Batmanghelich", "Kayhan", ""], ["Saeedi", "Ardavan", ""], ["Narasimhan", "Karthik", ""], ["Gershman", "Sam", ""]]}, {"id": "1604.00317", "submitter": "Ehud Ben-Reuven", "authors": "Ehud Ben-Reuven and Jacob Goldberger", "title": "A Semisupervised Approach for Language Identification based on Ladder\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we address the problem of training a neuralnetwork for language\nidentification using both labeled and unlabeled speech samples in the form of\ni-vectors. We propose a neural network architecture that can also handle\nout-of-set languages. We utilize a modified version of the recently proposed\nLadder Network semisupervised training procedure that optimizes the\nreconstruction costs of a stack of denoising autoencoders. We show that this\napproach can be successfully applied to the case where the training dataset is\ncomposed of both labeled and unlabeled acoustic data. The results show enhanced\nlanguage identification on the NIST 2015 language identification dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:26:57 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Ben-Reuven", "Ehud", ""], ["Goldberger", "Jacob", ""]]}, {"id": "1604.00400", "submitter": "Arman Cohan", "authors": "Arman Cohan and Nazli Goharian", "title": "Revisiting Summarization Evaluation for Scientific Articles", "comments": "LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of text summarization approaches have been mostly based on metrics\nthat measure similarities of system generated summaries with a set of human\nwritten gold-standard summaries. The most widely used metric in summarization\nevaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps\nbetween the terms and phrases in the sentences; therefore, in cases of\nterminology variations and paraphrasing, ROUGE is not as effective. Scientific\narticle summarization is one such case that is different from general domain\nsummarization (e.g. newswire data). We provide an extensive analysis of ROUGE's\neffectiveness as an evaluation metric for scientific summarization; we show\nthat, contrary to the common belief, ROUGE is not much reliable in evaluating\nscientific summaries. We furthermore show how different variants of ROUGE\nresult in very different correlations with the manual Pyramid scores. Finally,\nwe propose an alternative metric for summarization evaluation which is based on\nthe content relevance between a system generated summary and the corresponding\nhuman written summaries. We call our metric SERA (Summarization Evaluation by\nRelevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations\nwith manual scores which shows its effectiveness in evaluation of scientific\narticle summarization.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 20:06:46 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1604.00425", "submitter": "Shyam Upadhyay", "authors": "Shyam Upadhyay, Manaal Faruqui, Chris Dyer and Dan Roth", "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "comments": "To appear at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite interest in using cross-lingual knowledge to learn word embeddings\nfor various tasks, a systematic comparison of the possible approaches is\nlacking in the literature. We perform an extensive evaluation of four popular\napproaches of inducing cross-lingual embeddings, each requiring a different\nform of supervision, on four typographically different language pairs. Our\nevaluation setup spans four different tasks, including intrinsic evaluation on\nmono-lingual and cross-lingual similarity, and extrinsic evaluation on\ndownstream semantic and syntactic applications. We show that models which\nrequire expensive cross-lingual knowledge almost always perform better, but\ncheaply supervised models often prove competitive on certain tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 22:18:51 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 03:14:08 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Upadhyay", "Shyam", ""], ["Faruqui", "Manaal", ""], ["Dyer", "Chris", ""], ["Roth", "Dan", ""]]}, {"id": "1604.00461", "submitter": "Mo Yu", "authors": "Mo Yu, Mark Dredze, Raman Arora, Matthew Gormley", "title": "Embedding Lexical Features via Low-Rank Tensors", "comments": "Accepted by NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern NLP models rely heavily on engineered features, which often combine\nword and contextual information into complex lexical features. Such combination\nresults in large numbers of features, which can lead to over-fitting. We\npresent a new model that represents complex lexical features---comprised of\nparts for words, contextual information and labels---in a tensor that captures\nconjunction information among these parts. We apply low-rank tensor\napproximations to the corresponding parameter tensors to reduce the parameter\nspace and improve prediction speed. Furthermore, we investigate two methods for\nhandling features that include $n$-grams of mixed lengths. Our model achieves\nstate-of-the-art results on tasks in relation extraction, PP-attachment, and\npreposition disambiguation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 04:59:21 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Yu", "Mo", ""], ["Dredze", "Mark", ""], ["Arora", "Raman", ""], ["Gormley", "Matthew", ""]]}, {"id": "1604.00466", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed\n  Elgammal", "title": "Automatic Annotation of Structured Facts in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the application of fact-level image understanding, we present an\nautomatic method for data collection of structured visual facts from images\nwith captions. Example structured facts include attributed objects (e.g.,\n<flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man,\nwalking, dog>), and positional information (e.g., <vase, on, table>). The\ncollected annotations are in the form of fact-image pairs (e.g.,<man, walking,\ndog> and an image region containing this fact). With a language approach, the\nproposed method is able to collect hundreds of thousands of visual fact\nannotations with accuracy of 83% according to human judgment. Our method\nautomatically collected more than 380,000 visual fact annotations and more than\n110,000 unique visual facts from images with captions and localized them in\nimages in less than one day of processing time on standard CPU platforms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 06:35:45 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 18:58:10 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 00:04:22 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Cohen", "Scott", ""], ["Chang", "Walter", ""], ["Price", "Brian", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1604.00502", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Tobias Schnabel, Hinrich Sch\\\"utze", "title": "Online Updating of Word Representations for Part-of-Speech Tagging", "comments": "EMNLP'2015. Released POS tagger \"FLORS\" for online domain adaptation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose online unsupervised domain adaptation (DA), which is performed\nincrementally as data comes in and is applicable when batch DA is not possible.\nIn a part-of-speech (POS) tagging evaluation, we find that online unsupervised\nDA performs as well as batch DA.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 13:52:23 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Yin", "Wenpeng", ""], ["Schnabel", "Tobias", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1604.00503", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Hinrich Sch\\\"utze", "title": "Discriminative Phrase Embedding for Paraphrase Identification", "comments": "NAACL'2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work, concerning paraphrase identification task, on one hand contributes\nto expanding deep learning embeddings to include continuous and discontinuous\nlinguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN\nto learn the discriminative weights of words and phrases specific to paraphrase\ntask, so that a weighted sum of embeddings can represent sentences more\neffectively. Based on these two innovations we get competitive state-of-the-art\nperformance on paraphrase identification.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 13:57:02 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1604.00562", "submitter": "Jacob Andreas", "authors": "Jacob Andreas and Dan Klein", "title": "Reasoning About Pragmatics with Neural Listeners and Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for pragmatically describing scenes, in which contrastive\nbehavior results from a combination of inference-driven pragmatics and learned\nsemantics. Like previous learned approaches to language generation, our model\nuses a simple feature-driven architecture (here a pair of neural \"listener\" and\n\"speaker\" models) to ground language in the world. Like inference-driven\napproaches to pragmatics, our model actively reasons about listener behavior\nwhen selecting utterances. For training, our approach requires only ordinary\ncaptions, annotated _without_ demonstration of the pragmatic behavior the model\nultimately exhibits. In human evaluations on a referring expression game, our\napproach succeeds 81% of the time, compared to a 69% success rate using\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 21:52:03 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 13:48:20 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Andreas", "Jacob", ""], ["Klein", "Dan", ""]]}, {"id": "1604.00727", "submitter": "David Golub", "authors": "David Golub, Xiaodong He", "title": "Character-Level Question Answering with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a character-level encoder-decoder framework can be successfully\napplied to question answering with a structured knowledge base. We use our\nmodel for single-relation question answering and demonstrate the effectiveness\nof our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we\nimprove state-of-the-art accuracy from 63.9% to 70.9%, without use of\nensembles. Importantly, our character-level model has 16x fewer parameters than\nan equivalent word-level model, can be learned with significantly less data\ncompared to previous work, which relies on data augmentation, and is robust to\nnew entities in testing.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 02:43:23 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 23:09:31 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 21:12:47 GMT"}, {"version": "v4", "created": "Sun, 5 Jun 2016 02:02:10 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Golub", "David", ""], ["He", "Xiaodong", ""]]}, {"id": "1604.00734", "submitter": "Matthew Francis-Landau", "authors": "Matthew Francis-Landau, Greg Durrett and Dan Klein", "title": "Capturing Semantic Similarity for Entity Linking with Convolutional\n  Neural Networks", "comments": "Accepted at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in entity linking is making effective use of contextual\ninformation to disambiguate mentions that might refer to different entities in\ndifferent contexts. We present a model that uses convolutional neural networks\nto capture semantic correspondence between a mention's context and a proposed\ntarget entity. These convolutional networks operate at multiple granularities\nto exploit various kinds of topic information, and their rich parameterization\ngives them the capacity to learn which n-grams characterize different topics.\nWe combine these networks with a sparse linear model to achieve\nstate-of-the-art performance on multiple entity linking datasets, outperforming\nthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 03:58:31 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Francis-Landau", "Matthew", ""], ["Durrett", "Greg", ""], ["Klein", "Dan", ""]]}, {"id": "1604.00788", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Christopher D. Manning", "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid\n  Word-Character Models", "comments": "11pages, 4 figures. ACL 2016 camera-ready version. SOTA WMT'15\n  English-Czech 20.7 BLEU (+2.1-11.4 points)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all previous work on neural machine translation (NMT) has used quite\nrestricted vocabularies, perhaps with a subsequent method to patch in unknown\nwords. This paper presents a novel word-character solution to achieving open\nvocabulary NMT. We build hybrid systems that translate mostly at the word level\nand consult the character components for rare words. Our character-level\nrecurrent neural networks compute source word representations and recover\nunknown target words when needed. The twofold advantage of such a hybrid\napproach is that it is much faster and easier to train than character-based\nones; at the same time, it never produces unknown words as in the case of\nword-based models. On the WMT'15 English to Czech translation task, this hybrid\napproach offers an addition boost of +2.1-11.4 BLEU points over models that\nalready handle unknown words. Our best system achieves a new state-of-the-art\nresult with 20.7 BLEU score. We demonstrate that our character models can\nsuccessfully learn to not only generate well-formed words for Czech, a\nhighly-inflected language with a very complex vocabulary, but also build\ncorrect representations for English source words.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 09:30:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:50:19 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1604.00790", "submitter": "Cheng Wang", "authors": "Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel", "title": "Image Captioning with Deep Bidirectional LSTMs", "comments": "accepted by ACMMM 2016 as full paper and oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models \"translate\" image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 09:43:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 07:45:25 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 14:19:37 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Wang", "Cheng", ""], ["Yang", "Haojin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1604.00834", "submitter": "Andrzej Kulig", "authors": "Andrzej Kulig, Jaroslaw Kwapien, Tomasz Stanisz, Stanislaw Drozdz", "title": "In narrative texts punctuation marks obey the same statistics as words", "comments": "Information Sciences (inprint)", "journal-ref": "Information Sciences 375 (2017) 98-113", "doi": "10.1016/j.ins.2016.09.051", "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a grammar point of view, the role of punctuation marks in a sentence is\nformally defined and well understood. In semantic analysis punctuation plays\nalso a crucial role as a method of avoiding ambiguity of the meaning. A\ndifferent situation can be observed in the statistical analyses of language\nsamples, where the decision on whether the punctuation marks should be\nconsidered or should be neglected is seen rather as arbitrary and at present it\nbelongs to a researcher's preference. An objective of this work is to shed some\nlight onto this problem by providing us with an answer to the question whether\nthe punctuation marks may be treated as ordinary words and whether they should\nbe included in any analysis of the word co-occurences. We already know from our\nprevious study (S.~Dro\\.zd\\.z {\\it et al.}, Inf. Sci. 331 (2016) 32-44) that\nfull stops that determine the length of sentences are the main carrier of\nlong-range correlations. Now we extend that study and analyze statistical\nproperties of the most common punctuation marks in a few Indo-European\nlanguages, investigate their frequencies, and locate them accordingly in the\nZipf rank-frequency plots as well as study their role in the word-adjacency\nnetworks. We show that, from a statistical viewpoint, the punctuation marks\nreveal properties that are qualitatively similar to the properties of the most\nfrequent words like articles, conjunctions, pronouns, and prepositions. This\nrefers to both the Zipfian analysis and the network analysis. By adding the\npunctuation marks to the Zipf plots, we also show that these plots that are\nnormally described by the Zipf-Mandelbrot distribution largely restore the\npower-law Zipfian behaviour for the most frequent items.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 12:29:00 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 20:07:49 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Kulig", "Andrzej", ""], ["Kwapien", "Jaroslaw", ""], ["Stanisz", "Tomasz", ""], ["Drozdz", "Stanislaw", ""]]}, {"id": "1604.00933", "submitter": "Walid Shalaby", "authors": "Walid Shalaby, Khalifeh Al Jadda, Mohammed Korayem and Trey Grainger", "title": "Entity Type Recognition using an Ensemble of Distributional Semantic\n  Models to Enhance Query Understanding", "comments": "A short version of this paper has been accepted in \"COMPSAC 2016: The\n  40th IEEE Computer Society International Conference on Computers, Software &\n  Applications\"", "journal-ref": null, "doi": "10.1109/COMPSAC.2016.109", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an ensemble approach for categorizing search query entities in the\nrecruitment domain. Understanding the types of entities expressed in a search\nquery (Company, Skill, Job Title, etc.) enables more intelligent information\nretrieval based upon those entities compared to a traditional keyword-based\nsearch. Because search queries are typically very short, leveraging a\ntraditional bag-of-words model to identify entity types would be inappropriate\ndue to the lack of contextual information. Our approach instead combines clues\nfrom different sources of varying complexity in order to collect real-world\nknowledge about query entities. We employ distributional semantic\nrepresentations of query entities through two models: 1) contextual vectors\ngenerated from encyclopedic corpora like Wikipedia, and 2) high dimensional\nword embedding vectors generated from millions of job postings using word2vec.\nAdditionally, our approach utilizes both entity linguistic properties obtained\nfrom WordNet and ontological properties extracted from DBpedia. We evaluate our\napproach on a data set created at CareerBuilder; the largest job board in the\nUS. The data set contains entities extracted from millions of job\nseekers/recruiters search queries, job postings, and resume documents. After\nconstructing the distributional vectors of search entities, we use supervised\nmachine learning to infer search entity types. Empirical results show that our\napproach outperforms the state-of-the-art word2vec distributional semantics\nmodel trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of\n97% using the proposed distributional representations ensemble.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 16:18:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shalaby", "Walid", ""], ["Jadda", "Khalifeh Al", ""], ["Korayem", "Mohammed", ""], ["Grainger", "Trey", ""]]}, {"id": "1604.00938", "submitter": "Tomasz Jurczyk", "authors": "Tomasz Jurczyk and Jinho D. Choi", "title": "Multi-Field Structural Decomposition for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a precursory yet novel approach to the question answering\ntask using structural decomposition. Our system first generates linguistic\nstructures such as syntactic and semantic trees from text, decomposes them into\nmultiple fields, then indexes the terms in each field. For each question, it\ndecomposes the question into multiple fields, measures the relevance score of\neach field to the indexed ones, then ranks all documents by their relevance\nscores and weights associated with the fields, where the weights are learned\nthrough statistical modeling. Our final model gives an absolute improvement of\nover 40% to the baseline approach using simple search for detecting documents\ncontaining answers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 16:33:15 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Jurczyk", "Tomasz", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1604.01178", "submitter": "Alessandro Moschitti", "authors": "Aliaksei Severyn and Alessandro Moschitti", "title": "Modeling Relational Information in Question-Answer Pairs with\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose convolutional neural networks for learning an\noptimal representation of question and answer sentences. Their main aspect is\nthe use of relational information given by the matches between words from the\ntwo members of the pair. The matches are encoded as embeddings with additional\nparameters (dimensions), which are tuned by the network. These allows for\nbetter capturing interactions between questions and answers, resulting in a\nsignificant boost in accuracy. We test our models on two widely used answer\nsentence selection benchmarks. The results clearly show the effectiveness of\nour relational information, which allows our relatively simple network to\napproach the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:50:27 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Severyn", "Aliaksei", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "1604.01219", "submitter": "Yuting Qaing", "authors": "Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou and Leonid Sigal", "title": "Learning to Generate Posters of Scientific Papers", "comments": "in Proceedings of the 30th AAAI Conference on Artificial Intelligence\n  (AAAI'16), Phoenix, AZ, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often summarize their work in the form of posters. Posters\nprovide a coherent and efficient way to convey core ideas from scientific\npapers. Generating a good scientific poster, however, is a complex and time\nconsuming cognitive task, since such posters need to be readable, informative,\nand visually aesthetic. In this paper, for the first time, we study the\nchallenging problem of learning to generate posters from scientific papers. To\nthis end, a data-driven framework, that utilizes graphical models, is proposed.\nSpecifically, given content to display, the key elements of a good poster,\nincluding panel layout and attributes of each panel, are learned and inferred\nfrom data. Then, given inferred layout and attributes, composition of graphical\nelements within each panel is synthesized. To learn and validate our model, we\ncollect and make public a Poster-Paper dataset, which consists of scientific\npapers and corresponding posters with exhaustively labelled panels and\nattributes. Qualitative and quantitative results indicate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 11:18:04 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Qiang", "Yuting", ""], ["Fu", "Yanwei", ""], ["Guo", "Yanwen", ""], ["Zhou", "Zhi-Hua", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.01221", "submitter": "Guntis Barzdins", "authors": "Guntis Barzdins, Steve Renals, Didzis Gosko", "title": "Character-Level Neural Translation for Multilingual Media Monitoring in\n  the SUMMA Project", "comments": "LREC-2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper steps outside the comfort-zone of the traditional NLP tasks like\nautomatic speech recognition (ASR) and machine translation (MT) to addresses\ntwo novel problems arising in the automated multilingual news monitoring:\nsegmentation of the TV and radio program ASR transcripts into individual\nstories, and clustering of the individual stories coming from various sources\nand languages into storylines. Storyline clustering of stories covering the\nsame events is an essential task for inquisitorial media monitoring. We address\nthese two problems jointly by engaging the low-dimensional semantic\nrepresentation capabilities of the sequence to sequence neural translation\nmodels. To enable joint multi-task learning for multilingual neural translation\nof morphologically rich languages we replace the attention mechanism with the\nsliding-window mechanism and operate the sequence to sequence neural\ntranslation model on the character-level rather than on the word-level. The\nstory segmentation and storyline clustering problem is tackled by examining the\nlow-dimensional vectors produced as a side-product of the neural translation\nprocess. The results of this paper describe a novel approach to the automatic\nstory segmentation and storyline clustering problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 11:34:11 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Barzdins", "Guntis", ""], ["Renals", "Steve", ""], ["Gosko", "Didzis", ""]]}, {"id": "1604.01235", "submitter": "Vijay Krishna Menon Mr", "authors": "Vijay Krishna Menon, S. Rajendran, M. Anand Kumar, K.P. Soman", "title": "A new TAG Formalism for Tamil and Parser Analytics", "comments": "International Symposium for Dravidian Languages (iDravidian),\n  co-located with ICON2014, Goa University, Dec 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree adjoining grammar (TAG) is specifically suited for morph rich and\nagglutinated languages like Tamil due to its psycho linguistic features and\nparse time dependency and morph resolution. Though TAG and LTAG formalisms have\nbeen known for about 3 decades, efforts on designing TAG Syntax for Tamil have\nnot been entirely successful due to the complexity of its specification and the\nrich morphology of Tamil language. In this paper we present a minimalistic TAG\nfor Tamil without much morphological considerations and also introduce a parser\nimplementation with some obvious variations from the XTAG system\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 12:42:31 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Menon", "Vijay Krishna", ""], ["Rajendran", "S.", ""], ["Kumar", "M. Anand", ""], ["Soman", "K. P.", ""]]}, {"id": "1604.01243", "submitter": "Massimo Stella", "authors": "Massimo Stella and Markus Brede", "title": "Mental Lexicon Growth Modelling Reveals the Multiplexity of the English\n  Language", "comments": "14 pages, published in the Proceedings of the 7th Workshop on Complex\n  Networks CompleNet 2016. Complex Systems VII, Volume 644 of the series\n  Studies in Computational Intelligence pp 267-279, 2016", "journal-ref": null, "doi": "10.1007/978-3-319-30569-1_20", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we extend previous analyses of linguistic networks by adopting a\nmulti-layer network framework for modelling the human mental lexicon, i.e. an\nabstract mental repository where words and concepts are stored together with\ntheir linguistic patterns. Across a three-layer linguistic multiplex, we model\nEnglish words as nodes and connect them according to (i) phonological\nsimilarities, (ii) synonym relationships and (iii) free word associations. Our\nmain aim is to exploit this multi-layered structure to explore the influence of\nphonological and semantic relationships on lexicon assembly over time. We\npropose a model of lexicon growth which is driven by the phonological layer:\nwords are suggested according to different orderings of insertion (e.g. shorter\nword length, highest frequency, semantic multiplex features) and accepted or\nrejected subject to constraints. We then measure times of network assembly and\ncompare these to empirical data about the age of acquisition of words. In\nagreement with empirical studies in psycholinguistics, our results provide\nquantitative evidence for the hypothesis that word acquisition is driven by\nfeatures at multiple levels of organisation within language.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 13:04:35 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Stella", "Massimo", ""], ["Brede", "Markus", ""]]}, {"id": "1604.01272", "submitter": "Despoina Christou", "authors": "Despoina Christou", "title": "Feature extraction using Latent Dirichlet Allocation and Neural\n  Networks: A case study on movie synopses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 14:32:48 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Christou", "Despoina", ""]]}, {"id": "1604.01278", "submitter": "Guntis Barzdins", "authors": "Guntis Barzdins, Didzis Gosko", "title": "RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and\n  Character-Level Neural Translation on AMR Parsing Accuracy", "comments": "NAACL HLT 2016, SemEval-2016 Task 8 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two extensions to the AMR smatch scoring script are presented. The first\nextension com-bines the smatch scoring script with the C6.0 rule-based\nclassifier to produce a human-readable report on the error patterns frequency\nobserved in the scored AMR graphs. This first extension results in 4% gain over\nthe state-of-art CAMR baseline parser by adding to it a manually crafted\nwrapper fixing the identified CAMR parser errors. The second extension combines\na per-sentence smatch with an en-semble method for selecting the best AMR graph\namong the set of AMR graphs for the same sentence. This second modification\nau-tomatically yields further 0.4% gain when ap-plied to outputs of two\nnondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level\nneural translation AMR parser. For AMR parsing task the character-level neural\ntranslation attains surprising 7% gain over the carefully optimized word-level\nneural translation. Overall, we achieve smatch F1=62% on the SemEval-2016\nofficial scor-ing set and F1=67% on the LDC2015E86 test set.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 14:46:18 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Barzdins", "Guntis", ""], ["Gosko", "Didzis", ""]]}, {"id": "1604.01485", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski, Shuicheng Yan, Jiashi Feng", "title": "A Focused Dynamic Attention Model for Visual Question Answering", "comments": "Submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question and Answering (VQA) problems are attracting increasing\ninterest from multiple research disciplines. Solving VQA problems requires\ntechniques from both computer vision for understanding the visual contents of a\npresented image or video, as well as the ones from natural language processing\nfor understanding semantics of the question and generating the answers.\nRegarding visual content modeling, most of existing VQA methods adopt the\nstrategy of extracting global features from the image or video, which\ninevitably fails in capturing fine-grained information such as spatial\nconfiguration of multiple objects. Extracting features from auto-generated\nregions -- as some region-based image recognition methods do -- cannot\nessentially address this problem and may introduce some overwhelming irrelevant\nfeatures with the question. In this work, we propose a novel Focused Dynamic\nAttention (FDA) model to provide better aligned image content representation\nwith proposed questions. Being aware of the key words in the question, FDA\nemploys off-the-shelf object detector to identify important regions and fuse\nthe information from the regions and global features via an LSTM unit. Such\nquestion-driven representations are then combined with question representation\nand fed into a reasoning unit for generating the answers. Extensive evaluation\non a large-scale benchmark dataset, VQA, clearly demonstrate the superior\nperformance of FDA over well-established baselines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 05:16:10 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ilievski", "Ilija", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1604.01537", "submitter": "Xiaoyuan Yi", "authors": "Xiaoyuan Yi, Ruoyu Li, Maosong Sun", "title": "Generating Chinese Classical Poems with RNN Encoder-Decoder", "comments": "12 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take the generation of Chinese classical poem lines as a\nsequence-to-sequence learning problem, and build a novel system based on the\nRNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a\ntopic word as input. Our system can jointly learn semantic meaning within a\nsingle line, semantic relevance among lines in a poem, and the use of\nstructural, rhythmical and tonal patterns, without utilizing any constraint\ntemplates. Experimental results show that our system outperforms other\ncompetitive systems. We also find that the attention mechanism can capture the\nword associations in Chinese classical poetry and inverting target lines in\ntraining can improve performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 08:26:31 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Yi", "Xiaoyuan", ""], ["Li", "Ruoyu", ""], ["Sun", "Maosong", ""]]}, {"id": "1604.01692", "submitter": "Robyn Speer", "authors": "Robyn Speer and Joshua Chin", "title": "An Ensemble Method to Produce High-Quality Word Embeddings (2016)", "comments": "Corrected author name, revised reproducibility instructions that\n  didn't work anymore. 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A currently successful approach to computational semantics is to represent\nwords as embeddings in a machine-learned vector space. We present an ensemble\nmethod that combines embeddings produced by GloVe (Pennington et al., 2014) and\nword2vec (Mikolov et al., 2013) with structured knowledge from the semantic\nnetworks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al.,\n2013), merging their information into a common representation with a large,\nmultilingual vocabulary. The embeddings it produces achieve state-of-the-art\nperformance on many word-similarity evaluations. Its score of $\\rho = .596$ on\nan evaluation of rare words (Luong et al., 2013) is 16% higher than the\nprevious best known system.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 16:58:35 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 17:29:15 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Speer", "Robyn", ""], ["Chin", "Joshua", ""]]}, {"id": "1604.01696", "submitter": "Nasrin Mostafazadeh", "authors": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh,\n  Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli and James Allen", "title": "A Corpus and Evaluation Framework for Deeper Understanding of\n  Commonsense Stories", "comments": "In Proceedings of the 2016 North American Chapter of the ACL (NAACL\n  HLT), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation and learning of commonsense knowledge is one of the\nfoundational problems in the quest to enable deep language understanding. This\nissue is particularly challenging for understanding casual and correlational\nrelationships between events. While this topic has received a lot of interest\nin the NLP community, research has been hindered by the lack of a proper\nevaluation framework. This paper attempts to address this problem with a new\nframework for evaluating story understanding and script learning: the 'Story\nCloze Test'. This test requires a system to choose the correct ending to a\nfour-sentence story. We created a new corpus of ~50k five-sentence commonsense\nstories, ROCStories, to enable this evaluation. This corpus is unique in two\nways: (1) it captures a rich set of causal and temporal commonsense relations\nbetween daily events, and (2) it is a high quality collection of everyday life\nstories that can also be used for story generation. Experimental evaluation\nshows that a host of baselines and state-of-the-art models based on shallow\nlanguage understanding struggle to achieve a high score on the Story Cloze\nTest. We discuss these implications for script and story learning, and offer\nsuggestions for deeper language understanding.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 17:15:10 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Mostafazadeh", "Nasrin", ""], ["Chambers", "Nathanael", ""], ["He", "Xiaodong", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""], ["Vanderwende", "Lucy", ""], ["Kohli", "Pushmeet", ""], ["Allen", "James", ""]]}, {"id": "1604.01729", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Lisa Anne Hendricks, Raymond Mooney, Kate\n  Saenko", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined\n  from Text", "comments": "Accepted at EMNLP 2016. Project page:\n  http://vsubhashini.github.io/language_fusion.html", "journal-ref": "Proc.EMNLP (2016) pg.1961-1966", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how linguistic knowledge mined from large text\ncorpora can aid the generation of natural language descriptions of videos.\nSpecifically, we integrate both a neural language model and distributional\nsemantics trained on large text corpora into a recent LSTM-based architecture\nfor video description. We evaluate our approach on a collection of Youtube\nvideos as well as two large movie description datasets showing significant\nimprovements in grammaticality while modestly improving descriptive quality.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 19:01:28 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 20:37:42 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Hendricks", "Lisa Anne", ""], ["Mooney", "Raymond", ""], ["Saenko", "Kate", ""]]}, {"id": "1604.01792", "submitter": "Tom Sercu", "authors": "Tom Sercu, Vaibhava Goel", "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR", "comments": "Proc. Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep CNNs with small 3x3 kernels have recently been shown to achieve\nvery strong performance as acoustic models in hybrid NN-HMM speech recognition\nsystems. In this paper we investigate how to efficiently scale these models to\nlarger datasets. Specifically, we address the design choice of pooling and\npadding along the time dimension which renders convolutional evaluation of\nsequences highly inefficient. We propose a new CNN design without timepadding\nand without timepooling, which is slightly suboptimal for accuracy, but has two\nsignificant advantages: it enables sequence training and deployment by allowing\nefficient convolutional evaluation of full utterances, and, it allows for batch\nnormalization to be straightforwardly adopted to CNNs on sequence data. Through\nbatch normalization, we recover the lost peformance from removing the\ntime-pooling, while keeping the benefit of efficient convolutional evaluation.\nWe demonstrate the performance of our models both on larger scale data than\nbefore, and after sequence training. Our very deep CNN model sequence trained\non the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5\ntest-set, matching with a single model the performance of the 2015 IBM system\ncombination, which was the previous best published result.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:07:52 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 00:27:19 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Sercu", "Tom", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1604.01904", "submitter": "Yana A", "authors": "Ayana, Shiqi Shen, Yu Zhao, Zhiyuan Liu, Maosong Sun", "title": "Neural Headline Generation with Sentence-wise Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural models have been proposed for headline generation by\nlearning to map documents to headlines with recurrent neural networks.\nNevertheless, as traditional neural network utilizes maximum likelihood\nestimation for parameter optimization, it essentially constrains the expected\ntraining objective within word level rather than sentence level. Moreover, the\nperformance of model prediction significantly relies on training data\ndistribution. To overcome these drawbacks, we employ minimum risk training\nstrategy in this paper, which directly optimizes model parameters in sentence\nlevel with respect to evaluation metrics and leads to significant improvements\nfor headline generation. Experiment results show that our models outperforms\nstate-of-the-art systems on both English and Chinese headline generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 07:47:11 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 07:16:24 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Ayana", "", ""], ["Shen", "Shiqi", ""], ["Zhao", "Yu", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "1604.02027", "submitter": "Ke Jiang", "authors": "Ke Jiang and Suvrit Sra and Brian Kulis", "title": "Combinatorial Topic Models using Small-Variance Asymptotics", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have emerged as fundamental tools in unsupervised machine\nlearning. Most modern topic modeling algorithms take a probabilistic view and\nderive inference algorithms based on Latent Dirichlet Allocation (LDA) or its\nvariants. In contrast, we study topic modeling as a combinatorial optimization\nproblem, and propose a new objective function derived from LDA by passing to\nthe small-variance limit. We minimize the derived objective by using ideas from\ncombinatorial optimization, which results in a new, fast, and high-quality\ntopic modeling algorithm. In particular, we show that our results are\ncompetitive with popular LDA-based topic modeling approaches, and also discuss\nthe (dis)similarities between our approach and its probabilistic counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:04:16 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 03:11:02 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Jiang", "Ke", ""], ["Sra", "Suvrit", ""], ["Kulis", "Brian", ""]]}, {"id": "1604.02038", "submitter": "Fei Tian", "authors": "Fei Tian, Bin Gao, Di He, Tie-Yan Liu", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for\n  Themselves", "comments": "The submitted version was done in Feb.2016. Still in improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model\nthat assumes the generation of each word within a sentence to depend on both\nthe topic of the sentence and the whole history of its preceding words in the\nsentence. Different from conventional topic models that largely ignore the\nsequential order of words or their topic coherence, SLRTM gives full\ncharacterization to them by using a Recurrent Neural Networks (RNN) based\nframework. Experimental results have shown that SLRTM outperforms several\nstrong baselines on various tasks. Furthermore, SLRTM can automatically\ngenerate sentences given a topic (i.e., topics to sentences), which is a key\ntechnology for real world applications such as personalized short text\nconversation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:29:45 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 05:45:44 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Tian", "Fei", ""], ["Gao", "Bin", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1604.02125", "submitter": "Gordon Christie", "authors": "Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol,\n  Yash Goyal, Kevin Kochersberger, Dhruv Batra", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation &\n  Prepositional Attachment Resolution in Captioned Scenes", "comments": "*The first two authors contributed equally. Conference on Empirical\n  Methods in Natural Language Processing (EMNLP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to simultaneously perform semantic segmentation and\nprepositional phrase attachment resolution for captioned images. Some\nambiguities in language cannot be resolved without simultaneously reasoning\nabout an associated image. If we consider the sentence \"I shot an elephant in\nmy pajamas\", looking at language alone (and not using common sense), it is\nunclear if it is the person or the elephant wearing the pajamas or both. Our\napproach produces a diverse set of plausible hypotheses for both semantic\nsegmentation and prepositional phrase attachment resolution that are then\njointly reranked to select the most consistent pair. We show that our semantic\nsegmentation and prepositional phrase attachment resolution modules have\ncomplementary strengths, and that joint reasoning produces more accurate\nresults than any module operating in isolation. Multiple hypotheses are also\nshown to be crucial to improved multiple-module reasoning. Our vision and\nlanguage approach significantly outperforms the Stanford Parser (De Marneffe et\nal., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two\ndifferent experiments. We also make small improvements over DeepLab-CRF (Chen\net al., 2015).\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:26:56 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 19:05:27 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 01:47:35 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 04:08:19 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Christie", "Gordon", ""], ["Laddha", "Ankit", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Goyal", "Yash", ""], ["Kochersberger", "Kevin", ""], ["Batra", "Dhruv", ""]]}, {"id": "1604.02201", "submitter": "Barret Zoph", "authors": "Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 00:16:35 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Zoph", "Barret", ""], ["Yuret", "Deniz", ""], ["May", "Jonathan", ""], ["Knight", "Kevin", ""]]}, {"id": "1604.02506", "submitter": "Antonio Jose Jimeno Yepes", "authors": "Antonio Jimeno Yepes", "title": "Word embeddings and recurrent neural networks based on Long-Short Term\n  Memory nodes in supervised biomedical word sense disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense disambiguation helps identifying the proper sense of ambiguous\nwords in text. With large terminologies such as the UMLS Metathesaurus\nambiguities appear and highly effective disambiguation methods are required.\nSupervised learning algorithm methods are used as one of the approaches to\nperform disambiguation. Features extracted from the context of an ambiguous\nword are used to identify the proper sense of such a word. The type of features\nhave an impact on machine learning methods, thus affect disambiguation\nperformance. In this work, we have evaluated several types of features derived\nfrom the context of the ambiguous word and we have explored as well more global\nfeatures derived from MEDLINE using word embeddings. Results show that word\nembeddings improve the performance of more traditional features and allow as\nwell using recurrent neural network classifiers based on Long-Short Term Memory\n(LSTM) nodes. The combination of unigrams and word embeddings with an SVM sets\na new state of the art performance with a macro accuracy of 95.97 in the MSH\nWSD data set.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 01:14:05 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 12:04:10 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 19:44:52 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Yepes", "Antonio Jimeno", ""]]}, {"id": "1604.02594", "submitter": "Zhiyun Lu", "authors": "Zhiyun Lu, Vikas Sindhwani, Tara N. Sainath", "title": "Learning Compact Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), including long short-term memory (LSTM)\nRNNs, have produced state-of-the-art results on a variety of speech recognition\ntasks. However, these models are often too large in size for deployment on\nmobile devices with memory and latency constraints. In this work, we study\nmechanisms for learning compact RNNs and LSTMs via low-rank factorizations and\nparameter sharing schemes. Our goal is to investigate redundancies in recurrent\narchitectures where compression can be admitted without losing performance. A\nhybrid strategy of using structured matrices in the bottom layers and shared\nlow-rank factors on the top layers is found to be particularly effective,\nreducing the parameters of a standard LSTM by 75%, at a small cost of 0.3%\nincrease in WER, on a 2,000-hr English Voice Search task.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 19:09:22 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Lu", "Zhiyun", ""], ["Sindhwani", "Vikas", ""], ["Sainath", "Tara N.", ""]]}, {"id": "1604.02612", "submitter": "Mois\\'es Pereira", "authors": "Mois\\'es H. R. Pereira, Fl\\'avio L. C. P\\'adua, Adriano C. M. Pereira,\n  Fabr\\'icio Benevenuto, Daniel H. Dalip", "title": "Fusing Audio, Textual and Visual Features for Sentiment Analysis of News\n  Videos", "comments": "5 pages, 1 figure, International AAAI Conference on Web and Social\n  Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to perform sentiment analysis of news\nvideos, based on the fusion of audio, textual and visual clues extracted from\ntheir contents. The proposed approach aims at contributing to the\nsemiodiscoursive study regarding the construction of the ethos (identity) of\nthis media universe, which has become a central part of the modern-day lives of\nmillions of people. To achieve this goal, we apply state-of-the-art\ncomputational methods for (1) automatic emotion recognition from facial\nexpressions, (2) extraction of modulations in the participants' speeches and\n(3) sentiment analysis from the closed caption associated to the videos of\ninterest. More specifically, we compute features, such as, visual intensities\nof recognized emotions, field sizes of participants, voicing probability, sound\nloudness, speech fundamental frequencies and the sentiment scores (polarities)\nfrom text sentences in the closed caption. Experimental results with a dataset\ncontaining 520 annotated news videos from three Brazilian and one American\npopular TV newscasts show that our approach achieves an accuracy of up to 84%\nin the sentiments (tension levels) classification task, thus demonstrating its\nhigh potential to be used by media analysts in several applications,\nespecially, in the journalistic domain.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 22:00:27 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Pereira", "Mois\u00e9s H. R.", ""], ["P\u00e1dua", "Fl\u00e1vio L. C.", ""], ["Pereira", "Adriano C. M.", ""], ["Benevenuto", "Fabr\u00edcio", ""], ["Dalip", "Daniel H.", ""]]}, {"id": "1604.02843", "submitter": "Yuan Sun", "authors": "Yuan Sun and Zhen Zhu", "title": "Method of Tibetan Person Knowledge Extraction", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person knowledge extraction is the foundation of the Tibetan knowledge graph\nconstruction, which provides support for Tibetan question answering system,\ninformation retrieval, information extraction and other researches, and\npromotes national unity and social stability. This paper proposes a SVM and\ntemplate based approach to Tibetan person knowledge extraction. Through\nconstructing the training corpus, we build the templates based the shallow\nparsing analysis of Tibetan syntactic, semantic features and verbs. Using the\ntraining corpus, we design a hierarchical SVM classifier to realize the entity\nknowledge extraction. Finally, experimental results prove the method has\ngreater improvement in Tibetan person knowledge extraction.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 08:56:58 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Sun", "Yuan", ""], ["Zhu", "Zhen", ""]]}, {"id": "1604.02993", "submitter": "Karl Pichotta", "authors": "Karl Pichotta and Raymond J. Mooney", "title": "Using Sentence-Level LSTM Language Models for Script Inference", "comments": "To appear in Proceedings of the 54th Annual Meeting of the\n  Association for Computational Linguistics (ACL-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a small but growing body of research on statistical scripts, models\nof event sequences that allow probabilistic inference of implicit events from\ndocuments. These systems operate on structured verb-argument events produced by\nan NLP pipeline. We compare these systems with recent Recurrent Neural Net\nmodels that directly operate on raw tokens to predict sentences, finding the\nlatter to be roughly comparable to the former in terms of predicting missing\nevents in documents.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:21:05 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 19:05:14 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Pichotta", "Karl", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1604.03029", "submitter": "Juyong Park", "authors": "Semi Min and Juyong Park", "title": "Mapping Out Narrative Structures and Dynamics Using Networks and Textual\n  Information", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0226025", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication is often executed in the form of a narrative, an account\nof connected events composed of characters, actions, and settings. A coherent\nnarrative structure is therefore a requisite for a well-formulated narrative --\nbe it fictional or nonfictional -- for informative and effective communication,\nopening up the possibility of a deeper understanding of a narrative by studying\nits structural properties. In this paper we present a network-based framework\nfor modeling and analyzing the structure of a narrative, which is further\nexpanded by incorporating methods from computational linguistics to utilize the\nnarrative text. Modeling a narrative as a dynamically unfolding system, we\ncharacterize its progression via the growth patterns of the character network,\nand use sentiment analysis and topic modeling to represent the actual content\nof the narrative in the form of interaction maps between characters with\nassociated sentiment values and keywords. This is a network framework advanced\nbeyond the simple occurrence-based one most often used until now, allowing one\nto utilize the unique characteristics of a given narrative to a high degree.\nGiven the ubiquity and importance of narratives, such advanced network-based\nrepresentation and analysis framework may lead to a more systematic modeling\nand understanding of narratives for social interactions, expression of human\nsentiments, and communication.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 10:59:28 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Min", "Semi", ""], ["Park", "Juyong", ""]]}, {"id": "1604.03035", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Alexander M. Rush, Stuart M. Shieber", "title": "Learning Global Features for Coreference Resolution", "comments": "Accepted to NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is compelling evidence that coreference prediction would benefit from\nmodeling global information about entity-clusters. Yet, state-of-the-art\nperformance can be achieved with systems treating each mention prediction\nindependently, which we attribute to the inherent difficulty of crafting\ninformative cluster-level features. We instead propose to use recurrent neural\nnetworks (RNNs) to learn latent, global representations of entity clusters\ndirectly from their mentions. We show that such representations are especially\nuseful for the prediction of pronominal mentions, and can be incorporated into\nan end-to-end coreference system that outperforms the state of the art without\nrequiring any additional search.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 17:15:34 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wiseman", "Sam", ""], ["Rush", "Alexander M.", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1604.03114", "submitter": "Justine Zhang", "authors": "Justine Zhang, Ravi Kumar, Sujith Ravi, Cristian\n  Danescu-Niculescu-Mizil", "title": "Conversational flow in Oxford-style debates", "comments": "To appear at NAACL 2016. 5 pp, 1 fig. Data and other info available\n  at http://www.cs.cornell.edu/~cristian/debates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public debates are a common platform for presenting and juxtaposing diverging\nviews on important issues. In this work we propose a methodology for tracking\nhow ideas flow between participants throughout a debate. We use this approach\nin a case study of Oxford-style debates---a competitive format where the winner\nis determined by audience votes---and show how the outcome of a debate depends\non aspects of conversational flow. In particular, we find that winners tend to\nmake better use of a debate's interactive component than losers, by actively\npursuing their opponents' points rather than promoting their own ideas over the\ncourse of the conversation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 20:00:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Zhang", "Justine", ""], ["Kumar", "Ravi", ""], ["Ravi", "Sujith", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1604.03136", "submitter": "Piyush Bansal", "authors": "Arnav Sharma, Sakshi Gupta, Raveesh Motlani, Piyush Bansal, Manish\n  Srivastava, Radhika Mamidi, Dipti M. Sharma", "title": "Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the problem of shallow parsing of Hindi-English code-mixed\nsocial media text (CSMT) has been addressed. We have annotated the data,\ndeveloped a language identifier, a normalizer, a part-of-speech tagger and a\nshallow parser. To the best of our knowledge, we are the first to attempt\nshallow parsing on CSMT. The pipeline developed has been made available to the\nresearch community with the goal of enabling better text analysis of Hindi\nEnglish CSMT. The pipeline is accessible at http://bit.ly/csmt-parser-api .\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 20:24:52 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Sharma", "Arnav", ""], ["Gupta", "Sakshi", ""], ["Motlani", "Raveesh", ""], ["Bansal", "Piyush", ""], ["Srivastava", "Manish", ""], ["Mamidi", "Radhika", ""], ["Sharma", "Dipti M.", ""]]}, {"id": "1604.03209", "submitter": "Vicky Zayats", "authors": "Vicky Zayats, Mari Ostendorf and Hannaneh Hajishirzi", "title": "Disfluency Detection using a Bidirectional LSTM", "comments": "Submitted to INTERSPEECH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for disfluency detection using a Bidirectional\nLong-Short Term Memory neural network (BLSTM). In addition to the word\nsequence, the model takes as input pattern match features that were developed\nto reduce sensitivity to vocabulary size in training, which lead to improved\nperformance over the word sequence alone. The BLSTM takes advantage of explicit\nrepair states in addition to the standard reparandum states. The final output\nleverages integer linear programming to incorporate constraints of disfluency\nstructure. In experiments on the Switchboard corpus, the model achieves\nstate-of-the-art performance for both the standard disfluency detection task\nand the correction detection task. Analysis shows that the model has better\ndetection of non-repetition disfluencies, which tend to be much harder to\ndetect.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 02:34:00 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Zayats", "Vicky", ""], ["Ostendorf", "Mari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1604.03249", "submitter": "Marcus Rohrbach", "authors": "Marcus Rohrbach", "title": "Attributes as Semantic Units between Natural Language and Visual\n  Recognition", "comments": "book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive progress has been made in the fields of computer vision and\nnatural language processing. However, it remains a challenge to find the best\npoint of interaction for these very different modalities. In this chapter we\ndiscuss how attributes allow us to exchange information between the two\nmodalities and in this way lead to an interaction on a semantic level.\nSpecifically we discuss how attributes allow using knowledge mined from\nlanguage resources for recognizing novel visual categories, how we can generate\nsentence description about images and video, how we can ground natural language\nin visual content, and finally, how we can answer natural language questions\nabout images.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 05:23:26 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Rohrbach", "Marcus", ""]]}, {"id": "1604.03318", "submitter": "Sazid Zaman Khan", "authors": "A.B.M. Shamsuzzaman Sadi, Towfique Anam, Mohamed Abdirazak, Abdillahi\n  Hasan Adnan, Sazid Zaman Khan, Mohamed Mahmudur Rahman, Ghassan Samara", "title": "Applying Ontological Modeling on Quranic Nature Domain", "comments": "2016 7th International Conference on Information and Communication\n  Systems (ICICS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The holy Quran is the holy book of the Muslims. It contains information about\nmany domains. Often people search for particular concepts of holy Quran based\non the relations among concepts. An ontological modeling of holy Quran can be\nuseful in such a scenario. In this paper, we have modeled nature related\nconcepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource\nDescription Framework). Our methodology involves identifying nature related\nconcepts mentioned in holy Quran and identifying relations among those\nconcepts. These concepts and relations are represented as classes/instances and\nproperties of an OWL ontology. Later, in the result section it is shown that,\nusing the Ontological model, SPARQL queries can retrieve verses and concepts of\ninterest. Thus, this modeling helps semantic search and query on the holy\nQuran. In this work, we have used English translation of the holy Quran by\nSahih International, Protege OWL Editor and for querying we have used SPARQL.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 09:27:00 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Sadi", "A. B. M. Shamsuzzaman", ""], ["Anam", "Towfique", ""], ["Abdirazak", "Mohamed", ""], ["Adnan", "Abdillahi Hasan", ""], ["Khan", "Sazid Zaman", ""], ["Rahman", "Mohamed Mahmudur", ""], ["Samara", "Ghassan", ""]]}, {"id": "1604.03357", "submitter": "Anders S{\\o}gaard Anders S{\\o}gaard", "authors": "Sigrid Klerke, Yoav Goldberg and Anders S{\\o}gaard", "title": "Improving sentence compression by learning to predict gaze", "comments": "NAACL 2016. Received Best Short Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how eye-tracking corpora can be used to improve sentence compression\nmodels, presenting a novel multi-task learning algorithm based on multi-layer\nLSTMs. We obtain performance competitive with or better than state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:57:05 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Klerke", "Sigrid", ""], ["Goldberg", "Yoav", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1604.03390", "submitter": "Marc Bola\\~nos", "authors": "\\'Alvaro Peris, Marc Bola\\~nos, Petia Radeva and Francisco Casacuberta", "title": "Video Description using Bidirectional Recurrent Neural Networks", "comments": "8 pages, 3 figures, 1 table, Submitted to International Conference on\n  Artificial Neural Networks (ICANN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although traditionally used in the machine translation field, the\nencoder-decoder framework has been recently applied for the generation of video\nand image descriptions. The combination of Convolutional and Recurrent Neural\nNetworks in these models has proven to outperform the previous state of the\nart, obtaining more accurate video descriptions. In this work we propose\npushing further this model by introducing two contributions into the encoding\nstage. First, producing richer image representations by combining object and\nlocation information from Convolutional Neural Networks and second, introducing\nBidirectional Recurrent Neural Networks for capturing both forward and backward\ntemporal relationships in the input frames.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 13:09:01 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 12:28:07 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Peris", "\u00c1lvaro", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "1604.03627", "submitter": "Norah Abokhodair", "authors": "Norah Abokhodair, Daisy Yoo, David W. McDonald", "title": "Dissecting a Social Botnet: Growth, Content and Influence in Twitter", "comments": "13 pages, 4 figures, Presented at the ACM conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW 2016)", "journal-ref": null, "doi": "10.1145/2675133.2675208", "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social botnets have become an important phenomenon on social media. There are\nmany ways in which social bots can disrupt or influence online discourse, such\nas, spam hashtags, scam twitter users, and astroturfing. In this paper we\nconsidered one specific social botnet in Twitter to understand how it grows\nover time, how the content of tweets by the social botnet differ from regular\nusers in the same dataset, and lastly, how the social botnet may have\ninfluenced the relevant discussions. Our analysis is based on a qualitative\ncoding for approximately 3000 tweets in Arabic and English from the Syrian\nsocial bot that was active for 35 weeks on Twitter before it was shutdown. We\nfind that the growth, behavior and content of this particular botnet did not\nspecifically align with common conceptions of botnets. Further we identify\ninteresting aspects of the botnet that distinguish it from regular users.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 01:00:24 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Abokhodair", "Norah", ""], ["Yoo", "Daisy", ""], ["McDonald", "David W.", ""]]}, {"id": "1604.03968", "submitter": "Francis Ferraro", "authors": "Ting-Hao (Kenneth) Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan\n  Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet\n  Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende,\n  Michel Galley, Margaret Mitchell", "title": "Visual Storytelling", "comments": "to appear in NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first dataset for sequential vision-to-language, and explore\nhow this data may be used for the task of visual storytelling. The first\nrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211\nsequences, aligned to both descriptive (caption) and story language. We\nestablish several strong baselines for the storytelling task, and motivate an\nautomatic metric to benchmark progress. Modelling concrete description as well\nas figurative and social language, as provided in this dataset and the\nstorytelling task, has the potential to move artificial intelligence from basic\nunderstandings of typical visual scenes towards more and more human-like\nunderstanding of grounded event structure and subjective expression.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 20:27:43 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Ting-Hao", "", "", "Kenneth"], ["Huang", "", ""], ["Ferraro", "Francis", ""], ["Mostafazadeh", "Nasrin", ""], ["Misra", "Ishan", ""], ["Agrawal", "Aishwarya", ""], ["Devlin", "Jacob", ""], ["Girshick", "Ross", ""], ["He", "Xiaodong", ""], ["Kohli", "Pushmeet", ""], ["Batra", "Dhruv", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""], ["Vanderwende", "Lucy", ""], ["Galley", "Michel", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1604.04358", "submitter": "Lili Mou", "authors": "Xiang Li, Lili Mou, Rui Yan, Ming Zhang", "title": "StalemateBreaker: A Proactive Content-Introducing Approach to Automatic\n  Human-Computer Conversation", "comments": "Accepted by IJCAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing open-domain human-computer conversation systems are typically\npassive: they either synthesize or retrieve a reply provided a human-issued\nutterance. It is generally presumed that humans should take the role to lead\nthe conversation and introduce new content when a stalemate occurs, and that\nthe computer only needs to \"respond.\" In this paper, we propose\nStalemateBreaker, a conversation system that can proactively introduce new\ncontent when appropriate. We design a pipeline to determine when, what, and how\nto introduce new content during human-computer conversation. We further propose\na novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between\nconversation context and candidate replies. Experiments show that both the\ncontent-introducing approach and the reranking algorithm are effective. Our\nfull StalemateBreaker model outperforms a state-of-the-practice conversation\nsystem by +14.4% p@1 when a stalemate occurs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 05:51:29 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Li", "Xiang", ""], ["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Zhang", "Ming", ""]]}, {"id": "1604.04378", "submitter": "Shengxian Wan", "authors": "Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, Xueqi\n  Cheng", "title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN", "comments": "Accepted by IJCAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic matching, which aims to determine the matching degree between two\ntexts, is a fundamental problem for many NLP applications. Recently, deep\nlearning approach has been applied to this problem and significant improvements\nhave been achieved. In this paper, we propose to view the generation of the\nglobal interaction between two texts as a recursive process: i.e. the\ninteraction of two texts at each position is a composition of the interactions\nbetween their prefixes as well as the word level interaction at the current\nposition. Based on this idea, we propose a novel deep architecture, namely\nMatch-SRNN, to model the recursive matching structure. Firstly, a tensor is\nconstructed to capture the word level interactions. Then a spatial RNN is\napplied to integrate the local interactions recursively, with importance\ndetermined by four types of gates. Finally, the matching score is calculated\nbased on the global interaction. We show that, after degenerated to the exact\nmatching scenario, Match-SRNN can approximate the dynamic programming process\nof longest common subsequence. Thus, there exists a clear interpretation for\nMatch-SRNN. Our experiments on two semantic matching tasks showed the\neffectiveness of Match-SRNN, and its ability of visualizing the learned\nmatching structure.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 07:23:53 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wan", "Shengxian", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Guo", "Jiafeng", ""], ["Pang", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1604.04383", "submitter": "Milos Cernak", "authors": "Milos Cernak, Alexandros Lazaridis, Afsaneh Asaei, Philip N. Garner", "title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate\n  Speech Coding", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  Volume: 24, Issue: 12, Dec. 2016", "doi": "10.1109/TASLP.2016.2604566", "report-no": "Idiap-RR-11-2016", "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current very low bit rate (VLBR) speech coding systems use hidden Markov\nmodel (HMM) based speech recognition/synthesis techniques. This allows\ntransmission of information (such as phonemes) segment by segment that\ndecreases the bit rate. However, the encoder based on a phoneme speech\nrecognition may create bursts of segmental errors. Segmental errors are further\npropagated to optional suprasegmental (such as syllable) information coding.\nTogether with the errors of voicing detection in pitch parametrization,\nHMM-based speech coding creates speech discontinuities and unnatural speech\nsound artefacts.\n  In this paper, we propose a novel VLBR speech coding framework based on\nneural networks (NNs) for end-to-end speech analysis and synthesis without\nHMMs. The speech coding framework relies on phonological (sub-phonetic)\nrepresentation of speech, and it is designed as a composition of deep and\nspiking NNs: a bank of phonological analysers at the transmitter, and a\nphonological synthesizer at the receiver, both realised as deep NNs, and a\nspiking NN as an incremental and robust encoder of syllable boundaries for\ncoding of continuous fundamental frequency (F0). A combination of phonological\nfeatures defines much more sound patterns than phonetic features defined by\nHMM-based speech coders, and the finer analysis/synthesis code contributes into\nsmoother encoded speech. Listeners significantly prefer the NN-based approach\ndue to fewer discontinuities and speech artefacts of the encoded speech. A\nsingle forward pass is required during the speech encoding and decoding. The\nproposed VLBR speech coding operates at a bit rate of approximately 360 bits/s.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 07:35:00 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 20:38:29 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 07:56:01 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Cernak", "Milos", ""], ["Lazaridis", "Alexandros", ""], ["Asaei", "Afsaneh", ""], ["Garner", "Philip N.", ""]]}, {"id": "1604.04562", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M.\n  Rojas-Barahona, Pei-Hao Su, Stefan Ultes, Steve Young", "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System", "comments": "published at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 16:40:49 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 14:03:58 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 10:55:12 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Vandyke", "David", ""], ["Mrksic", "Nikola", ""], ["Gasic", "Milica", ""], ["Rojas-Barahona", "Lina M.", ""], ["Su", "Pei-Hao", ""], ["Ultes", "Stefan", ""], ["Young", "Steve", ""]]}, {"id": "1604.04661", "submitter": "Shihao Ji", "authors": "Shihao Ji, Nadathur Satish, Sheng Li, and Pradeep Dubey", "title": "Parallelizing Word2Vec in Shared and Distributed Memory", "comments": "Added more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2Vec is a widely used algorithm for extracting low-dimensional vector\nrepresentations of words. It generated considerable excitement in the machine\nlearning and natural language processing (NLP) communities recently due to its\nexceptional performance in many NLP applications such as named entity\nrecognition, sentiment analysis, machine translation and question answering.\nState-of-the-art algorithms including those by Mikolov et al. have been\nparallelized for multi-core CPU architectures but are based on vector-vector\noperations that are memory-bandwidth intensive and do not efficiently use\ncomputational resources. In this paper, we improve reuse of various data\nstructures in the algorithm through the use of minibatching, hence allowing us\nto express the problem using matrix multiply operations. We also explore\ndifferent techniques to distribute word2vec computation across nodes in a\ncompute cluster, and demonstrate good strong scalability up to 32 nodes. In\ncombination, these techniques allow us to scale up the computation near\nlinearly across cores and nodes, and process hundreds of millions of words per\nsecond, which is the fastest word2vec implementation to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 23:40:04 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 17:45:00 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ji", "Shihao", ""], ["Satish", "Nadathur", ""], ["Li", "Sheng", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1604.04677", "submitter": "Allen Schmaltz", "authors": "Allen Schmaltz, Yoon Kim, Alexander M. Rush, Stuart M. Shieber", "title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence\n  Correction", "comments": "To appear at BEA11, as part of the AESW 2016 Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that an attention-based encoder-decoder model can be used for\nsentence-level grammatical error identification for the Automated Evaluation of\nScientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder\nmodels can be used for the generation of corrections, in addition to error\nidentification, which is of interest for certain end-user applications. We show\nthat a character-based encoder-decoder model is particularly effective,\noutperforming other results on the AESW Shared Task on its own, and showing\ngains over a word-based counterpart. Our final model--a combination of three\ncharacter-based encoder-decoder models, one word-based encoder-decoder model,\nand a sentence-level CNN--is the highest performing system on the AESW 2016\nbinary prediction Shared Task.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 01:49:09 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Schmaltz", "Allen", ""], ["Kim", "Yoon", ""], ["Rush", "Alexander M.", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1604.04802", "submitter": "Nazneen Fatema Rajani", "authors": "Nazneen Fatema Rajani and Raymond J. Mooney", "title": "Supervised and Unsupervised Ensembling for Knowledge Base Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results on combining supervised and unsupervised methods to\nensemble multiple systems for two popular Knowledge Base Population (KBP)\ntasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and\nLinking (TEDL). We demonstrate that our combined system along with auxiliary\nfeatures outperforms the best performing system for both tasks in the 2015\ncompetition, several ensembling baselines, as well as the state-of-the-art\nstacking approach to ensembling KBP systems. The success of our technique on\ntwo different and challenging problems demonstrates the power and generality of\nour combined approach to ensembling.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 21:18:14 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Rajani", "Nazneen Fatema", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1604.04835", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text\n  Descriptions", "comments": "Submitted to AAAI.2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is an important, long-history topic in AI, and there\nhave been a large amount of work for knowledge graph embedding which projects\nsymbolic entities and relations into low-dimensional, real-valued vector space.\nHowever, most embedding methods merely concentrate on data fitting and ignore\nthe explicit semantic expression, leading to uninterpretable representations.\nThus, traditional embedding methods have limited potentials for many\napplications such as question answering, and entity classification. To this\nend, this paper proposes a semantic representation method for knowledge graph\n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that\nglobally extracts many aspects and then locally assigns a specific category in\neach aspect for every triple. Since both aspects and categories are\nsemantics-relevant, the collection of categories in each aspect is treated as\nthe semantic representation of this triple. Extensive experiments justify our\nmodel outperforms other state-of-the-art baselines substantially.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 07:15:33 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 02:39:15 GMT"}, {"version": "v3", "created": "Sat, 17 Jun 2017 04:33:41 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1604.04873", "submitter": "Andreas Scherbakov", "authors": "Andreas Scherbakov, Ekaterina Vylomova, Fei Liu, Timothy Baldwin", "title": "From Incremental Meaning to Semantic Unit (phrase by phrase)", "comments": "7 pages, 1 figure, International Workshop on Semantic Evaluation\n  (SemEval-2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an experimental approach to Detection of Minimal\nSemantic Units and their Meaning (DiMSUM), explored within the framework of\nSemEval 2016 Task 10. The approach is primarily based on a combination of word\nembeddings and parserbased features, and employs unidirectional incremental\ncomputation of compositional embeddings for multiword expressions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 14:07:34 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Scherbakov", "Andreas", ""], ["Vylomova", "Ekaterina", ""], ["Liu", "Fei", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1604.05073", "submitter": "Daniel Beck", "authors": "Daniel Beck, Adri\\`a de Gispert, Gonzalo Iglesias, Aurelien Waite,\n  Bill Byrne", "title": "Speed-Constrained Tuning for Statistical Machine Translation Using\n  Bayesian Optimization", "comments": "To appear at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatically finding the parameters of a\nstatistical machine translation system that maximize BLEU scores while ensuring\nthat decoding speed exceeds a minimum value. We propose the use of Bayesian\nOptimization to efficiently tune the speed-related decoding parameters by\neasily incorporating speed as a noisy constraint function. The obtained\nparameter values are guaranteed to satisfy the speed constraint with an\nassociated confidence margin. Across three language pairs and two speed\nconstraint values, we report overall optimization time reduction compared to\ngrid and random search. We also show that Bayesian Optimization can decouple\nspeed and BLEU measurements, resulting in a further reduction of overall\noptimization time as speed is measured over a small subset of sentences.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 10:27:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Beck", "Daniel", ""], ["de Gispert", "Adri\u00e0", ""], ["Iglesias", "Gonzalo", ""], ["Waite", "Aurelien", ""], ["Byrne", "Bill", ""]]}, {"id": "1604.05372", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov, Mikhail Kopotev, Tatyana Sviridenko, Lyubov Ivanova", "title": "Clustering Comparable Corpora of Russian and Ukrainian Academic Texts:\n  Word Embeddings and Semantic Fingerprints", "comments": "To be presented at 9th Workshop on Building and Using Comparable\n  Corpora, co-located with LREC-2016 (https://comparable.limsi.fr/bucc2016/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our experience in applying distributional semantics (neural word\nembeddings) to the problem of representing and clustering documents in a\nbilingual comparable corpus. Our data is a collection of Russian and Ukrainian\nacademic texts, for which topics are their academic fields. In order to build\nlanguage-independent semantic representations of these documents, we train\nneural distributional models on monolingual corpora and learn the optimal\nlinear transformation of vectors from one language to another. The resulting\nvectors are then used to produce `semantic fingerprints' of documents, serving\nas input to a clustering algorithm.\n  The presented method is compared to several baselines including `orthographic\ntranslation' with Levenshtein edit distance and outperforms them by a large\nmargin. We also show that language-independent `semantic fingerprints' are\nsuperior to multi-lingual clustering algorithms proposed in the previous work,\nat the same time requiring less linguistic resources.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 22:56:13 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Kopotev", "Mikhail", ""], ["Sviridenko", "Tatyana", ""], ["Ivanova", "Lyubov", ""]]}, {"id": "1604.05468", "submitter": "Rahul Kamath", "authors": "Rahul Kamath, Masanao Ochi, Yutaka Matsuo", "title": "Understanding Rating Behaviour and Predicting Ratings by Identifying\n  Representative Users", "comments": "The 29th Pacific Asia Conference on Language, Information and\n  Computation (PACLIC-29)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online user reviews describing various products and services are now abundant\non the web. While the information conveyed through review texts and ratings is\neasily comprehensible, there is a wealth of hidden information in them that is\nnot immediately obvious. In this study, we unlock this hidden value behind user\nreviews to understand the various dimensions along which users rate products.\nWe learn a set of users that represent each of these dimensions and use their\nratings to predict product ratings. Specifically, we work with restaurant\nreviews to identify users whose ratings are influenced by dimensions like\n'Service', 'Atmosphere' etc. in order to predict restaurant ratings and\nunderstand the variation in rating behaviour across different cuisines. While\nprevious approaches to obtaining product ratings require either a large number\nof user ratings or a few review texts, we show that it is possible to predict\nratings with few user ratings and no review text. Our experiments show that our\napproach outperforms other conventional methods by 16-27% in terms of RMSE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 08:31:23 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kamath", "Rahul", ""], ["Ochi", "Masanao", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1604.05499", "submitter": "Yijia Liu", "authors": "Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, Ting Liu", "title": "Exploring Segment Representations for Neural Segmentation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language processing (NLP) tasks can be generalized into\nsegmentation problem. In this paper, we combine semi-CRF with neural network to\nsolve NLP segmentation tasks. Our model represents a segment both by composing\nthe input units and embedding the entire segment. We thoroughly study different\ncomposition functions and different segment embeddings. We conduct extensive\nexperiments on two typical segmentation tasks: named entity recognition (NER)\nand Chinese word segmentation (CWS). Experimental results show that our neural\nsemi-CRF model benefits from representing the entire segment and achieves the\nstate-of-the-art performance on CWS benchmark dataset and competitive results\non the CoNLL03 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 10:08:49 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Liu", "Yijia", ""], ["Che", "Wanxiang", ""], ["Guo", "Jiang", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""]]}, {"id": "1604.05519", "submitter": "Lingxun Meng", "authors": "Lingxun Meng and Yan Li", "title": "M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep\n  Convolutional Network for Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent works using artificial neural networks based on distributed word\nrepresentation greatly boost performance on various natural language processing\ntasks, especially the answer selection problem. Nevertheless, most of the\nprevious works used deep learning methods (like LSTM-RNN, CNN, etc.) only to\ncapture semantic representation of each sentence separately, without\nconsidering the interdependence between each other. In this paper, we propose a\nnovel end-to-end learning framework which constitutes deep convolutional neural\nnetwork based on multi-modal similarity metric learning (M$^2$S-Net) on\npairwise tokens. The proposed model demonstrates its performance by surpassing\nprevious state-of-the-art systems on the answer selection benchmark, i.e.,\nTREC-QA dataset, in both MAP and MRR metrics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 11:09:20 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 05:12:18 GMT"}, {"version": "v3", "created": "Fri, 16 Mar 2018 08:09:57 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Meng", "Lingxun", ""], ["Li", "Yan", ""]]}, {"id": "1604.05525", "submitter": "Sonse Shimaoka", "authors": "Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel", "title": "An Attentive Neural Architecture for Fine-grained Entity Type\n  Classification", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel attention-based neural network model for the\ntask of fine-grained entity type classification that unlike previously proposed\nmodels recursively composes representations of entity mention contexts. Our\nmodel achieves state-of-the-art performance with 74.94% loose micro F1-score on\nthe well-established FIGER dataset, a relative improvement of 2.59%. We also\ninvestigate the behavior of the attention mechanism of our model and observe\nthat it can learn contextual linguistic expressions that indicate the\nfine-grained category memberships of an entity.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 11:39:53 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Shimaoka", "Sonse", ""], ["Stenetorp", "Pontus", ""], ["Inui", "Kentaro", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1604.05529", "submitter": "Barbara Plank", "authors": "Barbara Plank, Anders S{\\o}gaard, Yoav Goldberg", "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term\n  Memory Models and Auxiliary Loss", "comments": "In ACL 2016 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional long short-term memory (bi-LSTM) networks have recently proven\nsuccessful for various NLP sequence modeling tasks, but little is known about\ntheir reliance to input representations, target languages, data set size, and\nlabel noise. We address these issues and evaluate bi-LSTMs with word,\ncharacter, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to\ntraditional POS taggers across languages and data sizes. We also present a\nnovel bi-LSTM model, which combines the POS tagging loss function with an\nauxiliary loss function that accounts for rare words. The model obtains\nstate-of-the-art performance across 22 languages, and works especially well for\nmorphologically complex languages. Our analysis suggests that bi-LSTMs are less\nsensitive to training data size and label corruptions (at small noise levels)\nthan previously assumed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 11:53:09 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 15:28:03 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 08:17:43 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Plank", "Barbara", ""], ["S\u00f8gaard", "Anders", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1604.05559", "submitter": "Melvyn Drag", "authors": "Melvyn Drag, Gauthaman Vasudevan", "title": "Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts", "comments": "2 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that an efficient and popular method for calculating bigram\nfrequencies is unsuitable for bodies of short texts and offer a simple\nalternative. Our method has the same computational complexity as the old method\nand offers an exact count instead of an approximation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 18:51:18 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Drag", "Melvyn", ""], ["Vasudevan", "Gauthaman", ""]]}, {"id": "1604.05747", "submitter": "Francesco Maria Elia", "authors": "Francesco Elia", "title": "Syntactic and semantic classification of verb arguments using\n  dependency-based and rich semantic features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corpus Pattern Analysis (CPA) has been the topic of Semeval 2015 Task 15,\naimed at producing a system that can aid lexicographers in their efforts to\nbuild a dictionary of meanings for English verbs using the CPA annotation\nprocess. CPA parsing is one of the subtasks which this annotation process is\nmade of and it is the focus of this report. A supervised machine-learning\napproach has been implemented, in which syntactic features derived from parse\ntrees and semantic features derived from WordNet and word embeddings are used.\nIt is shown that this approach performs well, even with the data sparsity\nissues that characterize the dataset, and can obtain better results than other\nsystem by a margin of about 4% f-score.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 20:59:32 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Elia", "Francesco", ""]]}, {"id": "1604.05781", "submitter": "Thomas McAndrew", "authors": "Thomas C. McAndrew, Joshua C. Bongard, Christopher M. Danforth, Peter\n  S. Dodds, Paul D. H. Hines, and James P. Bagrow", "title": "What we write about when we write about causality: Features of causal\n  statements across large-scale social discourse", "comments": null, "journal-ref": "2016 IEEE/ACM International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM), San Francisco, CA, 2016, pp. 519-524", "doi": "10.1109/ASONAM.2016.7752284", "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and communicating relationships between causes and effects is\nimportant for understanding our world, but is affected by language structure,\ncognitive and emotional biases, and the properties of the communication medium.\nDespite the increasing importance of social media, much remains unknown about\ncausal statements made online. To study real-world causal attribution, we\nextract a large-scale corpus of causal statements made on the Twitter social\nnetwork platform as well as a comparable random control corpus. We compare\ncausal and control statements using statistical language and sentiment analysis\ntools. We find that causal statements have a number of significant lexical and\ngrammatical differences compared with controls and tend to be more negative in\nsentiment than controls. Causal statements made online tend to focus on news\nand current events, medicine and health, or interpersonal relationships, as\nshown by topic models. By quantifying the features and potential biases of\ncausality communication, this study improves our understanding of the accuracy\nof information and opinions found online.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 01:06:50 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 06:25:48 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["McAndrew", "Thomas C.", ""], ["Bongard", "Joshua C.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter S.", ""], ["Hines", "Paul D. H.", ""], ["Bagrow", "James P.", ""]]}, {"id": "1604.05800", "submitter": "Qingyu Yin", "authors": "Qingyu Yin, Weinan Zhang, Yu Zhang, Ting Liu", "title": "A Deep Neural Network for Chinese Zero Pronoun Resolution", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for Chinese zero pronoun resolution overlook semantic\ninformation. This is because zero pronouns have no descriptive information,\nwhich results in difficulty in explicitly capturing their semantic similarities\nwith antecedents. Moreover, when dealing with candidate antecedents,\ntraditional systems simply take advantage of the local information of a single\ncandidate antecedent while failing to consider the underlying information\nprovided by the other candidates from a global perspective. To address these\nweaknesses, we propose a novel zero pronoun-specific neural network, which is\ncapable of representing zero pronouns by utilizing the contextual information\nat the semantic level. In addition, when dealing with candidate antecedents, a\ntwo-level candidate encoder is employed to explicitly capture both the local\nand global information of candidate antecedents. We conduct experiments on the\nChinese portion of the OntoNotes 5.0 corpus. Experimental results show that our\napproach substantially outperforms the state-of-the-art method in various\nexperimental settings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 03:03:12 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 02:39:45 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 22:11:38 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Yin", "Qingyu", ""], ["Zhang", "Weinan", ""], ["Zhang", "Yu", ""], ["Liu", "Ting", ""]]}, {"id": "1604.05875", "submitter": "Tiep Mai", "authors": "Tiep Mai, Bichen Shi, Patrick K. Nicholson, Deepak Ajwani, Alessandra\n  Sala", "title": "Distributed Entity Disambiguation with Per-Mention Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity disambiguation, or mapping a phrase to its canonical representation in\na knowledge base, is a fundamental step in many natural language processing\napplications. Existing techniques based on global ranking models fail to\ncapture the individual peculiarities of the words and hence, either struggle to\nmeet the accuracy requirements of many real-world applications or they are too\ncomplex to satisfy real-time constraints of applications.\n  In this paper, we propose a new disambiguation system that learns specialized\nfeatures and models for disambiguating each ambiguous phrase in the English\nlanguage. To train and validate the hundreds of thousands of learning models\nfor this purpose, we use a Wikipedia hyperlink dataset with more than 170\nmillion labelled annotations. We provide an extensive experimental evaluation\nto show that the accuracy of our approach compares favourably with respect to\nmany state-of-the-art disambiguation systems. The training required for our\napproach can be easily distributed over a cluster. Furthermore, updating our\nsystem for new entities or calibrating it for special ones is a computationally\nfast process, that does not affect the disambiguation of the other entities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 09:53:42 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Mai", "Tiep", ""], ["Shi", "Bichen", ""], ["Nicholson", "Patrick K.", ""], ["Ajwani", "Deepak", ""], ["Sala", "Alessandra", ""]]}, {"id": "1604.05878", "submitter": "Johannes Welbl", "authors": "Johannes Welbl, Guillaume Bouchard, Sebastian Riedel", "title": "A Factorization Machine Framework for Testing Bigram Embeddings in\n  Knowledgebase Completion", "comments": "accepted for AKBC 2016 workshop, 6pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding-based Knowledge Base Completion models have so far mostly combined\ndistributed representations of individual entities or relations to compute\ntruth scores of missing links. Facts can however also be represented using\npairwise embeddings, i.e. embeddings for pairs of entities and relations. In\nthis paper we explore such bigram embeddings with a flexible Factorization\nMachine model and several ablations from it. We investigate the relevance of\nvarious bigram types on the fb15k237 dataset and find relative improvements\ncompared to a compositional model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 09:58:56 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Welbl", "Johannes", ""], ["Bouchard", "Guillaume", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1604.06045", "submitter": "Jason  Weston", "authors": "Jason Weston", "title": "Dialog-based Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-term goal of machine learning research is to build an intelligent\ndialog agent. Most research in natural language understanding has focused on\nlearning from fixed training sets of labeled data, with supervision either at\nthe word level (tagging, parsing tasks) or sentence level (question answering,\nmachine translation). This kind of supervision is not realistic of how humans\nlearn, where language is both learned by, and used for, communication. In this\nwork, we study dialog-based language learning, where supervision is given\nnaturally and implicitly in the response of the dialog partner during the\nconversation. We study this setup in two domains: the bAbI dataset of (Weston\net al., 2015) and large-scale question answering from (Dodge et al., 2015). We\nevaluate a set of baseline learning strategies on these tasks, and show that a\nnovel model incorporating predictive lookahead is a promising approach for\nlearning from a teacher's response. In particular, a surprising result is that\nit can learn to answer questions correctly without any reward-based supervision\nat all.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 18:06:49 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 18:27:03 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 14:02:08 GMT"}, {"version": "v4", "created": "Fri, 20 May 2016 02:53:30 GMT"}, {"version": "v5", "created": "Tue, 23 Aug 2016 18:46:16 GMT"}, {"version": "v6", "created": "Wed, 28 Sep 2016 21:30:27 GMT"}, {"version": "v7", "created": "Mon, 24 Oct 2016 20:00:13 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Weston", "Jason", ""]]}, {"id": "1604.06076", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren\n  Etzioni and Dan Roth", "title": "Question Answering via Integer Programming over Semi-Structured\n  Knowledge", "comments": "Extended version of the paper accepted to IJCAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering science questions posed in natural language is an important AI\nchallenge. Answering such questions often requires non-trivial inference and\nknowledge that goes beyond factoid retrieval. Yet, most systems for this task\nare based on relatively shallow Information Retrieval (IR) and statistical\ncorrelation techniques operating on large unstructured corpora. We propose a\nstructured inference system for this task, formulated as an Integer Linear\nProgram (ILP), that answers natural language questions using a semi-structured\nknowledge base derived from text, including questions requiring multi-step\ninference and a combination of multiple facts. On a dataset of real, unseen\nscience questions, our system significantly outperforms (+14%) the best\nprevious attempt at structured reasoning for this task, which used Markov Logic\nNetworks (MLNs). It also improves upon a previous ILP formulation by 17.7%.\nWhen combined with unstructured inference methods, the ILP system significantly\nboosts overall performance (+10%). Finally, we show our approach is\nsubstantially more robust to a simple answer perturbation compared to\nstatistical correlation methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 19:48:07 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Khashabi", "Daniel", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Clark", "Peter", ""], ["Etzioni", "Oren", ""], ["Roth", "Dan", ""]]}, {"id": "1604.06113", "submitter": "Wei Chu", "authors": "Wei Chu, Ruxin Chen", "title": "Speaker Cluster-Based Speaker Adaptive Training for Deep Neural Network\n  Acoustic Modeling", "comments": "Published at IEEE ICASSP 2015", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472688", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A speaker cluster-based speaker adaptive training (SAT) method under deep\nneural network-hidden Markov model (DNN-HMM) framework is presented in this\npaper. During training, speakers that are acoustically adjacent to each other\nare hierarchically clustered using an i-vector based distance metric. DNNs with\nspeaker dependent layers are then adaptively trained for each cluster of\nspeakers. Before decoding starts, an unseen speaker in test set is matched to\nthe closest speaker cluster through comparing i-vector based distances. The\npreviously trained DNN of the matched speaker cluster is used for decoding\nutterances of the test speaker. The performance of the proposed method on a\nlarge vocabulary spontaneous speech recognition task is evaluated on a training\nset of with 1500 hours of speech, and a test set of 24 speakers with 1774\nutterances. Comparing to a speaker independent DNN with a baseline word error\nrate of 11.6%, a relative 6.8% reduction in word error rate is observed from\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 20:10:41 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chu", "Wei", ""], ["Chen", "Ruxin", ""]]}, {"id": "1604.06225", "submitter": "Ido Kissos", "authors": "Ido Kissos, Nachum Dershowitz", "title": "OCR Error Correction Using Character Correction and Feature-Based Word\n  Classification", "comments": "Proceedings of the 12th IAPR International Workshop on Document\n  Analysis Systems (DAS2016), Santorini, Greece, April 11-14, 2016", "journal-ref": "Proceedings of the 12th IAPR International Workshop on Document\n  Analysis Systems (DAS 2016), Santorini, Greece, pp. 198-203 (2016)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of a learned classifier for post-OCR text\ncorrection. Experiments with the Arabic language show that this approach, which\nintegrates a weighted confusion matrix and a shallow language model, improves\nthe vast majority of segmentation and recognition errors, the most frequent\ntypes of error on our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 09:25:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kissos", "Ido", ""], ["Dershowitz", "Nachum", ""]]}, {"id": "1604.06274", "submitter": "Tianyi Luo", "authors": "Qixin Wang, Tianyi Luo, Dong Wang, Chao Xing", "title": "Chinese Song Iambics Generation with Neural Attention-based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and generating Chinese poems is a charming yet challenging task.\nTraditional approaches involve various language modeling and machine\ntranslation techniques, however, they perform not as well when generating poems\nwith complex pattern constraints, for example Song iambics, a famous type of\npoems that involve variable-length sentences and strict rhythmic patterns. This\npaper applies the attention-based sequence-to-sequence model to generate\nChinese Song iambics. Specifically, we encode the cue sentences by a\nbi-directional Long-Short Term Memory (LSTM) model and then predict the entire\niambic with the information provided by the encoder, in the form of an\nattention-based LSTM that can regularize the generation process by the fine\nstructure of the input cues. Several techniques are investigated to improve the\nmodel, including global context integration, hybrid style training, character\nvector initialization and adaptation. Both the automatic and subjective\nevaluation results show that our model indeed can learn the complex structural\nand rhythmic patterns of Song iambics, and the generation is rather successful.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 12:25:04 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 20:21:32 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Wang", "Qixin", ""], ["Luo", "Tianyi", ""], ["Wang", "Dong", ""], ["Xing", "Chao", ""]]}, {"id": "1604.06285", "submitter": "Longyue Wang", "authors": "Longyue Wang, Zhaopeng Tu, Xiaojun Zhang, Hang Li, Andy Way, Qun Liu", "title": "A Novel Approach to Dropped Pronoun Translation", "comments": "To appear in NAACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dropped Pronouns (DP) in which pronouns are frequently dropped in the source\nlanguage but should be retained in the target language are challenge in machine\ntranslation. In response to this problem, we propose a semi-supervised approach\nto recall possibly missing pronouns in the translation. Firstly, we build\ntraining data for DP generation in which the DPs are automatically labelled\naccording to the alignment information from a parallel corpus. Secondly, we\nbuild a deep learning-based DP generator for input sentences in decoding when\nno corresponding references exist. More specifically, the generation is\ntwo-phase: (1) DP position detection, which is modeled as a sequential\nlabelling task with recurrent neural networks; and (2) DP prediction, which\nemploys a multilayer perceptron with rich features. Finally, we integrate the\nabove outputs into our translation system to recall missing pronouns by both\nextracting rules from the DP-labelled training data and translating the\nDP-generated input sentences. Experimental results show that our approach\nachieves a significant improvement of 1.58 BLEU points in translation\nperformance with 66% F-score for DP generation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 12:55:29 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Wang", "Longyue", ""], ["Tu", "Zhaopeng", ""], ["Zhang", "Xiaojun", ""], ["Li", "Hang", ""], ["Way", "Andy", ""], ["Liu", "Qun", ""]]}, {"id": "1604.06361", "submitter": "Patrick Verga", "authors": "Patrick Verga and Andrew McCallum", "title": "Row-less Universal Schema", "comments": "AKBC 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal schema jointly embeds knowledge bases and textual patterns to\nreason about entities and relations for automatic knowledge base construction\nand information extraction. In the past, entity pairs and relations were\nrepresented as learned vectors with compatibility determined by a scoring\nfunction, limiting generalization to unseen text patterns and entities.\nRecently, 'column-less' versions of Universal Schema have used compositional\npattern encoders to generalize to all text patterns. In this work we take the\nnext step and propose a 'row-less' model of universal schema, removing explicit\nentity pair representations. Instead of learning vector representations for\neach entity pair in our training set, we treat an entity pair as a function of\nits relation types. In experimental results on the FB15k-237 benchmark we\ndemonstrate that we can match the performance of a comparable model with\nexplicit entity pair representations using a model of attention over relation\ntypes. We further demonstrate that the model per- forms with nearly the same\naccuracy on entity pairs never seen during training.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 15:39:01 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Verga", "Patrick", ""], ["McCallum", "Andrew", ""]]}, {"id": "1604.06529", "submitter": "Adhiguna Kuncoro", "authors": "Adhiguna Kuncoro, Yuichiro Sawai, Kevin Duh, Yuji Matsumoto", "title": "Dependency Parsing with LSTMs: An Empirical Evaluation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transition-based dependency parser using Recurrent Neural\nNetworks with Long Short-Term Memory (LSTM) units. This extends the feedforward\nneural network parser of Chen and Manning (2014) and enables modelling of\nentire sequences of shift/reduce transition decisions. On the Google Web\nTreebank, our LSTM parser is competitive with the best feedforward parser on\noverall accuracy and notably achieves more than 3% improvement for long-range\ndependencies, which has proved difficult for previous transition-based parsers\ndue to error propagation and limited context information. Our findings\nadditionally suggest that dropout regularisation on the embedding layer is\ncrucial to improve the LSTM's generalisation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 03:20:24 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 04:23:07 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Kuncoro", "Adhiguna", ""], ["Sawai", "Yuichiro", ""], ["Duh", "Kevin", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1604.06583", "submitter": "Ildik\\'o Pil\\'an", "authors": "Elena Volodina and Ildik\\'o Pil\\'an and Ingegerd Enstr\\\"om and Lorena\n  Llozhi and Peter Lundkvist and Gunl\\\"og Sundberg and Monica Sandell", "title": "SweLL on the rise: Swedish Learner Language corpus for European\n  Reference Level studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new resource for Swedish, SweLL, a corpus of Swedish Learner\nessays linked to learners' performance according to the Common European\nFramework of Reference (CEFR). SweLL consists of three subcorpora - SpIn,\nSW1203 and Tisus, collected from three different educational establishments.\nThe common metadata for all subcorpora includes age, gender, native languages,\ntime of residence in Sweden, type of written task. Depending on the subcorpus,\nlearner texts may contain additional information, such as text genres, topics,\ngrades. Five of the six CEFR levels are represented in the corpus: A1, A2, B1,\nB2 and C1 comprising in total 339 essays. C2 level is not included since\ncourses at C2 level are not offered. The work flow consists of collection of\nessays and permits, essay digitization and registration, meta-data annotation,\nautomatic linguistic annotation. Inter-rater agreement is presented on the\nbasis of SW1203 subcorpus. The work on SweLL is still ongoing with more than\n100 essays waiting in the pipeline. This article both describes the resource\nand the \"how-to\" behind the compilation of SweLL.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 09:19:07 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Volodina", "Elena", ""], ["Pil\u00e1n", "Ildik\u00f3", ""], ["Enstr\u00f6m", "Ingegerd", ""], ["Llozhi", "Lorena", ""], ["Lundkvist", "Peter", ""], ["Sundberg", "Gunl\u00f6g", ""], ["Sandell", "Monica", ""]]}, {"id": "1604.06635", "submitter": "Xipeng Qiu", "authors": "Peng Qian, Xipeng Qiu, Xuanjing Huang", "title": "Bridging LSTM Architecture and the Neural Dynamics during Reading", "comments": "25th International Joint Conference on Artificial Intelligence\n  IJCAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the long short-term memory neural network (LSTM) has attracted wide\ninterest due to its success in many tasks. LSTM architecture consists of a\nmemory cell and three gates, which looks similar to the neuronal networks in\nthe brain. However, there still lacks the evidence of the cognitive\nplausibility of LSTM architecture as well as its working mechanism. In this\npaper, we study the cognitive plausibility of LSTM by aligning its internal\narchitecture with the brain activity observed via fMRI when the subjects read a\nstory. Experiment results show that the artificial memory vector in LSTM can\naccurately predict the observed sequential brain activities, indicating the\ncorrelation between LSTM architecture and the cognitive process of story\nreading.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:51:11 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Qian", "Peng", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1604.06648", "submitter": "Denis Gordeev", "authors": "Denis Gordeev", "title": "Automatic verbal aggression detection for Russian and American\n  imageboards", "comments": "Procedia - Social and Behavioral Sciences (06.12.15) at Mephi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of aggression for Internet communities is rampant. Anonymous\nforums usually called imageboards are notorious for their aggressive and\ndeviant behaviour even in comparison with other Internet communities. This\nstudy is aimed at studying ways of automatic detection of verbal expression of\naggression for the most popular American (4chan.org) and Russian (2ch.hk)\nimageboards. A set of 1,802,789 messages was used for this study. The machine\nlearning algorithm word2vec was applied to detect the state of aggression. A\ndecent result is obtained for English (88%), the results for Russian are yet to\nbe improved.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 13:25:14 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Gordeev", "Denis", ""]]}, {"id": "1604.06650", "submitter": "Denis Gordeev", "authors": "Rodmonga Potapova and Denis Gordeev", "title": "Detecting state of aggression in sentences using CNN", "comments": "submitted for SPECOM-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study verbal expression of aggression and its detection\nusing machine learning and neural networks methods. We test our results using\nour corpora of messages from anonymous imageboards. We also compare Random\nforest classifier with convolutional neural network for \"Movie reviews with one\nsentence per review\" corpus.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 13:33:08 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Potapova", "Rodmonga", ""], ["Gordeev", "Denis", ""]]}, {"id": "1604.06721", "submitter": "Manfred Eppe", "authors": "Manfred Eppe, Sean Trott, Jerome Feldman", "title": "Exploiting Deep Semantics and Compositionality of Natural Language for\n  Human-Robot-Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a natural language interface for human robot interaction that\nimplements reasoning about deep semantics in natural language. To realize the\nrequired deep analysis, we employ methods from cognitive linguistics, namely\nthe modular and compositional framework of Embodied Construction Grammar (ECG)\n[Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference\nresolution problems and other issues related to deep semantics and\ncompositionality of natural language. This also includes verbal interaction\nwith humans to clarify commands and queries that are too ambiguous to be\nexecuted safely. We implement our NLU framework as a ROS package and present\nproof-of-concept scenarios with different robots, as well as a survey on the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:58:18 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Eppe", "Manfred", ""], ["Trott", "Sean", ""], ["Feldman", "Jerome", ""]]}, {"id": "1604.06896", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Hinrich Sch\\\"utze", "title": "Why and How to Pay Different Attention to Phrase Alignments of Different\n  Intensities", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies comparatively two typical sentence pair classification\ntasks: textual entailment (TE) and answer selection (AS), observing that phrase\nalignments of different intensities contribute differently in these tasks. We\naddress the problems of identifying phrase alignments of flexible granularity\nand pooling alignments of different intensities for these tasks. Examples for\nflexible granularity are alignments between two single words, between a single\nword and a phrase and between a short phrase and a long phrase. By intensity we\nroughly mean the degree of match, it ranges from identity over surface-form\nco-occurrence, rephrasing and other semantic relatedness to unrelated words as\nin lots of parenthesis text. Prior work (i) has limitations in phrase\ngeneration and representation, or (ii) conducts alignment at word and phrase\nlevels by handcrafted features or (iii) utilizes a single attention mechanism\nover alignment intensities without considering the characteristics of specific\ntasks, which limits the system's effectiveness across tasks. We propose an\narchitecture based on Gated Recurrent Unit that supports (i) representation\nlearning of phrases of arbitrary granularity and (ii) task-specific focusing of\nphrase alignments between two sentences by attention pooling. Experimental\nresults on TE and AS match our observation and are state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 11:53:43 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 06:41:21 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1604.06952", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Giuseppe Iurato", "title": "Visualization of Jacques Lacan's Registers of the Psychoanalytic Field,\n  and Discovery of Metaphor and of Metonymy. Analytical Case Study of Edgar\n  Allan Poe's \"The Purloined Letter\"", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start with a description of Lacan's work that we then take into our\nanalytics methodology. In a first investigation, a Lacan-motivated template of\nthe Poe story is fitted to the data. A segmentation of the storyline is used in\norder to map out the diachrony. Based on this, it will be shown how synchronous\naspects, potentially related to Lacanian registers, can be sought. This\ndemonstrates the effectiveness of an approach based on a model template of the\nstoryline narrative. In a second and more comprehensive investigation, we\ndevelop an approach for revealing, that is, uncovering, Lacanian register\nrelationships. Objectives of this work include the wide and general application\nof our methodology. This methodology is strongly based on the \"letting the data\nspeak\" Correspondence Analysis analytics platform of Jean-Paul Benz\\'ecri, that\nis also the geometric data analysis, both qualitative and quantitative\nanalytics, developed by Pierre Bourdieu.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 20:48:50 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 20:54:20 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 21:33:18 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Murtagh", "Fionn", ""], ["Iurato", "Giuseppe", ""]]}, {"id": "1604.07236", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam\n  Tsakalidis", "title": "Towards Real-Time, Country-Level Location Classification of Worldwide\n  Tweets", "comments": "Accepted for publication in IEEE Transactions on Knowledge and Data\n  Engineering (IEEE TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to much previous work that has focused on location classification\nof tweets restricted to a specific country, here we undertake the task in a\nbroader context by classifying global tweets at the country level, which is so\nfar unexplored in a real-time scenario. We analyse the extent to which a\ntweet's country of origin can be determined by making use of eight\ntweet-inherent features for classification. Furthermore, we use two datasets,\ncollected a year apart from each other, to analyse the extent to which a model\ntrained from historical tweets can still be leveraged for classification of new\ntweets. With classification experiments on all 217 countries in our datasets,\nas well as on the top 25 countries, we offer some insights into the best use of\ntweet-inherent features for an accurate country-level classification of tweets.\nWe find that the use of a single feature, such as the use of tweet content\nalone -- the most widely used feature in previous work -- leaves much to be\ndesired. Choosing an appropriate combination of both tweet content and metadata\ncan actually lead to substantial improvements of between 20\\% and 50\\%. We\nobserve that tweet content, the user's self-reported location and the user's\nreal name, all of which are inherent in a tweet and available in a real-time\nscenario, are particularly useful to determine the country of origin. We also\nexperiment on the applicability of a model trained on historical tweets to\nclassify new tweets, finding that the choice of a particular combination of\nfeatures whose utility does not fade over time can actually lead to comparable\nperformance, avoiding the need to retrain. However, the difficulty of achieving\naccurate classification increases slightly for countries with multiple\ncommonalities, especially for English and Spanish speaking countries.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 12:50:50 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 18:35:36 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 11:03:05 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Voss", "Alex", ""], ["Procter", "Rob", ""], ["Liakata", "Maria", ""], ["Wang", "Bo", ""], ["Tsakalidis", "Adam", ""]]}, {"id": "1604.07370", "submitter": "Christian Stab", "authors": "Christian Stab and Iryna Gurevych", "title": "Parsing Argumentation Structures in Persuasive Essays", "comments": "Under review in Computational Linguistics. First submission: 26\n  October 2015. Revised submission: 15 July 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a novel approach for parsing argumentation\nstructures. We identify argument components using sequence labeling at the\ntoken level and apply a new joint model for detecting argumentation structures.\nThe proposed model globally optimizes argument component types and\nargumentative relations using integer linear programming. We show that our\nmodel considerably improves the performance of base classifiers and\nsignificantly outperforms challenging heuristic baselines. Moreover, we\nintroduce a novel corpus of persuasive essays annotated with argumentation\nstructures. We show that our annotation scheme and annotation guidelines\nsuccessfully guide human annotators to substantial agreement. This corpus and\nthe annotation guidelines are freely available for ensuring reproducibility and\nto encourage future research in computational argumentation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 19:19:04 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 11:55:03 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Stab", "Christian", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1604.07407", "submitter": "Vlad Niculae", "authors": "Vlad Niculae and Cristian Danescu-Niculescu-Mizil", "title": "Conversational Markers of Constructive Discussions", "comments": "To appear at NAACL-HLT 2016. 11pp, 5 fig. Data and other info\n  available at http://vene.ro/constructive/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group discussions are essential for organizing every aspect of modern life,\nfrom faculty meetings to senate debates, from grant review panels to papal\nconclaves. While costly in terms of time and organization effort, group\ndiscussions are commonly seen as a way of reaching better decisions compared to\nsolutions that do not require coordination between the individuals (e.g.\nvoting)---through discussion, the sum becomes greater than the parts. However,\nthis assumption is not irrefutable: anecdotal evidence of wasteful discussions\nabounds, and in our own experiments we find that over 30% of discussions are\nunproductive.\n  We propose a framework for analyzing conversational dynamics in order to\ndetermine whether a given task-oriented discussion is worth having or not. We\nexploit conversational patterns reflecting the flow of ideas and the balance\nbetween the participants, as well as their linguistic choices. We apply this\nframework to conversations naturally occurring in an online collaborative world\nexploration game developed and deployed to support this research. Using this\nsetting, we show that linguistic cues and conversational patterns extracted\nfrom the first 20 seconds of a team discussion are predictive of whether it\nwill be a wasteful or a productive one.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 20:00:02 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Niculae", "Vlad", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1604.07809", "submitter": "Federico Nanni", "authors": "Federico Nanni and Pablo Ruiz Fabo", "title": "Entities as topic labels: Improving topic interpretability and\n  evaluability combining Entity Linking and Labeled LDA", "comments": "in Proceedings of Digital Humanities 2016, Krakow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to create a corpus exploration method providing topics that are\neasier to interpret than standard LDA topic models, here we propose combining\ntwo techniques called Entity linking and Labeled LDA. Our method identifies in\nan ontology a series of descriptive labels for each document in a corpus. Then\nit generates a specific topic for each label. Having a direct relation between\ntopics and labels makes interpretation easier; using an ontology as background\nknowledge limits label ambiguity. As our topics are described with a limited\nnumber of clear-cut labels, they promote interpretability, and this may help\nquantitative evaluation. We illustrate the potential of the approach by\napplying it in order to define the most relevant topics addressed by each party\nin the European Parliament's fifth mandate (1999-2004).\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 19:31:08 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Nanni", "Federico", ""], ["Fabo", "Pablo Ruiz", ""]]}, {"id": "1604.08095", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Yingyi Tan, Aravind Ganapathiraju", "title": "Accent Classification with Phonetic Vowel Representation", "comments": "Asian Conference on Pattern Recognition (ACPR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous accent classification research focused mainly on detecting accents\nwith pure acoustic information without recognizing accented speech. This work\ncombines phonetic knowledge such as vowels with acoustic information to build\nGuassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)\nfeatures, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With\ninput about 20-second accented speech, this system achieves classification rate\nof 51% on a 7-way classification system focusing on the major types of accents\nin English, which is competitive to the state-of-the-art results in this field.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 02:50:44 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Ge", "Zhenhao", ""], ["Tan", "Yingyi", ""], ["Ganapathiraju", "Aravind", ""]]}, {"id": "1604.08120", "submitter": "Paramita Mirza", "authors": "Paramita Mirza", "title": "Extracting Temporal and Causal Relations between Events", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured information resulting from temporal information processing is\ncrucial for a variety of natural language processing tasks, for instance to\ngenerate timeline summarization of events from news documents, or to answer\ntemporal/causal-related questions about some events. In this thesis we present\na framework for an integrated temporal and causal relation extraction system.\nWe first develop a robust extraction component for each type of relations, i.e.\ntemporal order and causality. We then combine the two extraction components\ninto an integrated relation extraction system, CATENA---CAusal and Temporal\nrelation Extraction from NAtural language texts---, by utilizing the\npresumption about event precedence in causality, that causing events must\nhappened BEFORE resulting events. Several resources and techniques to improve\nour relation extraction systems are also discussed, including word embeddings\nand training data expansion. Finally, we report our adaptation efforts of\ntemporal information processing for languages other than English, namely\nItalian and Indonesian.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 15:52:18 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Mirza", "Paramita", ""]]}, {"id": "1604.08242", "submitter": "George Saon", "authors": "George Saon, Tom Sercu, Steven Rennie and Hong-Kwang J. Kuo", "title": "The IBM 2016 English Conversational Telephone Speech Recognition System", "comments": "Submitted to Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a collection of acoustic and language modeling techniques that\nlowered the word error rate of our English conversational telephone LVCSR\nsystem to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation\ntestset. On the acoustic side, we use a score fusion of three strong models:\nrecurrent nets with maxout activations, very deep convolutional nets with 3x3\nkernels, and bidirectional long short-term memory nets which operate on FMLLR\nand i-vector features. On the language modeling side, we use an updated model\n\"M\" and hierarchical neural network LMs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 21:00:03 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:30:37 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Saon", "George", ""], ["Sercu", "Tom", ""], ["Rennie", "Steven", ""], ["Kuo", "Hong-Kwang J.", ""]]}, {"id": "1604.08504", "submitter": "Yao Lu", "authors": "Linqing Liu, Yao Lu, Ye Luo, Renxian Zhang, Laurent Itti and Jianwei\n  Lu", "title": "Detecting \"Smart\" Spammers On Social Network: A Topic Model Approach", "comments": "NAACL-HLT 2016, Student Research Workshop", "journal-ref": null, "doi": "10.18653/v1/N16-2007", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spammer detection on social network is a challenging problem. The rigid\nanti-spam rules have resulted in emergence of \"smart\" spammers. They resemble\nlegitimate users who are difficult to identify. In this paper, we present a\nnovel spammer classification approach based on Latent Dirichlet\nAllocation(LDA), a topic model. Our approach extracts both the local and the\nglobal information of topic distribution patterns, which capture the essence of\nspamming. Tested on one benchmark dataset and one self-collected dataset, our\nproposed method outperforms other state-of-the-art methods in terms of averaged\nF1-score.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 16:36:35 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 06:50:36 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Liu", "Linqing", ""], ["Lu", "Yao", ""], ["Luo", "Ye", ""], ["Zhang", "Renxian", ""], ["Itti", "Laurent", ""], ["Lu", "Jianwei", ""]]}, {"id": "1604.08561", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Mohammad R.K. Mofrad", "title": "Comparing Fifty Natural Languages and Twelve Genetic Languages Using\n  Word Embedding Language Divergence (WELD) as a Quantitative Measure of\n  Language Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new measure of distance between languages based on word\nembedding, called word embedding language divergence (WELD). WELD is defined as\ndivergence between unified similarity distribution of words between languages.\nUsing such a measure, we perform language comparison for fifty natural\nlanguages and twelve genetic languages. Our natural language dataset is a\ncollection of sentence-aligned parallel corpora from bible translations for\nfifty languages spanning a variety of language families. Although we use\nparallel corpora, which guarantees having the same content in all languages,\ninterestingly in many cases languages within the same family cluster together.\nIn addition to natural languages, we perform language comparison for the coding\nregions in the genomes of 12 different organisms (4 plants, 6 animals, and two\nhuman subjects). Our result confirms a significant high-level difference in the\ngenetic language model of humans/animals versus plants. The proposed method is\na step toward defining a quantitative measure of similarity between languages,\nwith applications in languages classification, genre identification, dialect\nidentification, and evaluation of translations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 19:10:47 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Mofrad", "Mohammad R. K.", ""]]}, {"id": "1604.08633", "submitter": "Allen Schmaltz", "authors": "Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber", "title": "Word Ordering Without Syntax", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on word ordering has argued that syntactic structure is\nimportant, or even required, for effectively recovering the order of a\nsentence. We find that, in fact, an n-gram language model with a simple\nheuristic gives strong results on this task. Furthermore, we show that a long\nshort-term memory (LSTM) language model is even more effective at recovering\norder, with our basic model outperforming a state-of-the-art syntactic model by\n11.5 BLEU points. Additional data and larger beams yield further gains, at the\nexpense of training and search time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 22:09:49 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 03:41:45 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Schmaltz", "Allen", ""], ["Rush", "Alexander M.", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1604.08672", "submitter": "Shufeng Xiong", "authors": "Shufeng Xiong, Yue Zhang, Donghong Ji, Yinxia Lou", "title": "Distance Metric Learning for Aspect Phrase Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect phrase grouping is an important task in aspect-level sentiment\nanalysis. It is a challenging problem due to polysemy and context dependency.\nWe propose an Attention-based Deep Distance Metric Learning (ADDML) method, by\nconsidering aspect phrase representation as well as context representation.\nFirst, leveraging the characteristics of the review text, we automatically\ngenerate aspect phrase sample pairs for distant supervision. Second, we feed\nword embeddings of aspect phrases and their contexts into an attention-based\nneural network to learn feature representation of contexts. Both aspect phrase\nembedding and context embedding are used to learn a deep feature subspace for\nmeasure the distances between aspect phrases for K-means clustering.\nExperiments on four review datasets show that the proposed method outperforms\nstate-of-the-art strong baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 02:44:02 GMT"}, {"version": "v2", "created": "Sun, 30 Oct 2016 02:09:15 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Xiong", "Shufeng", ""], ["Zhang", "Yue", ""], ["Ji", "Donghong", ""], ["Lou", "Yinxia", ""]]}, {"id": "1604.08781", "submitter": "Joseph Corneli", "authors": "Joseph Corneli and Miriam Corneli", "title": "Teaching natural language to computers", "comments": "6 pages, including 1 figure and 3 tables; accepted for presentation\n  at IJCAI2016 Workshop on Language Sense on Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"Natural Language,\" whether spoken and attended to by humans, or processed\nand generated by computers, requires networked structures that reflect creative\nprocesses in semantic, syntactic, phonetic, linguistic, social, emotional, and\ncultural modules. Being able to produce novel and useful behavior following\nrepeated practice gets to the root of both artificial intelligence and human\nlanguage. This paper investigates the modalities involved in language-like\napplications that computers -- and programmers -- engage with, and aims to fine\ntune the questions we ask to better account for context, self-awareness, and\nembodiment.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 11:36:25 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 05:29:24 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Corneli", "Joseph", ""], ["Corneli", "Miriam", ""]]}]