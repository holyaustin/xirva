[{"id": "1206.0042", "submitter": "Megan Belzner", "authors": "Megan Belzner, Sean Colin-Ellerin and Jorge H. Roman", "title": "Language Acquisition in Computers", "comments": "39 pages, 10 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project explores the nature of language acquisition in computers, guided\nby techniques similar to those used in children. While existing natural\nlanguage processing methods are limited in scope and understanding, our system\naims to gain an understanding of language from first principles and hence\nminimal initial input. The first portion of our system was implemented in Java\nand is focused on understanding the morphology of language using bigrams. We\nuse frequency distributions and differences between them to define and\ndistinguish languages. English and French texts were analyzed to determine a\ndifference threshold of 55 before the texts are considered to be in different\nlanguages, and this threshold was verified using Spanish texts. The second\nportion of our system focuses on gaining an understanding of the syntax of a\nlanguage using a recursive method. The program uses one of two possible methods\nto analyze given sentences based on either sentence patterns or surrounding\nwords. Both methods have been implemented in C++. The program is able to\nunderstand the structure of simple sentences and learn new words. In addition,\nwe have provided some suggestions regarding future work and potential\nextensions of the existing program.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2012 22:02:32 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Belzner", "Megan", ""], ["Colin-Ellerin", "Sean", ""], ["Roman", "Jorge H.", ""]]}, {"id": "1206.0377", "submitter": "Zoltan Szabo", "authors": "Balazs Pinter, Gyula Voros, Zoltan Szabo, Andras Lorincz", "title": "Automated Word Puzzle Generation via Topic Dictionaries", "comments": "4 pages", "journal-ref": "International Conference on Machine Learning (ICML-2012) -\n  Sparsity, Dictionaries and Projections in Machine Learning and Signal\n  Processing Workshop, Edinburgh, Scotland, 30 June 2012", "doi": null, "report-no": null, "categories": "cs.CL math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method for automated word puzzle generation. Contrary to\nprevious approaches in this novel field, the presented method does not rely on\nhighly structured datasets obtained with serious human annotation effort: it\nonly needs an unstructured and unannotated corpus (i.e., document collection)\nas input. The method builds upon two additional pillars: (i) a topic model,\nwhich induces a topic dictionary from the input corpus (examples include e.g.,\nlatent semantic analysis, group-structured dictionaries or latent Dirichlet\nallocation), and (ii) a semantic similarity measure of word pairs. Our method\ncan (i) generate automatically a large number of proper word puzzles of\ndifferent types, including the odd one out, choose the related word and\nseparate the topics puzzle. (ii) It can easily create domain-specific puzzles\nby replacing the corpus component. (iii) It is also capable of automatically\ngenerating puzzles with parameterizable levels of difficulty suitable for,\ne.g., beginners or intermediate learners.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 13:11:17 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Pinter", "Balazs", ""], ["Voros", "Gyula", ""], ["Szabo", "Zoltan", ""], ["Lorincz", "Andras", ""]]}, {"id": "1206.0381", "submitter": "Shamim Ripon", "authors": "Md. Nawab Yousuf Ali, Shamim Ripon and Shaikh Muhammad Allayear", "title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach", "comments": "7 pages, International Journal of Computer Science Issues (IJCSI),\n  Volume 9, Issue 3 May 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 13:23:18 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Ali", "Md. Nawab Yousuf", ""], ["Ripon", "Shamim", ""], ["Allayear", "Shaikh Muhammad", ""]]}, {"id": "1206.1066", "submitter": "Lillian Lee", "authors": "Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian\n  Danescu-Niculescu-Mizil, and Jennifer Spindel", "title": "Hedge detection as a lens on framing in the GMO debates: A position\n  paper", "comments": "10 pp; to appear in Proceedings of the ACL Workshop on\n  Extra-Propositional Aspects of Meaning in Computational Linguistics, 2012.\n  Data available at\n  https://confluence.cornell.edu/display/llresearch/HedgingFramingGMOs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the ways in which participants in public discussions frame\ntheir arguments is important in understanding how public opinion is formed. In\nthis paper, we adopt the position that it is time for more\ncomputationally-oriented research on problems involving framing. In the\ninterests of furthering that goal, we propose the following specific,\ninteresting and, we believe, relatively accessible question: In the controversy\nregarding the use of genetically-modified organisms (GMOs) in agriculture, do\npro- and anti-GMO articles differ in whether they choose to adopt a\n\"scientific\" tone?\n  Prior work on the rhetoric and sociology of science suggests that hedging may\ndistinguish popular-science text from text written by professional scientists\nfor their colleagues. We propose a detailed approach to studying whether hedge\ndetection can be used to understanding scientific framing in the GMO debates,\nand provide corpora to facilitate this study. Some of our preliminary analyses\nsuggest that hedges occur less frequently in scientific discourse than in\npopular text, a finding that contradicts prior assertions in the literature. We\nhope that our initial work and data will encourage others to pursue this\npromising line of inquiry.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 20:15:35 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Choi", "Eunsol", ""], ["Tan", "Chenhao", ""], ["Lee", "Lillian", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Spindel", "Jennifer", ""]]}, {"id": "1206.1069", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Liane Gabora and Sandro Sozzo", "title": "Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human\n  Thought", "comments": "31 pages, 5 figures", "journal-ref": "Topics in Cognitive Science, 5, pp. 737-772, 2013", "doi": "10.1111/tops.12042", "report-no": null, "categories": "cs.AI cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze different aspects of our quantum modeling approach of human\nconcepts, and more specifically focus on the quantum effects of contextuality,\ninterference, entanglement and emergence, illustrating how each of them makes\nits appearance in specific situations of the dynamics of human concepts and\ntheir combinations. We point out the relation of our approach, which is based\non an ontology of a concept as an entity in a state changing under influence of\na context, with the main traditional concept theories, i.e. prototype theory,\nexemplar theory and theory theory. We ponder about the question why quantum\ntheory performs so well in its modeling of human concepts, and shed light on\nthis question by analyzing the role of complex amplitudes, showing how they\nallow to describe interference in the statistics of measurement outcomes, while\nin the traditional theories statistics of outcomes originates in classical\nprobability weights, without the possibility of interference. The relevance of\ncomplex numbers, the appearance of entanglement, and the role of Fock space in\nexplaining contextual emergence, all as unique features of the quantum\nmodeling, are explicitly revealed in this paper by analyzing human concepts and\ntheir dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 20:24:34 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2013 02:33:27 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Aerts", "Diederik", ""], ["Gabora", "Liane", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1206.2009", "submitter": "Asma Boudhief", "authors": "Asma Boudhief, Mohsen Maraoui, Mounir Zrigui", "title": "Developing a model for a text database indexed pedagogically for\n  teaching the Arabic language", "comments": "43 pages,27 figures,12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this memory we made the design of an indexing model for Arabic language\nand adapting standards for describing learning resources used (the LOM and\ntheir application profiles) with learning conditions such as levels education\nof students, their levels of understanding...the pedagogical context with\ntaking into account the repre-sentative elements of the text, text's\nlength,...in particular, we highlight the specificity of the Arabic language\nwhich is a complex language, characterized by its flexion, its voyellation and\nits agglutination.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2012 09:27:51 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Boudhief", "Asma", ""], ["Maraoui", "Mohsen", ""], ["Zrigui", "Mounir", ""]]}, {"id": "1206.2010", "submitter": "Michele Filannino", "authors": "Michele Filannino", "title": "Temporal expression normalisation in natural language texts", "comments": "7 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. In this report, I describe a\nnovel rule-based architecture, built on top of a pre-existing system, which is\nable to normalise temporal expressions detected in English texts. Gold standard\ntemporally-annotated resources are limited in size and this makes research\ndifficult. The proposed system outperforms the state-of-the-art systems with\nrespect to TempEval-2 Shared Task (value attribute) and achieves substantially\nbetter results with respect to the pre-existing system on top of which it has\nbeen developed. I will also introduce a new free corpus consisting of 2822\nunique annotated temporal expressions. Both the corpus and the system are\nfreely available on-line.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2012 09:32:07 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Filannino", "Michele", ""]]}, {"id": "1206.3254", "submitter": "Amit Gruber", "authors": "Amit Gruber, Michal Rosen-Zvi, Yair Weiss", "title": "Latent Topic Models for Hypertext", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-230-239", "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent topic models have been successfully applied as an unsupervised topic\ndiscovery technique in large document collections. With the proliferation of\nhypertext document collection such as the Internet, there has also been great\ninterest in extending these approaches to hypertext [6, 9]. These approaches\ntypically model links in an analogous fashion to how they model words - the\ndocument-link co-occurrence matrix is modeled in the same way that the\ndocument-word co-occurrence matrix is modeled in standard topic models. In this\npaper we present a probabilistic generative model for hypertext document\ncollections that explicitly models the generation of links. Specifically, links\nfrom a word w to a document d depend directly on how frequent the topic of w is\nin d, in addition to the in-degree of d. We show how to perform EM learning on\nthis model efficiently. By not modeling links as analogous to words, we end up\nusing far fewer free parameters and obtain better link prediction results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:30:14 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Gruber", "Amit", ""], ["Rosen-Zvi", "Michal", ""], ["Weiss", "Yair", ""]]}, {"id": "1206.3293", "submitter": "Peter  Thwaites", "authors": "Peter Thwaites, Jim Q. Smith, Robert G. Cowell", "title": "Propagation using Chain Event Graphs", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-546-553", "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Chain Event Graph (CEG) is a graphial model which designed to embody\nconditional independencies in problems whose state spaces are highly asymmetric\nand do not admit a natural product structure. In this paer we present a\nprobability propagation algorithm which uses the topology of the CEG to build a\ntransporter CEG. Intriungly,the transporter CEG is directly analogous to the\ntriangulated Bayesian Network (BN) in the more conventional junction tree\npropagation algorithms used with BNs. The propagation method uses factorization\nformulae also analogous to (but different from) the ones using potentials on\ncliques and separators of the BN. It appears that the methods will be typically\nmore efficient than the BN algorithms when applied to contexts where there is\nsignificant asymmetry present.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:52:10 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Thwaites", "Peter", ""], ["Smith", "Jim Q.", ""], ["Cowell", "Robert G.", ""]]}, {"id": "1206.4522", "submitter": "Phil Gooch", "authors": "Phil Gooch", "title": "BADREX: In situ expansion and coreference of biomedical abbreviations\n  using dynamic regular expressions", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BADREX uses dynamically generated regular expressions to annotate term\ndefinition-term abbreviation pairs, and corefers unpaired acronyms and\nabbreviations back to their initial definition in the text. Against the\nMedstract corpus BADREX achieves precision and recall of 98% and 97%, and\nagainst a much larger corpus, 90% and 85%, respectively. BADREX yields improved\nperformance over previous approaches, requires no training data and allows\nruntime customisation of its input parameters. BADREX is freely available from\nhttps://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a\nplugin for the General Architecture for Text Engineering (GATE) framework and\nis licensed under the GPLv3.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:06:48 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Gooch", "Phil", ""]]}, {"id": "1206.4631", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi, Jonathan M Bischof", "title": "A Poisson convolution model for characterizing topical content with word\n  frequency and exclusivity", "comments": "Originally appeared in ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ongoing challenge in the analysis of document collections is how to\nsummarize content in terms of a set of inferred themes that can be interpreted\nsubstantively in terms of topics. The current practice of parametrizing the\nthemes in terms of most frequent words limits interpretability by ignoring the\ndifferential use of words across topics. We argue that words that are both\ncommon and exclusive to a theme are more effective at characterizing topical\ncontent. We consider a setting where professional editors have annotated\ndocuments to a collection of topic categories, organized into a tree, in which\nleaf-nodes correspond to the most specific topics. Each document is annotated\nto multiple categories, at different levels of the tree. We introduce a\nhierarchical Poisson convolution model to analyze annotated documents in this\nsetting. The model leverages the structure among categories defined by\nprofessional editors to infer a clear semantic description for each topic in\nterms of words that are both frequent and exclusive. We carry out a large\nrandomized experiment on Amazon Turk to demonstrate that topic summaries based\non the FREX score are more interpretable than currently established frequency\nbased summaries, and that the proposed model produces more efficient estimates\nof exclusivity than with currently models. We also develop a parallelized\nHamiltonian Monte Carlo sampler that allows the inference to scale to millions\nof documents.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:11:38 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 17:32:26 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 03:02:39 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Airoldi", "Edoardo M", ""], ["Bischof", "Jonathan M", ""]]}, {"id": "1206.4637", "submitter": "Paul Prasse", "authors": "Paul Prasse (University of Potsdam), Christoph Sawade (University of\n  Potsdam), Niels Landwehr (University of Potsdam), Tobias Scheffer (University\n  of Potsdam)", "title": "Learning to Identify Regular Expressions that Describe Email Campaigns", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of inferring a regular expression from a\ngiven set of strings that resembles, as closely as possible, the regular\nexpression that a human expert would have written to identify the language.\nThis is motivated by our goal of automating the task of postmasters of an email\nservice who use regular expressions to describe and blacklist email spam\ncampaigns. Training data contains batches of messages and corresponding regular\nexpressions that an expert postmaster feels confident to blacklist. We model\nthis task as a learning problem with structured output spaces and an\nappropriate loss function, derive a decoder and the resulting optimization\nproblem, and a report on a case study conducted with an email service.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:15:28 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Prasse", "Paul", "", "University of Potsdam"], ["Sawade", "Christoph", "", "University of\n  Potsdam"], ["Landwehr", "Niels", "", "University of Potsdam"], ["Scheffer", "Tobias", "", "University\n  of Potsdam"]]}, {"id": "1206.4958", "submitter": "Peiyou Song", "authors": "Peiyou Song, Anhei Shu, Anyu Zhou, Dan Wallach, Jedidiah R. Crandall", "title": "A Pointillism Approach for Natural Language Processing of Social Media", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chinese language poses challenges for natural language processing based\non the unit of a word even for formal uses of the Chinese language, social\nmedia only makes word segmentation in Chinese even more difficult. In this\ndocument we propose a pointillism approach to natural language processing.\nRather than words that have individual meanings, the basic unit of a\npointillism approach is trigrams of characters. These grams take on meaning in\naggregate when they appear together in a way that is correlated over time.\n  Our results from three kinds of experiments show that when words and topics\ndo have a meme-like trend, they can be reconstructed from only trigrams. For\nexample, for 4-character idioms that appear at least 99 times in one day in our\ndata, the unconstrained precision (that is, precision that allows for deviation\nfrom a lexicon when the result is just as correct as the lexicon version of the\nword or phrase) is 0.93. For longer words and phrases collected from\nWiktionary, including neologisms, the unconstrained precision is 0.87. We\nconsider these results to be very promising, because they suggest that it is\nfeasible for a machine to reconstruct complex idioms, phrases, and neologisms\nwith good precision without any notion of words. Thus the colorful and baroque\nuses of language that typify social media in challenging languages such as\nChinese may in fact be accessible to machines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2012 18:17:50 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Song", "Peiyou", ""], ["Shu", "Anhei", ""], ["Zhou", "Anyu", ""], ["Wallach", "Dan", ""], ["Crandall", "Jedidiah R.", ""]]}, {"id": "1206.5333", "submitter": "Leon Derczynski", "authors": "Naushad UzZaman, Hector Llorens, James Allen, Leon Derczynski, Marc\n  Verhagen and James Pustejovsky", "title": "TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe the TempEval-3 task which is currently in preparation for the\nSemEval-2013 evaluation exercise. The aim of TempEval is to advance research on\ntemporal information processing. TempEval-3 follows on from previous TempEval\nevents, incorporating: a three-part task structure covering event, temporal\nexpression and temporal relation extraction; a larger dataset; and single\noverall task quality scores.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2012 22:30:44 GMT"}, {"version": "v2", "created": "Sun, 25 May 2014 19:10:12 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["UzZaman", "Naushad", ""], ["Llorens", "Hector", ""], ["Allen", "James", ""], ["Derczynski", "Leon", ""], ["Verhagen", "Marc", ""], ["Pustejovsky", "James", ""]]}, {"id": "1206.5384", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "Tarek El-Shishtawy and Fatma El-Ghannam", "title": "Keyphrase Based Arabic Summarizer (KPAS)", "comments": "INFOS 2012, The 8th INFOS2012 International Conference on Informatics\n  and Systems, 14-16 May, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a computationally inexpensive and efficient generic\nsummarization algorithm for Arabic texts. The algorithm belongs to extractive\nsummarization family, which reduces the problem into representative sentences\nidentification and extraction sub-problems. Important keyphrases of the\ndocument to be summarized are identified employing combinations of statistical\nand linguistic features. The sentence extraction algorithm exploits keyphrases\nas the primary attributes to rank a sentence. The present experimental work,\ndemonstrates different techniques for achieving various summarization goals\nincluding: informative richness, coverage of both main and auxiliary topics,\nand keeping redundancy to a minimum. A scoring scheme is then adopted that\nbalances between these summarization goals. To evaluate the resulted Arabic\nsummaries with well-established systems, aligned English/Arabic texts are used\nthrough the experiments.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2012 12:19:38 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["El-Shishtawy", "Tarek", ""], ["El-Ghannam", "Fatma", ""]]}, {"id": "1206.5851", "submitter": "Daniel Gayo Avello", "authors": "Daniel Gayo-Avello", "title": "A meta-analysis of state-of-the-art electoral prediction from Twitter\n  data", "comments": "19 pages, 3 tables", "journal-ref": "Social Science Computer Review, August 23, 2013, 0894439313493979", "doi": "10.1177/0894439313493979", "report-no": null, "categories": "cs.SI cs.CL cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electoral prediction from Twitter data is an appealing research topic. It\nseems relatively straightforward and the prevailing view is overly optimistic.\nThis is problematic because while simple approaches are assumed to be good\nenough, core problems are not addressed. Thus, this paper aims to (1) provide a\nbalanced and critical review of the state of the art; (2) cast light on the\npresume predictive power of Twitter data; and (3) depict a roadmap to push\nforward the field. Hence, a scheme to characterize Twitter prediction methods\nis proposed. It covers every aspect from data collection to performance\nevaluation, through data processing and vote inference. Using that scheme,\nprior research is analyzed and organized to explain the main approaches taken\nup to date but also their weaknesses. This is the first meta-analysis of the\nwhole body of research regarding electoral prediction from Twitter data. It\nreveals that its presumed predictive power regarding electoral prediction has\nbeen rather exaggerated: although social media may provide a glimpse on\nelectoral outcomes current research does not provide strong evidence to support\nit can replace traditional polls. Finally, future lines of research along with\na set of requirements they must fulfill are provided.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 21:59:58 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Gayo-Avello", "Daniel", ""]]}, {"id": "1206.6403", "submitter": "Paramveer Dhillon", "authors": "Paramveer Dhillon (University of Pennsylvania), Jordan Rodu\n  (University of Pennsylvania), Dean Foster (University of Pennsylvania), Lyle\n  Ungar (University of Pennsylvania)", "title": "Two Step CCA: A new spectral method for estimating vector models of\n  words", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlabeled data is often used to learn representations which can be used to\nsupplement baseline features in a supervised learner. For example, for text\napplications where the words lie in a very high dimensional space (the size of\nthe vocabulary), one can learn a low rank \"dictionary\" by an\neigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA).\nIn this paper, we present a new spectral method based on CCA to learn an\neigenword dictionary. Our improved procedure computes two set of CCAs, the\nfirst one between the left and right contexts of the given word and the second\none between the projections resulting from this CCA and the word itself. We\nprove theoretically that this two-step procedure has lower sample complexity\nthan the simple single step procedure and also illustrate the empirical\nefficacy of our approach and the richness of representations learned by our Two\nStep CCA (TSCCA) procedure on the tasks of POS tagging and sentiment\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Dhillon", "Paramveer", "", "University of Pennsylvania"], ["Rodu", "Jordan", "", "University of Pennsylvania"], ["Foster", "Dean", "", "University of Pennsylvania"], ["Ungar", "Lyle", "", "University of Pennsylvania"]]}, {"id": "1206.6423", "submitter": "Cynthia Matuszek", "authors": "Cynthia Matuszek (University of Washington), Nicholas FitzGerald\n  (University of Washington), Luke Zettlemoyer (University of Washington),\n  Liefeng Bo (University of Washington), Dieter Fox (University of Washington)", "title": "A Joint Model of Language and Perception for Grounded Attribute Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become more ubiquitous and capable, it becomes ever more important\nto enable untrained users to easily interact with them. Recently, this has led\nto study of the language grounding problem, where the goal is to extract\nrepresentations of the meanings of natural language tied to perception and\nactuation in the physical world. In this paper, we present an approach for\njoint learning of language and perception models for grounded attribute\ninduction. Our perception model includes attribute classifiers, for example to\ndetect object color and shape, and the language model is based on a\nprobabilistic categorial grammar that enables the construction of rich,\ncompositional meaning representations. The approach is evaluated on the task of\ninterpreting sentences that describe sets of objects in a physical workspace.\nWe demonstrate accurate task performance and effective latent-variable concept\ninduction in physical grounded scenes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Matuszek", "Cynthia", "", "University of Washington"], ["FitzGerald", "Nicholas", "", "University of Washington"], ["Zettlemoyer", "Luke", "", "University of Washington"], ["Bo", "Liefeng", "", "University of Washington"], ["Fox", "Dieter", "", "University of Washington"]]}, {"id": "1206.6426", "submitter": "Andriy Mnih", "authors": "Andriy Mnih (University College London), Yee Whye Teh (University\n  College London)", "title": "A Fast and Simple Algorithm for Training Neural Probabilistic Language\n  Models", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": "In Proceedings of the 29th International Conference on Machine\n  Learning, pages 1751-1758, 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of their superior performance, neural probabilistic language models\n(NPLMs) remain far less widely used than n-gram models due to their notoriously\nlong training times, which are measured in weeks even for moderately-sized\ndatasets. Training NPLMs is computationally expensive because they are\nexplicitly normalized, which leads to having to consider all words in the\nvocabulary when computing the log-likelihood gradients.\n  We propose a fast and simple algorithm for training NPLMs based on\nnoise-contrastive estimation, a newly introduced procedure for estimating\nunnormalized continuous distributions. We investigate the behaviour of the\nalgorithm on the Penn Treebank corpus and show that it reduces the training\ntimes by more than an order of magnitude without affecting the quality of the\nresulting models. The algorithm is also more efficient and much more stable\nthan importance sampling because it requires far fewer noise samples to perform\nwell.\n  We demonstrate the scalability of the proposed approach by training several\nneural language models on a 47M-word corpus with a 80K-word vocabulary,\nobtaining state-of-the-art results on the Microsoft Research Sentence\nCompletion Challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mnih", "Andriy", "", "University College London"], ["Teh", "Yee Whye", "", "University\n  College London"]]}, {"id": "1206.6481", "submitter": "Yuhong Guo", "authors": "Yuhong Guo (Temple University), Min Xiao (Temple University)", "title": "Cross Language Text Classification via Subspace Co-Regularized\n  Multi-View Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many multilingual text classification problems, the documents in different\nlanguages often share the same set of categories. To reduce the labeling cost\nof training a classification model for each individual language, it is\nimportant to transfer the label knowledge gained from one language to another\nlanguage by conducting cross language classification. In this paper we develop\na novel subspace co-regularized multi-view learning method for cross language\ntext classification. This method is built on parallel corpora produced by\nmachine translation. It jointly minimizes the training error of each classifier\nin each language while penalizing the distance between the subspace\nrepresentations of parallel documents. Our empirical study on a large set of\ncross language text classification tasks shows the proposed method consistently\noutperforms a number of inductive methods, domain adaptation methods, and\nmulti-view learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Guo", "Yuhong", "", "Temple University"], ["Xiao", "Min", "", "Temple University"]]}, {"id": "1206.6735", "submitter": "Shay Cohen", "authors": "Shay B. Cohen, Carlos G\\'omez-Rodr\\'iguez, Giorgio Satta", "title": "Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique to remove spurious ambiguity from transition\nsystems for dependency parsing. Our technique chooses a canonical sequence of\ntransition operations (computation) for a given dependency tree. Our technique\ncan be applied to a large class of bottom-up transition systems, including for\ninstance Nivre (2004) and Attardi (2006).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2012 15:43:57 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Cohen", "Shay B.", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""], ["Satta", "Giorgio", ""]]}]