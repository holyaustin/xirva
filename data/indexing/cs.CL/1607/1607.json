[{"id": "1607.00030", "submitter": "Alexandra Birch", "authors": "Alexandra Birch, Omri Abend, Ondrej Bojar, Barry Haddow", "title": "HUME: Human UCCA-Based Evaluation of Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human evaluation of machine translation normally uses sentence-level measures\nsuch as relative ranking or adequacy scales. However, these provide no insight\ninto possible errors, and do not scale well with sentence length. We argue for\na semantics-based evaluation, which captures what meaning components are\nretained in the MT output, thus providing a more fine-grained analysis of\ntranslation quality, and enabling the construction and tuning of\nsemantics-based MT. We present a novel human semantic evaluation measure, Human\nUCCA-based MT Evaluation (HUME), building on the UCCA semantic representation\nscheme. HUME covers a wider range of semantic phenomena than previous methods\nand does not rely on semantic annotation of the potentially garbled MT output.\nWe experiment with four language pairs, demonstrating HUME's broad\napplicability, and report good inter-annotator agreement rates and correlation\nwith human adequacy scores.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:35:47 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 13:39:42 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Birch", "Alexandra", ""], ["Abend", "Omri", ""], ["Bojar", "Ondrej", ""], ["Haddow", "Barry", ""]]}, {"id": "1607.00070", "submitter": "Layla El Asri", "authors": "Layla El Asri and Jing He and Kaheer Suleman", "title": "A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue\n  Systems", "comments": "Accepted for publication at Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User simulation is essential for generating enough data to train a\nstatistical spoken dialogue system. Previous models for user simulation suffer\nfrom several drawbacks, such as the inability to take dialogue history into\naccount, the need of rigid structure to ensure coherent user behaviour, heavy\ndependence on a specific domain, the inability to output several user\nintentions during one dialogue turn, or the requirement of a summarized action\nspace for tractability. This paper introduces a data-driven user simulator\nbased on an encoder-decoder recurrent neural network. The model takes as input\na sequence of dialogue contexts and outputs a sequence of dialogue acts\ncorresponding to user intentions. The dialogue contexts include information\nabout the machine acts and the status of the user goal. We show on the Dialogue\nState Tracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence model\noutperforms an agenda-based simulator and an n-gram simulator, according to\nF-score. Furthermore, we show how this model can be used on the original action\nspace and thereby models user behaviour with finer granularity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 22:51:00 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Asri", "Layla El", ""], ["He", "Jing", ""], ["Suleman", "Kaheer", ""]]}, {"id": "1607.00139", "submitter": "Mike Thelwall Prof", "authors": "Mike Thelwall", "title": "TensiStrength: Stress and relaxation magnitude detection for social\n  media texts", "comments": "Thelwall, M. (in press). TensiStrength: Stress and relaxation\n  magnitude detection for social media texts. Information Processing &\n  Management", "journal-ref": null, "doi": "10.1016/j.ipm.2016.06.009", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer systems need to be able to react to stress in order to perform\noptimally on some tasks. This article describes TensiStrength, a system to\ndetect the strength of stress and relaxation expressed in social media text\nmessages. TensiStrength uses a lexical approach and a set of rules to detect\ndirect and indirect expressions of stress or relaxation, particularly in the\ncontext of transportation. It is slightly more effective than a comparable\nsentiment analysis program, although their similar performances occur despite\ndifferences on almost half of the tweets gathered. The effectiveness of\nTensiStrength depends on the nature of the tweets classified, with tweets that\nare rich in stress-related terms being particularly problematic. Although\ngeneric machine learning methods can give better performance than TensiStrength\noverall, they exploit topic-related terms in a way that may be undesirable in\npractical applications and that may not work as well in more focused contexts.\nIn conclusion, TensiStrength and generic machine learning approaches work well\nenough to be practical choices for intelligent applications that need to take\nadvantage of stress information, and the decision about which to use depends on\nthe nature of the texts analysed and the purpose of the task.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:50:02 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 13:44:49 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Thelwall", "Mike", ""]]}, {"id": "1607.00167", "submitter": "Pedro Saleiro", "authors": "Jo\\~ao Oliveira, Mike Pinto, Pedro Saleiro, Jorge Teixeira", "title": "SentiBubbles: Topic Modeling and Sentiment Visualization of\n  Entity-centric Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Media users tend to mention entities when reacting to news events. The\nmain purpose of this work is to create entity-centric aggregations of tweets on\na daily basis. By applying topic modeling and sentiment analysis, we create\ndata visualization insights about current events and people reactions to those\nevents from an entity-centric perspective.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 09:15:13 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 21:20:34 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Oliveira", "Jo\u00e3o", ""], ["Pinto", "Mike", ""], ["Saleiro", "Pedro", ""], ["Teixeira", "Jorge", ""]]}, {"id": "1607.00186", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Throwing fuel on the embers: Probability or Dichotomy, Cognitive or\n  Linguistic?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prof. Robert Berwick's abstract for his forthcoming invited talk at the\nACL2016 workshop on Cognitive Aspects of Computational Language Learning\nrevives an ancient debate. Entitled \"Why take a chance?\", Berwick seems to\nrefer implicitly to Chomsky's critique of the statistical approach of Harris as\nwell as the currently dominant paradigms in CoNLL.\n  Berwick avoids Chomsky's use of \"innate\" but states that \"the debate over the\nexistence of sophisticated mental grammars was settled with Chomsky's Logical\nStructure of Linguistic Theory (1957/1975)\", acknowledging that \"this debate\nhas often been revived\".\n  This paper agrees with the view that this debate has long since been settled,\nbut with the opposite outcome! Given the embers have not yet died away, and the\nquestions remain fundamental, perhaps it is appropriate to refuel the debate,\nso I would like to join Bob in throwing fuel on this fire by reviewing the\nevidence against the Chomskian position!\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 10:01:11 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1607.00198", "submitter": "Rudra Murthy V", "authors": "Rudra Murthy V, Mitesh Khapra and Pushpak Bhattacharyya", "title": "Sharing Network Parameters for Crosslingual Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state of the art approaches for Named Entity Recognition rely on hand\ncrafted features and annotated corpora. Recently Neural network based models\nhave been proposed which do not require handcrafted features but still require\nannotated corpora. However, such annotated corpora may not be available for\nmany languages. In this paper, we propose a neural network based model which\nallows sharing the decoder as well as word and character level parameters\nbetween two languages thereby allowing a resource fortunate language to aid a\nresource deprived language. Specifically, we focus on the case when limited\nannotated corpora is available in one language ($L_1$) and abundant annotated\ncorpora is available in another language ($L_2$). Sharing the network\narchitecture and parameters between $L_1$ and $L_2$ leads to improved\nperformance in $L_1$. Further, our approach does not require any hand crafted\nfeatures but instead directly learns meaningful feature representations from\nthe training data itself. We experiment with 4 language pairs and show that\nindeed in a resource constrained setup (lesser annotated corpora), a model\njointly trained with data from another language performs better than a model\ntrained only on the limited corpora in one language.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 10:35:59 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Murthy", "Rudra", "V"], ["Khapra", "Mitesh", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1607.00225", "submitter": "Chris Emmery", "authors": "St\\'ephan Tulkens, Chris Emmery, Walter Daelemans", "title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource", "comments": "in LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have recently seen a strong increase in interest as a result\nof strong performance gains on a variety of tasks. However, most of this\nresearch also underlined the importance of benchmark datasets, and the\ndifficulty of constructing these for a variety of language-specific tasks.\nStill, many of the datasets used in these tasks could prove to be fruitful\nlinguistic resources, allowing for unique observations into language use and\nvariability. In this paper we demonstrate the performance of multiple types of\nembeddings, created with both count and prediction-based architectures on a\nvariety of corpora, in two language-specific tasks: relation evaluation, and\ndialect identification. For the latter, we compare unsupervised methods with a\ntraditional, hand-crafted dictionary. With this research, we provide the\nembeddings themselves, the relation evaluation task benchmark for use in\nfurther research, and demonstrate how the benchmarked embeddings prove a useful\nunsupervised linguistic resource, effectively used in a downstream task.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 12:48:35 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Tulkens", "St\u00e9phan", ""], ["Emmery", "Chris", ""], ["Daelemans", "Walter", ""]]}, {"id": "1607.00325", "submitter": "Morten Kolb{\\ae}k", "authors": "Dong Yu, Morten Kolb{\\ae}k, Zheng-Hua Tan, and Jesper Jensen", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent\n  Multi-talker Speech Separation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning model, which supports permutation invariant\ntraining (PIT), for speaker independent multi-talker speech separation,\ncommonly known as the cocktail-party problem. Different from most of the prior\narts that treat speech separation as a multi-class regression problem and the\ndeep clustering technique that considers it a segmentation (or clustering)\nproblem, our model optimizes for the separation regression error, ignoring the\norder of mixing sources. This strategy cleverly solves the long-lasting label\npermutation problem that has prevented progress on deep learning based\ntechniques for speech separation. Experiments on the equal-energy mixing setup\nof a Danish corpus confirms the effectiveness of PIT. We believe improvements\nbuilt upon PIT can eventually solve the cocktail-party problem and enable\nreal-world adoption of, e.g., automatic meeting transcription and multi-party\nhuman-computer interaction, where overlapping speech is common.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 17:34:16 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 19:57:37 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yu", "Dong", ""], ["Kolb\u00e6k", "Morten", ""], ["Tan", "Zheng-Hua", ""], ["Jensen", "Jesper", ""]]}, {"id": "1607.00359", "submitter": "S\\'ebastien Gagnon", "authors": "S\\'ebastien Gagnon and Jean Rouat", "title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Model (HMM) is often regarded as the dynamical model of choice\nin many fields and applications. It is also at the heart of most\nstate-of-the-art speech recognition systems since the 70's. However, from\nGaussian mixture models HMMs (GMM-HMM) to deep neural network HMMs (DNN-HMM),\nthe underlying Markovian chain of state-of-the-art models did not changed much.\nThe \"left-to-right\" topology is mostly always employed because very few other\nalternatives exist. In this paper, we propose that finely-tuned HMM topologies\nare essential for precise temporal modelling and that this approach should be\ninvestigated in state-of-the-art HMM system. As such, we propose a\nproof-of-concept framework for learning efficient topologies by pruning down\ncomplex generic models. Speech recognition experiments that were conducted\nindicate that complex time dependencies can be better learned by this approach\nthan with classical \"left-to-right\" models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 19:20:50 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Gagnon", "S\u00e9bastien", ""], ["Rouat", "Jean", ""]]}, {"id": "1607.00410", "submitter": "Yusuke Watanabe Dr.", "authors": "Yusuke Watanabe, Kazuma Hashimoto, Yoshimasa Tsuruoka", "title": "Domain Adaptation for Neural Networks by Parameter Augmentation", "comments": "9 page. To appear in the first ACL Workshop on Representation\n  Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple domain adaptation method for neural networks in a\nsupervised setting. Supervised domain adaptation is a way of improving the\ngeneralization performance on the target domain by using the source domain\ndataset, assuming that both of the datasets are labeled. Recently, recurrent\nneural networks have been shown to be successful on a variety of NLP tasks such\nas caption generation; however, the existing domain adaptation techniques are\nlimited to (1) tune the model parameters by the target dataset after the\ntraining by the source dataset, or (2) design the network to have dual output,\none for the source domain and the other for the target domain. Reformulating\nthe idea of the domain adaptation technique proposed by Daume (2007), we\npropose a simple domain adaptation method, which can be applied to neural\nnetworks trained with a cross-entropy loss. On captioning datasets, we show\nperformance improvements over other domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 21:24:21 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Watanabe", "Yusuke", ""], ["Hashimoto", "Kazuma", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1607.00424", "submitter": "Ameet Soni", "authors": "Dileep Viswanathan and Ameet Soni and Jude Shavlik and Sriraam\n  Natarajan", "title": "Learning Relational Dependency Networks for Relation Extraction", "comments": "In Proceedings of Sixth International Workshop on Statistical\n  Relational AI at the 25th International Joint Conference on Artificial\n  Intelligence (IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of KBP slot filling -- extracting relation information\nfrom newswire documents for knowledge base construction. We present our\npipeline, which employs Relational Dependency Networks (RDNs) to learn\nlinguistic patterns for relation extraction. Additionally, we demonstrate how\nseveral components such as weak supervision, word2vec features, joint learning\nand the use of human advice, can be incorporated in this relational framework.\nWe evaluate the different components in the benchmark KBP 2015 task and show\nthat RDNs effectively model a diverse set of features and perform competitively\nwith current state-of-the-art relation extraction.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 22:11:38 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Viswanathan", "Dileep", ""], ["Soni", "Ameet", ""], ["Shavlik", "Jude", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1607.00534", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer", "title": "Text comparison using word vector representations and dimensionality\n  reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2015-01", "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a technique to compare large text sources using word\nvector representations (word2vec) and dimensionality reduction (t-SNE) and how\nit can be implemented using Python. The technique provides a bird's-eye view of\ntext sources, e.g. text summaries and their source material, and enables users\nto explore text sources like a geographical map. Word vector representations\ncapture many linguistic properties such as gender, tense, plurality and even\nsemantic concepts like \"capital city of\". Using dimensionality reduction, a 2D\nmap can be computed where semantically similar words are close to each other.\nThe technique uses the word2vec model from the gensim Python library and t-SNE\nfrom scikit-learn.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 17:17:22 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Heuer", "Hendrik", ""]]}, {"id": "1607.00570", "submitter": "Cedric De Boom", "authors": "Cedric De Boom, Steven Van Canneyt, Thomas Demeester, Bart Dhoedt", "title": "Representation learning for very short texts using weighted word\n  embedding aggregation", "comments": "8 pages, 3 figures, 2 tables, appears in Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2016.06.012", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text messages such as tweets are very noisy and sparse in their use of\nvocabulary. Traditional textual representations, such as tf-idf, have\ndifficulty grasping the semantic meaning of such texts, which is important in\napplications such as event detection, opinion mining, news recommendation, etc.\nWe constructed a method based on semantic word embeddings and frequency\ninformation to arrive at low-dimensional representations for short texts\ndesigned to capture semantic similarity. For this purpose we designed a\nweight-based model and a learning procedure based on a novel median-based loss\nfunction. This paper discusses the details of our model and the optimization\nmethods, together with the experimental results on both Wikipedia and Twitter\ndata. We find that our method outperforms the baseline approaches in the\nexperiments, and that it generalizes well on different word embeddings without\nretraining. Our method is therefore capable of retaining most of the semantic\ninformation in the text, and is applicable out-of-the-box.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 23:10:09 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["De Boom", "Cedric", ""], ["Van Canneyt", "Steven", ""], ["Demeester", "Thomas", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1607.00578", "submitter": "Henry Choi", "authors": "Heeyoul Choi and Kyunghyun Cho and Yoshua Bengio", "title": "Context-Dependent Word Representation for Neural Machine Translation", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first observe a potential weakness of continuous vector representations of\nsymbols in neural machine translation. That is, the continuous vector\nrepresentation, or a word embedding vector, of a symbol encodes multiple\ndimensions of similarity, equivalent to encoding more than one meaning of the\nword. This has the consequence that the encoder and decoder recurrent networks\nin neural machine translation need to spend substantial amount of their\ncapacity in disambiguating source and target words based on the context which\nis defined by a source sentence. Based on this observation, in this paper we\npropose to contextualize the word embedding vectors using a nonlinear\nbag-of-words representation of the source sentence. Additionally, we propose to\nrepresent special tokens (such as numbers, proper nouns and acronyms) with\ntyped symbols to facilitate translating those words that are not well-suited to\nbe translated via continuous vectors. The experiments on En-Fr and En-De reveal\nthat the proposed approaches of contextualization and symbolization improves\nthe translation quality of neural machine translation systems significantly.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 02:18:16 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Choi", "Heeyoul", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1607.00623", "submitter": "Kaveh Hassani", "authors": "Kaveh Hassani and Won-Sook Lee", "title": "Visualizing Natural Language Descriptions: A Survey", "comments": "Due to copyright most of the figures only appear in the journal\n  version", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, Article No. 17, June\n  2016", "doi": "10.1145/2932710", "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural language interface exploits the conceptual simplicity and\nnaturalness of the language to create a high-level user-friendly communication\nchannel between humans and machines. One of the promising applications of such\ninterfaces is generating visual interpretations of semantic content of a given\nnatural language that can be then visualized either as a static scene or a\ndynamic animation. This survey discusses requirements and challenges of\ndeveloping such systems and reports 26 graphical systems that exploit natural\nlanguage interfaces and addresses both artificial intelligence and\nvisualization aspects. This work serves as a frame of reference to researchers\nand to enable further advances in the field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 10:30:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hassani", "Kaveh", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1607.00718", "submitter": "Dennis Singh Moirangthem Mr", "authors": "Minsoo Kim, Moirangthem Dennis Singh, and Minho Lee", "title": "Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent\n  Unit for Summarization", "comments": "To appear in RepL4NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce temporal hierarchies to the sequence to sequence\n(seq2seq) model to tackle the problem of abstractive summarization of\nscientific articles. The proposed Multiple Timescale model of the Gated\nRecurrent Unit (MTGRU) is implemented in the encoder-decoder setting to better\ndeal with the presence of multiple compositionalities in larger texts. The\nproposed model is compared to the conventional RNN encoder-decoder, and the\nresults demonstrate that our model trains faster and shows significant\nperformance gains. The results also show that the temporal hierarchies help\nimprove the ability of seq2seq models to capture compositionalities better\nwithout the presence of highly complex architectural hierarchies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:55:17 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kim", "Minsoo", ""], ["Singh", "Moirangthem Dennis", ""], ["Lee", "Minho", ""]]}, {"id": "1607.00970", "submitter": "Lili Mou", "authors": "Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin", "title": "Sequence to Backward and Forward Sequences: A Content-Introducing\n  Approach to Generative Short-Text Conversation", "comments": "Accepted by COLING", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using neural networks to generate replies in human-computer dialogue systems\nis attracting increasing attention over the past few years. However, the\nperformance is not satisfactory: the neural network tends to generate safe,\nuniversally relevant replies which carry little meaning. In this paper, we\npropose a content-introducing approach to neural network-based generative\ndialogue systems. We first use pointwise mutual information (PMI) to predict a\nnoun as a keyword, reflecting the main gist of the reply. We then propose\nseq2BF, a \"sequence to backward and forward sequences\" model, which generates a\nreply containing the given keyword. Experimental results show that our approach\nsignificantly outperforms traditional sequence-to-sequence models in terms of\nhuman evaluation and the entropy measure, and that the predicted keyword can\nappear at an appropriate position in the reply.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:42:52 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:40:37 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mou", "Lili", ""], ["Song", "Yiping", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1607.00976", "submitter": "Silvio Amir", "authors": "Silvio Amir, Byron C. Wallace, Hao Lyu, Paula Carvalho M\\'ario J.\n  Silva", "title": "Modelling Context with User Embeddings for Sarcasm Detection in Social\n  Media", "comments": "published as a conference paper at CONLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep neural network for automated sarcasm detection. Recent\nwork has emphasized the need for models to capitalize on contextual features,\nbeyond lexical and syntactic cues present in utterances. For example, different\nspeakers will tend to employ sarcasm regarding different subjects and, thus,\nsarcasm detection models ought to encode such speaker information. Current\nmethods have achieved this by way of laborious feature engineering. By\ncontrast, we propose to automatically learn and then exploit user embeddings,\nto be used in concert with lexical signals to recognize sarcasm. Our approach\ndoes not require elaborate feature engineering (and concomitant data scraping);\nfitting user embeddings requires only the text from their previous posts. The\nexperimental results show that our model outperforms a state-of-the-art\napproach leveraging an extensive set of carefully crafted features.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 18:04:18 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 02:27:41 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Amir", "Silvio", ""], ["Wallace", "Byron C.", ""], ["Lyu", "Hao", ""], ["Silva", "Paula Carvalho M\u00e1rio J.", ""]]}, {"id": "1607.00992", "submitter": "Jay Pujara", "authors": "Jay Pujara and Lise Getoor", "title": "Generic Statistical Relational Entity Resolution in Knowledge Graphs", "comments": null, "journal-ref": "In the Sixth International Workshop on Statistical Relational AI,\n  2016", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution, the problem of identifying the underlying entity of\nreferences found in data, has been researched for many decades in many\ncommunities. A common theme in this research has been the importance of\nincorporating relational features into the resolution process. Relational\nentity resolution is particularly important in knowledge graphs (KGs), which\nhave a regular structure capturing entities and their interrelationships. We\nidentify three major problems in KG entity resolution: (1) intra-KG reference\nambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending\nKGs with new facts. We implement a framework that generalizes across these\nthree settings and exploits this regular structure of KGs. Our framework has\nmany advantages over custom solutions widely deployed in industry, including\ncollective inference, scalability, and interpretability. We apply our framework\nto two real-world KG entity resolution problems, ambiguity in NELL and merging\ndata from Freebase and MusicBrainz, demonstrating the importance of relational\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 19:02:47 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Pujara", "Jay", ""], ["Getoor", "Lise", ""]]}, {"id": "1607.01133", "submitter": "Meng Fang", "authors": "Meng Fang and Trevor Cohn", "title": "Learning when to trust distant supervision: An application to\n  low-resource POS tagging using cross-lingual projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross lingual projection of linguistic annotation suffers from many sources\nof bias and noise, leading to unreliable annotations that cannot be used\ndirectly. In this paper, we introduce a novel approach to sequence tagging that\nlearns to correct the errors from cross-lingual projection using an explicit\ndebiasing layer. This is framed as joint learning over two corpora, one tagged\nwith gold standard and the other with projected tags. We evaluated with only\n1,000 tokens tagged with gold standard tags, along with more plentiful parallel\ndata. Our system equals or exceeds the state-of-the-art on eight simulated\nlow-resource settings, as well as two real low-resource languages, Malagasy and\nKinyarwanda.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 07:31:22 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Fang", "Meng", ""], ["Cohn", "Trevor", ""]]}, {"id": "1607.01149", "submitter": "Ale\\v{s} Tamchyna", "authors": "Ale\\v{s} Tamchyna, Alexander Fraser, Ond\\v{r}ej Bojar, Marcin\n  Junczys-Dowmunt", "title": "Target-Side Context for Discriminative Models in Statistical Machine\n  Translation", "comments": "Accepted as a long paper for ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative translation models utilizing source context have been shown to\nhelp statistical machine translation performance. We propose a novel extension\nof this work using target context information. Surprisingly, we show that this\nmodel can be efficiently integrated directly in the decoding process. Our\napproach scales to large training data sizes and results in consistent\nimprovements in translation quality on four language pairs. We also provide an\nanalysis comparing the strengths of the baseline source-context model with our\nextended source-context and target-context model and we show that our extension\nallows us to better capture morphological coherence. Our work is freely\navailable as part of Moses.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 08:51:21 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Tamchyna", "Ale\u0161", ""], ["Fraser", "Alexander", ""], ["Bojar", "Ond\u0159ej", ""], ["Junczys-Dowmunt", "Marcin", ""]]}, {"id": "1607.01274", "submitter": "Baiyang Wang", "authors": "Baiyang Wang, Diego Klabjan", "title": "Temporal Topic Analysis with Endogenous and Exogenous Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling temporal textual data taking endogenous\nand exogenous processes into account. Such text documents arise in real world\napplications, including job advertisements and economic news articles, which\nare influenced by the fluctuations of the general economy. We propose a\nhierarchical Bayesian topic model which imposes a \"group-correlated\"\nhierarchical structure on the evolution of topics over time incorporating both\nprocesses, and show that this model can be estimated from Markov chain Monte\nCarlo sampling methods. We further demonstrate that this model captures the\nintrinsic relationships between the topic distribution and the time-dependent\nfactors, and compare its performance with latent Dirichlet allocation (LDA) and\ntwo other related models. The model is applied to two collections of documents\nto illustrate its empirical performance: online job advertisements from\nDirectEmployers Association and journalists' postings on BusinessInsider.com.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:16:55 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Wang", "Baiyang", ""], ["Klabjan", "Diego", ""]]}, {"id": "1607.01426", "submitter": "Rajarshi Das", "authors": "Rajarshi Das, Arvind Neelakantan, David Belanger, Andrew McCallum", "title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent\n  Neural Networks", "comments": "accepted to EACL 2017 (fixed latex formatting in previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to combine the rich multistep inference of symbolic logical\nreasoning with the generalization capabilities of neural networks. We are\nparticularly interested in complex reasoning about entities and relations in\ntext and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs\nto compose the distributed semantics of multi-hop paths in KBs; however for\nmultiple reasons, the approach lacks accuracy and practicality. This paper\nproposes three significant modeling advances: (1) we learn to jointly reason\nabout relations, entities, and entity-types; (2) we use neural attention\nmodeling to incorporate multiple paths; (3) we learn to share strength in a\nsingle RNN that represents logical composition across all relations. On a\nlargescale Freebase+ClueWeb prediction task, we achieve 25% error reduction,\nand a 53% error reduction on sparse relations due to shared strength. On chains\nof reasoning in WordNet we reduce error in mean quantile by 84% versus previous\nstate-of-the-art. The code and data are available at\nhttps://rajarshd.github.io/ChainsofReasoning\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 21:59:04 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 02:49:16 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 14:10:43 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Das", "Rajarshi", ""], ["Neelakantan", "Arvind", ""], ["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1607.01432", "submitter": "Kenton Lee", "authors": "Kenton Lee, Mike Lewis, Luke Zettlemoyer", "title": "Global Neural CCG Parsing with Optimality Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first global recursive neural parsing model with optimality\nguarantees during decoding. To support global features, we give up dynamic\nprograms and instead search directly in the space of all possible subtrees.\nAlthough this space is exponentially large in the sentence length, we show it\nis possible to learn an efficient A* parser. We augment existing parsing\nmodels, which have informative bounds on the outside score, with a global model\nthat has loose bounds but only needs to model non-local phenomena. The global\nmodel is trained with a new objective that encourages the parser to explore a\ntiny fraction of the search space. The approach is applied to CCG parsing,\nimproving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal\nparse for 99.9% of held-out sentences, exploring on average only 190 subtrees.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 22:25:10 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 01:41:43 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Lee", "Kenton", ""], ["Lewis", "Mike", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1607.01485", "submitter": "Normunds Gruzitis", "authors": "John J. Camilleri, Normunds Gruzitis, Gerardo Schneider", "title": "Extracting Formal Models from Normative Texts", "comments": null, "journal-ref": "Natural Language Processing and Information Systems, Lecture Notes\n  in Computer Science, Vol. 9612, Springer, 2016, pp. 403-408", "doi": "10.1007/978-3-319-41754-7_40", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normative texts are documents based on the deontic notions of obligation,\npermission, and prohibition. Our goal is to model such texts using the C-O\nDiagram formalism, making them amenable to formal analysis, in particular\nverifying that a text satisfies properties concerning causality of actions and\ntiming constraints. We present an experimental, semi-automatic aid to bridge\nthe gap between a normative text and its formal representation. Our approach\nuses dependency trees combined with our own rules and heuristics for extracting\nthe relevant components. The resulting tabular data can then be converted into\na C-O Diagram.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 06:25:07 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Camilleri", "John J.", ""], ["Gruzitis", "Normunds", ""], ["Schneider", "Gerardo", ""]]}, {"id": "1607.01490", "submitter": "Normunds Gruzitis", "authors": "Ren\\=ars Liepi\\c{n}\\v{s}, Uldis Boj\\=ars, Normunds Gr\\=uz\\=itis,\n  K\\=arlis \\v{C}er\\=ans, Edgars Celms", "title": "Towards Self-explanatory Ontology Visualization with Contextual\n  Verbalization", "comments": null, "journal-ref": "Databases and Information Systems, Communications in Computer and\n  Information Science, Vol. 615, Springer, 2016, pp. 3-17", "doi": "10.1007/978-3-319-40180-5_1", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies are one of the core foundations of the Semantic Web. To\nparticipate in Semantic Web projects, domain experts need to be able to\nunderstand the ontologies involved. Visual notations can provide an overview of\nthe ontology and help users to understand the connections among entities.\nHowever, the users first need to learn the visual notation before they can\ninterpret it correctly. Controlled natural language representation would be\nreadable right away and might be preferred in case of complex axioms, however,\nthe structure of the ontology would remain less apparent. We propose to combine\nontology visualizations with contextual ontology verbalizations of selected\nontology (diagram) elements, displaying controlled natural language (CNL)\nexplanations of OWL axioms corresponding to the selected visual notation\nelements. Thus, the domain experts will benefit from both the high-level\noverview provided by the graphical notation and the detailed textual\nexplanations of particular elements in the diagram.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 06:58:31 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Liepi\u0146\u0161", "Ren\u0101rs", ""], ["Boj\u0101rs", "Uldis", ""], ["Gr\u016bz\u012btis", "Normunds", ""], ["\u010cer\u0101ns", "K\u0101rlis", ""], ["Celms", "Edgars", ""]]}, {"id": "1607.01628", "submitter": "Evgeny Matusov", "authors": "Wenhu Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter", "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective way for biasing the attention\nmechanism of a sequence-to-sequence neural machine translation (NMT) model\ntowards the well-studied statistical word alignment models. We show that our\nnovel guided alignment training approach improves translation quality on\nreal-life e-commerce texts consisting of product titles and descriptions,\novercoming the problems posed by many unknown words and a large type/token\nratio. We also show that meta-data associated with input texts such as topic or\ncategory information can significantly improve translation quality when used as\nan additional signal to the decoder part of the network. With both novel\nfeatures, the BLEU score of the NMT system on a product title set improves from\n18.6 to 21.3%. Even larger MT quality gains are obtained through domain\nadaptation of a general domain NMT system to e-commerce data. The developed NMT\nsystem also performs well on the IWSLT speech translation task, where an\nensemble of four variant systems outperforms the phrase-based baseline by 2.1%\nBLEU absolute.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:13:12 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Chen", "Wenhu", ""], ["Matusov", "Evgeny", ""], ["Khadivi", "Shahram", ""], ["Peter", "Jan-Thorsten", ""]]}, {"id": "1607.01759", "submitter": "Armand Joulin", "authors": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov", "title": "Bag of Tricks for Efficient Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a simple and efficient baseline for text classification.\nOur experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation. We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 19:40:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 19:27:54 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 17:38:43 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Joulin", "Armand", ""], ["Grave", "Edouard", ""], ["Bojanowski", "Piotr", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1607.01856", "submitter": "Xiaoqing Li Xiaoqing Li", "authors": "Xiaoqing Li, Jiajun Zhang and Chengqing Zong", "title": "Neural Name Translation Improves Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to control computational complexity, neural machine translation\n(NMT) systems convert all rare words outside the vocabulary into a single unk\nsymbol. Previous solution (Luong et al., 2015) resorts to use multiple numbered\nunks to learn the correspondence between source and target rare words. However,\ntesting words unseen in the training corpus cannot be handled by this method.\nAnd it also suffers from the noisy word alignment. In this paper, we focus on a\nmajor type of rare words -- named entity (NE), and propose to translate them\nwith character level sequence to sequence model. The NE translation model is\nfurther used to derive high quality NE alignment in the bilingual training\ncorpus. With the integration of NE translation and alignment modules, our NMT\nsystem is able to surpass the baseline system by 2.9 BLEU points on the Chinese\nto English task.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 02:25:57 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Li", "Xiaoqing", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1607.01869", "submitter": "Mihajlo Grbovic", "authors": "Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio\n  Silvestri, Ricardo Baeza-Yates, Andrew Feng, Erik Ordentlich, Lee Yang, Gavin\n  Owens", "title": "Scalable Semantic Matching of Queries to Ads in Sponsored Search\n  Advertising", "comments": "10 pages, 4 figures, 39th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy", "journal-ref": "39th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval, SIGIR 2016, Pisa, Italy", "doi": "10.1145/2911451.2911538.", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored search represents a major source of revenue for web search engines.\nThis popular advertising model brings a unique possibility for advertisers to\ntarget users' immediate intent communicated through a search query, usually by\ndisplaying their ads alongside organic search results for queries deemed\nrelevant to their products or services. However, due to a large number of\nunique queries it is challenging for advertisers to identify all such relevant\nqueries. For this reason search engines often provide a service of advanced\nmatching, which automatically finds additional relevant queries for advertisers\nto bid on. We present a novel advanced matching approach based on the idea of\nsemantic embeddings of queries and ads. The embeddings were learned using a\nlarge data set of user search sessions, consisting of search queries, clicked\nads and search links, while utilizing contextual information such as dwell time\nand skipped ads. To address the large-scale nature of our problem, both in\nterms of data and vocabulary size, we propose a novel distributed algorithm for\ntraining of the embeddings. Finally, we present an approach for overcoming a\ncold-start problem associated with new ads and queries. We report results of\neditorial evaluation and online tests on actual search traffic. The results\nshow that our approach significantly outperforms baselines in terms of\nrelevance, coverage, and incremental revenue. Lastly, we open-source learned\nquery embeddings to be used by researchers in computational advertising and\nrelated fields.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 03:43:12 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Grbovic", "Mihajlo", ""], ["Djuric", "Nemanja", ""], ["Radosavljevic", "Vladan", ""], ["Silvestri", "Fabrizio", ""], ["Baeza-Yates", "Ricardo", ""], ["Feng", "Andrew", ""], ["Ordentlich", "Erik", ""], ["Yang", "Lee", ""], ["Owens", "Gavin", ""]]}, {"id": "1607.01958", "submitter": "Kalyani Joshi Ms", "authors": "Joshi Kalyani, Prof. H. N. Bharathi, Prof. Rao Jyothi", "title": "Stock trend prediction using news sentiment analysis", "comments": "11 PAGES, 4 FIGURES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Market Hypothesis is the popular theory about stock prediction.\nWith its failure much research has been carried in the area of prediction of\nstocks. This project is about taking non quantifiable data such as financial\nnews articles about a company and predicting its future stock trend with news\nsentiment classification. Assuming that news articles have impact on stock\nmarket, this is an attempt to study relationship between news and stock trend.\nTo show this, we created three different classification models which depict\npolarity of news articles being positive or negative. Observations show that RF\nand SVM perform well in all types of testing. Na\\\"ive Bayes gives good result\nbut not compared to the other two. Experiments are conducted to evaluate\nvarious aspects of the proposed model and encouraging results are obtained in\nall of the experiments. The accuracy of the prediction model is more than 80%\nand in comparison with news random labeling with 50% of accuracy; the model has\nincreased the accuracy by 30%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 10:48:34 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Kalyani", "Joshi", ""], ["Bharathi", "Prof. H. N.", ""], ["Jyothi", "Prof. Rao", ""]]}, {"id": "1607.01963", "submitter": "Liang Lu", "authors": "Liang Lu", "title": "Sequence Training and Adaptation of Highway Deep Neural Networks", "comments": "6 pages, 3 figures, published at IEEE SLT 2016. arXiv admin note:\n  text overlap with arXiv:1610.05812", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highway deep neural network (HDNN) is a type of depth-gated feedforward\nneural network, which has shown to be easier to train with more hidden layers\nand also generalise better compared to conventional plain deep neural networks\n(DNNs). Previously, we investigated a structured HDNN architecture for speech\nrecognition, in which the two gate functions were tied across all the hidden\nlayers, and we were able to train a much smaller model without sacrificing the\nrecognition accuracy. In this paper, we carry on the study of this architecture\nwith sequence-discriminative training criterion and speaker adaptation\ntechniques on the AMI meeting speech recognition corpus. We show that these two\ntechniques improve speech recognition accuracy on top of the model trained with\nthe cross entropy criterion. Furthermore, we demonstrate that the two gate\nfunctions that are tied across all the hidden layers are able to control the\ninformation flow over the whole network, and we can achieve considerable\nimprovements by only updating these gate functions in both sequence training\nand adaptation experiments.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 11:24:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 15:19:55 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 10:10:30 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 10:23:26 GMT"}, {"version": "v5", "created": "Wed, 22 Mar 2017 15:59:30 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Lu", "Liang", ""]]}, {"id": "1607.01990", "submitter": "Nuria Bel", "authors": "N\\'uria Bel, Mikel L. Forcada and Asunci\\'on G\\'omez-P\\'erez", "title": "A Maturity Model for Public Administration as Open Translation Data\n  Providers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any public administration that produces translation data can be a provider of\nuseful reusable data to meet its own translation needs and the ones of other\npublic organizations and private companies that work with texts of the same\ndomain. These data can also be crucial to produce domain-tuned Machine\nTranslation systems. The organization's management of the translation process,\nthe characteristics of the archives of the generated resources and of the\ninfrastructure available to support them determine the efficiency and the\neffectiveness with which the materials produced can be converted into reusable\ndata. However, it is of utmost importance that the organizations themselves\nfirst become aware of the goods they are producing and, second, adapt their\ninternal processes to become optimal providers. In this article, we propose a\nMaturity Model to help these organizations to achieve it by identifying the\ndifferent stages of the management of translation data that determine the path\nto the aforementioned goal.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:35:31 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Bel", "N\u00faria", ""], ["Forcada", "Mikel L.", ""], ["G\u00f3mez-P\u00e9rez", "Asunci\u00f3n", ""]]}, {"id": "1607.02061", "submitter": "Enrico Santus", "authors": "Emmanuele Chersoni, Enrico Santus, Alessandro Lenci, Philippe Blache\n  and Chu-Ren Huang", "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies on sentence processing suggest that the mental lexicon keeps\ntrack of the mutual expectations between words. Current DSMs, however,\nrepresent context words as separate features, thereby loosing important\ninformation for word expectations, such as word interrelations. In this paper,\nwe present a DSM that addresses this issue by defining verb contexts as joint\nsyntactic dependencies. We test our representation in a verb similarity task on\ntwo datasets, showing that joint contexts achieve performances comparable to\nsingle dependencies or even better. Moreover, they are able to overcome the\ndata sparsity problem of joint feature spaces, in spite of the limited size of\nour training corpus.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 16:00:33 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 10:49:58 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Chersoni", "Emmanuele", ""], ["Santus", "Enrico", ""], ["Lenci", "Alessandro", ""], ["Blache", "Philippe", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1607.02109", "submitter": "John J Nay", "authors": "John J. Nay", "title": "Predicting and Understanding Law-Making with Word Vectors and an\n  Ensemble Model", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0176999", "report-no": null, "categories": "cs.CL physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015,\nonly 2,513 were enacted. We developed a machine learning approach to\nforecasting the probability that any bill will become law. Starting in 2001\nwith the 107th Congress, we trained models on data from previous Congresses,\npredicted all bills in the current Congress, and repeated until the 113th\nCongress served as the test. For prediction we scored each sentence of a bill\nwith a language model that embeds legislative vocabulary into a\nhigh-dimensional, semantic-laden vector space. This language representation\nenables our investigation into which words increase the probability of\nenactment for any topic. To test the relative importance of text and context,\nwe compared the text model to a context-only model that uses variables such as\nwhether the bill's sponsor is in the majority party. To test the effect of\nchanges to bills after their introduction on our ability to predict their final\noutcome, we compared using the bill text and meta-data available at the time of\nintroduction with using the most recent data. At the time of introduction\ncontext-only predictions outperform text-only, and with the newest data\ntext-only outperforms context-only. Combining text and context always performs\nbest. We conducted a global sensitivity analysis on the combined model to\ndetermine important variables predicting enactment.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 18:08:59 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 17:12:33 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Nay", "John J.", ""]]}, {"id": "1607.02250", "submitter": "Yiming Cui", "authors": "Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang and Guoping Hu", "title": "Consensus Attention-based Neural Networks for Chinese Reading\n  Comprehension", "comments": "9+1 pages, published at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading comprehension has embraced a booming in recent NLP research. Several\ninstitutes have released the Cloze-style reading comprehension data, and these\nhave greatly accelerated the research of machine comprehension. In this work,\nwe firstly present Chinese reading comprehension datasets, which consist of\nPeople Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we\npropose a consensus attention-based neural network architecture to tackle the\nCloze-style reading comprehension problem, which aims to induce a consensus\nattention over every words in the query. Experimental results show that the\nproposed neural network significantly outperforms the state-of-the-art\nbaselines in several public datasets. Furthermore, we setup a baseline for\nChinese reading comprehension task, and hopefully this would speed up the\nprocess for future research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:46:48 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 05:49:42 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 09:21:09 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Chen", "Zhipeng", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1607.02310", "submitter": "Tamara Polajnar", "authors": "Tamara Polajnar", "title": "Collaborative Training of Tensors for Compositional Distributional\n  Semantics", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type-based compositional distributional semantic models present an\ninteresting line of research into functional representations of linguistic\nmeaning. One of the drawbacks of such models, however, is the lack of training\ndata required to train each word-type combination. In this paper we address\nthis by introducing training methods that share parameters between similar\nwords. We show that these methods enable zero-shot learning for words that have\nno training data at all, as well as enabling construction of high-quality\ntensors from very few training examples per word.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 11:01:56 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 14:09:05 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 11:17:57 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Polajnar", "Tamara", ""]]}, {"id": "1607.02355", "submitter": "Dr. Zubair Asghar", "authors": "Aurangzeb khan, Khairullah khan, Shakeel Ahmad, Fazal Masood Kundi,\n  Irum Tareen, Muhammad Zubair Asghar", "title": "Lexical Based Semantic Orientation of Online Customer Reviews and Blogs", "comments": null, "journal-ref": "Journal of American Science 2014;10(8)\n  http://www.jofamericanscience.org", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid increase in internet users along with growing power of online review\nsites and social media has given birth to sentiment analysis or opinion mining,\nwhich aims at determining what other people think and comment. Sentiments or\nOpinions contain public generated content about products, services, policies\nand politics. People are usually interested to seek positive and negative\nopinions containing likes and dislikes, shared by users for features of\nparticular product or service. This paper proposed sentence-level lexical based\ndomain independent sentiment classification method for different types of data\nsuch as reviews and blogs. The proposed method is based on general lexicons\ni.e. WordNet, SentiWordNet and user defined lexical dictionaries for semantic\norientation. The relations and glosses of these dictionaries provide solution\nto the domain portability problem. The method performs better than word and\ntext level corpus based machine learning methods for semantic orientation. The\nresults show the proposed method performs better as it shows precision of 87%\nand83% at document and sentence levels respectively for online comments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 13:20:35 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["khan", "Aurangzeb", ""], ["khan", "Khairullah", ""], ["Ahmad", "Shakeel", ""], ["Kundi", "Fazal Masood", ""], ["Tareen", "Irum", ""], ["Asghar", "Muhammad Zubair", ""]]}, {"id": "1607.02436", "submitter": "Rocco Tripodi", "authors": "Rocco Tripodi and Marcello Pelillo", "title": "Document Clustering Games in Static and Dynamic Scenarios", "comments": "This paper will be published in the series Lecture Notes in Computer\n  Science (LNCS) published by Springer, containing the ICPRAM 2016 best papers", "journal-ref": null, "doi": "10.1007/978-3-319-53375-9_2", "report-no": null, "categories": "cs.AI cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a game theoretic model for document clustering. Each\ndocument to be clustered is represented as a player and each cluster as a\nstrategy. The players receive a reward interacting with other players that they\ntry to maximize choosing their best strategies. The geometry of the data is\nmodeled with a weighted graph that encodes the pairwise similarity among\ndocuments, so that similar players are constrained to choose similar\nstrategies, updating their strategy preferences at each iteration of the games.\nWe used different approaches to find the prototypical elements of the clusters\nand with this information we divided the players into two disjoint sets, one\ncollecting players with a definite strategy and the other one collecting\nplayers that try to learn from others the correct strategy to play. The latter\nset of players can be considered as new data points that have to be clustered\naccording to previous information. This representation is useful in scenarios\nin which the data are streamed continuously. The evaluation of the system was\nconducted on 13 document datasets using different settings. It shows that the\nproposed method performs well compared to different document clustering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:17:12 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Tripodi", "Rocco", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1607.02467", "submitter": "Marc Dymetman", "authors": "Marc Dymetman, Chunyang Xiao", "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge", "comments": "Updated version of arXiv:1607.02467. Presented at the NIPS-2016 RNN\n  Symposium, Barcelona, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 17:35:51 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 10:56:22 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Dymetman", "Marc", ""], ["Xiao", "Chunyang", ""]]}, {"id": "1607.02501", "submitter": "Nemanja Spasojevic", "authors": "Adithya Rao, Nemanja Spasojevic", "title": "Actionable and Political Text Classification using Word Embeddings and\n  LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we apply word embeddings and neural networks with Long\nShort-Term Memory (LSTM) to text classification problems, where the\nclassification criteria are decided by the context of the application. We\nexamine two applications in particular. The first is that of Actionability,\nwhere we build models to classify social media messages from customers of\nservice providers as Actionable or Non-Actionable. We build models for over 30\ndifferent languages for actionability, and most of the models achieve accuracy\naround 85%, with some reaching over 90% accuracy. We also show that using LSTM\nneural networks with word embeddings vastly outperform traditional techniques.\nSecond, we explore classification of messages with respect to political\nleaning, where social media messages are classified as Democratic or\nRepublican. The model is able to classify messages with a high accuracy of\n87.57%. As part of our experiments, we vary different hyperparameters of the\nneural networks, and report the effect of such variation on the accuracy. These\nactionability models have been deployed to production and help company agents\nprovide customer support by prioritizing which messages to respond to. The\nmodel for political leaning has been opened and made available for wider use.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 19:53:56 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 20:53:32 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Rao", "Adithya", ""], ["Spasojevic", "Nemanja", ""]]}, {"id": "1607.02576", "submitter": "K Paramesha", "authors": "K Paramesha and K C Ravishankar", "title": "Analysis of opinionated text for opinion mining", "comments": "Sentiment Analysis, Features, Feature Engineering, Emotions, Word\n  Sense Disambiguation, Sentiment Lexicons, Meta-Information", "journal-ref": "Machine Learning and Applications: An International Journal\n  (MLAIJ) Vol.3, No.2, June 2016", "doi": "10.5121/mlaij.2016.3204", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In sentiment analysis, the polarities of the opinions expressed on an\nobject/feature are determined to assess the sentiment of a sentence or document\nwhether it is positive/negative/neutral. Naturally, the object/feature is a\nnoun representation which refers to a product or a component of a product, let\nus say, the \"lens\" in a camera and opinions emanating on it are captured in\nadjectives, verbs, adverbs and noun words themselves. Apart from such words,\nother meta-information and diverse effective features are also going to play an\nimportant role in influencing the sentiment polarity and contribute\nsignificantly to the performance of the system. In this paper, some of the\nassociated information/meta-data are explored and investigated in the sentiment\ntext. Based on the analysis results presented here, there is scope for further\nassessment and utilization of the meta-information as features in text\ncategorization, ranking text document, identification of spam documents and\npolarity classification problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 07:11:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 15:54:29 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Paramesha", "K", ""], ["Ravishankar", "K C", ""]]}, {"id": "1607.02769", "submitter": "Gitit Kehat", "authors": "Gitit Kehat and James Pustejovsky", "title": "Annotation Methodologies for Vision and Language Dataset Creation", "comments": "in Scene Understanding Workshop (SUNw) in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated datasets are commonly used in the training and evaluation of tasks\ninvolving natural language and vision (image description generation, action\nrecognition and visual question answering). However, many of the existing\ndatasets reflect problems that emerge in the process of data selection and\nannotation. Here we point out some of the difficulties and problems one\nconfronts when creating and validating annotated vision and language datasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 18:11:27 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kehat", "Gitit", ""], ["Pustejovsky", "James", ""]]}, {"id": "1607.02784", "submitter": "Duc-Thuan Vo", "authors": "Duc-Thuan Vo and Ebrahim Bagheri", "title": "Open Information Extraction", "comments": "This paper will appear in the Encyclopedia for Semantic Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Information Extraction (Open IE) systems aim to obtain relation tuples\nwith highly scalable extraction in portable across domain by identifying a\nvariety of relation phrases and their arguments in arbitrary sentences. The\nfirst generation of Open IE learns linear chain models based on unlexicalized\nfeatures such as Part-of-Speech (POS) or shallow tags to label the intermediate\nwords between pair of potential arguments for identifying extractable\nrelations. Open IE currently is developed in the second generation that is able\nto extract instances of the most frequently observed relation types such as\nVerb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic\nanalysis. They expose simple yet principled ways in which verbs express\nrelationships in linguistics such as verb phrase-based extraction or\nclause-based extraction. They obtain a significantly higher performance over\nprevious systems in the first generation. In this paper, we describe an\noverview of two Open IE generations including strengths, weaknesses and\napplication areas.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 20:39:24 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Vo", "Duc-Thuan", ""], ["Bagheri", "Ebrahim", ""]]}, {"id": "1607.02789", "submitter": "John Wieting", "authors": "John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu", "title": "Charagram: Embedding Words and Sentences via Character n-grams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Charagram embeddings, a simple approach for learning\ncharacter-based compositional models to embed textual sequences. A word or\nsentence is represented using a character n-gram count vector, followed by a\nsingle nonlinear transformation to yield a low-dimensional embedding. We use\nthree tasks for evaluation: word similarity, sentence similarity, and\npart-of-speech tagging. We demonstrate that Charagram embeddings outperform\nmore complex architectures based on character-level recurrent and convolutional\nneural networks, achieving new state-of-the-art performance on several\nsimilarity tasks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 21:59:19 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Wieting", "John", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1607.02791", "submitter": "Matilde Marcolli", "authors": "Kevin Shu, Sharjeel Aziz, Vy-Luan Huynh, David Warrick, Matilde\n  Marcolli", "title": "Syntactic Phylogenetic Trees", "comments": "21 pages, LaTeX, jpg figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we identify several serious problems that arise in the use of\nsyntactic data from the SSWL database for the purpose of computational\nphylogenetic reconstruction. We show that the most naive approach fails to\nproduce reliable linguistic phylogenetic trees. We identify some of the sources\nof the observed problems and we discuss how they may be, at least partly,\ncorrected by using additional information, such as prior subdivision into\nlanguage families and subfamilies, and a better use of the information about\nancient languages. We also describe how the use of phylogenetic algebraic\ngeometry can help in estimating to what extent the probability distribution at\nthe leaves of the phylogenetic tree obtained from the SSWL data can be\nconsidered reliable, by testing it on phylogenetic trees established by other\nforms of linguistic analysis. In simple examples, we find that, after\nrestricting to smaller language subfamilies and considering only those SSWL\nparameters that are fully mapped for the whole subfamily, the SSWL data match\nextremely well reliable phylogenetic trees, according to the evaluation of\nphylogenetic invariants. This is a promising sign for the use of SSWL data for\nlinguistic phylogenetics.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 22:25:43 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Shu", "Kevin", ""], ["Aziz", "Sharjeel", ""], ["Huynh", "Vy-Luan", ""], ["Warrick", "David", ""], ["Marcolli", "Matilde", ""]]}, {"id": "1607.02802", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt", "title": "Mapping distributional to model-theoretic semantic spaces: a baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been shown to be useful across state-of-the-art systems\nin many natural language processing tasks, ranging from question answering\nsystems to dependency parsing. (Herbelot and Vecchi, 2015) explored word\nembeddings and their utility for modeling language semantics. In particular,\nthey presented an approach to automatically map a standard distributional\nsemantic space onto a set-theoretic model using partial least squares\nregression. We show in this paper that a simple baseline achieves a +51%\nrelative improvement compared to their model on one of the two datasets they\nused, and yields competitive results on the second dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 01:20:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Dernoncourt", "Franck", ""]]}, {"id": "1607.02810", "submitter": "Mahnoosh Kholghi", "authors": "Mahnoosh Kholghi, Lance De Vine, Laurianne Sitbon, Guido Zuccon,\n  Anthony Nguyen", "title": "The Benefits of Word Embeddings Features for Active Learning in Clinical\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the use of unsupervised word embeddings and sequence\nfeatures for sample representation in an active learning framework built to\nextract clinical concepts from clinical free text. The objective is to further\nreduce the manual annotation effort while achieving higher effectiveness\ncompared to a set of baseline features. Unsupervised features are derived from\nskip-gram word embeddings and a sequence representation approach. The\ncomparative performance of unsupervised features and baseline hand-crafted\nfeatures in an active learning framework are investigated using a wide range of\nselection criteria including least confidence, information diversity,\ninformation density and diversity, and domain knowledge informativeness. Two\nclinical datasets are used for evaluation: the i2b2/VA 2010 NLP challenge and\nthe ShARe/CLEF 2013 eHealth Evaluation Lab. Our results demonstrate significant\nimprovements in terms of effectiveness as well as annotation effort savings\nacross both datasets. Using unsupervised features along with baseline features\nfor sample representation lead to further savings of up to 9% and 10% of the\ntoken and concept annotation rates, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 02:46:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 00:25:18 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 00:16:30 GMT"}, {"version": "v4", "created": "Tue, 15 Nov 2016 05:06:01 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kholghi", "Mahnoosh", ""], ["De Vine", "Lance", ""], ["Sitbon", "Laurianne", ""], ["Zuccon", "Guido", ""], ["Nguyen", "Anthony", ""]]}, {"id": "1607.03055", "submitter": "Derek Greene", "authors": "Derek Greene and James P. Cross", "title": "Exploring the Political Agenda of the European Parliament Using a\n  Dynamic Topic Modeling Approach", "comments": "Long version including appendix. arXiv admin note: substantial text\n  overlap with arXiv:1505.07302", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study analyzes the political agenda of the European Parliament (EP)\nplenary, how it has evolved over time, and the manner in which Members of the\nEuropean Parliament (MEPs) have reacted to external and internal stimuli when\nmaking plenary speeches. To unveil the plenary agenda and detect latent themes\nin legislative speeches over time, MEP speech content is analyzed using a new\ndynamic topic modeling method based on two layers of Non-negative Matrix\nFactorization (NMF). This method is applied to a new corpus of all English\nlanguage legislative speeches in the EP plenary from the period 1999-2014. Our\nfindings suggest that two-layer NMF is a valuable alternative to existing\ndynamic topic modeling approaches found in the literature, and can unveil niche\ntopics and associated vocabularies not captured by existing methods.\nSubstantively, our findings suggest that the political agenda of the EP evolves\nsignificantly over time and reacts to exogenous events such as EU Treaty\nreferenda and the emergence of the Euro-crisis. MEP contributions to the\nplenary agenda are also found to be impacted upon by voting behaviour and the\ncommittee structure of the Parliament.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 17:48:53 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Greene", "Derek", ""], ["Cross", "James P.", ""]]}, {"id": "1607.03316", "submitter": "Dirk Weissenborn", "authors": "Dirk Weissenborn", "title": "Separating Answers from Queries for Neural Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural architecture for answering queries, designed to\noptimally leverage explicit support in the form of query-answer memories. Our\nmodel is able to refine and update a given query while separately accumulating\nevidence for predicting the answer. Its architecture reflects this separation\nwith dedicated embedding matrices and loosely connected information pathways\n(modules) for updating the query and accumulating evidence. This separation of\nresponsibilities effectively decouples the search for query related support and\nthe prediction of the answer. On recent benchmark datasets for reading\ncomprehension, our model achieves state-of-the-art results. A qualitative\nanalysis reveals that the model effectively accumulates weighted evidence from\nthe query and over multiple support retrieval cycles which results in a robust\nanswer prediction.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 11:43:15 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 11:54:46 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 13:37:41 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Weissenborn", "Dirk", ""]]}, {"id": "1607.03474", "submitter": "Julian Georg Zilly", "authors": "Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn\\'ik and\n  J\\\"urgen Schmidhuber", "title": "Recurrent Highway Networks", "comments": "12 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n'deep' transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:36:50 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 17:07:42 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 19:39:22 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 21:10:42 GMT"}, {"version": "v5", "created": "Tue, 4 Jul 2017 19:29:23 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Zilly", "Julian Georg", ""], ["Srivastava", "Rupesh Kumar", ""], ["Koutn\u00edk", "Jan", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1607.03542", "submitter": "Matt Gardner", "authors": "Matt Gardner and Jayant Krishnamurthy", "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and\n  Formal Knowledge", "comments": "Re-written abstract and intro, other minor changes throughout. This\n  version published at AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional semantic parsers map language onto compositional, executable\nqueries in a fixed schema. This mapping allows them to effectively leverage the\ninformation contained in large, formal knowledge bases (KBs, e.g., Freebase) to\nanswer questions, but it is also fundamentally limiting---these semantic\nparsers can only assign meaning to language that falls within the KB's\nmanually-produced schema. Recently proposed methods for open vocabulary\nsemantic parsing overcome this limitation by learning execution models for\narbitrary language, essentially using a text corpus as a kind of knowledge\nbase. However, all prior approaches to open vocabulary semantic parsing replace\na formal KB with textual information, making no use of the KB in their models.\nWe show how to combine the disparate representations used by these two\napproaches, presenting for the first time a semantic parser that (1) produces\ncompositional, executable representations of language, (2) can successfully\nleverage the information contained in both a formal KB and a large corpus, and\n(3) is not limited to the schema of the underlying KB. We demonstrate\nsignificantly improved performance over state-of-the-art baselines on an\nopen-domain natural language question answering task.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 23:13:26 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 21:44:30 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Gardner", "Matt", ""], ["Krishnamurthy", "Jayant", ""]]}, {"id": "1607.03707", "submitter": "Hwiyeol Jo", "authors": "Hwiyeol Jo, Yohan Moon, Jong In Kim, and Jeong Ryu", "title": "Re-presenting a Story by Emotional Factors using Sentimental Analysis\n  Method", "comments": "Paper version of CogSci2016; We should correct poor English", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remembering an event is affected by personal emotional status. We examined\nthe psychological status and personal factors; depression (Center for\nEpidemiological Studies - Depression, Radloff, 1977), present affective\n(Positive Affective and Negative Affective Schedule, Watson et al., 1988), life\norient (Life Orient Test, Scheier & Carver, 1985), self-awareness (Core Self\nEvaluation Scale, Judge et al., 2003), and social factor (Social Support,\nSarason et al., 1983) of undergraduate students (N=64) and got summaries of a\nstory, Chronicle of a Death Foretold (Gabriel Garcia Marquez, 1981) from them.\nWe implement a sentimental analysis model based on convolutional neural network\n(LeCun & Bengio, 1995) to evaluate each summary. From the same vein used for\ntransfer learning (Pan & Yang, 2010), we collected 38,265 movie review data to\ntrain the model and then use them to score summaries of each student. The\nresults of CES-D and PANAS show the relationship between emotion and memory\nretrieval as follows: depressed people have shown a tendency of representing a\nstory more negatively, and they seemed less expressive. People with full of\nemotion - high in PANAS - have retrieved their memory more expressively than\nothers, using more negative words then others. The contributions of this study\ncan be summarized as follows: First, lightening the relationship between\nemotion and its effect during times of storing or retrieving a memory. Second,\nsuggesting objective methods to evaluate the intensity of emotion in natural\nlanguage format, using a sentimental analysis model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 12:48:33 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Jo", "Hwiyeol", ""], ["Moon", "Yohan", ""], ["Kim", "Jong In", ""], ["Ryu", "Jeong", ""]]}, {"id": "1607.03766", "submitter": "Benjamin Elizalde", "authors": "Sebastian Sager and Benjamin Elizalde and Damian Borth and Christian\n  Schulze and Bhiksha Raj and Ian Lane", "title": "AudioPairBank: Towards A Large-Scale Tag-Pair-Based Audio Content\n  Analysis", "comments": "This paper is a revised version of \"AudioSentibank: Large-scale\n  Semantic Ontology of Acoustic Concepts for Audio Content Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sound recognition has been used to identify sounds, such as car and\nriver. However, sounds have nuances that may be better described by\nadjective-noun pairs such as slow car, and verb-noun pairs such as flying\ninsects, which are under explored. Therefore, in this work we investigate the\nrelation between audio content and both adjective-noun pairs and verb-noun\npairs. Due to the lack of datasets with these kinds of annotations, we\ncollected and processed the AudioPairBank corpus consisting of a combined total\nof 1,123 pairs and over 33,000 audio files. One contribution is the previously\nunavailable documentation of the challenges and implications of collecting\naudio recordings with these type of labels. A second contribution is to show\nthe degree of correlation between the audio content and the labels through\nsound recognition experiments, which yielded results of 70% accuracy, hence\nalso providing a performance benchmark. The results and study in this paper\nencourage further exploration of the nuances in audio and are meant to\ncomplement similar research performed on images and text in multimedia\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 14:31:54 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 23:07:17 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 21:36:04 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Sager", "Sebastian", ""], ["Elizalde", "Benjamin", ""], ["Borth", "Damian", ""], ["Schulze", "Christian", ""], ["Raj", "Bhiksha", ""], ["Lane", "Ian", ""]]}, {"id": "1607.03780", "submitter": "James Henderson", "authors": "James Henderson and Diana Nicoleta Popa", "title": "A Vector Space for Distributional Semantics for Entailment", "comments": "To appear in Proc. 54th Annual Meeting of the Association\n  Computational Linguistics (ACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantics creates vector-space representations that capture\nmany forms of semantic similarity, but their relation to semantic entailment\nhas been less clear. We propose a vector-space model which provides a formal\nfoundation for a distributional semantics of entailment. Using a mean-field\napproximation, we develop approximate inference procedures and entailment\noperators over vectors of probabilities of features being known (versus\nunknown). We use this framework to reinterpret an existing\ndistributional-semantic model (Word2Vec) as approximating an entailment-based\nmodel of the distributions of words in contexts, thereby predicting lexical\nentailment relations. In both unsupervised and semi-supervised experiments on\nhyponymy detection, we get substantial improvements over previous results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 15:08:26 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Henderson", "James", ""], ["Popa", "Diana Nicoleta", ""]]}, {"id": "1607.03827", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Christian Mandery, Tamim Asfour", "title": "The KIT Motion-Language Dataset", "comments": "5 figures, 4 tables, submitted to Big Data journal, Special Issue on\n  Robotics", "journal-ref": null, "doi": "10.1089/big.2016.0028", "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking human motion and natural language is of great interest for the\ngeneration of semantic representations of human activities as well as for the\ngeneration of robot activities based on natural language input. However, while\nthere have been years of research in this area, no standardized and openly\navailable dataset exists to support the development and evaluation of such\nsystems. We therefore propose the KIT Motion-Language Dataset, which is large,\nopen, and extensible. We aggregate data from multiple motion capture databases\nand include them in our dataset using a unified representation that is\nindependent of the capture system or marker set, making it easy to work with\nthe data regardless of its origin. To obtain motion annotations in natural\nlanguage, we apply a crowd-sourcing approach and a web-based tool that was\nspecifically build for this purpose, the Motion Annotation Tool. We thoroughly\ndocument the annotation process itself and discuss gamification methods that we\nused to keep annotators motivated. We further propose a novel method,\nperplexity-based selection, which systematically selects motions for further\nannotation that are either under-represented in our dataset or that have\nerroneous annotations. We show that our method mitigates the two aforementioned\nproblems and ensures a systematic annotation process. We provide an in-depth\nanalysis of the structure and contents of our resulting dataset, which, as of\nOctober 10, 2016, contains 3911 motions with a total duration of 11.23 hours\nand 6278 annotations in natural language that contain 52,903 words. We believe\nthis makes our dataset an excellent choice that enables more transparent and\ncomparable research in this important area.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 17:08:01 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 14:24:47 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Plappert", "Matthias", ""], ["Mandery", "Christian", ""], ["Asfour", "Tamim", ""]]}, {"id": "1607.03895", "submitter": "Liye Fu", "authors": "Liye Fu and Cristian Danescu-Niculescu-Mizil and Lillian Lee", "title": "Tie-breaker: Using language models to quantify gender bias in sports\n  journalism", "comments": "Best paper award at the IJCAI workshop on NLP Meets Journalism; 5\n  pages, 2 figures; data and other info available at\n  http://www.cs.cornell.edu/~liye/tennis.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender bias is an increasingly important issue in sports journalism. In this\nwork, we propose a language-model-based approach to quantify differences in\nquestions posed to female vs. male athletes, and apply it to tennis post-match\ninterviews. We find that journalists ask male players questions that are\ngenerally more focused on the game when compared with the questions they ask\ntheir female counterparts. We also provide a fine-grained analysis of the\nextent to which the salience of this bias depends on various factors, such as\nquestion type, game outcome or player rank.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 20:00:01 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Fu", "Liye", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Lee", "Lillian", ""]]}, {"id": "1607.04110", "submitter": "Giulio Petrucci", "authors": "Giulio Petrucci, Chiara Ghidini, Marco Rospocher", "title": "Using Recurrent Neural Network for Learning Expressive Ontologies", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural Networks have been proven extremely effective in many\nnatural language processing tasks such as sentiment analysis, question\nanswering, or machine translation. Aiming to exploit such advantages in the\nOntology Learning process, in this technical report we present a detailed\ndescription of a Recurrent Neural Network based system to be used to pursue\nsuch goal.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 12:45:07 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Petrucci", "Giulio", ""], ["Ghidini", "Chiara", ""], ["Rospocher", "Marco", ""]]}, {"id": "1607.04315", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Neural Semantic Encoders", "comments": "Accepted in EACL 2017, added: comparison with NTM, qualitative\n  analysis and memory visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a memory augmented neural network for natural language\nunderstanding: Neural Semantic Encoders. NSE is equipped with a novel memory\nupdate rule and has a variable sized encoding memory that evolves over time and\nmaintains the understanding of input sequences through read}, compose and write\noperations. NSE can also access multiple and shared memories. In this paper, we\ndemonstrated the effectiveness and the flexibility of NSE on five different\nnatural language tasks: natural language inference, question answering,\nsentence classification, document sentiment analysis and machine translation\nwhere NSE achieved state-of-the-art performance when evaluated on publically\navailable benchmarks. For example, our shared-memory model showed an\nencouraging result on neural machine translation, improving an attention-based\nbaseline by approximately 1.0 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 20:58:26 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 14:01:11 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 15:41:13 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1607.04423", "submitter": "Yiming Cui", "authors": "Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu and Guoping Hu", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "comments": "8+2 pages. accepted as a conference paper at ACL2017 (long paper)", "journal-ref": "ACL 2017 Vol.1 Long Papers 593-602", "doi": "10.18653/v1/P17-1055", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloze-style queries are representative problems in reading comprehension.\nOver the past few months, we have seen much progress that utilizing neural\nnetwork approach to solve Cloze-style questions. In this paper, we present a\nnovel model called attention-over-attention reader for the Cloze-style reading\ncomprehension task. Our model aims to place another attention mechanism over\nthe document-level attention, and induces \"attended attention\" for final\npredictions. Unlike the previous works, our neural network model requires less\npre-defined hyper-parameters and uses an elegant architecture for modeling.\nExperimental results show that the proposed attention-over-attention model\nsignificantly outperforms various state-of-the-art systems by a large margin in\npublic datasets, such as CNN and Children's Book Test datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:10:11 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 09:46:02 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 06:17:42 GMT"}, {"version": "v4", "created": "Tue, 6 Jun 2017 02:51:54 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Cui", "Yiming", ""], ["Chen", "Zhipeng", ""], ["Wei", "Si", ""], ["Wang", "Shijin", ""], ["Liu", "Ting", ""], ["Hu", "Guoping", ""]]}, {"id": "1607.04492", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Neural Tree Indexers for Text Understanding", "comments": "Accepted at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) process input text sequentially and model\nthe conditional transition between word tokens. In contrast, the advantages of\nrecursive networks include that they explicitly model the compositionality and\nthe recursive structure of natural language. However, the current recursive\narchitecture is limited by its dependence on syntactic tree. In this paper, we\nintroduce a robust syntactic parsing-independent tree structured model, Neural\nTree Indexers (NTI) that provides a middle ground between the sequential RNNs\nand the syntactic treebased recursive models. NTI constructs a full n-ary tree\nby processing the input text with its node function in a bottom-up fashion.\nAttention mechanism can then be applied to both structure and node function. We\nimplemented and evaluated a binarytree model of NTI, showing the model achieved\nthe state-of-the-art performance on three different NLP tasks: natural language\ninference, answer sentence selection, and sentence classification,\noutperforming state-of-the-art recurrent and recursive neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 12:59:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:10:33 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1607.04576", "submitter": "John M. Pierre", "authors": "John M. Pierre, Mark Butler, Jacob Portnoff, and Luis Aguilar", "title": "Neural Discourse Modeling of Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown recent promise in many language-related tasks\nsuch as the modeling of conversations. We extend RNN-based sequence to sequence\nmodels to capture the long range discourse across many turns of conversation.\nWe perform a sensitivity analysis on how much additional context affects\nperformance, and provide quantitative and qualitative evidence that these\nmodels are able to capture discourse relationships across multiple utterances.\nOur results quantifies how adding an additional RNN layer for modeling\ndiscourse improves the quality of output utterances and providing more of the\nprevious conversation as input also improves performance. By searching the\ngenerated outputs for specific discourse markers we show how neural discourse\nmodels can exhibit increased coherence and cohesion in conversations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:43:40 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Pierre", "John M.", ""], ["Butler", "Mark", ""], ["Portnoff", "Jacob", ""], ["Aguilar", "Luis", ""]]}, {"id": "1607.04606", "submitter": "Edouard Grave", "authors": "Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov", "title": "Enriching Word Vectors with Subword Information", "comments": "Accepted to TACL. The two first authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter $n$-grams. A vector representation is associated to each character\n$n$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 18:27:55 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:41:07 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Grave", "Edouard", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1607.04660", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Victor Andrei and Ognjen Arandjelovic", "title": "Identification of promising research directions using machine learning\n  aided medical literature analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1512.08008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly expanding corpus of medical research literature presents major\nchallenges in the understanding of previous work, the extraction of maximum\ninformation from collected data, and the identification of promising research\ndirections. We present a case for the use of advanced machine learning\ntechniques as an aide in this task and introduce a novel methodology that is\nshown to be capable of extracting meaningful information from large\nlongitudinal corpora, and of tracking complex temporal changes within it.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 12:55:36 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Andrei", "Victor", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "1607.04683", "submitter": "Raziel Alvarez", "authors": "Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin", "title": "On the efficient representation and execution of deep acoustic models", "comments": "Accepted conference paper: \"The Annual Conference of the\n  International Speech Communication Association (Interspeech), 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a simple and computationally efficient quantization\nscheme that enables us to reduce the resolution of the parameters of a neural\nnetwork from 32-bit floating point values to 8-bit integer values. The proposed\nquantization scheme leads to significant memory savings and enables the use of\noptimized hardware instructions for integer arithmetic, thus significantly\nreducing the cost of inference. Finally, we propose a \"quantization aware\"\ntraining process that applies the proposed scheme during network training and\nfind that it allows us to recover most of the loss in accuracy introduced by\nquantization. We validate the proposed techniques by applying them to a long\nshort-term memory-based acoustic model on an open-ended large vocabulary speech\nrecognition task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 23:31:45 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 01:31:31 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Alvarez", "Raziel", ""], ["Prabhavalkar", "Rohit", ""], ["Bakhtin", "Anton", ""]]}, {"id": "1607.04853", "submitter": "Anirban Laha", "authors": "Anirban Laha and Vikas Raykar", "title": "An Empirical Evaluation of various Deep Learning Architectures for\n  Bi-Sequence Classification Tasks", "comments": "12 pages (content + references : 10 pages, appendix : 2 pages ). To\n  appear in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several tasks in argumentation mining and debating, question-answering, and\nnatural language inference involve classifying a sequence in the context of\nanother sequence (referred as bi-sequence classification). For several single\nsequence classification tasks, the current state-of-the-art approaches are\nbased on recurrent and convolutional neural networks. On the other hand, for\nbi-sequence classification problems, there is not much understanding as to the\nbest deep learning architecture. In this paper, we attempt to get an\nunderstanding of this category of problems by extensive empirical evaluation of\n19 different deep learning architectures (specifically on different ways of\nhandling context) for various problems originating in natural language\nprocessing like debating, textual entailment and question-answering. Following\nthe empirical evaluation, we offer our insights and conclusions regarding the\narchitectures we have considered. We also establish the first deep learning\nbaselines for three argumentation mining tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 11:15:38 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 20:59:05 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Laha", "Anirban", ""], ["Raykar", "Vikas", ""]]}, {"id": "1607.04982", "submitter": "Juntao Yu", "authors": "Juntao Yu and Bernd Bohnet", "title": "Dependency Language Models for Transition-based Dependency Parsing", "comments": "Accepted by IWPT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach to improve the accuracy of a strong\ntransition-based dependency parser by exploiting dependency language models\nthat are extracted from a large parsed corpus. We integrated a small number of\nfeatures based on the dependency language models into the parser. To\ndemonstrate the effectiveness of the proposed approach, we evaluate our parser\non standard English and Chinese data where the base parser could achieve\ncompetitive accuracy scores. Our enhanced parser achieved state-of-the-art\naccuracy on Chinese data and competitive results on English data. We gained a\nlarge absolute improvement of one point (UAS) on Chinese and 0.5 points for\nEnglish.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 09:04:31 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 21:41:17 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Yu", "Juntao", ""], ["Bohnet", "Bernd", ""]]}, {"id": "1607.05014", "submitter": "Steffen Eger", "authors": "Steffen Eger and Armin Hoenen and Alexander Mehler", "title": "Language classification from bilingual word embedding graphs", "comments": "To be published at Coling 2016", "journal-ref": "COLING 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of the second language in bilingual word embeddings in\nmonolingual semantic evaluation tasks. We find strongly and weakly positive\ncorrelations between down-stream task performance and second language\nsimilarity to the target language. Additionally, we show how bilingual word\nembeddings can be employed for the task of semantic language classification and\nthat joint semantic spaces vary in meaningful ways across second languages. Our\nresults support the hypothesis that semantic language similarity is influenced\nby both structural similarity as well as geography/contact.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 10:59:39 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 23:10:10 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Eger", "Steffen", ""], ["Hoenen", "Armin", ""], ["Mehler", "Alexander", ""]]}, {"id": "1607.05108", "submitter": "Zichao Yang", "authors": "Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, Alex Smola", "title": "Neural Machine Translation with Recurrent Attention Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing which words have been attended to in previous time steps while\ngenerating a translation is a rich source of information for predicting what\nwords will be attended to in the future. We improve upon the attention model of\nBahdanau et al. (2014) by explicitly modeling the relationship between previous\nand subsequent attention levels for each word using one recurrent network per\ninput word. This architecture easily captures informative features, such as\nfertility and regularities in relative distortion. In experiments, we show our\nparameterization of attention improves translation quality.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 14:44:26 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Yang", "Zichao", ""], ["Hu", "Zhiting", ""], ["Deng", "Yuntian", ""], ["Dyer", "Chris", ""], ["Smola", "Alex", ""]]}, {"id": "1607.05142", "submitter": "Matthias Gall\\'e", "authors": "Matthias Galle, Jean-Michel Renders, Guillaume Jacquet", "title": "Joint Event Detection and Entity Resolution: a Virtuous Cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering web documents has numerous applications, such as aggregating news\narticles into meaningful events, detecting trends and hot topics on the Web,\npreserving diversity in search results, etc. At the same time, the importance\nof named entities and, in particular, the ability to recognize them and to\nsolve the associated co-reference resolution problem are widely recognized as\nkey enabling factors when mining, aggregating and comparing content on the Web.\n  Instead of considering these two problems separately, we propose in this\npaper a method that tackles jointly the problem of clustering news articles\ninto events and cross-document co-reference resolution of named entities. The\nco-occurrence of named entities in the same clusters is used as an additional\nsignal to decide whether two referents should be merged into one entity. These\nrefined entities can in turn be used as enhanced features to re-cluster the\ndocuments and then be refined again, entering into a virtuous cycle that\nimproves simultaneously the performances of both tasks. We implemented a\nprototype system and report results using the TDT5 collection of news articles,\ndemonstrating the potential of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:51:11 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Galle", "Matthias", ""], ["Renders", "Jean-Michel", ""], ["Jacquet", "Guillaume", ""]]}, {"id": "1607.05174", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "Is spoken language all-or-nothing? Implications for future speech-based\n  human-machine interaction", "comments": "To appear in K. Jokinen & G. Wilcock (Eds.), Dialogues with Social\n  Robots - Enablements, Analyses, and Evaluation. Springer Lecture Notes in\n  Electrical Engineering (LNEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen significant market penetration for voice-based\npersonal assistants such as Apple's Siri. However, despite this success, user\ntake-up is frustratingly low. This position paper argues that there is a\nhabitability gap caused by the inevitable mismatch between the capabilities and\nexpectations of human users and the features and benefits provided by\ncontemporary technology. Suggestions are made as to how such problems might be\nmitigated, but a more worrisome question emerges: \"is spoken language\nall-or-nothing\"? The answer, based on contemporary views on the special nature\nof (spoken) language, is that there may indeed be a fundamental limit to the\ninteraction that can take place between mismatched interlocutors (such as\nhumans and machines). However, it is concluded that interactions between native\nand non-native speakers, or between adults and children, or even between humans\nand dogs, might provide critical inspiration for the design of future\nspeech-based human-machine interaction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:44:34 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1607.05241", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen", "title": "Imitation Learning with Recurrent Neural Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel view that unifies two frameworks that aim to solve\nsequential prediction problems: learning to search (L2S) and recurrent neural\nnetworks (RNN). We point out equivalences between elements of the two\nframeworks. By complementing what is missing from one framework comparing to\nthe other, we introduce a more advanced imitation learning framework that, on\none hand, augments L2S s notion of search space and, on the other hand,\nenhances RNNs training procedure to be more robust to compounding errors\narising from training on highly correlated examples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 19:01:00 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Nguyen", "Khanh", ""]]}, {"id": "1607.05368", "submitter": "Jey Han Lau", "authors": "Jey Han Lau, Timothy Baldwin", "title": "An Empirical Evaluation of doc2vec with Practical Insights into Document\n  Embedding Generation", "comments": "1st Workshop on Representation Learning for NLP", "journal-ref": "Proceedings of the 1st Workshop on Representation Learning for\n  NLP, Berlin, Germany, pp. 78--86", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec\n(Mikolov et al., 2013a) to learn document-level embeddings. Despite promising\nresults in the original paper, others have struggled to reproduce those\nresults. This paper presents a rigorous empirical evaluation of doc2vec over\ntwo tasks. We compare doc2vec to two baselines and two state-of-the-art\ndocument embedding methodologies. We found that doc2vec performs robustly when\nusing models trained on large external corpora, and can be further improved by\nusing pre-trained word embeddings. We also provide recommendations on\nhyper-parameter settings for general purpose applications, and release source\ncode to induce document embeddings using our trained doc2vec models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 01:55:55 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1607.05408", "submitter": "Matthias Gall\\'e", "authors": "Will Radford, Matthias Galle", "title": "Discriminating between similar languages in Twitter using label\n  propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the language of social media messages is an important first step\nin linguistic processing. Existing models for Twitter focus on content\nanalysis, which is successful for dissimilar language pairs. We propose a label\npropagation approach that takes the social graph of tweet authors into account\nas well as content to better tease apart similar languages. This results in\nstate-of-the-art shared task performance of $76.63\\%$, $1.4\\%$ higher than the\ntop system.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 05:38:58 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Radford", "Will", ""], ["Galle", "Matthias", ""]]}, {"id": "1607.05422", "submitter": "Animesh Dutta", "authors": "Abhijit Adhikari, Shivang Singh, Deepjyoti Mondal, Biswanath Dutta,\n  Animesh Dutta", "title": "A Novel Information Theoretic Framework for Finding Semantic Similarity\n  in WordNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information content (IC) based measures for finding semantic similarity is\ngaining preferences day by day. Semantics of concepts can be highly\ncharacterized by information theory. The conventional way for calculating IC is\nbased on the probability of appearance of concepts in corpora. Due to data\nsparseness and corpora dependency issues of those conventional approaches, a\nnew corpora independent intrinsic IC calculation measure has evolved. In this\npaper, we mainly focus on such intrinsic IC model and several topological\naspects of the underlying ontology. Accuracy of intrinsic IC calculation and\nsemantic similarity measure rely on these aspects deeply. Based on these\nanalysis we propose an information theoretic framework which comprises an\nintrinsic IC calculator and a semantic similarity model. Our approach is\ncompared with state of the art semantic similarity measures based on corpora\ndependent IC calculation as well as intrinsic IC based methods using several\nbenchmark data set. We also compare our model with the related Edge based,\nFeature based and Distributional approaches. Experimental results show that our\nintrinsic IC model gives high correlation value when applied to different\nsemantic similarity models. Our proposed semantic similarity model also\nachieves significant results when embedded with some state of the art IC models\nincluding ours.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:32:26 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Adhikari", "Abhijit", ""], ["Singh", "Shivang", ""], ["Mondal", "Deepjyoti", ""], ["Dutta", "Biswanath", ""], ["Dutta", "Animesh", ""]]}, {"id": "1607.05650", "submitter": "Shanta Phani", "authors": "Shanta Phani, Shibamouli Lahiri and Arindam Biswas", "title": "A Supervised Authorship Attribution Framework for Bengali Language", "comments": "This paper has been withdrawn by the authors as the results need to\n  be changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship Attribution is a long-standing problem in Natural Language\nProcessing. Several statistical and computational methods have been used to\nfind a solution to this problem. In this paper, we have proposed methods to\ndeal with the authorship attribution problem in Bengali.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 06:57:38 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 03:09:17 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Phani", "Shanta", ""], ["Lahiri", "Shibamouli", ""], ["Biswas", "Arindam", ""]]}, {"id": "1607.05666", "submitter": "Yuxuan Wang", "authors": "Yuxuan Wang, Pascal Getreuer, Thad Hughes, Richard F. Lyon, Rif A.\n  Saurous", "title": "Trainable Frontend For Robust and Far-Field Keyword Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and far-field speech recognition is critical to enable true hands-free\ncommunication. In far-field conditions, signals are attenuated due to distance.\nTo improve robustness to loudness variation, we introduce a novel frontend\ncalled per-channel energy normalization (PCEN). The key ingredient of PCEN is\nthe use of an automatic gain control based dynamic compression to replace the\nwidely used static (such as log or root) compression. We evaluate PCEN on the\nkeyword spotting task. On our large rerecorded noisy and far-field eval sets,\nwe show that PCEN significantly improves recognition performance. Furthermore,\nwe model PCEN as neural network layers and optimize high-dimensional PCEN\nparameters jointly with the keyword spotting acoustic model. The trained PCEN\nfrontend demonstrates significant further improvements without increasing model\ncomplexity or inference-time cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 17:17:58 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Wang", "Yuxuan", ""], ["Getreuer", "Pascal", ""], ["Hughes", "Thad", ""], ["Lyon", "Richard F.", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1607.05755", "submitter": "Shanta Phani", "authors": "Shanta Phani, Shibamouli Lahiri and Arindam Biswas", "title": "A New Bengali Readability Score", "comments": "This paper has been withdrawn by the author as the results need to be\n  changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed methods to analyze the readability of Bengali\nlanguage texts. We have got some exceptionally good results out of the\nexperiments.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 19:14:00 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 04:45:03 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 03:11:40 GMT"}, {"version": "v4", "created": "Tue, 14 Mar 2017 02:36:05 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Phani", "Shanta", ""], ["Lahiri", "Shibamouli", ""], ["Biswas", "Arindam", ""]]}, {"id": "1607.05809", "submitter": "Kun Xiong", "authors": "Kun Xiong, Anqi Cui, Zefeng Zhang, Ming Li", "title": "Neural Contextual Conversation Learning with Labeled Question-Answering\n  Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural conversational models tend to produce generic or safe responses in\ndifferent contexts, e.g., reply \\textit{\"Of course\"} to narrative statements or\n\\textit{\"I don't know\"} to questions. In this paper, we propose an end-to-end\napproach to avoid such problem in neural generative models. Additional memory\nmechanisms have been introduced to standard sequence-to-sequence (seq2seq)\nmodels, so that context can be considered while generating sentences. Three\nseq2seq models, which memorize a fix-sized contextual vector from hidden input,\nhidden input/output and a gated contextual attention structure respectively,\nhave been trained and tested on a dataset of labeled question-answering pairs\nin Chinese. The model with contextual attention outperforms others including\nthe state-of-the-art seq2seq models on perplexity test. The novel contextual\nmodel generates diverse and robust responses, and is able to carry out\nconversations on a wide range of topics appropriately.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 03:25:31 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Xiong", "Kun", ""], ["Cui", "Anqi", ""], ["Zhang", "Zefeng", ""], ["Li", "Ming", ""]]}, {"id": "1607.05818", "submitter": "Ruey-Cheng Chen", "authors": "Ruey-Cheng Chen, Reid Swanson, and Andrew S. Gordon", "title": "An Adaptation of Topic Modeling to Sentences", "comments": "8 pages, 2010, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in topic modeling have yielded effective methods for characterizing\nthe latent semantics of textual data. However, applying standard topic modeling\napproaches to sentence-level tasks introduces a number of challenges. In this\npaper, we adapt the approach of latent-Dirichlet allocation to include an\nadditional layer for incorporating information about the sentence boundaries in\ndocuments. We show that the addition of this minimal information of document\nstructure improves the perplexity results of a trained model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 04:22:50 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Chen", "Ruey-Cheng", ""], ["Swanson", "Reid", ""], ["Gordon", "Andrew S.", ""]]}, {"id": "1607.05822", "submitter": "Ruey-Cheng Chen", "authors": "Ruey-Cheng Chen", "title": "Incremental Learning for Fully Unsupervised Word Segmentation Using\n  Penalized Likelihood and Model Selection", "comments": "12 pages, 2014, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel incremental learning approach for unsupervised word\nsegmentation that combines features from probabilistic modeling and model\nselection. This includes super-additive penalties for addressing the cognitive\nburden imposed by long word formation, and new model selection criteria based\non higher-order generative assumptions. Our approach is fully unsupervised; it\nrelies on a small number of parameters that permits flexible modeling and a\nmechanism that automatically learns parameters from the data. Through\nexperimentation, we show that this intricate design has led to top-tier\nperformance in both phonemic and orthographic word segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 04:38:01 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 16:31:00 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Chen", "Ruey-Cheng", ""]]}, {"id": "1607.05968", "submitter": "Michael Spranger", "authors": "Michael Spranger and Jakob Suchan and Mehul Bhatt", "title": "Robust Natural Language Processing - Combining Reasoning, Cognitive\n  Semantics and Construction Grammar for Spatial Language", "comments": "in IJCAI'16: Proceedings of the 25th international joint conference\n  on Artificial intelligence, Palo Alto, 2016. AAAI Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for generating and understanding of dynamic and static\nspatial relations in robotic interaction setups. Robots describe an environment\nof moving blocks using English phrases that include spatial relations such as\n\"across\" and \"in front of\". We evaluate the system in robot-robot interactions\nand show that the system can robustly deal with visual perception errors,\nlanguage omissions and ungrammatical utterances.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:15:24 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Spranger", "Michael", ""], ["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1607.06025", "submitter": "Janez Starc", "authors": "Janez Starc and Dunja Mladeni\\'c", "title": "Constructing a Natural Language Inference Dataset using Generative\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference is an important task for Natural Language\nUnderstanding. It is concerned with classifying the logical relation between\ntwo sentences. In this paper, we propose several text generative neural\nnetworks for generating text hypothesis, which allows construction of new\nNatural Language Inference datasets. To evaluate the models, we propose a new\nmetric -- the accuracy of the classifier trained on the generated dataset. The\naccuracy obtained by our best generative model is only 2.7% lower than the\naccuracy of the classifier trained on the original, human crafted dataset.\nFurthermore, the best generated dataset combined with the original dataset\nachieves the highest accuracy. The best model learns a mapping embedding for\neach training example. By comparing various metrics we show that datasets that\nobtain higher ROUGE or METEOR scores do not necessarily yield higher\nclassification accuracies. We also provide analysis of what are the\ncharacteristics of a good dataset including the distinguishability of the\ngenerated datasets from the original one.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:59:21 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 08:33:27 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Starc", "Janez", ""], ["Mladeni\u0107", "Dunja", ""]]}, {"id": "1607.06153", "submitter": "Marek Rei", "authors": "Marek Rei, Helen Yannakoudakis", "title": "Compositional Sequence Labeling Models for Error Detection in Learner\n  Writing", "comments": "Proceedings of ACL 2016", "journal-ref": null, "doi": "10.18653/v1/P16-1112", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the first experiments using neural network models\nfor the task of error detection in learner writing. We perform a systematic\ncomparison of alternative compositional architectures and propose a framework\nfor error detection based on bidirectional LSTMs. Experiments on the CoNLL-14\nshared task dataset show the model is able to outperform other participants on\ndetecting errors in learner writing. Finally, the model is integrated with a\npublicly deployed self-assessment system, leading to performance comparable to\nhuman annotators.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 23:26:33 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Yannakoudakis", "Helen", ""]]}, {"id": "1607.06208", "submitter": "Xiaochang Peng", "authors": "Xiaochang Peng and Daniel Gildea", "title": "Exploring phrase-compositionality in skip-gram models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a variation of the skip-gram model which jointly\nlearns distributed word vector representations and their way of composing to\nform phrase embeddings. In particular, we propose a learning procedure that\nincorporates a phrase-compositionality function which can capture how we want\nto compose phrases vectors from their component word vectors. Our experiments\nshow improvement in word and phrase similarity tasks as well as syntactic tasks\nlike dependency parsing using the proposed joint models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 06:49:02 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Peng", "Xiaochang", ""], ["Gildea", "Daniel", ""]]}, {"id": "1607.06215", "submitter": "Qiyue Yin", "authors": "Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, Liang Wang", "title": "A Comprehensive Survey on Cross-modal Retrieval", "comments": "20 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, cross-modal retrieval has drawn much attention due to the\nrapid growth of multimodal data. It takes one type of data as the query to\nretrieve relevant data of another type. For example, a user can use a text to\nretrieve relevant pictures or videos. Since the query and its retrieved results\ncan be of different modalities, how to measure the content similarity between\ndifferent modalities of data remains a challenge. Various methods have been\nproposed to deal with such a problem. In this paper, we first review a number\nof representative methods for cross-modal retrieval and classify them into two\nmain groups: 1) real-valued representation learning, and 2) binary\nrepresentation learning. Real-valued representation learning methods aim to\nlearn real-valued common representations for different modalities of data. To\nspeed up the cross-modal retrieval, a number of binary representation learning\nmethods are proposed to map different modalities of data into a common Hamming\nspace. Then, we introduce several multimodal datasets in the community, and\nshow the experimental results on two commonly used multimodal datasets. The\ncomparison reveals the characteristic of different kinds of cross-modal\nretrieval methods, which is expected to benefit both practical applications and\nfuture research. Finally, we discuss open problems and future research\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 07:20:44 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Wang", "Kaiye", ""], ["Yin", "Qiyue", ""], ["Wang", "Wei", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "1607.06221", "submitter": "K Paramesha", "authors": "K Paramesha and K C Ravishankar", "title": "A Perspective on Sentiment Analysis", "comments": "Opinion;Feature Engineering; Sentiment;Subjective\n  Sentence;Objective;Contextual Polarity;Sentiment\n  Lexicon;Classification;Machine learning", "journal-ref": "Proceedings of ERCICA 2014 - Emerging Research in Computing\n  Information Communication and Applications, Vol. 1, Elsevier, NMIT,\n  Bengaluru,India, 2014, pp. 412-418", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sentiment Analysis (SA) is indeed a fascinating area of research which has\nstolen the attention of researchers as it has many facets and more importantly\nit promises economic stakes in the corporate and governance sector. SA has been\nstemmed out of text analytics and established itself as a separate identity and\na domain of research. The wide ranging results of SA have proved to influence\nthe way some critical decisions are taken. Hence, it has become relevant in\nthorough understanding of the different dimensions of the input, output and the\nprocesses and approaches of SA.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 07:48:08 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 16:54:34 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Paramesha", "K", ""], ["Ravishankar", "K C", ""]]}, {"id": "1607.06275", "submitter": "Peng Li", "authors": "Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu", "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain\n  Factoid Question Answering", "comments": "10 pages, 3 figures, withdraw experimental results on CNN/Daily Mail\n  datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While question answering (QA) with neural network, i.e. neural QA, has\nachieved promising results in recent years, lacking of large scale real-word QA\ndataset is still a challenge for developing and evaluating neural QA system. To\nalleviate this problem, we propose a large scale human annotated real-world QA\ndataset WebQA with more than 42k questions and 556k evidences. As existing\nneural QA methods resolve QA either as sequence generation or\nclassification/ranking problem, they face challenges of expensive softmax\ncomputation, unseen answers handling or separate candidate answer generation\ncomponent. In this work, we cast neural QA as a sequence labeling problem and\npropose an end-to-end sequence labeling model, which overcomes all the above\nchallenges. Experimental results on WebQA show that our model outperforms the\nbaselines significantly with an F1 score of 74.69% with word-based input, and\nthe performance drops only 3.72 F1 points with more challenging character-based\ninput.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:40:50 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 10:56:45 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Li", "Peng", ""], ["Li", "Wei", ""], ["He", "Zhengyan", ""], ["Wang", "Xuguang", ""], ["Cao", "Ying", ""], ["Zhou", "Jie", ""], ["Xu", "Wei", ""]]}, {"id": "1607.06299", "submitter": "Roman Klinger", "authors": "Janik Jaskolski, Fabian Siegberg, Thomas Tibroni, Philipp Cimiano,\n  Roman Klinger", "title": "Opinion Mining in Online Reviews About Distance Education Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of distance education programs is increasing at a fast pace.\nEn par with this development, online communication in fora, social media and\nreviewing platforms between students is increasing as well. Exploiting this\ninformation to support fellow students or institutions requires to extract the\nrelevant opinions in order to automatically generate reports providing an\noverview of pros and cons of different distance education programs. We report\non an experiment involving distance education experts with the goal to develop\na dataset of reviews annotated with relevant categories and aspects in each\ncategory discussed in the specific review together with an indication of the\nsentiment.\n  Based on this experiment, we present an approach to extract general\ncategories and specific aspects under discussion in a review together with\ntheir sentiment. We frame this task as a multi-label hierarchical text\nclassification problem and empirically investigate the performance of different\nclassification architectures to couple the prediction of a category with the\nprediction of particular aspects in this category. We evaluate different\narchitectures and show that a hierarchical approach leads to superior results\nin comparison to a flat model which makes decisions independently.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:43:21 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Jaskolski", "Janik", ""], ["Siegberg", "Fabian", ""], ["Tibroni", "Thomas", ""], ["Cimiano", "Philipp", ""], ["Klinger", "Roman", ""]]}, {"id": "1607.06330", "submitter": "Antonio San Mart\\'in", "authors": "Antonio San Mart\\'in", "title": "La representaci\\'on de la variaci\\'on contextual mediante definiciones\n  terminol\\'ogicas flexibles", "comments": "PhD Thesis. in Spanish. University of Granada. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this doctoral thesis, we apply premises of cognitive linguistics to\nterminological definitions and present a proposal called the flexible\nterminological definition. This consists of a set of definitions of the same\nconcept made up of a general definition (in this case, one encompassing the\nentire environmental domain) along with additional definitions describing the\nconcept from the perspective of the subdomains in which it is relevant. Since\ncontext is a determining factor in the construction of the meaning of lexical\nunits (including terms), we assume that terminological definitions can, and\nshould, reflect the effects of context, even though definitions have\ntraditionally been treated as the expression of meaning void of any contextual\neffect. The main objective of this thesis is to analyze the effects of\ncontextual variation on specialized environmental concepts with a view to their\nrepresentation in terminological definitions. Specifically, we focused on\ncontextual variation based on thematic restrictions. To accomplish the\nobjectives of this doctoral thesis, we conducted an empirical study consisting\nof the analysis of a set of contextually variable concepts and the creation of\na flexible definition for two of them. As a result of the first part of our\nempirical study, we divided our notion of domain-dependent contextual variation\ninto three different phenomena: modulation, perspectivization and\nsubconceptualization. These phenomena are additive in that all concepts\nexperience modulation, some concepts also undergo perspectivization, and\nfinally, a small number of concepts are additionally subjected to\nsubconceptualization. In the second part, we applied these notions to\nterminological definitions and we presented we presented guidelines on how to\nbuild flexible definitions, from the extraction of knowledge to the actual\nwriting of the definition.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:39:12 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Mart\u00edn", "Antonio San", ""]]}, {"id": "1607.06520", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blind application of machine learning runs the risk of amplifying biases\npresent in data. Such a danger is facing us with word embedding, a popular\nframework to represent text data as vectors which has been used in many machine\nlearning and natural language processing tasks. We show that even word\nembeddings trained on Google News articles exhibit female/male gender\nstereotypes to a disturbing extent. This raises concerns because their\nwidespread use, as we describe, often tends to amplify these biases.\nGeometrically, gender bias is first shown to be captured by a direction in the\nword embedding. Second, gender neutral words are shown to be linearly separable\nfrom gender definition words in the word embedding. Using these properties, we\nprovide a methodology for modifying an embedding to remove gender stereotypes,\nsuch as the association between between the words receptionist and female,\nwhile maintaining desired associations such as between the words queen and\nfemale. We define metrics to quantify both direct and indirect gender biases in\nembeddings, and develop algorithms to \"debias\" the embedding. Using\ncrowd-worker evaluation as well as standard benchmarks, we empirically\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\nwhile preserving the its useful properties such as the ability to cluster\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\nused in applications without amplifying gender bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 22:26:20 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1607.06532", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang, Hsin-Hsi Chen", "title": "Novel Word Embedding and Translation-based Language Modeling for\n  Extractive Speech Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding methods revolve around learning continuous distributed vector\nrepresentations of words with neural networks, which can capture semantic\nand/or syntactic cues, and in turn be used to induce similarity measures among\nwords, sentences and documents in context. Celebrated methods can be\ncategorized as prediction-based and count-based methods according to the\ntraining objectives and model architectures. Their pros and cons have been\nextensively analyzed and evaluated in recent studies, but there is relatively\nless work continuing the line of research to develop an enhanced learning\nmethod that brings together the advantages of the two model families. In\naddition, the interpretation of the learned word representations still remains\nsomewhat opaque. Motivated by the observations and considering the pressing\nneed, this paper presents a novel method for learning the word representations,\nwhich not only inherits the advantages of classic word embedding methods but\nalso offers a clearer and more rigorous interpretation of the learned word\nrepresentations. Built upon the proposed word embedding method, we further\nformulate a translation-based language modeling framework for the extractive\nspeech summarization task. A series of empirical evaluations demonstrate the\neffectiveness of the proposed word representation learning and language\nmodeling techniques in extractive speech summarization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 00:20:09 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Liu", "Shih-Hung", ""], ["Chen", "Berlin", ""], ["Wang", "Hsin-Min", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "1607.06556", "submitter": "Pengfei Liu", "authors": "PengFei Liu and Xipeng Qiu and Xuanjing Huang", "title": "Syntax-based Attention Model for Natural Language Inference", "comments": "Submitted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing attentional mechanism in neural network is a powerful concept,\nand has achieved impressive results in many natural language processing tasks.\nHowever, most of the existing models impose attentional distribution on a flat\ntopology, namely the entire input representation sequence. Clearly, any\nwell-formed sentence has its accompanying syntactic tree structure, which is a\nmuch rich topology. Applying attention to such topology not only exploits the\nunderlying syntax, but also makes attention more interpretable. In this paper,\nwe explore this direction in the context of natural language inference. The\nresults demonstrate its efficacy. We also perform extensive qualitative\nanalysis, deriving insights and intuitions of why and how our model works.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 04:21:54 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Liu", "PengFei", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1607.06560", "submitter": "Amol Patwardhan", "authors": "Amol S Patwardhan, Jacob Badeaux, Siavash, Gerald M Knapp", "title": "Automated Prediction of Temporal Relations", "comments": "8 pages, 1 figure, Technical report, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: There has been growing research interest in automated answering\nof questions or generation of summary of free form text such as news article.\nIn order to implement this task, the computer should be able to identify the\nsequence of events, duration of events, time at which event occurred and the\nrelationship type between event pairs, time pairs or event-time pairs. Specific\nProblem: It is important to accurately identify the relationship type between\ncombinations of event and time before the temporal ordering of events can be\ndefined. The machine learning approach taken in Mani et. al (2006) provides an\naccuracy of only 62.5 on the baseline data from TimeBank. The researchers used\nmaximum entropy classifier in their methodology. TimeML uses the TLINK\nannotation to tag a relationship type between events and time. The time\ncomplexity is quadratic when it comes to tagging documents with TLINK using\nhuman annotation. This research proposes using decision tree and parsing to\nimprove the relationship type tagging. This research attempts to solve the gaps\nin human annotation by automating the task of relationship type tagging in an\nattempt to improve the accuracy of event and time relationship in annotated\ndocuments. Scope information: The documents from the domain of news will be\nused. The tagging will be performed within the same document and not across\ndocuments. The relationship types will be identified only for a pair of event\nand time and not a chain of events. The research focuses on documents tagged\nusing the TimeML specification which contains tags such as EVENT, TLINK, and\nTIMEX. Each tag has attributes such as identifier, relation, POS, time etc.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 05:38:37 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Patwardhan", "Amol S", ""], ["Badeaux", "Jacob", ""], ["Siavash", "", ""], ["Knapp", "Gerald M", ""]]}, {"id": "1607.06852", "submitter": "Adam Summerville", "authors": "Adam James Summerville, James Ryan, Michael Mateas, Noah Wardrip-Fruin", "title": "CFGs-2-NLU: Sequence-to-Sequence Learning for Mapping Utterances to\n  Semantics and Pragmatics", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCSC-SOE-16-11", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to natural language understanding\nthat utilizes context-free grammars (CFGs) in conjunction with\nsequence-to-sequence (seq2seq) deep learning. Specifically, we take a CFG\nauthored to generate dialogue for our target application for NLU, a videogame,\nand train a long short-term memory (LSTM) recurrent neural network (RNN) to map\nthe surface utterances that it produces to traces of the grammatical expansions\nthat yielded them. Critically, this CFG was authored using a tool we have\ndeveloped that supports arbitrary annotation of the nonterminal symbols in the\ngrammar. Because we already annotated the symbols in this grammar for the\nsemantic and pragmatic considerations that our game's dialogue manager operates\nover, we can use the grammatical trace associated with any surface utterance to\ninfer such information. During gameplay, we translate player utterances into\ngrammatical traces (using our RNN), collect the mark-up attributed to the\nsymbols included in that trace, and pass this information to the dialogue\nmanager, which updates the conversation state accordingly. From an offline\nevaluation task, we demonstrate that our trained RNN translates surface\nutterances to grammatical traces with great accuracy. To our knowledge, this is\nthe first usage of seq2seq learning for conversational agents (our game's\ncharacters) who explicitly reason over semantic and pragmatic considerations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 22:05:20 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Summerville", "Adam James", ""], ["Ryan", "James", ""], ["Mateas", "Michael", ""], ["Wardrip-Fruin", "Noah", ""]]}, {"id": "1607.06875", "submitter": "Steve Doubleday", "authors": "Steve Doubleday, Sean Trott, Jerome Feldman", "title": "Processing Natural Language About Ongoing Actions", "comments": "6 pages, 8 figures. Updated with PIPE citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actions may not proceed as planned; they may be interrupted, resumed or\noverridden. This is a challenge to handle in a natural language understanding\nsystem. We describe extensions to an existing implementation for the control of\nautonomous systems by natural language, to enable such systems to handle\nincoming language requests regarding actions. Language Communication with\nAutonomous Systems (LCAS) has been extended with support for X-nets,\nparameterized executable schemas representing actions. X-nets enable the system\nto control actions at a desired level of granularity, while providing a\nmechanism for language requests to be processed asynchronously. Standard\nsemantics supported include requests to stop, continue, or override the\nexisting action. The specific domain demonstrated is the control of motion of a\nsimulated robot, but the approach is general, and could be applied to other\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 01:46:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 13:32:01 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Doubleday", "Steve", ""], ["Trott", "Sean", ""], ["Feldman", "Jerome", ""]]}, {"id": "1607.06952", "submitter": "Xinchi Chen", "authors": "Xinchi Chen, Xipeng Qiu, Xuanjing Huang", "title": "Neural Sentence Ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence ordering is a general and critical task for natural language\ngeneration applications. Previous works have focused on improving its\nperformance in an external, downstream task, such as multi-document\nsummarization. Given its importance, we propose to study it as an isolated\ntask. We collect a large corpus of academic texts, and derive a data driven\napproach to learn pairwise ordering of sentences, and validate the efficacy\nwith extensive experiments. Source codes and dataset of this paper will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 16:22:23 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Chen", "Xinchi", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1607.06961", "submitter": "Diego Amancio Dr.", "authors": "Vanessa Queiroz Marinho and Graeme Hirst and Diego Raphael Amancio", "title": "Authorship attribution via network motifs identification", "comments": "Preprint submitted for the 5th Brazilian Conference on Intelligent\n  Systems", "journal-ref": "2016 5th Brazilian Conference on Intelligent Systems (BRACIS),\n  Recife, Brazil, 2016, pp. 355-360", "doi": "10.1109/BRACIS.2016.071", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concepts and methods of complex networks can be used to analyse texts at\ntheir different complexity levels. Examples of natural language processing\n(NLP) tasks studied via topological analysis of networks are keyword\nidentification, automatic extractive summarization and authorship attribution.\nEven though a myriad of network measurements have been applied to study the\nauthorship attribution problem, the use of motifs for text analysis has been\nrestricted to a few works. The goal of this paper is to apply the concept of\nmotifs, recurrent interconnection patterns, in the authorship attribution task.\nThe absolute frequencies of all thirteen directed motifs with three nodes were\nextracted from the co-occurrence networks and used as classification features.\nThe effectiveness of these features was verified with four machine learning\nmethods. The results show that motifs are able to distinguish the writing style\nof different authors. In our best scenario, 57.5% of the books were correctly\nclassified. The chance baseline for this problem is 12.5%. In addition, we have\nfound that function words play an important role in these recurrent patterns.\nTaken together, our findings suggest that motifs should be further explored in\nother related linguistic tasks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 19:07:53 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Marinho", "Vanessa Queiroz", ""], ["Hirst", "Graeme", ""], ["Amancio", "Diego Raphael", ""]]}, {"id": "1607.07057", "submitter": "Tomas Brychcin", "authors": "Tomas Brychcin", "title": "Latent Tree Language Model", "comments": "Accepted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Latent Tree Language Model (LTLM), a novel\napproach to language modeling that encodes syntax and semantics of a given\nsentence as a tree of word roles.\n  The learning phase iteratively updates the trees by moving nodes according to\nGibbs sampling. We introduce two algorithms to infer a tree for a given\nsentence. The first one is based on Gibbs sampling. It is fast, but does not\nguarantee to find the most probable tree. The second one is based on dynamic\nprogramming. It is slower, but guarantees to find the most probable tree. We\nprovide comparison of both algorithms.\n  We combine LTLM with 4-gram Modified Kneser-Ney language model via linear\ninterpolation. Our experiments with English and Czech corpora show significant\nperplexity reductions (up to 46% for English and 49% for Czech) compared with\nstandalone 4-gram Modified Kneser-Ney language model.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 15:40:36 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 12:35:24 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 14:47:18 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Brychcin", "Tomas", ""]]}, {"id": "1607.07514", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Prashanth Vijayaraghavan and Deb Roy", "title": "Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM\n  Encoder-Decoder", "comments": "SIGIR 2016, July 17-21, 2016, Pisa. Proceedings of SIGIR 2016. Pisa,\n  Italy (2016)", "journal-ref": null, "doi": "10.1145/2911451.2914762", "report-no": null, "categories": "cs.CL cs.AI cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Tweet2Vec, a novel method for generating general-purpose vector\nrepresentation of tweets. The model learns tweet embeddings using\ncharacter-level CNN-LSTM encoder-decoder. We trained our model on 3 million,\nrandomly selected English-language tweets. The model was evaluated using two\nmethods: tweet semantic similarity and tweet sentiment categorization,\noutperforming the previous state-of-the-art in both tasks. The evaluations\ndemonstrate the power of the tweet embeddings generated by our model for\nvarious tweet categorization tasks. The vector representations generated by our\nmodel are generic, and hence can be applied to a variety of tasks. Though the\nmodel presented in this paper is trained on English-language tweets, the method\npresented can be used to learn tweet embeddings for different languages.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 00:58:14 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Vijayaraghavan", "Prashanth", ""], ["Roy", "Deb", ""]]}, {"id": "1607.07565", "submitter": "Michael Spranger", "authors": "Michael Spranger and Jakob Suchan and Mehul Bhatt and Manfred Eppe", "title": "Grounding Dynamic Spatial Relations for Embodied (Robot) Interaction", "comments": "in: Pham, D.-N. and Park, S.-B., editors, PRICAI 2014: Trends in\n  Artificial Intelligence, volume 8862 of Lecture Notes in Computer Science,\n  pages 958-971. Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computational model of the processing of dynamic\nspatial relations occurring in an embodied robotic interaction setup. A\ncomplete system is introduced that allows autonomous robots to produce and\ninterpret dynamic spatial phrases (in English) given an environment of moving\nobjects. The model unites two separate research strands: computational\ncognitive semantics and on commonsense spatial representation and reasoning.\nThe model for the first time demonstrates an integration of these different\nstrands.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 07:22:52 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Spranger", "Michael", ""], ["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Eppe", "Manfred", ""]]}, {"id": "1607.07602", "submitter": "Kumar Niraj", "authors": "Niraj Kumar and Premkumar Devanbu", "title": "OntoCat: Automatically categorizing knowledge in API Documentation", "comments": "To be submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most application development happens in the context of complex APIs;\nreference documentation for APIs has grown tremendously in variety, complexity,\nand volume, and can be difficult to navigate. There is a growing need to\ndevelop well-organized ways to access the knowledge latent in the\ndocumentation; several research efforts deal with the organization (ontology)\nof API-related knowledge. Extensive knowledge-engineering work, supported by a\nrigorous qualitative analysis, by Maalej & Robillard [3] has identified a\nuseful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain\nindependent technique to extract the knowledge types from the given API\nreference documentation. Our system, OntoCat, introduces total nine different\nfeatures and their semantic and statistical combinations to classify the\ndifferent knowledge types. We tested OntoCat on python API reference\ndocumentation. Our experimental results show the effectiveness of the system\nand opens the scope of probably related research areas (i.e., user behavior,\ndocumentation quality, etc.).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:19:46 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Kumar", "Niraj", ""], ["Devanbu", "Premkumar", ""]]}, {"id": "1607.07630", "submitter": "Michael Spranger", "authors": "Michael Spranger", "title": "Grounded Lexicon Acquisition - Case Studies in Spatial Language", "comments": "Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2013\n  Joint IEEE International Conferences on, pages 1-6. IEEE", "journal-ref": null, "doi": "10.1109/DevLrn.2013.6652534", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses grounded acquisition experiments of increasing\ncomplexity. Humanoid robots acquire English spatial lexicons from robot tutors.\nWe identify how various spatial language systems, such as projective, absolute\nand proximal can be learned. The proposed learning mechanisms do not rely on\ndirect meaning transfer or direct access to world models of interlocutors.\nFinally, we show how multiple systems can be acquired at the same time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 10:37:24 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Spranger", "Michael", ""]]}, {"id": "1607.07657", "submitter": "Yiou Lin", "authors": "Yiou Lin, Hang Lei, Prince Clement Addo and Xiaoyu Li", "title": "Machine Learned Resume-Job Matching Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job search through online matching engines nowadays are very prominent and\nbeneficial to both job seekers and employers. But the solutions of traditional\nengines without understanding the semantic meanings of different resumes have\nnot kept pace with the incredible changes in machine learning techniques and\ncomputing capability. These solutions are usually driven by manual rules and\npredefined weights of keywords which lead to an inefficient and frustrating\nsearch experience. To this end, we present a machine learned solution with rich\nfeatures and deep learning methods. Our solution includes three configurable\nmodules that can be plugged with little restrictions. Namely, unsupervised\nfeature extraction, base classifiers training and ensemble method learning. In\nour solution, rather than using manual rules, machine learned methods to\nautomatically detect the semantic similarity of positions are proposed. Then\nfour competitive \"shallow\" estimators and \"deep\" estimators are selected.\nFinally, ensemble methods to bag these estimators and aggregate their\nindividual predictions to form a final prediction are verified. Experimental\nresults of over 47 thousand resumes show that our solution can significantly\nimprove the predication precision current position, salary, educational\nbackground and company scale.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 12:04:31 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Lin", "Yiou", ""], ["Lei", "Hang", ""], ["Addo", "Prince Clement", ""], ["Li", "Xiaoyu", ""]]}, {"id": "1607.07788", "submitter": "Igor Barahona Dr", "authors": "Daria Micaela Hernandez, Monica Becue-Bertaut, Igor Barahona", "title": "How scientific literature has been evolving over the time? A novel\n  statistical approach using tracking verbal-based methods", "comments": null, "journal-ref": "JSM Proceedings (2014), Section on Statistical Learning and Data\n  Mining. Alexandria, VA. American Statistical Association. 1121-1131", "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a global vision of the scientific publications related\nwith the Systemic Lupus Erythematosus (SLE), taking as starting point abstracts\nof articles. Through the time, abstracts have been evolving towards higher\ncomplexity on used terminology, which makes necessary the use of sophisticated\nstatistical methods and answering questions including: how vocabulary is\nevolving through the time? Which ones are most influential articles? And which\none are the articles that introduced new terms and vocabulary? To answer these,\nwe analyze a dataset composed by 506 abstracts and downloaded from 115\ndifferent journals and cover a 18 year-period.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 17:59:55 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Hernandez", "Daria Micaela", ""], ["Becue-Bertaut", "Monica", ""], ["Barahona", "Igor", ""]]}, {"id": "1607.07931", "submitter": "Stuart Bradley", "authors": "Stuart Bradley", "title": "Synthetic Language Generation and Model Validation in BEAST2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating synthetic languages aids in the testing and validation of future\ncomputational linguistic models and methods. This thesis extends the BEAST2\nphylogenetic framework to add linguistic sequence generation under multiple\nmodels. The new plugin is then used to test the effects of the phenomena of\nword borrowing on the inference process under two widely used phylolinguistic\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 01:44:54 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bradley", "Stuart", ""]]}, {"id": "1607.07956", "submitter": "Yuezhang Li", "authors": "Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, Katia\n  Sycara", "title": "Joint Embedding of Hierarchical Categories and Entities for Concept\n  Categorization and Dataless Classification", "comments": "10 pages, submitted to Coling 2016. arXiv admin note: substantial\n  text overlap with arXiv:1605.03924", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the lack of structured knowledge applied in learning distributed\nrepresentation of cate- gories, existing work cannot incorporate category\nhierarchies into entity information. We propose a framework that embeds\nentities and categories into a semantic space by integrating structured\nknowledge and taxonomy hierarchy from large knowledge bases. The framework\nallows to com- pute meaningful semantic relatedness between entities and\ncategories. Our framework can han- dle both single-word concepts and\nmultiple-word concepts with superior performance on concept categorization and\nyield state of the art results on dataless hierarchical classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 04:51:17 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Li", "Yuezhang", ""], ["Zheng", "Ronghuo", ""], ["Tian", "Tian", ""], ["Hu", "Zhiting", ""], ["Iyer", "Rahul", ""], ["Sycara", "Katia", ""]]}, {"id": "1607.08074", "submitter": "Adrian Groza", "authors": "Adrian Groza, Oana Popa", "title": "Mining Arguments from Cancer Documents Using Natural Language Processing\n  and Ontologies", "comments": "ICCP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical domain, the continuous stream of scientific research contains\ncontradictory results supported by arguments and counter-arguments. As medical\nexpertise occurs at different levels, part of the human agents have\ndifficulties to face the huge amount of studies, but also to understand the\nreasons and pieces of evidences claimed by the proponents and the opponents of\nthe debated topic. To better understand the supporting arguments for new\nfindings related to current state of the art in the medical domain we need\ntools able to identify arguments in scientific papers. Our work here aims to\nfill the above technological gap.\n  Quite aware of the difficulty of this task, we embark to this road by relying\non the well-known interleaving of domain knowledge with natural language\nprocessing. To formalise the existing medical knowledge, we rely on ontologies.\nTo structure the argumentation model we use also the expressivity and reasoning\ncapabilities of Description Logics. To perform argumentation mining we\nformalise various linguistic patterns in a rule-based language. We tested our\nsolution against a corpus of scientific papers related to breast cancer. The\nrun experiments show a F-measure between 0.71 and 0.86 for identifying\nconclusions of an argument and between 0.65 and 0.86 for identifying premises\nof an argument.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:08:41 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Popa", "Oana", ""]]}, {"id": "1607.08592", "submitter": "Erkki Luuk", "authors": "Erkki Luuk", "title": "Modeling selectional restrictions in a relational type system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectional restrictions are semantic constraints on forming certain complex\ntypes in natural language. The paper gives an overview of modeling selectional\nrestrictions in a relational type system with morphological and syntactic\ntypes. We discuss some foundations of the system and ways of formalizing\nselectional restrictions.\n  Keywords: type theory, selectional restrictions, syntax, morphology\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:47:25 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Luuk", "Erkki", ""]]}, {"id": "1607.08692", "submitter": "Rui Wang", "authors": "Rui Wang, Hai Zhao, Sabine Ploux, Bao-Liang Lu, Masao Utiyama and\n  Eiichiro Sumita", "title": "A Novel Bilingual Word Embedding Method for Lexical Translation Using\n  Bilingual Sense Clique", "comments": "under review by COLING-2016", "journal-ref": null, "doi": "10.1145/3203078", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing methods for bilingual word embedding only consider\nshallow context or simple co-occurrence information. In this paper, we propose\na latent bilingual sense unit (Bilingual Sense Clique, BSC), which is derived\nfrom a maximum complete sub-graph of pointwise mutual information based graph\nover bilingual corpus. In this way, we treat source and target words equally\nand a separated bilingual projection processing that have to be used in most\nexisting works is not necessary any more. Several dimension reduction methods\nare evaluated to summarize the BSC-word relationship. The proposed method is\nevaluated on bilingual lexicon translation tasks and empirical results show\nthat bilingual sense embedding methods outperform existing bilingual word\nembedding methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 06:28:32 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 06:58:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Wang", "Rui", ""], ["Zhao", "Hai", ""], ["Ploux", "Sabine", ""], ["Lu", "Bao-Liang", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "1607.08693", "submitter": "Rui Wang", "authors": "Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama and Eiichro Sumita", "title": "Connecting Phrase based Statistical Machine Translation Adaptation", "comments": "under review by COLING-2016", "journal-ref": "It is published in COLING 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although more additional corpora are now available for Statistical Machine\nTranslation (SMT), only the ones which belong to the same or similar domains\nwith the original corpus can indeed enhance SMT performance directly. Most of\nthe existing adaptation methods focus on sentence selection. In comparison,\nphrase is a smaller and more fine grained unit for data selection, therefore we\npropose a straightforward and efficient connecting phrase based adaptation\nmethod, which is applied to both bilingual phrase pair and monolingual n-gram\nadaptation. The proposed method is evaluated on IWSLT/NIST data sets, and the\nresults show that phrase based SMT performance are significantly improved (up\nto +1.6 in comparison with phrase based SMT baseline system and +0.9 in\ncomparison with existing methods).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 06:29:37 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Wang", "Rui", ""], ["Zhao", "Hai", ""], ["Lu", "Bao-Liang", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichro", ""]]}, {"id": "1607.08720", "submitter": "Benjamin Rubinstein", "authors": "Jiazhen He, Benjamin I. P. Rubinstein, James Bailey, Rui Zhang, Sandra\n  Milligan", "title": "TopicResponse: A Marriage of Topic Modelling and Rasch Modelling for\n  Automatic Measurement in MOOCs", "comments": "In preparation for journal submission; Revisions to improve clarity\n  with additional examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the suitability of using automatically discovered topics\nfrom MOOC discussion forums for modelling students' academic abilities. The\nRasch model from psychometrics is a popular generative probabilistic model that\nrelates latent student skill, latent item difficulty, and observed student-item\nresponses within a principled, unified framework. According to scholarly\neducational theory, discovered topics can be regarded as appropriate\nmeasurement items if (1) students' participation across the discovered topics\nis well fit by the Rasch model, and if (2) the topics are interpretable to\nsubject-matter experts as being educationally meaningful. Such Rasch-scaled\ntopics, with associated difficulty levels, could be of potential benefit to\ncurriculum refinement, student assessment and personalised feedback. The\ntechnical challenge that remains, is to discover meaningful topics that\nsimultaneously achieve good statistical fit with the Rasch model. To address\nthis challenge, we combine the Rasch model with non-negative matrix\nfactorisation based topic modelling, jointly fitting both models. We\ndemonstrate the suitability of our approach with quantitative experiments on\ndata from three Coursera MOOCs, and with qualitative survey results on topic\ninterpretability on a Discrete Optimisation MOOC.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:17:45 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 04:30:38 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["He", "Jiazhen", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Bailey", "James", ""], ["Zhang", "Rui", ""], ["Milligan", "Sandra", ""]]}, {"id": "1607.08723", "submitter": "Emmanuel Dupoux", "authors": "Emmanuel Dupoux", "title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for\n  reverse-engineering the infant language-learner", "comments": "27 pages, 5 figures, 3 tables, supplementary materials", "journal-ref": "Dupoux, E. (2018). Cognitive science in the era of artificial\n  intelligence: A roadmap for reverse-engineering the infant language learner.\n  Cognition, 173, 43-59", "doi": "10.1016/j.cognition.2017.11.008", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During their first years of life, infants learn the language(s) of their\nenvironment at an amazing speed despite large cross cultural variations in\namount and complexity of the available language input. Understanding this\nsimple fact still escapes current cognitive and linguistic theories. Recently,\nspectacular progress in the engineering science, notably, machine learning and\nwearable technology, offer the promise of revolutionizing the study of\ncognitive development. Machine learning offers powerful learning algorithms\nthat can achieve human-like performance on many linguistic tasks. Wearable\nsensors can capture vast amounts of data, which enable the reconstruction of\nthe sensory experience of infants in their natural environment. The project of\n'reverse engineering' language development, i.e., of building an effective\nsystem that mimics infant's achievements appears therefore to be within reach.\nHere, we analyze the conditions under which such a project can contribute to\nour scientific understanding of early language development. We argue that\ninstead of defining a sub-problem or simplifying the data, computational models\nshould address the full complexity of the learning situation, and take as input\nthe raw sensory signals available to infants. This implies that (1) accessible\nbut privacy-preserving repositories of home data be setup and widely shared,\nand (2) models be evaluated at different linguistic levels through a benchmark\nof psycholinguist tests that can be passed by machines and humans alike, (3)\nlinguistically and psychologically plausible learning architectures be scaled\nup to real data using probabilistic/optimization principles from machine\nlearning. We discuss the feasibility of this approach and present preliminary\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:33:10 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 08:07:08 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 12:59:59 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 15:56:51 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dupoux", "Emmanuel", ""]]}, {"id": "1607.08725", "submitter": "Biao Zhang", "authors": "Biao Zhang, Deyi Xiong and Jinsong Su", "title": "Cseq2seq: Cyclic Sequence-to-Sequence Learning", "comments": "Submitted to EMNLP2016, original version, updated version of\n  \"Recurrent Neural Machine Translation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vanilla sequence-to-sequence learning (seq2seq) reads and encodes a\nsource sequence into a fixed-length vector only once, suffering from its\ninsufficiency in modeling structural correspondence between the source and\ntarget sequence. Instead of handling this insufficiency with a linearly\nweighted attention mechanism, in this paper, we propose to use a recurrent\nneural network (RNN) as an alternative (Cseq2seq-I). During decoding,\nCseq2seq-I cyclically feeds the previous decoding state back to the encoder as\nthe initial state of the RNN, and reencodes source representations to produce\ncontext vectors. We surprisingly find that the introduced RNN succeeds in\ndynamically detecting translationrelated source tokens according to the partial\ntarget sequence. Based on this finding, we further hypothesize that the partial\ntarget sequence can act as a feedback to improve the understanding of the\nsource sequence. To test this hypothesis, we propose cyclic\nsequence-to-sequence learning (Cseq2seq-II) which differs from the seq2seq only\nin the reintroduction of previous decoding state into the same encoder. We\nfurther perform parameter sharing on Cseq2seq-II to reduce parameter redundancy\nand enhance regularization. In particular, we share the weights of the encoder\nand decoder, and two targetside word embeddings, making Cseq2seq-II equivalent\nto a single conditional RNN model, with 31% parameters pruned but even better\nperformance. Cseq2seq-II not only preserves the simplicity of seq2seq but also\nyields comparable and promising results on machine translation tasks.\nExperiments on Chinese- English and English-German translation show that\nCseq2seq achieves significant and consistent improvements over seq2seq and is\nas competitive as the attention-based seq2seq model.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:35:10 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 21:59:29 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhang", "Biao", ""], ["Xiong", "Deyi", ""], ["Su", "Jinsong", ""]]}, {"id": "1607.08822", "submitter": "Peter Anderson", "authors": "Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould", "title": "SPICE: Semantic Propositional Image Caption Evaluation", "comments": "14 pages plus references, accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable interest in the task of automatically generating image\ncaptions. However, evaluation is challenging. Existing automatic evaluation\nmetrics are primarily sensitive to n-gram overlap, which is neither necessary\nnor sufficient for the task of simulating human judgment. We hypothesize that\nsemantic propositional content is an important component of human caption\nevaluation, and propose a new automated caption evaluation metric defined over\nscene graphs coined SPICE. Extensive evaluations across a range of models and\ndatasets indicate that SPICE captures human judgments over model-generated\ncaptions better than other automatic metrics (e.g., system-level correlation of\n0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and\n0.53 for METEOR). Furthermore, SPICE can answer questions such as `which\ncaption-generator best understands colors?' and `can caption-generators count?'\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 14:26:27 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Anderson", "Peter", ""], ["Fernando", "Basura", ""], ["Johnson", "Mark", ""], ["Gould", "Stephen", ""]]}, {"id": "1607.08864", "submitter": "Christoph Redl", "authors": "Christoph Redl", "title": "The DLVHEX System for Knowledge Representation: Recent Advances (System\n  Description)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures (arXiv:1607.08864)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DLVHEX system implements the HEX-semantics, which integrates answer set\nprogramming (ASP) with arbitrary external sources. Since its first release ten\nyears ago, significant advancements were achieved. Most importantly, the\nexploitation of properties of external sources led to efficiency improvements\nand flexibility enhancements of the language, and technical improvements on the\nsystem side increased user's convenience. In this paper, we present the current\nstatus of the system and point out the most important recent enhancements over\nearly versions. While existing literature focuses on theoretical aspects and\nspecific components, a bird's eye view of the overall system is missing. In\norder to promote the system for real-world applications, we further present\napplications which were already successfully realized on top of DLVHEX. This\npaper is under consideration for acceptance in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 16:26:54 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 13:23:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Redl", "Christoph", ""]]}, {"id": "1607.08883", "submitter": "Souvick Ghosh", "authors": "Satanu Ghosh, Souvick Ghosh, Dipankar Das", "title": "Labeling of Query Words using Conditional Random Field", "comments": "4 pages in Technical Report, FIRE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach on Query Word Labeling as an attempt in the\nshared task on Mixed Script Information Retrieval at Forum for Information\nRetrieval Evaluation (FIRE) 2015. The query is written in Roman script and the\nwords were in English or transliterated from Indian regional languages. A total\nof eight Indian languages were present in addition to English. We also\nidentified the Named Entities and special symbols as part of our task. A CRF\nbased machine learning framework was used for labeling the individual words\nwith their corresponding language labels. We used a dictionary based approach\nfor language identification. We also took into account the context of the word\nwhile identifying the language. Our system demonstrated an overall accuracy of\n75.5% for token level language identification. The strict F-measure scores for\nthe identification of token level language labels for Bengali, English and\nHindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure\nof our system was 0.7498.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:20:24 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Ghosh", "Satanu", ""], ["Ghosh", "Souvick", ""], ["Das", "Dipankar", ""]]}, {"id": "1607.08885", "submitter": "Souvick Ghosh", "authors": "Promita Maitra, Souvick Ghosh, Dipankar Das", "title": "Authorship Verification - An Approach based on Random Forest", "comments": "9 pages in Working Notes Papers of the CLEF 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution, being an important problem in many areas in-cluding\ninformation retrieval, computational linguistics, law and journalism etc., has\nbeen identified as a subject of increasingly research interest in the re-cent\nyears. In case of Author Identification task in PAN at CLEF 2015, the main\nfocus was given on cross-genre and cross-topic author verification tasks. We\nhave used several word-based and style-based features to identify the\ndif-ferences between the known and unknown problems of one given set and label\nthe unknown ones accordingly using a Random Forest based classifier.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:22:02 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Maitra", "Promita", ""], ["Ghosh", "Souvick", ""], ["Das", "Dipankar", ""]]}, {"id": "1607.08898", "submitter": "Tao Ding", "authors": "Tao Ding and Shimei Pan", "title": "Personalized Emphasis Framing for Persuasive Message Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a study on personalized emphasis framing which can\nbe used to tailor the content of a message to enhance its appeal to different\nindividuals. With this framework, we directly model content selection decisions\nbased on a set of psychologically-motivated domain-independent personal traits\nincluding personality (e.g., extraversion and conscientiousness) and basic\nhuman values (e.g., self-transcendence and hedonism). We also demonstrate how\nthe analysis results can be used in automated personalized content selection\nfor persuasive message generation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 19:16:08 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Ding", "Tao", ""], ["Pan", "Shimei", ""]]}]