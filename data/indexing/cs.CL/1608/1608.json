[{"id": "1608.00104", "submitter": "Chenguang Wang", "authors": "Chenguang Wang, Yangqiu Song, Dan Roth, Ming Zhang, Jiawei Han", "title": "World Knowledge as Indirect Supervision for Document Clustering", "comments": "33 pages, 53 figures, ACM TKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key obstacles in making learning protocols realistic in\napplications is the need to supervise them, a costly process that often\nrequires hiring domain experts. We consider the framework to use the world\nknowledge as indirect supervision. World knowledge is general-purpose\nknowledge, which is not designed for any specific domain. Then the key\nchallenges are how to adapt the world knowledge to domains and how to represent\nit for learning. In this paper, we provide an example of using world knowledge\nfor domain dependent document clustering. We provide three ways to specify the\nworld knowledge to domains by resolving the ambiguity of the entities and their\ntypes, and represent the data with world knowledge as a heterogeneous\ninformation network. Then we propose a clustering algorithm that can cluster\nmultiple types and incorporate the sub-type information as constraints. In the\nexperiments, we use two existing knowledge bases as our sources of world\nknowledge. One is Freebase, which is collaboratively collected knowledge about\nentities and their organizations. The other is YAGO2, a knowledge base\nautomatically extracted from Wikipedia and maps knowledge to the linguistic\nknowledge base, WordNet. Experimental results on two text benchmark datasets\n(20newsgroups and RCV1) show that incorporating world knowledge as indirect\nsupervision can significantly outperform the state-of-the-art clustering\nalgorithms as well as clustering algorithms enhanced with world knowledge\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 11:53:04 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Wang", "Chenguang", ""], ["Song", "Yangqiu", ""], ["Roth", "Dan", ""], ["Zhang", "Ming", ""], ["Han", "Jiawei", ""]]}, {"id": "1608.00112", "submitter": "Haitao Mi", "authors": "Haitao Mi and Zhiguo Wang and Abe Ittycheriah", "title": "Supervised Attentions for Neural Machine Translation", "comments": "6 pages. In Proceedings of EMNLP 2016. arXiv admin note: text overlap\n  with arXiv:1605.03148", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve the attention or alignment accuracy of neural\nmachine translation by utilizing the alignments of training sentence pairs. We\nsimply compute the distance between the machine attentions and the \"true\"\nalignments, and minimize this cost in the training procedure. Our experiments\non large-scale Chinese-to-English task show that our model improves both\ntranslation and alignment qualities significantly over the large-vocabulary\nneural machine translation system, and even beats a state-of-the-art\ntraditional syntax-based system.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 12:39:19 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Mi", "Haitao", ""], ["Wang", "Zhiguo", ""], ["Ittycheriah", "Abe", ""]]}, {"id": "1608.00255", "submitter": "Marek Zawadowski", "authors": "Justyna Grudzinska and Marek Zawadowski", "title": "Continuation semantics for multi-quantifier sentences: operation-based\n  approaches", "comments": "19 pages, corrections in the table on page 3. arXiv admin note: text\n  overlap with arXiv:1605.03981", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical scope-assignment strategies for multi-quantifier sentences involve\nquantifier phrase (QP)-movement. More recent continuation-based approaches\nprovide a compelling alternative, for they interpret QP's in situ - without\nresorting to Logical Forms or any structures beyond the overt syntax. The\ncontinuation-based strategies can be divided into two groups: those that locate\nthe source of scope-ambiguity in the rules of semantic composition and those\nthat attribute it to the lexical entries for the quantifier words. In this\npaper, we focus on the former operation-based approaches and the nature of the\nsemantic operations involved. More specifically, we discuss three such possible\noperation-based strategies for multi-quantifier sentences, together with their\nrelative merits and costs.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 20:01:04 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 22:50:41 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Grudzinska", "Justyna", ""], ["Zawadowski", "Marek", ""]]}, {"id": "1608.00272", "submitter": "Licheng Yu", "authors": "Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L.\n  Berg", "title": "Modeling Context in Referring Expressions", "comments": "19 pages, 6 figures, in ECCV 2016; authors, references and\n  acknowledgement updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans refer to objects in their environments all the time, especially in\ndialogue with other people. We explore generating and comprehending natural\nlanguage referring expressions for objects in images. In particular, we focus\non incorporating better measures of visual context into referring expression\nmodels and find that visual comparison to other objects within an image helps\nimprove performance significantly. We also develop methods to tie the language\ngeneration process together, so that we generate expressions for all objects of\na particular category jointly. Evaluation on three recent datasets - RefCOCO,\nRefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring\nexpression generation and comprehension.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:21:42 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 22:52:17 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2016 19:01:37 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Yu", "Licheng", ""], ["Poirson", "Patrick", ""], ["Yang", "Shan", ""], ["Berg", "Alexander C.", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1608.00293", "submitter": "Hiroshi Noji", "authors": "Hiroshi Noji", "title": "Left-corner Methods for Syntactic Modeling with Universal Structural\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary goal in this thesis is to identify better syntactic constraint or\nbias, that is language independent but also efficiently exploitable during\nsentence processing. We focus on a particular syntactic construction called\ncenter-embedding, which is well studied in psycholinguistics and noted to cause\nparticular difficulty for comprehension. Since people use language as a tool\nfor communication, one expects such complex constructions to be avoided for\ncommunication efficiency. From a computational perspective, center-embedding is\nclosely relevant to a left-corner parsing algorithm, which can capture the\ndegree of center-embedding of a parse tree being constructed. This connection\nsuggests left-corner methods can be a tool to exploit the universal syntactic\nconstraint that people avoid generating center-embedded structures. We explore\nsuch utilities of center-embedding as well as left-corner methods extensively\nthrough several theoretical and empirical examinations.\n  Our primary task is unsupervised grammar induction. In this task, the input\nto the algorithm is a collection of sentences, from which the model tries to\nextract the salient patterns on them as a grammar. This is a particularly hard\nproblem although we expect the universal constraint may help in improving the\nperformance since it can effectively restrict the possible search space for the\nmodel. We build the model by extending the left-corner parsing algorithm for\nefficiently tabulating the search space except those involving center-embedding\nup to a specific degree. We examine the effectiveness of our approach on many\ntreebanks, and demonstrate that often our constraint leads to better parsing\nperformance. We thus conclude that left-corner methods are particularly useful\nfor syntax-oriented systems, as it can exploit efficiently the inherent\nuniversal constraints in languages.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 01:18:53 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Noji", "Hiroshi", ""]]}, {"id": "1608.00318", "submitter": "Sungjin Ahn", "authors": "Sungjin Ahn, Heeyoul Choi, Tanel P\\\"arnamaa, Yoshua Bengio", "title": "A Neural Knowledge Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current language models have a significant limitation in the ability to\nencode and decode factual knowledge. This is mainly because they acquire such\nknowledge from statistical co-occurrences although most of the knowledge words\nare rarely observed. In this paper, we propose a Neural Knowledge Language\nModel (NKLM) which combines symbolic knowledge provided by the knowledge graph\nwith the RNN language model. By predicting whether the word to generate has an\nunderlying fact or not, the model can generate such knowledge-related words by\ncopying from the description of the predicted fact. In experiments, we show\nthat the NKLM significantly improves the performance while generating a much\nsmaller number of unknown words.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 04:42:49 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 15:34:01 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Ahn", "Sungjin", ""], ["Choi", "Heeyoul", ""], ["P\u00e4rnamaa", "Tanel", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1608.00329", "submitter": "Sujatha Das Gollapalli", "authors": "Sujatha Das Gollapalli and Xiao-li Li", "title": "Keyphrase Extraction using Sequential Labeling", "comments": "10 pages including 2 pages of references, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrases efficiently summarize a document's content and are used in various\ndocument processing and retrieval tasks. Several unsupervised techniques and\nclassifiers exist for extracting keyphrases from text documents. Most of these\nmethods operate at a phrase-level and rely on part-of-speech (POS) filters for\ncandidate phrase generation. In addition, they do not directly handle\nkeyphrases of varying lengths. We overcome these modeling shortcomings by\naddressing keyphrase extraction as a sequential labeling task in this paper. We\nexplore a basic set of features commonly used in NLP tasks as well as\npredictions from various unsupervised methods to train our taggers. In addition\nto a more natural modeling for the keyphrase extraction problem, we show that\ntagging models yield significant performance benefits over existing\nstate-of-the-art extraction methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 06:00:22 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 03:07:46 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Gollapalli", "Sujatha Das", ""], ["Li", "Xiao-li", ""]]}, {"id": "1608.00339", "submitter": "Jekaterina Novikova Dr.", "authors": "Jekaterina Novikova, Oliver Lemon and Verena Rieser", "title": "Crowd-sourcing NLG Data: Pictures Elicit Better Data", "comments": "The 9th International Natural Language Generation conference INLG,\n  2016. 10 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in corpus-based Natural Language Generation (NLG) hold the\npromise of being easily portable across domains, but require costly training\ndata, consisting of meaning representations (MRs) paired with Natural Language\n(NL) utterances. In this work, we propose a novel framework for crowdsourcing\nhigh quality NLG training data, using automatic quality control measures and\nevaluating different MRs with which to elicit data. We show that pictorial MRs\nresult in better NL data being collected than logic-based MRs: utterances\nelicited by pictorial MRs are judged as significantly more natural, more\ninformative, and better phrased, with a significant increase in average quality\nratings (around 0.5 points on a 6-point scale), compared to using the logical\nMRs. As the MR becomes more complex, the benefits of pictorial stimuli\nincrease. The collected data will be released as part of this submission.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 07:30:38 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Novikova", "Jekaterina", ""], ["Lemon", "Oliver", ""], ["Rieser", "Verena", ""]]}, {"id": "1608.00466", "submitter": "Madhusudan Lakshmana", "authors": "Madhusudan Lakshmana, Sundararajan Sellamanickam, Shirish Shevade,\n  Keerthi Selvaraj", "title": "Learning Semantically Coherent and Reusable Kernels in Convolution\n  Neural Nets for Sentence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art CNN models give good performance on sentence\nclassification tasks. The purpose of this work is to empirically study\ndesirable properties such as semantic coherence, attention mechanism and\nreusability of CNNs in these tasks. Semantically coherent kernels are\npreferable as they are a lot more interpretable for explaining the decision of\nthe learned CNN model. We observe that the learned kernels do not have semantic\ncoherence. Motivated by this observation, we propose to learn kernels with\nsemantic coherence using clustering scheme combined with Word2Vec\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\ntechnique to visualize attention mechanism of CNNs for decision explanation\npurpose. Reusable property enables kernels learned on one problem to be used in\nanother problem. This helps in efficient learning as only a few additional\ndomain specific filters may have to be learned. We demonstrate the efficacy of\nour core ideas of learning semantically coherent kernels and leveraging\nreusable kernels for efficient learning on several benchmark datasets.\nExperimental results show the usefulness of our approach by achieving\nperformance close to the state-of-the-art methods but with semantic and\nreusable properties.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:14:08 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 03:57:26 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Lakshmana", "Madhusudan", ""], ["Sellamanickam", "Sundararajan", ""], ["Shevade", "Shirish", ""], ["Selvaraj", "Keerthi", ""]]}, {"id": "1608.00470", "submitter": "Arpit Mittal", "authors": "Nikolaos Aletras, Arpit Mittal", "title": "Labeling Topics with Images using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topics generated by topic models are usually represented by lists of $t$\nterms or alternatively using short phrases and images. The current\nstate-of-the-art work on labeling topics using images selects images by\nre-ranking a small set of candidates for a given topic. In this paper, we\npresent a more generic method that can estimate the degree of association\nbetween any arbitrary pair of an unseen topic and image using a deep neural\nnetwork. Our method has better runtime performance $O(n)$ compared to $O(n^2)$\nfor the current state-of-the-art method, and is also significantly more\naccurate.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:27:16 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 16:49:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Aletras", "Nikolaos", ""], ["Mittal", "Arpit", ""]]}, {"id": "1608.00508", "submitter": "Paul Michel", "authors": "Paul Michel, Okko R\\\"as\\\"anen, Roland Thiolli\\`ere, Emmanuel Dupoux", "title": "Blind phoneme segmentation with temporal prediction errors", "comments": "7 pages 3 figures. Presented at ACL SRW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phonemic segmentation of speech is a critical step of speech recognition\nsystems. We propose a novel unsupervised algorithm based on sequence prediction\nmodels such as Markov chains and recurrent neural network. Our approach\nconsists in analyzing the error profile of a model trained to predict speech\nfeatures frame-by-frame. Specifically, we try to learn the dynamics of speech\nin the MFCC space and hypothesize boundaries from local maxima in the\nprediction error. We evaluate our system on the TIMIT dataset, with\nimprovements over similar methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 17:51:03 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 04:01:13 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Michel", "Paul", ""], ["R\u00e4s\u00e4nen", "Okko", ""], ["Thiolli\u00e8re", "Roland", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1608.00612", "submitter": "Abhyuday Jagannatha", "authors": "Abhyuday Jagannatha, Hong Yu", "title": "Structured prediction models for RNN based sequence labeling in clinical\n  text", "comments": "To appear in the proceedings of EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence labeling is a widely used method for named entity recognition and\ninformation extraction from unstructured natural language data. In clinical\ndomain one major application of sequence labeling involves extraction of\nmedical entities such as medication, indication, and side-effects from\nElectronic Health Record narratives. Sequence labeling in this domain, presents\nits own set of challenges and objectives. In this work we experimented with\nvarious CRF based structured learning models with Recurrent Neural Networks. We\nextend the previously studied LSTM-CRF models with explicit modeling of\npairwise potentials. We also propose an approximate version of skip-chain CRF\ninference with RNN potentials. We use these methodologies for structured\nprediction in order to improve the exact phrase detection of various medical\nentities.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 20:54:22 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Jagannatha", "Abhyuday", ""], ["Yu", "Hong", ""]]}, {"id": "1608.00789", "submitter": "Luk\\'a\\v{s} Svoboda", "authors": "Luk\\'a\\v{s} Svoboda and Tom\\'a\\v{s} Brychc\\'in", "title": "New word analogy corpus for exploring embeddings of Czech words", "comments": "paper accepted on Cicling 2016 conference, will be published in\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word embedding methods have been proven to be very useful in many tasks\nof NLP (Natural Language Processing). Much has been investigated about word\nembeddings of English words and phrases, but only little attention has been\ndedicated to other languages.\n  Our goal in this paper is to explore the behavior of state-of-the-art word\nembedding methods on Czech, the language that is characterized by very rich\nmorphology. We introduce new corpus for word analogy task that inspects\nsyntactic, morphosyntactic and semantic properties of Czech words and phrases.\nWe experiment with Word2Vec and GloVe algorithms and discuss the results on\nthis corpus. The corpus is available for the research community.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 12:31:06 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Svoboda", "Luk\u00e1\u0161", ""], ["Brychc\u00edn", "Tom\u00e1\u0161", ""]]}, {"id": "1608.00841", "submitter": "Mohammad Taher Pilehvar", "authors": "Jos\\'e Camacho-Collados, Ignacio Iacobacci, Roberto Navigli and\n  Mohammad Taher Pilehvar", "title": "Semantic Representations of Word Senses and Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the semantics of linguistic items in a machine-interpretable\nform has been a major goal of Natural Language Processing since its earliest\ndays. Among the range of different linguistic items, words have attracted the\nmost research attention. However, word representations have an important\nlimitation: they conflate different meanings of a word into a single vector.\nRepresentations of word senses have the potential to overcome this inherent\nlimitation. Indeed, the representation of individual word senses and concepts\nhas recently gained in popularity with several experimental results showing\nthat a considerable performance improvement can be achieved across different\nNLP applications upon moving from word level to the deeper sense and concept\nlevels. Another interesting point regarding the representation of concepts and\nword senses is that these models can be seamlessly applied to other linguistic\nitems, such as words, phrases and sentences.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:35:31 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Camacho-Collados", "Jos\u00e9", ""], ["Iacobacci", "Ignacio", ""], ["Navigli", "Roberto", ""], ["Pilehvar", "Mohammad Taher", ""]]}, {"id": "1608.00869", "submitter": "Daniela Gerz", "authors": "Daniela Gerz, Ivan Vuli\\'c, Felix Hill, Roi Reichart, Anna Korhonen", "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verbs play a critical role in the meaning of sentences, but these ubiquitous\nwords have received little attention in recent distributional semantics\nresearch. We introduce SimVerb-3500, an evaluation resource that provides human\nratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed\nverb types from the USF free-association database, providing at least three\nexamples for every VerbNet class. This broad coverage facilitates detailed\nanalyses of how syntactic and semantic phenomena together influence human\nunderstanding of verb meaning. Further, with significantly larger development\nand test sets than existing benchmarks, SimVerb-3500 enables more robust\nevaluation of representation learning architectures and promotes the\ndevelopment of methods tailored to verbs. We hope that SimVerb-3500 will enable\na richer understanding of the diversity and complexity of verb semantics and\nguide the development of systems that can effectively represent and interpret\nthis meaning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:35:12 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 15:39:53 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 06:20:24 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 14:35:14 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Gerz", "Daniela", ""], ["Vuli\u0107", "Ivan", ""], ["Hill", "Felix", ""], ["Reichart", "Roi", ""], ["Korhonen", "Anna", ""]]}, {"id": "1608.00892", "submitter": "Liang Lu", "authors": "Liang Lu and Michelle Guo and Steve Renals", "title": "Knowledge Distillation for Small-footprint Highway Networks", "comments": "5 pages, 2 figures, accepted to icassp 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has significantly advanced state-of-the-art of speech\nrecognition in the past few years. However, compared to conventional Gaussian\nmixture acoustic models, neural network models are usually much larger, and are\ntherefore not very deployable in embedded devices. Previously, we investigated\na compact highway deep neural network (HDNN) for acoustic modelling, which is a\ntype of depth-gated feedforward neural network. We have shown that HDNN-based\nacoustic models can achieve comparable recognition accuracy with much smaller\nnumber of model parameters compared to plain deep neural network (DNN) acoustic\nmodels. In this paper, we push the boundary further by leveraging on the\nknowledge distillation technique that is also known as {\\it teacher-student}\ntraining, i.e., we train the compact HDNN model with the supervision of a high\naccuracy cumbersome model. Furthermore, we also investigate sequence training\nand adaptation in the context of teacher-student training. Our experiments were\nperformed on the AMI meeting speech recognition corpus. With this technique, we\nsignificantly improved the recognition accuracy of the HDNN acoustic model with\nless than 0.8 million parameters, and narrowed the gap between this model and\nthe plain DNN with 30 million parameters.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 16:39:10 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 04:57:27 GMT"}, {"version": "v3", "created": "Tue, 20 Dec 2016 22:21:48 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Lu", "Liang", ""], ["Guo", "Michelle", ""], ["Renals", "Steve", ""]]}, {"id": "1608.00895", "submitter": "Patrick Doetsch", "authors": "Patrick Doetsch, Albert Zeyer, Paul Voigtlaender, Ilya Kulikov, Ralf\n  Schl\\\"uter, Hermann Ney", "title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we release our extensible and easily configurable neural network\ntraining software. It provides a rich set of functional layers with a\nparticular focus on efficient training of recurrent neural network topologies\non multiple GPUs. The source of the software package is public and freely\navailable for academic research purposes and can be used as a framework or as a\nstandalone tool which supports a flexible configuration. The software allows to\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\non both one dimensional data like speech or two dimensional data like\nhandwritten text and was used to develop successful submission systems in\nseveral evaluation campaigns.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 16:43:27 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 14:25:28 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Doetsch", "Patrick", ""], ["Zeyer", "Albert", ""], ["Voigtlaender", "Paul", ""], ["Kulikov", "Ilya", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1608.00929", "submitter": "Hao Tang", "authors": "Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu", "title": "Efficient Segmental Cascades for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative segmental models offer a way to incorporate flexible feature\nfunctions into speech recognition. However, their appeal has been limited by\ntheir computational requirements, due to the large number of possible segments\nto consider. Multi-pass cascades of segmental models introduce features of\nincreasing complexity in different passes, where in each pass a segmental model\nrescores lattices produced by a previous (simpler) segmental model. In this\npaper, we explore several ways of making segmental cascades efficient and\npractical: reducing the feature set in the first pass, frame subsampling, and\nvarious pruning approaches. In experiments on phonetic recognition, we find\nthat with a combination of such techniques, it is possible to maintain\ncompetitive performance while greatly reducing decoding, pruning, and training\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 18:45:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Weiran", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1608.00938", "submitter": "Mitchell Newberry", "authors": "Christopher A. Ahern, Mitchell G. Newberry, Robin Clark, Joshua B.\n  Plotkin", "title": "Evolutionary forces in language change", "comments": null, "journal-ref": null, "doi": "10.1038/nature24455", "report-no": null, "categories": "q-bio.PE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages and genes are both transmitted from generation to generation, with\nopportunity for differential reproduction and survivorship of forms. Here we\napply a rigorous inference framework, drawn from population genetics, to\ndistinguish between two broad mechanisms of language change: drift and\nselection. Drift is change that results from stochasticity in transmission and\nit may occur in the absence of any intrinsic difference between linguistic\nforms; whereas selection is truly an evolutionary force arising from intrinsic\ndifferences -- for example, when one form is preferred by members of the\npopulation. Using large corpora of parsed texts spanning the 12th century to\nthe 21st century, we analyze three examples of grammatical changes in English:\nthe regularization of past-tense verbs, the rise of the periphrastic `do', and\nsyntactic variation in verbal negation. We show that we can reject stochastic\ndrift in favor of a selective force driving some of these language changes, but\nnot others. The strength of drift depends on a word's frequency, and so drift\nprovides an alternative explanation for why some words are more prone to change\nthan others. Our results suggest an important role for stochasticity in\nlanguage change, and they provide a null model against which selective theories\nof language evolution must be compared.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 19:04:58 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ahern", "Christopher A.", ""], ["Newberry", "Mitchell G.", ""], ["Clark", "Robin", ""], ["Plotkin", "Joshua B.", ""]]}, {"id": "1608.01018", "submitter": "EPTCS", "authors": "Dimitrios Kartsaklis, Martha Lewis, Laura Rimell", "title": "Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection\n  of NLP, Physics and Cognitive Science", "comments": null, "journal-ref": "EPTCS 221, 2016", "doi": "10.4204/EPTCS.221", "report-no": null, "categories": "cs.CL cs.AI math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces\nat the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which\nwas held on the 11th of June at the University of Strathclyde, Glasgow, and was\nco-located with Quantum Physics and Logic (QPL 2016). Exploiting the common\nground provided by the concept of a vector space, the workshop brought together\nresearchers working at the intersection of Natural Language Processing (NLP),\ncognitive science, and physics, offering them an appropriate forum for\npresenting their uniquely motivated work and ideas. The interplay between these\nthree disciplines inspired theoretically motivated approaches to the\nunderstanding of how word meanings interact with each other in sentences and\ndiscourse, how diagrammatic reasoning depicts and simplifies this interaction,\nhow language models are determined by input from the world, and how word and\nsentence meanings interact logically. This first edition of the workshop\nconsisted of three invited talks from distinguished speakers (Hans Briegel,\nPeter G\\\"ardenfors, Dominic Widdows) and eight presentations of selected\ncontributed papers. Each submission was refereed by at least three members of\nthe Programme Committee, who delivered detailed and insightful comments and\nsuggestions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 22:28:45 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Kartsaklis", "Dimitrios", ""], ["Lewis", "Martha", ""], ["Rimell", "Laura", ""]]}, {"id": "1608.01056", "submitter": "Jacob Eisenstein", "authors": "Parminder Bhatia and Robert Guthrie and Jacob Eisenstein", "title": "Morphological Priors for Probabilistic Neural Word Embeddings", "comments": "Appeared at the Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2016, Austin)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings allow natural language processing systems to share\nstatistical information across related words. These embeddings are typically\nbased on distributional statistics, making it difficult for them to generalize\nto rare or unseen words. We propose to improve word embeddings by incorporating\nmorphological information, capturing shared sub-word features. Unlike previous\nwork that constructs word embeddings directly from morphemes, we combine\nmorphological and distributional information in a unified probabilistic\nframework, in which the word embedding is a latent variable. The morphological\ninformation provides a prior distribution on the latent word embeddings, which\nin turn condition a likelihood function over an observed corpus. This approach\nyields improvements on intrinsic word similarity evaluations, and also in the\ndownstream task of part-of-speech tagging.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 02:21:16 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 01:28:00 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Bhatia", "Parminder", ""], ["Guthrie", "Robert", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1608.01084", "submitter": "Christian Hadiwinoto", "authors": "Christian Hadiwinoto and Yang Liu and Hwee Tou Ng", "title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering\n  in Statistical Machine Translation", "comments": "7 pages, 1 figures, Proceedings of AAAI-16", "journal-ref": "Proceedings of AAAI-16. (pp. 2943--2949) (2016)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reordering poses a major challenge in machine translation (MT) between two\nlanguages with significant differences in word order. In this paper, we present\na novel reordering approach utilizing sparse features based on dependency word\npairs. Each instance of these features captures whether two words, which are\nrelated by a dependency link in the source sentence dependency parse tree,\nfollow the same order or are swapped in the translation output. Experiments on\nChinese-to-English translation show a statistically significant improvement of\n1.21 BLEU point using our approach, compared to a state-of-the-art statistical\nMT system that incorporates prior reordering approaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 06:24:01 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Hadiwinoto", "Christian", ""], ["Liu", "Yang", ""], ["Ng", "Hwee Tou", ""]]}, {"id": "1608.01238", "submitter": "Manuel Ciosici", "authors": "Manuel R. Ciosici", "title": "Improving Quality of Hierarchical Clustering for Large Data Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Brown clustering is a hard, hierarchical, bottom-up clustering of words in a\nvocabulary. Words are assigned to clusters based on their usage pattern in a\ngiven corpus. The resulting clusters and hierarchical structure can be used in\nconstructing class-based language models and for generating features to be used\nin NLP tasks. Because of its high computational cost, the most-used version of\nBrown clustering is a greedy algorithm that uses a window to restrict its\nsearch space. Like other clustering algorithms, Brown clustering finds a\nsub-optimal, but nonetheless effective, mapping of words to clusters. Because\nof its ability to produce high-quality, human-understandable cluster, Brown\nclustering has seen high uptake the NLP research community where it is used in\nthe preprocessing and feature generation steps.\n  Little research has been done towards improving the quality of Brown\nclusters, despite the greedy and heuristic nature of the algorithm. The\napproaches tried so far have focused on: studying the effect of the\ninitialisation in a similar algorithm; tuning the parameters used to define the\ndesired number of clusters and the behaviour of the algorithm; and including a\nseparate parameter to differentiate the window from the desired number of\nclusters. However, some of these approaches have not yielded significant\nimprovements in cluster quality.\n  In this thesis, a close analysis of the Brown algorithm is provided,\nrevealing important under-specifications and weaknesses in the original\nalgorithm. These have serious effects on cluster quality and reproducibility of\nresearch using Brown clustering. In the second part of the thesis, two\nmodifications are proposed. Finally, a thorough evaluation is performed,\nconsidering both the optimization criterion of Brown clustering and the\nperformance of the resulting class-based language models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 16:12:23 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Ciosici", "Manuel R.", ""]]}, {"id": "1608.01247", "submitter": "Sai Keshav Kolluru", "authors": "S.K Kolluru and Prasenjit Mukherjee", "title": "Query Clustering using Segment Specific Context Embeddings", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel query clustering approach to capture the broad\ninterest areas of users querying search engines. We make use of recent advances\nin NLP - word2vec and extend it to get query2vec, vector representations of\nqueries, based on query contexts, obtained from the top search results for the\nquery and use a highly scalable Divide & Merge clustering algorithm on top of\nthe query vectors, to get the clusters. We have tried this approach on a\nvariety of segments, including Retail, Travel, Health, Phones and found the\nclusters to be effective in discovering user's interest areas which have high\nmonetization potential.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 16:33:32 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 04:59:48 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Kolluru", "S. K", ""], ["Mukherjee", "Prasenjit", ""]]}, {"id": "1608.01281", "submitter": "Navdeep Jaitly", "authors": "Yuping Luo, Chung-Cheng Chiu, Navdeep Jaitly, Ilya Sutskever", "title": "Learning Online Alignments with Continuous Rewards Policy Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models with soft attention had significant success in\nmachine translation, speech recognition, and question answering. Though capable\nand easy to use, they require that the entirety of the input sequence is\navailable at the beginning of inference, an assumption that is not valid for\ninstantaneous translation and speech recognition. To address this problem, we\npresent a new method for solving sequence-to-sequence problems using hard\nonline alignments instead of soft offline alignments. The online alignments\nmodel is able to start producing outputs without the need to first process the\nentire input sequence. A highly accurate online sequence-to-sequence model is\nuseful because it can be used to build an accurate voice-based instantaneous\ntranslator. Our model uses hard binary stochastic decisions to select the\ntimesteps at which outputs will be produced. The model is trained to produce\nthese stochastic decisions using a standard policy gradient method. In our\nexperiments, we show that this model achieves encouraging performance on TIMIT\nand Wall Street Journal (WSJ) speech recognition datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:35:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Luo", "Yuping", ""], ["Chiu", "Chung-Cheng", ""], ["Jaitly", "Navdeep", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1608.01298", "submitter": "Peter Wittek", "authors": "S\\'andor Dar\\'anyi, Peter Wittek, Konstantinos Konstantinidis, Symeon\n  Papadopoulos, Efstratios Kontopoulos", "title": "A Physical Metaphor to Study Semantic Drift", "comments": "8 pages, 4 figures, to appear in Proceedings of SuCCESS-16, 1st\n  International Workshop on Semantic Change & Evolving Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In accessibility tests for digital preservation, over time we experience\ndrifts of localized and labelled content in statistical models of evolving\nsemantics represented as a vector field. This articulates the need to detect,\nmeasure, interpret and model outcomes of knowledge dynamics. To this end we\nemploy a high-performance machine learning algorithm for the training of\nextremely large emergent self-organizing maps for exploratory data analysis.\nThe working hypothesis we present here is that the dynamics of semantic drifts\ncan be modeled on a relaxed version of Newtonian mechanics called social\nmechanics. By using term distances as a measure of semantic relatedness vs.\ntheir PageRank values indicating social importance and applied as variable\n`term mass', gravitation as a metaphor to express changes in the semantic\ncontent of a vector field lends a new perspective for experimentation. From\n`term gravitation' over time, one can compute its generating potential whose\nfluctuations manifest modifications in pairwise term similarity vs. social\nimportance, thereby updating Osgood's semantic differential. The dataset\nexamined is the public catalog metadata of Tate Galleries, London.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 19:34:13 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Wittek", "Peter", ""], ["Konstantinidis", "Konstantinos", ""], ["Papadopoulos", "Symeon", ""], ["Kontopoulos", "Efstratios", ""]]}, {"id": "1608.01401", "submitter": "EPTCS", "authors": "Daniela Ashoush (Univesity of Oxford), Bob Coecke (Univesity of\n  Oxford)", "title": "Dual Density Operators and Natural Language Meaning", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 1-10", "doi": "10.4204/EPTCS.221.1", "report-no": null, "categories": "cs.CL cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density operators allow for representing ambiguity about a vector\nrepresentation, both in quantum theory and in distributional natural language\nmeaning. Formally equivalently, they allow for discarding part of the\ndescription of a composite system, where we consider the discarded part to be\nthe context. We introduce dual density operators, which allow for two\nindependent notions of context. We demonstrate the use of dual density\noperators within a grammatical-compositional distributional framework for\nnatural language meaning. We show that dual density operators can be used to\nsimultaneously represent: (i) ambiguity about word meanings (e.g. queen as a\nperson vs. queen as a band), and (ii) lexical entailment (e.g. tiger ->\nmammal). We provide a proof-of-concept example.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:12 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Ashoush", "Daniela", "", "Univesity of Oxford"], ["Coecke", "Bob", "", "Univesity of\n  Oxford"]]}, {"id": "1608.01402", "submitter": "EPTCS", "authors": "Josef Bolt (Univesity of Oxford), Bob Coecke (Univesity of Oxford),\n  Fabrizio Genovese (Univesity of Oxford), Martha Lewis (Univesity of Oxford),\n  Daniel Marsden (Univesity of Oxford), Robin Piedeleu (Univesity of Oxford)", "title": "Interacting Conceptual Spaces", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 11-19", "doi": "10.4204/EPTCS.221.2", "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose applying the categorical compositional scheme of [6] to conceptual\nspace models of cognition. In order to do this we introduce the category of\nconvex relations as a new setting for categorical compositional semantics,\nemphasizing the convex structure important to conceptual space applications. We\nshow how conceptual spaces for composite types such as adjectives and verbs can\nbe constructed. We illustrate this new model on detailed examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:21 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Bolt", "Josef", "", "Univesity of Oxford"], ["Coecke", "Bob", "", "Univesity of Oxford"], ["Genovese", "Fabrizio", "", "Univesity of Oxford"], ["Lewis", "Martha", "", "Univesity of Oxford"], ["Marsden", "Daniel", "", "Univesity of Oxford"], ["Piedeleu", "Robin", "", "Univesity of Oxford"]]}, {"id": "1608.01403", "submitter": "EPTCS", "authors": "Stephen McGregor (Queen Mary University of London), Matthew Purver\n  (Queen Mary University of London), Geraint Wiggins (Queen Mary University of\n  London)", "title": "Words, Concepts, and the Geometry of Analogy", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 39-48", "doi": "10.4204/EPTCS.221.5", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a geometric approach to the problem of modelling the\nrelationship between words and concepts, focusing in particular on analogical\nphenomena in language and cognition. Grounded in recent theories regarding\ngeometric conceptual spaces, we begin with an analysis of existing static\ndistributional semantic models and move on to an exploration of a dynamic\napproach to using high dimensional spaces of word meaning to project subspaces\nwhere analogies can potentially be solved in an online, contextualised way. The\ncrucial element of this analysis is the positioning of statistics in a\ngeometric environment replete with opportunities for interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:48 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["McGregor", "Stephen", "", "Queen Mary University of London"], ["Purver", "Matthew", "", "Queen Mary University of London"], ["Wiggins", "Geraint", "", "Queen Mary University of\n  London"]]}, {"id": "1608.01404", "submitter": "EPTCS", "authors": "Mehrnoosh Sadrzadeh (Queen Mary University of London)", "title": "Quantifier Scope in Categorical Compositional Distributional Semantics", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 49-57", "doi": "10.4204/EPTCS.221.6", "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work with J. Hedges, we formalised a generalised quantifiers\ntheory of natural language in categorical compositional distributional\nsemantics with the help of bialgebras. In this paper, we show how quantifier\nscope ambiguity can be represented in that setting and how this representation\ncan be generalised to branching quantifiers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:57 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Sadrzadeh", "Mehrnoosh", "", "Queen Mary University of London"]]}, {"id": "1608.01405", "submitter": "EPTCS", "authors": "John van de Wetering (Radboud University)", "title": "Entailment Relations on Distributions", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 58-66", "doi": "10.4204/EPTCS.221.7", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give an overview of partial orders on the space of\nprobability distributions that carry a notion of information content and serve\nas a generalisation of the Bayesian order given in (Coecke and Martin, 2011).\nWe investigate what constraints are necessary in order to get a unique notion\nof information content. These partial orders can be used to give an ordering on\nwords in vector space models of natural language meaning relating to the\ncontexts in which words are used, which is useful for a notion of entailment\nand word disambiguation. The construction used also points towards a way to\ncreate orderings on the space of density operators which allow a more\nfine-grained study of entailment. The partial orders in this paper are directed\ncomplete and form domains in the sense of domain theory.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:37:07 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["van de Wetering", "John", "", "Radboud University"]]}, {"id": "1608.01406", "submitter": "EPTCS", "authors": "William Zeng (Rigetti Computing), Bob Coecke (Univesity of Oxford)", "title": "Quantum Algorithms for Compositional Natural Language Processing", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 67-75", "doi": "10.4204/EPTCS.221.8", "report-no": null, "categories": "cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new application of quantum computing to the field of natural\nlanguage processing. Ongoing work in this field attempts to incorporate\ngrammatical structure into algorithms that compute meaning. In (Coecke,\nSadrzadeh and Clark, 2010), the authors introduce such a model (the CSC model)\nbased on tensor product composition. While this algorithm has many advantages,\nits implementation is hampered by the large classical computational resources\nthat it requires. In this work we show how computational shortcomings of the\nCSC approach could be resolved using quantum computation (possibly in addition\nto existing techniques for dimension reduction). We address the value of\nquantum RAM (Giovannetti,2008) for this model and extend an algorithm from\nWiebe, Braun and Lloyd (2012) into a quantum algorithm to categorize sentences\nin CSC. Our new algorithm demonstrates a quadratic speedup over classical\nmethods under certain conditions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:37:16 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Zeng", "William", "", "Rigetti Computing"], ["Coecke", "Bob", "", "Univesity of Oxford"]]}, {"id": "1608.01413", "submitter": "Subhro Roy", "authors": "Subhro Roy and Dan Roth", "title": "Solving General Arithmetic Word Problems", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel approach to automatically solving arithmetic word\nproblems. This is the first algorithmic approach that can handle arithmetic\nproblems with multiple steps and operations, without depending on additional\nannotations or predefined templates. We develop a theory for expression trees\nthat can be used to represent and evaluate the target arithmetic expressions;\nwe use it to uniquely decompose the target arithmetic problem to multiple\nclassification problems; we then compose an expression tree, combining these\nwith world knowledge through a constrained inference framework. Our classifiers\ngain from the use of {\\em quantity schemas} that supports better extraction of\nfeatures. Experimental results show that our method outperforms existing\nsystems, achieving state of the art performance on benchmark datasets of\narithmetic word problems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 01:47:23 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 11:50:41 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Roy", "Subhro", ""], ["Roth", "Dan", ""]]}, {"id": "1608.01448", "submitter": "Qingrong Xia", "authors": "Qingrong Xia, Zhenghua Li, Jiayuan Chao, Min Zhang", "title": "Word Segmentation on Micro-blog Texts with External Lexicon and\n  Heterogeneous Data", "comments": "12 pages, 3 figures, Proceedings of 5th {CCF} Conference on Natural\n  Language Processing and Chinese Computing (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our system designed for the NLPCC 2016 shared task on\nword segmentation on micro-blog texts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 07:37:54 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 12:43:43 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Xia", "Qingrong", ""], ["Li", "Zhenghua", ""], ["Chao", "Jiayuan", ""], ["Zhang", "Min", ""]]}, {"id": "1608.01561", "submitter": "Paheli Bhattacharya", "authors": "Paheli Bhattacharya, Pawan Goyal and Sudeshna Sarkar", "title": "UsingWord Embeddings for Query Translation for Hindi to English Cross\n  Language Information Retrieval", "comments": "17th International Conference on Intelligent Text Processing and\n  Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-Language Information Retrieval (CLIR) has become an important problem\nto solve in the recent years due to the growth of content in multiple languages\nin the Web. One of the standard methods is to use query translation from source\nto target language. In this paper, we propose an approach based on word\nembeddings, a method that captures contextual clues for a particular word in\nthe source language and gives those words as translations that occur in a\nsimilar context in the target language. Once we obtain the word embeddings of\nthe source and target language pairs, we learn a projection from source to\ntarget word embeddings, making use of a dictionary with word translation\npairs.We then propose various methods of query translation and aggregation. The\nadvantage of this approach is that it does not require the corpora to be\naligned (which is difficult to obtain for resource-scarce languages), a\ndictionary with word translation pairs is enough to train the word vectors for\ntranslation. We experiment with Forum for Information Retrieval and Evaluation\n(FIRE) 2008 and 2012 datasets for Hindi to English CLIR. The proposed word\nembedding based approach outperforms the basic dictionary based approach by 70%\nand when the word embeddings are combined with the dictionary, the hybrid\napproach beats the baseline dictionary based method by 77%. It outperforms the\nEnglish monolingual baseline by 15%, when combined with the translations\nobtained from Google Translate and Dictionary.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 14:44:52 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Goyal", "Pawan", ""], ["Sarkar", "Sudeshna", ""]]}, {"id": "1608.01884", "submitter": "Ernest Davis", "authors": "Ernest Davis", "title": "Winograd Schemas and Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Winograd schema is a pair of sentences that differ in a single word and\nthat contain an ambiguous pronoun whose referent is different in the two\nsentences and requires the use of commonsense knowledge or world knowledge to\ndisambiguate. This paper discusses how Winograd schemas and other sentence\npairs could be used as challenges for machine translation using distinctions\nbetween pronouns, such as gender, that appear in the target language but not in\nthe source.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 13:39:08 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 23:12:29 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Davis", "Ernest", ""]]}, {"id": "1608.01910", "submitter": "Cristina Espa\\~na-Bonet", "authors": "Pranava Swaroop Madhyastha, Cristina Espa\\~na-Bonet", "title": "Resolving Out-of-Vocabulary Words with Bilingual Embeddings in Machine\n  Translation", "comments": "6 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-vocabulary words account for a large proportion of errors in machine\ntranslation systems, especially when the system is used on a different domain\nthan the one where it was trained. In order to alleviate the problem, we\npropose to use a log-bilinear softmax-based model for vocabulary expansion,\nsuch that given an out-of-vocabulary source word, the model generates a\nprobabilistic list of possible translations in the target language. Our model\nuses only word embeddings trained on significantly large unlabelled monolingual\ncorpora and trains over a fairly small, word-to-word bilingual dictionary. We\ninput this probabilistic list into a standard phrase-based statistical machine\ntranslation system and obtain consistent improvements in translation quality on\nthe English-Spanish language pair. Especially, we get an improvement of 3.9\nBLEU points when tested over an out-of-domain test set.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 15:11:58 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Madhyastha", "Pranava Swaroop", ""], ["Espa\u00f1a-Bonet", "Cristina", ""]]}, {"id": "1608.01961", "submitter": "Mohammad Taher Pilehvar", "authors": "Mohammad Taher Pilehvar and Nigel Collier", "title": "De-Conflated Semantic Representations", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major deficiency of most semantic representation techniques is that they\nusually model a word type as a single point in the semantic space, hence\nconflating all the meanings that the word can have. Addressing this issue by\nlearning distinct representations for individual meanings of words has been the\nsubject of several research studies in the past few years. However, the\ngenerated sense representations are either not linked to any sense inventory or\nare unreliable for infrequent word senses. We propose a technique that tackles\nthese problems by de-conflating the representations of words based on the deep\nknowledge it derives from a semantic network. Our approach provides multiple\nadvantages in comparison to the past work, including its high coverage and the\nability to generate accurate representations even for infrequent word senses.\nWe carry out evaluations on six datasets across two semantic similarity tasks\nand report state-of-the-art results on most of them.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 18:14:19 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Pilehvar", "Mohammad Taher", ""], ["Collier", "Nigel", ""]]}, {"id": "1608.01965", "submitter": "Diego Amancio Dr.", "authors": "Camilo Akimushkin and Diego R. Amancio and Osvaldo N. Oliveira Jr", "title": "Text authorship identified using the dynamics of word co-occurrence\n  networks", "comments": null, "journal-ref": "PLoS ONE 12(1): e0170527, 2017", "doi": "10.1371/journal.pone.0170527", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of authorship in disputed documents still requires human\nexpertise, which is now unfeasible for many tasks owing to the large volumes of\ntext and authors in practical applications. In this study, we introduce a\nmethodology based on the dynamics of word co-occurrence networks representing\nwritten texts to classify a corpus of 80 texts by 8 authors. The texts were\ndivided into sections with equal number of linguistic tokens, from which time\nseries were created for 12 topological metrics. The series were proven to be\nstationary (p-value>0.05), which permits to use distribution moments as\nlearning attributes. With an optimized supervised learning procedure using a\nRadial Basis Function Network, 68 out of 80 texts were correctly classified,\ni.e. a remarkable 85% author matching success rate. Therefore, fluctuations in\npurely dynamic network metrics were found to characterize authorship, thus\nopening the way for the description of texts in terms of small evolving\nnetworks. Moreover, the approach introduced allows for comparison of texts with\ndiverse characteristics in a simple, fast fashion.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 20:33:22 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Akimushkin", "Camilo", ""], ["Amancio", "Diego R.", ""], ["Oliveira", "Osvaldo N.", "Jr"]]}, {"id": "1608.01972", "submitter": "Sun Kim", "authors": "Sun Kim, Nicolas Fiorini, W. John Wilbur and Zhiyong Lu", "title": "Bridging the Gap: Incorporating a Semantic Similarity Measure for\n  Effectively Mapping PubMed Queries to Documents", "comments": "10 pages, 1 figure, 3 tables", "journal-ref": null, "doi": "10.1016/j.jbi.2017.09.014", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main approach of traditional information retrieval (IR) is to examine how\nmany words from a query appear in a document. A drawback of this approach,\nhowever, is that it may fail to detect relevant documents where no or only few\nwords from a query are found. The semantic analysis methods such as LSA (latent\nsemantic analysis) and LDA (latent Dirichlet allocation) have been proposed to\naddress the issue, but their performance is not superior compared to common IR\napproaches. Here we present a query-document similarity measure motivated by\nthe Word Mover's Distance. Unlike other similarity measures, the proposed\nmethod relies on neural word embeddings to compute the distance between words.\nThis process helps identify related words when no direct matches are found\nbetween a query and a document. Our method is efficient and straightforward to\nimplement. The experimental results on TREC Genomics data show that our\napproach outperforms the BM25 ranking function by an average of 12% in mean\naverage precision. Furthermore, for a real-world dataset collected from the\nPubMed search logs, we combine the semantic measure with BM25 using a learning\nto rank method, which leads to improved ranking scores by up to 25%. This\nexperiment demonstrates that the proposed approach and BM25 nicely complement\neach other and together produce superior performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 18:53:42 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 19:33:57 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Kim", "Sun", ""], ["Fiorini", "Nicolas", ""], ["Wilbur", "W. John", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1608.02025", "submitter": "Jake Williams", "authors": "Jake Ryland Williams", "title": "Boundary-based MWE segmentation with text partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a fine-grained, text-chunking algorithm designed for the\ntask of multiword expressions (MWEs) segmentation. As a lexical class, MWEs\ninclude a wide variety of idioms, whose automatic identification are a\nnecessity for the handling of colloquial language. This algorithm's core\nnovelty is its use of non-word tokens, i.e., boundaries, in a bottom-up\nstrategy. Leveraging boundaries refines token-level information, forging\nhigh-level performance from relatively basic data. The generality of this\nmodel's feature space allows for its application across languages and domains.\nExperiments spanning 19 different languages exhibit a broadly-applicable,\nstate-of-the-art model. Evaluation against recent shared-task data places text\npartitioning as the overall, best performing MWE segmentation algorithm,\ncovering all MWE classes and multiple English domains (including user-generated\ntext). This performance, coupled with a non-combinatorial, fast-running design,\nproduces an ideal combination for implementations at scale, which are\nfacilitated through the release of open-source software.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 21:27:00 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 20:45:35 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 16:42:12 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Williams", "Jake Ryland", ""]]}, {"id": "1608.02071", "submitter": "Yun Liu", "authors": "Yun Liu, Kun-Ta Chuang, Fu-Wen Liang, Huey-Jen Su, Collin M. Stultz,\n  John V. Guttag", "title": "Transferring Knowledge from Text to Predict Disease Onset", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": "Proceedings of Machine Learning Research Volume 56", "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains such as medicine, training data is in short supply. In such\ncases, external knowledge is often helpful in building predictive models. We\npropose a novel method to incorporate publicly available domain expertise to\nbuild accurate models. Specifically, we use word2vec models trained on a\ndomain-specific corpus to estimate the relevance of each feature's text\ndescription to the prediction problem. We use these relevance estimates to\nrescale the features, causing more important features to experience weaker\nregularization.\n  We apply our method to predict the onset of five chronic diseases in the next\nfive years in two genders and two age groups. Our rescaling approach improves\nthe accuracy of the model, particularly when there are few positive examples.\nFurthermore, our method selects 60% fewer features, easing interpretation by\nphysicians. Our method is applicable to other domains where feature and outcome\ndescriptions are available.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 06:24:59 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Liu", "Yun", ""], ["Chuang", "Kun-Ta", ""], ["Liang", "Fu-Wen", ""], ["Su", "Huey-Jen", ""], ["Stultz", "Collin M.", ""], ["Guttag", "John V.", ""]]}, {"id": "1608.02076", "submitter": "Hao Cheng", "authors": "Hao Cheng and Hao Fang and Xiaodong He and Jianfeng Gao and Li Deng", "title": "Bi-directional Attention with Agreement for Dependency Parsing", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel bi-directional attention model for dependency parsing,\nwhich learns to agree on headword predictions from the forward and backward\nparsing directions. The parsing procedure for each direction is formulated as\nsequentially querying the memory component that stores continuous headword\nembeddings. The proposed parser makes use of {\\it soft} headword embeddings,\nallowing the model to implicitly capture high-order parsing history without\ndramatically increasing the computational complexity. We conduct experiments on\nEnglish, Chinese, and 12 other languages from the CoNLL 2006 shared task,\nshowing that the proposed model achieves state-of-the-art unlabeled attachment\nscores on 6 languages.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 07:16:31 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 08:52:31 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1608.02094", "submitter": "Leon Derczynski", "authors": "Leon Derczynski", "title": "Desiderata for Vector-Space Word Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of vector-space representations for words is currently available,\nwhich is growing. These consist of fixed-length vectors containing real values,\nwhich represent a word. The result is a representation upon which the power of\nmany conventional information processing and data mining techniques can be\nbrought to bear, as long as the representations are designed with some\nforethought and fit certain constraints. This paper details desiderata for the\ndesign of vector space representations of words.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 10:47:05 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Derczynski", "Leon", ""]]}, {"id": "1608.02097", "submitter": "Su Zhu", "authors": "Su Zhu, Kai Yu", "title": "Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken\n  Language Understanding", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the framework of encoder-decoder with attention for\nsequence labelling based spoken language understanding. We introduce\nBidirectional Long Short Term Memory - Long Short Term Memory networks\n(BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep\nlearning. In the sequence labelling task, the input and output sequences are\naligned word by word, while the attention mechanism cannot provide the exact\nalignment. To address this limitation, we propose a novel focus mechanism for\nencoder-decoder framework. Experiments on the standard ATIS dataset showed that\nBLSTM-LSTM with focus mechanism defined the new state-of-the-art by\noutperforming standard BLSTM and attention based encoder-decoder. Further\nexperiments also show that the proposed model is more robust to speech\nrecognition errors.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 11:41:05 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 14:50:11 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Zhu", "Su", ""], ["Yu", "Kai", ""]]}, {"id": "1608.02117", "submitter": "Ivan Vuli\\'c", "authors": "Ivan Vuli\\'c, Daniela Gerz, Douwe Kiela, Felix Hill, and Anna Korhonen", "title": "HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HyperLex - a dataset and evaluation resource that quantifies the\nextent of of the semantic category membership, that is, type-of relation also\nknown as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616\nconcept pairs. Cognitive psychology research has established that typicality\nand category/class membership are computed in human semantic memory as a\ngradual rather than binary relation. Nevertheless, most NLP research, and\nexisting large-scale invetories of concept category membership (WordNet,\nDBPedia, etc.) treat category membership and LE as binary. To address this, we\nasked hundreds of native English speakers to indicate typicality and strength\nof category membership between a diverse range of concept pairs on a\ncrowdsourcing platform. Our results confirm that category membership and LE are\nindeed more gradual than binary. We then compare these human judgements with\nthe predictions of automatic systems, which reveals a huge gap between human\nperformance and state-of-the-art LE, distributional and representation learning\nmodels, and substantial differences between the models themselves. We discuss a\npathway for improving semantic models to overcome this discrepancy, and\nindicate future application areas for improved graded LE systems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 15:29:34 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 15:07:53 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Vuli\u0107", "Ivan", ""], ["Gerz", "Daniela", ""], ["Kiela", "Douwe", ""], ["Hill", "Felix", ""], ["Korhonen", "Anna", ""]]}, {"id": "1608.02153", "submitter": "Uwe Springmann", "authors": "U. Springmann and A. L\\\"udeling", "title": "OCR of historical printings with an application to building diachronic\n  corpora: A case study using the RIDGES herbal corpus", "comments": "25 pages, 7 figures, 2 tables; will appear in Digital Humanities\n  Quarterly", "journal-ref": "Digital Humanities Quarterly 11, 2 (2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the results of a case study that applies Neural\nNetwork-based Optical Character Recognition (OCR) to scanned images of books\nprinted between 1487 and 1870 by training the OCR engine OCRopus\n[@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted].\nTraining specific OCR models was possible because the necessary *ground truth*\nis available as error-corrected diplomatic transcriptions. The OCR results have\nbeen evaluated for accuracy against the ground truth of unseen test sets.\nCharacter and word accuracies (percentage of correctly recognized items) for\nthe resulting machine-readable texts of individual documents range from 94% to\nmore than 99% (character level) and from 76% to 97% (word level). This includes\nthe earliest printed books, which were thought to be inaccessible by OCR\nmethods until recently. Furthermore, OCR models trained on one part of the\ncorpus consisting of books with different printing dates and different typesets\n*(mixed models)* have been tested for their predictive power on the books from\nthe other part containing yet other fonts, mostly yielding character accuracies\nwell above 90%. It therefore seems possible to construct generalized models\ntrained on a range of fonts that can be applied to a wide variety of historical\nprintings still giving good results. A moderate postcorrection effort of some\npages will then enable the training of individual models with even better\naccuracies. Using this method, diachronic corpora including early printings can\nbe constructed much faster and cheaper than by manual transcription. The OCR\nmethods reported here open up the possibility of transforming our printed\ntextual cultural heritage into electronic text by largely automatic means,\nwhich is a prerequisite for the mass conversion of scanned books.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 20:51:53 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 14:41:07 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Springmann", "U.", ""], ["L\u00fcdeling", "A.", ""]]}, {"id": "1608.02195", "submitter": "Felix Biessmann", "authors": "Felix Biessmann", "title": "Automating Political Bias Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day media generate large amounts of text. An unbiased view on media\nreports requires an understanding of the political bias of media content.\nAssistive technology for estimating the political bias of texts can be helpful\nin this context. This study proposes a simple statistical learning approach to\npredict political bias from text. Standard text features extracted from\nspeeches and manifestos of political parties are used to predict political bias\nin terms of political party affiliation and in terms of political views.\nResults indicate that political bias can be predicted with above chance\naccuracy. Mistakes of the model can be interpreted with respect to changes of\npolicies of political actors. Two approaches are presented to make the results\nmore interpretable: a) discriminative text features are related to the\npolitical orientation of a party and b) sentiment features of texts are\ncorrelated with a measure of political power. Political power appears to be\nstrongly correlated with positive sentiment of a text. To highlight some\npotential use cases a web application shows how the model can be used for texts\nfor which the political bias is not clear such as news articles.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 09:04:59 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Biessmann", "Felix", ""]]}, {"id": "1608.02214", "submitter": "Keisuke Sakaguchi", "authors": "Keisuke Sakaguchi, Kevin Duh, Matt Post, and Benjamin Van Durme", "title": "Robsut Wrod Reocginiton via semi-Character Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language processing mechanism by humans is generally more robust than\ncomputers. The Cmabrigde Uinervtisy (Cambridge University) effect from the\npsycholinguistics literature has demonstrated such a robust word processing\nmechanism, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with\nlittle cost. On the other hand, computational models for word recognition (e.g.\nspelling checkers) perform poorly on data with such noise. Inspired by the\nfindings from the Cmabrigde Uinervtisy effect, we propose a word recognition\nmodel based on a semi-character level recurrent neural network (scRNN). In our\nexperiments, we demonstrate that scRNN has significantly more robust\nperformance in word spelling correction (i.e. word recognition) compared to\nexisting spelling checkers and character-based convolutional neural network.\nFurthermore, we demonstrate that the model is cognitively plausible by\nreplicating a psycholinguistics experiment about human reading difficulty using\nour model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 13:28:46 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 07:56:39 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Sakaguchi", "Keisuke", ""], ["Duh", "Kevin", ""], ["Post", "Matt", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1608.02254", "submitter": "Stepan Kuznetsov", "authors": "Max Kanovich, Stepan Kuznetsov, Andre Scedrov", "title": "Reconciling Lambek's restriction, cut-elimination, and substitution in\n  the presence of exponential modalities", "comments": "23 pages. Submitted for publication to APAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lambek calculus can be considered as a version of non-commutative\nintuitionistic linear logic. One of the interesting features of the Lambek\ncalculus is the so-called \"Lambek's restriction,\" that is, the antecedent of\nany provable sequent should be non-empty. In this paper we discuss ways of\nextending the Lambek calculus with the linear logic exponential modality while\nkeeping Lambek's restriction. Interestingly enough, we show that for any system\nequipped with a reasonable exponential modality the following holds: if the\nsystem enjoys cut elimination and substitution to the full extent, then the\nsystem necessarily violates Lambek's restriction. Nevertheless, we show that\ntwo of the three conditions can be implemented. Namely, we design a system with\nLambek's restriction and cut elimination and another system with Lambek's\nrestriction and substitution. For both calculi we prove that they are\nundecidable, even if we take only one of the two divisions provided by the\nLambek calculus. The system with cut elimination and substitution and without\nLambek's restriction is folklore and known to be undecidable.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 18:17:02 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 15:57:23 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Kanovich", "Max", ""], ["Kuznetsov", "Stepan", ""], ["Scedrov", "Andre", ""]]}, {"id": "1608.02272", "submitter": "Ali Khodabakhsh", "authors": "Ali Khodabakhsh, Seyyed Saeed Sarfjoo, Umut Uludag, Osman Soyyigit,\n  Cenk Demiroglu", "title": "Incorporation of Speech Duration Information in Score Fusion of Speaker\n  Recognition Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years identity-vector (i-vector) based speaker verification (SV)\nsystems have become very successful. Nevertheless, environmental noise and\nspeech duration variability still have a significant effect on degrading the\nperformance of these systems. In many real-life applications, duration of\nrecordings are very short; as a result, extracted i-vectors cannot reliably\nrepresent the attributes of the speaker. Here, we investigate the effect of\nspeech duration on the performance of three state-of-the-art speaker\nrecognition systems. In addition, using a variety of available score fusion\nmethods, we investigate the effect of score fusion for those speaker\nverification techniques to benefit from the performance difference of different\nmethods under different enrollment and test speech duration conditions. This\ntechnique performed significantly better than the baseline score fusion\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 21:43:08 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Khodabakhsh", "Ali", ""], ["Sarfjoo", "Seyyed Saeed", ""], ["Uludag", "Umut", ""], ["Soyyigit", "Osman", ""], ["Demiroglu", "Cenk", ""]]}, {"id": "1608.02289", "submitter": "Rossano Schifanella", "authors": "Rossano Schifanella, Paloma de Juan, Joel Tetreault, Liangliang Cao", "title": "Detecting Sarcasm in Multimodal Social Platforms", "comments": "10 pages, 3 figures, final version published in the Proceedings of\n  ACM Multimedia 2016", "journal-ref": null, "doi": "10.1145/2964284.2964321", "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm is a peculiar form of sentiment expression, where the surface\nsentiment differs from the implied sentiment. The detection of sarcasm in\nsocial media platforms has been applied in the past mainly to textual\nutterances where lexical indicators (such as interjections and intensifiers),\nlinguistic markers, and contextual information (such as user profiles, or past\nconversations) were used to detect the sarcastic tone. However, modern social\nmedia platforms allow to create multimodal messages where audiovisual content\nis integrated with the text, making the analysis of a mode in isolation\npartial. In our work, we first study the relationship between the textual and\nvisual aspects in multimodal posts from three major social media platforms,\ni.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to\nquantify the extent to which images are perceived as necessary by human\nannotators. Moreover, we propose two different computational frameworks to\ndetect sarcasm that integrate the textual and visual modalities. The first\napproach exploits visual semantics trained on an external dataset, and\nconcatenates the semantics features with state-of-the-art textual features. The\nsecond method adapts a visual neural network initialized with parameters\ntrained on ImageNet to multimodal sarcastic posts. Results show the positive\neffect of combining modalities for the detection of sarcasm across platforms\nand methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 00:59:03 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Schifanella", "Rossano", ""], ["de Juan", "Paloma", ""], ["Tetreault", "Joel", ""], ["Cao", "Liangliang", ""]]}, {"id": "1608.02519", "submitter": "Marina Sokolova", "authors": "Marina Sokolova, Kanyi Huang, Stan Matwin, Joshua Ramisch, Vera\n  Sazonova, Renee Black, Chris Orwa, Sidney Ochieng, Nanjira Sambuli", "title": "Topic Modelling and Event Identification from Twitter Textual Data", "comments": "17 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous growth of social media content on the Internet has inspired\nthe development of the text analytics to understand and solve real-life\nproblems. Leveraging statistical topic modelling helps researchers and\npractitioners in better comprehension of textual content as well as provides\nuseful information for further analysis. Statistical topic modelling becomes\nespecially important when we work with large volumes of dynamic text, e.g.,\nFacebook or Twitter datasets. In this study, we summarize the message content\nof four data sets of Twitter messages relating to challenging social events in\nKenya. We use Latent Dirichlet Allocation (LDA) topic modelling to analyze the\ncontent. Our study uses two evaluation measures, Normalized Mutual Information\n(NMI) and topic coherence analysis, to select the best LDA models. The obtained\nLDA results show that the tool can be effectively used to extract discussion\ntopics and summarize them for further manual analysis\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 17:03:03 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Sokolova", "Marina", ""], ["Huang", "Kanyi", ""], ["Matwin", "Stan", ""], ["Ramisch", "Joshua", ""], ["Sazonova", "Vera", ""], ["Black", "Renee", ""], ["Orwa", "Chris", ""], ["Ochieng", "Sidney", ""], ["Sambuli", "Nanjira", ""]]}, {"id": "1608.02689", "submitter": "Nanyun Peng", "authors": "Nanyun Peng and Mark Dredze", "title": "Multi-task Domain Adaptation for Sequence Tagging", "comments": "The version for ACL 2017 Repl4NLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many domain adaptation approaches rely on learning cross domain shared\nrepresentations to transfer the knowledge learned in one domain to other\ndomains. Traditional domain adaptation only considers adapting for one task. In\nthis paper, we explore multi-task representation learning under the domain\nadaptation scenario. We propose a neural network framework that supports domain\nadaptation for multiple tasks simultaneously, and learns shared representations\nthat better generalize for domain adaptation. We apply the proposed framework\nto domain adaptation for sequence tagging problems considering two tasks:\nChinese word segmentation and named entity recognition. Experiments show that\nmulti-task domain adaptation works better than disjoint domain adaptation for\neach task, and achieves the state-of-the-art results for both tasks in the\nsocial media domain.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 04:38:38 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 14:31:13 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Peng", "Nanyun", ""], ["Dredze", "Mark", ""]]}, {"id": "1608.02717", "submitter": "Mateusz Malinowski", "authors": "Ashkan Mokarian and Mateusz Malinowski and Mario Fritz", "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task", "comments": "Accepted to BMVC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:24:02 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Mokarian", "Ashkan", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1608.02784", "submitter": "Nikos Papasarantopoulos", "authors": "Nikos Papasarantopoulos, Helen Jiang, Shay B. Cohen", "title": "Canonical Correlation Inference for Mapping Abstract Scenes to Text", "comments": "10 pages, accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique for structured prediction, based on canonical\ncorrelation analysis. Our learning algorithm finds two projections for the\ninput and the output spaces that aim at projecting a given input and its\ncorrect output into points close to each other. We demonstrate our technique on\na language-vision problem, namely the problem of giving a textual description\nto an \"abstract scene\".\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 12:26:19 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 19:53:13 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Papasarantopoulos", "Nikos", ""], ["Jiang", "Helen", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1608.02893", "submitter": "David Cox", "authors": "David Cox", "title": "Syntactically Informed Text Compression with Recurrent Neural Networks", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-contained system for constructing natural language models\nfor use in text compression. Our system improves upon previous neural network\nbased models by utilizing recent advances in syntactic parsing -- Google's\nSyntaxNet -- to augment character-level recurrent neural networks. RNNs have\nproven exceptional in modeling sequence data such as text, as their\narchitecture allows for modeling of long-term contextual information.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 01:30:45 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 20:55:41 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Cox", "David", ""]]}, {"id": "1608.02904", "submitter": "Jeniya Tabassum", "authors": "Jeniya Tabassum, Alan Ritter, Wei Xu", "title": "TweeTime: A Minimally Supervised Method for Recognizing and Normalizing\n  Time Expressions in Twitter", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe TweeTIME, a temporal tagger for recognizing and normalizing time\nexpressions in Twitter. Most previous work in social media analysis has to rely\non temporal resolvers that are designed for well-edited text, and therefore\nsuffer from the reduced performance due to domain mismatch. We present a\nminimally supervised method that learns from large quantities of unlabeled data\nand requires no hand-engineered rules or hand-annotated training corpora.\nTweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date\nexpressions, outperforming a broad range of state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 18:29:04 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 03:54:13 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 19:24:23 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 09:34:25 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Tabassum", "Jeniya", ""], ["Ritter", "Alan", ""], ["Xu", "Wei", ""]]}, {"id": "1608.02926", "submitter": "Michael Tessler", "authors": "Michael Henry Tessler and Noah D. Goodman", "title": "The Language of Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language provides simple ways of communicating generalizable knowledge to\neach other (e.g., \"Birds fly\", \"John hikes\", \"Fire makes smoke\"). Though found\nin every language and emerging early in development, the language of\ngeneralization is philosophically puzzling and has resisted precise\nformalization. Here, we propose the first formal account of generalizations\nconveyed with language that makes quantitative predictions about human\nunderstanding. We test our model in three diverse domains: generalizations\nabout categories (generic language), events (habitual language), and causes\n(causal language). The model explains the gradience in human endorsement\nthrough the interplay between a simple truth-conditional semantic theory and\ndiverse beliefs about properties, formalized in a probabilistic model of\nlanguage understanding. This work opens the door to understanding precisely how\nabstract knowledge is learned from language.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 19:40:21 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 12:50:16 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 00:06:07 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 22:40:51 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Tessler", "Michael Henry", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1608.02927", "submitter": "Baskaran Sankaran", "authors": "Baskaran Sankaran, Haitao Mi, Yaser Al-Onaizan, Abe Ittycheriah", "title": "Temporal Attention Model for Neural Machine Translation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based Neural Machine Translation (NMT) models suffer from attention\ndeficiency issues as has been observed in recent research. We propose a novel\nmechanism to address some of these limitations and improve the NMT attention.\nSpecifically, our approach memorizes the alignments temporally (within each\nsentence) and modulates the attention with the accumulated temporal memory, as\nthe decoder generates the candidate translation. We compare our approach\nagainst the baseline NMT model and two other related approaches that address\nthis issue either explicitly or implicitly. Large-scale experiments on two\nlanguage pairs show that our approach achieves better and robust gains over the\nbaseline and related NMT approaches. Our model further outperforms strong SMT\nbaselines in some settings even without using ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 19:42:14 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Sankaran", "Baskaran", ""], ["Mi", "Haitao", ""], ["Al-Onaizan", "Yaser", ""], ["Ittycheriah", "Abe", ""]]}, {"id": "1608.02996", "submitter": "Antonio Valerio Miceli Barone", "authors": "Antonio Valerio Miceli Barone", "title": "Towards cross-lingual distributed representations without parallel text\n  trained with adversarial autoencoders", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to learning vector representations of text that are\ncompatible between different languages usually require some amount of parallel\ntext, aligned at word, sentence or at least document level. We hypothesize\nhowever, that different natural languages share enough semantic structure that\nit should be possible, in principle, to learn compatible vector representations\njust by analyzing the monolingual distribution of words.\n  In order to evaluate this hypothesis, we propose a scheme to map word vectors\ntrained on a source language to vectors semantically compatible with word\nvectors trained on a target language using an adversarial autoencoder.\n  We present preliminary qualitative results and discuss possible future\ndevelopments of this technique, such as applications to cross-lingual sentence\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:24:16 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""]]}, {"id": "1608.03000", "submitter": "Nicholas Locascio", "authors": "Nicholas Locascio, Karthik Narasimhan, Eduardo DeLeon, Nate Kushman,\n  Regina Barzilay", "title": "Neural Generation of Regular Expressions from Natural Language with\n  Minimal Domain Knowledge", "comments": "to be published in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 23:05:03 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Locascio", "Nicholas", ""], ["Narasimhan", "Karthik", ""], ["DeLeon", "Eduardo", ""], ["Kushman", "Nate", ""], ["Barzilay", "Regina", ""]]}, {"id": "1608.03030", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and George Mulcaire and Shobhit Hathi and Mari Ostendorf\n  and Noah A. Smith", "title": "Hierarchical Character-Word Models for Language Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media messages' brevity and unconventional spelling pose a challenge\nto language identification. We introduce a hierarchical model that learns\ncharacter and contextualized word-level representations for language\nidentification. Our method performs well against strong base- lines, and can\nalso reveal code-switching.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 03:05:26 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Jaech", "Aaron", ""], ["Mulcaire", "George", ""], ["Hathi", "Shobhit", ""], ["Ostendorf", "Mari", ""], ["Smith", "Noah A.", ""]]}, {"id": "1608.03065", "submitter": "C. Maria Keet", "authors": "C. Maria Keet", "title": "An assessment of orthographic similarity measures for several African\n  languages", "comments": "9 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Interfaces and tools such as spellcheckers and Web search in\none's own language are known to be useful in ICT-mediated communication. Most\nlanguages in Southern Africa are under-resourced, however. Therefore, it would\nbe very useful if both the generic and the few language-specific NLP tools\ncould be reused or easily adapted across languages. This depends on the notion,\nand extent, of similarity between the languages. We assess this from the angle\nof orthography and corpora. Twelve versions of the Universal Declaration of\nHuman Rights (UDHR) are examined, showing clusters of languages, and which are\nthus more or less amenable to cross-language adaptation of NLP tools, which do\nnot match with Guthrie zones. To examine the generalisability of these results,\nwe zoom in on isiZulu both quantitatively and qualitatively with four other\ncorpora and texts in different genres. The results show that the UDHR is a\ntypical text document orthographically. The results also provide insight into\nusability of typical measures such as lexical diversity and genre, and that the\nsame statistic may mean different things in different documents. While NLTK for\nPython could be used for basic analyses of text, it, and similar NLP tools,\nwill need considerable customization.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 07:45:46 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Keet", "C. Maria", ""]]}, {"id": "1608.03192", "submitter": "Tim Weninger PhD", "authors": "Salvador Agui\\~naga and Rodrigo Palacios and David Chiang and Tim\n  Weninger", "title": "Growing Graphs with Hyperedge Replacement Graph Grammars", "comments": "18 pages, 19 figures, accepted to CIKM 2016 in Indianapolis, IN", "journal-ref": null, "doi": "10.1145/2983323.2983826", "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discovering the underlying structures present in large real world graphs is a\nfundamental scientific problem. In this paper we show that a graph's clique\ntree can be used to extract a hyperedge replacement grammar. If we store an\nordering from the extraction process, the extracted graph grammar is guaranteed\nto generate an isomorphic copy of the original graph. Or, a stochastic\napplication of the graph grammar rules can be used to quickly create random\ngraphs. In experiments on large real world networks, we show that random\ngraphs, generated from extracted graph grammars, exhibit a wide range of\nproperties that are very similar to the original graphs. In addition to graph\nproperties like degree or eigenvector centrality, what a graph \"looks like\"\nultimately depends on small details in local graph substructures that are\ndifficult to define at a global level. We show that our generative graph model\nis able to preserve these local substructures when generating new graphs and\nperforms well on new and difficult tests of model robustness.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 14:46:15 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Agui\u00f1aga", "Salvador", ""], ["Palacios", "Rodrigo", ""], ["Chiang", "David", ""], ["Weninger", "Tim", ""]]}, {"id": "1608.03448", "submitter": "Frank Rudzicz", "authors": "Stefania Raimondo and Frank Rudzicz", "title": "Sex, drugs, and violence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting inappropriate content can be a difficult NLP task,\nrequiring understanding context and innuendo, not just identifying specific\nkeywords. Due to the large quantity of online user-generated content, automatic\ndetection is becoming increasingly necessary. We take a largely unsupervised\napproach using a large corpus of narratives from a community-based\nself-publishing website and a small segment of crowd-sourced annotations. We\nexplore topic modelling using latent Dirichlet allocation (and a variation),\nand use these to regress appropriateness ratings, effectively automating rating\nfor suitability. The results suggest that certain topics inferred may be useful\nin detecting latent inappropriateness -- yielding recall up to 96% and low\nregression errors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 13:10:02 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Raimondo", "Stefania", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1608.03542", "submitter": "Daniel Hewlett", "authors": "Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin,\n  Andrew Fandrianto, Jay Han, Matthew Kelcey, David Berthelot", "title": "WikiReading: A Novel Large-scale Language Understanding Task over\n  Wikipedia", "comments": null, "journal-ref": "Proceedings of the 54th Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), 2016, pp. 1535-1545", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present WikiReading, a large-scale natural language understanding task and\npublicly-available dataset with 18 million instances. The task is to predict\ntextual values from the structured knowledge base Wikidata by reading the text\nof the corresponding Wikipedia articles. The task contains a rich variety of\nchallenging classification and extraction sub-tasks, making it well-suited for\nend-to-end models such as deep neural networks (DNNs). We compare various\nstate-of-the-art DNN-based architectures for document classification,\ninformation extraction, and question answering. We find that models supporting\na rich answer space, such as word or character sequences, perform best. Our\nbest-performing model, a word-level sequence to sequence model with a mechanism\nto copy out-of-vocabulary words, obtains an accuracy of 71.8%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 17:34:12 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 19:58:44 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Hewlett", "Daniel", ""], ["Lacoste", "Alexandre", ""], ["Jones", "Llion", ""], ["Polosukhin", "Illia", ""], ["Fandrianto", "Andrew", ""], ["Han", "Jay", ""], ["Kelcey", "Matthew", ""], ["Berthelot", "David", ""]]}, {"id": "1608.03587", "submitter": "Alexander Koplenig", "authors": "Alexander Koplenig, Peter Meyer, Sascha Wolfer, and Carolin\n  Mueller-Spitzer", "title": "The statistical trade-off between word order and word structure -\n  large-scale evidence for the principle of least effort", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0173614", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages employ different strategies to transmit structural and grammatical\ninformation. While, for example, grammatical dependency relationships in\nsentences are mainly conveyed by the ordering of the words for languages like\nMandarin Chinese, or Vietnamese, the word ordering is much less restricted for\nlanguages such as Inupiatun or Quechua, as those languages (also) use the\ninternal structure of words (e.g. inflectional morphology) to mark grammatical\nrelationships in a sentence. Based on a quantitative analysis of more than\n1,500 unique translations of different books of the Bible in more than 1,100\ndifferent languages that are spoken as a native language by approximately 6\nbillion people (more than 80% of the world population), we present large-scale\nevidence for a statistical trade-off between the amount of information conveyed\nby the ordering of words and the amount of information conveyed by internal\nword structure: languages that rely more strongly on word order information\ntend to rely less on word structure information and vice versa. In addition, we\nfind that - despite differences in the way information is expressed - there is\nalso evidence for a trade-off between different books of the biblical canon\nthat recurs with little variation across languages: the more informative the\nword order of the book, the less informative its word structure and vice versa.\nWe argue that this might suggest that, on the one hand, languages encode\ninformation in very different (but efficient) ways. On the other hand,\ncontent-related and stylistic features are statistically encoded in very\nsimilar ways.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 09:01:04 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 11:46:30 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Koplenig", "Alexander", ""], ["Meyer", "Peter", ""], ["Wolfer", "Sascha", ""], ["Mueller-Spitzer", "Carolin", ""]]}, {"id": "1608.03764", "submitter": "Michael Spranger", "authors": "Michael Spranger and Sucheendra K. Palaniappan and Samik Ghosh", "title": "Extracting Biological Pathway Models From NLP Event Representations", "comments": null, "journal-ref": "Proceedings of the 2015 Workshop on Biomedical Natural Language\n  Processing (BioNLP 2015), pages 42-51. Association for Computational\n  Linguistics", "doi": null, "report-no": null, "categories": "cs.CL q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an an open-source software system for the automatic\nconversion of NLP event representations to system biology structured data\ninterchange formats such as SBML and BioPAX. It is part of a larger effort to\nmake results of the NLP community available for system biology pathway\nmodelers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 11:58:00 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Spranger", "Michael", ""], ["Palaniappan", "Sucheendra K.", ""], ["Ghosh", "Samik", ""]]}, {"id": "1608.03767", "submitter": "Michael Spranger", "authors": "Michael Spranger and Sucheendra K. Palaniappan and Samik Ghosh", "title": "Measuring the State of the Art of Automated Pathway Curation Using Graph\n  Algorithms - A Case Study of the mTOR Pathway", "comments": null, "journal-ref": "Proceedings of the 15th Workshop on Biomedical Natural Language\n  Processing, Berlin, Germany, 2016, pages 119-127. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the difference between human pathway curation and\ncurrent NLP systems. We propose graph analysis methods for quantifying the gap\nbetween human curated pathway maps and the output of state-of-the-art automatic\nNLP systems. Evaluation is performed on the popular mTOR pathway. Based on\nanalyzing where current systems perform well and where they fail, we identify\npossible avenues for progress.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 12:10:24 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Spranger", "Michael", ""], ["Palaniappan", "Sucheendra K.", ""], ["Ghosh", "Samik", ""]]}, {"id": "1608.03785", "submitter": "Martha Lewis", "authors": "Yaared Al-Mehairi, Bob Coecke, Martha Lewis", "title": "Compositional Distributional Cognition", "comments": "To appear in Quantum Interaction 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of\n[32] within the categorical compositional semantics (CatCo) of [13], forming a\nmodel of categorical compositional cognition (CatCog). This resolves intrinsic\nproblems with ICS such as the fact that representations inhabit an unbounded\nspace and that sentences with differing tree structures cannot be directly\ncompared. We do so in a way that makes the most of the grammatical structure\navailable, in contrast to strategies like circular convolution. Using the CatCo\nmodel also allows us to make use of tools developed for CatCo such as the\nrepresentation of ambiguity and logical reasoning via density matrices,\nstructural meanings for words such as relative pronouns, and addressing over-\nand under-extension, all of which are present in cognitive processes. Moreover\nthe CatCog framework is sufficiently flexible to allow for entirely different\nrepresentations of meaning, such as conceptual spaces. Interestingly, since the\nCatCo model was largely inspired by categorical quantum mechanics, so is\nCatCog.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:13:10 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Al-Mehairi", "Yaared", ""], ["Coecke", "Bob", ""], ["Lewis", "Martha", ""]]}, {"id": "1608.03803", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov, Erik Velldal, Lilja {\\O}vrelid", "title": "Redefining part-of-speech classes with distributional semantic models", "comments": null, "journal-ref": "Proceedings of The 20th SIGNLL Conference on Computational Natural\n  Language Learning, 2016, pp. 115-125", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies how word embeddings trained on the British National Corpus\ninteract with part of speech boundaries. Our work targets the Universal PoS tag\nset, which is currently actively being used for annotation of a range of\nlanguages. We experiment with training classifiers for predicting PoS tags for\nwords based on their embeddings. The results show that the information about\nPoS affiliation contained in the distributional vectors allows us to discover\ngroups of words with distributional patterns that differ from other words of\nthe same part of speech.\n  This data often reveals hidden inconsistencies of the annotation process or\nguidelines. At the same time, it supports the notion of `soft' or `graded' part\nof speech affiliations. Finally, we show that information about PoS is\ndistributed among dozens of vector components, not limited to only one or two\nfeatures.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 14:15:13 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Velldal", "Erik", ""], ["\u00d8vrelid", "Lilja", ""]]}, {"id": "1608.03902", "submitter": "Muhammad Imran", "authors": "Dat Tien Nguyen, Kamela Ali Al Mannai, Shafiq Joty, Hassan Sajjad,\n  Muhammad Imran, Prasenjit Mitra", "title": "Rapid Classification of Crisis-Related Data on Social Networks using\n  Convolutional Neural Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of social media, in particular microblogging platforms such as\nTwitter, as a conduit for actionable and tactical information during disasters\nis increasingly acknowledged. However, time-critical analysis of big crisis\ndata on social media streams brings challenges to machine learning techniques,\nespecially the ones that use supervised learning. The Scarcity of labeled data,\nparticularly in the early hours of a crisis, delays the machine learning\nprocess. The current state-of-the-art classification methods require a\nsignificant amount of labeled data specific to a particular event for training\nplus a lot of feature engineering to achieve best results. In this work, we\nintroduce neural network based classification methods for binary and\nmulti-class tweet classification task. We show that neural network based models\ndo not require any feature engineering and perform better than state-of-the-art\nmethods. In the early hours of a disaster when no labeled data is available,\nour proposed method makes the best use of the out-of-event data and achieves\ngood results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 20:19:16 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Mannai", "Kamela Ali Al", ""], ["Joty", "Shafiq", ""], ["Sajjad", "Hassan", ""], ["Imran", "Muhammad", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1608.03938", "submitter": "Christopher Thompson", "authors": "Christopher Thompson, Josh Introne, and Clint Young", "title": "Determining Health Utilities through Data Mining of Social Media", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Health utilities' measure patient preferences for perfect health compared to\nspecific unhealthy states, such as asthma, a fractured hip, or colon cancer.\nWhen integrated over time, these estimations are called quality adjusted life\nyears (QALYs). Until now, characterizing health utilities (HUs) required\ndetailed patient interviews or written surveys. While reliable and specific,\nthis data remained costly due to efforts to locate, enlist and coordinate\nparticipants. Thus the scope, context and temporality of diseases examined has\nremained limited.\n  Now that more than a billion people use social media, we propose a novel\nstrategy: use natural language processing to analyze public online\nconversations for signals of the severity of medical conditions and correlate\nthese to known HUs using machine learning. In this work, we filter a dataset\nthat originally contained 2 billion tweets for relevant content on 60 diseases.\nUsing this data, our algorithm successfully distinguished mild from severe\ndiseases, which had previously been categorized only by traditional techniques.\nThis represents progress towards two related applications: first, predicting\nHUs where such information is nonexistent; and second, (where rich HU data\nalready exists) estimating temporal or geographic patterns of disease severity\nthrough data mining.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 04:02:38 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Thompson", "Christopher", ""], ["Introne", "Josh", ""], ["Young", "Clint", ""]]}, {"id": "1608.03995", "submitter": "Chandler May", "authors": "Chandler May, Ryan Cotterell, Benjamin Van Durme", "title": "An Analysis of Lemmatization on Topic Models of Morphologically Rich\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are typically represented by top-$m$ word lists for human\ninterpretation. The corpus is often pre-processed with lemmatization (or\nstemming) so that those representations are not undermined by a proliferation\nof words with similar meanings, but there is little public work on the effects\nof that pre-processing. Recent work studied the effect of stemming on topic\nmodels of English texts and found no supporting evidence for the practice. We\nstudy the effect of lemmatization on topic models of Russian Wikipedia\narticles, finding in one configuration that it significantly improves\ninterpretability according to a word intrusion metric. We conclude that\nlemmatization may benefit topic models on morphologically rich languages, but\nthat further investigation is needed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 15:54:10 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 16:18:47 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["May", "Chandler", ""], ["Cotterell", "Ryan", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1608.04020", "submitter": "Stepan Kuznetsov", "authors": "Max Kanovich, Stepan Kuznetsov, Andre Scedrov", "title": "Undecidability of the Lambek calculus with subexponential and bracket\n  modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lambek calculus is a well-known logical formalism for modelling natural\nlanguage syntax. The original calculus covered a substantial number of\nintricate natural language phenomena, but only those restricted to the\ncontext-free setting. In order to address more subtle linguistic issues, the\nLambek calculus has been extended in various ways. In particular, Morrill and\nValentin (2015) introduce an extension with so-called exponential and bracket\nmodalities. Their extension is based on a non-standard contraction rule for the\nexponential that interacts with the bracket structure in an intricate way. The\nstandard contraction rule is not admissible in this calculus. In this paper we\nprove undecidability of the derivability problem in their calculus. We also\ninvestigate restricted decidable fragments considered by Morrill and Valentin\nand we show that these fragments belong to the NP class.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 19:30:12 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 22:06:04 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Kanovich", "Max", ""], ["Kuznetsov", "Stepan", ""], ["Scedrov", "Andre", ""]]}, {"id": "1608.04089", "submitter": "Kerry Zhang", "authors": "Kerry Zhang, Jussi Karlgren, Cheng Zhang, Jens Lagergren", "title": "Viewpoint and Topic Modeling of Current Events", "comments": "16 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple sides to every story, and while statistical topic models\nhave been highly successful at topically summarizing the stories in corpora of\ntext documents, they do not explicitly address the issue of learning the\ndifferent sides, the viewpoints, expressed in the documents. In this paper, we\nshow how these viewpoints can be learned completely unsupervised and\nrepresented in a human interpretable form. We use a novel approach of applying\nCorrLDA2 for this purpose, which learns topic-viewpoint relations that can be\nused to form groups of topics, where each group represents a viewpoint. A\ncorpus of documents about the Israeli-Palestinian conflict is then used to\ndemonstrate how a Palestinian and an Israeli viewpoint can be learned. By\nleveraging the magnitudes and signs of the feature weights of a linear SVM, we\nintroduce a principled method to evaluate associations between topics and\nviewpoints. With this, we demonstrate, both quantitatively and qualitatively,\nthat the learned topic groups are contextually coherent, and form consistently\ncorrect topic-viewpoint associations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 11:36:52 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Zhang", "Kerry", ""], ["Karlgren", "Jussi", ""], ["Zhang", "Cheng", ""], ["Lagergren", "Jens", ""]]}, {"id": "1608.04147", "submitter": "Georgios Spithourakis", "authors": "Georgios P. Spithourakis, Isabelle Augenstein, Sebastian Riedel", "title": "Numerically Grounded Language Models for Semantic Error Correction", "comments": "accepted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic error detection and correction is an important task for applications\nsuch as fact checking, speech-to-text or grammatical error correction. Current\napproaches generally focus on relatively shallow semantics and do not account\nfor numeric quantities. Our approach uses language models grounded in numbers\nwithin the text. Such groundings are easily achieved for recurrent neural\nlanguage model architectures, which can be further conditioned on incomplete\nbackground knowledge bases. Our evaluation on clinical reports shows that\nnumerical grounding improves perplexity by 33% and F1 for semantic error\ncorrection by 5 points when compared to ungrounded approaches. Conditioning on\na knowledge base yields further improvements.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 22:34:22 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Spithourakis", "Georgios P.", ""], ["Augenstein", "Isabelle", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1608.04207", "submitter": "Einat Kermany", "authors": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lot of research interest in encoding variable length sentences\ninto fixed length vectors, in a way that preserves the sentence meanings. Two\ncommon methods include representations based on averaging word vectors, and\nrepresentations based on the hidden states of recurrent neural networks such as\nLSTMs. The sentence vectors are used as features for subsequent machine\nlearning tasks or for pre-training in the context of deep learning. However,\nnot much is known about the properties that are encoded in these sentence\nrepresentations and about the language information they capture. We propose a\nframework that facilitates better understanding of the encoded representations.\nWe define prediction tasks around isolated aspects of sentence structure\n(namely sentence length, word content, and word order), and score\nrepresentations by the ability to train a classifier to solve each prediction\ntask when using the representation as input. We demonstrate the potential\ncontribution of the approach by analyzing different sentence representation\nmechanisms. The analysis sheds light on the relative strengths of different\nsentence embedding methods with respect to these low level prediction tasks,\nand on the effect of the encoded vector's dimensionality on the resulting\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 08:51:38 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 13:22:13 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 06:58:50 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Adi", "Yossi", ""], ["Kermany", "Einat", ""], ["Belinkov", "Yonatan", ""], ["Lavi", "Ofer", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1608.04434", "submitter": "Emre Erturk", "authors": "Emre Erturk, Hong Shi", "title": "Natural Language Processing using Hadoop and KOSHIK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing, as a data analytics related technology, is used\nwidely in many research areas such as artificial intelligence, human language\nprocessing, and translation. At present, due to explosive growth of data, there\nare many challenges for natural language processing. Hadoop is one of the\nplatforms that can process the large amount of data required for natural\nlanguage processing. KOSHIK is one of the natural language processing\narchitectures, and utilizes Hadoop and contains language processing components\nsuch as Stanford CoreNLP and OpenNLP. This study describes how to build a\nKOSHIK platform with the relevant tools, and provides the steps to analyze wiki\ndata. Finally, it evaluates and discusses the advantages and disadvantages of\nthe KOSHIK architecture, and gives recommendations on improving the processing\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 23:09:21 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Erturk", "Emre", ""], ["Shi", "Hong", ""]]}, {"id": "1608.04465", "submitter": "Trevor Cohn", "authors": "Ehsan Shareghi, Matthias Petri, Gholamreza Haffari and Trevor Cohn", "title": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed\n  Suffix Trees", "comments": "14 pages in Transactions of the Association for Computational\n  Linguistics (TACL) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient methods for storing and querying are critical for scaling\nhigh-order n-gram language models to large corpora. We propose a language model\nbased on compressed suffix trees, a representation that is highly compact and\ncan be easily held in memory, while supporting queries needed in computing\nlanguage model probabilities on-the-fly. We present several optimisations which\nimprove query runtimes up to 2500x, despite only incurring a modest increase in\nconstruction time and memory usage. For large corpora and high Markov orders,\nour method is highly competitive with the state-of-the-art KenLM package. It\nimposes much lower memory requirements, often by orders of magnitude, and has\nruntimes that are either similar (for training) or comparable (for querying).\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 02:33:21 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Shareghi", "Ehsan", ""], ["Petri", "Matthias", ""], ["Haffari", "Gholamreza", ""], ["Cohn", "Trevor", ""]]}, {"id": "1608.04485", "submitter": "Douglas Bagnall", "authors": "Douglas Bagnall", "title": "Authorship clustering using multi-headed recurrent neural networks", "comments": "14 pages, 5 figures; notebook for PAN@CLEF 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurrent neural network that has been trained to separately model the\nlanguage of several documents by unknown authors is used to measure similarity\nbetween the documents. It is able to find clues of common authorship even when\nthe documents are very short and about disparate topics. While it is easy to\nmake statistically significant predictions regarding authorship, it is\ndifficult to group documents into definite clusters with high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 05:04:46 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Bagnall", "Douglas", ""]]}, {"id": "1608.04631", "submitter": "Marcello Federico", "authors": "Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, Marcello Federico", "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 1-5, 2016, Austin, Texas, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Within the field of Statistical Machine Translation (SMT), the neural\napproach (NMT) has recently emerged as the first technology able to challenge\nthe long-standing dominance of phrase-based approaches (PBMT). In particular,\nat the IWSLT 2015 evaluation campaign, NMT outperformed well established\nstate-of-the-art PBMT systems on English-German, a language pair known to be\nparticularly hard because of morphology and syntactic differences. To\nunderstand in what respects NMT provides better translation quality than PBMT,\nwe perform a detailed analysis of neural versus phrase-based SMT outputs,\nleveraging high quality post-edits performed by professional translators on the\nIWSLT data. For the first time, our analysis provides useful insights on what\nlinguistic phenomena are best modeled by neural models -- such as the\nreordering of verbs -- while pointing out other aspects that remain to be\nimproved.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:04:18 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 09:20:08 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Bentivogli", "Luisa", ""], ["Bisazza", "Arianna", ""], ["Cettolo", "Mauro", ""], ["Federico", "Marcello", ""]]}, {"id": "1608.04670", "submitter": "Ajinkya More", "authors": "Ajinkya More", "title": "Attribute Extraction from Product Titles in eCommerce", "comments": "Accepted at the Workshop on Enterprise Intelligence, KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a named entity extraction system for detecting attributes\nin product titles of eCommerce retailers like Walmart. The absence of syntactic\nstructure in such short pieces of text makes extracting attribute values a\nchallenging problem. We find that combining sequence labeling algorithms such\nas Conditional Random Fields and Structured Perceptron with a curated\nnormalization scheme produces an effective system for the task of extracting\nproduct attribute values from titles. To keep the discussion concrete, we will\nillustrate the mechanics of the system from the point of view of a particular\nattribute - brand. We also discuss the importance of an attribute extraction\nsystem in the context of retail websites with large product catalogs, compare\nour approach to other potential approaches to this problem and end the paper\nwith a discussion of the performance of our system for extracting attributes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 03:34:13 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["More", "Ajinkya", ""]]}, {"id": "1608.04738", "submitter": "Shenjian Zhao", "authors": "Shenjian Zhao, Zhihua Zhang", "title": "An Efficient Character-Level Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation aims at building a single large neural network\nthat can be trained to maximize translation performance. The encoder-decoder\narchitecture with an attention mechanism achieves a translation performance\ncomparable to the existing state-of-the-art phrase-based systems on the task of\nEnglish-to-French translation. However, the use of large vocabulary becomes the\nbottleneck in both training and improving the performance. In this paper, we\npropose an efficient architecture to train a deep character-level neural\nmachine translation by introducing a decimator and an interpolator. The\ndecimator is used to sample the source sequence before encoding while the\ninterpolator is used to resample after decoding. Such a deep model has two\nmajor advantages. It avoids the large vocabulary issue radically; at the same\ntime, it is much faster and more memory-efficient in training than conventional\ncharacter-based models. More interestingly, our model is able to translate the\nmisspelled word like human beings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 07:44:02 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 06:49:32 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhao", "Shenjian", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1608.04767", "submitter": "Steven Neale", "authors": "Steven Neale, Valeria de Paiva, Arantxa Otegi, Alexandre Rademaker", "title": "Proceedings of the LexSem+Logics Workshop 2016", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical semantics continues to play an important role in driving research\ndirections in NLP, with the recognition and understanding of context becoming\nincreasingly important in delivering successful outcomes in NLP tasks. Besides\ntraditional processing areas such as word sense and named entity\ndisambiguation, the creation and maintenance of dictionaries, annotated corpora\nand resources have become cornerstones of lexical semantics research and\nproduced a wealth of contextual information that NLP processes can exploit. New\nefforts both to link and construct from scratch such information - as Linked\nOpen Data or by way of formal tools coming from logic, ontologies and automated\nreasoning - have increased the interoperability and accessibility of resources\nfor lexical and computational semantics, even in those languages for which they\nhave previously been limited.\n  LexSem+Logics 2016 combines the 1st Workshop on Lexical Semantics for\nLesser-Resources Languages and the 3rd Workshop on Logics and Ontologies. The\naccepted papers in our program covered topics across these two areas,\nincluding: the encoding of plurals in Wordnets, the creation of a thesaurus\nfrom multiple sources based on semantic similarity metrics, and the use of\ncross-lingual treebanks and annotations for universal part-of-speech tagging.\nWe also welcomed talks from two distinguished speakers: on Portuguese lexical\nknowledge bases (different approaches, results and their application in NLP\ntasks) and on new strategies for open information extraction (the capture of\nverb-based propositions from massive text corpora).\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 16:23:55 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Neale", "Steven", ""], ["de Paiva", "Valeria", ""], ["Otegi", "Arantxa", ""], ["Rademaker", "Alexandre", ""]]}, {"id": "1608.04808", "submitter": "Hao Fang", "authors": "Hao Fang, Hao Cheng, Mari Ostendorf", "title": "Learning Latent Local Conversation Modes for Predicting Community\n  Endorsement in Online Discussions", "comments": "10 pages, 7 figures", "journal-ref": "SocialNLP Workshop at Conf. Empirical Methods Natural Language\n  Process. (EMNLP), 2016", "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social media platforms offer a mechanism for readers to react to\ncomments, both positively and negatively, which in aggregate can be thought of\nas community endorsement. This paper addresses the problem of predicting\ncommunity endorsement in online discussions, leveraging both the participant\nresponse structure and the text of the comment. The different types of features\nare integrated in a neural network that uses a novel architecture to learn\nlatent modes of discussion structure that perform as well as deep neural\nnetworks but are more interpretable. In addition, the latent modes can be used\nto weight text features thereby improving prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 23:37:43 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 09:46:24 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Fang", "Hao", ""], ["Cheng", "Hao", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1608.04868", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi and George Fazekas and Brian McFee and Kyunghyun Cho and\n  Mark Sandler", "title": "Towards Music Captioning: Generating Music Playlist Descriptions", "comments": "2 pages, ISMIR 2016 Late-breaking/session extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Descriptions are often provided along with recommendations to help users'\ndiscovery. Recommending automatically generated music playlists (e.g.\npersonalised playlists) introduces the problem of generating descriptions. In\nthis paper, we propose a method for generating music playlist descriptions,\nwhich is called as music captioning. In the proposed method, audio content\nanalysis and natural language processing are adopted to utilise the information\nof each track.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 06:24:46 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 05:23:51 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["McFee", "Brian", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1608.04917", "submitter": "Igor Mozeti\\v{c}", "authors": "Darko Cherepnalkoski, Andreas Karpf, Igor Mozetic, Miha Grcar", "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call\n  Votes and Twitter Activities", "comments": null, "journal-ref": "PLoS ONE 11(11): e0166586, 2016", "doi": "10.1371/journal.pone.0166586", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the cohesion within and the coalitions between political groups in\nthe Eighth European Parliament (2014--2019) by analyzing two entirely different\naspects of the behavior of the Members of the European Parliament (MEPs) in the\npolicy-making processes. On one hand, we analyze their co-voting patterns and,\non the other, their retweeting behavior. We make use of two diverse datasets in\nthe analysis. The first one is the roll-call vote dataset, where cohesion is\nregarded as the tendency to co-vote within a group, and a coalition is formed\nwhen the members of several groups exhibit a high degree of co-voting agreement\non a subject. The second dataset comes from Twitter; it captures the retweeting\n(i.e., endorsing) behavior of the MEPs and implies cohesion (retweets within\nthe same group) and coalitions (retweets between groups) from a completely\ndifferent perspective.\n  We employ two different methodologies to analyze the cohesion and coalitions.\nThe first one is based on Krippendorff's Alpha reliability, used to measure the\nagreement between raters in data-analysis scenarios, and the second one is\nbased on Exponential Random Graph Models, often used in social-network\nanalysis. We give general insights into the cohesion of political groups in the\nEuropean Parliament, explore whether coalitions are formed in the same way for\ndifferent policy areas, and examine to what degree the retweeting behavior of\nMEPs corresponds to their co-voting patterns. A novel and interesting aspect of\nour work is the relationship between the co-voting and retweeting patterns.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 10:10:14 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 09:47:42 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cherepnalkoski", "Darko", ""], ["Karpf", "Andreas", ""], ["Mozetic", "Igor", ""], ["Grcar", "Miha", ""]]}, {"id": "1608.04983", "submitter": "Joon-Hyuk Chang", "authors": "Jeehye Lee, Myungin Lee, and Joon-Hyuk Chang", "title": "Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models\n  for Reverberant Speech Recognition", "comments": "9 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant speech recognition is a challenge, particularly due to the corruption\nof speech signals by reverberation caused by large distances between the\nspeaker and microphone. In order to cope with a wide range of reverberations in\nreal-world situations, we present novel approaches for acoustic modeling\nincluding an ensemble of deep neural networks (DNNs) and an ensemble of jointly\ntrained DNNs. First, multiple DNNs are established, each of which corresponds\nto a different reverberation time 60 (RT60) in a setup step. Also, each model\nin the ensemble of DNN acoustic models is further jointly trained, including\nboth feature mapping and acoustic modeling, where the feature mapping is\ndesigned for the dereverberation as a front-end. In a testing phase, the two\nmost likely DNNs are chosen from the DNN ensemble using maximum a posteriori\n(MAP) probabilities, computed in an online fashion by using maximum likelihood\n(ML)-based blind RT60 estimation and then the posterior probability outputs\nfrom two DNNs are combined using the ML-based weights as a simple average.\nExtensive experiments demonstrate that the proposed approach leads to\nsubstantial improvements in speech recognition accuracy over the conventional\nDNN baseline systems under diverse reverberant conditions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 14:43:17 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Lee", "Jeehye", ""], ["Lee", "Myungin", ""], ["Chang", "Joon-Hyuk", ""]]}, {"id": "1608.05014", "submitter": "Vered Shwartz", "authors": "Vered Shwartz and Ido Dagan", "title": "Path-based vs. Distributional Information in Recognizing Lexical\n  Semantic Relations", "comments": "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the\n  Lexicon (CogALex-V), in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing various semantic relations between terms is beneficial for many\nNLP tasks. While path-based and distributional information sources are\nconsidered complementary for this task, the superior results the latter showed\nrecently suggested that the former's contribution might have become obsolete.\nWe follow the recent success of an integrated neural method for hypernymy\ndetection (Shwartz et al., 2016) and extend it to recognize multiple relations.\nThe empirical results show that this method is effective in the multiclass\nsetting as well. We further show that the path-based information source always\ncontributes to the classification, and analyze the cases in which it mostly\ncomplements the distributional information.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 16:27:49 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 09:46:36 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 15:27:21 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2016 08:57:39 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Shwartz", "Vered", ""], ["Dagan", "Ido", ""]]}, {"id": "1608.05129", "submitter": "Liang Wu", "authors": "Liang Wu, Fred Morstatter, Huan Liu", "title": "SlangSD: Building and Using a Sentiment Dictionary of Slang Words for\n  Short-Text Sentiment Classification", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment in social media is increasingly considered as an important resource\nfor customer segmentation, market understanding, and tackling other\nsocio-economic issues. However, sentiment in social media is difficult to\nmeasure since user-generated content is usually short and informal. Although\nmany traditional sentiment analysis methods have been proposed, identifying\nslang sentiment words remains untackled. One of the reasons is that slang\nsentiment words are not available in existing dictionaries or sentiment\nlexicons. To this end, we propose to build the first sentiment dictionary of\nslang words to aid sentiment analysis of social media content. It is laborious\nand time-consuming to collect and label the sentiment polarity of a\ncomprehensive list of slang words. We present an approach to leverage web\nresources to construct an extensive Slang Sentiment word Dictionary (SlangSD)\nthat is easy to maintain and extend. SlangSD is publicly available for research\npurposes. We empirically show the advantages of using SlangSD, the newly-built\nslang sentiment word dictionary for sentiment classification, and provide\nexamples demonstrating its ease of use with an existing sentiment system.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 23:32:57 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Wu", "Liang", ""], ["Morstatter", "Fred", ""], ["Liu", "Huan", ""]]}, {"id": "1608.05243", "submitter": "Ana Marasovi\\'c", "authors": "Ana Marasovi\\'c and Anette Frank", "title": "Multilingual Modal Sense Classification using a Convolutional Neural\n  Network", "comments": "Final version, accepted at the 1st Workshop on Representation\n  Learning for NLP, held in conjunction with ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal sense classification (MSC) is a special WSD task that depends on the\nmeaning of the proposition in the modal's scope. We explore a CNN architecture\nfor classifying modal sense in English and German. We show that CNNs are\nsuperior to manually designed feature-based classifiers and a standard NN\nclassifier. We analyze the feature maps learned by the CNN and identify known\nand previously unattested linguistic features. We benchmark the CNN on a\nstandard WSD task, where it compares favorably to models using\nsense-disambiguated target vectors.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 11:41:45 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Marasovi\u0107", "Ana", ""], ["Frank", "Anette", ""]]}, {"id": "1608.05374", "submitter": "Srikanth Ronanki", "authors": "Srikanth Ronanki and Siva Reddy and Bajibabu Bollepalli and Simon King", "title": "DNN-based Speech Synthesis for Indian Languages from ASCII text", "comments": "6 pages, 5 figures -- Accepted in 9th ISCA Speech Synthesis Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-Speech synthesis in Indian languages has a seen lot of progress over\nthe decade partly due to the annual Blizzard challenges. These systems assume\nthe text to be written in Devanagari or Dravidian scripts which are nearly\nphonemic orthography scripts. However, the most common form of computer\ninteraction among Indians is ASCII written transliterated text. Such text is\ngenerally noisy with many variations in spelling for the same word. In this\npaper we evaluate three approaches to synthesize speech from such noisy ASCII\ntext: a naive Uni-Grapheme approach, a Multi-Grapheme approach, and a\nsupervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the\nASCII text to a phonetic script, and then learn a Deep Neural Network to\nsynthesize speech from that. We train and test our models on Blizzard Challenge\ndatasets that were transliterated to ASCII using crowdsourcing. Our experiments\non Hindi, Tamil and Telugu demonstrate that our models generate speech of\ncompetetive quality from ASCII text compared to the speech synthesized from the\nnative scripts. All the accompanying transliterated datasets are released for\npublic access.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 18:58:39 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Ronanki", "Srikanth", ""], ["Reddy", "Siva", ""], ["Bollepalli", "Bajibabu", ""], ["King", "Simon", ""]]}, {"id": "1608.05426", "submitter": "Omer Levy", "authors": "Omer Levy, Anders S{\\o}gaard, Yoav Goldberg", "title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from\n  Sentence Alignments", "comments": "EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cross-lingual word embeddings have been studied extensively in recent\nyears, the qualitative differences between the different algorithms remain\nvague. We observe that whether or not an algorithm uses a particular feature\nset (sentence IDs) accounts for a significant performance gap among these\nalgorithms. This feature set is also used by traditional alignment algorithms,\nsuch as IBM Model-1, which demonstrate similar performance to state-of-the-art\nembedding algorithms on a variety of benchmarks. Overall, we observe that\ndifferent algorithmic approaches for utilizing the sentence ID feature space\nresult in similar performance. This paper draws both empirical and theoretical\nparallels between the embedding and alignment literature, and suggests that\nadding additional sources of information, which go beyond the traditional\nsignal of bilingual sentence-aligned corpora, may substantially improve\ncross-lingual word embeddings, and that future baselines should at least take\nsuch features into account.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 20:27:46 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 20:49:18 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Levy", "Omer", ""], ["S\u00f8gaard", "Anders", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1608.05457", "submitter": "Takeshi Onishi", "authors": "Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel and David\n  McAllester", "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "comments": "To appear at EMNLP 2016. Our dataset is available at\n  tticnlp.github.io/who_did_what", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have constructed a new \"Who-did-What\" dataset of over 200,000\nfill-in-the-gap (cloze) multiple choice reading comprehension problems\nconstructed from the LDC English Gigaword newswire corpus. The WDW dataset has\na variety of novel features. First, in contrast with the CNN and Daily Mail\ndatasets (Hermann et al., 2015) we avoid using article summaries for question\nformation. Instead, each problem is formed from two independent articles --- an\narticle given as the passage to be read and a separate article on the same\nevents used to form the question. Second, we avoid anonymization --- each\nchoice is a person named entity. Third, the problems have been filtered to\nremove a fraction that are easily solved by simple baselines, while remaining\n84% solvable by humans. We report performance benchmarks of standard systems\nand propose the WDW dataset as a challenge task for the community.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 00:13:10 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Onishi", "Takeshi", ""], ["Wang", "Hai", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["McAllester", "David", ""]]}, {"id": "1608.05528", "submitter": "Ivan Vuli\\'c", "authors": "Ivan Vuli\\'c, Roy Schwartz, Ari Rappoport, Roi Reichart, and Anna\n  Korhonen", "title": "Automatic Selection of Context Configurations for Improved\n  Class-Specific Word Representations", "comments": "CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with identifying contexts useful for training word\nrepresentation models for different word classes such as adjectives (A), verbs\n(V), and nouns (N). We introduce a simple yet effective framework for an\nautomatic selection of class-specific context configurations. We construct a\ncontext configuration space based on universal dependency relations between\nwords, and efficiently search this space with an adapted beam search algorithm.\nIn word similarity tasks for each word class, we show that our framework is\nboth effective and efficient. Particularly, it improves the Spearman's rho\ncorrelation with human scores on SimLex-999 over the best previously proposed\nclass-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected\ncontext configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of\nall dependency-based contexts, resulting in a reduced training time. Our\nresults generalise: we show that the configurations our algorithm learns for\none English training setup outperform previously proposed context types in\nanother training setup for English. Moreover, basing the configuration space on\nuniversal dependencies, it is possible to transfer the learned configurations\nto German and Italian. We also demonstrate improved per-class results over\nother context types in these two languages.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 08:30:35 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 09:26:29 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 13:11:53 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Vuli\u0107", "Ivan", ""], ["Schwartz", "Roy", ""], ["Rappoport", "Ari", ""], ["Reichart", "Roi", ""], ["Korhonen", "Anna", ""]]}, {"id": "1608.05554", "submitter": "Qingfu Zhu", "authors": "Qingfu Zhu and Weinan Zhang and Lianqiang Zhou and Ting Liu", "title": "Learning to Start for Sequence to Sequence Architecture", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sequence to sequence architecture is widely used in the response\ngeneration and neural machine translation to model the potential relationship\nbetween two sentences. It typically consists of two parts: an encoder that\nreads from the source sentence and a decoder that generates the target sentence\nword by word according to the encoder's output and the last generated word.\nHowever, it faces to the cold start problem when generating the first word as\nthere is no previous word to refer. Existing work mainly use a special start\nsymbol </s>to generate the first word. An obvious drawback of these work is\nthat there is not a learnable relationship between words and the start symbol.\nFurthermore, it may lead to the error accumulation for decoding when the first\nword is incorrectly generated. In this paper, we proposed a novel approach to\nlearning to generate the first word in the sequence to sequence architecture\nrather than using the start symbol. Experimental results on the task of\nresponse generation of short text conversation show that the proposed approach\noutperforms the state-of-the-art approach in both of the automatic and manual\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 09:48:13 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhu", "Qingfu", ""], ["Zhang", "Weinan", ""], ["Zhou", "Lianqiang", ""], ["Liu", "Ting", ""]]}, {"id": "1608.05604", "submitter": "Frank Keller", "authors": "Michael Hahn, Frank Keller", "title": "Modeling Human Reading with Neural Attention", "comments": "EMNLP 2016, pp. 85-95, Austin, TX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans read text, they fixate some words and skip others. However, there\nhave been few attempts to explain skipping behavior with computational models,\nas most existing work has focused on predicting reading times (e.g.,~using\nsurprisal). In this paper, we propose a novel approach that models both\nskipping and reading, using an unsupervised architecture that combines a neural\nattention with autoencoding, trained on raw text using reinforcement learning.\nOur model explains human reading behavior as a tradeoff between precision of\nlanguage understanding (encoding the input accurately) and economy of attention\n(fixating as few words as possible). We evaluate the model on the Dundee\neye-tracking corpus, showing that it accurately predicts skipping behavior and\nreading times, is competitive with surprisal, and captures known qualitative\nfeatures of human reading.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:03:46 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 09:38:46 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hahn", "Michael", ""], ["Keller", "Frank", ""]]}, {"id": "1608.05605", "submitter": "St\\'ephan Tulkens", "authors": "St\\'ephan Tulkens and Simon \\v{S}uster and Walter Daelemans", "title": "Using Distributed Representations to Disambiguate Biomedical and\n  Clinical Concepts", "comments": "6 pages, 1 figure, presented at the 15th Workshop on Biomedical\n  Natural Language Processing, Berlin 2016", "journal-ref": "Proceedings of the 15th Workshop on Biomedical Natural Language\n  Processing, Berlin, Germany, 2016, pages 77-82. Association for Computational\n  Linguistics", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report a knowledge-based method for Word Sense\nDisambiguation in the domains of biomedical and clinical text. We combine word\nrepresentations created on large corpora with a small number of definitions\nfrom the UMLS to create concept representations, which we then compare to\nrepresentations of the context of ambiguous terms. Using no relational\ninformation, we obtain comparable performance to previous approaches on the\nMSH-WSD dataset, which is a well-known dataset in the biomedical domain.\nAdditionally, our method is fast and easy to set up and extend to other\ndomains. Supplementary materials, including source code, can be found at https:\n//github.com/clips/yarn\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:05:03 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Tulkens", "St\u00e9phan", ""], ["\u0160uster", "Simon", ""], ["Daelemans", "Walter", ""]]}, {"id": "1608.05777", "submitter": "Lei Xu", "authors": "Lei Xu, Ziyun Wang, Ayana, Zhiyuan Liu, Maosong Sun", "title": "Topic Sensitive Neural Headline Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have recently been used in text summarization including\nheadline generation. The model can be trained using a set of document-headline\npairs. However, the model does not explicitly consider topical similarities and\ndifferences of documents. We suggest to categorizing documents into various\ntopics so that documents within the same topic are similar in content and share\nsimilar summarization patterns. Taking advantage of topic information of\ndocuments, we propose topic sensitive neural headline generation model. Our\nmodel can generate more accurate summaries guided by document topics. We test\nour model on LCSTS dataset, and experiments show that our method outperforms\nother baselines on each topic and achieves the state-of-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 03:43:29 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Xu", "Lei", ""], ["Wang", "Ziyun", ""], ["Ayana", "", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "1608.05813", "submitter": "Chee Seng Chan", "authors": "Ying Hua Tan, Chee Seng Chan", "title": "phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning", "comments": "This paper introduces phrase-based image captioning. Accepted in\n  ACCV2016 (extended version, 21 pages, 12 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A picture is worth a thousand words. Not until recently, however, we noticed\nsome success stories in understanding of visual scenes: a model that is able to\ndetect/name objects, describe their attributes, and recognize their\nrelationships/interactions. In this paper, we propose a phrase-based\nhierarchical Long Short-Term Memory (phi-LSTM) model to generate image\ndescription. The proposed model encodes sentence as a sequence of combination\nof phrases and words, instead of a sequence of words alone as in those\nconventional solutions. The two levels of this model are dedicated to i) learn\nto generate image relevant noun phrases, and ii) produce appropriate image\ndescription from the phrases and other words in the corpus. Adopting a\nconvolutional neural network to learn image features and the LSTM to learn the\nword sequence in a sentence, the proposed model has shown better or competitive\nresults in comparison to the state-of-the-art models on Flickr8k and Flickr30k\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 12:12:09 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 09:02:18 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 14:27:03 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 22:05:38 GMT"}, {"version": "v5", "created": "Thu, 26 Oct 2017 15:16:57 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Tan", "Ying Hua", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1608.05852", "submitter": "Jifan Chen", "authors": "Jifan Chen, Kan Chen, Xipeng Qiu, Qi Zhang, Xuanjing Huang, Zheng\n  Zhang", "title": "Learning Word Embeddings from Intrinsic and Extrinsic Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While word embeddings are currently predominant for natural language\nprocessing, most of existing models learn them solely from their contexts.\nHowever, these context-based word embeddings are limited since not all words'\nmeaning can be learned based on only context. Moreover, it is also difficult to\nlearn the representation of the rare words due to data sparsity problem. In\nthis work, we address these issues by learning the representations of words by\nintegrating their intrinsic (descriptive) and extrinsic (contextual)\ninformation. To prove the effectiveness of our model, we evaluate it on four\ntasks, including word similarity, reverse dictionaries,Wiki link prediction,\nand document classification. Experiment results show that our model is powerful\nin both word and document modeling.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 17:34:38 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chen", "Jifan", ""], ["Chen", "Kan", ""], ["Qiu", "Xipeng", ""], ["Zhang", "Qi", ""], ["Huang", "Xuanjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "1608.05859", "submitter": "Ofir Press", "authors": "Ofir Press, Lior Wolf", "title": "Using the Output Embedding to Improve Language Models", "comments": "To appear in EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the topmost weight matrix of neural network language models. We show\nthat this matrix constitutes a valid word embedding. When training language\nmodels, we recommend tying the input embedding and this output embedding. We\nanalyze the resulting update rules and show that the tied embedding evolves in\na more similar way to the output embedding than to the input embedding in the\nuntied model. We also offer a new method of regularizing the output embedding.\nOur methods lead to a significant reduction in perplexity, as we are able to\nshow on a variety of neural network language models. Finally, we show that\nweight tying can reduce the size of neural translation models to less than half\nof their original size without harming their performance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 18:32:05 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 13:32:42 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 17:50:20 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Press", "Ofir", ""], ["Wolf", "Lior", ""]]}, {"id": "1608.06043", "submitter": "Zhaopeng Tu", "authors": "Zhaopeng Tu and Yang Liu and Zhengdong Lu and Xiaohua Liu and Hang Li", "title": "Context Gates for Neural Machine Translation", "comments": "Accepted by TACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural machine translation (NMT), generation of a target word depends on\nboth source and target contexts. We find that source contexts have a direct\nimpact on the adequacy of a translation while target contexts affect the\nfluency. Intuitively, generation of a content word should rely more on the\nsource context and generation of a functional word should rely more on the\ntarget context. Due to the lack of effective control over the influence from\nsource and target contexts, conventional NMT tends to yield fluent but\ninadequate translations. To address this problem, we propose context gates\nwhich dynamically control the ratios at which source and target contexts\ncontribute to the generation of target words. In this way, we can enhance both\nthe adequacy and fluency of NMT with more careful control of the information\nflow from contexts. Experiments show that our approach significantly improves\nupon a standard attention-based NMT system by +2.3 BLEU points.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 03:19:27 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 03:09:50 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 07:14:27 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tu", "Zhaopeng", ""], ["Liu", "Yang", ""], ["Lu", "Zhengdong", ""], ["Liu", "Xiaohua", ""], ["Li", "Hang", ""]]}, {"id": "1608.06111", "submitter": "Marco Damonte", "authors": "Marco Damonte, Shay B. Cohen, Giorgio Satta", "title": "An Incremental Parser for Abstract Meaning Representation", "comments": "EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meaning Representation (AMR) is a semantic representation for natural\nlanguage that embeds annotations related to traditional tasks such as named\nentity recognition, semantic role labeling, word sense disambiguation and\nco-reference resolution. We describe a transition-based parser for AMR that\nparses sentences left-to-right, in linear time. We further propose a test-suite\nthat assesses specific subtasks that are helpful in comparing AMR parsers, and\nshow that our parser is competitive with the state of the art on the LDC2015E86\ndataset and that it outperforms state-of-the-art parsers for recovering named\nentities and handling polarity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 10:30:18 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 08:13:13 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 14:04:16 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 17:20:14 GMT"}, {"version": "v5", "created": "Mon, 10 Apr 2017 14:18:14 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Damonte", "Marco", ""], ["Cohen", "Shay B.", ""], ["Satta", "Giorgio", ""]]}, {"id": "1608.06134", "submitter": "Srikanth Ronanki", "authors": "Srikanth Ronanki and Oliver Watts and Simon King and Gustav Eje Henter", "title": "Median-Based Generation of Synthetic Speech Durations using a\n  Non-Parametric Approach", "comments": "7 pages, 1 figure -- Accepted for presentation at IEEE Workshop on\n  Spoken Language Technology (SLT 2016)", "journal-ref": null, "doi": "10.1109/SLT.2016.7846337", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new approach to duration modelling for statistical\nparametric speech synthesis in which a recurrent statistical model is trained\nto output a phone transition probability at each timestep (acoustic frame).\nUnlike conventional approaches to duration modelling -- which assume that\nduration distributions have a particular form (e.g., a Gaussian) and use the\nmean of that distribution for synthesis -- our approach can in principle model\nany distribution supported on the non-negative integers. Generation from this\nmodel can be performed in many ways; here we consider output generation based\non the median predicted duration. The median is more typical (more probable)\nthan the conventional mean duration, is robust to training-data irregularities,\nand enables incremental generation. Furthermore, a frame-level approach to\nduration prediction is consistent with a longer-term goal of modelling\ndurations and acoustic features together. Results indicate that the proposed\nmethod is competitive with baseline approaches in approximating the median\nduration of held-out natural speech.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 11:52:55 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 13:24:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ronanki", "Srikanth", ""], ["Watts", "Oliver", ""], ["King", "Simon", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "1608.06378", "submitter": "Bo-Hsiang Tseng", "authors": "Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, Lin-Shan Lee", "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening\n  Comprehension Test by Machine", "comments": "Accepted conference paper: \"The Annual Conference of the\n  International Speech Communication Association (Interspeech), 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia or spoken content presents more attractive information than plain\ntext content, but it's more difficult to display on a screen and be selected by\na user. As a result, accessing large collections of the former is much more\ndifficult and time-consuming than the latter for humans. It's highly attractive\nto develop a machine which can automatically understand spoken content and\nsummarize the key information for humans to browse over. In this endeavor, we\npropose a new task of machine comprehension of spoken content. We define the\ninitial goal as the listening comprehension test of TOEFL, a challenging\nacademic English examination for English learners whose native language is not\nEnglish. We further propose an Attention-based Multi-hop Recurrent Neural\nNetwork (AMRNN) architecture for this task, achieving encouraging results in\nthe initial tests. Initial results also have shown that word-level attention is\nprobably more robust than sentence-level attention for this task with ASR\nerrors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 04:27:41 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Tseng", "Bo-Hsiang", ""], ["Shen", "Sheng-Syun", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1608.06386", "submitter": "Mayank Singh", "authors": "Soham Dan, Sanyam Agarwal, Mayank Singh, Pawan Goyal and Animesh\n  Mukherjee", "title": "Which techniques does your application use?: An information extraction\n  framework for scientific articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Every field of research consists of multiple application areas with various\ntechniques routinely used to solve problems in these wide range of application\nareas. With the exponential growth in research volumes, it has become difficult\nto keep track of the ever-growing number of application areas as well as the\ncorresponding problem solving techniques. In this paper, we consider the\ncomputational linguistics domain and present a novel information extraction\nsystem that automatically constructs a pool of all application areas in this\ndomain and appropriately links them with corresponding problem solving\ntechniques. Further, we categorize individual research articles based on their\napplication area and the techniques proposed/used in the article. k-gram based\ndiscounting method along with handwritten rules and bootstrapped pattern\nlearning is employed to extract application areas. Subsequently, a language\nmodeling approach is proposed to characterize each article based on its\napplication area. Similarly, regular expressions and high-scoring noun phrases\nare used for the extraction of the problem solving techniques. We propose a\ngreedy approach to characterize each article based on the techniques. Towards\nthe end, we present a table representing the most frequent techniques adopted\nfor a particular application area. Finally, we propose three use cases\npresenting an extensive temporal analysis of the usage of techniques and\napplication areas.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 05:27:46 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Dan", "Soham", ""], ["Agarwal", "Sanyam", ""], ["Singh", "Mayank", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1608.06459", "submitter": "James P. Cross", "authors": "Henrik Hermansson and James P. Cross", "title": "Tracking Amendments to Legislation and Other Political Texts with a\n  Novel Minimum-Edit-Distance Algorithm: DocuToads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political scientists often find themselves tracking amendments to political\ntexts. As different actors weigh in, texts change as they are drafted and\nredrafted, reflecting political preferences and power. This study provides a\nnovel solution to the prob- lem of detecting amendments to political text based\nupon minimum edit distances. We demonstrate the usefulness of two\nlanguage-insensitive, transparent, and efficient minimum-edit-distance\nalgorithms suited for the task. These algorithms are capable of providing an\naccount of the types (insertions, deletions, substitutions, and trans-\npositions) and substantive amount of amendments made between version of texts.\nTo illustrate the usefulness and efficiency of the approach we replicate two\nexisting stud- ies from the field of legislative studies. Our results\ndemonstrate that minimum edit distance methods can produce superior measures of\ntext amendments to hand-coded efforts in a fraction of the time and resource\ncosts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 10:55:23 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Hermansson", "Henrik", ""], ["Cross", "James P.", ""]]}, {"id": "1608.06549", "submitter": "Jun-Wei Lin", "authors": "Jun-Wei Lin and Farn Wang", "title": "Using Semantic Similarity for Input Topic Identification in\n  Crawling-based Web Application Testing", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To automatically test web applications, crawling-based techniques are usually\nadopted to mine the behavior models, explore the state spaces or detect the\nviolated invariants of the applications. However, in existing crawlers, rules\nfor identifying the topics of input text fields, such as login ids, passwords,\nemails, dates and phone numbers, have to be manually configured. Moreover, the\nrules for one application are very often not suitable for another. In addition,\nwhen several rules conflict and match an input text field to more than one\ntopics, it can be difficult to determine which rule suggests a better match.\nThis paper presents a natural-language approach to automatically identify the\ntopics of encountered input fields during crawling by semantically comparing\ntheir similarities with the input fields in labeled corpus. In our evaluation\nwith 100 real-world forms, the proposed approach demonstrated comparable\nperformance to the rule-based one. Our experiments also show that the accuracy\nof the rule-based approach can be improved by up to 19% when integrated with\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:34:55 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Lin", "Jun-Wei", ""], ["Wang", "Farn", ""]]}, {"id": "1608.06651", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Marcel Worring", "title": "Unsupervised, Efficient and Semantic Expertise Retrieval", "comments": "WWW2016, Proceedings of the 25th International Conference on World\n  Wide Web. 2016", "journal-ref": null, "doi": "10.1145/2872427.2882974", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised discriminative model for the task of retrieving\nexperts in online document collections. We exclusively employ textual evidence\nand avoid explicit feature engineering by learning distributed word\nrepresentations in an unsupervised way. We compare our model to\nstate-of-the-art unsupervised statistical vector space and probabilistic\ngenerative approaches. Our proposed log-linear model achieves the retrieval\nperformance levels of state-of-the-art document-centric methods with the low\ninference cost of so-called profile-centric approaches. It yields a\nstatistically significant improved ranking over vector space and generative\nmodels in most cases, matching the performance of supervised methods on various\nbenchmarks. That is, by using solely text we can do as well as methods that\nwork with external evidence and/or relevance feedback. A contrastive analysis\nof rankings produced by discriminative and generative approaches shows that\nthey have complementary strengths due to the ability of the unsupervised\ndiscriminative model to perform semantic matching.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 20:55:09 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 04:57:54 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Worring", "Marcel", ""]]}, {"id": "1608.06656", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Evangelos Kanoulas, Maarten de Rijke", "title": "Lexical Query Modeling in Session Search", "comments": "ICTIR2016, Proceedings of the 2nd ACM International Conference on the\n  Theory of Information Retrieval. 2016", "journal-ref": null, "doi": "10.1145/2970398.2970422", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical query modeling has been the leading paradigm for session search. In\nthis paper, we analyze TREC session query logs and compare the performance of\ndifferent lexical matching approaches for session search. Naive methods based\non term frequency weighing perform on par with specialized session models. In\naddition, we investigate the viability of lexical query models in the setting\nof session search. We give important insights into the potential and\nlimitations of lexical query modeling for session search and propose future\ndirections for the field of session search.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 21:07:50 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1608.06697", "submitter": "Maite Taboada", "authors": "Cliff Goddard, Maite Taboada, Radoslava Trnavac", "title": "Semantic descriptions of 24 evaluational adjectives, for application in\n  sentiment analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": "SFU-CMPT TR 2016-42-1", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the Natural Semantic Metalanguage (NSM) approach (Goddard and\nWierzbicka 2014) to the lexical-semantic analysis of English evaluational\nadjectives and compare the results with the picture developed in the Appraisal\nFramework (Martin and White 2005). The analysis is corpus-assisted, with\nexamples mainly drawn from film and book reviews, and supported by\ncollocational and statistical information from WordBanks Online. We propose NSM\nexplications for 24 evaluational adjectives, arguing that they fall into five\ngroups, each of which corresponds to a distinct semantic template. The groups\ncan be sketched as follows: \"First-person thought-plus-affect\", e.g. wonderful;\n\"Experiential\", e.g. entertaining; \"Experiential with bodily reaction\", e.g.\ngripping; \"Lasting impact\", e.g. memorable; \"Cognitive evaluation\", e.g.\ncomplex, excellent. These groupings and semantic templates are compared with\nthe classifications in the Appraisal Framework's system of Appreciation. In\naddition, we are particularly interested in sentiment analysis, the automatic\nidentification of evaluation and subjectivity in text. We discuss the relevance\nof the two frameworks for sentiment analysis and other language technology\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 03:36:04 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Goddard", "Cliff", ""], ["Taboada", "Maite", ""], ["Trnavac", "Radoslava", ""]]}, {"id": "1608.06718", "submitter": "Jos\\'e Camacho-Collados", "authors": "Jos\\'e Camacho Collados and Claudio Delli Bovi and Alessandro Raganato\n  and Roberto Navigli", "title": "A Large-Scale Multilingual Disambiguation of Glosses", "comments": "Accepted in LREC 2016", "journal-ref": "Proceedings of the Tenth International Conference on Language\n  Resources and Evaluation (LREC), 2016, pages 1701-1708, Portoroz, Slovenia", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking concepts and named entities to knowledge bases has become a crucial\nNatural Language Understanding task. In this respect, recent works have shown\nthe key advantage of exploiting textual definitions in various Natural Language\nProcessing applications. However, to date there are no reliable large-scale\ncorpora of sense-annotated textual definitions available to the research\ncommunity. In this paper we present a large-scale high-quality corpus of\ndisambiguated glosses in multiple languages, comprising sense annotations of\nboth concepts and named entities from a unified sense inventory. Our approach\nfor the construction and disambiguation of the corpus builds upon the structure\nof a large multilingual semantic network and a state-of-the-art disambiguation\nsystem; first, we gather complementary information of equivalent definitions\nacross different languages to provide context for disambiguation, and then we\ncombine it with a semantic similarity-based refinement. As a result we obtain a\nmultilingual corpus of textual definitions featuring over 38 million\ndefinitions in 263 languages, and we make it freely available at\nhttp://lcl.uniroma1.it/disambiguated-glosses. Experiments on Open Information\nExtraction and Sense Clustering show how two state-of-the-art approaches\nimprove their performance by integrating our disambiguated corpus into their\npipeline.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 05:30:45 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Collados", "Jos\u00e9 Camacho", ""], ["Bovi", "Claudio Delli", ""], ["Raganato", "Alessandro", ""], ["Navigli", "Roberto", ""]]}, {"id": "1608.06757", "submitter": "Sebastian Arnold", "authors": "Sebastian Arnold, Felix A. Gers, Torsten Kilias, Alexander L\\\"oser", "title": "Robust Named Entity Recognition in Idiosyncratic Domains", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition often fails in idiosyncratic domains. That causes a\nproblem for depending tasks, such as entity linking and relation extraction. We\npropose a generic and robust approach for high-recall named entity recognition.\nOur approach is easy to train and offers strong generalization over diverse\ndomain-specific language, such as news documents (e.g. Reuters) or biomedical\ntext (e.g. Medline). Our approach is based on deep contextual sequence learning\nand utilizes stacked bidirectional LSTM networks. Our model is trained with\nonly few hundred labeled sentences and does not rely on further external\nknowledge. We report from our results F1 scores in the range of 84-94% on\nstandard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 09:06:14 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Arnold", "Sebastian", ""], ["Gers", "Felix A.", ""], ["Kilias", "Torsten", ""], ["L\u00f6ser", "Alexander", ""]]}, {"id": "1608.06794", "submitter": "Thomas Kober", "authors": "Thomas Kober, Julie Weeds, Jeremy Reffin and David Weir", "title": "Improving Sparse Word Representations with Distributional Inference for\n  Semantic Composition", "comments": "To appear at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional models are derived from co-occurrences in a corpus, where only\na small proportion of all possible plausible co-occurrences will be observed.\nThis results in a very sparse vector space, requiring a mechanism for inferring\nmissing knowledge. Most methods face this challenge in ways that render the\nresulting word representations uninterpretable, with the consequence that\nsemantic composition becomes hard to model. In this paper we explore an\nalternative which involves explicitly inferring unobserved co-occurrences using\nthe distributional neighbourhood. We show that distributional inference\nimproves sparse word representations on several word similarity benchmarks and\ndemonstrate that our model is competitive with the state-of-the-art for\nadjective-noun, noun-noun and verb-object compositions while being fully\ninterpretable.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 12:38:45 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Kober", "Thomas", ""], ["Weeds", "Julie", ""], ["Reffin", "Jeremy", ""], ["Weir", "David", ""]]}, {"id": "1608.07076", "submitter": "Ond\\v{r}ej Du\\v{s}ek", "authors": "Ond\\v{r}ej Du\\v{s}ek, Filip Jur\\v{c}\\'i\\v{c}ek", "title": "A Context-aware Natural Language Generator for Dialogue Systems", "comments": "Accepted as a short paper for SIGDIAL 2016", "journal-ref": "Proceedings of the SIGDIAL 2016 Conference, pages 185-190, Los\n  Angeles, USA, 13-15 September 2016", "doi": "10.18653/v1/W16-3622", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel natural language generation system for spoken dialogue\nsystems capable of entraining (adapting) to users' way of speaking, providing\ncontextually appropriate responses. The generator is based on recurrent neural\nnetworks and the sequence-to-sequence approach. It is fully trainable from data\nwhich include preceding context along with responses to be generated. We show\nthat the context-aware generator yields significant improvements over the\nbaseline in both automatic metrics and a human pairwise preference test.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 10:43:56 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Du\u0161ek", "Ond\u0159ej", ""], ["Jur\u010d\u00ed\u010dek", "Filip", ""]]}, {"id": "1608.07094", "submitter": "Mahamad Suhil", "authors": "D S Guru, Mahamad Suhil", "title": "A Novel Term_Class Relevance Measure for Text Categorization", "comments": "12 pages, 6 figures, 2 tables", "journal-ref": "Procedia Computer Science, vol.45, pp.13-22, 2015", "doi": "10.1016/j.procs.2015.03.074", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new measure called Term_Class relevance to\ncompute the relevancy of a term in classifying a document into a particular\nclass. The proposed measure estimates the degree of relevance of a given term,\nin placing an unlabeled document to be a member of a known class, as a product\nof Class_Term weight and Class_Term density; where the Class_Term weight is the\nratio of the number of documents of the class containing the term to the total\nnumber of documents containing the term and the Class_Term density is the\nrelative density of occurrence of the term in the class to the total occurrence\nof the term in the entire population. Unlike the other existing term weighting\nschemes such as TF-IDF and its variants, the proposed relevance measure takes\ninto account the degree of relative participation of the term across all\ndocuments of the class to the entire population. To demonstrate the\nsignificance of the proposed measure experimentation has been conducted on the\n20 Newsgroups dataset. Further, the superiority of the novel measure is brought\nout through a comparative analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 11:46:06 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 12:51:50 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Guru", "D S", ""], ["Suhil", "Mahamad", ""]]}, {"id": "1608.07115", "submitter": "David Weir", "authors": "David Weir, Julie Weeds, Jeremy Reffin and Thomas Kober", "title": "Aligning Packed Dependency Trees: a theory of composition for\n  distributional semantics", "comments": "To appear in Special issue of Computational Linguistics - Formal\n  Distributional Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for compositional distributional semantics in\nwhich the distributional contexts of lexemes are expressed in terms of anchored\npacked dependency trees. We show that these structures have the potential to\ncapture the full sentential contexts of a lexeme and provide a uniform basis\nfor the composition of distributional knowledge in a way that captures both\nmutual disambiguation and generalization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 12:44:05 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Weir", "David", ""], ["Weeds", "Julie", ""], ["Reffin", "Jeremy", ""], ["Kober", "Thomas", ""]]}, {"id": "1608.07187", "submitter": "Aylin Caliskan", "authors": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan", "title": "Semantics derived automatically from language corpora contain human-like\n  biases", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.1126/science.aal4230", "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence and machine learning are in a period of astounding\ngrowth. However, there are concerns that these technologies may be used, either\nwith or without intention, to perpetuate the prejudice and unfairness that\nunfortunately characterizes many human institutions. Here we show for the first\ntime that human-like semantic biases result from the application of standard\nmachine learning to ordinary language---the same sort of language humans are\nexposed to every day. We replicate a spectrum of standard human biases as\nexposed by the Implicit Association Test and other well-known psychological\nstudies. We replicate these using a widely used, purely statistical\nmachine-learning model---namely, the GloVe word embedding---trained on a corpus\nof text from the Web. Our results indicate that language itself contains\nrecoverable and accurate imprints of our historic biases, whether these are\nmorally neutral as towards insects or flowers, problematic as towards race or\ngender, or even simply veridical, reflecting the {\\em status quo} for the\ndistribution of gender with respect to careers or first names. These\nregularities are captured by machine learning along with the rest of semantics.\nIn addition to our empirical findings concerning language, we also contribute\nnew methods for evaluating bias in text, the Word Embedding Association Test\n(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results\nhave implications not only for AI and machine learning, but also for the fields\nof psychology, sociology, and human ethics, since they raise the possibility\nthat mere exposure to everyday language can account for the biases we replicate\nhere.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 15:07:17 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 18:23:06 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 19:03:45 GMT"}, {"version": "v4", "created": "Thu, 25 May 2017 17:50:31 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Caliskan", "Aylin", ""], ["Bryson", "Joanna J.", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1608.07253", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas", "title": "Learning Latent Vector Spaces for Product Search", "comments": "CIKM2016, Proceedings of the 25th ACM International Conference on\n  Information and Knowledge Management. 2016", "journal-ref": null, "doi": "10.1145/2983323.2983702", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel latent vector space model that jointly learns the latent\nrepresentations of words, e-commerce products and a mapping between the two\nwithout the need for explicit annotations. The power of the model lies in its\nability to directly model the discriminative relation between products and a\nparticular word. We compare our method to existing latent vector space models\n(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank\nsetting. Our latent vector space model achieves its enhanced performance as it\nlearns better product representations. Furthermore, the mapping from words to\nproducts and the representations of words benefit directly from the errors\npropagated back from the product representations during parameter estimation.\nWe provide an in-depth analysis of the performance of our model and analyze the\nstructure of the learned representations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:57:50 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1608.07639", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson and Gal\n  Chechik", "title": "Learning to generalize to new compositions in image understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:34:00 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Atzmon", "Yuval", ""], ["Berant", "Jonathan", ""], ["Kezami", "Vahid", ""], ["Globerson", "Amir", ""], ["Chechik", "Gal", ""]]}, {"id": "1608.07720", "submitter": "Fei Li", "authors": "Fei Li, Meishan Zhang, Guohong Fu, Tao Qian, Donghong Ji", "title": "A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Relation classification is associated with many potential applications in the\nartificial intelligence area. Recent approaches usually leverage neural\nnetworks based on structure features such as syntactic or dependency features\nto solve this problem. However, high-cost structure features make such\napproaches inconvenient to be directly used. In addition, structure features\nare probably domain-dependent. Therefore, this paper proposes a bi-directional\nlong-short-term-memory recurrent-neural-network (Bi-LSTM-RNN) model based on\nlow-cost sequence features to address relation classification. This model\ndivides a sentence or text segment into five parts, namely two target entities\nand their three contexts. It learns the representations of entities and their\ncontexts, and uses them to classify relations. We evaluate our model on two\nstandard benchmark datasets in different domains, namely SemEval-2010 Task 8\nand BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves\ncomparable performance compared with other models using sequence features. In\nthe latter dataset, our model obtains the third best results compared with\nother models in the official evaluation. Moreover, we find that the context\nbetween two target entities plays the most important role in relation\nclassification. Furthermore, statistic experiments show that the context\nbetween two target entities can be used as an approximate replacement of the\nshortest dependency path when dependency parsing is not used.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 15:41:22 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Li", "Fei", ""], ["Zhang", "Meishan", ""], ["Fu", "Guohong", ""], ["Qian", "Tao", ""], ["Ji", "Donghong", ""]]}, {"id": "1608.07738", "submitter": "Enrico Santus", "authors": "Enrico Santus, Emmanuele Chersoni, Alessandro Lenci, Chu-Ren Huang,\n  Philippe Blache", "title": "Testing APSyn against Vector Cosine on Similarity Estimation", "comments": "8 pages, 1 figure, 4 tables, PACLIC, cosine, vectors, DSMs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Distributional Semantic Models (DSMs), Vector Cosine is widely used to\nestimate similarity between word vectors, although this measure was noticed to\nsuffer from several shortcomings. The recent literature has proposed other\nmethods which attempt to mitigate such biases. In this paper, we intend to\ninvestigate APSyn, a measure that computes the extent of the intersection\nbetween the most associated contexts of two target words, weighting it by\ncontext relevance. We evaluated this metric in a similarity estimation task on\nseveral popular test sets, and our results show that APSyn is in fact highly\ncompetitive, even with respect to the results reported in the literature for\nword embeddings. On top of it, APSyn addresses some of the weaknesses of Vector\nCosine, performing well also on genuine similarity estimation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 19:57:55 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 10:46:25 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Santus", "Enrico", ""], ["Chersoni", "Emmanuele", ""], ["Lenci", "Alessandro", ""], ["Huang", "Chu-Ren", ""], ["Blache", "Philippe", ""]]}, {"id": "1608.07775", "submitter": "Juei-Yang Hsu", "authors": "Wei Fang, Jui-Yang Hsu, Hung-yi Lee, Lin-Shan Lee", "title": "Hierarchical Attention Model for Improved Machine Comprehension of\n  Spoken Content", "comments": "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken\n  Language Technology (SLT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia or spoken content presents more attractive information than plain\ntext content, but the former is more difficult to display on a screen and be\nselected by a user. As a result, accessing large collections of the former is\nmuch more difficult and time-consuming than the latter for humans. It's\ntherefore highly attractive to develop machines which can automatically\nunderstand spoken content and summarize the key information for humans to\nbrowse over. In this endeavor, a new task of machine comprehension of spoken\ncontent was proposed recently. The initial goal was defined as the listening\ncomprehension test of TOEFL, a challenging academic English examination for\nEnglish learners whose native languages are not English. An Attention-based\nMulti-hop Recurrent Neural Network (AMRNN) architecture was also proposed for\nthis task, which considered only the sequential relationship within the speech\nutterances. In this paper, we propose a new Hierarchical Attention Model (HAM),\nwhich constructs multi-hopped attention mechanism over tree-structured rather\nthan sequential representations for the utterances. Improved comprehension\nperformance robust with respect to ASR errors were obtained.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 06:48:14 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 03:19:40 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 12:17:13 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Fang", "Wei", ""], ["Hsu", "Jui-Yang", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1608.07836", "submitter": "Barbara Plank", "authors": "Barbara Plank", "title": "What to do about non-standard (or non-canonical) language in NLP", "comments": "KONVENS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world data differs radically from the benchmark corpora we use in\nnatural language processing (NLP). As soon as we apply our technologies to the\nreal world, performance drops. The reason for this problem is obvious: NLP\nmodels are trained on samples from a limited set of canonical varieties that\nare considered standard, most prominently English newswire. However, there are\nmany dimensions, e.g., socio-demographics, language, genre, sentence type, etc.\non which texts can differ from the standard. The solution is not obvious: we\ncannot control for all factors, and it is not clear how to best go beyond the\ncurrent practice of training on homogeneous data from a single domain and\nlanguage.\n  In this paper, I review the notion of canonicity, and how it shapes our\ncommunity's approach to language. I argue for leveraging what I call fortuitous\ndata, i.e., non-obvious data that is hitherto neglected, hidden in plain sight,\nor raw data that needs to be refined. If we embrace the variety of this\nheterogeneous data by combining it with proper algorithms, we will not only\nproduce more robust models, but will also enable adaptive language technology\ncapable of addressing natural language variation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 17:51:41 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Plank", "Barbara", ""]]}, {"id": "1608.07852", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu", "title": "Quantitative Analyses of Chinese Poetry of Tang and Song Dynasties:\n  Using Changing Colors and Innovative Terms as Examples", "comments": "2016 International Conference on Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tang (618-907 AD) and Song (960-1279) dynasties are two very important\nperiods in the development of Chinese literary. The most influential forms of\nthe poetry in Tang and Song were Shi and Ci, respectively. Tang Shi and Song Ci\nestablished crucial foundations of the Chinese literature, and their influences\nin both literary works and daily lives of the Chinese communities last until\ntoday.\n  We can analyze and compare the Complete Tang Shi and the Complete Song Ci\nfrom various viewpoints. In this presentation, we report our findings about the\ndifferences in their vocabularies. Interesting new words that started to appear\nin Song Ci and continue to be used in modern Chinese were identified. Colors\nare an important ingredient of the imagery in poetry, and we discuss the most\nfrequent color words that appeared in Tang Shi and Song Ci.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 20:31:37 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Liu", "Chao-Lin", ""]]}, {"id": "1608.07905", "submitter": "Shuohang Wang", "authors": "Shuohang Wang and Jing Jiang", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "comments": "11 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine comprehension of text is an important problem in natural language\nprocessing. A recently released dataset, the Stanford Question Answering\nDataset (SQuAD), offers a large number of real questions and their answers\ncreated by humans through crowdsourcing. SQuAD provides a challenging testbed\nfor evaluating machine comprehension algorithms, partly because compared with\nprevious datasets, in SQuAD the answers do not come from a small set of\ncandidate answers and they have variable lengths. We propose an end-to-end\nneural architecture for the task. The architecture is based on match-LSTM, a\nmodel we proposed previously for textual entailment, and Pointer Net, a\nsequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the\noutput tokens to be from the input sequences. We propose two ways of using\nPointer Net for our task. Our experiments show that both of our two models\nsubstantially outperform the best results obtained by Rajpurkar et al.(2016)\nusing logistic regression and manually crafted features.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 03:42:50 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 03:39:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Wang", "Shuohang", ""], ["Jiang", "Jing", ""]]}, {"id": "1608.08176", "submitter": "Amritanshu Agrawal", "authors": "Amritanshu Agrawal, Wei Fu, Tim Menzies", "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based\n  Software Engineering)", "comments": "15 pages + 2 page references. Accepted to IST", "journal-ref": "Information and Software Technology Journal, 2018", "doi": "10.1016/j.infsof.2018.02.005", "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Topic modeling finds human-readable structures in unstructured\ntextual data. A widely used topic modeler is Latent Dirichlet allocation. When\nrun on different datasets, LDA suffers from \"order effects\" i.e. different\ntopics are generated if the order of training data is shuffled. Such order\neffects introduce a systematic error for any study. This error can relate to\nmisleading results;specifically, inaccurate topic descriptions and a reduction\nin the efficacy of text mining classification results. Objective: To provide a\nmethod in which distributions generated by LDA are more stable and can be used\nfor further analysis. Method: We use LDADE, a search-based software engineering\ntool that tunes LDA's parameters using DE (Differential Evolution). LDADE is\nevaluated on data from a programmer information exchange site (Stackoverflow),\ntitle and abstract text of thousands ofSoftware Engineering (SE) papers, and\nsoftware defect reports from NASA. Results were collected across different\nimplementations of LDA (Python+Scikit-Learn, Scala+Spark); across different\nplatforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using\nGibbs sampling). Results were scored via topic stability and text mining\nclassification accuracy. Results: In all treatments: (i) standard LDA exhibits\nvery large topic instability; (ii) LDADE's tunings dramatically reduce cluster\ninstability; (iii) LDADE also leads to improved performances for supervised as\nwell as unsupervised learning. Conclusion: Due to topic instability, using\nstandard LDA with its \"off-the-shelf\" settings should now be depreciated. Also,\nin future, we should require SE papers that use LDA to test and (if needed)\nmitigate LDA topic instability. Finally, LDADE is a candidate technology for\neffectively and efficiently reducing that instability.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:45:00 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 01:19:06 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 04:49:42 GMT"}, {"version": "v4", "created": "Tue, 20 Feb 2018 17:26:51 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Agrawal", "Amritanshu", ""], ["Fu", "Wei", ""], ["Menzies", "Tim", ""]]}, {"id": "1608.08188", "submitter": "Danna Gurari", "authors": "Danna Gurari and Kristen Grauman", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) systems are emerging from a desire to empower\nusers to ask any natural language question about visual content and receive a\nvalid answer in response. However, close examination of the VQA problem reveals\nan unavoidable, entangled problem that multiple humans may or may not always\nagree on a single answer to a visual question. We train a model to\nautomatically predict from a visual question whether a crowd would agree on a\nsingle answer. We then propose how to exploit this system in a novel\napplication to efficiently allocate human effort to collect answers to visual\nquestions. Specifically, we propose a crowdsourcing system that automatically\nsolicits fewer human responses when answer agreement is expected and more human\nresponses when answer disagreement is expected. Our system improves upon\nexisting crowdsourcing systems, typically eliminating at least 20% of human\neffort with no loss to the information collected from the crowd.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:24:25 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Gurari", "Danna", ""], ["Grauman", "Kristen", ""]]}, {"id": "1608.08339", "submitter": "Taehwan Kim", "authors": "Taehwan Kim", "title": "American Sign Language fingerspelling recognition from video: Methods\n  for unrestricted recognition and signer-independence", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we study the problem of recognizing video sequences of\nfingerspelled letters in American Sign Language (ASL). Fingerspelling comprises\na significant but relatively understudied part of ASL, and recognizing it is\nchallenging for a number of reasons: It involves quick, small motions that are\noften highly coarticulated; it exhibits significant variation between signers;\nand there has been a dearth of continuous fingerspelling data collected. In\nthis work, we propose several types of recognition approaches, and explore the\nsigner variation problem. Our best-performing models are segmental\n(semi-Markov) conditional random fields using deep neural network-based\nfeatures. In the signer-dependent setting, our recognizers achieve up to about\n8% letter error rates. The signer-independent setting is much more challenging,\nbut with neural network adaptation we achieve up to 17% letter error rates.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 06:12:22 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Kim", "Taehwan", ""]]}, {"id": "1608.08515", "submitter": "Ivana Balazevic", "authors": "Ivana Balazevic, Mikio Braun, Klaus-Robert M\\\"uller", "title": "Language Detection For Short Text Messages In Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the constant growth of the World Wide Web and the number of documents in\ndifferent languages accordingly, the need for reliable language detection tools\nhas increased as well. Platforms such as Twitter with predominantly short texts\nare becoming important information resources, which additionally imposes the\nneed for short texts language detection algorithms. In this paper, we show how\nincorporating personalized user-specific information into the language\ndetection algorithm leads to an important improvement of detection results. To\nchoose the best algorithm for language detection for short text messages, we\ninvestigate several machine learning approaches. These approaches include the\nuse of the well-known classifiers such as SVM and logistic regression, a\ndictionary based approach, and a probabilistic model based on modified\nKneser-Ney smoothing. Furthermore, the extension of the probabilistic model to\ninclude additional user-specific information such as evidence accumulation per\nuser and user interface language is explored, with the goal of improving the\nclassification performance. The proposed approaches are evaluated on randomly\ncollected Twitter data containing Latin as well as non-Latin alphabet languages\nand the quality of the obtained results is compared, followed by the selection\nof the best performing algorithm. This algorithm is then evaluated against two\nalready existing general language detection tools: Chromium Compact Language\nDetector 2 (CLD2) and langid, where our method significantly outperforms the\nresults achieved by both of the mentioned methods. Additionally, a preview of\nbenefits and possible applications of having a reliable language detection\nalgorithm is given.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:43:52 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Balazevic", "Ivana", ""], ["Braun", "Mikio", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1608.08716", "submitter": "Aishwarya Agrawal", "authors": "C. Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret\n  Mitchell, Dhruv Batra, Devi Parikh", "title": "Measuring Machine Intelligence Through Visual Question Answering", "comments": "AI Magazine, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machines have become more intelligent, there has been a renewed interest\nin methods for measuring their intelligence. A common approach is to propose\ntasks for which a human excels, but one which machines find difficult. However,\nan ideal task should also be easy to evaluate and not be easily gameable. We\nbegin with a case study exploring the recently popular task of image captioning\nand its limitations as a task for measuring machine intelligence. An\nalternative and more promising task is Visual Question Answering that tests a\nmachine's ability to reason about language and vision. We describe a dataset\nunprecedented in size created for the task that contains over 760,000 human\ngenerated questions about images. Using around 10 million human generated\nanswers, machines may be easily evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:56:00 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Zitnick", "C. Lawrence", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Mitchell", "Margaret", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1608.08738", "submitter": "St\\'ephan Tulkens", "authors": "St\\'ephan Tulkens, Lisa Hilte, Elise Lodewyckx, Ben Verhoeven, Walter\n  Daelemans", "title": "A Dictionary-based Approach to Racism Detection in Dutch Social Media", "comments": "7 pages, presented at the first workshop on Text Analytics for\n  Cybersecurity and Online Safety (TA-COS), collocated with LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dictionary-based approach to racism detection in Dutch social\nmedia comments, which were retrieved from two public Belgian social media sites\nlikely to attract racist reactions. These comments were labeled as racist or\nnon-racist by multiple annotators. For our approach, three discourse\ndictionaries were created: first, we created a dictionary by retrieving\npossibly racist and more neutral terms from the training data, and then\naugmenting these with more general words to remove some bias. A second\ndictionary was created through automatic expansion using a \\texttt{word2vec}\nmodel trained on a large corpus of general Dutch text. Finally, a third\ndictionary was created by manually filtering out incorrect expansions. We\ntrained multiple Support Vector Machines, using the distribution of words over\nthe different categories in the dictionaries as features. The best-performing\nmodel used the manually cleaned dictionary and obtained an F-score of 0.46 for\nthe racist class on a test set consisting of unseen Dutch comments, retrieved\nfrom the same sites used for the training set. The automated expansion of the\ndictionary only slightly boosted the model's performance, and this increase in\nperformance was not statistically significant. The fact that the coverage of\nthe expanded dictionaries did increase indicates that the words that were\nautomatically added did occur in the corpus, but were not able to meaningfully\nimpact performance. The dictionaries, code, and the procedure for requesting\nthe corpus are available at: https://github.com/clips/hades\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 06:28:28 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Tulkens", "St\u00e9phan", ""], ["Hilte", "Lisa", ""], ["Lodewyckx", "Elise", ""], ["Verhoeven", "Ben", ""], ["Daelemans", "Walter", ""]]}, {"id": "1608.08868", "submitter": "Su Lin Blodgett", "authors": "Su Lin Blodgett, Lisa Green, and Brendan O'Connor", "title": "Demographic Dialectal Variation in Social Media: A Case Study of\n  African-American English", "comments": "To be published in EMNLP 2016, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though dialectal language is increasingly abundant on social media, few\nresources exist for developing NLP tools to handle such language. We conduct a\ncase study of dialectal language in online conversational text by investigating\nAfrican-American English (AAE) on Twitter. We propose a distantly supervised\nmodel to identify AAE-like language from demographics associated with\ngeo-located messages, and we verify that this language follows well-known AAE\nlinguistic phenomena. In addition, we analyze the quality of existing language\nidentification and dependency parsing tools on AAE-like text, demonstrating\nthat they perform poorly on such text compared to text associated with white\nspeakers. We also provide an ensemble classifier for language identification\nwhich eliminates this disparity and release a new corpus of tweets containing\nAAE-like language.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 14:12:01 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Blodgett", "Su Lin", ""], ["Green", "Lisa", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1608.08927", "submitter": "Payam Siyari", "authors": "Payam Siyari and Matthias Gall\\'e", "title": "The Generalized Smallest Grammar Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Smallest Grammar Problem -- the problem of finding the smallest\ncontext-free grammar that generates exactly one given sequence -- has never\nbeen successfully applied to grammatical inference. We investigate the reasons\nand propose an extended formulation that seeks to minimize non-recursive\ngrammars, instead of straight-line programs. In addition, we provide very\nefficient algorithms that approximate the minimization problem of this class of\ngrammars. Our empirical evaluation shows that we are able to find smaller\nmodels than the current best approximations to the Smallest Grammar Problem on\nstandard benchmarks, and that the inferred rules capture much better the\nsyntactic structure of natural language.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:23:07 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Siyari", "Payam", ""], ["Gall\u00e9", "Matthias", ""]]}, {"id": "1608.08940", "submitter": "Luis Argerich", "authors": "Luis Argerich, Joaqu\\'in Torr\\'e Zaffaroni, Mat\\'ias J Cano", "title": "Hash2Vec, Feature Hashing for Word Embeddings", "comments": "ASAI 2016, 45JAIIO", "journal-ref": "45 JAIIO - ASAI 2016 - ISSN: 2451-7585 - Pages 33-40", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose the application of feature hashing to create word\nembeddings for natural language processing. Feature hashing has been used\nsuccessfully to create document vectors in related tasks like document\nclassification. In this work we show that feature hashing can be applied to\nobtain word embeddings in linear time with the size of the data. The results\nshow that this algorithm, that does not need training, is able to capture the\nsemantic meaning of words. We compare the results against GloVe showing that\nthey are similar. As far as we know this is the first application of feature\nhashing to the word embeddings problem and the results indicate this is a\nscalable technique with practical results for NLP applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:01:09 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Argerich", "Luis", ""], ["Zaffaroni", "Joaqu\u00edn Torr\u00e9", ""], ["Cano", "Mat\u00edas J", ""]]}, {"id": "1608.08953", "submitter": "Margrit Betke", "authors": "Mehrnoosh Sameki, Mattia Gentil, Kate K. Mays, Lei Guo, and Margrit\n  Betke", "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during\n  the 2016 U.S. Presidential Election", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinions about the 2016 U.S. Presidential Candidates have been expressed in\nmillions of tweets that are challenging to analyze automatically. Crowdsourcing\nthe analysis of political tweets effectively is also difficult, due to large\ninter-rater disagreements when sarcasm is involved. Each tweet is typically\nanalyzed by a fixed number of workers and majority voting. We here propose a\ncrowdsourcing framework that instead uses a dynamic allocation of the number of\nworkers. We explore two dynamic-allocation methods: (1) The number of workers\nqueried to label a tweet is computed offline based on the predicted difficulty\nof discerning the sentiment of a particular tweet. (2) The number of crowd\nworkers is determined online, during an iterative crowd sourcing process, based\non inter-rater agreements between labels.We applied our approach to 1,000\ntwitter messages about the four U.S. presidential candidates Clinton, Cruz,\nSanders, and Trump, collected during February 2016. We implemented the two\nproposed methods using decision trees that allocate more crowd efforts to\ntweets predicted to be sarcastic. We show that our framework outperforms the\ntraditional static allocation scheme. It collects opinion labels from the crowd\nat a much lower cost while maintaining labeling accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:20:09 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 18:05:46 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Sameki", "Mehrnoosh", ""], ["Gentil", "Mattia", ""], ["Mays", "Kate K.", ""], ["Guo", "Lei", ""], ["Betke", "Margrit", ""]]}, {"id": "1608.08974", "submitter": "Yash Goyal", "authors": "Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown striking progress and obtained\nstate-of-the-art results in many AI research fields in the recent years.\nHowever, it is often unsatisfying to not know why they predict what they do. In\nthis paper, we address the problem of interpreting Visual Question Answering\n(VQA) models. Specifically, we are interested in finding what part of the input\n(pixels in images or words in questions) the VQA model focuses on while\nanswering the question. To tackle this problem, we use two visualization\ntechniques -- guided backpropagation and occlusion -- to find important words\nin the question and important regions in the image. We then present qualitative\nand quantitative analyses of these importance maps. We found that even without\nexplicit attention mechanisms, VQA models may sometimes be implicitly attending\nto relevant regions in the image, and often to appropriate words in the\nquestion.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:11:29 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 19:51:06 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Goyal", "Yash", ""], ["Mohapatra", "Akrit", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}]