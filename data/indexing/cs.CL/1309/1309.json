[{"id": "1309.0326", "submitter": "Micha{\\l} {\\L}opuszy\\'nski", "authors": "Micha{\\l} {\\L}opuszy\\'nski, {\\L}ukasz Bolikowski", "title": "Tagging Scientific Publications using Wikipedia and Natural Language\n  Processing Tools. Comparison on the ArXiv Dataset", "comments": null, "journal-ref": "Communications in Computer and Information Science Volume 416,\n  Springer 2014, pp 16-27", "doi": "10.1007/978-3-319-08425-1_3", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we compare two simple methods of tagging scientific\npublications with labels reflecting their content. As a first source of labels\nWikipedia is employed, second label set is constructed from the noun phrases\noccurring in the analyzed corpus. We examine the statistical properties and the\neffectiveness of both approaches on the dataset consisting of abstracts from\n0.7 million of scientific documents deposited in the ArXiv preprint collection.\nWe believe that obtained tags can be later on applied as useful document\nfeatures in various machine learning tasks (document similarity, clustering,\ntopic modelling, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 09:09:27 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 14:30:21 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 14:48:29 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["\u0141opuszy\u0144ski", "Micha\u0142", ""], ["Bolikowski", "\u0141ukasz", ""]]}, {"id": "1309.1014", "submitter": "Bruno Mery", "authors": "Bruno Mery (LaBRI), Christian Retor\\'e (LaBRI)", "title": "Advances in the Logical Representation of Lexical Semantics", "comments": null, "journal-ref": "NLCS'13 - Natural Language and Computer Science - 2013 (2013)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of lexical semantics and pragmatics in the analysis of the\nmeaning of natural lan- guage has prompted changes to the global framework\nderived from Montague. In those works, the original lexicon, in which words\nwere assigned an atomic type of a single-sorted logic, has been re- placed by a\nset of many-facetted lexical items that can compose their meaning with salient\ncontextual properties using a rich typing system as a guide. Having related our\nproposal for such an expanded framework \\LambdaTYn, we present some recent\nadvances in the logical formalisms associated, including constraints on lexical\ntransformations and polymorphic quantifiers, and ongoing discussions and\nresearch on the granularity of the type system and the limits of transitivity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 12:56:37 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Mery", "Bruno", "", "LaBRI"], ["Retor\u00e9", "Christian", "", "LaBRI"]]}, {"id": "1309.1125", "submitter": "Ana Mendes", "authors": "Ana Cristina Mendes, Lu\\'isa Coheur, S\\'ergio Curto", "title": "Learning to answer questions", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open-domain Question-Answering system that learns to answer\nquestions based on successful past interactions. We follow a pattern-based\napproach to Answer-Extraction, where (lexico-syntactic) patterns that relate a\nquestion to its answer are automatically learned and used to answer future\nquestions. Results show that our approach contributes to the system's best\nperformance when it is conjugated with typical Answer-Extraction strategies.\nMoreover, it allows the system to learn with the answered questions and to\nrectify wrong or unsolved past questions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 18:10:22 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Mendes", "Ana Cristina", ""], ["Coheur", "Lu\u00edsa", ""], ["Curto", "S\u00e9rgio", ""]]}, {"id": "1309.1129", "submitter": "Nisheeth Joshi", "authors": "Rashmi Gupta, Nisheeth Joshi, Iti Mathur", "title": "Analysing Quality of English-Hindi Machine Translation Engine Outputs\n  Using Bayesian Classification", "comments": null, "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 4, No. 4, July 2013", "doi": "10.5121/ijaia.2013.4415", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem for estimating the quality of machine\ntranslation outputs which are independent of human intervention and are\ngenerally addressed using machine learning techniques.There are various\nmeasures through which a machine learns translations quality. Automatic\nEvaluation metrics produce good co-relation at corpus level but cannot produce\nthe same results at the same segment or sentence level. In this paper 16\nfeatures are extracted from the input sentences and their translations and a\nquality score is obtained based on Bayesian inference produced from training\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 18:23:30 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Gupta", "Rashmi", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1309.1501", "submitter": "Aleksandr Aravkin", "authors": "Tara N. Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E.\n  Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y. Aravkin, Bhuvana\n  Ramabhadran", "title": "Improvements to deep convolutional neural networks for LVCSR", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 22:06:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 14:33:09 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 11:51:39 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sainath", "Tara N.", ""], ["Kingsbury", "Brian", ""], ["Mohamed", "Abdel-rahman", ""], ["Dahl", "George E.", ""], ["Saon", "George", ""], ["Soltau", "Hagen", ""], ["Beran", "Tomas", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1309.1508", "submitter": "Aleksandr Aravkin", "authors": "Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin,\n  Bhuvana Ramabhadran", "title": "Accelerating Hessian-free optimization for deep neural networks by\n  implicit preconditioning and sampling", "comments": "this paper is not supposed to be posted publically before the\n  conference in December due to company policy. another co-author was not\n  informed of this and posted without the permission of the first author. pls\n  remove", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hessian-free training has become a popular parallel second or- der\noptimization technique for Deep Neural Network training. This study aims at\nspeeding up Hessian-free training, both by means of decreasing the amount of\ndata used for training, as well as through reduction of the number of Krylov\nsubspace solver iterations used for implicit estimation of the Hessian. In this\npaper, we develop an L-BFGS based preconditioning scheme that avoids the need\nto access the Hessian explicitly. Since L-BFGS cannot be regarded as a\nfixed-point iteration, we further propose the employment of flexible Krylov\nsubspace solvers that retain the desired theoretical convergence guarantees of\ntheir conventional counterparts. Second, we propose a new sampling algorithm,\nwhich geometrically increases the amount of data utilized for gradient and\nKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,\nwe find that these methodologies provide roughly a 1.5x speed-up, whereas, on a\n300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no\nloss in WER. These results suggest that even further speed-up is expected, as\nproblems scale and complexity grows.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 23:21:02 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 14:34:31 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 12:05:51 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sainath", "Tara N.", ""], ["Horesh", "Lior", ""], ["Kingsbury", "Brian", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1309.1536", "submitter": "Weibing Deng", "authors": "W.B. Deng, A.E. Allahverdyan, B. Li, Q.A. Wang", "title": "Rank-frequency relation for Chinese characters", "comments": "To appear in European Physical Journal B (EPJ B), 2014 (22 pages, 7\n  figures)", "journal-ref": "Eur. Phys. J. B (2014) 87: 47", "doi": "10.1140/epjb/e2014-40805-2", "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Zipf's law for Chinese characters perfectly holds for\nsufficiently short texts (few thousand different characters). The scenario of\nits validity is similar to the Zipf's law for words in short English texts. For\nlong Chinese texts (or for mixtures of short Chinese texts), rank-frequency\nrelations for Chinese characters display a two-layer, hierarchic structure that\ncombines a Zipfian power-law regime for frequent characters (first layer) with\nan exponential-like regime for less frequent characters (second layer). For\nthese two layers we provide different (though related) theoretical descriptions\nthat include the range of low-frequency characters (hapax legomena). The\ncomparative analysis of rank-frequency relations for Chinese characters versus\nEnglish words illustrates the extent to which the characters play for Chinese\nwriters the same role as the words for those writing within alphabetical\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 04:48:19 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2014 15:47:54 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Deng", "W. B.", ""], ["Allahverdyan", "A. E.", ""], ["Li", "B.", ""], ["Wang", "Q. A.", ""]]}, {"id": "1309.1649", "submitter": "Jinho D. Choi", "authors": "Jinho D. Choi", "title": "Preparing Korean Data for the Shared Task on Parsing Morphologically\n  Rich Languages", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document gives a brief description of Korean data prepared for the SPMRL\n2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for\nthe shared task. All constituent trees are collected from the KAIST Treebank\nand transformed to the Penn Treebank style. All dependency trees are converted\nfrom the transformed constituent trees using heuristics and labeling rules de-\nsigned specifically for the KAIST Treebank. In addition to the gold-standard\nmorphological analysis provided by the KAIST Treebank, two sets of automatic\nmorphological analysis are provided for the shared task, one is generated by\nthe HanNanum morphological analyzer, and the other is generated by the Sejong\nmorphological analyzer.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 14:28:02 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 14:11:46 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Choi", "Jinho D.", ""]]}, {"id": "1309.1939", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "The placement of the head that minimizes online memory: a complex\n  systems approach", "comments": "Minor changes (language improved; typos in Eqs. 5, 6 and 13\n  corrected)", "journal-ref": "Language Dynamics and Change 5 (1), 114-137 (2015)", "doi": "10.1163/22105832-00501007", "report-no": null, "categories": "cs.CL nlin.AO physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the length of a syntactic dependency determines its\nonline memory cost. Thus, the problem of the placement of a head and its\ndependents (complements or modifiers) that minimizes online memory is\nequivalent to the problem of the minimum linear arrangement of a star tree.\nHowever, how that length is translated into cognitive cost is not known. This\nstudy shows that the online memory cost is minimized when the head is placed at\nthe center, regardless of the function that transforms length into cost,\nprovided only that this function is strictly monotonically increasing. Online\nmemory defines a quasi-convex adaptive landscape with a single central minimum\nif the number of elements is odd and two central minima if that number is even.\nWe discuss various aspects of the dynamics of word order of subject (S), verb\n(V) and object (O) from a complex systems perspective and suggest that word\norders tend to evolve by swapping adjacent constituents from an initial or\nearly SOV configuration that is attracted towards a central word order by\nonline memory minimization. We also suggest that the stability of SVO is due to\nat least two factors, the quasi-convex shape of the adaptive landscape in the\nonline memory dimension and online memory adaptations that avoid regression to\nSOV. Although OVS is also optimal for placing the verb at the center, its low\nfrequency is explained by its long distance to the seminal SOV in the\npermutation space.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2013 08:31:09 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 07:12:49 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1309.2471", "submitter": "Harinder Singh", "authors": "Harinder Singh and Parteek Kumar", "title": "Implementation of nlization framework for verbs, pronouns and\n  determiners with eugene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UNL system is designed and implemented by a nonprofit organization, UNDL\nFoundation at Geneva in 1999. UNL applications are application softwares that\nallow end users to accomplish natural language tasks, such as translating,\nsummarizing, retrieving or extracting information, etc. Two major web based\napplication softwares are Interactive ANalyzer (IAN), which is a natural\nlanguage analysis system. It represents natural language sentences as semantic\nnetworks in the UNL format. Other application software is dEep-to-sUrface\nGENErator (EUGENE), which is an open-source interactive NLizer. It generates\nnatural language sentences out of semantic networks represented in the UNL\nformat. In this paper, NLization framework with EUGENE is focused, while using\nUNL system for accomplishing the task of machine translation. In whole\nNLization process, EUGENE takes a UNL input and delivers an output in natural\nlanguage without any human intervention. It is language-independent and has to\nbe parametrized to the natural language input through a dictionary and a\ngrammar, provided as separate interpretable files. In this paper, it is\nexplained that how UNL input is syntactically and semantically analyzed with\nthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns\nand determiners for Punjabi natural language.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 12:03:32 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Singh", "Harinder", ""], ["Kumar", "Parteek", ""]]}, {"id": "1309.2853", "submitter": "Samuel Cruz-Lara", "authors": "Alexandre Denis (LORIA), Samuel Cruz-Lara (LORIA), Nadia Bellalem\n  (LORIA)", "title": "General Purpose Textual Sentiment Analysis and Emotion Detection Tools", "comments": "Workshop on Emotion and Computing (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual sentiment analysis and emotion detection consists in retrieving the\nsentiment or emotion carried by a text or document. This task can be useful in\nmany domains: opinion mining, prediction, feedbacks, etc. However, building a\ngeneral purpose tool for doing sentiment analysis and emotion detection raises\na number of issues, theoretical issues like the dependence to the domain or to\nthe language but also pratical issues like the emotion representation for\ninteroperability. In this paper we present our sentiment/emotion analysis\ntools, the way we propose to circumvent the di culties and the applications\nthey are used for.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 15:16:26 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Denis", "Alexandre", "", "LORIA"], ["Cruz-Lara", "Samuel", "", "LORIA"], ["Bellalem", "Nadia", "", "LORIA"]]}, {"id": "1309.3323", "submitter": "Ted Underwood", "authors": "Ted Underwood, Michael L. Black, Loretta Auvil, Boris Capitanu", "title": "Mapping Mutable Genres in Structurally Complex Volumes", "comments": "Preprint accepted for the 2013 IEEE International Conference on Big\n  Data. Revised to include corroborating evidence from a smaller workset", "journal-ref": null, "doi": "10.1109/BigData.2013.6691676", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mine large digital libraries in humanistically meaningful ways, scholars\nneed to divide them by genre. This is a task that classification algorithms are\nwell suited to assist, but they need adjustment to address the specific\nchallenges of this domain. Digital libraries pose two problems of scale not\nusually found in the article datasets used to test these algorithms. 1) Because\nlibraries span several centuries, the genres being identified may change\ngradually across the time axis. 2) Because volumes are much longer than\narticles, they tend to be internally heterogeneous, and the classification task\nneeds to begin with segmentation. We describe a multi-layered solution that\ntrains hidden Markov models to segment volumes, and uses ensembles of\noverlapping classifiers to address historical change. We test this approach on\na collection of 469,200 volumes drawn from HathiTrust Digital Library. To\ndemonstrate the humanistic value of these methods, we extract 32,209 volumes of\nfiction from the digital library, and trace the changing proportions of first-\nand third-person narration in the corpus. We note that narrative points of view\nseem to have strong associations with particular themes and genres.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 22:27:59 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 17:37:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Underwood", "Ted", ""], ["Black", "Michael L.", ""], ["Auvil", "Loretta", ""], ["Capitanu", "Boris", ""]]}, {"id": "1309.3946", "submitter": "Anuj Sharma Dr", "authors": "Anuj Sharma, Shubhamoy Dey", "title": "Using Self-Organizing Maps for Sentiment Analysis", "comments": "13 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web 2.0 services have enabled people to express their opinions, experience\nand feelings in the form of user-generated content. Sentiment analysis or\nopinion mining involves identifying, classifying and aggregating opinions as\nper their positive or negative polarity. This paper investigates the efficacy\nof different implementations of Self-Organizing Maps (SOM) for sentiment based\nvisualization and classification of online reviews. Specifically, this paper\nimplements the SOM algorithm for both supervised and unsupervised learning from\ntext documents. The unsupervised SOM algorithm is implemented for sentiment\nbased visualization and classification tasks. For supervised sentiment\nanalysis, a competitive learning algorithm known as Learning Vector\nQuantization is used. Both algorithms are also compared with their respective\nmulti-pass implementations where a quick rough ordering pass is followed by a\nfine tuning pass. The experimental results on the online movie review data set\nshow that SOMs are well suited for sentiment based classification and sentiment\npolarity visualization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 13:21:45 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Sharma", "Anuj", ""], ["Dey", "Shubhamoy", ""]]}, {"id": "1309.3949", "submitter": "Anuj Sharma Dr", "authors": "Anuj sharma, Shubhamoy Dey", "title": "Performance Investigation of Feature Selection Methods", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis or opinion mining has become an open research domain after\nproliferation of Internet and Web 2.0 social media. People express their\nattitudes and opinions on social media including blogs, discussion forums,\ntweets, etc. and, sentiment analysis concerns about detecting and extracting\nsentiment or opinion from online text. Sentiment based text classification is\ndifferent from topical text classification since it involves discrimination\nbased on expressed opinion on a topic. Feature selection is significant for\nsentiment analysis as the opinionated text may have high dimensions, which can\nadversely affect the performance of sentiment analysis classifier. This paper\nexplores applicability of feature selection methods for sentiment analysis and\ninvestigates their performance for classification in term of recall, precision\nand accuracy. Five feature selection methods (Document Frequency, Information\nGain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment\nfeature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews\ncorpus with a size of 2000 documents. The experimental results show that\nInformation Gain gave consistent results and Gain Ratio performs overall best\nfor sentimental feature selection while sentiment lexicons gave poor\nperformance. Furthermore, we found that performance of the classifier depends\non appropriate number of representative feature selected from text.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 13:27:04 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["sharma", "Anuj", ""], ["Dey", "Shubhamoy", ""]]}, {"id": "1309.4035", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Domain and Function: A Dual-Space Model of Semantic Relations and\n  Compositions", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research (JAIR), (2012), 44,\n  533-585", "doi": "10.1613/jair.3640", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given appropriate representations of the semantic relations between carpenter\nand wood and between mason and stone (for example, vectors in a vector space\nmodel), a suitable algorithm should be able to recognize that these relations\nare highly similar (carpenter is to wood as mason is to stone; the relations\nare analogous). Likewise, with representations of dog, house, and kennel, an\nalgorithm should be able to recognize that the semantic composition of dog and\nhouse, dog house, is highly similar to kennel (dog house and kennel are\nsynonymous). It seems that these two tasks, recognizing relations and\ncompositions, are closely connected. However, up to now, the best models for\nrelations are significantly different from the best models for compositions. In\nthis paper, we introduce a dual-space model that unifies these two tasks. This\nmodel matches the performance of the best previous models for relations and\ncompositions. The dual-space model consists of a space for measuring domain\nsimilarity and a space for measuring function similarity. Carpenter and wood\nshare the same domain, the domain of carpentry. Mason and stone share the same\ndomain, the domain of masonry. Carpenter and mason share the same function, the\nfunction of artisans. Wood and stone share the same function, the function of\nmaterials. In the composition dog house, kennel has some domain overlap with\nboth dog and house (the domains of pets and buildings). The function of kennel\nis similar to the function of house (the function of shelters). By combining\ndomain and function similarities in various ways, we can model relations,\ncompositions, and other aspects of semantics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 16:51:02 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1309.4058", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Why SOV might be initially preferred and then lost or recovered? A\n  theoretical framework", "comments": null, "journal-ref": "Proceedings of Evolang 2014, Cartmill, E. A., Roberts, S., Lyn, H.\n  & Cornish, H. (eds.), pp. 66-73 (2014)", "doi": "10.1142/9789814603638_0007", "report-no": null, "categories": "cs.CL nlin.AO physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little is known about why SOV order is initially preferred and then discarded\nor recovered. Here we present a framework for understanding these and many\nrelated word order phenomena: the diversity of dominant orders, the existence\nof free words orders, the need of alternative word orders and word order\nreversions and cycles in evolution. Under that framework, word order is\nregarded as a multiconstraint satisfaction problem in which at least two\nconstraints are in conflict: online memory minimization and maximum\npredictability.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 18:21:54 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1309.4168", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Quoc V. Le, Ilya Sutskever", "title": "Exploiting Similarities among Languages for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionaries and phrase tables are the basis of modern statistical machine\ntranslation systems. This paper develops a method that can automate the process\nof generating and extending dictionaries and phrase tables. Our method can\ntranslate missing word and phrase entries by learning language structures based\non large monolingual data and mapping between languages from small bilingual\ndata. It uses distributed representation of words and learns a linear mapping\nbetween vector spaces of languages. Despite its simplicity, our method is\nsurprisingly effective: we can achieve almost 90% precision@5 for translation\nof words between English and Spanish. This method makes little assumption about\nthe languages, so it can be used to extend and refine dictionaries and\ntranslation tables for any language pairs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 03:23:13 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Mikolov", "Tomas", ""], ["Le", "Quoc V.", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1309.4628", "submitter": "Grzegorz Chrupala", "authors": "Grzegorz Chrupa{\\l}a", "title": "Text segmentation with character-level text embeddings", "comments": "Workshop on Deep Learning for Audio, Speech and Language Processing,\n  ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning word representations has recently seen much success in computational\nlinguistics. However, assuming sequences of word tokens as input to linguistic\nanalysis is often unjustified. For many languages word segmentation is a\nnon-trivial task and naturally occurring text is sometimes a mixture of natural\nlanguage strings and other character data. We propose to learn text\nrepresentations directly from raw character sequences by training a Simple\nrecurrent Network to predict the next character in text. The network uses its\nhidden layer to evolve abstract representations of the character sequences it\nsees. To demonstrate the usefulness of the learned text embeddings, we use them\nas features in a supervised character level text segmentation and labeling\ntask: recognizing spans of text containing programming language code. By using\nthe embeddings as features we are able to substantially improve over a baseline\nwhich uses only surface character n-grams.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 12:38:34 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "1309.5174", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, N. Siddharth, Jeffrey Mark Siskind", "title": "Saying What You're Looking For: Linguistics Meets Video Search", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to searching large video corpora for video clips which\ndepict a natural-language query in the form of a sentence. This approach uses\ncompositional semantics to encode subtle meaning that is lost in other systems,\nsuch as the difference between two sentences which have identical words but\nentirely different meaning: \"The person rode the horse} vs. \\emph{The horse\nrode the person\". Given a video-sentence pair and a natural-language parser,\nalong with a grammar that describes the space of sentential queries, we produce\na score which indicates how well the video depicts the sentence. We produce\nsuch a score for each video clip in a corpus and return a ranked list of clips.\nFurthermore, this approach addresses two fundamental problems simultaneously:\ndetecting and tracking objects, and recognizing whether those tracks depict the\nquery. Because both tracking and object detection are unreliable, this uses\nknowledge about the intended sentential query to focus the tracker on the\nrelevant participants and ensures that the resulting tracks are described by\nthe sentential query. While earlier work was limited to single-word queries\nwhich correspond to either verbs or nouns, we show how one can search for\ncomplex queries which contain multiple phrases, such as prepositional phrases,\nand modifiers, such as adverbs. We demonstrate this approach by searching for\n141 queries involving people and horses interacting with each other in 10\nfull-length Hollywood movies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 05:07:29 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Barbu", "Andrei", ""], ["Siddharth", "N.", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1309.5223", "submitter": "Ralf Steinberger", "authors": "Ralf Steinberger, Mohamed Ebrahim, Marco Turchi", "title": "JRC EuroVoc Indexer JEX - A freely available multi-label categorisation\n  tool", "comments": null, "journal-ref": "Proceedings of the 8th international conference on Language\n  Resources and Evaluation (LREC'2012), pp. 798-805, Istanbul, 21-27 May 2012", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700\nhierarchically organised subject domains used by European Institutions and many\nauthorities in Member States of the European Union (EU) for the classification\nand retrieval of official documents. JEX is JRC-developed multi-label\nclassification software that learns from manually labelled data to\nautomatically assign EuroVoc descriptors to new documents in a profile-based\ncategory-ranking task. The JEX release consists of trained classifiers for 22\nofficial EU languages, of parallel training data in the same languages, of an\ninterface that allows viewing and amending the assignment results, and of a\nmodule that allows users to re-train the tool on their own document\ncollections. JEX allows advanced users to change the document representation so\nas to possibly improve the categorisation result through linguistic\npre-processing. JEX can be used as a tool for interactive EuroVoc descriptor\nassignment to increase speed and consistency of the human categorisation\nprocess, or it can be used fully automatically. The output of JEX is a\nlanguage-independent EuroVoc feature vector lending itself also as input to\nvarious other Language Technology tasks, including cross-lingual clustering and\nclassification, cross-lingual plagiarism detection, sentence selection and\nranking, and more.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 09:51:59 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Steinberger", "Ralf", ""], ["Ebrahim", "Mohamed", ""], ["Turchi", "Marco", ""]]}, {"id": "1309.5226", "submitter": "Ralf Steinberger", "authors": "Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos,\n  Patrick Schl\\\"uter", "title": "DGT-TM: A freely Available Translation Memory in 22 Languages", "comments": null, "journal-ref": "Proceedings of the 8th international conference on Language\n  Resources and Evaluation (LREC'2012), pp. 454-459, Istanbul, 21-27 May 2012", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European Commission's (EC) Directorate General for Translation, together\nwith the EC's Joint Research Centre, is making available a large translation\nmemory (TM; i.e. sentences and their professionally produced translations)\ncovering twenty-two official European Union (EU) languages and their 231\nlanguage pairs. Such a resource is typically used by translation professionals\nin combination with TM software to improve speed and consistency of their\ntranslations. However, this resource has also many uses for translation studies\nand for language technology applications, including Statistical Machine\nTranslation (SMT), terminology extraction, Named Entity Recognition (NER),\nmultilingual classification and clustering, and many more. In this reference\npaper for DGT-TM, we introduce this new resource, provide statistics regarding\nits size, and explain how it was produced and how to use it.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 10:02:58 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Steinberger", "Ralf", ""], ["Eisele", "Andreas", ""], ["Klocek", "Szymon", ""], ["Pilos", "Spyridon", ""], ["Schl\u00fcter", "Patrick", ""]]}, {"id": "1309.5290", "submitter": "Ralf Steinberger", "authors": "Ralf Steinberger, Bruno Pouliquen, Erik van der Goot", "title": "An introduction to the Europe Media Monitor family of applications", "comments": null, "journal-ref": "In: Fredric Gey, Noriko Kando & Jussi Karlgren (eds.): Information\n  Access in a Multilingual World - Proceedings of the SIGIR 2009 Workshop\n  (SIGIR-CLIR'2009), pp. 1-8. Boston, USA. 23 July 2009", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most large organizations have dedicated departments that monitor the media to\nkeep up-to-date with relevant developments and to keep an eye on how they are\nrepresented in the news. Part of this media monitoring work can be automated.\nIn the European Union with its 23 official languages, it is particularly\nimportant to cover media reports in many languages in order to capture the\ncomplementary news content published in the different countries. It is also\nimportant to be able to access the news content across languages and to merge\nthe extracted information. We present here the four publicly accessible systems\nof the Europe Media Monitor (EMM) family of applications, which cover between\n19 and 50 languages (see http://press.jrc.it/overview.html). We give an\noverview of their functionality and discuss some of the implications of the\nfact that they cover quite so many languages. We discuss design issues\nnecessary to be able to achieve this high multilinguality, as well as the\nbenefits of this multilinguality.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 15:03:58 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Steinberger", "Ralf", ""], ["Pouliquen", "Bruno", ""], ["van der Goot", "Erik", ""]]}, {"id": "1309.5319", "submitter": "Clement Moulin-Frier", "authors": "Cl\\'ement Moulin-Frier (INRIA Bordeaux - Sud-Ouest, GIPSA-lab), M. A.\n  Arbib (USC)", "title": "Recognizing Speech in a Novel Accent: The Motor Theory of Speech\n  Perception Reframed", "comments": null, "journal-ref": "Biological Cybernetics 107, 4 (2013) 421-447", "doi": "10.1007/s00422-013-0557-3", "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motor theory of speech perception holds that we perceive the speech of\nanother in terms of a motor representation of that speech. However, when we\nhave learned to recognize a foreign accent, it seems plausible that recognition\nof a word rarely involves reconstruction of the speech gestures of the speaker\nrather than the listener. To better assess the motor theory and this\nobservation, we proceed in three stages. Part 1 places the motor theory of\nspeech perception in a larger framework based on our earlier models of the\nadaptive formation of mirror neurons for grasping, and for viewing extensions\nof that mirror system as part of a larger system for neuro-linguistic\nprocessing, augmented by the present consideration of recognizing speech in a\nnovel accent. Part 2 then offers a novel computational model of how a listener\ncomes to understand the speech of someone speaking the listener's native\nlanguage with a foreign accent. The core tenet of the model is that the\nlistener uses hypotheses about the word the speaker is currently uttering to\nupdate probabilities linking the sound produced by the speaker to phonemes in\nthe native language repertoire of the listener. This, on average, improves the\nrecognition of later words. This model is neutral regarding the nature of the\nrepresentations it uses (motor vs. auditory). It serve as a reference point for\nthe discussion in Part 3, which proposes a dual-stream neuro-linguistic\narchitecture to revisits claims for and against the motor theory of speech\nperception and the relevance of mirror neurons, and extracts some implications\nfor the reframing of the motor theory.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 16:47:48 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Moulin-Frier", "Cl\u00e9ment", "", "INRIA Bordeaux - Sud-Ouest, GIPSA-lab"], ["Arbib", "M. A.", "", "USC"]]}, {"id": "1309.5391", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Even the Abstract have Colour: Consensus in Word-Colour Associations", "comments": "Even the Abstract have Colour: Consensus in Word-Colour Associations,\n  Saif Mohammad, In Proceedings of the 49th Annual Meeting of the Association\n  for Computational Linguistics: Human Language Technologies, June 2011,\n  Portland, OR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colour is a key component in the successful dissemination of information.\nSince many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complemented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. A word-choice question was used to obtain sense-level\nannotations and to ensure data quality. We focus especially on abstract\nconcepts and emotions to show that even they tend to have strong colour\nassociations. Thus, using the right colours can not only improve semantic\ncoherence, but also inspire the desired emotional response.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 21:06:46 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "1309.5652", "submitter": "Nizar Habash", "authors": "Mona Diab, Nizar Habash, Owen Rambow and Ryan Roth", "title": "LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual", "comments": "14 pages; one cover", "journal-ref": null, "doi": null, "report-no": "CLCSL-0S7--1031-02", "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Linguistic Data Consortium (LDC) has developed hundreds of data corpora\nfor natural language processing (NLP) research. Among these are a number of\nannotated treebank corpora for Arabic. Typically, these corpora consist of a\nsingle collection of annotated documents. NLP research, however, usually\nrequires multiple data sets for the purposes of training models, developing\ntechniques, and final evaluation. Therefore it becomes necessary to divide the\ncorpora used into the required data sets (divisions). This document details a\nset of rules that have been defined to enable consistent divisions for old and\nnew Arabic treebanks (ATB) and related corpora.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 21:09:07 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Diab", "Mona", ""], ["Habash", "Nizar", ""], ["Rambow", "Owen", ""], ["Roth", "Ryan", ""]]}, {"id": "1309.5657", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "T.El-Shishtawy", "title": "A Hybrid Algorithm for Matching Arabic Names", "comments": null, "journal-ref": "International Journal of Computational Linguistics Research Volume\n  4 Number 2 June 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new hybrid algorithm which combines both of token-based and\ncharacter-based approaches is presented. The basic Levenshtein approach has\nbeen extended to token-based distance metric. The distance metric is enhanced\nto set the proper granularity level behavior of the algorithm. It smoothly maps\na threshold of misspellings differences at the character level, and the\nimportance of token level errors in terms of token's position and frequency.\nUsing a large Arabic dataset, the experimental results show that the proposed\nalgorithm overcomes successfully many types of errors such as: typographical\nerrors, omission or insertion of middle name components, omission of\nnon-significant popular name components, and different writing styles character\nvariations. When compared the results with other classical algorithms, using\nthe same dataset, the proposed algorithm was found to increase the minimum\nsuccess level of best tested algorithms, while achieving higher upper limits .\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 22:06:26 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["El-Shishtawy", "T.", ""]]}, {"id": "1309.5843", "submitter": "Marco Guerini", "authors": "Marco Guerini, Lorenzo Gatti, Marco Turchi", "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet", "comments": "To appear in Proceedings of EMNLP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning a positive or negative score to a word out of context (i.e. a\nword's prior polarity) is a challenging task for sentiment analysis. In the\nliterature, various approaches based on SentiWordNet have been proposed. In\nthis paper, we compare the most often used techniques together with newly\nproposed ones and incorporate all of them in a learning framework to see\nwhether blending them can further improve the estimation of prior polarity\nscores. Using two different versions of SentiWordNet and testing regression and\nclassification models across tasks and datasets, our learning approach\nconsistently outperforms the single metrics, providing a new state-of-the-art\napproach in computing words' prior polarity for sentiment analysis. We conclude\nour investigation showing interesting biases in calculated prior polarity\nscores when word Part of Speech and annotator gender are considered.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 15:26:09 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Guerini", "Marco", ""], ["Gatti", "Lorenzo", ""], ["Turchi", "Marco", ""]]}, {"id": "1309.5909", "submitter": "Saif Mohammad Dr.", "authors": "Saif Mohammad", "title": "From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels\n  and Fairy Tales", "comments": null, "journal-ref": "In Proceedings of the ACL Workshop on Language Technology for\n  Cultural Heritage, Social Sciences, and Humanities (LaTeCH), 2011, Portland,\n  OR", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today we have access to unprecedented amounts of literary texts. However,\nsearch still relies heavily on key words. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in both individual books and across very large collections. We\nintroduce the concept of emotion word density, and using the Brothers Grimm\nfairy tales as example, we show how collections of text can be organized for\nbetter search. Using the Google Books Corpus we show how to determine an\nentity's emotion associations from co-occurring words. Finally, we compare\nemotion words in fairy tales and novels, to show that fairy tales have a much\nwider range of emotion word densities than novels.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 18:43:56 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Mohammad", "Saif", ""]]}, {"id": "1309.5942", "submitter": "Saif Mohammad Dr.", "authors": "Saif Mohammad", "title": "Colourful Language: Measuring Word-Colour Associations", "comments": "arXiv admin note: substantial text overlap with arXiv:1309.5391", "journal-ref": "Colourful Language: Measuring Word-Colour Associations, Saif\n  Mohammad, In Proceedings of the ACL 2011 Workshop on Cognitive Modeling and\n  Computational Linguistics (CMCL), June 2011, Portland, OR", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complimented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. We focus especially on abstract concepts and emotions to show\nthat even though they cannot be physically visualized, they too tend to have\nstrong colour associations. Finally, we show how word-colour associations\nmanifest themselves in language, and quantify usefulness of co-occurrence and\npolarity cues in automatically detecting colour associations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 21:10:56 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Mohammad", "Saif", ""]]}, {"id": "1309.6047", "submitter": "Nikolay Lyubimov", "authors": "Nikolay Lyubimov and Mikhail Kotov", "title": "Non-negative Matrix Factorization with Linear Constraints for\n  Single-Channel Speech Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a non-negative matrix factorization (NMF)-based\napproach to the semi-supervised single-channel speech enhancement problem where\nonly non-stationary additive noise signals are given. The proposed method\nrelies on sinusoidal model of speech production which is integrated inside NMF\nframework using linear constraints on dictionary atoms. This method is further\ndeveloped to regularize harmonic amplitudes. Simple multiplicative algorithms\nare presented. The experimental evaluation was made on TIMIT corpus mixed with\nvarious types of noise. It has been shown that the proposed method outperforms\nsome of the state-of-the-art noise suppression techniques in terms of\nsignal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 05:14:53 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Lyubimov", "Nikolay", ""], ["Kotov", "Mikhail", ""]]}, {"id": "1309.6162", "submitter": "Ralf Steinberger", "authors": "Ralf Steinberger, Bruno Pouliquen, Mijail Kabadjov, Erik van der Goot", "title": "JRC-Names: A freely available, highly multilingual named entity resource", "comments": null, "journal-ref": "Proceedings of the 8th International Conference Recent Advances in\n  Natural Language Processing (RANLP'2011), pp. 104-110. Hissar, Bulgaria,\n  12-14 September 2011", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new, freely available, highly multilingual named\nentity resource for person and organisation names that has been compiled over\nseven years of large-scale multilingual news analysis combined with Wikipedia\nmining, resulting in 205,000 per-son and organisation names plus about the same\nnumber of spelling variants written in over 20 different scripts and in many\nmore languages. This resource, produced as part of the Europe Media Monitor\nactivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number\nof purposes. These include improving name search in databases or on the\ninternet, seeding machine learning systems to learn named entity recognition\nrules, improve machine translation results, and more. We describe here how this\nresource was created; we give statistics on its current size; we address the\nissue of morphological inflection; and we give details regarding its\nfunctionality. Updates to this resource will be made available daily.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 14:09:53 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Steinberger", "Ralf", ""], ["Pouliquen", "Bruno", ""], ["Kabadjov", "Mijail", ""], ["van der Goot", "Erik", ""]]}, {"id": "1309.6176", "submitter": "Xin Zheng", "authors": "Xin Zheng, Zhiyong Wu, Helen Meng, Weifeng Li, Lianhong Cai", "title": "Feature Learning with Gaussian Restricted Boltzmann Machine for Robust\n  Speech Recognition", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we first present a new variant of Gaussian restricted\nBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann\nmachine (MGRBM), with its definition and learning algorithm. Then we propose\nusing a learned GRBM or MGRBM to extract better features for robust speech\nrecognition. Our experiments on Aurora2 show that both GRBM-extracted and\nMGRBM-extracted feature performs much better than Mel-frequency cepstral\ncoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)\nacoustic model, and MGRBM-extracted feature is slightly better.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 13:51:28 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Zheng", "Xin", ""], ["Wu", "Zhiyong", ""], ["Meng", "Helen", ""], ["Li", "Weifeng", ""], ["Cai", "Lianhong", ""]]}, {"id": "1309.6185", "submitter": "Ralf Steinberger", "authors": "Maud Ehrmann, Leonida della Rocca, Ralf Steinberger, Hristo Tanev", "title": "Acronym recognition and processing in 22 languages", "comments": null, "journal-ref": "Proceedings of the 9th Conference 'Recent Advances in Natural\n  Language Processing' (RANLP), pp. 237-244. Hissar, Bulgaria, 7-13 September\n  2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting work on recognising acronyms of the form Long-Form\n(Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news\narticles in twenty-two languages, as part of our more general effort to\nrecognise entities and their variants in news text and to use them for the\nautomatic analysis of the news, including the linking of related news across\nlanguages. We show how the acronym recognition patterns, initially developed\nfor medical terms, needed to be adapted to the more general news domain and we\npresent evaluation results. We describe our effort to automatically merge the\nnumerous long-form variants referring to the same short-form, while keeping\nnon-related long-forms separate. Finally, we provide extensive statistics on\nthe frequency and the distribution of short-form/long-form pairs across\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 14:41:33 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Ehrmann", "Maud", ""], ["della Rocca", "Leonida", ""], ["Steinberger", "Ralf", ""], ["Tanev", "Hristo", ""]]}, {"id": "1309.6202", "submitter": "Ralf Steinberger", "authors": "Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella,\n  Erik van der Goot, Matina Halkia, Bruno Pouliquen, Jenya Belyaeva", "title": "Sentiment Analysis in the News", "comments": null, "journal-ref": "Proceedings of the 7th International Conference on Language\n  Resources and Evaluation (LREC'2010), pp. 2216-2220. Valletta, Malta, 19-21\n  May 2010", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have brought a significant growth in the volume of research in\nsentiment analysis, mostly on highly subjective text types (movie or product\nreviews). The main difference these texts have with news articles is that their\ntarget is clearly defined and unique across the text. Following different\nannotation efforts and the analysis of the issues encountered, we realised that\nnews opinion mining is different from that of other text types. We identified\nthree subtasks that need to be addressed: definition of the target; separation\nof the good and bad news content from the good and bad sentiment expressed on\nthe target; and analysis of clearly marked opinion that is expressed\nexplicitly, not needing interpretation or the use of world knowledge.\nFurthermore, we distinguish three different possible views on newspaper\narticles - author, reader and text, which have to be addressed differently at\nthe time of analysing sentiment. Given these definitions, we present work on\nmining opinions about entities in English language news, in which (a) we test\nthe relative suitability of various sentiment dictionaries and (b) we attempt\nto separate positive or negative opinion from good or bad news. In the\nexperiments described here, we tested whether or not subject domain-defining\nvocabulary should be ignored. Results showed that this idea is more appropriate\nin the context of news opinion mining and that the approaches taking this into\nconsideration produce a better performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 15:11:43 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Balahur", "Alexandra", ""], ["Steinberger", "Ralf", ""], ["Kabadjov", "Mijail", ""], ["Zavarella", "Vanni", ""], ["van der Goot", "Erik", ""], ["Halkia", "Matina", ""], ["Pouliquen", "Bruno", ""], ["Belyaeva", "Jenya", ""]]}, {"id": "1309.6347", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad and Tony (Wenda) Yang", "title": "Tracking Sentiment in Mail: How Genders Differ on Emotional Axes", "comments": "In Proceedings of the ACL 2011 Workshop on ACL 2011 Workshop on\n  Computational Approaches to Subjectivity and Sentiment Analysis (WASSA), June\n  2011, Portland, OR. arXiv admin note: text overlap with arXiv:1309.5909", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of email, we now have access to unprecedented amounts\nof text that we ourselves have written. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in many types of mail. We create a large word--emotion\nassociation lexicon by crowdsourcing, and use it to compare emotions in love\nletters, hate mail, and suicide notes. We show that there are marked\ndifferences across genders in how they use emotion words in work-place email.\nFor example, women use many words from the joy--sadness axis, whereas men\nprefer terms from the fear--trust axis. Finally, we show visualizations that\ncan help people track emotions in their emails.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 21:14:45 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Mohammad", "Saif M.", "", "Wenda"], ["Tony", "", "", "Wenda"], ["Yang", "", ""]]}, {"id": "1309.6352", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad and Svetlana Kiritchenko", "title": "Using Nuances of Emotion to Identify Personality", "comments": "In Proceedings of the ICWSM Workshop on Computational Personality\n  Recognition, July 2013, Boston, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past work on personality detection has shown that frequency of lexical\ncategories such as first person pronouns, past tense verbs, and sentiment words\nhave significant correlations with personality traits. In this paper, for the\nfirst time, we show that fine affect (emotion) categories such as that of\nexcitement, guilt, yearning, and admiration are significant indicators of\npersonality. Additionally, we perform experiments to show that the gains\nprovided by the fine affect categories are not obtained by using coarse affect\ncategories alone or with specificity features alone. We employ these features\nin five SVM classifiers for detecting five personality traits through essays.\nWe find that the use of fine emotion features leads to statistically\nsignificant improvement over a competitive baseline, whereas the use of coarse\naffect and specificity features does not.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 21:21:02 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Kiritchenko", "Svetlana", ""]]}, {"id": "1309.6650", "submitter": "Adrian Paschke", "authors": "Haytham Al-Feel, Ralph Schafermeier, Adrian Paschke", "title": "An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching", "comments": "http://www.ijcsi.org/papers/IJCSI-10-2-1-497-503.pdf", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 2, No 1, March 2013 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784", "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies are considered as the backbone of the Semantic Web. With the\nrising success of the Semantic Web, the number of participating communities\nfrom different countries is constantly increasing. The growing number of\nontologies available in different natural languages leads to an\ninteroperability problem. In this paper, we discuss several approaches for\nontology matching; examine similarities and differences, identify weaknesses,\nand compare the existing automated approaches with the manual approaches for\nintegrating multilingual ontologies. In addition to that, we propose a new\narchitecture for a multilingual ontology matching service. As a case study we\nused an example of two multilingual enterprise ontologies - the university\nontology of Freie Universitaet Berlin and the ontology for Fayoum University in\nEgypt.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 20:13:28 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Al-Feel", "Haytham", ""], ["Schafermeier", "Ralph", ""], ["Paschke", "Adrian", ""]]}, {"id": "1309.6722", "submitter": "Duyu Tang", "authors": "Tang Duyu, Qin Bing, Zhou LanJun, Wong KamFai, Zhao Yanyan, Liu Ting", "title": "Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the automatic extraction of domain-specific sentiment\nword (DSSW), which is a fundamental subtask of sentiment analysis. Most\nprevious work utilizes manual patterns for this task. However, the performance\nof those methods highly relies on the labelled patterns or selected seeds. In\norder to overcome the above problem, this paper presents an automatic framework\nto detect large-scale domain-specific patterns for DSSW extraction. To this\nend, sentiment seeds are extracted from massive dataset of user comments.\nSubsequently, these sentiment seeds are expanded by synonyms using a\nbootstrapping mechanism. Simultaneously, a synonymy graph is built and the\ngraph propagation algorithm is applied on the built synonymy graph. Afterwards,\nsyntactic and sequential relations between target words and high-ranked\nsentiment words are extracted automatically to construct large-scale patterns,\nwhich are further used to extracte DSSWs. The experimental results in three\ndomains reveal the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 05:18:12 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Duyu", "Tang", ""], ["Bing", "Qin", ""], ["LanJun", "Zhou", ""], ["KamFai", "Wong", ""], ["Yanyan", "Zhao", ""], ["Ting", "Liu", ""]]}, {"id": "1309.6874", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Eric P. Xing", "title": "Integrating Document Clustering and Topic Modeling", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-694-703", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document clustering and topic modeling are two closely related tasks which\ncan mutually benefit each other. Topic modeling can project documents into a\ntopic space which facilitates effective document clustering. Cluster labels\ndiscovered by document clustering can be incorporated into topic models to\nextract local topics specific to each cluster and global topics shared by all\nclusters. In this paper, we propose a multi-grain clustering topic model\n(MGCTM) which integrates document clustering and topic modeling into a unified\nframework and jointly performs the two tasks to achieve the overall best\nperformance. Our model tightly couples two components: a mixture component used\nfor discovering latent groups in document collection and a topic model\ncomponent used for mining multi-grain topics including local topics specific to\neach cluster and global topics shared across clusters.We employ variational\ninference to approximate the posterior of hidden variables and learn model\nparameters. Experiments on two datasets demonstrate the effectiveness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:02 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Xie", "Pengtao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1309.7270", "submitter": "Ahmed Abbasi", "authors": "Tianjun Fu, Ahmed Abbasi, Daniel Zeng and Hsinchun Chen", "title": "Evaluating the Usefulness of Sentiment Information for Focused Crawlers", "comments": "Fu, T., Abbasi, A., Zeng, D., and Chen, H. \"Evaluating the Usefulness\n  of Sentiment Information for Focused Crawlers,\" In Proceedings of the 20th\n  Annual Workshop on Information Technologies and Systems, St. Louis, MO,\n  December 11-12, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the prevalence of sentiment-related content on the Web, there has\nbeen limited work on focused crawlers capable of effectively collecting such\ncontent. In this study, we evaluated the efficacy of using sentiment-related\ninformation for enhanced focused crawling of opinion-rich web content regarding\na particular topic. We also assessed the impact of using sentiment-labeled web\ngraphs to further improve collection accuracy. Experimental results on a large\ntest bed encompassing over half a million web pages revealed that focused\ncrawlers utilizing sentiment information as well as sentiment-labeled web\ngraphs are capable of gathering more holistic collections of opinion-related\ncontent regarding a particular topic. The results have important implications\nfor business and marketing intelligence gathering efforts in the Web 2.0 era.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 15:14:34 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Fu", "Tianjun", ""], ["Abbasi", "Ahmed", ""], ["Zeng", "Daniel", ""], ["Chen", "Hsinchun", ""]]}, {"id": "1309.7312", "submitter": "Himangshu Sarma", "authors": "Himangshu Sarma, Navanath Saharia, Utpal Sharma, Smriti Kumar Sinha,\n  Mancha Jyoti Malakar", "title": "Development and Transcription of Assamese Speech Corpus", "comments": "4 page,National Conferance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A balanced speech corpus is the basic need for any speech processing task. In\nthis report we describe our effort on development of Assamese speech corpus. We\nmainly focused on some issues and challenges faced during development of the\ncorpus. Being a less computationally aware language, this is the first effort\nto develop speech corpus for Assamese. As corpus development is an ongoing\nprocess, in this paper we report only the initial task.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 17:54:14 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Sarma", "Himangshu", ""], ["Saharia", "Navanath", ""], ["Sharma", "Utpal", ""], ["Sinha", "Smriti Kumar", ""], ["Malakar", "Mancha Jyoti", ""]]}, {"id": "1309.7340", "submitter": "Jiwei Li", "authors": "Jiwei Li and Claire Cardie", "title": "Early Stage Influenza Detection from Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza is an acute respiratory illness that occurs virtually every year\nand results in substantial disease, death and expense. Detection of Influenza\nin its earliest stage would facilitate timely action that could reduce the\nspread of the illness. Existing systems such as CDC and EISS which try to\ncollect diagnosis data, are almost entirely manual, resulting in about two-week\ndelays for clinical data acquisition. Twitter, a popular microblogging service,\nprovides us with a perfect source for early-stage flu detection due to its\nreal- time nature. For example, when a flu breaks out, people that get the flu\nmay post related tweets which enables the detection of the flu breakout\npromptly. In this paper, we investigate the real-time flu detection problem on\nTwitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporal\nunsupervised Bayesian algorithm based on a 4 phase Markov Network, trying to\nidentify the flu breakout at the earliest stage. We test our model on real\nTwitter datasets from the United States along with baselines in multiple\napplications, such as real-time flu breakout detection, future epidemic phase\nprediction, or Influenza-like illness (ILI) physician visits. Experimental\nresults show the robustness and effectiveness of our approach. We build up a\nreal time flu reporting system based on the proposed approach, and we are\nhopeful that it would help government or health organizations in identifying\nflu outbreaks and facilitating timely actions to decrease unnecessary\nmortality.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 19:47:11 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 18:01:47 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2013 21:09:39 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Li", "Jiwei", ""], ["Cardie", "Claire", ""]]}]