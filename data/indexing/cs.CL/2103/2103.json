[{"id": "2103.00109", "submitter": "Ye Zhang", "authors": "Ye Zhang, Yuan Cao, Mahdis Mahdieh, Jeffrey Zhao, Yonghui Wu", "title": "Improving Longer-range Dialogue State Tracking", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue state tracking (DST) is a pivotal component in task-oriented\ndialogue systems. While it is relatively easy for a DST model to capture belief\nstates in short conversations, the task of DST becomes more challenging as the\nlength of a dialogue increases due to the injection of more distracting\ncontexts. In this paper, we aim to improve the overall performance of DST with\na special focus on handling longer dialogues. We tackle this problem from three\nperspectives: 1) A model designed to enable hierarchical slot status\nprediction; 2) Balanced training procedure for generic and task-specific\nlanguage understanding; 3) Data perturbation which enhances the model's ability\nin handling longer conversations. We conduct experiments on the MultiWOZ\nbenchmark, and demonstrate the effectiveness of each component via a set of\nablation tests, especially on longer conversations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 02:44:28 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 21:19:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhang", "Ye", ""], ["Cao", "Yuan", ""], ["Mahdieh", "Mahdis", ""], ["Zhao", "Jeffrey", ""], ["Wu", "Yonghui", ""]]}, {"id": "2103.00153", "submitter": "Isabelle Augenstein", "authors": "Preslav Nakov, Vibha Nayak, Kyle Dent, Ameya Bhatawdekar, Sheikh\n  Muhammad Sarwar, Momchil Hardalov, Yoan Dinkov, Dimitrina Zlatkova, Guillaume\n  Bouchard, Isabelle Augenstein", "title": "Detecting Abusive Language on Online Platforms: A Critical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abusive language on online platforms is a major societal problem, often\nleading to important societal problems such as the marginalisation of\nunderrepresented minorities. There are many different forms of abusive language\nsuch as hate speech, profanity, and cyber-bullying, and online platforms seek\nto moderate it in order to limit societal harm, to comply with legislation, and\nto create a more inclusive environment for their users. Within the field of\nNatural Language Processing, researchers have developed different methods for\nautomatically detecting abusive language, often focusing on specific\nsubproblems or on narrow communities, as what is considered abusive language\nvery much differs by context. We argue that there is currently a dichotomy\nbetween what types of abusive language online platforms seek to curb, and what\nresearch efforts there are to automatically detect abusive language. We thus\nsurvey existing methods as well as content moderation policies by online\nplatforms in this light, and we suggest directions for future work.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 08:01:10 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nakov", "Preslav", ""], ["Nayak", "Vibha", ""], ["Dent", "Kyle", ""], ["Bhatawdekar", "Ameya", ""], ["Sarwar", "Sheikh Muhammad", ""], ["Hardalov", "Momchil", ""], ["Dinkov", "Yoan", ""], ["Zlatkova", "Dimitrina", ""], ["Bouchard", "Guillaume", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2103.00199", "submitter": "Abdul Hameed Azeemi", "authors": "Abdul Hameed Azeemi, Adeel Waheed", "title": "COVID-19 Tweets Analysis through Transformer Language Models", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the public sentiment and perception in a healthcare crisis is\nessential for developing appropriate crisis management techniques. While some\nstudies have used Twitter data for predictive modelling during COVID-19,\nfine-grained sentiment analysis of the opinion of people on social media during\nthis pandemic has not yet been done. In this study, we perform an in-depth,\nfine-grained sentiment analysis of tweets in COVID-19. For this purpose, we\nperform supervised training of four transformer language models on the\ndownstream task of multi-label classification of tweets into seven tone\nclasses: [confident, anger, fear, joy, sadness, analytical, tentative]. We\nachieve a LRAP (Label Ranking Average Precision) score of 0.9267 through\nRoBERTa. This trained transformer model is able to correctly predict, with high\naccuracy, the tone of a tweet. We then leverage this model for predicting tones\nfor 200,000 tweets on COVID-19. We then perform a country-wise analysis of the\ntone of tweets, and extract useful indicators of the psychological condition\nabout the people in this pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 12:06:33 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Azeemi", "Abdul Hameed", ""], ["Waheed", "Adeel", ""]]}, {"id": "2103.00232", "submitter": "V\\'it Novotn\\'y", "authors": "Eniafe Festus Ayetiran (1), Petr Sojka (1), V\\'it Novotn\\'y (1) ((1)\n  Faculty of Informatics Masaryk University)", "title": "EDS-MEMBED: Multi-sense embeddings based on enhanced distributional\n  semantic structures via a graph walk over word senses", "comments": null, "journal-ref": "Knowledge-Based Systems. 219 (2021) 106902", "doi": "10.1016/j.knosys.2021.106902", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several language applications often require word semantics as a core part of\ntheir processing pipeline, either as precise meaning inference or semantic\nsimilarity. Multi-sense embeddings (M-SE) can be exploited for this important\nrequirement. M-SE seeks to represent each word by their distinct senses in\norder to resolve the conflation of meanings of words as used in different\ncontexts. Previous works usually approach this task by training a model on a\nlarge corpus and often ignore the effect and usefulness of the semantic\nrelations offered by lexical resources. However, even with large training data,\ncoverage of all possible word senses is still an issue. In addition, a\nconsiderable percentage of contextual semantic knowledge are never learned\nbecause a huge amount of possible distributional semantic structures are never\nexplored. In this paper, we leverage the rich semantic structures in WordNet\nusing a graph-theoretic walk technique over word senses to enhance the quality\nof multi-sense embeddings. This algorithm composes enriched texts from the\noriginal texts. Furthermore, we derive new distributional semantic similarity\nmeasures for M-SE from prior ones. We adapt these measures to word sense\ndisambiguation (WSD) aspect of our experiment. We report evaluation results on\n11 benchmark datasets involving WSD and Word Similarity tasks and show that our\nmethod for enhancing distributional semantic structures improves embeddings\nquality on the baselines. Despite the small training data, it achieves\nstate-of-the-art performance on some of the datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 14:36:55 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ayetiran", "Eniafe Festus", ""], ["Sojka", "Petr", ""], ["Novotn\u00fd", "V\u00edt", ""]]}, {"id": "2103.00242", "submitter": "Isabelle Augenstein", "authors": "Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein", "title": "A Survey on Stance Detection for Mis- and Disinformation Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting attitudes expressed in texts, also known as stance detection, has\nbecome an important task for the detection of false information online, be it\nmisinformation (unintentionally false) or disinformation (intentionally false,\nspread deliberately with malicious intent). Stance detection has been framed in\ndifferent ways, including: (a) as a component of fact-checking, rumour\ndetection, and detecting previously fact-checked claims; or (b) as a task in\nits own right. While there have been prior efforts to contrast stance detection\nwith other related social media tasks such as argumentation mining and\nsentiment analysis, there is no survey examining the relationship between\nstance detection detection and mis- and disinformation detection from a\nholistic viewpoint, which is the focus of this survey. We review and analyse\nexisting work in this area, before discussing lessons learnt and future\nchallenges.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:27:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hardalov", "Momchil", ""], ["Arora", "Arnav", ""], ["Nakov", "Preslav", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2103.00293", "submitter": "Taha Aksu", "authors": "Taha Aksu and Zhengyuan Liu and Nancy F. Chen and Min-Yen Kan", "title": "N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking", "comments": "8 pages, 5 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the creation of task-oriented conversational data is costly, data\naugmentation techniques have been proposed to create synthetic data to improve\nmodel performance in new domains. Up to now, these learning-based techniques\n(e.g. paraphrasing) still require a moderate amount of data, making application\nto low-resource settings infeasible. To tackle this problem, we introduce an\naugmentation framework that creates synthetic task-oriented dialogues,\noperating with as few as 5 shots. Our framework utilizes belief state\nannotations to define dialogue functions of each turn pair. It then creates\ntemplates of pairs through de-lexicalization, where the dialogue function\ncodifies the allowable incoming and outgoing links of each template. To\ngenerate new dialogues, our framework composes allowable adjacent templates in\na bottom-up manner. We evaluate our framework using TRADE as the base DST\nmodel, observing significant improvements in the fine-tuning scenarios within a\nlow-resource setting. We conclude that this end-to-end dialogue augmentation\nframework can be a practical tool for natural language understanding\nperformance in emerging task-oriented dialogue domains.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 18:55:12 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 19:07:10 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 17:47:04 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Aksu", "Taha", ""], ["Liu", "Zhengyuan", ""], ["Chen", "Nancy F.", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2103.00324", "submitter": "Manuel Sam Ribeiro", "authors": "Manuel Sam Ribeiro, Joanne Cleland, Aciel Eshky, Korin Richmond, Steve\n  Renals", "title": "Exploiting ultrasound tongue imaging for the automatic detection of\n  speech articulation errors", "comments": "15 pages, 9 figures, 6 tables", "journal-ref": "Speech Communication, Volume 128, April 2021, Pages 24-34", "doi": "10.1016/j.specom.2021.02.001", "report-no": null, "categories": "eess.AS cs.CL cs.SD q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Speech sound disorders are a common communication impairment in childhood.\nBecause speech disorders can negatively affect the lives and the development of\nchildren, clinical intervention is often recommended. To help with diagnosis\nand treatment, clinicians use instrumented methods such as spectrograms or\nultrasound tongue imaging to analyse speech articulations. Analysis with these\nmethods can be laborious for clinicians, therefore there is growing interest in\nits automation. In this paper, we investigate the contribution of ultrasound\ntongue imaging for the automatic detection of speech articulation errors. Our\nsystems are trained on typically developing child speech and augmented with a\ndatabase of adult speech using audio and ultrasound. Evaluation on typically\ndeveloping speech indicates that pre-training on adult speech and jointly using\nultrasound and audio gives the best results with an accuracy of 86.9%. To\nevaluate on disordered speech, we collect pronunciation scores from experienced\nspeech and language therapists, focusing on cases of velar fronting and gliding\nof /r/. The scores show good inter-annotator agreement for velar fronting, but\nnot for gliding errors. For automatic velar fronting error detection, the best\nresults are obtained when jointly using ultrasound and audio. The best system\ncorrectly detects 86.6% of the errors identified by experienced clinicians. Out\nof all the segments identified as errors by the best system, 73.2% match errors\nidentified by clinicians. Results on automatic gliding detection are harder to\ninterpret due to poor inter-annotator agreement, but appear promising. Overall\nfindings suggest that automatic detection of speech articulation errors has\npotential to be integrated into ultrasound intervention software for\nautomatically quantifying progress during speech therapy.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 21:16:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ribeiro", "Manuel Sam", ""], ["Cleland", "Joanne", ""], ["Eshky", "Aciel", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "2103.00333", "submitter": "Manuel Sam Ribeiro", "authors": "Manuel Sam Ribeiro, Aciel Eshky, Korin Richmond, Steve Renals", "title": "Silent versus modal multi-speaker speech recognition from ultrasound and\n  video", "comments": "5 pages, 5 figures, Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate multi-speaker speech recognition from ultrasound images of the\ntongue and video images of the lips. We train our systems on imaging data from\nmodal speech, and evaluate on matched test sets of two speaking modes: silent\nand modal speech. We observe that silent speech recognition from imaging data\nunderperforms compared to modal speech recognition, likely due to a\nspeaking-mode mismatch between training and testing. We improve silent speech\nrecognition performance using techniques that address the domain mismatch, such\nas fMLLR and unsupervised model adaptation. We also analyse the properties of\nsilent and modal speech in terms of utterance duration and the size of the\narticulatory space. To estimate the articulatory space, we compute the convex\nhull of tongue splines, extracted from ultrasound tongue images. Overall, we\nobserve that the duration of silent speech is longer than that of modal speech,\nand that silent speech covers a smaller articulatory space than modal speech.\nAlthough these two properties are statistically significant across speaking\nmodes, they do not directly correlate with word error rates from speech\nrecognition.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 21:34:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ribeiro", "Manuel Sam", ""], ["Eshky", "Aciel", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "2103.00380", "submitter": "Abheesht Sharma", "authors": "Abheesht Sharma and Harshit Pandey", "title": "LRG at TREC 2020: Document Ranking with XLNet-Based Models", "comments": "Published at TREC 2020", "journal-ref": "In Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC\n  2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a good information retrieval system in popular mediums of\nentertainment is a quickly growing area of investigation for companies and\nresearchers alike. We delve into the domain of information retrieval for\npodcasts. In Spotify's Podcast Challenge, we are given a user's query with a\ndescription to find the most relevant short segment from the given dataset\nhaving all the podcasts. Previous techniques that include solely classical\nInformation Retrieval (IR) techniques, perform poorly when descriptive queries\nare presented. On the other hand, models which exclusively rely on large neural\nnetworks tend to perform better. The downside to this technique is that a\nconsiderable amount of time and computing power are required to infer the\nresult. We experiment with two hybrid models which first filter out the best\npodcasts based on user's query with a classical IR technique, and then perform\nre-ranking on the shortlisted documents based on the detailed description using\na transformer-based model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 03:04:29 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 13:49:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sharma", "Abheesht", ""], ["Pandey", "Harshit", ""]]}, {"id": "2103.00422", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Tatsuya Kawahara", "title": "Alignment Knowledge Distillation for Online Streaming Attention-based\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes an efficient training method for online streaming\nattention-based encoder-decoder (AED) automatic speech recognition (ASR)\nsystems. AED models have achieved competitive performance in offline scenarios\nby jointly optimizing all components. They have recently been extended to an\nonline streaming framework via models such as monotonic chunkwise attention\n(MoChA). However, the elaborate attention calculation process is not robust for\nlong-form speech utterances. Moreover, the sequence-level training objective\nand time-restricted streaming encoder cause a nonnegligible delay in token\nemission during inference. To address these problems, we propose CTC\nsynchronous training (CTC-ST), in which CTC alignments are leveraged as a\nreference for token boundaries to enable a MoChA model to learn optimal\nmonotonic input-output alignments. We formulate a purely end-to-end training\nobjective to synchronize the boundaries of MoChA to those of CTC. The CTC model\nshares an encoder with the MoChA model to enhance the encoder representation.\nMoreover, the proposed method provides alignment information learned in the CTC\nbranch to the attention-based decoder. Therefore, CTC-ST can be regarded as\nself-distillation of alignment knowledge from CTC to MoChA. Experimental\nevaluations on a variety of benchmark datasets show that the proposed method\nsignificantly reduces recognition errors and emission latency simultaneously,\nespecially for long-form and noisy speech. We also compare CTC-ST with several\nmethods that distill alignment knowledge from a hybrid ASR system and show that\nthe CTC-ST can achieve a comparable tradeoff of accuracy and latency without\nrelying on external alignment information. The best MoChA system shows\nperformance comparable to that of RNN-transducer (RNN-T).\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:17:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2103.00453", "submitter": "Timo Schick", "authors": "Timo Schick, Sahana Udupa, Hinrich Sch\\\"utze", "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based\n  Bias in NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models often require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we investigate whether pretrained language models\nat least know when they exhibit some undesirable bias or produce toxic content.\nBased on our findings, we propose a decoding algorithm that reduces the\nprobability of a model producing problematic text given only a textual\ndescription of the undesired behavior. This algorithm does not rely on manually\ncurated word lists, nor does it require any training data or changes to the\nmodel's parameters. While our approach does by no means eliminate the issue of\nlanguage models generating biased text, we believe it to be an important step\nin this direction.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:07:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Schick", "Timo", ""], ["Udupa", "Sahana", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2103.00455", "submitter": "Omar Sharif", "authors": "Omar Sharif, Eftekhar Hossain, Mohammed Moshiul Hoque", "title": "NLP-CUET@DravidianLangTech-EACL2021: Offensive Language Detection from\n  Multilingual Code-Mixed Text using Transformers", "comments": "EACL-2021 workshop paper, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing accessibility of the internet facilitated social media usage\nand encouraged individuals to express their opinions liberally. Nevertheless,\nit also creates a place for content polluters to disseminate offensive posts or\ncontents. Most of such offensive posts are written in a cross-lingual manner\nand can easily evade the online surveillance systems. This paper presents an\nautomated system that can identify offensive text from multilingual code-mixed\ndata. In the task, datasets provided in three languages including Tamil,\nMalayalam and Kannada code-mixed with English where participants are asked to\nimplement separate models for each language. To accomplish the tasks, we\nemployed two machine learning techniques (LR, SVM), three deep learning (LSTM,\nLSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R)\nbased methods. Results show that XLM-R outperforms other techniques in Tamil\nand Malayalam languages while m-BERT achieves the highest score in the Kannada\nlanguage. The proposed models gained weighted $f_1$ score of $0.76$ (for\nTamil), $0.93$ (for Malayalam), and $0.71$ (for Kannada) with a rank of\n$3^{rd}$, $5^{th}$ and $4^{th}$ respectively.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:10:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sharif", "Omar", ""], ["Hossain", "Eftekhar", ""], ["Hoque", "Mohammed Moshiul", ""]]}, {"id": "2103.00464", "submitter": "Omar Sharif", "authors": "Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque", "title": "NLP-CUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech Detection\n  using Cross-lingual Representation Learner", "comments": "Winner LT-EDI workshop EACL-2021, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, several systems have been developed to regulate the spread\nof negativity and eliminate aggressive, offensive or abusive contents from the\nonline platforms. Nevertheless, a limited number of researches carried out to\nidentify positive, encouraging and supportive contents. In this work, our goal\nis to identify whether a social media post/comment contains hope speech or not.\nWe propose three distinct models to identify hope speech in English, Tamil and\nMalayalam language to serve this purpose. To attain this goal, we employed\nvarious machine learning (support vector machine, logistic regression,\nensemble), deep learning (convolutional neural network + long short term\nmemory) and transformer (m-BERT, Indic-BERT, XLNet, XLM-Roberta) based methods.\nResults indicate that XLM-Roberta outdoes all other techniques by gaining a\nweighted $f_1$-score of $0.93$, $0.60$ and $0.85$ respectively for English,\nTamil and Malayalam language. Our team has achieved $1^{st}$, $2^{nd}$ and\n$1^{st}$ rank in these three tasks respectively.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:30:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hossain", "Eftekhar", ""], ["Sharif", "Omar", ""], ["Hoque", "Mohammed Moshiul", ""]]}, {"id": "2103.00466", "submitter": "Omar Sharif", "authors": "Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque", "title": "NLP-CUET@DravidianLangTech-EACL2021: Investigating Visual and Textual\n  Features to Identify Trolls from Multimodal Social Media Memes", "comments": "3rd rank DravidianLangTech workshop shared task, EACL-2021, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past few years, the meme has become a new way of communication on the\nInternet. As memes are the images with embedded text, it can quickly spread\nhate, offence and violence. Classifying memes are very challenging because of\ntheir multimodal nature and region-specific interpretation. A shared task is\norganized to develop models that can identify trolls from multimodal social\nmedia memes. This work presents a computational model that we have developed as\npart of our participation in the task. Training data comes in two forms: an\nimage with embedded Tamil code-mixed text and an associated caption given in\nEnglish. We investigated the visual and textual features using CNN, VGG16,\nInception, Multilingual-BERT, XLM-Roberta, XLNet models. Multimodal features\nare extracted by combining image (CNN, ResNet50, Inception) and text (Long\nshort term memory network) features via early fusion approach. Results indicate\nthat the textual approach with XLNet achieved the highest weighted $f_1$-score\nof $0.58$, which enabled our model to secure $3^{rd}$ rank in this task.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:36:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hossain", "Eftekhar", ""], ["Sharif", "Omar", ""], ["Hoque", "Mohammed Moshiul", ""]]}, {"id": "2103.00479", "submitter": "Kishlay Jha", "authors": "Kishlay Jha", "title": "Knowledge-Base Enriched Word Embeddings for Biomedical Domain", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word embeddings have been shown adept at capturing the semantic and syntactic\nregularities of the natural language text, as a result of which these\nrepresentations have found their utility in a wide variety of downstream\ncontent analysis tasks. Commonly, these word embedding techniques derive the\ndistributed representation of words based on the local context information.\nHowever, such approaches ignore the rich amount of explicit information present\nin knowledge-bases. This is problematic, as it might lead to poor\nrepresentation for words with insufficient local context such as domain\nspecific words. Furthermore, the problem becomes pronounced in domain such as\nbio-medicine where the presence of these domain specific words are relatively\nhigh. Towards this end, in this project, we propose a new word embedding based\nmodel for biomedical domain that jointly leverages the information from\navailable corpora and domain knowledge in order to generate knowledge-base\npowered embeddings. Unlike existing approaches, the proposed methodology is\nsimple but adept at capturing the precise knowledge available in domain\nresources in an accurate way. Experimental results on biomedical concept\nsimilarity and relatedness task validates the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:18:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Jha", "Kishlay", ""]]}, {"id": "2103.00482", "submitter": "Yuqi Si", "authors": "Yuqi Si, Elmer V Bernstam, Kirk Roberts", "title": "Generalized and Transferable Patient Language Representation for\n  Phenotyping with Limited Data", "comments": "Journal of Biomedical Informatics (in press)", "journal-ref": null, "doi": "10.1016/j.jbi.2021.103726", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm of representation learning through transfer learning has the\npotential to greatly enhance clinical natural language processing. In this\nwork, we propose a multi-task pre-training and fine-tuning approach for\nlearning generalized and transferable patient representations from medical\nlanguage. The model is first pre-trained with different but related\nhigh-prevalence phenotypes and further fine-tuned on downstream target tasks.\nOur main contribution focuses on the impact this technique can have on\nlow-prevalence phenotypes, a challenging task due to the dearth of data. We\nvalidate the representation from pre-training, and fine-tune the multi-task\npre-trained models on low-prevalence phenotypes including 38 circulatory\ndiseases, 23 respiratory diseases, and 17 genitourinary diseases. We find\nmulti-task pre-training increases learning efficiency and achieves consistently\nhigh performance across the majority of phenotypes. Most important, the\nmulti-task pre-training is almost always either the best-performing model or\nperforms tolerably close to the best-performing model, a property we refer to\nas robust. All these results lead us to conclude that this multi-task transfer\nlearning architecture is a robust approach for developing generalized and\ntransferable patient language representations for numerous phenotypes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:18:02 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Si", "Yuqi", ""], ["Bernstam", "Elmer V", ""], ["Roberts", "Kirk", ""]]}, {"id": "2103.00488", "submitter": "Chunguang Pan", "authors": "Chunguang Pan, Bingyan Song, Shengguang Wang and Zhipeng Luo", "title": "BERT-based Acronym Disambiguation with Multiple Training Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Acronym disambiguation (AD) task aims to find the correct expansions of an\nambiguous ancronym in a given sentence. Although it is convenient to use\nacronyms, sometimes they could be difficult to understand. Identifying the\nappropriate expansions of an acronym is a practical task in natural language\nprocessing. Since few works have been done for AD in scientific field, we\npropose a binary classification model incorporating BERT and several training\nstrategies including dynamic negative sample selection, task adaptive\npretraining, adversarial training and pseudo labeling in this paper.\nExperiments on SciAD show the effectiveness of our proposed model and our score\nranks 1st in SDU@AAAI-21 shared task 2: Acronym Disambiguation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 05:40:21 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 10:36:11 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pan", "Chunguang", ""], ["Song", "Bingyan", ""], ["Wang", "Shengguang", ""], ["Luo", "Zhipeng", ""]]}, {"id": "2103.00492", "submitter": "Zhuo Xu", "authors": "Zhuo Xu", "title": "RoBERTa-wwm-ext Fine-Tuning for Chinese Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bidirectional Encoder Representations from Transformers (BERT) have shown to\nbe a promising way to dramatically improve the performance across various\nNatural Language Processing tasks [Devlin et al., 2019]. Meanwhile, progress\nmade over the past few years by various Neural Net-work has also proved the\neffectiveness of Neural Network in the field of Natural Language Processing. In\nthis project, RoBERTa-wwm-ext [Cui et al., 2019] pre-train language model was\nadopted and fine-tuned for Chinese text classification. The models were able to\nclassify Chinese texts into two categories, containing descriptions of legal\nbehavior and descriptions of illegal behavior. Four different models are also\nproposed in the paper. Those models will use RoBERTa-wwm-extas their embedding\nlayer and feed the embedding into different neural networks. The motivation\nbe-hind proposing these models is straightforward. By introducing complex\noutput layer architecture, the overall performance of the models could be\nimproved. All the models were trained on a data set derived from Chinese public\ncourt records, and the performance of different models were compared.The\nexperiment shows that the performance of pro-posed models failed to beat the\noriginal RoBERTa-wwm-ext model in terms of accuracy and training efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:57:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Zhuo", ""]]}, {"id": "2103.00498", "submitter": "He Zhao", "authors": "He Zhao, Dinh Phung, Viet Huynh, Yuan Jin, Lan Du, Wray Buntine", "title": "Topic Modelling Meets Deep Neural Networks: A Survey", "comments": "A review on Neural Topic Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic modelling has been a successful technique for text analysis for almost\ntwenty years. When topic modelling met deep neural networks, there emerged a\nnew and increasingly popular research area, neural topic models, with over a\nhundred models developed and a wide range of applications in neural language\nunderstanding such as text generation, summarisation and language models. There\nis a need to summarise research developments and discuss open problems and\nfuture directions. In this paper, we provide a focused yet comprehensive\noverview of neural topic models for interested researchers in the AI community,\nso as to facilitate them to navigate and innovate in this fast-growing research\narea. To the best of our knowledge, ours is the first review focusing on this\nspecific topic.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:59:28 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhao", "He", ""], ["Phung", "Dinh", ""], ["Huynh", "Viet", ""], ["Jin", "Yuan", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""]]}, {"id": "2103.00508", "submitter": "Miguel Arana-Catania", "authors": "M. Arana-Catania, F.A. Van Lier, Rob Procter, Nataliya Tkachenko,\n  Yulan He, Arkaitz Zubiaga, Maria Liakata", "title": "Citizen Participation and Machine Learning for a Better Democracy", "comments": "19 pages, 5 figures, 4 tables, to appear in Digital Government:\n  Research and Practice (DGOV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of democratic systems is a crucial task as confirmed by its\nselection as one of the Millennium Sustainable Development Goals by the United\nNations. In this article, we report on the progress of a project that aims to\naddress barriers, one of which is information overload, to achieving effective\ndirect citizen participation in democratic decision-making processes. The main\nobjectives are to explore if the application of Natural Language Processing\n(NLP) and machine learning can improve citizens' experience of digital citizen\nparticipation platforms. Taking as a case study the \"Decide Madrid\" Consul\nplatform, which enables citizens to post proposals for policies they would like\nto see adopted by the city council, we used NLP and machine learning to provide\nnew ways to (a) suggest to citizens proposals they might wish to support; (b)\ngroup citizens by interests so that they can more easily interact with each\nother; (c) summarise comments posted in response to proposals; (d) assist\ncitizens in aggregating and developing proposals. Evaluation of the results\nconfirms that NLP and machine learning have a role to play in addressing some\nof the barriers users of platforms such as Consul currently experience.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:30:07 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Arana-Catania", "M.", ""], ["Van Lier", "F. A.", ""], ["Procter", "Rob", ""], ["Tkachenko", "Nataliya", ""], ["He", "Yulan", ""], ["Zubiaga", "Arkaitz", ""], ["Liakata", "Maria", ""]]}, {"id": "2103.00536", "submitter": "Tanishq Chaudhary", "authors": "Tanishq Chaudhary, Mayank Goel, Radhika Mamidi", "title": "Towards Conversational Humor Analysis and Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Well-defined jokes can be divided neatly into a setup and a punchline. While\nmost works on humor today talk about a joke as a whole, the idea of generating\npunchlines to a setup has applications in conversational humor, where funny\nremarks usually occur with a non-funny context. Thus, this paper is based\naround two core concepts: Classification and the Generation of a punchline from\na particular setup based on the Incongruity Theory. We first implement a\nfeature-based machine learning model to classify humor. For humor generation,\nwe use a neural model, and then merge the classical rule-based approaches with\nthe neural approach to create a hybrid model. The idea behind being: combining\ninsights gained from other tasks with the setup-punchline model and thus\napplying it to existing text generation approaches. We then use and compare our\nmodel with human written jokes with the help of human evaluators in a\ndouble-blind study.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:22:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chaudhary", "Tanishq", ""], ["Goel", "Mayank", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2103.00562", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Wei-Ting Chen, Bowen Zhang, David Lee, J. Harry Caufield,\n  Kai-Wei Chang, Yizhou Sun, Peipei Ping and Wei Wang", "title": "CREATe: Clinical Report Extraction and Annotation Technology", "comments": "7 Figures, ICDE 2021 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical case reports are written descriptions of the unique aspects of a\nparticular clinical case, playing an essential role in sharing clinical\nexperiences about atypical disease phenotypes and new therapies. However, to\nour knowledge, there has been no attempt to develop an end-to-end system to\nannotate, index, or otherwise curate these reports. In this paper, we propose a\nnovel computational resource platform, CREATe, for extracting, indexing, and\nquerying the contents of clinical case reports. CREATe fosters an environment\nof sustainable resource support and discovery, enabling researchers to overcome\nthe challenges of information science. An online video of the demonstration can\nbe viewed at https://youtu.be/Q8owBQYTjDc.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:50:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhou", "Yichao", ""], ["Chen", "Wei-Ting", ""], ["Zhang", "Bowen", ""], ["Lee", "David", ""], ["Caufield", "J. Harry", ""], ["Chang", "Kai-Wei", ""], ["Sun", "Yizhou", ""], ["Ping", "Peipei", ""], ["Wang", "Wei", ""]]}, {"id": "2103.00573", "submitter": "Ekaterina Artemova", "authors": "Vladislav Mikhailov and Ekaterina Taktasheva and Elina Sigdel and\n  Ekaterina Artemova", "title": "RuSentEval: Linguistic Source, Encoder Force!", "comments": "The paper is accepted to BSNLP workshop at EACL 2021. The title\n  follows Power Rangers Mystic Force series (Roll Call Team-Morph: \"Magical\n  Source, Mystic Force!\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of pre-trained transformer language models has brought a great\ndeal of interest on how these models work, and what they learn about language.\nHowever, prior research in the field is mainly devoted to English, and little\nis known regarding other languages. To this end, we introduce RuSentEval, an\nenhanced set of 14 probing tasks for Russian, including ones that have not been\nexplored yet. We apply a combination of complementary probing methods to\nexplore the distribution of various linguistic properties in five multilingual\ntransformers for two typologically contrasting languages -- Russian and\nEnglish. Our results provide intriguing findings that contradict the common\nunderstanding of how linguistic knowledge is represented, and demonstrate that\nsome properties are learned in a similar manner despite the language\ndifferences.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:43:42 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 11:40:25 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Mikhailov", "Vladislav", ""], ["Taktasheva", "Ekaterina", ""], ["Sigdel", "Elina", ""], ["Artemova", "Ekaterina", ""]]}, {"id": "2103.00676", "submitter": "Tom Roth", "authors": "Tom Roth, Yansong Gao, Alsharif Abuadbba, Surya Nepal, Wei Liu", "title": "Token-Modification Adversarial Attacks for Natural Language Processing:\n  A Survey", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are now many adversarial attacks for natural language processing\nsystems. Of these, a vast majority achieve success by modifying individual\ndocument tokens, which we call here a \\textit{token-modification} attack. Each\ntoken-modification attack is defined by a specific combination of fundamental\n\\textit{components}, such as a constraint on the adversary or a particular\nsearch algorithm. Motivated by this observation, we survey existing\ntoken-modification attacks and extract the components of each. We use an\nattack-independent framework to structure our survey which results in an\neffective categorisation of the field and an easy comparison of components. We\nhope this survey will guide new researchers to this field and spark further\nresearch into the individual attack components.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:00:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Roth", "Tom", ""], ["Gao", "Yansong", ""], ["Abuadbba", "Alsharif", ""], ["Nepal", "Surya", ""], ["Liu", "Wei", ""]]}, {"id": "2103.00728", "submitter": "Julia Li", "authors": "Wang Zijia, Li Ye, Zhu Zhongkai", "title": "BERT-based knowledge extraction method of unstructured domain text", "comments": "This article is in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the development and business adoption of knowledge graph, there is an\nincreasing demand for extracting entities and relations of knowledge graphs\nfrom unstructured domain documents. This makes the automatic knowledge\nextraction for domain text quite meaningful. This paper proposes a knowledge\nextraction method based on BERT, which is used to extract knowledge points from\nunstructured specific domain texts (such as insurance clauses in the insurance\nindustry) automatically to save manpower of knowledge graph construction.\nDifferent from the commonly used methods which are based on rules, templates or\nentity extraction models, this paper converts the domain knowledge points into\nquestion and answer pairs and uses the text around the answer in documents as\nthe context. The method adopts a BERT-based model similar to BERT's SQuAD\nreading comprehension task. The model is fine-tuned. And it is used to directly\nextract knowledge points from more insurance clauses. According to the test\nresults, the model performance is good.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 03:24:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zijia", "Wang", ""], ["Ye", "Li", ""], ["Zhongkai", "Zhu", ""]]}, {"id": "2103.00747", "submitter": "Feng Zhou", "authors": "Jackie Ayoub, X. Jessie Yang, Feng Zhou", "title": "Combat COVID-19 Infodemic Using Explainable Natural Language Processing\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Misinformation of COVID-19 is prevalent on social media as the pandemic\nunfolds, and the associated risks are extremely high. Thus, it is critical to\ndetect and combat such misinformation. Recently, deep learning models using\nnatural language processing techniques, such as BERT (Bidirectional Encoder\nRepresentations from Transformers), have achieved great successes in detecting\nmisinformation. In this paper, we proposed an explainable natural language\nprocessing model based on DistilBERT and SHAP (Shapley Additive exPlanations)\nto combat misinformation about COVID-19 due to their efficiency and\neffectiveness. First, we collected a dataset of 984 claims about COVID-19 with\nfact checking. By augmenting the data using back-translation, we doubled the\nsample size of the dataset and the DistilBERT model was able to obtain good\nperformance (accuracy: 0.972; areas under the curve: 0.993) in detecting\nmisinformation about COVID-19. Our model was also tested on a larger dataset\nfor AAAI2021 - COVID-19 Fake News Detection Shared Task and obtained good\nperformance (accuracy: 0.938; areas under the curve: 0.985). The performance on\nboth datasets was better than traditional machine learning models. Second, in\norder to boost public trust in model prediction, we employed SHAP to improve\nmodel explainability, which was further evaluated using a between-subjects\nexperiment with three conditions, i.e., text (T), text+SHAP explanation (TSE),\nand text+SHAP explanation+source and evidence (TSESE). The participants were\nsignificantly more likely to trust and share information related to COVID-19 in\nthe TSE and TSESE conditions than in the T condition. Our results provided good\nimplications in detecting misinformation about COVID-19 and improving public\ntrust.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:28:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ayoub", "Jackie", ""], ["Yang", "X. Jessie", ""], ["Zhou", "Feng", ""]]}, {"id": "2103.00751", "submitter": "Ahsaas Bajaj", "authors": "Ahsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Pradhiksha Ashok\n  Kumar, Rheeya Uppaal, Bradford Windsor, Eliot Brenner, Dominic Dotterrer,\n  Rajarshi Das and Andrew McCallum", "title": "Long Document Summarization in a Low Resource Setting using Pretrained\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization is the task of compressing a long document into a\ncoherent short document while retaining salient information. Modern abstractive\nsummarization methods are based on deep neural networks which often require\nlarge training datasets. Since collecting summarization datasets is an\nexpensive and time-consuming task, practical industrial settings are usually\nlow-resource. In this paper, we study a challenging low-resource setting of\nsummarizing long legal briefs with an average source document length of 4268\nwords and only 120 available (document, summary) pairs. To account for data\nscarcity, we used a modern pretrained abstractive summarizer BART (Lewis et\nal., 2020), which only achieves 17.9 ROUGE-L as it struggles with long\ndocuments. We thus attempt to compress these long documents by identifying\nsalient sentences in the source which best ground the summary, using a novel\nalgorithm based on GPT-2 (Radford et al., 2019) language model perplexity\nscores, that operates within the low resource regime. On feeding the compressed\ndocuments to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats\nseveral competitive salience detection baselines. Furthermore, the identified\nsalient sentences tend to agree with an independent human labeling by domain\nexperts.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:43:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Bajaj", "Ahsaas", ""], ["Dangati", "Pavitra", ""], ["Krishna", "Kalpesh", ""], ["Kumar", "Pradhiksha Ashok", ""], ["Uppaal", "Rheeya", ""], ["Windsor", "Bradford", ""], ["Brenner", "Eliot", ""], ["Dotterrer", "Dominic", ""], ["Das", "Rajarshi", ""], ["McCallum", "Andrew", ""]]}, {"id": "2103.00791", "submitter": "Renbo Zhu", "authors": "Renbo Zhu, Meng Ma, Ping Wang", "title": "RAGA: Relation-aware Graph Attention Networks for Global Entity\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) is the task to discover entities referring to the same\nreal-world object from different knowledge graphs (KGs), which is the most\ncrucial step in integrating multi-source KGs. The majority of the existing\nembeddings-based entity alignment methods embed entities and relations into a\nvector space based on relation triples of KGs for local alignment. As these\nmethods insufficiently consider the multiple relations between entities, the\nstructure information of KGs has not been fully leveraged. In this paper, we\npropose a novel framework based on Relation-aware Graph Attention Networks to\ncapture the interactions between entities and relations. Our framework adopts\nthe self-attention mechanism to spread entity information to the relations and\nthen aggregate relation information back to entities. Furthermore, we propose a\nglobal alignment algorithm to make one-to-one entity alignments with a\nfine-grained similarity matrix. Experiments on three real-world cross-lingual\ndatasets show that our framework outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:30:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhu", "Renbo", ""], ["Ma", "Meng", ""], ["Wang", "Ping", ""]]}, {"id": "2103.00820", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded\n  Dialogues", "comments": "Accepted at ICLR (International Conference on Learning\n  Representations) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compared to traditional visual question answering, video-grounded dialogues\nrequire additional reasoning over dialogue context to answer questions in a\nmulti-turn setting. Previous approaches to video-grounded dialogues mostly use\ndialogue context as a simple text input without modelling the inherent\ninformation flows at the turn level. In this paper, we propose a novel\nframework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers\ninformation flows among dialogue turns through a semantic graph constructed\nbased on lexical components in each question and answer. PDC model then learns\nto predict reasoning paths over this semantic graph. Our path prediction model\npredicts a path from the current turn through past dialogue turns that contain\nadditional visual cues to answer the current question. Our reasoning model\nsequentially processes both visual and textual information through this\nreasoning path and the propagated features are used to generate the answer. Our\nexperimental results demonstrate the effectiveness of our method and provide\nadditional insights on how models use semantic dependencies in a dialogue\ncontext to retrieve visual cues.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:39:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2103.00823", "submitter": "Junyang Lin", "authors": "Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang,\n  Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou,\n  Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin\n  Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, Hongxia Yang", "title": "M6: A Chinese Multimodal Pretrainer", "comments": "12 pages, technical report. Extension of paper \"M6\" accepted to KDD\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we construct the largest dataset for multimodal pretraining in\nChinese, which consists of over 1.9TB images and 292GB texts that cover a wide\nrange of domains. We propose a cross-modal pretraining method called M6,\nreferring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for\nunified pretraining on the data of single modality and multiple modalities. We\nscale the model size up to 10 billion and 100 billion parameters, and build the\nlargest pretrained model in Chinese. We apply the model to a series of\ndownstream applications, and demonstrate its outstanding performance in\ncomparison with strong baselines. Furthermore, we specifically design a\ndownstream task of text-guided image generation, and show that the finetuned M6\ncan create high-quality images with high resolution and abundant details.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:46:27 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 06:03:16 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 04:14:00 GMT"}, {"version": "v4", "created": "Sat, 29 May 2021 09:16:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lin", "Junyang", ""], ["Men", "Rui", ""], ["Yang", "An", ""], ["Zhou", "Chang", ""], ["Ding", "Ming", ""], ["Zhang", "Yichang", ""], ["Wang", "Peng", ""], ["Wang", "Ang", ""], ["Jiang", "Le", ""], ["Jia", "Xianyan", ""], ["Zhang", "Jie", ""], ["Zhang", "Jianwei", ""], ["Zou", "Xu", ""], ["Li", "Zhikang", ""], ["Deng", "Xiaodong", ""], ["Liu", "Jie", ""], ["Xue", "Jinbao", ""], ["Zhou", "Huiling", ""], ["Ma", "Jianxin", ""], ["Yu", "Jin", ""], ["Li", "Yong", ""], ["Lin", "Wei", ""], ["Zhou", "Jingren", ""], ["Tang", "Jie", ""], ["Yang", "Hongxia", ""]]}, {"id": "2103.00854", "submitter": "Rajaswa Patil", "authors": "Rajaswa Patil, Jasleen Dhillon, Siddhant Mahurkar, Saumitra Kulkarni,\n  Manav Malhotra and Veeky Baths", "title": "Vy\\=akarana: A Colorless Green Benchmark for Syntactic Evaluation in\n  Indic Languages", "comments": "Accepted at ACL-IJCNLP SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there has been significant progress towards developing NLU resources\nfor Indic languages, syntactic evaluation has been relatively less explored.\nUnlike English, Indic languages have rich morphosyntax, grammatical genders,\nfree linear word-order, and highly inflectional morphology. In this paper, we\nintroduce Vy\\=akarana: a benchmark of gender-balanced Colorless Green sentences\nin Indic languages for syntactic evaluation of multilingual language models.\nThe benchmark comprises four syntax-related tasks: PoS Tagging, Syntax\nTree-depth Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We\nuse the datasets from the evaluation tasks to probe five multilingual language\nmodels of varying architectures for syntax in Indic languages. Due to its\nprevalence, we also include a code-switching setting in our experiments. Our\nresults show that the token-level and sentence-level representations from the\nIndic language models (IndicBERT and MuRIL) do not capture the syntax in Indic\nlanguages as efficiently as the other highly multilingual language models.\nFurther, our layer-wise probing experiments reveal that while mBERT,\nDistilmBERT, and XLM-R localize the syntax in middle layers, the Indic language\nmodels do not show such syntactic localization.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:07:58 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 15:34:26 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Patil", "Rajaswa", ""], ["Dhillon", "Jasleen", ""], ["Mahurkar", "Siddhant", ""], ["Kulkarni", "Saumitra", ""], ["Malhotra", "Manav", ""], ["Baths", "Veeky", ""]]}, {"id": "2103.00993", "submitter": "Xu Tan", "authors": "Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao,\n  Tie-Yan Liu", "title": "AdaSpeech: Adaptive Text to Speech for Custom Voice", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom voice, a specific text to speech (TTS) service in commercial speech\nplatforms, aims to adapt a source TTS model to synthesize personal voice for a\ntarget speaker using few speech data. Custom voice presents two unique\nchallenges for TTS adaptation: 1) to support diverse customers, the adaptation\nmodel needs to handle diverse acoustic conditions that could be very different\nfrom source speech data, and 2) to support a large number of customers, the\nadaptation parameters need to be small enough for each target speaker to reduce\nmemory usage while maintaining high voice quality. In this work, we propose\nAdaSpeech, an adaptive TTS system for high-quality and efficient customization\nof new voices. We design several techniques in AdaSpeech to address the two\nchallenges in custom voice: 1) To handle different acoustic conditions, we use\ntwo acoustic encoders to extract an utterance-level vector and a sequence of\nphoneme-level vectors from the target speech during training; in inference, we\nextract the utterance-level vector from a reference speech and use an acoustic\npredictor to predict the phoneme-level vectors. 2) To better trade off the\nadaptation parameters and voice quality, we introduce conditional layer\nnormalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this\npart in addition to speaker embedding for adaptation. We pre-train the source\nTTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets\n(with different acoustic conditions from LibriTTS) with few adaptation data,\ne.g., 20 sentences, about 1 minute speech. Experiment results show that\nAdaSpeech achieves much better adaptation quality than baseline methods, with\nonly about 5K specific parameters for each speaker, which demonstrates its\neffectiveness for custom voice. Audio samples are available at\nhttps://speechresearch.github.io/adaspeech/.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:28:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Mingjian", ""], ["Tan", "Xu", ""], ["Li", "Bohan", ""], ["Liu", "Yanqing", ""], ["Qin", "Tao", ""], ["Zhao", "Sheng", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2103.01032", "submitter": "Juliette Millet", "authors": "Juliette Millet, Jean-Remi King", "title": "Inductive biases, pretraining and fine-tuning jointly account for brain\n  responses to speech", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our ability to comprehend speech remains, to date, unrivaled by deep learning\nmodels. This feat could result from the brain's ability to fine-tune generic\nsound representations for speech-specific processes. To test this hypothesis,\nwe compare i) five types of deep neural networks to ii) human brain responses\nelicited by spoken sentences and recorded in 102 Dutch subjects using\nfunctional Magnetic Resonance Imaging (fMRI). Each network was either trained\non an acoustics scene classification, a speech-to-text task (based on Bengali,\nEnglish, or Dutch), or not trained. The similarity between each model and the\nbrain is assessed by correlating their respective activations after an optimal\nlinear projection. The differences in brain-similarity across networks revealed\nthree main results. First, speech representations in the brain can be accounted\nfor by random deep networks. Second, learning to classify acoustic scenes leads\ndeep nets to increase their brain similarity. Third, learning to process\nphonetically-related speech inputs (i.e., Dutch vs English) leads deep nets to\nreach higher levels of brain-similarity than learning to process\nphonetically-distant speech inputs (i.e. Dutch vs Bengali). Together, these\nresults suggest that the human brain fine-tunes its heavily-trained auditory\nhierarchy to learn to process speech.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:11:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Millet", "Juliette", ""], ["King", "Jean-Remi", ""]]}, {"id": "2103.01065", "submitter": "Badr AlKhamissi", "authors": "Badr AlKhamissi, Mohamed Gabr, Muhammad ElNokrashy, Khaled Essam", "title": "Adapting MARBERT for Improved Arabic Dialect Identification: Submission\n  to the NADI 2021 Shared Task", "comments": "This work was accepted at the Sixth Arabic Natural Language\n  Processing Workshop (EACL/WANLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we tackle the Nuanced Arabic Dialect Identification (NADI)\nshared task (Abdul-Mageed et al., 2021) and demonstrate state-of-the-art\nresults on all of its four subtasks. Tasks are to identify the geographic\norigin of short Dialectal (DA) and Modern Standard Arabic (MSA) utterances at\nthe levels of both country and province. Our final model is an ensemble of\nvariants built on top of MARBERT that achieves an F1-score of 34.03% for DA at\nthe country-level development set -- an improvement of 7.63% from previous\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:19:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["AlKhamissi", "Badr", ""], ["Gabr", "Mohamed", ""], ["ElNokrashy", "Muhammad", ""], ["Essam", "Khaled", ""]]}, {"id": "2103.01075", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen\n  Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler", "title": "OmniNet: Omnidirectional Representations from Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Omnidirectional Representations from Transformers\n(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive\nfield, each token is allowed to attend to all tokens in the entire network.\nThis process can also be interpreted as a form of extreme or intensive\nattention mechanism that has the receptive field of the entire width and depth\nof the network. To this end, the omnidirectional attention is learned via a\nmeta-learner, which is essentially another self-attention based model. In order\nto mitigate the computationally expensive costs of full receptive field\nattention, we leverage efficient self-attention models such as kernel-based\n(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer\net al.) as the meta-learner. Extensive experiments are conducted on\nautoregressive language modeling (LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The experiments show that OmniNet achieves\nconsiderable improvements across these tasks, including achieving\nstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation in Vision Transformers leads to\nsignificant improvements on image recognition tasks on both few-shot learning\nand fine-tuning setups.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:31:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Aribandi", "Vamsi", ""], ["Gupta", "Jai", ""], ["Pham", "Philip", ""], ["Qin", "Zhen", ""], ["Bahri", "Dara", ""], ["Juan", "Da-Cheng", ""], ["Metzler", "Donald", ""]]}, {"id": "2103.01126", "submitter": "Andr\\'e Bodmer Dr.", "authors": "Michael Freunek and Andr\\'e Bodmer", "title": "BERT based patent novelty search by training claims to their own\n  description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method to concatenate patent claims to their own\ndescription. By applying this method, BERT trains suitable descriptions for\nclaims. Such a trained BERT (claim-to-description- BERT) could be able to\nidentify novelty relevant descriptions for patents. In addition, we introduce a\nnew scoring scheme, relevance scoring or novelty scoring, to process the output\nof BERT in a meaningful way. We tested the method on patent applications by\ntraining BERT on the first claims of patents and corresponding descriptions.\nBERT's output has been processed according to the relevance score and the\nresults compared with the cited X documents in the search reports. The test\nshowed that BERT has scored some of the cited X documents as highly relevant.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:54:50 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 07:56:25 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 13:39:23 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 07:15:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Freunek", "Michael", ""], ["Bodmer", "Andr\u00e9", ""]]}, {"id": "2103.01169", "submitter": "Luca Maria Aiello", "authors": "Sanja Scepanovic, Luca Maria Aiello, Ke Zhou, Sagar Joglekar, Daniele\n  Quercia", "title": "The Healthy States of America: Creating a Health Taxonomy with Social\n  Media", "comments": "In proceedings of the International Conference on Web and Social\n  Media (ICWSM'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the uptake of social media, researchers have mined online discussions\nto track the outbreak and evolution of specific diseases or chronic conditions\nsuch as influenza or depression. To broaden the set of diseases under study, we\ndeveloped a Deep Learning tool for Natural Language Processing that extracts\nmentions of virtually any medical condition or disease from unstructured social\nmedia text. With that tool at hand, we processed Reddit and Twitter posts,\nanalyzed the clusters of the two resulting co-occurrence networks of\nconditions, and discovered that they correspond to well-defined categories of\nmedical conditions. This resulted in the creation of the first comprehensive\ntaxonomy of medical conditions automatically derived from online discussions.\nWe validated the structure of our taxonomy against the official International\nStatistical Classification of Diseases and Related Health Problems (ICD-11),\nfinding matches of our clusters with 20 official categories, out of 22. Based\non the mentions of our taxonomy's sub-categories on Reddit posts geo-referenced\nin the U.S., we were then able to compute disease-specific health scores. As\nopposed to counts of disease mentions or counts with no knowledge of our\ntaxonomy's structure, we found that our disease-specific health scores are\ncausally linked with the officially reported prevalence of 18 conditions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:07:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Scepanovic", "Sanja", ""], ["Aiello", "Luca Maria", ""], ["Zhou", "Ke", ""], ["Joglekar", "Sagar", ""], ["Quercia", "Daniele", ""]]}, {"id": "2103.01196", "submitter": "Kashif Ahmad", "authors": "Kashif Ahmad, Firoj Alam, Junaid Qadir, Basheer Qolomany, Imran Khan,\n  Talhat Khan, Muhammad Suleman, Naina Said, Syed Zohaib Hassan, Asma Gul, Ala\n  Al-Fuqaha", "title": "Sentiment Analysis of Users' Reviews on COVID-19 Contact Tracing Apps\n  with a Benchmark Dataset", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contact tracing has been globally adopted in the fight to control the\ninfection rate of COVID-19. Thanks to digital technologies, such as smartphones\nand wearable devices, contacts of COVID-19 patients can be easily traced and\ninformed about their potential exposure to the virus. To this aim, several\ninteresting mobile applications have been developed. However, there are\never-growing concerns over the working mechanism and performance of these\napplications. The literature already provides some interesting exploratory\nstudies on the community's response to the applications by analyzing\ninformation from different sources, such as news and users' reviews of the\napplications. However, to the best of our knowledge, there is no existing\nsolution that automatically analyzes users' reviews and extracts the evoked\nsentiments. In this work, we propose a pipeline starting from manual annotation\nvia a crowd-sourcing study and concluding on the development and training of AI\nmodels for automatic sentiment analysis of users' reviews. In total, we employ\neight different methods achieving up to an average F1-Scores 94.8% indicating\nthe feasibility of automatic sentiment analysis of users' reviews on the\nCOVID-19 contact tracing applications. We also highlight the key advantages,\ndrawbacks, and users' concerns over the applications. Moreover, we also collect\nand annotate a large-scale dataset composed of 34,534 reviews manually\nannotated from the contract tracing applications of 46 distinct countries. The\npresented analysis and the dataset are expected to provide a baseline/benchmark\nfor future research in the domain.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:43:10 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ahmad", "Kashif", ""], ["Alam", "Firoj", ""], ["Qadir", "Junaid", ""], ["Qolomany", "Basheer", ""], ["Khan", "Imran", ""], ["Khan", "Talhat", ""], ["Suleman", "Muhammad", ""], ["Said", "Naina", ""], ["Hassan", "Syed Zohaib", ""], ["Gul", "Asma", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "2103.01209", "submitter": "Drew A. Hudson", "authors": "Drew A. Hudson and C. Lawrence Zitnick", "title": "Generative Adversarial Transformers", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:54:04 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:39:04 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 03:13:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hudson", "Drew A.", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "2103.01242", "submitter": "Avia Efrat", "authors": "Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy", "title": "Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in\n  Language", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current NLP datasets targeting ambiguity can be solved by a native speaker\nwith relative ease. We present Cryptonite, a large-scale dataset based on\ncryptic crosswords, which is both linguistically complex and naturally sourced.\nEach example in Cryptonite is a cryptic clue, a short phrase or sentence with a\nmisleading surface reading, whose solving requires disambiguating semantic,\nsyntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues\npose a challenge even for experienced solvers, though top-tier experts can\nsolve them with almost 100% accuracy. Cryptonite is a challenging task for\ncurrent models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6%\naccuracy, on par with the accuracy of a rule-based clue solver (8.6%).\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:01:01 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Efrat", "Avia", ""], ["Shaham", "Uri", ""], ["Kilman", "Dan", ""], ["Levy", "Omer", ""]]}, {"id": "2103.01273", "submitter": "Rob van der Goot", "authors": "Rob van der Goot, Ahmet \\\"Ust\\\"un, Barbara Plank", "title": "On the Effectiveness of Dataset Embeddings in Mono-lingual,Multi-lingual\n  and Zero-shot Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent complementary strands of research have shown that leveraging\ninformation on the data source through encoding their properties into\nembeddings can lead to performance increase when training a single model on\nheterogeneous data sources. However, it remains unclear in which situations\nthese dataset embeddings are most effective, because they are used in a large\nvariety of settings, languages and tasks. Furthermore, it is usually assumed\nthat gold information on the data source is available, and that the test data\nis from a distribution seen during training. In this work, we compare the\neffect of dataset embeddings in mono-lingual settings, multi-lingual settings,\nand with predicted data source label in a zero-shot setting. We evaluate on\nthree morphosyntactic tasks: morphological tagging, lemmatization, and\ndependency parsing, and use 104 datasets, 66 languages, and two different\ndataset grouping strategies. Performance increases are highest when the\ndatasets are of the same language, and we know from which distribution the\ntest-instance is drawn. In contrast, for setups where the data is from an\nunseen distribution, performance increase vanishes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:34:32 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 07:32:04 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["van der Goot", "Rob", ""], ["\u00dcst\u00fcn", "Ahmet", ""], ["Plank", "Barbara", ""]]}, {"id": "2103.01287", "submitter": "Ziming Li", "authors": "Ziming Li and Dookun Park and Julia Kiseleva and Young-Bum Kim and\n  Sungjin Lee", "title": "DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn\n  Dialogues", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital assistants are experiencing rapid growth due to their ability to\nassist users with day-to-day tasks where most dialogues are happening\nmulti-turn. However, evaluating multi-turn dialogues remains challenging,\nespecially at scale. We suggest a context-sensitive method to estimate the\nturn-level satisfaction for dialogue considering various types of user\npreferences. The costs of interactions between users and dialogue systems are\nformulated using a budget consumption concept. We assume users have an initial\ninteraction budget for a dialogue formed based on the task complexity and that\neach turn has a cost. When the task is completed, or the budget has been\nexhausted, users quit the dialogue. We demonstrate our method's effectiveness\nby extensive experimentation with a simulated dialogue platform and real\nmulti-turn dialogues.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:00:28 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 16:06:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Ziming", ""], ["Park", "Dookun", ""], ["Kiseleva", "Julia", ""], ["Kim", "Young-Bum", ""], ["Lee", "Sungjin", ""]]}, {"id": "2103.01328", "submitter": "Tong Xiang", "authors": "Tong Xiang, Sean MacAvaney, Eugene Yang, Nazli Goharian", "title": "ToxCCIn: Toxic Content Classification with Interpretability", "comments": "Long paper accepted to WASSA2021@EACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of transformer-based models in terms of\neffectiveness on a variety of tasks, their decisions often remain opaque to\nhumans. Explanations are particularly important for tasks like offensive\nlanguage or toxicity detection on social media because a manual appeal process\nis often in place to dispute automatically flagged content. In this work, we\npropose a technique to improve the interpretability of these models, based on a\nsimple and powerful assumption: a post is at least as toxic as its most toxic\nspan. We incorporate this assumption into transformer models by scoring a post\nbased on the maximum toxicity of its spans and augmenting the training process\nto identify correct spans. We find this approach effective and can produce\nexplanations that exceed the quality of those provided by Logistic Regression\nanalysis (often regarded as a highly-interpretable model), according to a human\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:17:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Xiang", "Tong", ""], ["MacAvaney", "Sean", ""], ["Yang", "Eugene", ""], ["Goharian", "Nazli", ""]]}, {"id": "2103.01334", "submitter": "Juan Sebasti\\'an Lara Ram\\'irez", "authors": "Juan S. Lara, Mario Ezra Aragon, Fabio A. Gonzalez, Manuel\n  Montes-y-Gomez", "title": "Deep Bag-of-Sub-Emotions for Depression Detection in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the Deep Bag-of-Sub-Emotions (DeepBoSE), a novel deep\nlearning model for depression detection in social media. The model is\nformulated such that it internally computes a differentiable Bag-of-Features\n(BoF) representation that incorporates emotional information. This is achieved\nby a reinterpretation of classical weighting schemes like term\nfrequency-inverse document frequency into probabilistic deep learning\noperations. An important advantage of the proposed method is that it can be\ntrained under the transfer learning paradigm, which is useful to enhance\nconventional BoF models that cannot be directly integrated into deep learning\narchitectures. Experiments were performed in the eRisk17 and eRisk18 datasets\nfor the depression detection task; results show that DeepBoSE outperforms\nconventional BoF representations and it is competitive with the state of the\nart, achieving a F1-score over the positive class of 0.64 in eRisk17 and 0.65\nin eRisk18.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:39:47 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Lara", "Juan S.", ""], ["Aragon", "Mario Ezra", ""], ["Gonzalez", "Fabio A.", ""], ["Montes-y-Gomez", "Manuel", ""]]}, {"id": "2103.01345", "submitter": "Will Hipson", "authors": "Will E. Hipson and Saif M. Mohammad", "title": "Emotion Dynamics in Movie Dialogues", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:02:16 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 13:40:39 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 11:54:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hipson", "Will E.", ""], ["Mohammad", "Saif M.", ""]]}, {"id": "2103.01346", "submitter": "Pengyu Nie", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "title": "Roosterize: Suggesting Lemma Names for Coq Verification Projects Using\n  Deep Learning", "comments": "Accepted in International Conference on Software Engineering,\n  Demonstrations Track (ICSE-DEMO 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naming conventions are an important concern in large verification projects\nusing proof assistants, such as Coq. In particular, lemma names are used by\nproof engineers to effectively understand and modify Coq code. However,\nproviding accurate and informative lemma names is a complex task, which is\ncurrently often carried out manually. Even when lemma naming is automated using\nrule-based tools, generated names may fail to adhere to important conventions\nnot specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which\nautomatically suggests lemma names in Coq projects. Roosterize leverages a\nneural network model trained on existing Coq code, thus avoiding manual\nspecification of naming conventions. To allow proof engineers to conveniently\naccess suggestions from Roosterize during Coq project development, we\nintegrated the toolchain into the popular Visual Studio Code editor. Our\nevaluation shows that Roosterize substantially outperforms strong baselines for\nsuggesting lemma names and is useful in practice. The demo video for Roosterize\ncan be viewed at: https://youtu.be/HZ5ac7Q14rc.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:07:44 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 04:26:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nie", "Pengyu", ""], ["Palmskog", "Karl", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}, {"id": "2103.01378", "submitter": "Alon Jacovi", "authors": "Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin\n  Choi, Yoav Goldberg", "title": "Contrastive Explanations for Model Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:36:45 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Jacovi", "Alon", ""], ["Swayamdipta", "Swabha", ""], ["Ravfogel", "Shauli", ""], ["Elazar", "Yanai", ""], ["Choi", "Yejin", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2103.01399", "submitter": "Aryaman Arora", "authors": "Aryaman Arora, Nitin Venkateswaran, Nathan Schneider", "title": "Hindi-Urdu Adposition and Case Supersenses v1.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  These are the guidelines for the application of SNACS (Semantic Network of\nAdposition and Case Supersenses; Schneider et al. 2018) to Modern Standard\nHindi of Delhi. SNACS is an inventory of 50 supersenses (semantic labels) for\nlabelling the use of adpositions and case markers with respect to both\nlexical-semantic function and relation to the underlying context. The English\nguidelines (Schneider et al., 2020) were used as a model for this document.\n  Besides the case system, Hindi has an extremely rich adpositional system\nbuilt on the oblique genitive, with productive incorporation of loanwords even\nin present-day Hinglish.\n  This document is aligned with version 2.5 of the English guidelines.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:25:02 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Arora", "Aryaman", ""], ["Venkateswaran", "Nitin", ""], ["Schneider", "Nathan", ""]]}, {"id": "2103.01421", "submitter": "Lihao Wang", "authors": "Lihao Wang, Zongyi Li, Xiaoqing Zheng", "title": "Unsupervised Word Segmentation with Bi-directional Neural Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an unsupervised word segmentation model, in which the learning\nobjective is to maximize the generation probability of a sentence given its all\npossible segmentation. Such generation probability can be factorized into the\nlikelihood of each possible segment given the context in a recursive way. In\norder to better capture the long- and short-term dependencies, we propose to\nuse bi-directional neural language models to better capture the features of\nsegment's context. Two decoding algorithms are also described to combine the\ncontext features from both directions to generate the final segmentation, which\nhelps to reconcile word boundary ambiguities. Experimental results showed that\nour context-sensitive unsupervised segmentation model achieved state-of-the-art\nat different evaluation settings on various data sets for Chinese, and the\ncomparable result for Thai.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:21:22 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wang", "Lihao", ""], ["Li", "Zongyi", ""], ["Zheng", "Xiaoqing", ""]]}, {"id": "2103.01534", "submitter": "Yu Cao", "authors": "Yu Cao, Liang Ding, Zhiliang Tian, Meng Fang", "title": "Towards Efficiently Diversifying Dialogue Generation via Embedding\n  Augmentation", "comments": "5 pages, 2 figures, ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue generation models face the challenge of producing generic and\nrepetitive responses. Unlike previous augmentation methods that mostly focus on\ntoken manipulation and ignore the essential variety within a single sample\nusing hard labels, we propose to promote the generation diversity of the neural\ndialogue models via soft embedding augmentation along with soft labels in this\npaper. Particularly, we select some key input tokens and fuse their embeddings\ntogether with embeddings from their semantic-neighbor tokens. The new\nembeddings serve as the input of the model to replace the original one.\nBesides, soft labels are used in loss calculation, resulting in multi-target\nsupervision for a given input. Our experimental results on two datasets\nillustrate that our proposed method is capable of generating more diverse\nresponses than raw models while remains a similar n-gram accuracy that ensures\nthe quality of generated responses.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:28:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cao", "Yu", ""], ["Ding", "Liang", ""], ["Tian", "Zhiliang", ""], ["Fang", "Meng", ""]]}, {"id": "2103.01544", "submitter": "Ashutosh Modi", "authors": "Aaditya Singh and Shreeshail Hingane and Saim Wani and Ashutosh Modi", "title": "An End-to-End Network for Emotion-Cause Pair Extraction", "comments": "Accepted at WASSA-2021, 5 Pages + 2 Pages (references) + 2 Pages\n  (Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all\npotential clause-pairs of emotions and their corresponding causes in a\ndocument. Unlike the more well-studied task of Emotion Cause Extraction (ECE),\nECPE does not require the emotion clauses to be provided as annotations.\nPrevious works on ECPE have either followed a multi-stage approach where\nemotion extraction, cause extraction, and pairing are done independently or use\ncomplex architectures to resolve its limitations. In this paper, we propose an\nend-to-end model for the ECPE task. Due to the unavailability of an English\nlanguage ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline\nfor the ECPE task on this dataset. On this dataset, the proposed method\nproduces significant performance improvements (~6.5 increase in F1 score) over\nthe multi-stage approach and achieves comparable performance to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:03:03 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:57:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Singh", "Aaditya", ""], ["Hingane", "Shreeshail", ""], ["Wani", "Saim", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2103.01576", "submitter": "Jan Philipp Portisch", "authors": "Jan Portisch and Michael Hladik and Heiko Paulheim", "title": "FinMatcher at FinSim-2: Hypernym Detection in the Financial Services\n  Domain using Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the FinMatcher system and its results for the FinSim 2021\nshared task which is co-located with the Workshop on Financial Technology on\nthe Web (FinWeb) in conjunction with The Web Conference. The FinSim-2 shared\ntask consists of a set of concept labels from the financial services domain.\nThe goal is to find the most relevant top-level concept from a given set of\nconcepts. The FinMatcher system exploits three publicly available knowledge\ngraphs, namely WordNet, Wikidata, and WebIsALOD. The graphs are used to\ngenerate explicit features as well as latent features which are fed into a\nneural classifier to predict the closest hypernym.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:56:28 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Portisch", "Jan", ""], ["Hladik", "Michael", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2103.01594", "submitter": "Haolan Zhan", "authors": "Haolan Zhan, Hainan Zhang, Hongshen Chen, Lei Shen, Zhuoye Ding,\n  Yongjun Bao, Weipeng Yan, Yanyan Lan", "title": "Probing Product Description Generation via Posterior Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In product description generation (PDG), the user-cared aspect is critical\nfor the recommendation system, which can not only improve user's experiences\nbut also obtain more clicks. High-quality customer reviews can be considered as\nan ideal source to mine user-cared aspects. However, in reality, a large number\nof new products (known as long-tailed commodities) cannot gather sufficient\namount of customer reviews, which brings a big challenge in the product\ndescription generation task. Existing works tend to generate the product\ndescription solely based on item information, i.e., product attributes or title\nwords, which leads to tedious contents and cannot attract customers\neffectively. To tackle this problem, we propose an adaptive posterior network\nbased on Transformer architecture that can utilize user-cared information from\ncustomer reviews. Specifically, we first extend the self-attentive Transformer\nencoder to encode product titles and attributes. Then, we apply an adaptive\nposterior distillation module to utilize useful review information, which\nintegrates user-cared aspects to the generation process. Finally, we apply a\nTransformer-based decoding phase with copy mechanism to automatically generate\nthe product description. Besides, we also collect a large-scare Chinese product\ndescription dataset to support our work and further research in this field.\nExperimental results show that our model is superior to traditional generative\nmodels in both automatic indicators and human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:38:38 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhan", "Haolan", ""], ["Zhang", "Hainan", ""], ["Chen", "Hongshen", ""], ["Shen", "Lei", ""], ["Ding", "Zhuoye", ""], ["Bao", "Yongjun", ""], ["Yan", "Weipeng", ""], ["Lan", "Yanyan", ""]]}, {"id": "2103.01616", "submitter": "Prashanth Vijayaraghavan", "authors": "Prashanth Vijayaraghavan, Hugo Larochelle, Deb Roy", "title": "Interpretable Multi-Modal Hate Speech Detection", "comments": "5 pages, Accepted at the International Conference on Machine Learning\n  AI for Social Good Workshop, Long Beach, United States, 2019", "journal-ref": "ICML Workshop on AI for Social Good, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With growing role of social media in shaping public opinions and beliefs\nacross the world, there has been an increased attention to identify and counter\nthe problem of hate speech on social media. Hate speech on online spaces has\nserious manifestations, including social polarization and hate crimes. While\nprior works have proposed automated techniques to detect hate speech online,\nthese techniques primarily fail to look beyond the textual content. Moreover,\nfew attempts have been made to focus on the aspects of interpretability of such\nmodels given the social and legal implications of incorrect predictions. In\nthis work, we propose a deep neural multi-modal model that can: (a) detect hate\nspeech by effectively capturing the semantics of the text along with\nsocio-cultural context in which a particular hate expression is made, and (b)\nprovide interpretable insights into decisions of our model. By performing a\nthorough evaluation of different modeling techniques, we demonstrate that our\nmodel is able to outperform the existing state-of-the-art hate speech\nclassification approaches. Finally, we show the importance of social and\ncultural context features towards unearthing clusters associated with different\ncategories of hate.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:12:26 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Vijayaraghavan", "Prashanth", ""], ["Larochelle", "Hugo", ""], ["Roy", "Deb", ""]]}, {"id": "2103.01620", "submitter": "Charlotte Caucheteux", "authors": "Charlotte Caucheteux, Alexandre Gramfort, Jean-Remi King", "title": "Disentangling Syntax and Semantics in the Brain with Deep Networks", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The activations of language transformers like GPT-2 have been shown to\nlinearly map onto brain activity during speech comprehension. However, the\nnature of these activations remains largely unknown and presumably conflate\ndistinct linguistic classes. Here, we propose a taxonomy to factorize the\nhigh-dimensional activations of language models into four combinatorial\nclasses: lexical, compositional, syntactic, and semantic representations. We\nthen introduce a statistical method to decompose, through the lens of GPT-2's\nactivations, the brain activity of 345 subjects recorded with functional\nmagnetic resonance imaging (fMRI) during the listening of ~4.6 hours of\nnarrated text. The results highlight two findings. First, compositional\nrepresentations recruit a more widespread cortical network than lexical ones,\nand encompass the bilateral temporal, parietal and prefrontal cortices. Second,\ncontrary to previous claims, syntax and semantics are not associated with\nseparated modules, but, instead, appear to share a common and distributed\nneural substrate. Overall, this study introduces a versatile framework to\nisolate, in the brain activity, the distributed representations of linguistic\nconstructs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:24:05 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 09:59:36 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Caucheteux", "Charlotte", ""], ["Gramfort", "Alexandre", ""], ["King", "Jean-Remi", ""]]}, {"id": "2103.01661", "submitter": "Meng Li", "authors": "Meng Li, Shiyu Zhou, Bo Xu", "title": "Long-Running Speech Recognizer:An End-to-End Multi-Task Learning\n  Framework for Online ASR and VAD", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we use End-to-end automatic speech recognition (E2E-ASR) system for\nreal-world applications, a voice activity detection (VAD) system is usually\nneeded to improve the performance and to reduce the computational cost by\ndiscarding non-speech parts in the audio. This paper presents a novel\nend-to-end (E2E), multi-task learning (MTL) framework that integrates ASR and\nVAD into one model. The proposed system, which we refer to as Long-Running\nSpeech Recognizer (LR-SR), learns ASR and VAD jointly from two seperate\ntask-specific datasets in the training stage. With the assistance of VAD, the\nASR performance improves as its connectionist temporal classification (CTC)\nloss function can leverage the VAD alignment information. In the inference\nstage, the LR-SR system removes non-speech parts at low computational cost and\nrecognizes speech parts with high robustness. Experimental results on segmented\nspeech data show that the proposed MTL framework outperforms the baseline\nsingle-task learning (STL) framework in ASR task. On unsegmented speech data,\nwe find that the LR-SR system outperforms the baseline ASR systems that build\nan extra GMM-based or DNN-based voice activity detector.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:49:03 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Meng", ""], ["Zhou", "Shiyu", ""], ["Xu", "Bo", ""]]}, {"id": "2103.01664", "submitter": "Roman Klinger", "authors": "Lara Grimminger and Roman Klinger", "title": "Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020\n  US Elections on the Basis of Offensive Speech and Stance Detection", "comments": "WASSA 2021 at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The 2020 US Elections have been, more than ever before, characterized by\nsocial media campaigns and mutual accusations. We investigate in this paper if\nthis manifests also in online communication of the supporters of the candidates\nBiden and Trump, by uttering hateful and offensive communication. We formulate\nan annotation task, in which we join the tasks of hateful/offensive speech\ndetection and stance detection, and annotate 3000 Tweets from the campaign\nperiod, if they express a particular stance towards a candidate. Next to the\nestablished classes of favorable and against, we add mixed and neutral stances\nand also annotate if a candidate is mentioned without an opinion expression.\nFurther, we annotate if the tweet is written in an offensive style. This\nenables us to analyze if supporters of Joe Biden and the Democratic Party\ncommunicate differently than supporters of Donald Trump and the Republican\nParty. A BERT baseline classifier shows that the detection if somebody is a\nsupporter of a candidate can be performed with high quality (.89 F1 for Trump\nand .91 F1 for Biden), while the detection that somebody expresses to be\nagainst a candidate is more challenging (.79 F1 and .64 F1, respectively). The\nautomatic detection of hate/offensive speech remains challenging (with .53 F1).\nOur corpus is publicly available and constitutes a novel resource for\ncomputational modelling of offensive language under consideration of stances.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:59:54 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Grimminger", "Lara", ""], ["Klinger", "Roman", ""]]}, {"id": "2103.01667", "submitter": "Roman Klinger", "authors": "Enrica Troiano and Sebastian Pad\\'o and Roman Klinger", "title": "Emotion Ratings: How Intensity, Annotation Confidence and Agreements are\n  Entangled", "comments": "WASSA 2021 at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When humans judge the affective content of texts, they also implicitly assess\nthe correctness of such judgment, that is, their confidence. We hypothesize\nthat people's (in)confidence that they performed well in an annotation task\nleads to (dis)agreements among each other. If this is true, confidence may\nserve as a diagnostic tool for systematic differences in annotations. To probe\nour assumption, we conduct a study on a subset of the Corpus of Contemporary\nAmerican English, in which we ask raters to distinguish neutral sentences from\nemotion-bearing ones, while scoring the confidence of their answers. Confidence\nturns out to approximate inter-annotator disagreements. Further, we find that\nconfidence is correlated to emotion intensity: perceiving stronger affect in\ntext prompts annotators to more certain classification performances. This\ninsight is relevant for modelling studies of intensity, as it opens the\nquestion wether automatic regressors or classifiers actually predict intensity,\nor rather human's self-perceived confidence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:04:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Troiano", "Enrica", ""], ["Pad\u00f3", "Sebastian", ""], ["Klinger", "Roman", ""]]}, {"id": "2103.01679", "submitter": "Anshul Wadhawan", "authors": "Anshul Wadhawan", "title": "AraBERT and Farasa Segmentation Based Approach For Sarcasm and Sentiment\n  Detection in Arabic Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2:\nSarcasm and Sentiment Detection. One of the subtasks aims at developing a\nsystem that identifies whether a given Arabic tweet is sarcastic in nature or\nnot, while the other aims to identify the sentiment of the Arabic tweet. We\napproach the task in two steps. The first step involves pre processing the\nprovided ArSarcasm-v2 dataset by performing insertions, deletions and\nsegmentation operations on various parts of the text. The second step involves\nexperimenting with multiple variants of two transformer based models,\nAraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the\nSarcasm and Sentiment Detection subtasks respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:33:50 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wadhawan", "Anshul", ""]]}, {"id": "2103.01706", "submitter": "Thomas Hellstr\\\"om", "authors": "Maitreyee Tewari, Thomas Hellstr\\\"om, Suna Bensch", "title": "Conversational Norms for Human-Robot Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a recently initiated research project aiming at\nsupporting development of computerised dialogue systems that handle breaches of\nconversational norms such as the Gricean maxims, which describe how dialogue\nparticipants ideally form their utterances in order to be informative,\nrelevant, brief, etc. Our approach is to model dialogue and norms with\nco-operating distributed grammar systems (CDGSs), and to develop methods to\ndetect breaches and to handle them in dialogue systems for verbal human-robot\ninteraction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:28:18 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tewari", "Maitreyee", ""], ["Hellstr\u00f6m", "Thomas", ""], ["Bensch", "Suna", ""]]}, {"id": "2103.01713", "submitter": "Noortje Venhuizen", "authors": "Noortje J. Venhuizen and Petra Hendriks and Matthew W. Crocker and\n  Harm Brouwer", "title": "Distributional Formal Semantics", "comments": "To appear in: Information and Computation (WoLLIC 2019 Special Issue)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural language semantics has recently sought to combine the complementary\nstrengths of formal and distributional approaches to meaning. More\nspecifically, proposals have been put forward to augment formal semantic\nmachinery with distributional meaning representations, thereby introducing the\nnotion of semantic similarity into formal semantics, or to define\ndistributional systems that aim to incorporate formal notions such as\nentailment and compositionality. However, given the fundamentally different\n'representational currency' underlying formal and distributional approaches -\nmodels of the world versus linguistic co-occurrence - their unification has\nproven extremely difficult. Here, we define a Distributional Formal Semantics\nthat integrates distributionality into a formal semantic system on the level of\nformal models. This approach offers probabilistic, distributed meaning\nrepresentations that are also inherently compositional, and that naturally\ncapture fundamental semantic notions such as quantification and entailment.\nFurthermore, we show how the probabilistic nature of these representations\nallows for probabilistic inference, and how the information-theoretic notion of\n\"information\" (measured in terms of Entropy and Surprisal) naturally follows\nfrom it. Finally, we illustrate how meaning representations can be derived\nincrementally from linguistic input using a recurrent neural network model, and\nhow the resultant incremental semantic construction procedure intuitively\ncaptures key semantic phenomena, including negation, presupposition, and\nanaphoricity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:38:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Venhuizen", "Noortje J.", ""], ["Hendriks", "Petra", ""], ["Crocker", "Matthew W.", ""], ["Brouwer", "Harm", ""]]}, {"id": "2103.01819", "submitter": "Vassilina Nikoulina", "authors": "Vassilina Nikoulina, Maxat Tezekbayev, Nuradil Kozhakhmet, Madina\n  Babazhanova, Matthias Gall\\'e, Zhenisbek Assylbekov", "title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ongoing debate in the NLP community whether modern language\nmodels contain linguistic knowledge, recovered through so-called\n\\textit{probes}. In this paper we study whether linguistic knowledge is a\nnecessary condition for good performance of modern language models, which we\ncall the \\textit{rediscovery hypothesis}.\n  In the first place we show that language models that are significantly\ncompressed but perform well on their pretraining objectives retain good scores\nwhen probed for linguistic structures. This result supports the rediscovery\nhypothesis and leads to the second contribution of our paper: an\ninformation-theoretic framework that relates language modeling objective with\nlinguistic information. This framework also provides a metric to measure the\nimpact of linguistic information on the word prediction task. We reinforce our\nanalytical results with various experiments, both on synthetic and on real\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 15:57:39 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Nikoulina", "Vassilina", ""], ["Tezekbayev", "Maxat", ""], ["Kozhakhmet", "Nuradil", ""], ["Babazhanova", "Madina", ""], ["Gall\u00e9", "Matthias", ""], ["Assylbekov", "Zhenisbek", ""]]}, {"id": "2103.01834", "submitter": "Zhengzhong Liu", "authors": "Zhengzhong Liu, Guanxiong Ding, Avinash Bukkittu, Mansi Gupta, Pengzhi\n  Gao, Atif Ahmed, Shikun Zhang, Xin Gao, Swapnil Singhavi, Linwei Li, Wei Wei,\n  Zecong Hu, Haoran Shi, Xiaodan Liang, Teruko Mitamura, Eric P. Xing, and\n  Zhiting Hu", "title": "A Data-Centric Framework for Composable NLP Workflows", "comments": "8 pages, 4 figures, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte1 (for workflow infrastructure and NLP\nfunction processors) and Stave2 (for user interaction, visualization, and\nannotation).\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:19:44 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:57:35 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liu", "Zhengzhong", ""], ["Ding", "Guanxiong", ""], ["Bukkittu", "Avinash", ""], ["Gupta", "Mansi", ""], ["Gao", "Pengzhi", ""], ["Ahmed", "Atif", ""], ["Zhang", "Shikun", ""], ["Gao", "Xin", ""], ["Singhavi", "Swapnil", ""], ["Li", "Linwei", ""], ["Wei", "Wei", ""], ["Hu", "Zecong", ""], ["Shi", "Haoran", ""], ["Liang", "Xiaodan", ""], ["Mitamura", "Teruko", ""], ["Xing", "Eric P.", ""], ["Hu", "Zhiting", ""]]}, {"id": "2103.01863", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Asli Celikyilmaz, Michel Galley, Chenyan Xiong,\n  Yizhe Zhang, Mohit Bansal, Jianfeng Gao", "title": "Data Augmentation for Abstractive Query-Focused Multi-Document\n  Summarization", "comments": "AAAI 2021 (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress in Query-focused Multi-Document Summarization (QMDS) has been\nlimited by the lack of sufficient largescale high-quality training datasets. We\npresent two QMDS training datasets, which we construct using two data\naugmentation methods: (1) transferring the commonly used single-document\nCNN/Daily Mail summarization dataset to create the QMDSCNN dataset, and (2)\nmining search-query logs to create the QMDSIR dataset. These two datasets have\ncomplementary properties, i.e., QMDSCNN has real summaries but queries are\nsimulated, while QMDSIR has real queries but simulated summaries. To cover both\nthese real summary and query aspects, we build abstractive end-to-end neural\nnetwork models on the combined datasets that yield new state-of-the-art\ntransfer results on DUC datasets. We also introduce new hierarchical encoders\nthat enable a more efficient encoding of the query together with multiple\ndocuments. Empirical results demonstrate that our data augmentation and\nencoding methods outperform baseline models on automatic metrics, as well as on\nhuman evaluations along multiple attributes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:57:01 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Celikyilmaz", "Asli", ""], ["Galley", "Michel", ""], ["Xiong", "Chenyan", ""], ["Zhang", "Yizhe", ""], ["Bansal", "Mohit", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2103.01867", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, David Rosenberg, Gideon Mann, Mohit Bansal", "title": "Dual Reinforcement-Based Specification Generation for Image De-Rendering", "comments": "AAAI 2021 Scientific Document Understanding Workshop (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in deep learning have led to promising progress in inferring\ngraphics programs by de-rendering computer-generated images. However, current\nmethods do not explore which decoding methods lead to better inductive bias for\ninferring graphics programs. In our work, we first explore the effectiveness of\nLSTM-RNN versus Transformer networks as decoders for order-independent graphics\nprograms. Since these are sequence models, we must choose an ordering of the\nobjects in the graphics programs for likelihood training. We found that the\nLSTM performance was highly sensitive to the sequence ordering (random order\nvs. pattern-based order), while Transformer performance was roughly independent\nof the sequence ordering. Further, we present a policy gradient based\nreinforcement learning approach for better inductive bias in the decoder via\nmultiple diverse rewards based both on the graphics program specification and\nthe rendered image. We also explore the combination of these complementary\nrewards. We achieve state-of-the-art results on two graphics program generation\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:04:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Rosenberg", "David", ""], ["Mann", "Gideon", ""], ["Bansal", "Mohit", ""]]}, {"id": "2103.01893", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Yuxuan Wang", "title": "Listen, Read, and Identify: Multimodal Singing Language Identification\n  of Music", "comments": "ISMIR 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a multimodal singing language classification model that uses both\naudio content and textual metadata. LRID-Net, the proposed model, takes an\naudio signal and a language probability vector estimated from the metadata and\noutputs the probabilities of the target languages. Optionally, LRID-Net is\nfacilitated with modality dropouts to handle a missing modality. In the\nexperiment, we trained several LRID-Nets with varying modality dropout\nconfiguration and tested them with various combinations of input modalities.\nThe experiment results demonstrate that using multimodal input improves\nperformance. The results also suggest that adopting modality dropout does not\ndegrade the performance of the model when there are full modality inputs while\nenabling the model to handle missing modality cases to some extent.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:45:04 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 02:10:47 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 08:24:54 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 19:23:41 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Choi", "Keunwoo", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2103.01894", "submitter": "Michael Neumann", "authors": "Michael Neumann and Ngoc Thang Vu", "title": "Investigations on Audiovisual Emotion Recognition in Noisy Conditions", "comments": "Published at the IEEE workshop on Spoken Language Technology (SLT)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we explore audiovisual emotion recognition under noisy acoustic\nconditions with a focus on speech features. We attempt to answer the following\nresearch questions: (i) How does speech emotion recognition perform on noisy\ndata? and (ii) To what extend does a multimodal approach improve the accuracy\nand compensate for potential performance degradation at different noise levels?\nWe present an analytical investigation on two emotion datasets with\nsuperimposed noise at different signal-to-noise ratios, comparing three types\nof acoustic features. Visual features are incorporated with a hybrid fusion\napproach: The first neural network layers are separate modality-specific ones,\nfollowed by at least one shared layer before the final prediction. The results\nshow a significant performance decrease when a model trained on clean audio is\napplied to noisy data and that the addition of visual features alleviates this\neffect.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:45:16 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Neumann", "Michael", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2103.01910", "submitter": "Josiah Wang", "authors": "Josiah Wang, Pranava Madhyastha, Josiel Figueiredo, Chiraag Lala,\n  Lucia Specia", "title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset", "comments": "Manuscript update: (i) Added links to the dataset and evaluation\n  toolkit; (ii) Section 6.1.4: Added random and n-gram baselines to the\n  fill-in-the-blank task, and added further discussion at the end of the\n  section; (iii) Section 6.2.3: Further elaboration on the ALI metric; (iv)\n  Section 6.2.4: Corrected results for the lexical translation task (Table 8),\n  and updated the discussions accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a large-scale multimodal and multilingual dataset that\naims to facilitate research on grounding words to images in their contextual\nusage in language. The dataset consists of images selected to unambiguously\nillustrate concepts expressed in sentences from movie subtitles. The dataset is\na valuable resource as (i) the images are aligned to text fragments rather than\nwhole sentences; (ii) multiple images are possible for a text fragment and a\nsentence; (iii) the sentences are free-form and real-world like; (iv) the\nparallel texts are multilingual. We set up a fill-in-the-blank game for humans\nto evaluate the quality of the automatic image selection process of our\ndataset. We show the utility of the dataset on two automatic tasks: (i)\nfill-in-the blank; (ii) lexical translation. Results of the human evaluation\nand automatic models demonstrate that images can be a useful complement to the\ntextual context. The dataset will benefit research on visual grounding of words\nespecially in the context of free-form sentences, and can be obtained from\nhttps://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:09:07 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:56:02 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wang", "Josiah", ""], ["Madhyastha", "Pranava", ""], ["Figueiredo", "Josiel", ""], ["Lala", "Chiraag", ""], ["Specia", "Lucia", ""]]}, {"id": "2103.01913", "submitter": "Krishna Srinivasan", "authors": "Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky,\n  Marc Najork", "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The milestone improvements brought about by deep representation learning and\npre-training techniques have led to large performance gains across downstream\nNLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large\nhigh-quality visio-linguistic datasets for learning complementary information\n(across image and text modalities). In this paper, we introduce the\nWikipedia-based Image Text (WIT) Dataset\n(https://github.com/google-research-datasets/wit) to better facilitate\nmultimodal, multilingual learning. WIT is composed of a curated set of 37.6\nmillion entity rich image-text examples with 11.5 million unique images across\n108 Wikipedia languages. Its size enables WIT to be used as a pretraining\ndataset for multimodal models, as we show when applied to downstream tasks such\nas image-text retrieval. WIT has four main and unique advantages. First, WIT is\nthe largest multimodal dataset by the number of image-text examples by 3x (at\nthe time of writing). Second, WIT is massively multilingual (first of its kind)\nwith coverage over 100+ languages (each of which has at least 12K examples) and\nprovides cross-lingual texts for many images. Third, WIT represents a more\ndiverse set of concepts and real world entities relative to what previous\ndatasets cover. Lastly, WIT provides a very challenging real-world test set, as\nwe empirically illustrate using an image-text retrieval task as an example.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:13:54 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 16:41:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Srinivasan", "Krishna", ""], ["Raman", "Karthik", ""], ["Chen", "Jiecao", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2103.02141", "submitter": "Chenhao Wang", "authors": "Chenhao Wang, Yubo Chen, Zhipeng Xue, Yang Zhou, Jun Zhao", "title": "CogNet: Bridging Linguistic Knowledge, World Knowledge and Commonsense\n  Knowledge", "comments": "AAAI 2021 Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present CogNet, a knowledge base (KB) dedicated to\nintegrating three types of knowledge: (1) linguistic knowledge from FrameNet,\nwhich schematically describes situations, objects and events. (2) world\nknowledge from YAGO, Freebase, DBpedia and Wikidata, which provides explicit\nknowledge about specific instances. (3) commonsense knowledge from ConceptNet,\nwhich describes implicit general facts. To model these different types of\nknowledge consistently, we introduce a three-level unified frame-styled\nrepresentation architecture. To integrate free-form commonsense knowledge with\nother structured knowledge, we propose a strategy that combines automated\nlabeling and crowdsourced annotation. At present, CogNet integrates 1,000+\nsemantic frames from linguistic KBs, 20,000,000+ frame instances from world\nKBs, as well as 90,000+ commonsense assertions from commonsense KBs. All these\ndata can be easily queried and explored on our online platform, and free to\ndownload in RDF format for utilization under a CC-BY-SA 4.0 license. The demo\nand data are available at http://cognet.top/.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:47:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wang", "Chenhao", ""], ["Chen", "Yubo", ""], ["Xue", "Zhipeng", ""], ["Zhou", "Yang", ""], ["Zhao", "Jun", ""]]}, {"id": "2103.02143", "submitter": "Hao Peng", "authors": "Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith,\n  Lingpeng Kong", "title": "Random Feature Attention", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers are state-of-the-art models for a variety of sequence modeling\ntasks. At their core is an attention function which models pairwise\ninteractions between the inputs at every timestep. While attention is powerful,\nit does not scale efficiently to long sequences due to its quadratic time and\nspace complexity in the sequence length. We propose RFA, a linear time and\nspace attention that uses random feature methods to approximate the softmax\nfunction, and explore its application in transformers. RFA can be used as a\ndrop-in replacement for conventional softmax attention and offers a\nstraightforward way of learning with recency bias through an optional gating\nmechanism. Experiments on language modeling and machine translation demonstrate\nthat RFA achieves similar or better performance compared to strong transformer\nbaselines. In the machine translation experiment, RFA decodes twice as fast as\na vanilla transformer. Compared to existing efficient transformer variants, RFA\nis competitive in terms of both accuracy and efficiency on three long text\nclassification datasets. Our analysis shows that RFA's efficiency gains are\nespecially notable on long sequences, suggesting that RFA will be particularly\nuseful in tasks that require working with large inputs, fast decoding speed, or\nlow memory footprints.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:48:56 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 21:24:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Peng", "Hao", ""], ["Pappas", "Nikolaos", ""], ["Yogatama", "Dani", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah A.", ""], ["Kong", "Lingpeng", ""]]}, {"id": "2103.02190", "submitter": "Marie-Jean Meurs", "authors": "Diego Maupom\\'e and Marie-Jean Meurs", "title": "An Iterative Contextualization Algorithm with Second-Order Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Combining the representations of the words that make up a sentence into a\ncohesive whole is difficult, since it needs to account for the order of words,\nand to establish how the words present relate to each other. The solution we\npropose consists in iteratively adjusting the context. Our algorithm starts\nwith a presumably erroneous value of the context, and adjusts this value with\nrespect to the tokens at hand. In order to achieve this, representations of\nwords are built combining their symbolic embedding with a positional encoding\ninto single vectors. The algorithm then iteratively weighs and aggregates these\nvectors using our novel second-order attention mechanism. Our models report\nstrong results in several well-known text classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:34:50 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Maupom\u00e9", "Diego", ""], ["Meurs", "Marie-Jean", ""]]}, {"id": "2103.02205", "submitter": "Haoran Xu", "authors": "Haoran Xu, Seth Ebner, Mahsa Yarmohammadi, Aaron Steven White,\n  Benjamin Van Durme and Kenton Murray", "title": "Gradual Fine-Tuning for Low-Resource Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning is known to improve NLP models by adapting an initial model\ntrained on more plentiful but less domain-salient examples to data in a target\ndomain. Such domain adaptation is typically done using one stage of\nfine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process\ncan yield substantial further gains and can be applied without modifying the\nmodel or learning objective.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:24:54 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Xu", "Haoran", ""], ["Ebner", "Seth", ""], ["Yarmohammadi", "Mahsa", ""], ["White", "Aaron Steven", ""], ["Van Durme", "Benjamin", ""], ["Murray", "Kenton", ""]]}, {"id": "2103.02212", "submitter": "Haoran Xu", "authors": "Haoran Xu and Philipp Koehn", "title": "Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear embedding transformation has been shown to be effective for zero-shot\ncross-lingual transfer tasks and achieve surprisingly promising results.\nHowever, cross-lingual embedding space mapping is usually studied in static\nword-level embeddings, where a space transformation is derived by aligning\nrepresentations of translation pairs that are referred from dictionaries. We\nmove further from this line and investigate a contextual embedding alignment\napproach which is sense-level and dictionary-free. To enhance the quality of\nthe mapping, we also provide a deep view of properties of contextual\nembeddings, i.e., anisotropy problem and its solution. Experiments on zero-shot\ndependency parsing through the concept-shared space built by our embedding\ntransformation substantially outperform state-of-the-art methods using\nmultilingual embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:50:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xu", "Haoran", ""], ["Koehn", "Philipp", ""]]}, {"id": "2103.02227", "submitter": "Kun Wu", "authors": "Ao Zhang, Kun Wu, Lijie Wang, Zhenghua Li, Xinyan Xiao, Hua Wu, Min\n  Zhang, Haifeng Wang", "title": "Data Augmentation with Hierarchical SQL-to-Question Generation for\n  Cross-domain Text-to-SQL Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation has attracted a lot of research attention in the deep\nlearning era for its ability in alleviating data sparseness. The lack of data\nfor unseen evaluation databases is exactly the major challenge for cross-domain\ntext-to-SQL parsing. Previous works either require human intervention to\nguarantee the quality of generated data, or fail to handle complex SQL queries.\nThis paper presents a simple yet effective data augmentation framework. First,\ngiven a database, we automatically produce a large amount of SQL queries based\non an abstract syntax tree grammar. We require the generated queries cover at\nleast 80% of SQL patterns in the training data for better distribution\nmatching. Second, we propose a hierarchical SQL-to-question generation model to\nobtain high-quality natural language questions, which is the major contribution\nof this work. Experiments on three cross-domain datasets, i.e., WikiSQL and\nSpider in English, and DuSQL in Chinese, show that our proposed data\naugmentation framework can consistently improve performance over strong\nbaselines, and in particular the hierarchical generation model is the key for\nthe improvement.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 07:37:38 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 07:33:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Ao", ""], ["Wu", "Kun", ""], ["Wang", "Lijie", ""], ["Li", "Zhenghua", ""], ["Xiao", "Xinyan", ""], ["Wu", "Hua", ""], ["Zhang", "Min", ""], ["Wang", "Haifeng", ""]]}, {"id": "2103.02252", "submitter": "Aizaz Hussain", "authors": "Aizaz Hussain, Muhammad Umair Arshad", "title": "An Attention Based Neural Network for Code Switching Detection: English\n  & Roman Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-switching is a common phenomenon among people with diverse lingual\nbackground and is widely used on the internet for communication purposes. In\nthis paper, we present a Recurrent Neural Network combined with the Attention\nModel for Language Identification in Code-Switched Data in English and low\nresource Roman Urdu. The attention model enables the architecture to learn the\nimportant features of the languages hence classifying the code switched data.\nWe demonstrated our approach by comparing the results with state of the art\nmodels i.e. Hidden Markov Models, Conditional Random Field and Bidirectional\nLSTM. The models evaluation, using confusion matrix metrics, showed that the\nattention mechanism provides improved the precision and accuracy as compared to\nthe other models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:36:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hussain", "Aizaz", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2103.02262", "submitter": "Runzhe Zhan", "authors": "Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao", "title": "Meta-Curriculum Learning for Domain Adaptation in Neural Machine\n  Translation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-learning has been sufficiently validated to be beneficial for\nlow-resource neural machine translation (NMT). However, we find that\nmeta-trained NMT fails to improve the translation performance of the domain\nunseen at the meta-training stage. In this paper, we aim to alleviate this\nissue by proposing a novel meta-curriculum learning for domain adaptation in\nNMT. During meta-training, the NMT first learns the similar curricula from each\ndomain to avoid falling into a bad local optimum early, and finally learns the\ncurricula of individualities to improve the model robustness for learning\ndomain-specific knowledge. Experimental results on 10 different low-resource\ndomains show that meta-curriculum learning can improve the translation\nperformance of both familiar and unfamiliar domains. All the codes and data are\nfreely available at https://github.com/NLP2CT/Meta-Curriculum.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:58:39 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhan", "Runzhe", ""], ["Liu", "Xuebo", ""], ["Wong", "Derek F.", ""], ["Chao", "Lidia S.", ""]]}, {"id": "2103.02269", "submitter": "Fabio Celli PhD", "authors": "Fabio Celli", "title": "Lex2vec: making Explainable Word Embeddings via Lexical Resources", "comments": "3 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this technical report, we propose an algorithm, called Lex2vec that\nexploits lexical resources to inject information into word embeddings and name\nthe embedding dimensions by means of knowledge bases. We evaluate the optimal\nparameters to extract a number of informative labels that is readable and has a\ngood coverage for the embedding dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:11:18 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 10:18:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Celli", "Fabio", ""]]}, {"id": "2103.02280", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey, Arman\n  Cohan, Nazli Goharian", "title": "Simplified Data Wrangling with ir_datasets", "comments": "SIGIR 2021 Resource", "journal-ref": null, "doi": "10.1145/3404835.3463254", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing the data for Information Retrieval (IR) experiments can be\nchallenging. Dataset documentation is scattered across the Internet and once\none obtains a copy of the data, there are numerous different data formats to\nwork with. Even basic formats can have subtle dataset-specific nuances that\nneed to be considered for proper use. To help mitigate these challenges, we\nintroduce a new robust and lightweight tool (ir_datasets) for acquiring,\nmanaging, and performing typical operations over datasets used in IR. We\nprimarily focus on textual datasets used for ad-hoc search. This tool provides\nboth a Python and command line interface to numerous IR datasets and\nbenchmarks. To our knowledge, this is the most extensive tool of its kind.\nIntegrations with popular IR indexing and experimentation toolkits demonstrate\nthe tool's utility. We also provide documentation of these datasets through the\nir_datasets catalog: https://ir-datasets.com/. The catalog acts as a hub for\ninformation on datasets used in IR, providing core information about what data\neach benchmark provides as well as links to more detailed information. We\nwelcome community contributions and intend to continue to maintain and grow\nthis tool.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:38:36 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 13:13:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["MacAvaney", "Sean", ""], ["Yates", "Andrew", ""], ["Feldman", "Sergey", ""], ["Downey", "Doug", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "2103.02298", "submitter": "Yanpeng Zhao", "authors": "Yanpeng Zhao, Ivan Titov", "title": "An Empirical Study of Compound PCFGs", "comments": "Accepted to Adapt-NLP at EACL 2021. Our code is available at\n  https://github.com/zhaoyanpeng/cpcfg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compound probabilistic context-free grammars (C-PCFGs) have recently\nestablished a new state of the art for phrase-structure grammar induction.\nHowever, due to the high time-complexity of chart-based representation and\ninference, it is difficult to investigate them comprehensively. In this work,\nwe rely on a fast implementation of C-PCFGs to conduct evaluation complementary\nto that of~\\citet{kim-etal-2019-compound}. We highlight three key findings: (1)\nC-PCFGs are data-efficient, (2) C-PCFGs make the best use of global\nsentence-level information in preterminal rule probabilities, and (3) the best\nconfigurations of C-PCFGs on English do not always generalize to\nmorphology-rich languages.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:24:26 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhao", "Yanpeng", ""], ["Titov", "Ivan", ""]]}, {"id": "2103.02333", "submitter": "Cennet Oguz", "authors": "Cennet Oguz, Ngoc Thang Vu", "title": "Few-shot Learning for Slot Tagging with Attentive Relational Network", "comments": "EACL, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Metric-based learning is a well-known family of methods for few-shot\nlearning, especially in computer vision. Recently, they have been used in many\nnatural language processing applications but not for slot tagging. In this\npaper, we explore metric-based learning methods in the slot tagging task and\npropose a novel metric-based learning architecture - Attentive Relational\nNetwork. Our proposed method extends relation networks, making them more\nsuitable for natural language processing applications in general, by leveraging\npretrained contextual embeddings such as ELMO and BERT and by using attention\nmechanism. The results on SNIPS data show that our proposed method outperforms\nother state-of-the-art metric-based learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:24:24 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Oguz", "Cennet", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2103.02336", "submitter": "Claus Weihs", "authors": "Claus Weihs and Sarah Buschfeld", "title": "Combining Prediction and Interpretation in Decision Trees (PrInDT) -- a\n  Linguistic Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that conditional inference trees and ensembles are\nsuitable methods for modeling linguistic variation. As against earlier\nlinguistic applications, however, we claim that their suitability is strongly\nincreased if we combine prediction and interpretation. To that end, we have\ndeveloped a statistical method, PrInDT (Prediction and Interpretation with\nDecision Trees), which we introduce and discuss in the present paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:32:20 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 17:37:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Weihs", "Claus", ""], ["Buschfeld", "Sarah", ""]]}, {"id": "2103.02410", "submitter": "Xiao Liu", "authors": "Xiao Liu, Da Yin, Xingjian Zhang, Kai Su, Kan Wu, Hongxia Yang, Jie\n  Tang", "title": "OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To enrich language models with domain knowledge is crucial but difficult.\nBased on the world's largest public academic graph Open Academic Graph (OAG),\nwe pre-train an academic language model, namely OAG-BERT, which integrates\nmassive heterogeneous entities including paper, author, concept, venue, and\naffiliation. To better endow OAG-BERT with the ability to capture entity\ninformation, we develop novel pre-training strategies including heterogeneous\nentity type embedding, entity-aware 2D positional encoding, and span-aware\nentity masking. For zero-shot inference, we design a special decoding strategy\nto allow OAG-BERT to generate entity names from scratch. We evaluate the\nOAG-BERT on various downstream academic tasks, including NLP benchmarks,\nzero-shot entity inference, heterogeneous graph link prediction, and author\nname disambiguation. Results demonstrate the effectiveness of the proposed\npre-training approach to both comprehending academic texts and modeling\nknowledge from heterogeneous entities. OAG-BERT has been deployed to multiple\nreal-world applications, such as reviewer recommendations and paper tagging in\nthe AMiner system. It is also available to the public through the CogDL\npackage.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:00:57 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 09:40:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Liu", "Xiao", ""], ["Yin", "Da", ""], ["Zhang", "Xingjian", ""], ["Su", "Kai", ""], ["Wu", "Kan", ""], ["Yang", "Hongxia", ""], ["Tang", "Jie", ""]]}, {"id": "2103.02523", "submitter": "Mayank Agarwal", "authors": "Mayank Agarwal, Tathagata Chakraborti, Quchen Fu, David Gros, Xi\n  Victoria Lin, Jaron Maene, Kartik Talamadupula, Zhongwei Teng, Jules White", "title": "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash\n  Commands", "comments": "Competition URL: http://ibm.biz/nlc2cmd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of\nnatural language processing to the command line. Participants were tasked with\nbuilding models that can transform descriptions of command line tasks in\nEnglish to their Bash syntax. This is a report on the competition with details\nof the task, metrics, data, attempted solutions, and lessons learned.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:56:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Agarwal", "Mayank", ""], ["Chakraborti", "Tathagata", ""], ["Fu", "Quchen", ""], ["Gros", "David", ""], ["Lin", "Xi Victoria", ""], ["Maene", "Jaron", ""], ["Talamadupula", "Kartik", ""], ["Teng", "Zhongwei", ""], ["White", "Jules", ""]]}, {"id": "2103.02537", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Cen Chen, W. Bruce Croft, Kalpesh Krishna and Mohit\n  Iyyer", "title": "Weakly-Supervised Open-Retrieval Conversational Question Answering", "comments": "Accepted to ECIR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on Question Answering (QA) and Conversational QA (ConvQA)\nemphasize the role of retrieval: a system first retrieves evidence from a large\ncollection and then extracts answers. This open-retrieval ConvQA setting\ntypically assumes that each question is answerable by a single span of text\nwithin a particular passage (a span answer). The supervision signal is thus\nderived from whether or not the system can recover an exact match of this\nground-truth answer span from the retrieved passages. This method is referred\nto as span-match weak supervision. However, information-seeking conversations\nare challenging for this span-match method since long answers, especially\nfreeform answers, are not necessarily strict spans of any passage. Therefore,\nwe introduce a learned weak supervision approach that can identify a\nparaphrased span of the known answer in a passage. Our experiments on QuAC and\nCoQA datasets show that the span-match weak supervisor can only handle\nconversations with span answers, and has less satisfactory results for freeform\nanswers generated by people. Our method is more flexible as it can handle both\nspan answers and freeform answers. Moreover, our method can be more powerful\nwhen combined with the span-match method which shows it is complementary to the\nspan-match method. We also conduct in-depth analyses to show more insights on\nopen-retrieval ConvQA under a weak supervision setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:23:30 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Chen", "Cen", ""], ["Croft", "W. Bruce", ""], ["Krishna", "Kalpesh", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2103.02548", "submitter": "Xiaoyang Wang", "authors": "Xiaoyang Wang, Chen Li, Jianqiao Zhao, Dong Yu", "title": "NaturalConv: A Chinese Dialogue Dataset Towards Multi-turn Topic-driven\n  Conversation", "comments": "Accepted as a main track paper at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Chinese multi-turn topic-driven conversation\ndataset, NaturalConv, which allows the participants to chat anything they want\nas long as any element from the topic is mentioned and the topic shift is\nsmooth. Our corpus contains 19.9K conversations from six domains, and 400K\nutterances with an average turn number of 20.1. These conversations contain\nin-depth discussions on related topics or widely natural transition between\nmultiple topics. We believe either way is normal for human conversation. To\nfacilitate the research on this corpus, we provide results of several benchmark\nmodels. Comparative results show that for this dataset, our current models are\nnot able to provide significant improvement by introducing background\nknowledge/topic. Therefore, the proposed dataset should be a good benchmark for\nfurther research to evaluate the validity and naturalness of multi-turn\nconversation systems. Our dataset is available at\nhttps://ai.tencent.com/ailab/nlp/dialogue/#datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:38:33 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 17:12:20 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Xiaoyang", ""], ["Li", "Chen", ""], ["Zhao", "Jianqiao", ""], ["Yu", "Dong", ""]]}, {"id": "2103.02585", "submitter": "Sravana Reddy", "authors": "Sravana Reddy, Yongze Yu, Aasish Pappu, Aswin Sivaraman, Rezvaneh\n  Rezapour, Rosie Jones", "title": "Detecting Extraneous Content in Podcasts", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Podcast episodes often contain material extraneous to the main content, such\nas advertisements, interleaved within the audio and the written descriptions.\nWe present classifiers that leverage both textual and listening patterns in\norder to detect such content in podcast descriptions and audio transcripts. We\ndemonstrate that our models are effective by evaluating them on the downstream\ntask of podcast summarization and show that we can substantively improve ROUGE\nscores and reduce the extraneous content generated in the summaries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:30:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Reddy", "Sravana", ""], ["Yu", "Yongze", ""], ["Pappu", "Aasish", ""], ["Sivaraman", "Aswin", ""], ["Rezapour", "Rezvaneh", ""], ["Jones", "Rosie", ""]]}, {"id": "2103.02636", "submitter": "Mandar Gogate", "authors": "Kia Dashtipour, Mandar Gogate, Erik Cambria, Amir Hussain", "title": "A Novel Context-Aware Multimodal Framework for Persian Sentiment\n  Analysis", "comments": "Accepted in Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2021.02.020", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most recent works on sentiment analysis have exploited the text modality.\nHowever, millions of hours of video recordings posted on social media platforms\neveryday hold vital unstructured information that can be exploited to more\neffectively gauge public perception. Multimodal sentiment analysis offers an\ninnovative solution to computationally understand and harvest sentiments from\nvideos by contextually exploiting audio, visual and textual cues. In this\npaper, we, firstly, present a first of its kind Persian multimodal dataset\ncomprising more than 800 utterances, as a benchmark resource for researchers to\nevaluate multimodal sentiment analysis approaches in Persian language.\nSecondly, we present a novel context-aware multimodal sentiment analysis\nframework, that simultaneously exploits acoustic, visual and textual cues to\nmore accurately determine the expressed sentiment. We employ both\ndecision-level (late) and feature-level (early) fusion methods to integrate\naffective cross-modal information. Experimental results demonstrate that the\ncontextual integration of multimodal features such as textual, acoustic and\nvisual features deliver better performance (91.39%) compared to unimodal\nfeatures (89.24%).\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 19:09:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dashtipour", "Kia", ""], ["Gogate", "Mandar", ""], ["Cambria", "Erik", ""], ["Hussain", "Amir", ""]]}, {"id": "2103.02644", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Zhepei Wang, Xilin Jiang and Paris Smaragdis", "title": "Compute and memory efficient universal sound source separation", "comments": "Accepted to Journal of Signal Processing Systems\n  https://www.springer.com/journal/11265. arXiv admin note: substantial text\n  overlap with arXiv:2007.06833", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 19:16:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 23:14:36 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Wang", "Zhepei", ""], ["Jiang", "Xilin", ""], ["Smaragdis", "Paris", ""]]}, {"id": "2103.02691", "submitter": "Waheed Ahmed Abro", "authors": "Waheed Ahmed Abro, Annalena Aicher, Niklas Rach, Stefan Ultes,\n  Wolfgang Minker, Guilin Qi", "title": "Natural Language Understanding for Argumentative Dialogue Systems in the\n  Opinion Building Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a natural language understanding (NLU) framework for\nargumentative dialogue systems in the information-seeking and opinion building\ndomain. Our approach distinguishes multiple user intents and identifies system\narguments the user refers to in his or her natural language utterances. Our\nmodel is applicable in an argumentative dialogue system that allows the user to\ninform him-/herself about and build his/her opinion towards a controversial\ntopic. In order to evaluate the proposed approach, we collect user utterances\nfor the interaction with the respective system and labeled with intent and\nreference argument in an extensive online study. The data collection includes\nmultiple topics and two different user types (native speakers from the UK and\nnon-native speakers from China). The evaluation indicates a clear advantage of\nthe utilized techniques over baseline approaches, as well as a robustness of\nthe proposed approach against new topics and different language proficiency as\nwell as cultural background of the user.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:17:24 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Abro", "Waheed Ahmed", ""], ["Aicher", "Annalena", ""], ["Rach", "Niklas", ""], ["Ultes", "Stefan", ""], ["Minker", "Wolfgang", ""], ["Qi", "Guilin", ""]]}, {"id": "2103.02711", "submitter": "Mark Stamp", "authors": "Aparna Sunil Kale and Fabio Di Troia and Mark Stamp", "title": "Malware Classification with Word Embedding Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malware classification is an important and challenging problem in information\nsecurity. Modern malware classification techniques rely on machine learning\nmodels that can be trained on features such as opcode sequences, API calls, and\nbyte $n$-grams, among many others. In this research, we consider opcode\nfeatures. We implement hybrid machine learning techniques, where we engineer\nfeature vectors by training hidden Markov models -- a technique that we refer\nto as HMM2Vec -- and Word2Vec embeddings on these opcode sequences. The\nresulting HMM2Vec and Word2Vec embedding vectors are then used as features for\nclassification algorithms. Specifically, we consider support vector machine\n(SVM), $k$-nearest neighbor ($k$-NN), random forest (RF), and convolutional\nneural network (CNN) classifiers. We conduct substantial experiments over a\nvariety of malware families. Our experiments extend well beyond any previous\nwork in this field.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:57:11 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kale", "Aparna Sunil", ""], ["Di Troia", "Fabio", ""], ["Stamp", "Mark", ""]]}, {"id": "2103.02728", "submitter": "Cosmin Badea", "authors": "Cosmin Badea, Gregory Artus", "title": "Morality, Machines and the Interpretation Problem: A value-based,\n  Wittgensteinian approach to building Moral Agents", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the attempt to build morality into machines is subject to what\nwe call the Interpretation problem, whereby any rule we give the machine is\nopen to infinite interpretation in ways that we might morally disapprove of,\nand that the interpretation problem in Artificial Intelligence is an\nillustration of Wittgenstein's general claim that no rule can contain the\ncriteria for its own application. Using games as an example, we attempt to\ndefine the structure of normative spaces and argue that any rule-following\nwithin a normative space is guided by values that are external to that space\nand which cannot themselves be represented as rules. In light of this problem,\nwe analyse the types of mistakes an artificial moral agent could make and we\nmake suggestions about how to build morality into machines by getting them to\ninterpret the rules we give in accordance with these external values, through\nexplicit moral reasoning and the presence of structured values, the adjustment\nof causal power assigned to the agent and interaction with human agents, such\nthat the machine develops a virtuous character and the impact of the\ninterpretation problem is minimised.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:34:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Badea", "Cosmin", ""], ["Artus", "Gregory", ""]]}, {"id": "2103.02800", "submitter": "Zejian Liu", "authors": "Zejian Liu, Gang Li and Jian Cheng", "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural\n  Language Processing", "comments": null, "journal-ref": "Design, Automation & Test in Europe (DATE) 2021", "doi": null, "report-no": null, "categories": "cs.AR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BERT is the most recent Transformer-based model that achieves\nstate-of-the-art performance in various NLP tasks. In this paper, we\ninvestigate the hardware acceleration of BERT on FPGA for edge computing. To\ntackle the issue of huge computational complexity and memory footprint, we\npropose to fully quantize the BERT (FQ-BERT), including weights, activations,\nsoftmax, layer normalization, and all the intermediate results. Experiments\ndemonstrate that the FQ-BERT can achieve 7.94x compression for weights with\nnegligible performance loss. We then propose an accelerator tailored for the\nFQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a\nperformance-per-watt of 3.18 fps/W, which is 28.91x and 12.72x over Intel(R)\nCore(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 02:49:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Zejian", ""], ["Li", "Gang", ""], ["Cheng", "Jian", ""]]}, {"id": "2103.02878", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Kexin Wang, Chao Wang, Haiqing Chen, Huan Chen", "title": "An Emotion-controlled Dialog Response Generation Model with Dynamic\n  Vocabulary", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response generation task, proper sentimental expressions can obviously\nimprove the human-like level of the responses. However, for real application in\nonline systems, high QPS (queries per second, an indicator of the flow capacity\nof on-line systems) is required, and a dynamic vocabulary mechanism has been\nproved available in improving speed of generative models. In this paper, we\nproposed an emotion-controlled dialog response generation model based on the\ndynamic vocabulary mechanism, and the experimental results show the benefit of\nthis model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:58:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Song", "Shuangyong", ""], ["Wang", "Kexin", ""], ["Wang", "Chao", ""], ["Chen", "Haiqing", ""], ["Chen", "Huan", ""]]}, {"id": "2103.02895", "submitter": "Daniel Bernau", "authors": "Dominik Wunderlich, Daniel Bernau, Francesco Ald\\`a, Javier\n  Parra-Arnau, Thorsten Strufe", "title": "On the privacy-utility trade-off in differentially private hierarchical\n  text classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models for text classification can leak sensitive or\nconfidential training data information to adversaries due to training data\nmemorization. Using differential privacy during model training can mitigate\nleakage attacks against trained models by perturbing the training optimizer.\nHowever, for hierarchical text classification a multiplicity of model\narchitectures is available and it is unclear whether some architectures yield a\nbetter trade-off between remaining model accuracy and model leakage under\ndifferentially private training perturbation than others. We use a white-box\nmembership inference attack to assess the information leakage of three widely\nused neural network architectures for hierarchical text classification under\ndifferential privacy. We show that relatively weak differential privacy\nguarantees already suffice to completely mitigate the membership inference\nattack, thus resulting only in a moderate decrease in utility. More\nspecifically, for large datasets with long texts we observed transformer-based\nmodels to achieve an overall favorable privacy-utility trade-off, while for\nsmaller datasets with shorter texts CNNs are preferable.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:51:00 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wunderlich", "Dominik", ""], ["Bernau", "Daniel", ""], ["Ald\u00e0", "Francesco", ""], ["Parra-Arnau", "Javier", ""], ["Strufe", "Thorsten", ""]]}, {"id": "2103.02917", "submitter": "Kalina Bontcheva", "authors": "Tracie Farrell, Mehmet Bakir, Kalina Bontcheva", "title": "MP Twitter Engagement and Abuse Post-first COVID-19 Lockdown in the UK:\n  White Paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The UK has had a volatile political environment for some years now, with\nBrexit and leadership crises marking the past five years. With this work, we\nwanted to understand more about how the global health emergency, COVID-19,\ninfluences the amount, type or topics of abuse that UK politicians receive when\nengaging with the public. With this work, we wanted to understand more about\nhow the global health emergency, COVID-19, influences the amount, type or\ntopics of abuse that UK politicians receive when engaging with the public. This\nwork covers the period of June to December 2020 and analyses Twitter abuse in\nreplies to UK MPs. This work is a follow-up from our analysis of online abuse\nduring the first four months of the COVID-19 pandemic in the UK. The paper\nexamines overall abuse levels during this new seven month period, analyses\nreactions to members of different political parties and the UK government, and\nthe relationship between online abuse and topics such as Brexit, government's\nCOVID-19 response and policies, and social issues. In addition, we have also\nexamined the presence of conspiracy theories posted in abusive replies to MPs\nduring the period. We have found that abuse levels toward UK MPs were at an\nall-time high in December 2020 (5.4% of all reply tweets sent to MPs). This is\nalmost 1% higher that the two months preceding the General Election. In a\ndeparture from the trend seen in the first four months of the pandemic, MPs\nfrom the Tory party received the highest percentage of abusive replies from\nJuly 2020 onward, which stays above 5% starting from September 2020 onward, as\nthe COVID-19 crisis deepened and the Brexit negotiations with the EU started\nnearing completion.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:45:00 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 10:45:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Farrell", "Tracie", ""], ["Bakir", "Mehmet", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "2103.03095", "submitter": "Libo Qin", "authors": "Libo Qin, Tianbao Xie, Wanxiang Che, Ting Liu", "title": "A Survey on Spoken Language Understanding: Recent Advances and New\n  Frontiers", "comments": "Accepted at IJCAI2021. Resources in\n  \\url{https://github.com/yizhen20133868/Awesome-SLU-Survey}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Understanding (SLU) aims to extract the semantics frame of\nuser queries, which is a core component in a task-oriented dialog system. With\nthe burst of deep neural networks and the evolution of pre-trained language\nmodels, the research of SLU has obtained significant breakthroughs. However,\nthere remains a lack of a comprehensive survey summarizing existing approaches\nand recent trends, which motivated the work presented in this article. In this\npaper, we survey recent advances and new frontiers in SLU. Specifically, we\ngive a thorough review of this research field, covering different aspects\nincluding (1) new taxonomy: we provide a new perspective for SLU filed,\nincluding single model vs. joint model, implicit joint modeling vs. explicit\njoint modeling in joint model, non pre-trained paradigm vs. pre-trained\nparadigm;(2) new frontiers: some emerging areas in complex SLU as well as the\ncorresponding challenges; (3) abundant open-source resources: to help the\ncommunity, we have collected, organized the related papers, baseline projects\nand leaderboard on a public website where SLU researchers could directly access\nto the recent progress. We hope that this survey can shed a light on future\nresearch in SLU field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:22:00 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 11:53:04 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Qin", "Libo", ""], ["Xie", "Tianbao", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2103.03108", "submitter": "Hamidreza Ghader", "authors": "Hamidreza Ghader", "title": "An empirical analysis of phrase-based and neural machine translation", "comments": "PhD thesis, University of Amsterdam, October 2020.\n  https://pure.uva.nl/ws/files/51388868/Thesis.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Two popular types of machine translation (MT) are phrase-based and neural\nmachine translation systems. Both of these types of systems are composed of\nmultiple complex models or layers. Each of these models and layers learns\ndifferent linguistic aspects of the source language. However, for some of these\nmodels and layers, it is not clear which linguistic phenomena are learned or\nhow this information is learned. For phrase-based MT systems, it is often clear\nwhat information is learned by each model, and the question is rather how this\ninformation is learned, especially for its phrase reordering model. For neural\nmachine translation systems, the situation is even more complex, since for many\ncases it is not exactly clear what information is learned and how it is\nlearned.\n  To shed light on what linguistic phenomena are captured by MT systems, we\nanalyze the behavior of important models in both phrase-based and neural MT\nsystems. We consider phrase reordering models from phrase-based MT systems to\ninvestigate which words from inside of a phrase have the biggest impact on\ndefining the phrase reordering behavior. Additionally, to contribute to the\ninterpretability of neural MT systems we study the behavior of the attention\nmodel, which is a key component in neural MT systems and the closest model in\nfunctionality to phrase reordering models in phrase-based systems. The\nattention model together with the encoder hidden state representations form the\nmain components to encode source side linguistic information in neural MT. To\nthis end, we also analyze the information captured in the encoder hidden state\nrepresentations of a neural MT system. We investigate the extent to which\nsyntactic and lexical-semantic information from the source side is captured by\nhidden state representations of different neural MT architectures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:28:28 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ghader", "Hamidreza", ""]]}, {"id": "2103.03125", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang and Hai Zhao", "title": "Advances in Multi-turn Dialogue Comprehension: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machines to understand natural language and interact with humans is\nan elusive and essential task in the field of artificial intelligence. In\nrecent years, a diversity of dialogue systems has been designed with the rapid\ndevelopment of deep learning researches, especially the recent pre-trained\nlanguage models. Among these studies, the fundamental yet challenging part is\ndialogue comprehension whose role is to teach the machines to read and\ncomprehend the dialogue context before responding. In this paper, we review the\nprevious methods from the perspective of dialogue modeling. We summarize the\ncharacteristics and challenges of dialogue comprehension in contrast to\nplain-text reading comprehension. Then, we discuss three typical patterns of\ndialogue modeling that are widely-used in dialogue comprehension tasks such as\nresponse selection and conversation question-answering, as well as\ndialogue-related language modeling techniques to enhance PrLMs in dialogue\nscenarios. Finally, we highlight the technical advances in recent years and\npoint out the lessons we can learn from the empirical analysis and the\nprospects towards a new frontier of researches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:50:17 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2103.03142", "submitter": "Abhijeet Awasthi", "authors": "Abhijeet Awasthi, Aman Kansal, Sunita Sarawagi, Preethi Jyothi", "title": "Error-driven Fixed-Budget ASR Personalization for Accented Speakers", "comments": "In ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of personalizing ASR models while being constrained by a\nfixed budget on recording speaker-specific utterances. Given a speaker and an\nASR model, we propose a method of identifying sentences for which the speaker's\nutterances are likely to be harder for the given ASR model to recognize. We\nassume a tiny amount of speaker-specific data to learn phoneme-level error\nmodels which help us select such sentences. We show that speaker's utterances\non the sentences selected using our error model indeed have larger error rates\nwhen compared to speaker's utterances on randomly selected sentences. We find\nthat fine-tuning the ASR model on the sentence utterances selected with the\nhelp of error models yield higher WER improvements in comparison to fine-tuning\non an equal number of randomly selected sentence utterances. Thus, our method\nprovides an efficient way of collecting speaker utterances under budget\nconstraints for personalizing ASR models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 16:36:59 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 17:13:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Awasthi", "Abhijeet", ""], ["Kansal", "Aman", ""], ["Sarawagi", "Sunita", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2103.03233", "submitter": "Ha Nguyen", "authors": "Ha Nguyen, Yannick Est\\`eve, Laurent Besacier", "title": "An Empirical Study of End-to-end Simultaneous Speech Translation\n  Decoding Strategies", "comments": "This paper has been accepted for presentation at IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decoding strategy for end-to-end simultaneous speech\ntranslation. We leverage end-to-end models trained in offline mode and conduct\nan empirical study for two language pairs (English-to-German and\nEnglish-to-Portuguese). We also investigate different output token\ngranularities including characters and Byte Pair Encoding (BPE) units. The\nresults show that the proposed decoding approach allows to control BLEU/Average\nLagging trade-off along different latency regimes. Our best decoding settings\nachieve comparable results with a strong cascade model evaluated on the\nsimultaneous translation track of IWSLT 2020 shared task.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:55:40 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Nguyen", "Ha", ""], ["Est\u00e8ve", "Yannick", ""], ["Besacier", "Laurent", ""]]}, {"id": "2103.03296", "submitter": "Atharva Kulkarni", "authors": "Atharva Kulkarni, Sunanda Somwase, Shivam Rajput, and Manisha Marathe", "title": "PVG at WASSA 2021: A Multi-Input, Multi-Task, Transformer-Based\n  Architecture for Empathy and Distress Prediction", "comments": "Accepted at the 11th Workshop on Computational Approaches to\n  Subjectivity, Sentiment and Social Media Analysis (WASSA 2021), co-located\n  with EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Active research pertaining to the affective phenomenon of empathy and\ndistress is invaluable for improving human-machine interaction. Predicting\nintensities of such complex emotions from textual data is difficult, as these\nconstructs are deeply rooted in the psychological theory. Consequently, for\nbetter prediction, it becomes imperative to take into account ancillary factors\nsuch as the psychological test scores, demographic features, underlying latent\nprimitive emotions, along with the text's undertone and its psychological\ncomplexity. This paper proffers team PVG's solution to the WASSA 2021 Shared\nTask on Predicting Empathy and Emotion in Reaction to News Stories. Leveraging\nthe textual data, demographic features, psychological test score, and the\nintrinsic interdependencies of primitive emotions and empathy, we propose a\nmulti-input, multi-task framework for the task of empathy score prediction.\nHere, the empathy score prediction is considered the primary task, while\nemotion and empathy classification are considered secondary auxiliary tasks.\nFor the distress score prediction task, the system is further boosted by the\naddition of lexical features. Our submission ranked 1$^{st}$ based on the\naverage correlation (0.545) as well as the distress correlation (0.574), and\n2$^{nd}$ for the empathy Pearson correlation (0.517).\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:12:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kulkarni", "Atharva", ""], ["Somwase", "Sunanda", ""], ["Rajput", "Shivam", ""], ["Marathe", "Manisha", ""]]}, {"id": "2103.03335", "submitter": "Leonid Boytsov", "authors": "Iurii Mokrii, Leonid Boytsov, Pavel Braslavski", "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with\n  BERT-based Ranking Models", "comments": null, "journal-ref": "SIGIR 2021 (44th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval)", "doi": "10.1145/3404835.3463093", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 21:08:06 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 16:34:14 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 03:18:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mokrii", "Iurii", ""], ["Boytsov", "Leonid", ""], ["Braslavski", "Pavel", ""]]}, {"id": "2103.03373", "submitter": "Sunghyun Park", "authors": "Han Li, Sunghyun Park, Aswarth Dara, Jinseok Nam, Sungjin Lee,\n  Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya", "title": "Neural model robustness for skill routing in large-scale conversational\n  AI systems: A design choice exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art large-scale conversational AI or intelligent digital\nassistant systems in industry comprises a set of components such as Automatic\nSpeech Recognition (ASR) and Natural Language Understanding (NLU). For some of\nthese systems that leverage a shared NLU ontology (e.g., a centralized\nintent/slot schema), there exists a separate skill routing component to\ncorrectly route a request to an appropriate skill, which is either a\nfirst-party or third-party application that actually executes on a user\nrequest. The skill routing component is needed as there are thousands of skills\nthat can either subscribe to the same intent and/or subscribe to an intent\nunder specific contextual conditions (e.g., device has a screen). Ensuring\nmodel robustness or resilience in the skill routing component is an important\nproblem since skills may dynamically change their subscription in the ontology\nafter the skill routing model has been deployed to production. We show how\ndifferent modeling design choices impact the model robustness in the context of\nskill routing on a state-of-the-art commercial conversational AI system,\nspecifically on the choices around data augmentation, model architecture, and\noptimization method. We show that applying data augmentation can be a very\neffective and practical way to drastically improve model robustness.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 22:54:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Li", "Han", ""], ["Park", "Sunghyun", ""], ["Dara", "Aswarth", ""], ["Nam", "Jinseok", ""], ["Lee", "Sungjin", ""], ["Kim", "Young-Bum", ""], ["Matsoukas", "Spyros", ""], ["Sarikaya", "Ruhi", ""]]}, {"id": "2103.03446", "submitter": "Jialong Tang", "authors": "Jinsong Su, Jialong Tang, Hui Jiang, Ziyao Lu, Yubin Ge, Linfeng Song,\n  Deyi Xiong, Le Sun, Jiebo Luo", "title": "Enhanced Aspect-Based Sentiment Analysis Models with Progressive\n  Self-supervised Attention Learning", "comments": "31 pages. arXiv admin note: text overlap with arXiv:1906.01213", "journal-ref": "Artificial Intelligence 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In aspect-based sentiment analysis (ABSA), many neural models are equipped\nwith an attention mechanism to quantify the contribution of each context word\nto sentiment prediction. However, such a mechanism suffers from one drawback:\nonly a few frequent words with sentiment polarities are tended to be taken into\nconsideration for final sentiment decision while abundant infrequent sentiment\nwords are ignored by models. To deal with this issue, we propose a progressive\nself-supervised attention learning approach for attentional ABSA models. In\nthis approach, we iteratively perform sentiment prediction on all training\ninstances, and continually learn useful attention supervision information in\nthe meantime. During training, at each iteration, context words with the\nhighest impact on sentiment prediction, identified based on their attention\nweights or gradients, are extracted as words with active/misleading influence\non the correct/incorrect prediction for each instance. Words extracted in this\nway are masked for subsequent iterations. To exploit these extracted words for\nrefining ABSA models, we augment the conventional training objective with a\nregularization term that encourages ABSA models to not only take full advantage\nof the extracted active context words but also decrease the weights of those\nmisleading words. We integrate the proposed approach into three\nstate-of-the-art neural ABSA models. Experiment results and in-depth analyses\nshow that our approach yields better attention results and significantly\nenhances the performance of all three models. We release the source code and\ntrained models at https://github.com/DeepLearnXMU/PSSAttention.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:50:05 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Su", "Jinsong", ""], ["Tang", "Jialong", ""], ["Jiang", "Hui", ""], ["Lu", "Ziyao", ""], ["Ge", "Yubin", ""], ["Song", "Linfeng", ""], ["Xiong", "Deyi", ""], ["Sun", "Le", ""], ["Luo", "Jiebo", ""]]}, {"id": "2103.03448", "submitter": "Jialong Tang", "authors": "Jialong Tang, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Xinyan Xiao,\n  Hua Wu", "title": "Syntactic and Semantic-driven Learning for Open Information Extraction", "comments": "11 pages", "journal-ref": "Findings of ACL: EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest bottlenecks in building accurate, high coverage neural\nopen IE systems is the need for large labelled corpora. The diversity of open\ndomain corpora and the variety of natural language expressions further\nexacerbate this problem. In this paper, we propose a syntactic and\nsemantic-driven learning approach, which can learn neural open IE models\nwithout any human-labelled data by leveraging syntactic and semantic knowledge\nas noisier, higher-level supervisions. Specifically, we first employ syntactic\npatterns as data labelling functions and pretrain a base model using the\ngenerated labels. Then we propose a syntactic and semantic-driven reinforcement\nlearning algorithm, which can effectively generalize the base model to open\nsituations with high accuracy. Experimental results show that our approach\nsignificantly outperforms the supervised counterparts, and can even achieve\ncompetitive performance to supervised state-of-the-art (SoA) model\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:59:40 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tang", "Jialong", ""], ["Lu", "Yaojie", ""], ["Lin", "Hongyu", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Xiao", "Xinyan", ""], ["Wu", "Hua", ""]]}, {"id": "2103.03457", "submitter": "Lijun Wu", "authors": "Jinhua Zhu, Lijun Wu, Yingce Xia, Shufang Xie, Tao Qin, Wengang Zhou,\n  Houqiang Li, Tie-Yan Liu", "title": "IOT: Instance-wise Layer Reordering for Transformer Structures", "comments": "Accepted at ICLR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With sequentially stacked self-attention, (optional) encoder-decoder\nattention, and feed-forward layers, Transformer achieves big success in natural\nlanguage processing (NLP), and many variants have been proposed. Currently,\nalmost all these models assume that the layer order is fixed and kept the same\nacross data samples. We observe that different data samples actually favor\ndifferent orders of the layers. Based on this observation, in this work, we\nbreak the assumption of the fixed layer order in the Transformer and introduce\ninstance-wise layer reordering into the model structure. Our Instance-wise\nOrdered Transformer (IOT) can model variant functions by reordered layers,\nwhich enables each sample to select the better one to improve the model\nperformance under the constraint of almost the same number of parameters. To\nachieve this, we introduce a light predictor with negligible parameter and\ninference cost to decide the most capable and favorable layer order for any\ninput sequence. Experiments on 3 tasks (neural machine translation, abstractive\nsummarization, and code generation) and 9 datasets demonstrate consistent\nimprovements of our method. We further show that our method can also be applied\nto other architectures beyond Transformer. Our code is released at Github.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:44:42 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhu", "Jinhua", ""], ["Wu", "Lijun", ""], ["Xia", "Yingce", ""], ["Xie", "Shufang", ""], ["Qin", "Tao", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2103.03509", "submitter": "Seongsik Park", "authors": "Seongsik Park and Harksoo Kim", "title": "Dual Pointer Network for Fast Extraction of Multiple Relations in a\n  Sentence", "comments": null, "journal-ref": "Applied Sciences (SI: Natural Language Processing: Emerging Neural\n  Approaches and Applications), Vol.10(11), 2020", "doi": "10.3390/app10113851", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Relation extraction is a type of information extraction task that recognizes\nsemantic relationships between entities in a sentence. Many previous studies\nhave focused on extracting only one semantic relation between two entities in a\nsingle sentence. However, multiple entities in a sentence are associated\nthrough various relations. To address this issue, we propose a relation\nextraction model based on a dual pointer network with a multi-head attention\nmechanism. The proposed model finds n-to-1 subject-object relations using a\nforward object decoder. Then, it finds 1-to-n subject-object relations using a\nbackward subject decoder. Our experiments confirmed that the proposed model\noutperformed previous models, with an F1-score of 80.8% for the ACE-2005 corpus\nand an F1-score of 78.3% for the NYT corpus.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:36:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Park", "Seongsik", ""], ["Kim", "Harksoo", ""]]}, {"id": "2103.03541", "submitter": "Mutian He", "authors": "Mutian He, Jingzhou Yang, Lei He, Frank K. Soong", "title": "Multilingual Byte2Speech Models for Scalable Low-resource Speech\n  Synthesis", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To scale neural speech synthesis to various real-world languages, we present\na multilingual end-to-end framework that maps byte inputs to spectrograms, thus\nallowing arbitrary input scripts. Besides strong results on 40+ languages, the\nframework demonstrates capabilities to adapt to new languages under extreme\nlow-resource and even few-shot scenarios of merely 40s transcribed recording,\nwithout the need of per-language resources like lexicon, extra corpus,\nauxiliary models, or linguistic expertise, thus ensuring scalability. While it\nretains satisfactory intelligibility and naturalness matching rich-resource\nmodels. Exhaustive comparative and ablation studies are performed to reveal the\npotential of the framework for low-resource languages. Furthermore, we propose\na novel method to extract language-specific sub-networks in a multilingual\nmodel for a better understanding of its mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:41:45 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 11:29:43 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["He", "Mutian", ""], ["Yang", "Jingzhou", ""], ["He", "Lei", ""], ["Soong", "Frank K.", ""]]}, {"id": "2103.03580", "submitter": "Sara Durrani", "authors": "Sara Durrani, Muhammad Umair Arshad", "title": "Transfer Learning based Speech Affect Recognition in Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been established that Speech Affect Recognition for low resource\nlanguages is a difficult task. Here we present a Transfer learning based Speech\nAffect Recognition approach in which: we pre-train a model for high resource\nlanguage affect recognition task and fine tune the parameters for low resource\nlanguage using Deep Residual Network. Here we use standard four data sets to\ndemonstrate that transfer learning can solve the problem of data scarcity for\nAffect Recognition task. We demonstrate that our approach is efficient by\nachieving 74.7 percent UAR on RAVDESS as source and Urdu data set as a target.\nThrough an ablation study, we have identified that pre-trained model adds most\nof the features information, improvement in results and solves less data\nissues. Using this knowledge, we have also experimented on SAVEE and EMO-DB\ndata set by setting Urdu as target language where only 400 utterances of data\nis available. This approach achieves high Unweighted Average Recall (UAR) when\ncompared with existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:30:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Durrani", "Sara", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2103.03583", "submitter": "Wei Zhang", "authors": "Wei Zhang, Zeyuan Chen, Chao Dong, Wen Wang, Hongyuan Zha, Jianyong\n  Wang", "title": "Graph-Based Tri-Attention Network for Answer Ranking in CQA", "comments": "9 pages (published in AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In community-based question answering (CQA) platforms, automatic answer\nranking for a given question is critical for finding potentially popular\nanswers in early times. The mainstream approaches learn to generate answer\nranking scores based on the matching degree between question and answer\nrepresentations as well as the influence of respondents. However, they\nencounter two main limitations: (1) Correlations between answers in the same\nquestion are often overlooked. (2) Question and respondent representations are\nbuilt independently of specific answers before affecting answer\nrepresentations. To address the limitations, we devise a novel graph-based\ntri-attention network, namely GTAN, which has two innovations. First, GTAN\nproposes to construct a graph for each question and learn answer correlations\nfrom each graph through graph neural networks (GNNs). Second, based on the\nrepresentations learned from GNNs, an alternating tri-attention method is\ndeveloped to alternatively build target-aware respondent representations,\nanswer-specific question representations, and context-aware answer\nrepresentations by attention computation. GTAN finally integrates the above\nrepresentations to generate answer ranking scores. Experiments on three\nreal-world CQA datasets demonstrate GTAN significantly outperforms\nstate-of-the-art answer ranking methods, validating the rationality of the\nnetwork architecture.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:40:38 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 02:19:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Wei", ""], ["Chen", "Zeyuan", ""], ["Dong", "Chao", ""], ["Wang", "Wen", ""], ["Zha", "Hongyuan", ""], ["Wang", "Jianyong", ""]]}, {"id": "2103.03589", "submitter": "Albina Khusainova", "authors": "Albina Khusainova, Adil Khan, Ad\\'in Ram\\'irez Rivera, Vitaly Romanov", "title": "Hierarchical Transformer for Multilingual Machine Translation", "comments": "Accepted to VarDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The choice of parameter sharing strategy in multilingual machine translation\nmodels determines how optimally parameter space is used and hence, directly\ninfluences ultimate translation quality. Inspired by linguistic trees that show\nthe degree of relatedness between different languages, the new general approach\nto parameter sharing in multilingual machine translation was suggested\nrecently. The main idea is to use these expert language hierarchies as a basis\nfor multilingual architecture: the closer two languages are, the more\nparameters they share. In this work, we test this idea using the Transformer\narchitecture and show that despite the success in previous work there are\nproblems inherent to training such hierarchical models. We demonstrate that in\ncase of carefully chosen training strategy the hierarchical architecture can\noutperform bilingual models and multilingual models with full parameter\nsharing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:51:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Khusainova", "Albina", ""], ["Khan", "Adil", ""], ["Rivera", "Ad\u00edn Ram\u00edrez", ""], ["Romanov", "Vitaly", ""]]}, {"id": "2103.03598", "submitter": "Bhavya Ghai", "authors": "Bhavya Ghai, Md Naimul Hoque, Klaus Mueller", "title": "WordBias: An Interactive Visual Tool for Discovering Intersectional\n  Biases Encoded in Word Embeddings", "comments": "Accepted to ACM SIGCHI 2021 LBW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:04:35 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ghai", "Bhavya", ""], ["Hoque", "Md Naimul", ""], ["Mueller", "Klaus", ""]]}, {"id": "2103.03642", "submitter": "Jie Wang", "authors": "Jiajun Chen, Huarui He, Feng Wu, Jie Wang", "title": "Topology-Aware Correlations Between Relations for Inductive Link\n  Prediction in Knowledge Graphs", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive link prediction -- where entities during training and inference\nstages can be different -- has been shown to be promising for completing\ncontinuously evolving knowledge graphs. Existing models of inductive reasoning\nmainly focus on predicting missing links by learning logical rules. However,\nmany existing approaches do not take into account semantic correlations between\nrelations, which are commonly seen in real-world knowledge graphs. To address\nthis challenge, we propose a novel inductive reasoning approach, namely TACT,\nwhich can effectively exploit Topology-Aware CorrelaTions between relations in\nan entity-independent manner. TACT is inspired by the observation that the\nsemantic correlation between two relations is highly correlated to their\ntopological structure in knowledge graphs. Specifically, we categorize all\nrelation pairs into several topological patterns, and then propose a Relational\nCorrelation Network (RCN) to learn the importance of the different patterns for\ninductive link prediction. Experiments demonstrate that TACT can effectively\nmodel semantic correlations between relations, and significantly outperforms\nexisting state-of-the-art methods on benchmark datasets for the inductive link\nprediction task.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:00:10 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Chen", "Jiajun", ""], ["He", "Huarui", ""], ["Wu", "Feng", ""], ["Wang", "Jie", ""]]}, {"id": "2103.03730", "submitter": "Masayu Leylia Khodra", "authors": "Adylan Roaffa Ilmy and Masayu Leylia Khodra", "title": "Parsing Indonesian Sentence into Abstract Meaning Representation using\n  Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Abstract Meaning Representation (AMR) provides many information of a sentence\nsuch as semantic relations, coreferences, and named entity relation in one\nrepresentation. However, research on AMR parsing for Indonesian sentence is\nfairly limited. In this paper, we develop a system that aims to parse an\nIndonesian sentence using a machine learning approach. Based on Zhang et al.\nwork, our system consists of three steps: pair prediction, label prediction,\nand graph construction. Pair prediction uses dependency parsing component to\nget the edges between the words for the AMR. The result of pair prediction is\npassed to the label prediction process which used a supervised learning\nalgorithm to predict the label between the edges of the AMR. We used simple\nsentence dataset that is gathered from articles and news article sentences. Our\nmodel achieved the SMATCH score of 0.820 for simple sentence test data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:01:59 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ilmy", "Adylan Roaffa", ""], ["Khodra", "Masayu Leylia", ""]]}, {"id": "2103.03732", "submitter": "Masayu Leylia Khodra", "authors": "Annisa Nurul Azhar and Masayu Leylia Khodra", "title": "Fine-tuning Pretrained Multilingual BERT Model for Indonesian\n  Aspect-based Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Although previous research on Aspect-based Sentiment Analysis (ABSA) for\nIndonesian reviews in hotel domain has been conducted using CNN and XGBoost,\nits model did not generalize well in test data and high number of OOV words\ncontributed to misclassification cases. Nowadays, most state-of-the-art results\nfor wide array of NLP tasks are achieved by utilizing pretrained language\nrepresentation. In this paper, we intend to incorporate one of the foremost\nlanguage representation model, BERT, to perform ABSA in Indonesian reviews\ndataset. By combining multilingual BERT (m-BERT) with task transformation\nmethod, we manage to achieve significant improvement by 8% on the F1-score\ncompared to the result from our previous study.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:05:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Azhar", "Annisa Nurul", ""], ["Khodra", "Masayu Leylia", ""]]}, {"id": "2103.03736", "submitter": "Masayu Leylia Khodra", "authors": "Yuly Haruka Berliana Gunawan and Masayu Leylia Khodra", "title": "Multi-document Summarization using Semantic Role Labeling and Semantic\n  Graph for Indonesian News Article", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we proposed a multi-document summarization system using\nsemantic role labeling (SRL) and semantic graph for Indonesian news articles.\nIn order to improve existing summarizer, our system modified summarizer that\nemployed subject, predicate, object, and adverbial (SVOA) extraction for\npredicate argument structure (PAS) extraction. SVOA extraction is replaced with\nSRL model for Indonesian. We also replace the genetic algorithm to identify\nimportant PAS with the decision tree classifier since the summarizer without\ngenetic algorithm gave better performance. The decision tree model is employed\nto identify important PAS. The decision tree model with 10 features achieved\nbetter performance than decision tree with 4 sentence features. Experiments and\nevaluations are conducted to generate 100 words summary and 200 words summary.\nThe evaluation shows the proposed model get 0.313 average ROUGE-2 recall in 100\nwords summary and 0.394 average ROUGE-2 recall in 200 words summary.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:09:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gunawan", "Yuly Haruka Berliana", ""], ["Khodra", "Masayu Leylia", ""]]}, {"id": "2103.03755", "submitter": "Alexander Sutherland", "authors": "A. Sutherland, S. Magg, S. Wermter", "title": "Leveraging Recursive Processing for Neural-Symbolic Affect-Target\n  Associations", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8851875", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Explaining the outcome of deep learning decisions based on affect is\nchallenging but necessary if we expect social companion robots to interact with\nusers on an emotional level. In this paper, we present a commonsense approach\nthat utilizes an interpretable hybrid neural-symbolic system to associate\nextracted targets, noun chunks determined to be associated with the expressed\nemotion, with affective labels from a natural language expression. We leverage\na pre-trained neural network that is well adapted to tree and sub-tree\nprocessing, the Dependency Tree-LSTM, to learn the affect labels of dynamic\ntargets, determined through symbolic rules, in natural language. We find that\nmaking use of the unique properties of the recursive network provides higher\naccuracy and interpretability when compared to other unstructured and\nsequential methods for determining target-affect associations in an\naspect-based sentiment analysis task.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:32:38 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Sutherland", "A.", ""], ["Magg", "S.", ""], ["Wermter", "S.", ""]]}, {"id": "2103.03775", "submitter": "Jianyou Wang", "authors": "Jianyou Wang, Xiaoxuan Zhang, Yuren Zhou, Christopher Suh, Cynthia\n  Rudin", "title": "There Once Was a Really Bad Poet, It Was Automated but You Didn't Know\n  It", "comments": "Paper accepted and will be published at TACL (Transactions of the\n  Association for Computational Linguistics) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Limerick generation exemplifies some of the most difficult challenges faced\nin poetry generation, as the poems must tell a story in only five lines, with\nconstraints on rhyme, stress, and meter. To address these challenges, we\nintroduce LimGen, a novel and fully automated system for limerick generation\nthat outperforms state-of-the-art neural network-based poetry models, as well\nas prior rule-based poetry models. LimGen consists of three important pieces:\nthe Adaptive Multi-Templated Constraint algorithm that constrains our search to\nthe space of realistic poems, the Multi-Templated Beam Search algorithm which\nsearches efficiently through the space, and the probabilistic Storyline\nalgorithm that provides coherent storylines related to a user-provided prompt\nword. The resulting limericks satisfy poetic constraints and have thematically\ncoherent storylines, which are sometimes even funny (when we are lucky).\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:03:55 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Jianyou", ""], ["Zhang", "Xiaoxuan", ""], ["Zhou", "Yuren", ""], ["Suh", "Christopher", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2103.03820", "submitter": "Melissa Roemmele", "authors": "Melissa Roemmele, Deep Sidhpura, Steve DeNeefe and Ling Tsou", "title": "AnswerQuest: A System for Generating Question-Answer Items from\n  Multi-Paragraph Documents", "comments": "Accepted at demo track of EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One strategy for facilitating reading comprehension is to present information\nin a question-and-answer format. We demo a system that integrates the tasks of\nquestion answering (QA) and question generation (QG) in order to produce Q&A\nitems that convey the content of multi-paragraph documents. We report some\nexperiments for QA and QG that yield improvements on both tasks, and assess how\nthey interact to produce a list of Q&A items for a text. The demo is accessible\nat qna.sdl.com.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:36:04 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Roemmele", "Melissa", ""], ["Sidhpura", "Deep", ""], ["DeNeefe", "Steve", ""], ["Tsou", "Ling", ""]]}, {"id": "2103.03842", "submitter": "Christopher Malon", "authors": "Christopher Malon", "title": "Overcoming Poor Word Embeddings with Word Definitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modern natural language understanding models depend on pretrained subword\nembeddings, but applications may need to reason about words that were never or\nrarely seen during pretraining. We show that examples that depend critically on\na rarer word are more challenging for natural language inference models. Then\nwe explore how a model could learn to use definitions, provided in natural\ntext, to overcome this handicap. Our model's understanding of a definition is\nusually weaker than a well-modeled word embedding, but it recovers most of the\nperformance gap from using a completely untrained word.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:57:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Malon", "Christopher", ""]]}, {"id": "2103.03872", "submitter": "Ethan Perez", "authors": "Ethan Perez, Douwe Kiela, Kyunghyun Cho", "title": "Rissanen Data Analysis: Examining Dataset Characteristics via\n  Description Length", "comments": "Code at https://github.com/ethanjperez/rda along with a script to run\n  RDA on your own dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to determine if a certain capability helps to achieve\nan accurate model of given data. We view labels as being generated from the\ninputs by a program composed of subroutines with different capabilities, and we\nposit that a subroutine is useful if and only if the minimal program that\ninvokes it is shorter than the one that does not. Since minimum program length\nis uncomputable, we instead estimate the labels' minimum description length\n(MDL) as a proxy, giving us a theoretically-grounded method for analyzing\ndataset characteristics. We call the method Rissanen Data Analysis (RDA) after\nthe father of MDL, and we showcase its applicability on a wide variety of\nsettings in NLP, ranging from evaluating the utility of generating subquestions\nbefore answering a question, to analyzing the value of rationales and\nexplanations, to investigating the importance of different parts of speech, and\nuncovering dataset gender bias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:58:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Perez", "Ethan", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2103.03874", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and\n  Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "comments": "Code and the MATH dataset is available at\n  https://github.com/hendrycks/math/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:59:39 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hendrycks", "Dan", ""], ["Burns", "Collin", ""], ["Kadavath", "Saurav", ""], ["Arora", "Akul", ""], ["Basart", "Steven", ""], ["Tang", "Eric", ""], ["Song", "Dawn", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2103.03940", "submitter": "Henrique Siqueira", "authors": "Henrique Siqueira, Alexander Sutherland, Pablo Barros, Mattias Kerzel,\n  Sven Magg, Stefan Wermter", "title": "Disambiguating Affective Stimulus Associations for Robot Perception and\n  Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effectively recognising and applying emotions to interactions is a highly\ndesirable trait for social robots. Implicitly understanding how subjects\nexperience different kinds of actions and objects in the world is crucial for\nnatural HRI interactions, with the possibility to perform positive actions and\navoid negative actions. In this paper, we utilize the NICO robot's appearance\nand capabilities to give the NICO the ability to model a coherent affective\nassociation between a perceived auditory stimulus and a temporally asynchronous\nemotion expression. This is done by combining evaluations of emotional valence\nfrom vision and language. NICO uses this information to make decisions about\nwhen to extend conversations in order to accrue more affective information if\nthe representation of the association is not coherent. Our primary contribution\nis providing a NICO robot with the ability to learn the affective associations\nbetween a perceived auditory stimulus and an emotional expression. NICO is able\nto do this for both individual subjects and specific stimuli, with the aid of\nan emotion-driven dialogue system that rectifies emotional expression\nincoherences. The robot is then able to use this information to determine a\nsubject's enjoyment of perceived auditory stimuli in a real HRI scenario.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:55:48 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Siqueira", "Henrique", ""], ["Sutherland", "Alexander", ""], ["Barros", "Pablo", ""], ["Kerzel", "Mattias", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "2103.04037", "submitter": "Andrew Shin", "authors": "Andrew Shin, Masato Ishii, Takuya Narihira", "title": "Perspectives and Prospects on Transformer Architecture for Cross-Modal\n  Tasks with Language and Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer architectures have brought about fundamental changes to\ncomputational linguistic field, which had been dominated by recurrent neural\nnetworks for many years. Its success also implies drastic changes in\ncross-modal tasks with language and vision, and many researchers have already\ntackled the issue. In this paper, we review some of the most critical\nmilestones in the field, as well as overall trends on how transformer\narchitecture has been incorporated into visuolinguistic cross-modal tasks.\nFurthermore, we discuss its current limitations and speculate upon some of the\nprospects that we find imminent.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:44:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Shin", "Andrew", ""], ["Ishii", "Masato", ""], ["Narihira", "Takuya", ""]]}, {"id": "2103.04044", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Dongjin Choi, Shenyu Xu, Diyi Yang", "title": "Putting Humans in the Natural Language Processing Loop: A Survey", "comments": "The paper is accepted to the HCI+NLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we design Natural Language Processing (NLP) systems that learn from\nhuman feedback? There is a growing research body of Human-in-the-loop (HITL)\nNLP frameworks that continuously integrate human feedback to improve the model\nitself. HITL NLP research is nascent but multifarious -- solving various NLP\nproblems, collecting diverse feedback from different people, and applying\ndifferent methods to learn from collected feedback. We present a survey of HITL\nNLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI)\ncommunities that highlights its short yet inspiring history, and thoroughly\nsummarize recent frameworks focusing on their tasks, goals, human interactions,\nand feedback learning methods. Finally, we discuss future directions for\nintegrating human feedback in the NLP development loop.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:26:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Zijie J.", ""], ["Choi", "Dongjin", ""], ["Xu", "Shenyu", ""], ["Yang", "Diyi", ""]]}, {"id": "2103.04097", "submitter": "No\\'e Tits", "authors": "No\\'e Tits, Kevin El Haddad and Thierry Dutoit", "title": "Analysis and Assessment of Controllability of an Expressive Deep\n  Learning-based TTS system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we study the controllability of an Expressive TTS system\ntrained on a dataset for a continuous control. The dataset is the Blizzard 2013\ndataset based on audiobooks read by a female speaker containing a great\nvariability in styles and expressiveness. Controllability is evaluated with\nboth an objective and a subjective experiment. The objective assessment is\nbased on a measure of correlation between acoustic features and the dimensions\nof the latent space representing expressiveness. The subjective assessment is\nbased on a perceptual experiment in which users are shown an interface for\nControllable Expressive TTS and asked to retrieve a synthetic utterance whose\nexpressiveness subjectively corresponds to that a reference utterance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 11:06:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tits", "No\u00e9", ""], ["Haddad", "Kevin El", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2103.04156", "submitter": "Eleni Partalidou", "authors": "Eleni Partalidou, Despina Christou and Grigorios Tsoumakas", "title": "Improving Zero-Shot Entity Retrieval through Effective Dense\n  Representations", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Linking (EL) seeks to align entity mentions in text to entries in a\nknowledge-base and is usually comprised of two phases: candidate generation and\ncandidate ranking. While most methods focus on the latter, it is the candidate\ngeneration phase that sets an upper bound to both time and accuracy performance\nof the overall EL system. This work's contribution is a significant improvement\nin candidate generation which thus raises the performance threshold for EL, by\ngenerating candidates that include the gold entity in the least candidate set\n(top-K). We propose a simple approach that efficiently embeds mention-entity\npairs in dense space through a BERT-based bi-encoder. Specifically, we extend\n(Wu et al., 2020) by introducing a new pooling function and incorporating\nentity type side-information. We achieve a new state-of-the-art 84.28% accuracy\non top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on\nthe top-64 of (Wu et al., 2020). We report the results from extensive\nexperimentation using our proposed model on both seen and unseen entity\ndatasets. Our results suggest that our method could be a useful complement to\nexisting EL approaches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 17:00:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Partalidou", "Eleni", ""], ["Christou", "Despina", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2103.04176", "submitter": "Razvan Bunescu", "authors": "Mike Chen and Razvan Bunescu", "title": "Changing the Narrative Perspective: From Deictic to Anaphoric Point of\n  View", "comments": "To appear in Information Processing & Management, Special Issue on\n  Creative Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce the task of changing the narrative point of view, where\ncharacters are assigned a narrative perspective that is different from the one\noriginally used by the writer. The resulting shift in the narrative point of\nview alters the reading experience and can be used as a tool in fiction writing\nor to generate types of text ranging from educational to self-help and\nself-diagnosis. We introduce a benchmark dataset containing a wide range of\ntypes of narratives annotated with changes in point of view from deictic (first\nor second person) to anaphoric (third person) and describe a pipeline for\nprocessing raw text that relies on a neural architecture for mention selection.\nEvaluations on the new benchmark dataset show that the proposed architecture\nsubstantially outperforms the baselines by generating mentions that are less\nambiguous and more natural.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:03:42 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Mike", ""], ["Bunescu", "Razvan", ""]]}, {"id": "2103.04180", "submitter": "Hugh Perkins", "authors": "Hugh Perkins", "title": "A Framework for Measuring Compositional Inductive Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework for measuring the compositional inductive bias of\nmodels in the context of emergent communications. We devise corrupted\ncompositional grammars that probe for limitations in the compositional\ninductive bias of frequently used models. We use these corrupted compositional\ngrammars to compare and contrast a wide range of models, and to compare the\nchoice of soft, Gumbel, and discrete representations. We propose a hierarchical\nmodel which might show an inductive bias towards relocatable atomic groups of\ntokens, thus potentially encouraging the emergence of words. We experiment with\nprobing for the compositional inductive bias of sender and receiver networks in\nisolation, and also placed end-to-end, as an auto-encoder.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:25:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Perkins", "Hugh", ""]]}, {"id": "2103.04222", "submitter": "Adam Goodkind", "authors": "Adam Goodkind", "title": "TypeShift: A User Interface for Visualizing the Typing Production\n  Process", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TypeShift is a tool for visualizing linguistic patterns in the timing of\ntyping production. Language production is a complex process which draws on\nlinguistic, cognitive and motor skills. By visualizing holistic trends in the\ntyping process, TypeShift aims to elucidate the often noisy information signals\nthat are used to represent typing patterns, both at the word-level and\ncharacter-level. It accomplishes this by enabling a researcher to compare and\ncontrast specific linguistic phenomena, and compare an individual typing\nsession to multiple group averages. Finally, although TypeShift was originally\ndesigned for typing data, it can easy be adapted to accommodate speech data, as\nwell. A web demo is available at https://angoodkind.shinyapps.io/TypeShift/.\nThe source code can be accessed at https://github.com/angoodkind/TypeShift.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:59:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Goodkind", "Adam", ""]]}, {"id": "2103.04225", "submitter": "Ife Adebara", "authors": "Ife Adebara, Muhammad Abdul-Mageed, Miikka Silfverberg", "title": "Translating the Unseen? Yoruba-English MT in Low-Resource,\n  Morphologically-Unmarked Settings", "comments": "Accepted at AfricanNLP @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating between languages where certain features are marked\nmorphologically in one but absent or marked contextually in the other is an\nimportant test case for machine translation. When translating into English\nwhich marks (in)definiteness morphologically, from Yor\\`ub\\'a which uses bare\nnouns but marks these features contextually, ambiguities arise. In this work,\nwe perform fine-grained analysis on how an SMT system compares with two NMT\nsystems (BiLSTM and Transformer) when translating bare nouns in Yor\\`ub\\'a into\nEnglish. We investigate how the systems what extent they identify BNs,\ncorrectly translate them, and compare with human translation patterns. We also\nanalyze the type of errors each model makes and provide a linguistic\ndescription of these errors. We glean insights for evaluating model performance\nin low-resource settings. In translating bare nouns, our results show the\ntransformer model outperforms the SMT and BiLSTM models for 4 categories, the\nBiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the\nNMT models for 1 category.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 01:24:09 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 04:46:10 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 17:55:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Adebara", "Ife", ""], ["Abdul-Mageed", "Muhammad", ""], ["Silfverberg", "Miikka", ""]]}, {"id": "2103.04290", "submitter": "Joseph Valencia", "authors": "Joseph Valencia, Erin Yao", "title": "MTLHealth: A Deep Learning System for Detecting Disturbing Content in\n  Student Essays", "comments": "typo in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Essay submissions to standardized tests like the ACT occasionally include\nreferences to bullying, self-harm, violence, and other forms of disturbing\ncontent. Graders must take great care to identify cases like these and decide\nwhether to alert authorities on behalf of students who may be in danger. There\nis a growing need for robust computer systems to support human decision-makers\nby automatically flagging potential instances of disturbing content. This paper\ndescribes MTLHealth, a disturbing content detection pipeline built around\nrecent advances from computational linguistics, particularly pre-trained\nlanguage model Transformer networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:51:33 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 21:37:01 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Valencia", "Joseph", ""], ["Yao", "Erin", ""]]}, {"id": "2103.04294", "submitter": "Aditya Khandelwal", "authors": "Aditya Khandelwal and Vahida Attar", "title": "Orthogonal Attention: A Cloze-Style Approach to Negation Scope\n  Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negation Scope Resolution is an extensively researched problem, which is used\nto locate the words affected by a negation cue in a sentence. Recent works have\nshown that simply finetuning transformer-based architectures yield\nstate-of-the-art results on this task. In this work, we look at Negation Scope\nResolution as a Cloze-Style task, with the sentence as the Context and the cue\nwords as the Query. We also introduce a novel Cloze-Style Attention mechanism\ncalled Orthogonal Attention, which is inspired by Self Attention. First, we\npropose a framework for developing Orthogonal Attention variants, and then\npropose 4 Orthogonal Attention variants: OA-C, OA-CA, OA-EM, and OA-EMB. Using\nthese Orthogonal Attention layers on top of an XLNet backbone, we outperform\nthe finetuned XLNet state-of-the-art for Negation Scope Resolution, achieving\nthe best results to date on all 4 datasets we experiment with: BioScope\nAbstracts, BioScope Full Papers, SFU Review Corpus and the *sem 2012 Dataset\n(Sherlock).\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 08:10:33 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Khandelwal", "Aditya", ""], ["Attar", "Vahida", ""]]}, {"id": "2103.04350", "submitter": "Yujing Wang", "authors": "Jiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu,\n  Yunhai Tong", "title": "Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees", "comments": "EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models like BERT achieve superior performances in\nvarious NLP tasks without explicit consideration of syntactic information.\nMeanwhile, syntactic information has been proved to be crucial for the success\nof NLP applications. However, how to incorporate the syntax trees effectively\nand efficiently into pre-trained Transformers is still unsettled. In this\npaper, we address this problem by proposing a novel framework named\nSyntax-BERT. This framework works in a plug-and-play mode and is applicable to\nan arbitrary pre-trained checkpoint based on Transformer architecture.\nExperiments on various datasets of natural language understanding verify the\neffectiveness of syntax trees and achieve consistent improvement over multiple\npre-trained models, including BERT, RoBERTa, and T5.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:11:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bai", "Jiangang", ""], ["Wang", "Yujing", ""], ["Chen", "Yiren", ""], ["Yang", "Yaming", ""], ["Bai", "Jing", ""], ["Yu", "Jing", ""], ["Tong", "Yunhai", ""]]}, {"id": "2103.04353", "submitter": "Tarek Naous", "authors": "Tarek Naous, Wissam Antoun, Reem A. Mahmoud, and Hazem Hajj", "title": "Empathetic BERT2BERT Conversational Model: Learning Arabic Language\n  Generation with Little Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling empathetic behavior in Arabic dialogue agents is an important aspect\nof building human-like conversational models. While Arabic Natural Language\nProcessing has seen significant advances in Natural Language Understanding\n(NLU) with language models such as AraBERT, Natural Language Generation (NLG)\nremains a challenge. The shortcomings of NLG encoder-decoder models are\nprimarily due to the lack of Arabic datasets suitable to train NLG models such\nas conversational agents. To overcome this issue, we propose a\ntransformer-based encoder-decoder initialized with AraBERT parameters. By\ninitializing the weights of the encoder and decoder with AraBERT pre-trained\nweights, our model was able to leverage knowledge transfer and boost\nperformance in response generation. To enable empathy in our conversational\nmodel, we train it using the ArabicEmpatheticDialogues dataset and achieve high\nperformance in empathetic response generation. Specifically, our model achieved\na low perplexity value of 17.0 and an increase in 5 BLEU points compared to the\nprevious state-of-the-art model. Also, our proposed model was rated highly by\n85 human evaluators, validating its high capability in exhibiting empathy while\ngenerating relevant and fluent responses in open-domain settings.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:23:51 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Naous", "Tarek", ""], ["Antoun", "Wissam", ""], ["Mahmoud", "Reem A.", ""], ["Hajj", "Hazem", ""]]}, {"id": "2103.04386", "submitter": "Serge Sharoff", "authors": "Nouran Khallaf, Serge Sharoff", "title": "Automatic Difficulty Classification of Arabic Sentences", "comments": "Accepted at WANLP 2021", "journal-ref": "The Sixth Arabic Natural Language Processing Workshop (WANLP 2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Modern Standard Arabic (MSA) Sentence difficulty\nclassifier, which predicts the difficulty of sentences for language learners\nusing either the CEFR proficiency levels or the binary classification as simple\nor complex. We compare the use of sentence embeddings of different kinds\n(fastText, mBERT , XLM-R and Arabic-BERT), as well as traditional language\nfeatures such as POS tags, dependency trees, readability scores and frequency\nlists for language learners. Our best results have been achieved using\nfined-tuned Arabic-BERT. The accuracy of our 3-way CEFR classification is F-1\nof 0.80 and 0.75 for Arabic-Bert and XLM-R classification respectively and 0.71\nSpearman correlation for regression. Our binary difficulty classifier reaches\nF-1 0.94 and F-1 0.98 for sentence-pair semantic similarity classifier.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 16:02:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Khallaf", "Nouran", ""], ["Sharoff", "Serge", ""]]}, {"id": "2103.04399", "submitter": "Binyuan Hui", "authors": "Binyuan Hui, Xiang Shi, Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun,\n  Xiaodan Zhu", "title": "Improving Text-to-SQL with Schema Dependency Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-SQL aims to map natural language questions to SQL queries. The\nsketch-based method combined with execution-guided (EG) decoding strategy has\nshown a strong performance on the WikiSQL benchmark. However, execution-guided\ndecoding relies on database execution, which significantly slows down the\ninference process and is hence unsatisfactory for many real-world applications.\nIn this paper, we present the Schema Dependency guided multi-task Text-to-SQL\nmodel (SDSQL) to guide the network to effectively capture the interactions\nbetween questions and schemas. The proposed model outperforms all existing\nmethods in both the settings with or without EG. We show the schema dependency\nlearning partially cover the benefit from EG and alleviates the need for it.\nSDSQL without EG significantly reduces time consumption during inference,\nsacrificing only a small amount of performance and provides more flexibility\nfor downstream applications.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 16:56:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hui", "Binyuan", ""], ["Shi", "Xiang", ""], ["Geng", "Ruiying", ""], ["Li", "Binhua", ""], ["Li", "Yongbin", ""], ["Sun", "Jian", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2103.04437", "submitter": "Alex Brandsen", "authors": "Alex Brandsen, Suzan Verberne, Karsten Lambers, Milco Wansleeben", "title": "Usability Evaluation for Online Professional Search in the Dutch\n  Archaeology Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents AGNES, the first information retrieval system for\narchaeological grey literature, allowing full-text search of these long\narchaeological documents.\n  This search system has a web interface that allows archaeology professionals\nand scholars to search through a collection of over 60,000 Dutch excavation\nreports, totalling 361 million words. We conducted a user study for the\nevaluation of AGNES's search interface, with a small but diverse user group.\nThe evaluation was done by screen capturing and a think aloud protocol,\ncombined with a user interface feedback questionnaire. The evaluation covered\nboth controlled use (completion of a pre-defined task) as well as free use\n(completion of a freely chosen task). The free use allows us to study the\ninformation needs of archaeologists, as well as their interactions with the\nsearch system. We conclude that: (1) the information needs of archaeologists\nare typically recall-oriented, often requiring a list of items as answer; (2)\nthe users prefer the use of free-text queries over metadata filters, confirming\nthe value of a free-text search system; (3) the compilation of a diverse user\ngroup contributed to the collection of diverse issues as feedback for improving\nthe system. We are currently refining AGNES's user interface and improving its\nprecision for archaeological entities, so that AGNES will help archaeologists\nto answer their research questions more effectively and efficiently, leading to\na more coherent narrative of the past.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:48:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Brandsen", "Alex", ""], ["Verberne", "Suzan", ""], ["Lambers", "Karsten", ""], ["Wansleeben", "Milco", ""]]}, {"id": "2103.04469", "submitter": "Adam Goodkind", "authors": "Adam Goodkind and Klinton Bicknell", "title": "Local word statistics affect reading times independently of surprisal", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surprisal theory has provided a unifying framework for understanding many\nphenomena in sentence processing (Hale, 2001; Levy, 2008a), positing that a\nword's conditional probability given all prior context fully determines\nprocessing difficulty. Problematically for this claim, one local statistic,\nword frequency, has also been shown to affect processing, even when conditional\nprobability given context is held constant. Here, we ask whether other local\nstatistics have a role in processing, or whether word frequency is a special\ncase. We present the first clear evidence that more complex local statistics,\nword bigram and trigram probability, also affect processing independently of\nsurprisal. These findings suggest a significant and independent role of local\nstatistics in processing. Further, it motivates research into new\ngeneralizations of surprisal that can also explain why local statistical\ninformation should have an outsized effect.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 22:18:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 03:11:05 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Goodkind", "Adam", ""], ["Bicknell", "Klinton", ""]]}, {"id": "2103.04518", "submitter": "Tariq Alhindi", "authors": "Tariq Alhindi and Debanjan Ghosh", "title": "\"Sharks are not the threat humans are\": Argument Component Segmentation\n  in School Student Essays", "comments": "Accepted to the 16th Workshop on Innovative Use of NLP for Building\n  Educational Applications. Co-located with EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Argument mining is often addressed by a pipeline method where segmentation of\ntext into argumentative units is conducted first and proceeded by an argument\ncomponent identification task. In this research, we apply a token-level\nclassification to identify claim and premise tokens from a new corpus of\nargumentative essays written by middle school students. To this end, we compare\na variety of state-of-the-art models such as discrete features and deep\nlearning architectures (e.g., BiLSTM networks and BERT-based architectures) to\nidentify the argument components. We demonstrate that a BERT-based multi-task\nlearning architecture (i.e., token and sentence level classification)\nadaptively pretrained on a relevant unlabeled dataset obtains the best results\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 02:40:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Alhindi", "Tariq", ""], ["Ghosh", "Debanjan", ""]]}, {"id": "2103.04567", "submitter": "Wei Peng", "authors": "Wei Peng, Yue Hu, Jing Yu, Luxi Xing, Yuqiang Xie, Zihao Zhu, Yajing\n  Sun", "title": "MCR-Net: A Multi-Step Co-Interactive Relation Network for Unanswerable\n  Questions on Machine Reading Comprehension", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering systems usually use keyword searches to retrieve potential\npassages related to a question, and then extract the answer from passages with\nthe machine reading comprehension methods. However, many questions tend to be\nunanswerable in the real world. In this case, it is significant and challenging\nhow the model determines when no answer is supported by the passage and\nabstains from answering. Most of the existing systems design a simple\nclassifier to determine answerability implicitly without explicitly modeling\nmutual interaction and relation between the question and passage, leading to\nthe poor performance for determining the unanswerable questions. To tackle this\nproblem, we propose a Multi-Step Co-Interactive Relation Network (MCR-Net) to\nexplicitly model the mutual interaction and locate key clues from coarse to\nfine by introducing a co-interactive relation module. The co-interactive\nrelation module contains a stack of interaction and fusion blocks to\ncontinuously integrate and fuse history-guided and current-query-guided clues\nin an explicit way. Experiments on the SQuAD 2.0 and DuReader datasets show\nthat our model achieves a remarkable improvement, outperforming the BERT-style\nbaselines in literature. Visualization analysis also verifies the importance of\nthe mutual interaction between the question and passage.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 06:38:14 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 05:36:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Peng", "Wei", ""], ["Hu", "Yue", ""], ["Yu", "Jing", ""], ["Xing", "Luxi", ""], ["Xie", "Yuqiang", ""], ["Zhu", "Zihao", ""], ["Sun", "Yajing", ""]]}, {"id": "2103.04692", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala and John A. Bateman", "title": "Semiotically-grounded distant viewing of diagrams: insights from two\n  multimodal corpora", "comments": "22 pages, 11 figures. Under review at Digital Scholarship in the\n  Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we bring together theories of multimodal communication and\ncomputational methods to study how primary school science diagrams combine\nmultiple expressive resources. We position our work within the field of digital\nhumanities, and show how annotations informed by multimodality research, which\ntarget expressive resources and discourse structure, allow imposing structure\non the output of computational methods. We illustrate our approach by analysing\ntwo multimodal diagram corpora: the first corpus is intended to support\nresearch on automatic diagram processing, whereas the second is oriented\ntowards studying diagrams as a mode of communication. Our results show that\nmultimodally-informed annotations can bring out structural patterns in the\ndiagrams, which also extend across diagrams that deal with different topics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:04:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hiippala", "Tuomo", ""], ["Bateman", "John A.", ""]]}, {"id": "2103.04941", "submitter": "Nathaniel Weir", "authors": "Jiefu Ou, Nathaniel Weir, Anton Belyy, Felix Yu, and Benjamin Van\n  Durme", "title": "InFillmore: Frame-Guided Language Generation with Bidirectional Context", "comments": "Appearing in *SEM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structured extension to bidirectional-context conditional\nlanguage generation, or \"infilling,\" inspired by Frame Semantic theory\n(Fillmore, 1976). Guidance is provided through two approaches: (1) model\nfine-tuning, conditioning directly on observed symbolic frames, and (2) a novel\nextension to disjunctive lexically constrained decoding that leverages frame\nsemantic lexical units. Automatic and human evaluations confirm that\nframe-guided generation allows for explicit manipulation of intended infill\nsemantics, with minimal loss in distinguishability from human-generated text.\nOur methods flexibly apply to a variety of use scenarios, and we provide a\ncodebase and interactive demo available from\nhttps://nlp.jhu.edu/demos/infillmore.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:59:41 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 19:22:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ou", "Jiefu", ""], ["Weir", "Nathaniel", ""], ["Belyy", "Anton", ""], ["Yu", "Felix", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2103.05028", "submitter": "Rajarshi Bhowmik", "authors": "Rajarshi Bhowmik and Karl Stratos and Gerard de Melo", "title": "Fast and Effective Biomedical Entity Linking Using a Dual Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biomedical entity linking is the task of identifying mentions of biomedical\nconcepts in text documents and mapping them to canonical entities in a target\nthesaurus. Recent advancements in entity linking using BERT-based models follow\na retrieve and rerank paradigm, where the candidate entities are first selected\nusing a retriever model, and then the retrieved candidates are ranked by a\nreranker model. While this paradigm produces state-of-the-art results, they are\nslow both at training and test time as they can process only one mention at a\ntime. To mitigate these issues, we propose a BERT-based dual encoder model that\nresolves multiple mentions in a document in one shot. We show that our proposed\nmodel is multiple times faster than existing BERT-based models while being\ncompetitive in accuracy for biomedical entity linking. Additionally, we modify\nour dual encoder model for end-to-end biomedical entity linking that performs\nboth mention span detection and entity disambiguation and out-performs two\nrecently proposed models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:32:28 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bhowmik", "Rajarshi", ""], ["Stratos", "Karl", ""], ["de Melo", "Gerard", ""]]}, {"id": "2103.05069", "submitter": "Abdul Waheed", "authors": "Abdul Waheed, Muskan Goyal, Nimisha Mittal, Deepak Gupta", "title": "Domain Controlled Title Generation with Human Evaluation", "comments": "Accepted at ICICC-2021 for publication in Springer AISC series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study automatic title generation and present a method for generating\ndomain-controlled titles for scientific articles. A good title allows you to\nget the attention that your research deserves. A title can be interpreted as a\nhigh-compression description of a document containing information on the\nimplemented process. For domain-controlled titles, we used the pre-trained\ntext-to-text transformer model and the additional token technique. Title tokens\nare sampled from a local distribution (which is a subset of global vocabulary)\nof the domain-specific vocabulary and not global vocabulary, thereby generating\na catchy title and closely linking it to its corresponding abstract. Generated\ntitles looked realistic, convincing, and very close to the ground truth. We\nhave performed automated evaluation using ROUGE metric and human evaluation\nusing five parameters to make a comparison between human and machine-generated\ntitles. The titles produced were considered acceptable with higher metric\nratings in contrast to the original titles. Thus we concluded that our research\nproposes a promising method for domain-controlled title generation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:55:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Waheed", "Abdul", ""], ["Goyal", "Muskan", ""], ["Mittal", "Nimisha", ""], ["Gupta", "Deepak", ""]]}, {"id": "2103.05070", "submitter": "Vipul Raheja", "authors": "Kostiantyn Omelianchuk, Vipul Raheja, Oleksandr Skurzhanskyi", "title": "Text Simplification by Tagging", "comments": "15 pages. Accepted to BEA @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edit-based approaches have recently shown promising results on multiple\nmonolingual sequence transduction tasks. In contrast to conventional\nsequence-to-sequence (Seq2Seq) models, which learn to generate text from\nscratch as they are trained on parallel corpora, these methods have proven to\nbe much more effective since they are able to learn to make fast and accurate\ntransformations while leveraging powerful pre-trained language models. Inspired\nby these ideas, we present TST, a simple and efficient Text Simplification\nsystem based on sequence Tagging, leveraging pre-trained Transformer-based\nencoders. Our system makes simplistic data augmentations and tweaks in training\nand inference on a pre-existing system, which makes it less reliant on large\namounts of parallel training data, provides more control over the outputs and\nenables faster inference speeds. Our best model achieves near state-of-the-art\nperformance on benchmark test datasets for the task. Since it is fully\nnon-autoregressive, it achieves faster inference speeds by over 11 times than\nthe current state-of-the-art text simplification system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:57:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Omelianchuk", "Kostiantyn", ""], ["Raheja", "Vipul", ""], ["Skurzhanskyi", "Oleksandr", ""]]}, {"id": "2103.05081", "submitter": "Ke Li", "authors": "Ke Li, Daniel Povey, Sanjeev Khudanpur", "title": "A Parallelizable Lattice Rescoring Strategy with Neural Language Models", "comments": "To appear at ICASSP 2021. 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a parallel computation strategy and a posterior-based\nlattice expansion algorithm for efficient lattice rescoring with neural\nlanguage models (LMs) for automatic speech recognition. First, lattices from\nfirst-pass decoding are expanded by the proposed posterior-based lattice\nexpansion algorithm. Second, each expanded lattice is converted into a minimal\nlist of hypotheses that covers every arc. Each hypothesis is constrained to be\nthe best path for at least one arc it includes. For each lattice, the neural LM\nscores of the minimal list are computed in parallel and are then integrated\nback to the lattice in the rescoring stage. Experiments on the Switchboard\ndataset show that the proposed rescoring strategy obtains comparable\nrecognition performance and generates more compact lattices than a competitive\nbaseline method. Furthermore, the parallel rescoring method offers more\nflexibility by simplifying the integration of PyTorch-trained neural LMs for\nlattice rescoring with Kaldi.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:23:12 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Li", "Ke", ""], ["Povey", "Daniel", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2103.05103", "submitter": "Shikha Dubey", "authors": "Farrukh Olimov, Shikha Dubey, Labina Shrestha, Tran Trung Tin, Moongu\n  Jeon", "title": "Image Captioning using Multiple Transformers for Self-Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time image captioning, along with adequate precision, is the main\nchallenge of this research field. The present work, Multiple Transformers for\nSelf-Attention Mechanism (MTSM), utilizes multiple transformers to address\nthese problems. The proposed algorithm, MTSM, acquires region proposals using a\ntransformer detector (DETR). Consequently, MTSM achieves the self-attention\nmechanism by transferring these region proposals and their visual and\ngeometrical features through another transformer and learns the objects' local\nand global interconnections. The qualitative and quantitative results of the\nproposed algorithm, MTSM, are shown on the MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:35:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Olimov", "Farrukh", ""], ["Dubey", "Shikha", ""], ["Shrestha", "Labina", ""], ["Tin", "Tran Trung", ""], ["Jeon", "Moongu", ""]]}, {"id": "2103.05131", "submitter": "Sanjeev Kumar Karn", "authors": "Sanjeev Kumar Karn, Francine Chen, Yan-Ying Chen, Ulli Waltinger and\n  Hinrich Schuetze", "title": "Few-Shot Learning of an Interleaved Text Summarization Model by\n  Pretraining with Synthetic Data", "comments": "Adapt-NLP: The Second Workshop on Domain Adaptation for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interleaved texts, where posts belonging to different threads occur in a\nsequence, commonly occur in online chat posts, so that it can be time-consuming\nto quickly obtain an overview of the discussions. Existing systems first\ndisentangle the posts by threads and then extract summaries from those threads.\nA major issue with such systems is error propagation from the disentanglement\ncomponent. While end-to-end trainable summarization system could obviate\nexplicit disentanglement, such systems require a large amount of labeled data.\nTo address this, we propose to pretrain an end-to-end trainable hierarchical\nencoder-decoder system using synthetic interleaved texts. We show that by\nfine-tuning on a real-world meeting dataset (AMI), such a system out-performs a\ntraditional two-step system by 22%. We also compare against transformer models\nand observed that pretraining with synthetic data both the encoder and decoder\noutperforms the BertSumExtAbs transformer model which pretrains only the\nencoder on a large dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:58:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Karn", "Sanjeev Kumar", ""], ["Chen", "Francine", ""], ["Chen", "Yan-Ying", ""], ["Waltinger", "Ulli", ""], ["Schuetze", "Hinrich", ""]]}, {"id": "2103.05132", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou and Mohammed Sabry", "title": "AfriVEC: Word Embedding Models for African Languages. Case Study of Fon\n  and Nobiin", "comments": null, "journal-ref": "Africa NLP, EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From Word2Vec to GloVe, word embedding models have played key roles in the\ncurrent state-of-the-art results achieved in Natural Language Processing.\nDesigned to give significant and unique vectorized representations of words and\nentities, those models have proven to efficiently extract similarities and\nestablish relationships reflecting semantic and contextual meaning among words\nand entities. African Languages, representing more than 31% of the worldwide\nspoken languages, have recently been subject to lots of research. However, to\nthe best of our knowledge, there are currently very few to none word embedding\nmodels for those languages words and entities, and none for the languages under\nstudy in this paper. After describing Glove, Word2Vec, and Poincar\\'e\nembeddings functionalities, we build Word2Vec and Poincar\\'e word embedding\nmodels for Fon and Nobiin, which show promising results. We test the\napplicability of transfer learning between these models as a landmark for\nAfrican Languages to jointly involve in mitigating the scarcity of their\nresources, and attempt to provide linguistic and social interpretations of our\nresults. Our main contribution is to arouse more interest in creating word\nembedding models proper to African Languages, ready for use, and that can\nsignificantly improve the performances of Natural Language Processing\ndownstream tasks on them. The official repository and implementation is at\nhttps://github.com/bonaventuredossou/afrivec\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:58:20 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 05:35:22 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Sabry", "Mohammed", ""]]}, {"id": "2103.05135", "submitter": "Fanchao Meng", "authors": "Fanchao Meng", "title": "A Topological Approach to Compare Document Semantics Based on a New\n  Variant of Syntactic N-grams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper delivers a new perspective of thinking and utilizing syntactic\nn-grams (sn-grams). Sn-grams are a type of non-linear n-grams which have been\nplaying a critical role in many NLP tasks. Introducing sn-grams to comparing\ndocument semantics thus is an appealing application, and few studies have\nreported progress at this. However, when proceeding on this application, we\nfound three major issues of sn-grams: lack of significance, being sensitive to\nword orders and failing on capture indirect syntactic relations. To address\nthese issues, we propose a new variant of sn-grams named generalized phrases\n(GPs). Then based on GPs we propose a topological approach, named DSCoH, to\ncompute document semantic similarities. DSCoH has been extensively tested on\nthe document semantics comparison and the document clustering tasks. The\nexperimental results show that DSCoH can outperform state-of-the-art\nembedding-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:16:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Meng", "Fanchao", ""]]}, {"id": "2103.05149", "submitter": "Abdelrahman Mohamed", "authors": "Alex Xiao, Christian Fuegen, Abdelrahman Mohamed", "title": "Contrastive Semi-supervised Learning for ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-labeling is the most adopted method for pre-training automatic speech\nrecognition (ASR) models. However, its performance suffers from the supervised\nteacher model's degrading quality in low-resource setups and under domain\ntransfer. Inspired by the successes of contrastive representation learning for\ncomputer vision and speech applications, and more recently for supervised\nlearning of visual objects, we propose Contrastive Semi-supervised Learning\n(CSL). CSL eschews directly predicting teacher-generated pseudo-labels in favor\nof utilizing them to select positive and negative examples. In the challenging\ntask of transcribing public social media videos, using CSL reduces the WER by\n8% compared to the standard Cross-Entropy pseudo-labeling (CE-PL) when 10hr of\nsupervised data is used to annotate 75,000hr of videos. The WER reduction jumps\nto 19% under the ultra low-resource condition of using 1hr labels for teacher\nsupervision. CSL generalizes much better in out-of-domain conditions, showing\nup to 17% WER reduction compared to the best CE-PL pre-trained model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:20:37 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Xiao", "Alex", ""], ["Fuegen", "Christian", ""], ["Mohamed", "Abdelrahman", ""]]}, {"id": "2103.05167", "submitter": "Gihyeon Choi", "authors": "Gihyeon Choi, Shinhyeok Oh and Harksoo Kim", "title": "Improving Document-Level Sentiment Classification Using Importance of\n  Sentences", "comments": "12 pages, 7 figures, 5 tables", "journal-ref": "Entropy, Vol.22(12), pp.1-11, 2020.11", "doi": "10.3390/e22121336", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous researchers have considered sentiment analysis as a document\nclassification task, in which input documents are classified into predefined\nsentiment classes. Although there are sentences in a document that support\nimportant evidences for sentiment analysis and sentences that do not, they have\ntreated the document as a bag of sentences. In other words, they have not\nconsidered the importance of each sentence in the document. To effectively\ndetermine polarity of a document, each sentence in the document should be dealt\nwith different degrees of importance. To address this problem, we propose a\ndocument-level sentence classification model based on deep neural networks, in\nwhich the importance degrees of sentences in documents are automatically\ndetermined through gate mechanisms. To verify our new sentiment analysis model,\nwe conducted experiments using the sentiment datasets in the four different\ndomains such as movie reviews, hotel reviews, restaurant reviews, and music\nreviews. In the experiments, the proposed model outperformed previous\nstate-of-the-art models that do not consider importance differences of\nsentences in a document. The experimental results show that the importance of\nsentences should be considered in a document-level sentiment classification\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:29:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Choi", "Gihyeon", ""], ["Oh", "Shinhyeok", ""], ["Kim", "Harksoo", ""]]}, {"id": "2103.05231", "submitter": "Meng Zhou", "authors": "Meng Zhou, Zechen Li, Pengtao Xie", "title": "Self-supervised Regularization for Text Classification", "comments": "16 pages, 3 figures, to be published in Transactions of the\n  Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text classification is a widely studied problem and has broad applications.\nIn many real-world problems, the number of texts for training classification\nmodels is limited, which renders these models prone to overfitting. To address\nthis problem, we propose SSL-Reg, a data-dependent regularization approach\nbased on self-supervised learning (SSL). SSL is an unsupervised learning\napproach which defines auxiliary tasks on input data without using any\nhuman-provided labels and learns data representations by solving these\nauxiliary tasks. In SSL-Reg, a supervised classification task and an\nunsupervised SSL task are performed simultaneously. The SSL task is\nunsupervised, which is defined purely on input texts without using any\nhuman-provided labels. Training a model using an SSL task can prevent the model\nfrom being overfitted to a limited number of class labels in the classification\ntask. Experiments on 17 text classification datasets demonstrate the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:35:52 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 01:42:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhou", "Meng", ""], ["Li", "Zechen", ""], ["Xie", "Pengtao", ""]]}, {"id": "2103.05284", "submitter": "Ziqi Zhang", "authors": "Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng,\n  Weiming Hu", "title": "Open-book Video Captioning with Retrieve-Copy-Generate Network", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the rapid emergence of short videos and the requirement for content\nunderstanding and creation, the video captioning task has received increasing\nattention in recent years. In this paper, we convert traditional video\ncaptioning task into a new paradigm, \\ie, Open-book Video Captioning, which\ngenerates natural language under the prompts of video-content-relevant\nsentences, not limited to the video itself. To address the open-book video\ncaptioning problem, we propose a novel Retrieve-Copy-Generate network, where a\npluggable video-to-text retriever is constructed to retrieve sentences as hints\nfrom the training corpus effectively, and a copy-mechanism generator is\nintroduced to extract expressions from multi-retrieved sentences dynamically.\nThe two modules can be trained end-to-end or separately, which is flexible and\nextensible. Our framework coordinates the conventional retrieval-based methods\nwith orthodox encoder-decoder methods, which can not only draw on the diverse\nexpressions in the retrieved sentences but also generate natural and accurate\ncontent of the video. Extensive experiments on several benchmark datasets show\nthat our proposed approach surpasses the state-of-the-art performance,\nindicating the effectiveness and promising of the proposed paradigm in the task\nof video captioning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:17:17 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhang", "Ziqi", ""], ["Qi", "Zhongang", ""], ["Yuan", "Chunfeng", ""], ["Shan", "Ying", ""], ["Li", "Bing", ""], ["Deng", "Ying", ""], ["Hu", "Weiming", ""]]}, {"id": "2103.05327", "submitter": "Adi Haviv", "authors": "Adi Haviv, Jonathan Berant and Amir Globerson", "title": "BERTese: Learning to Speak to BERT", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large pre-trained language models have been shown to encode large amounts of\nworld and commonsense knowledge in their parameters, leading to substantial\ninterest in methods for extracting that knowledge. In past work, knowledge was\nextracted by taking manually-authored queries and gathering paraphrases for\nthem using a separate pipeline. In this work, we propose a method for\nautomatically rewriting queries into \"BERTese\", a paraphrase query that is\ndirectly optimized towards better knowledge extraction. To encourage meaningful\nrewrites, we add auxiliary loss functions that encourage the query to\ncorrespond to actual language tokens. We empirically show our approach\noutperforms competing baselines, obviating the need for complex pipelines.\nMoreover, BERTese provides some insight into the type of language that helps\nlanguage models perform knowledge extraction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:17:22 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 08:33:01 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Haviv", "Adi", ""], ["Berant", "Jonathan", ""], ["Globerson", "Amir", ""]]}, {"id": "2103.05345", "submitter": "Varvara Logacheva", "authors": "Nikolay Babakov, Varvara Logacheva, Olga Kozlova, Nikita Semenov and\n  Alexander Panchenko", "title": "Detecting Inappropriate Messages on Sensitive Topics that Could Harm a\n  Company's Reputation", "comments": "Accepted to the Balto-Slavic NLP workshop 2021 co-located with\n  EACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Not all topics are equally \"flammable\" in terms of toxicity: a calm\ndiscussion of turtles or fishing less often fuels inappropriate toxic dialogues\nthan a discussion of politics or sexual minorities. We define a set of\nsensitive topics that can yield inappropriate and toxic messages and describe\nthe methodology of collecting and labeling a dataset for appropriateness. While\ntoxicity in user-generated data is well-studied, we aim at defining a more\nfine-grained notion of inappropriateness. The core of inappropriateness is that\nit can harm the reputation of a speaker. This is different from toxicity in two\nrespects: (i) inappropriateness is topic-related, and (ii) inappropriate\nmessage is not toxic but still unacceptable. We collect and release two\ndatasets for Russian: a topic-labeled dataset and an appropriateness-labeled\ndataset. We also release pre-trained classification models trained on this\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:50:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Babakov", "Nikolay", ""], ["Logacheva", "Varvara", ""], ["Kozlova", "Olga", ""], ["Semenov", "Nikita", ""], ["Panchenko", "Alexander", ""]]}, {"id": "2103.05552", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tommi Jauhiainen, Tharindu Ranasinghe, Marcos Zampieri", "title": "Comparing Approaches to Dravidian Language Identification", "comments": "Accepted to VarDial 2021 @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the submissions by team HWR to the Dravidian Language\nIdentification (DLI) shared task organized at VarDial 2021 workshop. The DLI\ntraining set includes 16,674 YouTube comments written in Roman script\ncontaining code-mixed text with English and one of the three South Dravidian\nlanguages: Kannada, Malayalam, and Tamil. We submitted results generated using\ntwo models, a Naive Bayes classifier with adaptive language models, which has\nshown to obtain competitive performance in many language and dialect\nidentification tasks, and a transformer-based model which is widely regarded as\nthe state-of-the-art in a number of NLP tasks. Our first submission was sent in\nthe closed submission track using only the training set provided by the shared\ntask organisers, whereas the second submission is considered to be open as it\nused a pretrained model trained with external data. Our team attained shared\nsecond position in the shared task with the submission based on Naive Bayes.\nOur results reinforce the idea that deep learning methods are not as\ncompetitive in language identification related tasks as they are in many other\ntext classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:58:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jauhiainen", "Tommi", ""], ["Ranasinghe", "Tharindu", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2103.05638", "submitter": "Madhusudan Verma", "authors": "Madhusudan Verma", "title": "Beyond Nystr\\\"omformer -- Approximation of self-attention by Spectral\n  Shifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is a powerful tool for many natural language tasks which is based\non self-attention, a mechanism that encodes the dependence of other tokens on\neach specific token, but the computation of self-attention is a bottleneck due\nto its quadratic time complexity. There are various approaches to reduce the\ntime complexity and approximation of matrix is one such. In Nystr\\\"omformer,\nthe authors used Nystr\\\"om based method for approximation of softmax. The\nNystr\\\"om method generates a fast approximation to any large-scale symmetric\npositive semidefinite (SPSD) matrix using only a few columns of the SPSD\nmatrix. However, since the Nystr\\\"om approximation is low-rank when the\nspectrum of the SPSD matrix decays slowly, the Nystr\\\"om approximation is of\nlow accuracy. Here an alternative method is proposed for approximation which\nhas a much stronger error bound than the Nystr\\\"om method. The time complexity\nof this same as Nystr\\\"omformer which is $O\\left({n}\\right)$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:48:16 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Verma", "Madhusudan", ""]]}, {"id": "2103.05639", "submitter": "Israel Abebe Azime", "authors": "Israel Abebe Azime and Nebil Mohammed", "title": "An Amharic News Text classification Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In NLP, text classification is one of the primary problems we try to solve\nand its uses in language analyses are indisputable. The lack of labeled\ntraining data made it harder to do these tasks in low resource languages like\nAmharic. The task of collecting, labeling, annotating, and making valuable this\nkind of data will encourage junior researchers, schools, and machine learning\npractitioners to implement existing classification models in their language. In\nthis short paper, we aim to introduce the Amharic text classification dataset\nthat consists of more than 50k news articles that were categorized into 6\nclasses. This dataset is made available with easy baseline performances to\nencourage studies and better performance experiments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:36:39 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Azime", "Israel Abebe", ""], ["Mohammed", "Nebil", ""]]}, {"id": "2103.05683", "submitter": "Amey Hengle", "authors": "Amey Hengle, Atharva Kshirsagar, Shaily Desai and Manisha Marathe", "title": "Combining Context-Free and Contextualized Representations for Arabic\n  Sarcasm Detection and Sentiment Identification", "comments": "7 pages, 1 figure, The Sixth Arabic Natural Language Processing\n  Workshop. (WANLP 2021), held in conjunction with EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their inception, transformer-based language models have led to\nimpressive performance gains across multiple natural language processing tasks.\nFor Arabic, the current state-of-the-art results on most datasets are achieved\nby the AraBERT language model. Notwithstanding these recent advancements,\nsarcasm and sentiment detection persist to be challenging tasks in Arabic,\ngiven the language's rich morphology, linguistic disparity and dialectal\nvariations. This paper proffers team SPPU-AASM's submission for the WANLP\nArSarcasm shared-task 2021, which centers around the sarcasm and sentiment\npolarity detection of Arabic tweets. The study proposes a hybrid model,\ncombining sentence representations from AraBERT with static word vectors\ntrained on Arabic social media corpora. The proposed system achieves a\nF1-sarcastic score of 0.62 and a F-PN score of 0.715 for the sarcasm and\nsentiment detection tasks, respectively. Simulation results show that the\nproposed system outperforms multiple existing approaches for both the tasks,\nsuggesting that the amalgamation of context-free and context-dependent text\nrepresentations can help capture complementary facets of word meaning in\nArabic. The system ranked second and tenth in the respective sub-tasks of\nsarcasm detection and sentiment identification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:39:43 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Hengle", "Amey", ""], ["Kshirsagar", "Atharva", ""], ["Desai", "Shaily", ""], ["Marathe", "Manisha", ""]]}, {"id": "2103.05815", "submitter": "Alexander Sutherland", "authors": "A. Sutherland, S. Bensch, T. Hellstr\\\"om, S. Magg, S.Wermter", "title": "Tell Me Why You Feel That Way: Processing Compositional Dependency for\n  Tree-LSTM Aspect Sentiment Triplet Extraction (TASTE)", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": "10.1007/978-3-030-61609-0_52", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sentiment analysis has transitioned from classifying the sentiment of an\nentire sentence to providing the contextual information of what targets exist\nin a sentence, what sentiment the individual targets have, and what the causal\nwords responsible for that sentiment are. However, this has led to elaborate\nrequirements being placed on the datasets needed to train neural networks on\nthe joint triplet task of determining an entity, its sentiment, and the causal\nwords for that sentiment. Requiring this kind of data for training systems is\nproblematic, as they suffer from stacking subjective annotations and domain\nover-fitting leading to poor model generalisation when applied in new contexts.\nThese problems are also likely to be compounded as we attempt to jointly\ndetermine additional contextual elements in the future. To mitigate these\nproblems, we present a hybrid neural-symbolic method utilising a Dependency\nTree-LSTM's compositional sentiment parse structure and complementary symbolic\nrules to correctly extract target-sentiment-cause triplets from sentences\nwithout the need for triplet training data. We show that this method has the\npotential to perform in line with state-of-the-art approaches while also\nsimplifying the data required and providing a degree of interpretability\nthrough the Tree-LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 01:52:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sutherland", "A.", ""], ["Bensch", "S.", ""], ["Hellstr\u00f6m", "T.", ""], ["Magg", "S.", ""], ["Wermter", "S.", ""]]}, {"id": "2103.05825", "submitter": "Suvir Mirchandani", "authors": "Suvir Mirchandani, Siddharth Karamcheti, Dorsa Sadigh", "title": "ELLA: Exploration through Learned Language Abstraction", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building agents capable of understanding language instructions is critical to\neffective and robust human-AI collaboration. Recent work focuses on training\nthese instruction following agents via reinforcement learning in environments\nwith synthetic language; however, these instructions often define long-horizon,\nsparse-reward tasks, and learning policies requires many episodes of\nexperience. To this end, we introduce ELLA: Exploration through Learned\nLanguage Abstraction, a reward shaping approach that correlates high-level\ninstructions with simpler low-level instructions to enrich the sparse rewards\nafforded by the environment. ELLA has two key elements: 1) A termination\nclassifier that identifies when agents complete low-level instructions, and 2)\nA relevance classifier that correlates low-level instructions with success on\nhigh-level tasks. We learn the termination classifier offline from pairs of\ninstructions and terminal states. Notably, in departure from prior work in\nlanguage and abstraction, we learn the relevance classifier online, without\nrelying on an explicit decomposition of high-level instructions to low-level\ninstructions. On a suite of complex grid world environments with varying\ninstruction complexities and reward sparsity, ELLA shows a significant gain in\nsample efficiency across several environments compared to competitive\nlanguage-based reward shaping and no-shaping methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 02:18:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mirchandani", "Suvir", ""], ["Karamcheti", "Siddharth", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2103.05841", "submitter": "Joshua Minot", "authors": "Joshua R. Minot, Nicholas Cheney, Marc Maier, Danne C. Elbers,\n  Christopher M. Danforth, and Peter Sheridan Dodds", "title": "Interpretable bias mitigation for textual data: Reducing gender bias in\n  patient notes while maintaining classification performance", "comments": "31 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical systems in general, and patient treatment decisions and outcomes in\nparticular, are affected by bias based on gender and other demographic\nelements. As language models are increasingly applied to medicine, there is a\ngrowing interest in building algorithmic fairness into processes impacting\npatient care. Much of the work addressing this question has focused on biases\nencoded in language models -- statistical estimates of the relationships\nbetween concepts derived from distant reading of corpora. Building on this\nwork, we investigate how word choices made by healthcare practitioners and\nlanguage models interact with regards to bias. We identify and remove gendered\nlanguage from two clinical-note datasets and describe a new debiasing procedure\nusing BERT-based gender classifiers. We show minimal degradation in health\ncondition classification tasks for low- to medium-levels of bias removal via\ndata augmentation. Finally, we compare the bias semantically encoded in the\nlanguage models with the bias empirically observed in health records. This work\noutlines an interpretable approach for using data augmentation to identify and\nreduce the potential for bias in natural language processing pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:09:30 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Minot", "Joshua R.", ""], ["Cheney", "Nicholas", ""], ["Maier", "Marc", ""], ["Elbers", "Danne C.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "2103.05908", "submitter": "Freddy C. Chua", "authors": "Freddy C. Chua, Nigel P. Duffy", "title": "DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the challenge of extracting structured information from business\ndocuments without detailed annotations. We propose Deep Conditional\nProbabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional\ncomplex documents and use Recursive Neural Networks to create an end-to-end\nsystem for finding the most probable parse that represents the structured\ninformation to be extracted. This system is trained end-to-end with scanned\ndocuments as input and only relational-records as labels. The\nrelational-records are extracted from existing databases avoiding the cost of\nannotating documents by hand. We apply this approach to extract information\nfrom scanned invoices achieving state-of-the-art results despite using no\nhand-annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:35:21 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 15:30:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chua", "Freddy C.", ""], ["Duffy", "Nigel P.", ""]]}, {"id": "2103.05944", "submitter": "Xiuying Chen", "authors": "Mingfei Guo, Xiuying Chen, Juntao Li, Dongyan Zhao, Rui Yan", "title": "How does Truth Evolve into Fake News? An Empirical Study of Fake News\n  Evolution", "comments": "5 pages, 2 figures", "journal-ref": "The Web Conference 2021, Workshop on News Recommendation and\n  Intelligence", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically identifying fake news from the Internet is a challenging\nproblem in deception detection tasks. Online news is modified constantly during\nits propagation, e.g., malicious users distort the original truth and make up\nfake news. However, the continuous evolution process would generate\nunprecedented fake news and cheat the original model. We present the Fake News\nEvolution (FNE) dataset: a new dataset tracking the fake news evolution\nprocess. Our dataset is composed of 950 paired data, each of which consists of\narticles representing the three significant phases of the evolution process,\nwhich are the truth, the fake news, and the evolved fake news. We observe the\nfeatures during the evolution and they are the disinformation techniques, text\nsimilarity, top 10 keywords, classification accuracy, parts of speech, and\nsentiment properties.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:01:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Guo", "Mingfei", ""], ["Chen", "Xiuying", ""], ["Li", "Juntao", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2103.05951", "submitter": "Surafel Melaku Lakew", "authors": "Surafel M. Lakew, Matteo Negri, Marco Turchi", "title": "Self-Learning for Zero Shot Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Machine Translation (NMT) approaches employing monolingual data are\nshowing steady improvements in resource rich conditions. However, evaluations\nusing real-world low-resource languages still result in unsatisfactory\nperformance. This work proposes a novel zero-shot NMT modeling approach that\nlearns without the now-standard assumption of a pivot language sharing parallel\ndata with the zero-shot source and target languages. Our approach is based on\nthree stages: initialization from any pre-trained NMT model observing at least\nthe target language, augmentation of source sides leveraging target monolingual\ndata, and learning to optimize the initial model to the zero-shot pair, where\nthe latter two constitute a self-learning cycle. Empirical findings involving\nfour diverse (in terms of a language family, script and relatedness) zero-shot\npairs show the effectiveness of our approach with up to +5.93 BLEU improvement\nagainst a supervised bilingual baseline. Compared to unsupervised NMT,\nconsistent improvements are observed even in a domain-mismatch setting,\nattesting to the usability of our method.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:15:19 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Lakew", "Surafel M.", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2103.06010", "submitter": "Lizhi Cheng", "authors": "Lizhi Cheng, Weijia Jia, Wenmian Yang", "title": "A Result based Portable Framework for Spoken Language Understanding", "comments": "ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU), which is a core component of the\ntask-oriented dialogue system, has made substantial progress in the research of\nsingle-turn dialogue. However, the performance in multi-turn dialogue is still\nnot satisfactory in the sense that the existing multi-turn SLU methods have low\nportability and compatibility for other single-turn SLU models. Further,\nexisting multi-turn SLU methods do not exploit the historical predicted results\nwhen predicting the current utterance, which wastes helpful information. To gap\nthose shortcomings, in this paper, we propose a novel Result-based Portable\nFramework for SLU (RPFSLU). RPFSLU allows most existing single-turn SLU models\nto obtain the contextual information from multi-turn dialogues and takes full\nadvantage of predicted results in the dialogue history during the current\nprediction. Experimental results on the public dataset KVRET have shown that\nall SLU models in baselines acquire enhancement by RPFSLU on multi-turn SLU\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:06:26 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Cheng", "Lizhi", ""], ["Jia", "Weijia", ""], ["Yang", "Wenmian", ""]]}, {"id": "2103.06057", "submitter": "Adarsh Kumar", "authors": "Yash Butala, Kanishk Singh, Adarsh Kumar and Shrey Shrivastava", "title": "Team Phoenix at WASSA 2021: Emotion Analysis on News Stories with\n  Pre-Trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion is fundamental to humanity. The ability to perceive, understand and\nrespond to social interactions in a human-like manner is one of the most\ndesired capabilities in artificial agents, particularly in social-media bots.\nOver the past few years, computational understanding and detection of emotional\naspects in language have been vital in advancing human-computer interaction.\nThe WASSA Shared Task 2021 released a dataset of news-stories across two\ntracks, Track-1 for Empathy and Distress Prediction and Track-2 for\nMulti-Dimension Emotion prediction at the essay-level. We describe our system\nentry for the WASSA 2021 Shared Task (for both Track-1 and Track-2), where we\nleveraged the information from Pre-trained language models for Track-specific\nTasks. Our proposed models achieved an Average Pearson Score of 0.417 and a\nMacro-F1 Score of 0.502 in Track 1 and Track 2, respectively. In the Shared\nTask leaderboard, we secured 4th rank in Track 1 and 2nd rank in Track 2.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:00:54 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Butala", "Yash", ""], ["Singh", "Kanishk", ""], ["Kumar", "Adarsh", ""], ["Shrivastava", "Shrey", ""]]}, {"id": "2103.06078", "submitter": "Sachin Pawar", "authors": "Sachin Pawar, Ravina More, Girish K. Palshikar, Pushpak Bhattacharyya,\n  Vasudeva Varma", "title": "Knowledge-based Extraction of Cause-Effect Relations from Biomedical\n  Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a knowledge-based approach for extraction of Cause-Effect (CE)\nrelations from biomedical text. Our approach is a combination of an\nunsupervised machine learning technique to discover causal triggers and a set\nof high-precision linguistic rules to identify cause/effect arguments of these\ncausal triggers. We evaluate our approach using a corpus of 58,761\nLeukaemia-related PubMed abstracts consisting of 568,528 sentences. We could\nextract 152,655 CE triplets from this corpus where each triplet consists of a\ncause phrase, an effect phrase and a causal trigger. As compared to the\nexisting knowledge base - SemMedDB (Kilicoglu et al., 2012), the number of\nextractions are almost twice. Moreover, the proposed approach outperformed the\nexisting technique SemRep (Rindflesch and Fiszman, 2003) on a dataset of 500\nsentences.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:31:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Pawar", "Sachin", ""], ["More", "Ravina", ""], ["Palshikar", "Girish K.", ""], ["Bhattacharyya", "Pushpak", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2103.06089", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Charlie Nash, Jesse Engel, Karen Simonyan", "title": "Variable-rate discrete representation learning", "comments": "26 pages, 15 figures, samples can be found at https://vdrl.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantically meaningful information content in perceptual signals is usually\nunevenly distributed. In speech signals for example, there are often many\nsilences, and the speed of pronunciation can vary considerably. In this work,\nwe propose slow autoencoders (SlowAEs) for unsupervised learning of high-level\nvariable-rate discrete representations of sequences, and apply them to speech.\nWe show that the resulting event-based representations automatically grow or\nshrink depending on the density of salient information in the input signals,\nwhile still allowing for faithful signal reconstruction. We develop run-length\nTransformers (RLTs) for event-based representation modelling and use them to\nconstruct language models in the speech domain, which are able to generate\ngrammatical and semantically coherent utterances and continuations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:42:31 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Dieleman", "Sander", ""], ["Nash", "Charlie", ""], ["Engel", "Jesse", ""], ["Simonyan", "Karen", ""]]}, {"id": "2103.06118", "submitter": "Sachin Pawar", "authors": "Sachin Pawar, Pushpak Bhattacharyya, Girish K. Palshikar", "title": "Techniques for Jointly Extracting Entities and Relations: A Survey", "comments": "Accepted at 20th International Conference on Computational\n  Linguistics and Intelligent Text Processing (CICLing 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation Extraction is an important task in Information Extraction which\ndeals with identifying semantic relations between entity mentions.\nTraditionally, relation extraction is carried out after entity extraction in a\n\"pipeline\" fashion, so that relation extraction only focuses on determining\nwhether any semantic relation exists between a pair of extracted entity\nmentions. This leads to propagation of errors from entity extraction stage to\nrelation extraction stage. Also, entity extraction is carried out without any\nknowledge about the relations. Hence, it was observed that jointly performing\nentity and relation extraction is beneficial for both the tasks. In this paper,\nwe survey various techniques for jointly extracting entities and relations. We\ncategorize techniques based on the approach they adopt for joint extraction,\ni.e. whether they employ joint inference or joint modelling or both. We further\ndescribe some representative techniques for joint inference and joint\nmodelling. We also describe two standard datasets, evaluation techniques and\nperformance of the joint extraction approaches on these datasets. We present a\nbrief analysis of application of a general domain joint extraction approach to\na Biomedical dataset. This survey is useful for researchers as well as\npractitioners in the field of Information Extraction, by covering a broad\nlandscape of joint extraction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:18:24 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Pawar", "Sachin", ""], ["Bhattacharyya", "Pushpak", ""], ["Palshikar", "Girish K.", ""]]}, {"id": "2103.06130", "submitter": "Jumanah Alshehri", "authors": "Jumanah Alshehri, Marija Stanojevic, Eduard Dragut, Zoran Obradovic", "title": "Stay on Topic, Please: Aligning User Comments to the Content of a News\n  Article", "comments": "Accepted as a full paper at the 43rd European Conference on\n  Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists have shown that up to 50% if the content posted to a news\narticle have no relation to its journalistic content. In this study we propose\na classification algorithm to categorize user comments posted to a new article\nbase don their alignment to its content. The alignment seek to match user\ncomments to an article based on similarity off content, entities in discussion,\nand topic. We proposed a BERTAC, BAERT-based approach that learn jointly\narticle-comment embeddings and infers the relevance class of comments. We\nintroduce an ordinal classification loss that penalizes the difference between\nthe predicted and true label. We conduct a thorough study to show influence of\nthe proposed loss on the learning process. The results on five representative\nnews outlets show that our approach can learn the comment class with up to 36%\naverage accuracy improvement compering to the baselines, and up to 25%\ncompering to the BA-BC model. BA-BC is out approach that consists of two models\naimed to capture dis-jointly the formal language of news articles and the\ninformal language of comments. We also conduct a user study to evaluate human\nlabeling performance to understand the difficulty of the classification task.\nThe user agreement on comment-article alignment is \"moderate\" per\nKrippendorff's alpha score, which suggests that the classification task is\ndifficult.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:29:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Alshehri", "Jumanah", ""], ["Stanojevic", "Marija", ""], ["Dragut", "Eduard", ""], ["Obradovic", "Zoran", ""]]}, {"id": "2103.06198", "submitter": "Radha Kopparti", "authors": "Radha Kopparti and Tillman Weyde", "title": "Relational Weight Priors in Neural Networks for Abstract Pattern\n  Learning and Language Modelling", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have become the dominant approach in natural language\nprocessing (NLP). However, in recent years, it has become apparent that there\nare shortcomings in systematicity that limit the performance and data\nefficiency of deep learning in NLP. These shortcomings can be clearly shown in\nlower-level artificial tasks, mostly on synthetic data. Abstract patterns are\nthe best known examples of a hard problem for neural networks in terms of\ngeneralisation to unseen data. They are defined by relations between items,\nsuch as equality, rather than their values. It has been argued that these\nlow-level problems demonstrate the inability of neural networks to learn\nsystematically. In this study, we propose Embedded Relation Based Patterns\n(ERBP) as a novel way to create a relational inductive bias that encourages\nlearning equality and distance-based relations for abstract patterns. ERBP is\nbased on Relation Based Patterns (RBP), but modelled as a Bayesian prior on\nnetwork weights and implemented as a regularisation term in otherwise standard\nnetwork learning. ERBP is is easy to integrate into standard neural networks\nand does not affect their learning capacity. In our experiments, ERBP priors\nlead to almost perfect generalisation when learning abstract patterns from\nsynthetic noise-free sequences. ERBP also improves natural language models on\nthe word and character level and pitch prediction in melodies with RNN, GRU and\nLSTM networks. We also find improvements in in the more complex tasks of\nlearning of graph edit distance and compositional sentence entailment. ERBP\nconsistently improves over RBP and over standard networks, showing that it\nenables abstract pattern learning which contributes to performance in natural\nlanguage tasks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:21:16 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kopparti", "Radha", ""], ["Weyde", "Tillman", ""]]}, {"id": "2103.06268", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball", "title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review", "comments": "Code and the CUAD dataset are available at\n  https://github.com/TheAtticusProject/cuad/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many specialized domains remain untouched by deep learning, as large labeled\ndatasets require expensive expert annotators. We address this bottleneck within\nthe legal domain by introducing the Contract Understanding Atticus Dataset\n(CUAD), a new dataset for legal contract review. CUAD was created with dozens\nof legal experts from The Atticus Project and consists of over 13,000\nannotations. The task is to highlight salient portions of a contract that are\nimportant for a human to review. We find that Transformer models have nascent\nperformance, but that this performance is strongly influenced by model design\nand training dataset size. Despite these promising results, there is still\nsubstantial room for improvement. As one of the only large, specialized NLP\nbenchmarks annotated by experts, CUAD can serve as a challenging research\nbenchmark for the broader NLP community.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:59:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Hendrycks", "Dan", ""], ["Burns", "Collin", ""], ["Chen", "Anya", ""], ["Ball", "Spencer", ""]]}, {"id": "2103.06304", "submitter": "Letitia Parcalabescu", "authors": "Letitia Parcalabescu, Nils Trost, Anette Frank", "title": "What is Multimodality?", "comments": "Paper accepted for publication at MMSR 2021; 10 pages, 5 figures", "journal-ref": "Proceedings of the 1st Workshop on Multimodal Semantic\n  Representations (MMSR), 2021, Groningen, Netherlands (Online), Association\n  for Computational Linguistics, p. 1--10", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last years have shown rapid developments in the field of multimodal\nmachine learning, combining e.g., vision, text or speech. In this position\npaper we explain how the field uses outdated definitions of multimodality that\nprove unfit for the machine learning era. We propose a new task-relative\ndefinition of (multi)modality in the context of multimodal machine learning\nthat focuses on representations and information that are relevant for a given\nmachine learning task. With our new definition of multimodality we aim to\nprovide a missing foundation for multimodal research, an important component of\nlanguage grounding and a crucial milestone towards NLU.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 19:14:07 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 09:17:44 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:32:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Parcalabescu", "Letitia", ""], ["Trost", "Nils", ""], ["Frank", "Anette", ""]]}, {"id": "2103.06332", "submitter": "Kalpesh Krishna", "authors": "Kalpesh Krishna, Aurko Roy, Mohit Iyyer", "title": "Hurdles to Progress in Long-form Question Answering", "comments": "NAACL 2021 camera ready (18 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of long-form question answering (LFQA) involves retrieving documents\nrelevant to a given question and using them to generate a paragraph-length\nanswer. While many models have recently been proposed for LFQA, we show in this\npaper that the task formulation raises fundamental challenges regarding\nevaluation and dataset creation that currently preclude meaningful modeling\nprogress. To demonstrate these challenges, we first design a new system that\nrelies on sparse attention and contrastive retriever learning to achieve\nstate-of-the-art performance on the ELI5 LFQA dataset. While our system tops\nthe public leaderboard, a detailed analysis reveals several troubling trends:\n(1) our system's generated answers are not actually grounded in the documents\nthat it retrieves; (2) ELI5 contains significant train / validation overlap, as\nat least 81% of ELI5 validation questions occur in paraphrased form in the\ntraining set; (3) ROUGE-L is not an informative metric of generated answer\nquality and can be easily gamed; and (4) human evaluations used for other text\ngeneration tasks are unreliable for LFQA. We offer suggestions to mitigate each\nof these issues, which we hope will lead to more rigorous LFQA research and\nmeaningful progress in the future.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:32:30 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 12:01:54 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Krishna", "Kalpesh", ""], ["Roy", "Aurko", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2103.06333", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei\n  Chang", "title": "Unified Pre-training for Program Understanding and Generation", "comments": "NAACL 2021 (camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code summarization and generation empower conversion between programming\nlanguage (PL) and natural language (NL), while code translation avails the\nmigration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program\nand language understanding and generation tasks. PLBART is pre-trained on an\nextensive collection of Java and Python functions and associated NL text via\ndenoising autoencoding. Experiments on code summarization in the English\nlanguage, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover,\nexperiments on discriminative tasks, e.g., program repair, clone detection, and\nvulnerable code detection, demonstrate PLBART's effectiveness in program\nunderstanding. Furthermore, analysis reveals that PLBART learns program syntax,\nstyle (e.g., identifier naming convention), logical flow (e.g., if block inside\nan else block is equivalent to else if block) that are crucial to program\nsemantics and thus excels even with limited annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:32:59 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 19:48:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Chakraborty", "Saikat", ""], ["Ray", "Baishakhi", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2103.06352", "submitter": "Kevin Lybarger", "authors": "Kevin Lybarger, Linzee Mabrey, Matthew Thau, Pavan K. Bhatraju, Mark\n  Wurfel, Meliha Yetisgen", "title": "Identifying ARDS using the Hierarchical Attention Network with Sentence\n  Objectives Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute respiratory distress syndrome (ARDS) is a life-threatening condition\nthat is often undiagnosed or diagnosed late. ARDS is especially prominent in\nthose infected with COVID-19. We explore the automatic identification of ARDS\nindicators and confounding factors in free-text chest radiograph reports. We\npresent a new annotated corpus of chest radiograph reports and introduce the\nHierarchical Attention Network with Sentence Objectives (HANSO) text\nclassification framework. HANSO utilizes fine-grained annotations to improve\ndocument classification performance. HANSO can extract ARDS-related information\nwith high performance by leveraging relation annotations, even if the annotated\nspans are noisy. Using annotated chest radiograph images as a gold standard,\nHANSO identifies bilateral infiltrates, an indicator of ARDS, in chest\nradiograph reports with performance (0.87 F1) comparable to human annotations\n(0.84 F1). This algorithm could facilitate more efficient and expeditious\nidentification of ARDS by clinicians and researchers and contribute to the\ndevelopment of new therapies to improve patient care.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:50:11 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lybarger", "Kevin", ""], ["Mabrey", "Linzee", ""], ["Thau", "Matthew", ""], ["Bhatraju", "Pavan K.", ""], ["Wurfel", "Mark", ""], ["Yetisgen", "Meliha", ""]]}, {"id": "2103.06357", "submitter": "Ari Klein", "authors": "Ari Z. Klein, Arjun Magge, Graciela Gonzalez-Hernandez", "title": "ReportAGE: Automatically extracting the exact age of Twitter users based\n  on self-reports in tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancing the utility of social media data for research applications requires\nmethods for automatically detecting demographic information about social media\nstudy populations, including users' age. The objective of this study was to\ndevelop and evaluate a method that automatically identifies the exact age of\nusers based on self-reports in their tweets. Our end-to-end automatic natural\nlanguage processing (NLP) pipeline, ReportAGE, includes query patterns to\nretrieve tweets that potentially mention an age, a classifier to distinguish\nretrieved tweets that self-report the user's exact age (\"age\" tweets) and those\nthat do not (\"no age\" tweets), and rule-based extraction to identify the age.\nTo develop and evaluate ReportAGE, we manually annotated 11,000 tweets that\nmatched the query patterns. Based on 1000 tweets that were annotated by all\nfive annotators, inter-annotator agreement (Fleiss' kappa) was 0.80 for\ndistinguishing \"age\" and \"no age\" tweets, and 0.95 for identifying the exact\nage among the \"age\" tweets on which the annotators agreed. A deep neural\nnetwork classifier, based on a RoBERTa-Large pretrained model, achieved the\nhighest F1-score of 0.914 (precision = 0.905, recall = 0.942) for the \"age\"\nclass. When the age extraction was evaluated using the classifier's\npredictions, it achieved an F1-score of 0.855 (precision = 0.805, recall =\n0.914) for the \"age\" class. When it was evaluated directly on the held-out test\nset, it achieved an F1-score of 0.931 (precision = 0.873, recall = 0.998) for\nthe \"age\" class. We deployed ReportAGE on more than 1.2 billion tweets posted\nby 245,927 users, and predicted ages for 132,637 (54%) of them. Scaling the\ndetection of exact age to this large number of users can advance the utility of\nsocial media data for research applications that do not align with the\npredefined age groupings of extant binary or multi-class classification\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:00:19 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Klein", "Ari Z.", ""], ["Magge", "Arjun", ""], ["Gonzalez-Hernandez", "Graciela", ""]]}, {"id": "2103.06369", "submitter": "Alexander Jones", "authors": "Alex Jones and Derry Tanti Wijaya", "title": "Majority Voting with Bidirectional Pre-translation For Bitext Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Obtaining high-quality parallel corpora is of paramount importance for\ntraining NMT systems. However, as many language pairs lack adequate\ngold-standard training data, a popular approach has been to mine so-called\n\"pseudo-parallel\" sentences from paired documents in two languages. In this\npaper, we outline some problems with current methods, propose computationally\neconomical solutions to those problems, and demonstrate success with novel\nmethods on the Tatoeba similarity search benchmark and on a downstream task,\nnamely NMT. We uncover the effect of resource-related factors (i.e. how much\nmonolingual/bilingual data is available for a given language) on the optimal\nchoice of bitext mining approach, and echo problems with the oft-used BUCC\ndataset that have been observed by others. We make the code and data used for\nour experiments publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:24:01 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:59:49 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Jones", "Alex", ""], ["Wijaya", "Derry Tanti", ""]]}, {"id": "2103.06370", "submitter": "Govardana Sachithanandam Ramachandran", "authors": "Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, Caiming Xiong", "title": "Causal-aware Safe Policy Improvement for Task-oriented dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of reinforcement learning's (RL) in solving complex tasks\nis most often attributed to its capacity to explore and exploit an environment\nwhere it has been trained. Sample efficiency is usually not an issue since\ncheap simulators are available to sample data on-policy. On the other hand,\ntask oriented dialogues are usually learnt from offline data collected using\nhuman demonstrations. Collecting diverse demonstrations and annotating them is\nexpensive. Unfortunately, use of RL methods trained on off-policy data are\nprone to issues of bias and generalization, which are further exacerbated by\nstochasticity in human response and non-markovian belief state of a dialogue\nmanagement system. To this end, we propose a batch RL framework for task\noriented dialogue policy learning: causal aware safe policy improvement\n(CASPI). This method gives guarantees on dialogue policy's performance and also\nlearns to shape rewards according to intentions behind human responses, rather\nthan just mimicking demonstration data; this couple with batch-RL helps overall\nwith sample efficiency of the framework. We demonstrate the effectiveness of\nthis framework on a dialogue-context-to-text Generation and end-to-end dialogue\ntask of the Multiwoz2.0 dataset. The proposed method outperforms the current\nstate of the art on these metrics, in both case. In the end-to-end case, our\nmethod trained only on 10\\% of the data was able to out perform current state\nin three out of four evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:34:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ramachandran", "Govardana Sachithanandam", ""], ["Hashimoto", "Kazuma", ""], ["Xiong", "Caiming", ""]]}, {"id": "2103.06402", "submitter": "Seanie Lee", "authors": "Donggyu Kim, Seanie Lee", "title": "Self-supervised Text-to-SQL Learning with Header Alignment Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since we can leverage a large amount of unlabeled data without any human\nsupervision to train a model and transfer the knowledge to target tasks,\nself-supervised learning is a de-facto component for the recent success of deep\nlearning in various fields. However, in many cases, there is a discrepancy\nbetween a self-supervised learning objective and a task-specific objective. In\norder to tackle such discrepancy in Text-to-SQL task, we propose a novel\nself-supervised learning framework. We utilize the task-specific properties of\nText-to-SQL task and the underlying structures of table contents to train the\nmodels to learn useful knowledge of the \\textit{header-column} alignment task\nfrom unlabeled table data. We are able to transfer the knowledge to the\nsupervised Text-to-SQL training with annotated samples, so that the model can\nleverage the knowledge to better perform the \\textit{header-span} alignment\ntask to predict SQL statements. Experimental results show that our\nself-supervised learning framework significantly improves the performance of\nthe existing strong BERT based models without using large external corpora. In\nparticular, our method is effective for training the model with scarce labeled\ndata. The source code of this work is available in GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 01:09:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Kim", "Donggyu", ""], ["Lee", "Seanie", ""]]}, {"id": "2103.06410", "submitter": "Chenguang Zhu", "authors": "Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng", "title": "MediaSum: A Large-scale Media Interview Dataset for Dialogue\n  Summarization", "comments": "Dataset: https://github.com/zcgzcgzcg1/MediaSum/", "journal-ref": "North American Chapter of the Association for Computational\n  Linguistics (NAACL), Mexico City, Mexico, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MediaSum, a large-scale media interview dataset consisting of 463.6K\ntranscripts with abstractive summaries. To create this dataset, we collect\ninterview transcripts from NPR and CNN and employ the overview and topic\ndescriptions as summaries. Compared with existing public corpora for dialogue\nsummarization, our dataset is an order of magnitude larger and contains complex\nmulti-party conversations from multiple domains. We conduct statistical\nanalysis to demonstrate the unique positional bias exhibited in the transcripts\nof televised and radioed interviews. We also show that MediaSum can be used in\ntransfer learning to improve a model's performance on other dialogue\nsummarization tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 01:47:42 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 01:47:14 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhu", "Chenguang", ""], ["Liu", "Yang", ""], ["Mei", "Jie", ""], ["Zeng", "Michael", ""]]}, {"id": "2103.06413", "submitter": "Pengyu Cheng", "authors": "Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, Lawrence Carin", "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text\n  Encoders", "comments": "Accepted by the 9th International Conference on Learning\n  Representations (ICLR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained text encoders, such as BERT, have been applied increasingly in\nvarious natural language processing (NLP) tasks, and have recently demonstrated\nsignificant performance gains. However, recent studies have demonstrated the\nexistence of social bias in these pretrained NLP models. Although prior works\nhave made progress on word-level debiasing, improved sentence-level fairness of\npretrained encoders still lacks exploration. In this paper, we proposed the\nfirst neural debiasing method for a pretrained sentence encoder, which\ntransforms the pretrained encoder outputs into debiased representations via a\nfair filter (FairFil) network. To learn the FairFil, we introduce a contrastive\nlearning framework that not only minimizes the correlation between filtered\nembeddings and bias words but also preserves rich semantic information of the\noriginal sentences. On real-world datasets, our FairFil effectively reduces the\nbias degree of pretrained text encoders, while continuously showing desirable\nperformance on downstream tasks. Moreover, our post-hoc method does not require\nany retraining of the text encoders, further enlarging FairFil's application\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:01:14 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Cheng", "Pengyu", ""], ["Hao", "Weituo", ""], ["Yuan", "Siyang", ""], ["Si", "Shijing", ""], ["Carin", "Lawrence", ""]]}, {"id": "2103.06418", "submitter": "Xiaoqi Jiao", "authors": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin\n  Li, Fang Wang and Qun Liu", "title": "LightMBERT: A Simple Yet Effective Method for Multilingual BERT\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The multilingual pre-trained language models (e.g, mBERT, XLM and XLM-R) have\nshown impressive performance on cross-lingual natural language understanding\ntasks. However, these models are computationally intensive and difficult to be\ndeployed on resource-restricted devices. In this paper, we propose a simple yet\neffective distillation method (LightMBERT) for transferring the cross-lingual\ngeneralization ability of the multilingual BERT to a small student model. The\nexperiment results empirically demonstrate the efficiency and effectiveness of\nLightMBERT, which is significantly better than the baselines and performs\ncomparable to the teacher mBERT.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:24:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Jiao", "Xiaoqi", ""], ["Yin", "Yichun", ""], ["Shang", "Lifeng", ""], ["Jiang", "Xin", ""], ["Chen", "Xiao", ""], ["Li", "Linlin", ""], ["Wang", "Fang", ""], ["Liu", "Qun", ""]]}, {"id": "2103.06434", "submitter": "Rohola Zandie", "authors": "Rohola Zandie and Mohammad H. Mahoor", "title": "Topical Language Generation using Transformers", "comments": "Accepted in the Journal of Natural Language Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale transformer-based language models (LMs) demonstrate impressive\ncapabilities in open text generation. However, controlling the generated text's\nproperties such as the topic, style, and sentiment is challenging and often\nrequires significant changes to the model architecture or retraining and\nfine-tuning the model on new supervised data. This paper presents a novel\napproach for Topical Language Generation (TLG) by combining a pre-trained LM\nwith topic modeling information. We cast the problem using Bayesian probability\nformulation with topic probabilities as a prior, LM probabilities as the\nlikelihood, and topical language generation probability as the posterior. In\nlearning the model, we derive the topic probability distribution from the\nuser-provided document's natural structure. Furthermore, we extend our model by\nintroducing new parameters and functions to influence the quantity of the\ntopical features presented in the generated text. This feature would allow us\nto easily control the topical properties of the generated text. Our\nexperimental results demonstrate that our model outperforms the\nstate-of-the-art results on coherency, diversity, and fluency while being\nfaster in decoding.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:45:24 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zandie", "Rohola", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "2103.06450", "submitter": "Sumeet Sohan Singh", "authors": "Sumeet S. Singh, Sergey Karayev", "title": "Full Page Handwriting Recognition via Image to Sequence Extraction", "comments": "To appear in ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:37:29 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 18:52:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Singh", "Sumeet S.", ""], ["Karayev", "Sergey", ""]]}, {"id": "2103.06459", "submitter": "Linlin Liu", "authors": "Linlin Liu, Thien Hai Nguyen, Shafiq Joty, Lidong Bing, Luo Si", "title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual word embeddings (CLWE) have been proven useful in many\ncross-lingual tasks. However, most existing approaches to learn CLWE including\nthe ones with contextual embeddings are sense agnostic. In this work, we\npropose a novel framework to align contextual embeddings at the sense level by\nleveraging cross-lingual signal from bilingual dictionaries only. We\noperationalize our framework by first proposing a novel sense-aware cross\nentropy loss to model word senses explicitly. The monolingual ELMo and BERT\nmodels pretrained with our sense-aware cross entropy loss demonstrate\nsignificant performance improvement for word sense disambiguation tasks. We\nthen propose a sense alignment objective on top of the sense-aware cross\nentropy loss for cross-lingual model pretraining, and pretrain cross-lingual\nmodels for several language pairs (English to German/Spanish/Japanese/Chinese).\nCompared with the best baseline results, our cross-lingual models achieve\n0.52%, 2.09% and 1.29% average performance improvements on zero-shot\ncross-lingual NER, sentiment classification and XNLI tasks, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:55:35 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liu", "Linlin", ""], ["Nguyen", "Thien Hai", ""], ["Joty", "Shafiq", ""], ["Bing", "Lidong", ""], ["Si", "Luo", ""]]}, {"id": "2103.06490", "submitter": "Parag Dutta", "authors": "Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir,\n  Ambedkar Dukkipati", "title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation", "comments": "Two of the authors had published similar manuscripts on arXiv. So\n  withdrawing this one. All further updations will be reflected at\n  arXiv:1911.00234", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate further such redundant examples\nchosen by an AL strategy. We show that A$\\mathbf{^2}$L is widely applicable by\nusing it in conjunction with several different AL strategies and NLP tasks. We\nempirically demonstrate that the proposed approach is further able to reduce\nthe data requirements of state-of-the-art AL strategies by an absolute\npercentage reduction of $\\approx\\mathbf{3-25\\%}$ on multiple NLP tasks while\nachieving the same performance with no additional computation overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:27:31 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:49:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hazra", "Rishi", ""], ["Dutta", "Parag", ""], ["Gupta", "Shubham", ""], ["Qaathir", "Mohammed Abdul", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "2103.06500", "submitter": "Stanislav Peshterliev", "authors": "Stan Peshterliev, Barlas Oguz, Debojeet Chatterjee, Hakan Inan, Vikas\n  Bhardwaj", "title": "Conversational Answer Generation and Factuality for Reading\n  Comprehension Question-Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) is an important use case on voice assistants. A\npopular approach to QA is extractive reading comprehension (RC) which finds an\nanswer span in a text passage. However, extractive answers are often unnatural\nin a conversational context which results in suboptimal user experience. In\nthis work, we investigate conversational answer generation for QA. We propose\nAnswerBART, an end-to-end generative RC model which combines answer generation\nfrom multiple passages with passage ranking and answerability. Moreover, a\nhurdle in applying generative RC are hallucinations where the answer is\nfactually inconsistent with the passage text. We leverage recent work from\nsummarization to evaluate factuality. Experiments show that AnswerBART\nsignificantly improves over previous best published results on MS MARCO 2.1\nNLGEN by 2.5 ROUGE-L and NarrativeQA by 9.4 ROUGE-L.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:53:07 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Peshterliev", "Stan", ""], ["Oguz", "Barlas", ""], ["Chatterjee", "Debojeet", ""], ["Inan", "Hakan", ""], ["Bhardwaj", "Vikas", ""]]}, {"id": "2103.06511", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji and Matti H\\\"oltt\\\"a and Pekka Marttinen", "title": "Does the Magic of BERT Apply to Medical Code Assignment? A Quantitative\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised pretraining is an integral part of many natural language\nprocessing systems, and transfer learning with language models has achieved\nremarkable results in many downstream tasks. In the clinical application of\nmedical code assignment, diagnosis and procedure codes are inferred from\nlengthy clinical notes such as hospital discharge summaries. However, it is not\nclear if pretrained models are useful for medical code prediction without\nfurther architecture engineering. This paper conducts a comprehensive\nquantitative analysis of various contextualized language models' performance,\npretrained in different domains, for medical code assignment from clinical\nnotes. We propose a hierarchical fine-tuning architecture to capture\ninteractions between distant words and adopt label-wise attention to exploit\nlabel information. Contrary to current trends, we demonstrate that a carefully\ntrained classical CNN outperforms attention-based models on a MIMIC-III subset\nwith frequent codes. Our empirical findings suggest directions for improving\nthe medical code assignment application.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:23:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ji", "Shaoxiong", ""], ["H\u00f6ltt\u00e4", "Matti", ""], ["Marttinen", "Pekka", ""]]}, {"id": "2103.06598", "submitter": "Anne Lauscher", "authors": "Niklas Friedrich, Anne Lauscher, Simone Paolo Ponzetto and Goran\n  Glava\\v{s}", "title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding\n  Spaces", "comments": "Accepted as EACL21 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research efforts in NLP have demonstrated that distributional word\nvector spaces often encode stereotypical human biases, such as racism and\nsexism. With word representations ubiquitously used in NLP models and\npipelines, this raises ethical issues and jeopardizes the fairness of language\ntechnologies. While there exists a large body of work on bias measures and\ndebiasing methods, to date, there is no platform that would unify these\nresearch efforts and make bias measuring and debiasing of representation spaces\nwidely accessible. In this work, we present DebIE, the first integrated\nplatform for (1) measuring and (2) mitigating bias in word embeddings. Given an\n(i) embedding space (users can choose between the predefined spaces or upload\ntheir own) and (ii) a bias specification (users can choose between existing\nbias specifications or create their own), DebIE can (1) compute several\nmeasures of implicit and explicit bias and modify the embedding space by\nexecuting two (mutually composable) debiasing models. DebIE's functionality can\nbe accessed through four different interfaces: (a) a web application, (b) a\ndesktop application, (c) a REST-ful API, and (d) as a command-line application.\nDebIE is available at: debie.informatik.uni-mannheim.de.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:55:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Friedrich", "Niklas", ""], ["Lauscher", "Anne", ""], ["Ponzetto", "Simone Paolo", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2103.06605", "submitter": "Jingang Wang", "authors": "Jiahao Bu, Lei Ren, Shuang Zheng, Yang Yang, Jingang Wang, Fuzheng\n  Zhang, Wei Wu", "title": "ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment\n  Analysis and Rating Prediction", "comments": "11 Pages, 5 Figures, Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis has attracted increasing attention in e-commerce. The\nsentiment polarities underlying user reviews are of great value for business\nintelligence. Aspect category sentiment analysis (ACSA) and review rating\nprediction (RP) are two essential tasks to detect the fine-to-coarse sentiment\npolarities. %Considering the sentiment of the aspects(ACSA) and the overall\nreview rating(RP) simultaneously has the potential to improve the overall\nperformance. ACSA and RP are highly correlated and usually employed jointly in\nreal-world e-commerce scenarios. While most public datasets are constructed for\nACSA and RP separately, which may limit the further exploitation of both tasks.\nTo address the problem and advance related researches, we present a large-scale\nChinese restaurant review dataset \\textbf{ASAP} including $46,730$ genuine\nreviews from a leading online-to-offline (O2O) e-commerce platform in China.\nBesides a $5$-star scale rating, each review is manually annotated according to\nits sentiment polarities towards $18$ pre-defined aspect categories. We hope\nthe release of the dataset could shed some light on the fields of sentiment\nanalysis. Moreover, we propose an intuitive yet effective joint model for ACSA\nand RP. Experimental results demonstrate that the joint model outperforms\nstate-of-the-art baselines on both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:00:59 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:32:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bu", "Jiahao", ""], ["Ren", "Lei", ""], ["Zheng", "Shuang", ""], ["Yang", "Yang", ""], ["Wang", "Jingang", ""], ["Zhang", "Fuzheng", ""], ["Wu", "Wei", ""]]}, {"id": "2103.06628", "submitter": "Albina Khusainova", "authors": "Vitaly Romanov and Albina Khusainova", "title": "Evaluation of Morphological Embeddings for the Russian Language", "comments": "Published in Proceedings of the 2019 3rd International Conference on\n  Natural Language Processing and Information Retrieval", "journal-ref": null, "doi": "10.1145/3342827.3342846", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A number of morphology-based word embedding models were introduced in recent\nyears. However, their evaluation was mostly limited to English, which is known\nto be a morphologically simple language. In this paper, we explore whether and\nto what extent incorporating morphology into word embeddings improves\nperformance on downstream NLP tasks, in the case of morphologically rich\nRussian language. NLP tasks of our choice are POS tagging, Chunking, and NER --\nfor Russian language, all can be mostly solved using only morphology without\nunderstanding the semantics of words. Our experiments show that\nmorphology-based embeddings trained with Skipgram objective do not outperform\nexisting embedding model -- FastText. Moreover, a more complex, but morphology\nunaware model, BERT, allows to achieve significantly greater performance on the\ntasks that presumably require understanding of a word's morphology.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:59:11 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Romanov", "Vitaly", ""], ["Khusainova", "Albina", ""]]}, {"id": "2103.06648", "submitter": "Hyunmin Jeon", "authors": "Hyunmin Jeon, Gary Geunbae Lee", "title": "Domain State Tracking for a Simplified Dialogue System", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-oriented dialogue systems aim to help users achieve their goals in\nspecific domains. Recent neural dialogue systems use the entire dialogue\nhistory for abundant contextual information accumulated over multiple\nconversational turns. However, the dialogue history becomes increasingly longer\nas the number of turns increases, thereby increasing memory usage and\ncomputational costs. In this paper, we present DoTS (Domain State Tracking for\na Simplified Dialogue System), a task-oriented dialogue system that uses a\nsimplified input context instead of the entire dialogue history. However,\nneglecting the dialogue history can result in a loss of contextual information\nfrom previous conversational turns. To address this issue, DoTS tracks the\ndomain state in addition to the belief state and uses it for the input context.\nUsing this simplified input, DoTS improves the inform rate and success rate by\n1.09 points and 1.24 points, respectively, compared to the previous\nstate-of-the-art model on MultiWOZ, which is a well-known benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 13:00:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Jeon", "Hyunmin", ""], ["Lee", "Gary Geunbae", ""]]}, {"id": "2103.06678", "submitter": "Go Inoue", "authors": "Go Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda Bouamor, Nizar\n  Habash", "title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained\n  Language Models", "comments": "Accepted to WANLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:11:43 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Inoue", "Go", ""], ["Alhafni", "Bashar", ""], ["Baimukan", "Nurpeiis", ""], ["Bouamor", "Houda", ""], ["Habash", "Nizar", ""]]}, {"id": "2103.06689", "submitter": "Carlos Mullov", "authors": "Carlos Mullov and Ngoc-Quan Pham and Alexander Waibel", "title": "Unsupervised Transfer Learning in Multilingual Neural Machine\n  Translation with Cross-Lingual Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we look into adding a new language to a multilingual NMT system\nin an unsupervised fashion. Under the utilization of pre-trained cross-lingual\nword embeddings we seek to exploit a language independent multilingual sentence\nrepresentation to easily generalize to a new language. While using\ncross-lingual embeddings for word lookup we decode from a yet entirely unseen\nsource language in a process we call blind decoding. Blindly decoding from\nPortuguese using a basesystem containing several Romance languages we achieve\nscores of 36.4 BLEU for Portuguese-English and 12.8 BLEU for Russian-English.\nIn an attempt to train the mapping from the encoder sentence representation to\na new target language we use our model as an autoencoder. Merely training to\ntranslate from Portuguese to Portuguese while freezing the encoder we achieve\n26 BLEU on English-Portuguese, and up to 28 BLEU when adding artificial noise\nto the input. Lastly we explore a more practical adaptation approach through\nnon-iterative backtranslation, exploiting our model's ability to produce high\nquality translations through blind decoding. This yields us up to 34.6 BLEU on\nEnglish-Portuguese, attaining near parity with a model adapted on real\nbilingual data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:22:08 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Mullov", "Carlos", ""], ["Pham", "Ngoc-Quan", ""], ["Waibel", "Alexander", ""]]}, {"id": "2103.06716", "submitter": "David Qiu", "authors": "David Qiu, Qiujia Li, Yanzhang He, Yu Zhang, Bo Li, Liangliang Cao,\n  Rohit Prabhavalkar, Deepti Bhatia, Wei Li, Ke Hu, Tara N. Sainath, Ian McGraw", "title": "Learning Word-Level Confidence For Subword End-to-End ASR", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of word-level confidence estimation in subword-based\nend-to-end (E2E) models for automatic speech recognition (ASR). Although prior\nworks have proposed training auxiliary confidence models for ASR systems, they\ndo not extend naturally to systems that operate on word-pieces (WP) as their\nvocabulary. In particular, ground truth WP correctness labels are needed for\ntraining confidence models, but the non-unique tokenization from word to WP\ncauses inaccurate labels to be generated. This paper proposes and studies two\nconfidence models of increasing complexity to solve this problem. The final\nmodel uses self-attention to directly learn word-level confidence without\nneeding subword tokenization, and exploits full context features from multiple\nhypotheses to improve confidence accuracy. Experiments on Voice Search and\nlong-tail test sets show standard metrics (e.g., NCE, AUC, RMSE) improving\nsubstantially. The proposed confidence module also enables a model selection\napproach to combine an on-device E2E model with a hybrid model on the server to\naddress the rare word recognition problem for the E2E model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:03:33 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Qiu", "David", ""], ["Li", "Qiujia", ""], ["He", "Yanzhang", ""], ["Zhang", "Yu", ""], ["Li", "Bo", ""], ["Cao", "Liangliang", ""], ["Prabhavalkar", "Rohit", ""], ["Bhatia", "Deepti", ""], ["Li", "Wei", ""], ["Hu", "Ke", ""], ["Sainath", "Tara N.", ""], ["McGraw", "Ian", ""]]}, {"id": "2103.06752", "submitter": "Daniel Vollmers", "authors": "Daniel Vollmers (1), Rricha Jalota (1), Diego Moussallem (1), Hardik\n  Topiwala (1), Axel-Cyrille Ngonga Ngomo (1), and Ricardo Usbeck (2) ((1) Data\n  Science Group, Paderborn University, Germany, (2) Fraunhofer IAIS, Dresden,\n  Germany)", "title": "Knowledge Graph Question Answering using Graph-Pattern Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph Question Answering (KGQA) systems are based on machine\nlearning algorithms, requiring thousands of question-answer pairs as training\nexamples or natural language processing pipelines that need module fine-tuning.\nIn this paper, we present a novel QA approach, dubbed TeBaQA. Our approach\nlearns to answer questions based on graph isomorphisms from basic graph\npatterns of SPARQL queries. Learning basic graph patterns is efficient due to\nthe small number of possible patterns. This novel paradigm reduces the amount\nof training data necessary to achieve state-of-the-art performance. TeBaQA also\nspeeds up the domain adaption process by transforming the QA system development\ntask into a much smaller and easier data compilation task. In our evaluation,\nTeBaQA achieves state-of-the-art performance on QALD-8 and delivers comparable\nresults on QALD-9 and LC-QuAD v1. Additionally, we performed a fine-grained\nevaluation on complex queries that deal with aggregation and superlative\nquestions as well as an ablation study, highlighting future research\nchallenges.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:03:24 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Vollmers", "Daniel", ""], ["Jalota", "Rricha", ""], ["Moussallem", "Diego", ""], ["Topiwala", "Hardik", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Usbeck", "Ricardo", ""]]}, {"id": "2103.06758", "submitter": "Tuhin Chakrabarty Mr", "authors": "Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan", "title": "ENTRUST: Argument Reframing with Language Models and Entailment", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Framing involves the positive or negative presentation of an argument or\nissue depending on the audience and goal of the speaker (Entman 1983).\nDifferences in lexical framing, the focus of our work, can have large effects\non peoples' opinions and beliefs. To make progress towards reframing arguments\nfor positive effects, we create a dataset and method for this task. We use a\nlexical resource for \"connotations\" to create a parallel corpus and propose a\nmethod for argument reframing that combines controllable text generation\n(positive connotation) with a post-decoding entailment component (same\ndenotation). Our results show that our method is effective compared to strong\nbaselines along the dimensions of fluency, meaning, and\ntrustworthiness/reduction of fear.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:15:13 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:25:23 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 00:06:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chakrabarty", "Tuhin", ""], ["Hidey", "Christopher", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2103.06769", "submitter": "Pierre-Yves Oudeyer", "authors": "Manfred Eppe and Pierre-Yves Oudeyer", "title": "Intelligent behavior depends on the ecological niche: Scaling up AI to\n  human-like intelligence in socio-cultural environments", "comments": "Keywords: developmental AI, general artificial intelligence,\n  human-like AI, embodiment, cultural evolution, language, socio-cultural\n  skills", "journal-ref": "KI - K\\\"unstliche Intelligenz KI - K\\\"unstliche Intelligenz\n  (German Journal of Artificial Intelligence), 2021", "doi": "10.1007/s13218-020-00696-1", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a perspective on the future of AI, discussing directions\nfor machines models of human-like intelligence. We explain how developmental\nand evolutionary theories of human cognition should further inform artificial\nintelligence. We emphasize the role of ecological niches in sculpting\nintelligent behavior, and in particular that human intelligence was\nfundamentally shaped to adapt to a constantly changing socio-cultural\nenvironment. We argue that a major limit of current work in AI is that it is\nmissing this perspective, both theoretically and experimentally. Finally, we\ndiscuss the promising approach of developmental artificial intelligence,\nmodeling infant development through multi-scale interaction between\nintrinsically motivated learning, embodiment and a fastly changing\nsocio-cultural environment. This paper takes the form of an interview of\nPierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI -\nK{\\\"{u}}nstliche Intelligenz special issue in developmental robotics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:24:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Eppe", "Manfred", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2103.06779", "submitter": "Tuhin Chakrabarty Mr", "authors": "Tuhin Chakrabarty, Xurui Zhang, Smaranda Muresan, Nanyun Peng", "title": "MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating metaphors is a challenging task as it requires a proper\nunderstanding of abstract concepts, making connections between unrelated\nconcepts, and deviating from the literal meaning. In this paper, we aim to\ngenerate a metaphoric sentence given a literal expression by replacing relevant\nverbs. Based on a theoretically-grounded connection between metaphors and\nsymbols, we propose a method to automatically construct a parallel corpus by\ntransforming a large number of metaphorical sentences from the Gutenberg Poetry\ncorpus (Jacobs, 2018) to their literal counterpart using recent advances in\nmasked language modeling coupled with commonsense inference. For the generation\ntask, we incorporate a metaphor discriminator to guide the decoding of a\nsequence to sequence model fine-tuned on our parallel data to generate\nhigh-quality metaphors. Human evaluation on an independent test set of literal\nstatements shows that our best model generates metaphors better than three\nwell-crafted baselines 66% of the time on average. A task-based evaluation\nshows that human-written poems enhanced with metaphors proposed by our model\nare preferred 68% of the time compared to poems without metaphors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:39:19 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 00:05:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chakrabarty", "Tuhin", ""], ["Zhang", "Xurui", ""], ["Muresan", "Smaranda", ""], ["Peng", "Nanyun", ""]]}, {"id": "2103.06799", "submitter": "Xavier Garcia", "authors": "Xavier Garcia, Noah Constant, Ankur P. Parikh, Orhan Firat", "title": "Towards Continual Learning for Multilingual Machine Translation via\n  Vocabulary Substitution", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a straightforward vocabulary adaptation scheme to extend the\nlanguage capacity of multilingual machine translation models, paving the way\ntowards efficient continual learning for multilingual machine translation. Our\napproach is suitable for large-scale datasets, applies to distant languages\nwith unseen scripts, incurs only minor degradation on the translation\nperformance for the original language pairs and provides competitive\nperformance even in the case where we only possess monolingual data for the new\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:10:21 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Garcia", "Xavier", ""], ["Constant", "Noah", ""], ["Parikh", "Ankur P.", ""], ["Firat", "Orhan", ""]]}, {"id": "2103.06816", "submitter": "Arko Barman", "authors": "Hannah Lei (1), Weiqi Lu (1), Alan Ji (1), Emmett Bertram (1), Paul\n  Gao (1), Xiaoqian Jiang (2), Arko Barman (1) ((1) Rice University, Houston,\n  United States, (2) The University of Texas Health Science Center at Houston,\n  United States)", "title": "COVID-19 Smart Chatbot Prototype for Patient Monitoring", "comments": "This manuscript is under consideration for the AMIA 2021 Annual\n  Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many COVID-19 patients developed prolonged symptoms after the infection,\nincluding fatigue, delirium, and headache. The long-term health impact of these\nconditions is still not clear. It is necessary to develop a way to follow up\nwith these patients for monitoring their health status to support timely\nintervention and treatment. In the lack of sufficient human resources to follow\nup with patients, we propose a novel smart chatbot solution backed with machine\nlearning to collect information (i.e., generating digital diary) in a\npersonalized manner. In this article, we describe the design framework and\ncomponents of our prototype.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:37:55 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lei", "Hannah", ""], ["Lu", "Weiqi", ""], ["Ji", "Alan", ""], ["Bertram", "Emmett", ""], ["Gao", "Paul", ""], ["Jiang", "Xiaoqian", ""], ["Barman", "Arko", ""]]}, {"id": "2103.06874", "submitter": "Jonathan Clark", "authors": "Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting", "title": "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipelined NLP systems have largely been superseded by end-to-end neural\nmodeling, yet nearly all commonly-used models still require an explicit\ntokenization step. While recent tokenization approaches based on data-derived\nsubword lexicons are less brittle than manually engineered tokenizers, these\ntechniques are not equally suited to all languages, and the use of any fixed\nvocabulary may limit a model's ability to adapt. In this paper, we present\nCANINE, a neural encoder that operates directly on character sequences, without\nexplicit tokenization or vocabulary, and a pre-training strategy that operates\neither directly on characters or optionally uses subwords as a soft inductive\nbias. To use its finer-grained input effectively and efficiently, CANINE\ncombines downsampling, which reduces the input sequence length, with a deep\ntransformer stack, which encodes context. CANINE outperforms a comparable mBERT\nmodel by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite\nhaving 28% fewer model parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:57:44 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 17:58:09 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 17:55:23 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Clark", "Jonathan H.", ""], ["Garrette", "Dan", ""], ["Turc", "Iulia", ""], ["Wieting", "John", ""]]}, {"id": "2103.06884", "submitter": "Albina Khusainova", "authors": "Vitaly Romanov and Albina Khusainova", "title": "Evaluation of Morphological Embeddings for English and Russian Languages", "comments": "Published in Proceedings of the 3rd Workshop on Evaluating Vector\n  Space Representations for {NLP}. arXiv admin note: text overlap with\n  arXiv:2103.06628", "journal-ref": null, "doi": "10.18653/v1/W19-2010", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper evaluates morphology-based embeddings for English and Russian\nlanguages. Despite the interest and introduction of several morphology-based\nword embedding models in the past and acclaimed performance improvements on\nword similarity and language modeling tasks, in our experiments, we did not\nobserve any stable preference over two of our baseline models - SkipGram and\nFastText. The performance exhibited by morphological embeddings is the average\nof the two baselines mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:34:57 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Romanov", "Vitaly", ""], ["Khusainova", "Albina", ""]]}, {"id": "2103.06922", "submitter": "Mengnan Du", "authors": "Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck\n  Dernoncourt, Jiuxiang Gu, Tong Sun and Xia Hu", "title": "Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU\n  Models", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies indicate that NLU models are prone to rely on shortcut\nfeatures for prediction, without achieving true language understanding. As a\nresult, these models fail to generalize to real-world out-of-distribution data.\nIn this work, we show that the words in the NLU training set can be modeled as\na long-tailed distribution. There are two findings: 1) NLU models have strong\npreference for features located at the head of the long-tailed distribution,\nand 2) Shortcut features are picked up during very early few iterations of the\nmodel training. These two observations are further employed to formulate a\nmeasurement which can quantify the shortcut degree of each training sample.\nBased on this shortcut measurement, we propose a shortcut mitigation framework\nLTGR, to suppress the model from making overconfident predictions for samples\nwith large shortcut degree. Experimental results on three NLU benchmarks\ndemonstrate that our long-tailed distribution explanation accurately reflects\nthe shortcut learning behavior of NLU models. Experimental analysis further\nindicates that LTGR can improve the generalization accuracy on OOD data, while\npreserving the accuracy on in-distribution data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:39:56 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:11:32 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 22:38:11 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Du", "Mengnan", ""], ["Manjunatha", "Varun", ""], ["Jain", "Rajiv", ""], ["Deshpande", "Ruchi", ""], ["Dernoncourt", "Franck", ""], ["Gu", "Jiuxiang", ""], ["Sun", "Tong", ""], ["Hu", "Xia", ""]]}, {"id": "2103.06924", "submitter": "Ant\\'onio Branco", "authors": "Ant\\'onio Branco", "title": "Anaphoric Binding: an integrated overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The interpretation of anaphors depends on their antecedents as the semantic\nvalue that an anaphor eventually conveys is co-specified by the value of its\nantecedent. Interestingly, when occurring in a given syntactic position,\ndifferent anaphors may have different sets of admissible antecedents. Such\ndifferences are the basis for the categorization of anaphoric expressions\naccording to their anaphoric capacity, being important to determine what are\nthe sets of admissible antecedents and how to represent and process this\nanaphoric capacity for each type of anaphor.\n  From an empirical perspective, these constraints stem from what appears as\nquite cogent generalisations and exhibit a universal character, given their\ncross linguistic validity. From a conceptual point of view, in turn, the\nrelations among binding constraints involve non-trivial cross symmetry, which\nlends them a modular nature and provides further strength to the plausibility\nof their universal character. This kind of anaphoric binding constraints\nappears thus as a most significant subset of natural language knowledge,\nusually referred to as binding theory.\n  This paper provides an integrated overview of these constraints holding on\nthe pairing of nominal anaphors with their admissible antecedents that are\nbased on grammatical relations and structure. Along with the increasing\ninterest on neuro-symbolic approaches to natural language, this paper seeks to\ncontribute to revive the interest on this most intriguing research topic.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:48:36 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Branco", "Ant\u00f3nio", ""]]}, {"id": "2103.06942", "submitter": "Kartik Vempala", "authors": "Kartik Vempala (Bloomberg LP)", "title": "Imagined-Trailing-Whitespace-Agnostic Levenshtein Distance For Plaintext\n  Table Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard algorithm for Levenshtein distance, treats trailing whitespace\nthe same as any other letter or symbol. However, when humans compare 2 strings,\nwe implicitly assume that both strings are padded by infinite trailing\nwhitespace. This informs our expectations for what the costs for insertion,\ndeletion and replacement, should be. This violation of our expectations results\nin non-intuitive edit distance values. To account for this specific human\nintuition, a naive approach which considers \"all possible\" substrings of\ntrailing whitespace would yield an $O(n^3)$ algorithm. In this work, we provide\nan efficient $O(n^2)$ algorithm to compute the same. Keywords: Imagined\nInfinite Trailing Whitespace, Human Friendly, Intuitive Edit Distance, Table\nDetection, Table Alignment\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:39:40 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Vempala", "Kartik", "", "Bloomberg LP"]]}, {"id": "2103.06944", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg and Chris van der Lee and Emiel Krahmer", "title": "Preregistering NLP Research", "comments": "Accepted at NAACL2021; pre-final draft, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preregistration refers to the practice of specifying what you are going to\ndo, and what you expect to find in your study, before carrying out the study.\nThis practice is increasingly common in medicine and psychology, but is rarely\ndiscussed in NLP. This paper discusses preregistration in more detail, explores\nhow NLP researchers could preregister their work, and presents several\npreregistration questions for different kinds of studies. Finally, we argue in\nfavour of registered reports, which could provide firmer grounds for slow\nscience in NLP research. The goal of this paper is to elicit a discussion in\nthe NLP community, which we hope to synthesise into a general NLP\npreregistration form in future research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:44:31 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 22:02:14 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["van Miltenburg", "Emiel", ""], ["van der Lee", "Chris", ""], ["Krahmer", "Emiel", ""]]}, {"id": "2103.06960", "submitter": "Elise Jing", "authors": "Elise Jing, Yong-Yeol Ahn", "title": "Characterizing Partisan Political Narratives about COVID-19 on Twitter", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID-19 pandemic is a global crisis that has been testing every society\nand exposing the critical role of local politics in crisis response. In the\nUnited States, there has been a strong partisan divide which resulted in\npolarization of individual behaviors and divergent policy adoption across\nregions. Here, to better understand such divide, we characterize and compare\nthe pandemic narratives of the Democratic and Republican politicians on social\nmedia using novel computational methods including computational framing\nanalysis and semantic role analysis. By analyzing tweets from the politicians\nin the U.S., including the president, members of Congress, and state governors,\nwe systematically uncover the contrasting narratives in terms of topics,\nframes, and agents that shape their narratives. We found that the Democrats'\nnarrative tends to be more concerned with the pandemic as well as financial and\nsocial support, while the Republicans discuss more about other political\nentities such as China. By using contrasting framing and semantic roles, the\nDemocrats emphasize the government's role in responding to the pandemic, and\nthe Republicans emphasize the roles of individuals and support for small\nbusinesses. Both parties' narratives also include shout-outs to their followers\nand blaming of the other party. Our findings concretely expose the gaps in the\n\"elusive consensus\" between the two parties. Our methodologies may be applied\nto computationally study narratives in various domains.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:24:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Jing", "Elise", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "2103.06964", "submitter": "Gaurav Kumar", "authors": "Gaurav Kumar, Philipp Koehn, Sanjeev Khudanpur", "title": "Learning Policies for Multilingual Training of Neural Machine\n  Translation Systems", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resource Multilingual Neural Machine Translation (MNMT) is typically\ntasked with improving the translation performance on one or more language pairs\nwith the aid of high-resource language pairs. In this paper, we propose two\nsimple search based curricula -- orderings of the multilingual training data --\nwhich help improve translation performance in conjunction with existing\ntechniques such as fine-tuning. Additionally, we attempt to learn a curriculum\nfor MNMT from scratch jointly with the training of the translation system with\nthe aid of contextual multi-arm bandits. We show on the FLORES low-resource\ntranslation dataset that these learned curricula can provide better starting\npoints for fine tuning and improve overall performance of the translation\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:38:04 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kumar", "Gaurav", ""], ["Koehn", "Philipp", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2103.06968", "submitter": "Gaurav Kumar", "authors": "Gaurav Kumar, Philipp Koehn, Sanjeev Khudanpur", "title": "Learning Feature Weights using Reward Modeling for Denoising Parallel\n  Corpora", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large web-crawled corpora represent an excellent resource for improving the\nperformance of Neural Machine Translation (NMT) systems across several language\npairs. However, since these corpora are typically extremely noisy, their use is\nfairly limited. Current approaches to dealing with this problem mainly focus on\nfiltering using heuristics or single features such as language model scores or\nbi-lingual similarity. This work presents an alternative approach which learns\nweights for multiple sentence-level features. These feature weights which are\noptimized directly for the task of improving translation performance, are used\nto score and filter sentences in the noisy corpora more effectively. We provide\nresults of applying this technique to building NMT systems using the Paracrawl\ncorpus for Estonian-English and show that it beats strong single feature\nbaselines and hand designed combinations. Additionally, we analyze the\nsensitivity of this method to different types of noise and explore if the\nlearned weights generalize to other language pairs using the Maltese-English\nParacrawl corpus.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:45:45 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kumar", "Gaurav", ""], ["Koehn", "Philipp", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2103.07011", "submitter": "Liang Qiu", "authors": "Liang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan Shi, Zhou Yu,\n  Song-Chun Zhu", "title": "Towards Socially Intelligent Agents with Mental State Transition and\n  Human Utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a socially intelligent agent involves many challenges, one of which\nis to track the agent's mental state transition and teach the agent to make\nrational decisions guided by its utility like a human. Towards this end, we\npropose to incorporate a mental state parser and utility model into dialogue\nagents. The hybrid mental state parser extracts information from both the\ndialogue and event observations and maintains a graphical representation of the\nagent's mind; Meanwhile, the utility model is a ranking model that learns human\npreferences from a crowd-sourced social commonsense dataset, Social IQA.\nEmpirical results show that the proposed model attains state-of-the-art\nperformance on the dialogue/action/emotion prediction task in the fantasy\ntext-adventure game dataset, LIGHT. We also show example cases to demonstrate:\n(\\textit{i}) how the proposed mental state parser can assist agent's decision\nby grounding on the context like locations and objects, and (\\textit{ii}) how\nthe utility model can help the agent make reasonable decisions in a dilemma. To\nthe best of our knowledge, we are the first work that builds a socially\nintelligent agent by incorporating a hybrid mental state parser for both\ndiscrete events and continuous dialogues parsing and human-like utility\nmodeling.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:06:51 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Qiu", "Liang", ""], ["Zhao", "Yizhou", ""], ["Liang", "Yuan", ""], ["Lu", "Pan", ""], ["Shi", "Weiyan", ""], ["Yu", "Zhou", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.07040", "submitter": "Yusen Lin", "authors": "Yusen Lin, Jiayong Lin, Shuaicheng Zhang, Haoying Dai", "title": "Bilingual Dictionary-based Language Model Pretraining for Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have demonstrated a perceivable improvement on the performance\nof neural machine translation by applying cross-lingual language model\npretraining (Lample and Conneau, 2019), especially the Translation Language\nModeling (TLM). To alleviate the need for expensive parallel corpora by TLM, in\nthis work, we incorporate the translation information from dictionaries into\nthe pretraining process and propose a novel Bilingual Dictionary-based Language\nModel (BDLM). We evaluate our BDLM in Chinese, English, and Romanian. For\nChinese-English, we obtained a 55.0 BLEU on WMT-News19 (Tiedemann, 2012) and a\n24.3 BLEU on WMT20 news-commentary, outperforming the Vanilla Transformer\n(Vaswani et al., 2017) by more than 8.4 BLEU and 2.3 BLEU, respectively.\nAccording to our results, the BDLM also has advantages on convergence speed and\npredicting rare words. The increase in BLEU for WMT16 Romanian-English also\nshows its effectiveness in low-resources language translation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 02:01:22 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Lin", "Yusen", ""], ["Lin", "Jiayong", ""], ["Zhang", "Shuaicheng", ""], ["Dai", "Haoying", ""]]}, {"id": "2103.07052", "submitter": "Yifan Zhang", "authors": "Yifan Zhang, Dainis Boumber, Marjan Hosseinia, Fan Yang, Arjun\n  Mukherjee", "title": "Improving Authorship Verification using Linguistic Divergence", "comments": "Published in ROMCIR 2021. Workshop held as part of ECIR 2021. March\n  28 - April 1, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an unsupervised solution to the Authorship Verification task that\nutilizes pre-trained deep language models to compute a new metric called\nDV-Distance. The proposed metric is a measure of the difference between the two\nauthors comparing against pre-trained language models. Our design addresses the\nproblem of non-comparability in authorship verification, frequently encountered\nin small or cross-domain corpora. To the best of our knowledge, this paper is\nthe first one to introduce a method designed with non-comparability in mind\nfrom the ground up, rather than indirectly. It is also one of the first to use\nDeep Language Models in this setting. The approach is intuitive, and it is easy\nto understand and interpret through visualization. Experiments on four datasets\nshow our methods matching or surpassing current state-of-the-art and strong\nbaselines in most tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:01:17 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhang", "Yifan", ""], ["Boumber", "Dainis", ""], ["Hosseinia", "Marjan", ""], ["Yang", "Fan", ""], ["Mukherjee", "Arjun", ""]]}, {"id": "2103.07098", "submitter": "Sumeet Kumar", "authors": "Sumeet Kumar, Ramon Villa Cox, Matthew Babcock, Kathleen M. Carley", "title": "A Weakly Supervised Approach for Classifying Stance in Twitter Replies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversations on social media (SM) are increasingly being used to investigate\nsocial issues on the web, such as online harassment and rumor spread. For such\nissues, a common thread of research uses adversarial reactions, e.g., replies\npointing out factual inaccuracies in rumors. Though adversarial reactions are\nprevalent in online conversations, inferring those adverse views (or stance)\nfrom the text in replies is difficult and requires complex natural language\nprocessing (NLP) models. Moreover, conventional NLP models for stance mining\nneed labeled data for supervised learning. Getting labeled conversations can\nitself be challenging as conversations can be on any topic, and topics change\nover time. These challenges make learning the stance a difficult NLP problem.\n  In this research, we first create a new stance dataset comprised of three\ndifferent topics by labeling both users' opinions on the topics (as in pro/con)\nand users' stance while replying to others' posts (as in favor/oppose). As we\nfind limitations with supervised approaches, we propose a weakly-supervised\napproach to predict the stance in Twitter replies. Our novel method allows\nusing a smaller number of hashtags to generate weak labels for Twitter replies.\nCompared to supervised learning, our method improves the mean F1-macro by 8\\%\non the hand-labeled dataset without using any hand-labeled examples in the\ntraining set. We further show the applicability of our proposed method on COVID\n19 related conversations on Twitter.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 06:02:45 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kumar", "Sumeet", ""], ["Cox", "Ramon Villa", ""], ["Babcock", "Matthew", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "2103.07102", "submitter": "Hanwen Zha", "authors": "Hanwen Zha, Zhiyu Chen and Xifeng Yan", "title": "Inductive Relation Prediction by BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation prediction in knowledge graphs is dominated by embedding based\nmethods which mainly focus on the transductive setting. Unfortunately, they are\nnot able to handle inductive learning where unseen entities and relations are\npresent and cannot take advantage of prior knowledge. Furthermore, their\ninference process is not easily explainable. In this work, we propose an\nall-in-one solution, called BERTRL (BERT-based Relational Learning), which\nleverages pre-trained language model and fine-tunes it by taking relation\ninstances and their possible reasoning paths as training samples. BERTRL\noutperforms the SOTAs in 15 out of 18 cases in both inductive and transductive\nsettings. Meanwhile, it demonstrates strong generalization capability in\nfew-shot learning and is explainable.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 06:27:11 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zha", "Hanwen", ""], ["Chen", "Zhiyu", ""], ["Yan", "Xifeng", ""]]}, {"id": "2103.07162", "submitter": "Wei-Tsung Kao", "authors": "Wei-Tsung Kao, Hung-Yi Lee", "title": "Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of\n  Pre-trained Models' Transferability", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate whether the power of the models pre-trained on\ntext data, such as BERT, can be transferred to general token sequence\nclassification applications. To verify pre-trained models' transferability, we\ntest the pre-trained models on (1) text classification tasks with meanings of\ntokens mismatches, and (2) real-world non-text token sequence classification\ndata, including amino acid sequence, DNA sequence, and music. We find that even\non non-text data, the models pre-trained on text converge faster than the\nrandomly initialized models, and the testing performance of the pre-trained\nmodels is merely slightly worse than the models designed for the specific\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:19:14 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kao", "Wei-Tsung", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "2103.07170", "submitter": "Liwen Zhang", "authors": "Yixian Liu, Liwen Zhang, Wenjuan Han, Yue Zhang, Kewei Tu", "title": "Constrained Text Generation with Global Guidance -- Case Study on\n  CommonGen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies constrained text generation, which is to generate\nsentences under certain pre-conditions. We focus on CommonGen, the task of\ngenerating text based on a set of concepts, as a representative task of\nconstrained text generation. Traditional methods mainly rely on supervised\ntraining to maximize the likelihood of target sentences.However, global\nconstraints such as common sense and coverage cannot be incorporated into the\nlikelihood objective of the autoregressive decoding process. In this paper, we\nconsider using reinforcement learning to address the limitation, measuring\nglobal constraints including fluency, common sense and concept coverage with a\ncomprehensive score, which serves as the reward for reinforcement learning.\nBesides, we design a guided decoding method at the word, fragment and sentence\nlevels. Experiments demonstrate that our method significantly increases the\nconcept coverage and outperforms existing models in various automatic\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:40:49 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Liu", "Yixian", ""], ["Zhang", "Liwen", ""], ["Han", "Wenjuan", ""], ["Zhang", "Yue", ""], ["Tu", "Kewei", ""]]}, {"id": "2103.07186", "submitter": "Aleksandr Laptev", "authors": "Aleksandr Laptev, Andrei Andrusenko, Ivan Podluzhny, Anton Mitrofanov,\n  Ivan Medennikov, Yuri Matveev", "title": "Dynamic Acoustic Unit Augmentation With BPE-Dropout for Low-Resource\n  End-to-End Speech Recognition", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": "10.3390/s21093063", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of speech assistants, adapting server-intended\nautomatic speech recognition (ASR) solutions to a direct device has become\ncrucial. Researchers and industry prefer to use end-to-end ASR systems for\non-device speech recognition tasks. This is because end-to-end systems can be\nmade resource-efficient while maintaining a higher quality compared to hybrid\nsystems. However, building end-to-end models requires a significant amount of\nspeech data. Another challenging task associated with speech assistants is\npersonalization, which mainly lies in handling out-of-vocabulary (OOV) words.\nIn this work, we consider building an effective end-to-end ASR system in\nlow-resource setups with a high OOV rate, embodied in Babel Turkish and Babel\nGeorgian tasks. To address the aforementioned problems, we propose a method of\ndynamic acoustic unit augmentation based on the BPE-dropout technique. It\nnon-deterministically tokenizes utterances to extend the token's contexts and\nto regularize their distribution for the model's recognition of unseen words.\nIt also reduces the need for optimal subword vocabulary size search. The\ntechnique provides a steady improvement in regular and personalized\n(OOV-oriented) speech recognition tasks (at least 6% relative WER and 25%\nrelative F-score) at no additional computational cost. Owing to the use of\nBPE-dropout, our monolingual Turkish Conformer established a competitive result\nwith 22.2% character error rate (CER) and 38.9% word error rate (WER), which is\nclose to the best published multilingual system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:10:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Laptev", "Aleksandr", ""], ["Andrusenko", "Andrei", ""], ["Podluzhny", "Ivan", ""], ["Mitrofanov", "Anton", ""], ["Medennikov", "Ivan", ""], ["Matveev", "Yuri", ""]]}, {"id": "2103.07191", "submitter": "Arkil Patel", "authors": "Arkil Patel, Satwik Bhattamishra, Navin Goyal", "title": "Are NLP Models really able to Solve Simple Math Word Problems?", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of designing NLP solvers for math word problems (MWP) has seen\nsustained research activity and steady gains in the test accuracy. Since\nexisting solvers achieve high performance on the benchmark datasets for\nelementary level MWPs containing one-unknown arithmetic word problems, such\nproblems are often considered \"solved\" with the bulk of research attention\nmoving to more complex MWPs. In this paper, we restrict our attention to\nEnglish MWPs taught in grades four and lower. We provide strong evidence that\nthe existing MWP solvers rely on shallow heuristics to achieve high performance\non the benchmark datasets. To this end, we show that MWP solvers that do not\nhave access to the question asked in the MWP can still solve a large fraction\nof MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve\nsurprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP,\ncreated by applying carefully chosen variations over examples sampled from\nexisting datasets. The best accuracy achieved by state-of-the-art models is\nsubstantially lower on SVAMP, thus showing that much remains to be done even\nfor the simplest of the MWPs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:23:47 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 06:11:12 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Patel", "Arkil", ""], ["Bhattamishra", "Satwik", ""], ["Goyal", "Navin", ""]]}, {"id": "2103.07199", "submitter": "Fadhl Eryani", "authors": "Eryani Fadhl and Habash Nizar", "title": "Automatic Romanization of Arabic Bibliographic Records", "comments": "WANLP 2021 Camera-ready, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  International library standards require cataloguers to tediously input\nRomanization of their catalogue records for the benefit of library users\nwithout specific language expertise. In this paper, we present the first\nreported results on the task of automatic Romanization of undiacritized Arabic\nbibliographic entries. This complex task requires the modeling of Arabic\nphonology, morphology, and even semantics. We collected a 2.5M word corpus of\nparallel Arabic and Romanized bibliographic entries, and benchmarked a number\nof models that vary in terms of complexity and resource dependence. Our best\nsystem reaches 89.3% exact word Romanization on a blind test set. We make our\ndata and code publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:46:32 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Fadhl", "Eryani", ""], ["Nizar", "Habash", ""]]}, {"id": "2103.07259", "submitter": "Severin Laicher", "authors": "Severin Laicher, Sinan Kurtyigit, Dominik Schlechtweg, Jonas Kuhn,\n  Sabine Schulte im Walde", "title": "Explaining and Improving BERT Performance on Lexical Semantic Change\n  Detection", "comments": "EACL SRW, 6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Type- and token-based embedding architectures are still competing in lexical\nsemantic change detection. The recent success of type-based models in\nSemEval-2020 Task 1 has raised the question why the success of token-based\nmodels on a variety of other NLP tasks does not translate to our field. We\ninvestigate the influence of a range of variables on clusterings of BERT\nvectors and show that its low performance is largely due to orthographic\ninformation on the target word, which is encoded even in the higher layers of\nBERT representations. By reducing the influence of orthography we considerably\nimprove BERT's performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:29:30 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Laicher", "Severin", ""], ["Kurtyigit", "Sinan", ""], ["Schlechtweg", "Dominik", ""], ["Kuhn", "Jonas", ""], ["Walde", "Sabine Schulte im", ""]]}, {"id": "2103.07277", "submitter": "Joseph Marvin Imperial", "authors": "Joseph Marvin Imperial, Ethel Ong", "title": "A Simple Post-Processing Technique for Improving Readability Assessment\n  of Texts using Word Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing the proper difficulty levels of reading materials or texts in\ngeneral is the first step towards effective comprehension and learning. In this\nstudy, we improve the conventional methodology of automatic readability\nassessment by incorporating the Word Mover's Distance (WMD) of ranked texts as\nan additional post-processing technique to further ground the difficulty level\ngiven by a model. Results of our experiments on three multilingual datasets in\nFilipino, German, and English show that the post-processing technique\noutperforms previous vanilla and ranking-based models using SVM.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:51:38 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Imperial", "Joseph Marvin", ""], ["Ong", "Ethel", ""]]}, {"id": "2103.07352", "submitter": "Zhenhao Li", "authors": "Zhenhao Li, Marek Rei, Lucia Specia", "title": "Improving Translation Robustness with Visual Cues and Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation models are brittle to input noise. Current\nrobustness techniques mostly adapt models to existing noisy texts, but these\nmodels generally fail when faced with unseen noise and their performance\ndegrades on clean texts. In this paper, we introduce the idea of visual context\nto improve translation robustness against noisy texts. In addition, we propose\na novel error correction training regime by treating error correction as an\nauxiliary task to further improve robustness. Experiments on English-French and\nEnglish-German translation show that both multimodality and error correction\ntraining are beneficial for model robustness to known and new types of errors,\nwhile keeping the quality on clean texts.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:31:34 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Li", "Zhenhao", ""], ["Rei", "Marek", ""], ["Specia", "Lucia", ""]]}, {"id": "2103.07449", "submitter": "Hongyin Luo", "authors": "Hongyin Luo, Shang-Wen Li, Seunghak Yu, James Glass", "title": "Cooperative Learning of Zero-Shot Machine Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained language models have significantly improved the performance of\ndown-stream language understanding tasks, including extractive question\nanswering, by providing high-quality contextualized word embeddings. However,\nlearning question answering models still need large-scaled data annotation in\nspecific domains. In this work, we propose a cooperative, self-play learning\nframework, REGEX, for question generation and answering. REGEX is built upon a\nmasked answer extraction task with an interactive learning environment\ncontaining an answer entity REcognizer, a question Generator, and an answer\nEXtractor. Given a passage with a masked entity, the generator generates a\nquestion around the entity, and the extractor is trained to extract the masked\nentity with the generated question and raw texts. The framework allows the\ntraining of question generation and answering models on any text corpora\nwithout annotation. We further leverage a reinforcement learning technique to\nreward generating high-quality questions and to improve the answer extraction\nmodel's performance. Experiment results show that REGEX outperforms the\nstate-of-the-art (SOTA) pretrained language models and zero-shot approaches on\nstandard question-answering benchmarks, and yields the new SOTA performance\nunder the zero-shot setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:22:28 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 07:05:01 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luo", "Hongyin", ""], ["Li", "Shang-Wen", ""], ["Yu", "Seunghak", ""], ["Glass", "James", ""]]}, {"id": "2103.07538", "submitter": "Sandeep Soni", "authors": "Sandeep Soni and Lauren Klein and Jacob Eisenstein", "title": "Abolitionist Networks: Modeling Language Change in Nineteenth-Century\n  Activist Newspapers", "comments": "23 pages, 6 figures, 2 tables", "journal-ref": "Journal of Cultural Analytics (2021)", "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The abolitionist movement of the nineteenth-century United States remains\namong the most significant social and political movements in US history.\nAbolitionist newspapers played a crucial role in spreading information and\nshaping public opinion around a range of issues relating to the abolition of\nslavery. These newspapers also serve as a primary source of information about\nthe movement for scholars today, resulting in powerful new accounts of the\nmovement and its leaders. This paper supplements recent qualitative work on the\nrole of women in abolition's vanguard, as well as the role of the Black press,\nwith a quantitative text modeling approach. Using diachronic word embeddings,\nwe identify which newspapers tended to lead lexical semantic innovations -- the\nintroduction of new usages of specific words -- and which newspapers tended to\nfollow. We then aggregate the evidence across hundreds of changes into a\nweighted network with the newspapers as nodes; directed edge weights represent\nthe frequency with which each newspaper led the other in the adoption of a\nlexical semantic change. Analysis of this network reveals pathways of lexical\nsemantic influence, distinguishing leaders from followers, as well as others\nwho stood apart from the semantic changes that swept through this period. More\nspecifically, we find that two newspapers edited by women -- THE PROVINCIAL\nFREEMAN and THE LILY -- led a large number of semantic changes in our corpus,\nlending additional credence to the argument that a multiracial coalition of\nwomen led the abolitionist movement in terms of both thought and action. It\nalso contributes additional complexity to the scholarship that has sought to\ntease apart the relation of the abolitionist movement to the women's suffrage\nmovement, and the vexed racial politics that characterized their relation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 21:26:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Soni", "Sandeep", ""], ["Klein", "Lauren", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "2103.07552", "submitter": "Jason Wei", "authors": "Jason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, Shiqi Xu", "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation,\n  and Curriculum Learning", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": "10.18653/v1/2021.naacl-main.434", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot text classification is a fundamental NLP task in which a model aims\nto classify text into a large number of categories, given only a few training\nexamples per category. This paper explores data augmentation -- a technique\nparticularly suitable for training with limited data -- for this few-shot,\nhighly-multiclass text classification setting. On four diverse text\nclassification tasks, we find that common data augmentation techniques can\nimprove the performance of triplet networks by up to 3.0% on average.\n  To further boost performance, we present a simple training strategy called\ncurriculum data augmentation, which leverages curriculum learning by first\ntraining on only original examples and then introducing augmented data as\ntraining progresses. We explore a two-stage and a gradual schedule, and find\nthat, compared with standard single-stage training, curriculum data\naugmentation trains faster, improves performance, and remains robust to high\namounts of noising from augmentation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:07:35 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wei", "Jason", ""], ["Huang", "Chengyu", ""], ["Vosoughi", "Soroush", ""], ["Cheng", "Yu", ""], ["Xu", "Shiqi", ""]]}, {"id": "2103.07554", "submitter": "Florian Kreyssig", "authors": "Adnan Haider and Chao Zhang and Florian L. Kreyssig and Philip C.\n  Woodland", "title": "A Distributed Optimisation Framework Combining Natural Gradient with\n  Hessian-Free for Discriminative Sequence Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel natural gradient and Hessian-free (NGHF)\noptimisation framework for neural network training that can operate efficiently\nin a distributed manner. It relies on the linear conjugate gradient (CG)\nalgorithm to combine the natural gradient (NG) method with local curvature\ninformation from Hessian-free (HF) or other second-order methods. A solution to\na numerical issue in CG allows effective parameter updates to be generated with\nfar fewer CG iterations than usually used (e.g. 5-8 instead of 200). This work\nalso presents a novel preconditioning approach to improve the progress made by\nindividual CG iterations for models with shared parameters. Although applicable\nto other training losses and model structures, NGHF is investigated in this\npaper for lattice-based discriminative sequence training for hybrid hidden\nMarkov model acoustic models using a standard recurrent neural network, long\nshort-term memory, and time delay neural network models for output probability\ncalculation. Automatic speech recognition experiments are reported on the\nmulti-genre broadcast data set for a range of different acoustic model types.\nThese experiments show that NGHF achieves larger word error rate reductions\nthan standard stochastic gradient descent or Adam, while requiring orders of\nmagnitude fewer parameter updates.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:18:34 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Haider", "Adnan", ""], ["Zhang", "Chao", ""], ["Kreyssig", "Florian L.", ""], ["Woodland", "Philip C.", ""]]}, {"id": "2103.07567", "submitter": "Fatemehsadat Mireshghallah", "authors": "Fatemehsadat Mireshghallah, Huseyin A. Inan, Marcello Hasegawa, Victor\n  R\\\"uhle, Taylor Berg-Kirkpatrick, Robert Sim", "title": "Privacy Regularization: Joint Privacy-Utility Optimization in Language\n  Models", "comments": "NAACL-HLT 2021 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural language models are known to have a high capacity for memorization of\ntraining samples. This may have serious privacy implications when training\nmodels on user content such as email correspondence. Differential privacy (DP),\na popular choice to train models with privacy guarantees, comes with\nsignificant costs in terms of utility degradation and disparate impact on\nsubgroups of users. In this work, we introduce two privacy-preserving\nregularization methods for training language models that enable joint\noptimization of utility and privacy through (1) the use of a discriminator and\n(2) the inclusion of a triplet-loss term. We compare our methods with DP\nthrough extensive evaluation. We show the advantages of our regularizers with\nfavorable utility-privacy trade-off, faster training with the ability to tap\ninto existing optimization approaches, and ensuring uniform treatment of\nunder-represented subgroups.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 23:17:43 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 01:01:59 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mireshghallah", "Fatemehsadat", ""], ["Inan", "Huseyin A.", ""], ["Hasegawa", "Marcello", ""], ["R\u00fchle", "Victor", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Sim", "Robert", ""]]}, {"id": "2103.07575", "submitter": "Yusen Lin", "authors": "Yusen Lin", "title": "A Review on Semi-Supervised Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relation extraction (RE) plays an important role in extracting knowledge from\nunstructured text but requires a large amount of labeled corpus. To reduce the\nexpensive annotation efforts, semisupervised learning aims to leverage both\nlabeled and unlabeled data. In this paper, we review and compare three typical\nmethods in semi-supervised RE with deep learning or meta-learning:\nself-ensembling, which forces consistent under perturbations but may confront\ninsufficient supervision; self-training, which iteratively generates pseudo\nlabels and retrain itself with the enlarged labeled set; dual learning, which\nleverages a primal task and a dual task to give mutual feedback. Mean-teacher\n(Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al.,\n2019) are elaborated as the representatives to alleviate the weakness of these\nthree methods, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 23:43:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lin", "Yusen", ""]]}, {"id": "2103.07601", "submitter": "Ruiqi Zhong", "authors": "Charlie Snell, Ruiqi Zhong, Dan Klein, Jacob Steinhardt", "title": "Approximating How Single Head Attention Learns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Why do models often attend to salient words, and how does this evolve\nthroughout training? We approximate model training as a two stage process:\nearly on in training when the attention weights are uniform, the model learns\nto translate individual input word `i` to `o` if they co-occur frequently.\nLater, the model learns to attend to `i` while the correct output is $o$\nbecause it knows `i` translates to `o`. To formalize, we define a model\nproperty, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`\ntranslates to `o`), and claim that it drives the learning of the attention.\nThis claim is supported by the fact that before the attention mechanism is\nlearned, KTIW can be learned from word co-occurrence statistics, but not the\nother way around. Particularly, we can construct a training distribution that\nmakes KTIW hard to learn, the learning of the attention fails, and the model\ncannot even learn the simple task of copying the input words to the output. Our\napproximation explains why models sometimes attend to salient words, and\ninspires a toy example where a multi-head attention model can overcome the\nabove hard training distribution by improving learning dynamics rather than\nexpressiveness.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:32:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Snell", "Charlie", ""], ["Zhong", "Ruiqi", ""], ["Klein", "Dan", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2103.07649", "submitter": "Xinran Zhang", "authors": "Xinran Zhang, Maosong Sun, Jiafeng Liu and Xiaobing Li", "title": "Improving Diversity of Neural Text Generation via Inverse Probability\n  Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neural text generation suffers from the text degeneration issue such as\nrepetition. Traditional methods only focus on truncating the unreliable \"tail\"\nof the distribution, and do not address the \"head\" part, which we show might\ncontain tedious or even repetitive candidates with high probability that lead\nto repetition loops. They also do not address the issue that human text does\nnot always favor high probability words. Inspired by these, in this work we\npropose a heuristic sampling method. We propose to use interquartile range of\nthe predicted distribution to determine the \"head\" part, then permutate and\nrescale the \"head\" with inverse probability. This aims at decreasing the\nprobability for the tedious and possibly repetitive candidates with higher\nprobability, and increasing the probability for the rational but more\nsurprising candidates with lower probability. The proposed algorithm provides a\ncontrollable variation on the predicted distribution which enhances diversity\nwithout compromising rationality of the distribution. We use pre-trained\nlanguage model to compare our algorithm with nucleus sampling. Results show\nthat our algorithm can effectively increase the diversity of generated samples\nwhile achieving close resemblance to human text.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 08:17:40 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 03:26:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Xinran", ""], ["Sun", "Maosong", ""], ["Liu", "Jiafeng", ""], ["Li", "Xiaobing", ""]]}, {"id": "2103.07656", "submitter": "Xinran Zhang", "authors": "Xinran Zhang, Maosong Sun, Jiafeng Liu and Xiaobing Li", "title": "Embedding Calibration for Music Semantic Similarity using\n  Auto-regressive Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the advantages of using natural language processing (NLP) technology\nfor music is to fully exploit the embedding based representation learning\nparadigm that can easily handle classical tasks such as semantic similarity.\nHowever, recent researches have revealed the poor performance issue of common\nbaseline methods for semantic similarity in NLP. They show that some simple\nembedding calibration methods can easily promote the performance of semantic\nsimilarity without extra training hence is ready-to-use. Nevertheless, it is\nstill unclear which is the best combination of calibration methods and by how\nmuch can we further improve the performance with such methods. Most\nimportantly, previous works are based on auto-encoder Transformer, hence the\nperformance under auto-regressive model for music is unclear. These render the\nfollowing open questions: does embedding based semantic similarity also apply\nfor auto-regressive music model, does poor baseline issue for semantic\nsimilarity also exists, and if so, are there unexplored embedding calibration\nmethods to better promote the performance of music semantic similarity? In this\npaper, we answer these questions by exploring different combination of\nembedding calibration under auto-regressive language model for symbolic music.\nOur results show that music semantic similarity works under auto-regressive\nmodel, and also suffers from poor baseline issues like in NLP. Furthermore, we\nprovide optimal combination of embedding calibration that has not been explored\nin previous researches. Results show that such combination of embedding\ncalibration can greatly improve music semantic similarity without further\ntraining tasks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 08:36:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhang", "Xinran", ""], ["Sun", "Maosong", ""], ["Liu", "Jiafeng", ""], ["Li", "Xiaobing", ""]]}, {"id": "2103.07659", "submitter": "Donghong Gu", "authors": "Jiaqian Wang, Donghong Gu, Chi Yang, Yun Xue, Zhengxin Song, Haoliang\n  Zhao, Luwei Xiao", "title": "Targeted aspect based multimodal sentiment analysis:an attention capsule\n  extraction and multi-head fusion network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multimodal sentiment analysis has currently identified its significance in a\nvariety of domains. For the purpose of sentiment analysis, different aspects of\ndistinguishing modalities, which correspond to one target, are processed and\nanalyzed. In this work, we propose the targeted aspect-based multimodal\nsentiment analysis (TABMSA) for the first time. Furthermore, an attention\ncapsule extraction and multi-head fusion network (EF-Net) on the task of TABMSA\nis devised. The multi-head attention (MHA) based network and the ResNet-152 are\nemployed to deal with texts and images, respectively. The integration of MHA\nand capsule network aims to capture the interaction among the multimodal\ninputs. In addition to the targeted aspect, the information from the context\nand the image is also incorporated for sentiment delivered. We evaluate the\nproposed model on two manually annotated datasets. the experimental results\ndemonstrate the effectiveness of our proposed model for this new task.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 09:11:24 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Jiaqian", ""], ["Gu", "Donghong", ""], ["Yang", "Chi", ""], ["Xue", "Yun", ""], ["Song", "Zhengxin", ""], ["Zhao", "Haoliang", ""], ["Xiao", "Luwei", ""]]}, {"id": "2103.07665", "submitter": "Shaowei Chen", "authors": "Shaowei Chen, Yu Wang, Jie Liu, Yuelin Wang", "title": "Bidirectional Machine Reading Comprehension for Aspect Sentiment Triplet\n  Extraction", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect sentiment triplet extraction (ASTE), which aims to identify aspects\nfrom review sentences along with their corresponding opinion expressions and\nsentiments, is an emerging task in fine-grained opinion mining. Since ASTE\nconsists of multiple subtasks, including opinion entity extraction, relation\ndetection, and sentiment classification, it is critical and challenging to\nappropriately capture and utilize the associations among them. In this paper,\nwe transform ASTE task into a multi-turn machine reading comprehension (MTMRC)\ntask and propose a bidirectional MRC (BMRC) framework to address this\nchallenge. Specifically, we devise three types of queries, including\nnon-restrictive extraction queries, restrictive extraction queries and\nsentiment classification queries, to build the associations among different\nsubtasks. Furthermore, considering that an aspect sentiment triplet can derive\nfrom either an aspect or an opinion expression, we design a bidirectional MRC\nstructure. One direction sequentially recognizes aspects, opinion expressions,\nand sentiments to obtain triplets, while the other direction identifies opinion\nexpressions first, then aspects, and at last sentiments. By making the two\ndirections complement each other, our framework can identify triplets more\ncomprehensively. To verify the effectiveness of our approach, we conduct\nextensive experiments on four benchmark datasets. The experimental results\ndemonstrate that BMRC achieves state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 09:30:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Shaowei", ""], ["Wang", "Yu", ""], ["Liu", "Jie", ""], ["Wang", "Yuelin", ""]]}, {"id": "2103.07679", "submitter": "Yun-Hsuan Liu", "authors": "Ke-Jyun Wang, Yun-Hsuan Liu, Hung-Ting Su, Jen-Wei Wang, Yu-Siang\n  Wang, Winston H. Hsu, Wen-Chin Chen", "title": "OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene\n  Grounding", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To effectively apply robots in working environments and assist humans, it is\nessential to develop and evaluate how visual grounding (VG) can affect machine\nperformance on occluded objects. However, current VG works are limited in\nworking environments, such as offices and warehouses, where objects are usually\noccluded due to space utilization issues. In our work, we propose a novel\nOCID-Ref dataset featuring a referring expression segmentation task with\nreferring expressions of occluded objects. OCID-Ref consists of 305,694\nreferring expressions from 2,300 scenes with providing RGB image and point\ncloud inputs. To resolve challenging occlusion issues, we argue that it's\ncrucial to take advantage of both 2D and 3D signals to resolve challenging\nocclusion issues. Our experimental results demonstrate the effectiveness of\naggregating 2D and 3D signals but referring to occluded objects still remains\nchallenging for the modern visual grounding systems. OCID-Ref is publicly\navailable at https://github.com/lluma/OCID-Ref\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:38:15 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 09:03:46 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Ke-Jyun", ""], ["Liu", "Yun-Hsuan", ""], ["Su", "Hung-Ting", ""], ["Wang", "Jen-Wei", ""], ["Wang", "Yu-Siang", ""], ["Hsu", "Winston H.", ""], ["Chen", "Wen-Chin", ""]]}, {"id": "2103.07762", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou and Chris C. Emezue", "title": "OkwuGb\\'e: End-to-End Speech Recognition for Fon and Igbo", "comments": null, "journal-ref": "African NLP, EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language is inherent and compulsory for human communication. Whether\nexpressed in a written or spoken way, it ensures understanding between people\nof the same and different regions. With the growing awareness and effort to\ninclude more low-resourced languages in NLP research, African languages have\nrecently been a major subject of research in machine translation, and other\ntext-based areas of NLP. However, there is still very little comparable\nresearch in speech recognition for African languages. Interestingly, some of\nthe unique properties of African languages affecting NLP, like their\ndiacritical and tonal complexities, have a major root in their speech,\nsuggesting that careful speech interpretation could provide more intuition on\nhow to deal with the linguistic complexities of African languages for\ntext-based NLP. OkwuGb\\'e is a step towards building speech recognition systems\nfor African low-resourced languages. Using Fon and Igbo as our case study, we\nconduct a comprehensive linguistic analysis of each language and describe the\ncreation of end-to-end, deep neural network-based speech recognition models for\nboth languages. We present a state-of-art ASR model for Fon, as well as\nbenchmark ASR model results for Igbo. Our linguistic analyses (for Fon and\nIgbo) provide valuable insights and guidance into the creation of speech\nrecognition models for other African low-resourced languages, as well as guide\nfuture NLP research for Fon and Igbo. The Fon and Igbo models source code have\nbeen made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:02:44 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 04:35:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Emezue", "Chris C.", ""]]}, {"id": "2103.07766", "submitter": "Endri Kacupaj", "authors": "Joan Plepi, Endri Kacupaj, Kuldeep Singh, Harsh Thakkar, Jens Lehmann", "title": "Context Transformer with Stacked Pointer Networks for Conversational\n  Question Answering over Knowledge Graphs", "comments": "18th Extended Semantic Web Conference 2021 (ESWC'2021) - Research\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural semantic parsing approaches have been widely used for Question\nAnswering (QA) systems over knowledge graphs. Such methods provide the\nflexibility to handle QA datasets with complex queries and a large number of\nentities. In this work, we propose a novel framework named CARTON, which\nperforms multi-task semantic parsing for handling the problem of conversational\nquestion answering over a large-scale knowledge graph. Our framework consists\nof a stack of pointer networks as an extension of a context transformer model\nfor parsing the input question and the dialog history. The framework generates\na sequence of actions that can be executed on the knowledge graph. We evaluate\nCARTON on a standard dataset for complex sequential question answering on which\nCARTON outperforms all baselines. Specifically, we observe performance\nimprovements in F1-score on eight out of ten question types compared to the\nprevious state of the art. For logical reasoning questions, an improvement of\n11 absolute points is reached.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:16:43 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 11:30:50 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Plepi", "Joan", ""], ["Kacupaj", "Endri", ""], ["Singh", "Kuldeep", ""], ["Thakkar", "Harsh", ""], ["Lehmann", "Jens", ""]]}, {"id": "2103.07769", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer\n  Elsayed, Alberto Barr\\'on-Cede\\~no, Paolo Papotti, Shaden Shaar, Giovanni Da\n  San Martino", "title": "Automated Fact-Checking for Assisting Human Fact-Checkers", "comments": "fact-checking, fact-checkers, check-worthiness, detecting previously\n  fact-checked claims, evidence retrieval", "journal-ref": "IJCAI-2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:29:14 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 12:27:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nakov", "Preslav", ""], ["Corney", "David", ""], ["Hasanain", "Maram", ""], ["Alam", "Firoj", ""], ["Elsayed", "Tamer", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Papotti", "Paolo", ""], ["Shaar", "Shaden", ""], ["Martino", "Giovanni Da San", ""]]}, {"id": "2103.07771", "submitter": "Endri Kacupaj", "authors": "Endri Kacupaj, Barshana Banerjee, Kuldeep Singh, Jens Lehmann", "title": "ParaQA: A Question Answering Dataset with Paraphrase Responses for\n  Single-Turn Conversation", "comments": "18th Extended Semantic Web Conference 2021 (ESWC'2021) - Resources\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents ParaQA, a question answering (QA) dataset with multiple\nparaphrased responses for single-turn conversation over knowledge graphs (KG).\nThe dataset was created using a semi-automated framework for generating diverse\nparaphrasing of the answers using techniques such as back-translation. The\nexisting datasets for conversational question answering over KGs\n(single-turn/multi-turn) focus on question paraphrasing and provide only up to\none answer verbalization. However, ParaQA contains 5000 question-answer pairs\nwith a minimum of two and a maximum of eight unique paraphrased responses for\neach question. We complement the dataset with baseline models and illustrate\nthe advantage of having multiple paraphrased answers through commonly used\nmetrics such as BLEU and METEOR. The ParaQA dataset is publicly available on a\npersistent URI for broader usage and adaptation in the research community.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:53:07 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kacupaj", "Endri", ""], ["Banerjee", "Barshana", ""], ["Singh", "Kuldeep", ""], ["Lehmann", "Jens", ""]]}, {"id": "2103.07785", "submitter": "Matt Grenander", "authors": "Matt Grenander, Robert Belfer, Ekaterina Kochmar, Iulian V. Serban,\n  Fran\\c{c}ois St-Hilaire, Jackie C. K. Cheung", "title": "Deep Discourse Analysis for Generating Personalized Feedback in\n  Intelligent Tutor Systems", "comments": "Accepted at EAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore creating automated, personalized feedback in an intelligent\ntutoring system (ITS). Our goal is to pinpoint correct and incorrect concepts\nin student answers in order to achieve better student learning gains. Although\nautomatic methods for providing personalized feedback exist, they do not\nexplicitly inform students about which concepts in their answers are correct or\nincorrect. Our approach involves decomposing students answers using neural\ndiscourse segmentation and classification techniques. This decomposition yields\na relational graph over all discourse units covered by the reference solutions\nand student answers. We use this inferred relational graph structure and a\nneural classifier to match student answers with reference solutions and\ngenerate personalized feedback. Although the process is completely automated\nand data-driven, the personalized feedback generated is highly contextual,\ndomain-aware and effectively targets each student's misconceptions and\nknowledge gaps. We test our method in a dialogue-based ITS and demonstrate that\nour approach results in high-quality feedback and significantly improved\nstudent learning gains.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 20:33:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Grenander", "Matt", ""], ["Belfer", "Robert", ""], ["Kochmar", "Ekaterina", ""], ["Serban", "Iulian V.", ""], ["St-Hilaire", "Fran\u00e7ois", ""], ["Cheung", "Jackie C. K.", ""]]}, {"id": "2103.07792", "submitter": "Jitin Krishnan", "authors": "Jitin Krishnan, Antonios Anastasopoulos, Hemant Purohit, and Huzefa\n  Rangwala", "title": "Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent\n  Prediction and Slot Filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting user intent and detecting the corresponding slots from text are\ntwo key problems in Natural Language Understanding (NLU). In the context of\nzero-shot learning, this task is typically approached by either using\nrepresentations from pre-trained multilingual transformers such as mBERT, or by\nmachine translating the source data into the known target language and then\nfine-tuning. Our work focuses on a particular scenario where the target\nlanguage is unknown during training. To this goal, we propose a novel method to\naugment the monolingual source data using multilingual code-switching via\nrandom translations to enhance a transformer's language neutrality when\nfine-tuning it for a downstream task. This method also helps discover novel\ninsights on how code-switching with different language families around the\nworld impact the performance on the target language. Experiments on the\nbenchmark dataset of MultiATIS++ yielded an average improvement of +4.2% in\naccuracy for intent task and +1.8% in F1 for slot task using our method over\nthe state-of-the-art across 8 different languages. Furthermore, we present an\napplication of our method for crisis informatics using a new human-annotated\ntweet dataset of slot filling in English and Haitian Creole, collected during\nHaiti earthquake disaster.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 21:05:09 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 16:39:48 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Krishnan", "Jitin", ""], ["Anastasopoulos", "Antonios", ""], ["Purohit", "Hemant", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "2103.07808", "submitter": "Youngwoo Kim", "authors": "Youngwoo Kim, Cheng Li, Bingyang Ye, Amir Tahmasebi and Javed Aslam", "title": "Supervised Learning in the Presence of Noise: Application in ICD-10 Code\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  ICD coding is the international standard for capturing and reporting health\nconditions and diagnosis for revenue cycle management in healthcare. Manually\nassigning ICD codes is prone to human error due to the large code vocabulary\nand the similarities between codes. Since machine learning based approaches\nrequire ground truth training data, the inconsistency among human coders is\nmanifested as noise in labeling, which makes the training and evaluation of ICD\nclassifiers difficult in presence of such noise. This paper investigates the\ncharacteristics of such noise in manually-assigned ICD-10 codes and\nfurthermore, proposes a method to train robust ICD-10 classifiers in the\npresence of labeling noise. Our research concluded that the nature of such\nnoise is systematic. Most of the existing methods for handling label noise\nassume that the noise is completely random and independent of features or\nlabels, which is not the case for ICD data. Therefore, we develop a new method\nfor training robust classifiers in the presence of systematic noise. We first\nidentify ICD-10 codes that human coders tend to misuse or confuse, based on the\ncodes' locations in the ICD-10 hierarchy, the types of the codes, and baseline\nclassifier's prediction behaviors; we then develop a novel training strategy\nthat accounts for such noise. We compared our method with the baseline that\ndoes not handle label noise and the baseline methods that assume random noise,\nand demonstrated that our proposed method outperforms all baselines when\nevaluated on expert validated labels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 23:05:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kim", "Youngwoo", ""], ["Li", "Cheng", ""], ["Ye", "Bingyang", ""], ["Tahmasebi", "Amir", ""], ["Aslam", "Javed", ""]]}, {"id": "2103.07829", "submitter": "Chenliang Li", "authors": "Chenliang Li, Ming Yan, Haiyang Xu, Fuli Luo, Wei Wang, Bin Bi,\n  Songfang Huang", "title": "SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple\n  Levels", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-language pre-training (VLP) on large-scale image-text pairs has\nrecently witnessed rapid progress for learning cross-modal representations.\nExisting pre-training methods either directly concatenate image representation\nand text representation at a feature level as input to a single-stream\nTransformer, or use a two-stream cross-modal Transformer to align the\nimage-text representation at a high-level semantic space. In real-world\nimage-text data, we observe that it is easy for some of the image-text pairs to\nalign simple semantics on both modalities, while others may be related after\nhigher-level abstraction. Therefore, in this paper, we propose a new\npre-training method SemVLP, which jointly aligns both the low-level and\nhigh-level semantics between image and text representations. The model is\npre-trained iteratively with two prevalent fashions: single-stream pre-training\nto align at a fine-grained feature level and two-stream pre-training to align\nhigh-level semantics, by employing a shared Transformer network with a\npluggable cross-modal attention module. An extensive set of experiments have\nbeen conducted on four well-established vision-language understanding tasks to\ndemonstrate the effectiveness of the proposed SemVLP in aligning cross-modal\nrepresentations towards different semantic granularities.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 02:39:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Li", "Chenliang", ""], ["Yan", "Ming", ""], ["Xu", "Haiyang", ""], ["Luo", "Fuli", ""], ["Wang", "Wei", ""], ["Bi", "Bin", ""], ["Huang", "Songfang", ""]]}, {"id": "2103.07833", "submitter": "Pranav Narayanan Venkit", "authors": "Pranav Venkit, Zeba Karishma, Chi-Yang Hsu, Rahul Katiki, Kenneth\n  Huang, Shomir Wilson, Patrick Dudas", "title": "A `Sourceful' Twist: Emoji Prediction Based on Sentiment, Hashtags and\n  Application Source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We widely use emojis in social networking to heighten, mitigate or negate the\nsentiment of the text. Emoji suggestions already exist in many cross-platform\napplications but an emoji is predicted solely based a few prominent words\ninstead of understanding the subject and substance of the text. Through this\npaper, we showcase the importance of using Twitter features to help the model\nunderstand the sentiment involved and hence to predict the most suitable emoji\nfor the text. Hashtags and Application Sources like Android, etc. are two\nfeatures which we found to be important yet underused in emoji prediction and\nTwitter sentiment analysis on the whole. To approach this shortcoming and to\nfurther understand emoji behavioral patterns, we propose a more balanced\ndataset by crawling additional Twitter data, including timestamp, hashtags, and\napplication source acting as additional attributes to the tweet. Our data\nanalysis and neural network model performance evaluations depict that using\nhashtags and application sources as features allows to encode different\ninformation and is effective in emoji prediction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 03:05:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Venkit", "Pranav", ""], ["Karishma", "Zeba", ""], ["Hsu", "Chi-Yang", ""], ["Katiki", "Rahul", ""], ["Huang", "Kenneth", ""], ["Wilson", "Shomir", ""], ["Dudas", "Patrick", ""]]}, {"id": "2103.07845", "submitter": "Hui Li", "authors": "Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li,\n  Rongxin Wu", "title": "Improving Code Summarization with Block-wise Abstract Syntax Tree\n  Splitting", "comments": "Accepted in 29th IEEE/ACM International Conference on Program\n  Comprehension (ICPC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic code summarization frees software developers from the heavy burden\nof manual commenting and benefits software development and maintenance.\nAbstract Syntax Tree (AST), which depicts the source code's syntactic\nstructure, has been incorporated to guide the generation of code summaries.\nHowever, existing AST based methods suffer from the difficulty of training and\ngenerate inadequate code summaries. In this paper, we present the Block-wise\nAbstract Syntax Tree Splitting method (BASTS for short), which fully utilizes\nthe rich tree-form syntax structure in ASTs, for improving code summarization.\nBASTS splits the code of a method based on the blocks in the dominator tree of\nthe Control Flow Graph, and generates a split AST for each code split. Each\nsplit AST is then modeled by a Tree-LSTM using a pre-training strategy to\ncapture local non-linear syntax encoding. The learned syntax encoding is\ncombined with code encoding, and fed into Transformer to generate high-quality\ncode summaries. Comprehensive experiments on benchmarks have demonstrated that\nBASTS significantly outperforms state-of-the-art approaches in terms of various\nevaluation metrics. To facilitate reproducibility, our implementation is\navailable at https://github.com/XMUDM/BASTS.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 05:04:06 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:15:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Lin", "Chen", ""], ["Ouyang", "Zhichao", ""], ["Zhuang", "Junqing", ""], ["Chen", "Jianqiang", ""], ["Li", "Hui", ""], ["Wu", "Rongxin", ""]]}, {"id": "2103.07875", "submitter": "Heewoong Park", "authors": "Heewoong Park, Sukhyun Cho, Jonghun Park", "title": "Learning a Word-Level Language Model with Sentence-Level Noise\n  Contrastive Estimation for Contextual Sentence Probability Estimation", "comments": "8 pages, 1 figures, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the probability distribution of sentences or word sequences is a\nkey process in natural language processing. While word-level language models\n(LMs) have been widely adopted for computing the joint probabilities of word\nsequences, they have difficulty in capturing a context long enough for sentence\nprobability estimation (SPE). To overcome this, recent studies introduced\ntraining methods using sentence-level noise-contrastive estimation (NCE) with\nrecurrent neural networks (RNNs). In this work, we attempt to extend it for\ncontextual SPE, which aims to estimate a conditional sentence probability given\na previous text. The proposed NCE samples negative sentences independently of a\nprevious text so that the trained model gives higher probabilities to the\nsentences that are more consistent with \\textcolor{blue}{the} context. We apply\nour method to a simple word-level RNN LM to focus on the effect of the\nsentence-level NCE training rather than on the network architecture. The\nquality of estimation was evaluated against multiple-choice cloze-style\nquestions including both human and automatically generated questions. The\nexperimental results show that the proposed method improved the SPE quality for\nthe word-level RNN LM.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 09:17:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Park", "Heewoong", ""], ["Cho", "Sukhyun", ""], ["Park", "Jonghun", ""]]}, {"id": "2103.07929", "submitter": "Anya Belz", "authors": "Anya Belz, Shubham Agarwal, Anastasia Shimorina, Ehud Reiter", "title": "A Systematic Review of Reproducibility Research in Natural Language\n  Processing", "comments": "To be published in proceedings of EACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Against the background of what has been termed a reproducibility crisis in\nscience, the NLP field is becoming increasingly interested in, and\nconscientious about, the reproducibility of its results. The past few years\nhave seen an impressive range of new initiatives, events and active research in\nthe area. However, the field is far from reaching a consensus about how\nreproducibility should be defined, measured and addressed, with diversity of\nviews currently increasing rather than converging. With this focused\ncontribution, we aim to provide a wide-angle, and as near as possible complete,\nsnapshot of current work on reproducibility in NLP, delineating differences and\nsimilarities, and providing pointers to common denominators.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 13:53:05 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 11:11:14 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Belz", "Anya", ""], ["Agarwal", "Shubham", ""], ["Shimorina", "Anastasia", ""], ["Reiter", "Ehud", ""]]}, {"id": "2103.08001", "submitter": "Amartya Hatua", "authors": "Amartya Hatua, Arjun Mukherjee and Rakesh M. Verma", "title": "Claim Verification using a Multi-GAN based Model", "comments": "Paper is submitted at LDK 2021 3rd Conference on Language, Data and\n  Knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes research on claim verification carried out using a\nmultiple GAN-based model. The proposed model consists of three pairs of\ngenerators and discriminators. The generator and discriminator pairs are\nresponsible for generating synthetic data for supported and refuted claims and\nclaim labels. A theoretical discussion about the proposed model is provided to\nvalidate the equilibrium state of the model. The proposed model is applied to\nthe FEVER dataset, and a pre-trained language model is used for the input text\ndata. The synthetically generated data helps to gain information which helps\nthe model to perform better than state of the art models and other standard\nclassifiers.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 19:15:53 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 22:17:50 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 05:02:28 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Hatua", "Amartya", ""], ["Mukherjee", "Arjun", ""], ["Verma", "Rakesh M.", ""]]}, {"id": "2103.08052", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou and Chris C. Emezue", "title": "Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine\n  Translation: The Case of Fon Language", "comments": null, "journal-ref": "African NLP, EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building effective neural machine translation (NMT) models for very\nlow-resourced and morphologically rich African indigenous languages is an open\nchallenge. Besides the issue of finding available resources for them, a lot of\nwork is put into preprocessing and tokenization. Recent studies have shown that\nstandard tokenization methods do not always adequately deal with the\ngrammatical, diacritical, and tonal properties of some African languages. That,\ncoupled with the extremely low availability of training samples, hinders the\nproduction of reliable NMT models. In this paper, using Fon language as a case\nstudy, we revisit standard tokenization methods and introduce\nWord-Expressions-Based (WEB) tokenization, a human-involved super-words\ntokenization strategy to create a better representative vocabulary for\ntraining. Furthermore, we compare our tokenization strategy to others on the\nFon-French and French-Fon translation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:12:14 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 13:00:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Emezue", "Chris C.", ""]]}, {"id": "2103.08139", "submitter": "Yufang Liu", "authors": "Yufang Liu, Tao Ji, Yuanbin Wu, Man Lan", "title": "Generating CCG Categories", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous CCG supertaggers usually predict categories using multi-class\nclassification. Despite their simplicity, internal structures of categories are\nusually ignored. The rich semantics inside these structures may help us to\nbetter handle relations among categories and bring more robustness into\nexisting supertaggers. In this work, we propose to generate categories rather\nthan classify them: each category is decomposed into a sequence of smaller\natomic tags, and the tagger aims to generate the correct sequence. We show that\nwith this finer view on categories, annotations of different categories could\nbe shared and interactions with sentence contexts could be enhanced. The\nproposed category generator is able to achieve state-of-the-art tagging (95.5%\naccuracy) and parsing (89.8% labeled F1) performances on the standard CCGBank.\nFurthermore, its performances on infrequent (even unseen) categories,\nout-of-domain texts and low resource language give promising results on\nintroducing generation models to the general CCG analyses.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:01:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liu", "Yufang", ""], ["Ji", "Tao", ""], ["Wu", "Yuanbin", ""], ["Lan", "Man", ""]]}, {"id": "2103.08199", "submitter": "Tadahiro Taniguchi", "authors": "Yasuaki Okuda, Ryo Ozaki, and Tadahiro Taniguchi", "title": "Double Articulation Analyzer with Prosody for Unsupervised Word and\n  Phoneme Discovery", "comments": "11 pages, Submitted to IEEE Transactions on Cognitive and\n  Developmental Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants acquire words and phonemes from unsegmented speech signals using\nsegmentation cues, such as distributional, prosodic, and co-occurrence cues.\nMany pre-existing computational models that represent the process tend to focus\non distributional or prosodic cues. This paper proposes a nonparametric\nBayesian probabilistic generative model called the prosodic hierarchical\nDirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM,\nan extension of HDP-HLM, considers both prosodic and distributional cues within\na single integrative generative model. We conducted three experiments on\ndifferent types of datasets, and demonstrate the validity of the proposed\nmethod. The results show that the Prosodic DAA successfully uses prosodic cues\nand outperforms a method that solely uses distributional cues. The main\ncontributions of this study are as follows: 1) We develop a probabilistic\ngenerative model for time series data including prosody that potentially has a\ndouble articulation structure; 2) We propose the Prosodic DAA by deriving the\ninference procedure for Prosodic HDP-HLM and show that Prosodic DAA can\ndiscover words directly from continuous human speech signals using statistical\ninformation and prosodic information in an unsupervised manner; 3) We show that\nprosodic cues contribute to word segmentation more in naturally distributed\ncase words, i.e., they follow Zipf's law.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:17:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Okuda", "Yasuaki", ""], ["Ozaki", "Ryo", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2103.08200", "submitter": "Jiaxin Pan", "authors": "Jiaxin Pan, Min Peng, Yiyan Zhang", "title": "Mention-centered Graph Neural Network for Document-level Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction aims to discover relations between\nentities across a whole document. How to build the dependency of entities from\ndifferent sentences in a document remains to be a great challenge. Current\napproaches either leverage syntactic trees to construct document-level graphs\nor aggregate inference information from different sentences. In this paper, we\nbuild cross-sentence dependencies by inferring compositional relations between\ninter-sentence mentions. Adopting aggressive linking strategy, intermediate\nrelations are reasoned on the document-level graphs by mention convolution. We\nfurther notice the generalization problem of NA instances, which is caused by\nincomplete annotation and worsened by fully-connected mention pairs. An\nimproved ranking loss is proposed to attend this problem. Experiments show the\nconnections between different mentions are crucial to document-level relation\nextraction, which enables the model to extract more meaningful higher-level\ncompositional relations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:19:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Pan", "Jiaxin", ""], ["Peng", "Min", ""], ["Zhang", "Yiyan", ""]]}, {"id": "2103.08207", "submitter": "Ziqiang Zhang", "authors": "Zi-Qiang Zhang, Yan Song, Ming-Hui Wu, Xin Fang, Li-Rong Dai", "title": "XLST: Cross-lingual Self-training to Learn Multilingual Representation\n  for Low Resource Speech Recognition", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a weakly supervised multilingual representation\nlearning framework, called cross-lingual self-training (XLST). XLST is able to\nutilize a small amount of annotated data from high-resource languages to\nimprove the representation learning on multilingual un-annotated data.\nSpecifically, XLST uses a supervised trained model to produce initial\nrepresentations and another model to learn from them, by maximizing the\nsimilarity between output embeddings of these two models. Furthermore, the\nmoving average mechanism and multi-view data augmentation are employed, which\nare experimentally shown to be crucial to XLST. Comprehensive experiments have\nbeen conducted on the CommonVoice corpus to evaluate the effectiveness of XLST.\nResults on 5 downstream low-resource ASR tasks shows that our multilingual\npretrained model achieves relatively 18.6% PER reduction over the\nstate-of-the-art self-supervised method, with leveraging additional 100 hours\nof annotated English data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:33:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhang", "Zi-Qiang", ""], ["Song", "Yan", ""], ["Wu", "Ming-Hui", ""], ["Fang", "Xin", ""], ["Dai", "Li-Rong", ""]]}, {"id": "2103.08364", "submitter": "Claudio Fantinuoli", "authors": "Claudio Fantinuoli, Bianca Prandi", "title": "Towards the evaluation of automatic simultaneous speech translation from\n  a communicative perspective", "comments": "Accepted to IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, automatic speech-to-speech and speech-to-text translation\nhas gained momentum thanks to advances in artificial intelligence, especially\nin the domains of speech recognition and machine translation. The quality of\nsuch applications is commonly tested with automatic metrics, such as BLEU,\nprimarily with the goal of assessing improvements of releases or in the context\nof evaluation campaigns. However, little is known about how the output of such\nsystems is perceived by end users or how they compare to human performances in\nsimilar communicative tasks.\n  In this paper, we present the results of an experiment aimed at evaluating\nthe quality of a real-time speech translation engine by comparing it to the\nperformance of professional simultaneous interpreters. To do so, we adopt a\nframework developed for the assessment of human interpreters and use it to\nperform a manual evaluation on both human and machine performances. In our\nsample, we found better performance for the human interpreters in terms of\nintelligibility, while the machine performs slightly better in terms of\ninformativeness. The limitations of the study and the possible enhancements of\nthe chosen framework are discussed. Despite its intrinsic limitations, the use\nof this framework represents a first step towards a user-centric and\ncommunication-oriented methodology for evaluating real-time automatic speech\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:09:00 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 21:56:49 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fantinuoli", "Claudio", ""], ["Prandi", "Bianca", ""]]}, {"id": "2103.08387", "submitter": "Hongyang Gao", "authors": "Hongyang Gao, Yi Liu, Xuan Zhang, Shuiwang Ji", "title": "Sent2Matrix: Folding Character Sequences in Serpentine Manifolds for\n  Two-Dimensional Sentence", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study text representation methods using deep models. Current methods, such\nas word-level embedding and character-level embedding schemes, treat texts as\neither a sequence of atomic words or a sequence of characters. These methods\neither ignore word morphologies or word boundaries. To overcome these\nlimitations, we propose to convert texts into 2-D representations and develop\nthe Sent2Matrix method. Our method allows for the explicit incorporation of\nboth word morphologies and boundaries. When coupled with a novel serpentine\npadding method, our Sent2Matrix method leads to an interesting visualization in\nwhich 1-D character sequences are folded into 2-D serpentine manifolds.\nNotably, our method is the first attempt to represent texts in 2-D formats.\nExperimental results on text classification tasks shown that our method\nconsistently outperforms prior embedding methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:52:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gao", "Hongyang", ""], ["Liu", "Yi", ""], ["Zhang", "Xuan", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2103.08466", "submitter": "Abdelrahim Elmadany", "authors": "Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Houda\n  Bouamor, Nizar Habash", "title": "NADI 2021: The Second Nuanced Arabic Dialect Identification Shared Task", "comments": "arXiv admin note: text overlap with arXiv:2010.11334", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the findings and results of the Second Nuanced Arabic Dialect\nIdentification Shared Task (NADI 2021). This Shared Task includes four\nsubtasks: country-level Modern Standard Arabic (MSA) identification (Subtask\n1.1), country-level dialect identification (Subtask 1.2), province-level MSA\nidentification (Subtask 2.1), and province-level sub-dialect identification\n(Subtask 2.2). The shared task dataset covers a total of 100 provinces from 21\nArab countries, collected from the Twitter domain. A total of 53 teams from 23\ncountries registered to participate in the tasks, thus reflecting the interest\nof the community in this area. We received 16 submissions for Subtask 1.1 from\nfive teams, 27 submissions for Subtask 1.2 from eight teams, 12 submissions for\nSubtask 2.1 from four teams, and 13 Submissions for subtask 2.2 from four\nteams.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:59:37 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 22:57:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Zhang", "Chiyu", ""], ["Elmadany", "AbdelRahim", ""], ["Bouamor", "Houda", ""], ["Habash", "Nizar", ""]]}, {"id": "2103.08490", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Sebastian Ruder, Graham Neubig", "title": "Multi-view Subword Regularization", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual pretrained representations generally rely on subword\nsegmentation algorithms to create a shared multilingual vocabulary. However,\nstandard heuristic algorithms often lead to sub-optimal segmentation,\nespecially for languages with limited amounts of data. In this paper, we take\ntwo major steps towards alleviating this problem. First, we demonstrate\nempirically that applying existing subword regularization methods(Kudo, 2018;\nProvilkov et al., 2020) during fine-tuning of pre-trained multilingual\nrepresentations improves the effectiveness of cross-lingual transfer. Second,\nto take full advantage of different possible input segmentations, we propose\nMulti-view Subword Regularization (MVR), a method that enforces the consistency\nbetween predictions of using inputs tokenized by the standard and probabilistic\nsegmentations. Results on the XTREME multilingual benchmark(Hu et al., 2020)\nshow that MVR brings consistent improvements of up to 2.5 points over using\nstandard segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:07:42 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 14:34:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Xinyi", ""], ["Ruder", "Sebastian", ""], ["Neubig", "Graham", ""]]}, {"id": "2103.08541", "submitter": "Tal Schuster", "authors": "Tal Schuster, Adam Fisch, Regina Barzilay", "title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical fact verification models use retrieved written evidence to verify\nclaims. Evidence sources, however, often change over time as more information\nis gathered and revised. In order to adapt, models must be sensitive to subtle\ndifferences in supporting evidence. We present VitaminC, a benchmark infused\nwith challenging cases that require fact verification models to discern and\nadjust to slight factual changes. We collect over 100,000 Wikipedia revisions\nthat modify an underlying fact, and leverage these revisions, together with\nadditional synthetically constructed ones, to create a total of over 400,000\nclaim-evidence pairs. Unlike previous resources, the examples in VitaminC are\ncontrastive, i.e., they contain evidence pairs that are nearly identical in\nlanguage and content, with the exception that one supports a given claim while\nthe other does not. We show that training using this design increases\nrobustness -- improving accuracy by 10% on adversarial fact verification and 6%\non adversarial natural language inference (NLI). Moreover, the structure of\nVitaminC leads us to define additional tasks for fact-checking resources:\ntagging relevant words in the evidence for verifying the claim, identifying\nfactual revisions, and providing automatic edits via factually consistent text\ngeneration.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:05:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Schuster", "Tal", ""], ["Fisch", "Adam", ""], ["Barzilay", "Regina", ""]]}, {"id": "2103.08545", "submitter": "Miruna Clinciu", "authors": "Miruna Clinciu, Arash Eshghi, and Helen Hastie", "title": "A Study of Automatic Metrics for the Evaluation of Natural Language\n  Explanations", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": "2021.eacl-main.202", "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As transparency becomes key for robotics and AI, it will be necessary to\nevaluate the methods through which transparency is provided, including\nautomatically generated natural language (NL) explanations. Here, we explore\nparallels between the generation of such explanations and the much-studied\nfield of evaluation of Natural Language Generation (NLG). Specifically, we\ninvestigate which of the NLG evaluation measures map well to explanations. We\npresent the ExBAN corpus: a crowd-sourced corpus of NL explanations for\nBayesian Networks. We run correlations comparing human subjective ratings with\nNLG automatic measures. We find that embedding-based automatic NLG evaluation\nmethods, such as BERTScore and BLEURT, have a higher correlation with human\nratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work\nhas implications for Explainable AI and transparent robotic and autonomous\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:10:39 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Clinciu", "Miruna", ""], ["Eshghi", "Arash", ""], ["Hastie", "Helen", ""]]}, {"id": "2103.08647", "submitter": "David Adelani", "authors": "David I. Adelani, Dana Ruiter, Jesujoba O. Alabi, Damilola Adebonojo,\n  Adesina Ayeni, Mofe Adeyemi, Ayodele Awokoya, Cristina Espa\\~na-Bonet", "title": "The Effect of Domain and Diacritics in Yor\\`ub\\'a-English Neural Machine\n  Translation", "comments": "Accepted to MT Summit 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massively multilingual machine translation (MT) has shown impressive\ncapabilities, including zero and few-shot translation between low-resource\nlanguage pairs. However, these models are often evaluated on high-resource\nlanguages with the assumption that they generalize to low-resource ones. The\ndifficulty of evaluating MT models on low-resource pairs is often due to lack\nof standardized evaluation datasets. In this paper, we present MENYO-20k, the\nfirst multi-domain parallel corpus with a special focus on clean orthography\nfor Yor\\`ub\\'a--English with standardized train-test splits for benchmarking.\nWe provide several neural MT benchmarks and compare them to the performance of\npopular pre-trained (massively multilingual) MT models both for the\nheterogeneous test set and its subdomains. Since these pre-trained models use\nhuge amounts of data with uncertain quality, we also analyze the effect of\ndiacritics, a major characteristic of Yor\\`ub\\'a, in the training data. We\ninvestigate how and when this training condition affects the final quality and\nintelligibility of a translation. Our models outperform massively multilingual\nmodels such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when\ntranslating to Yor\\`ub\\'a, setting a high quality benchmark for future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:52:32 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:16:33 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Adelani", "David I.", ""], ["Ruiter", "Dana", ""], ["Alabi", "Jesujoba O.", ""], ["Adebonojo", "Damilola", ""], ["Ayeni", "Adesina", ""], ["Adeyemi", "Mofe", ""], ["Awokoya", "Ayodele", ""], ["Espa\u00f1a-Bonet", "Cristina", ""]]}, {"id": "2103.08656", "submitter": "Jos\\'e Miguel Bened\\'i", "authors": "Mauricio Maca, Jos\\'e Miguel Bened\\'i and Joan Andreu S\\'anchez", "title": "Discriminative Learning for Probabilistic Context-Free Grammars based on\n  Generalized H-Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal framework for the development of a family of\ndiscriminative learning algorithms for Probabilistic Context-Free Grammars\n(PCFGs) based on a generalization of criterion-H. First of all, we propose the\nH-criterion as the objective function and the Growth Transformations as the\noptimization method, which allows us to develop the final expressions for the\nestimation of the parameters of the PCFGs. And second, we generalize the\nH-criterion to take into account the set of reference interpretations and the\nset of competing interpretations, and we propose a new family of objective\nfunctions that allow us to develop the expressions of the estimation\ntransformations for PCFGs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 19:07:17 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Maca", "Mauricio", ""], ["Bened\u00ed", "Jos\u00e9 Miguel", ""], ["S\u00e1nchez", "Joan Andreu", ""]]}, {"id": "2103.08759", "submitter": "Gene Louis Kim", "authors": "Gene Louis Kim, Viet Duong, Xin Lu, Lenhart Schubert", "title": "A Transition-based Parser for Unscoped Episodic Logical Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Episodic Logic:Unscoped Logical Form\" (EL-ULF) is a semantic representation\ncapturing predicate-argument structure as well as more challenging aspects of\nlanguage within the Episodic Logic formalism. We present the first learned\napproach for parsing sentences into ULFs, using a growing set of annotated\nexamples. The results provide a strong baseline for future improvement. Our\nmethod learns a sequence-to-sequence model for predicting the transition action\nsequence within a modified cache transition system. We evaluate the efficacy of\ntype grammar-based constraints, a word-to-symbol lexicon, and transition system\nstate features in this task. Our system is available at\nhttps://github.com/genelkim/ulf-transition-parser We also present the first\nofficial annotated ULF dataset at\nhttps://www.cs.rochester.edu/u/gkim21/ulf/resources/.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 23:09:32 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Kim", "Gene Louis", ""], ["Duong", "Viet", ""], ["Lu", "Xin", ""], ["Schubert", "Lenhart", ""]]}, {"id": "2103.08780", "submitter": "Maximilian Kupi", "authors": "Maximilian Kupi, Michael Bodnar, Nikolas Schmidt, and Carlos Eduardo\n  Posada", "title": "dictNN: A Dictionary-Enhanced CNN Approach for Classifying Hate Speech\n  on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hate speech on social media is a growing concern, and automated methods have\nso far been sub-par at reliably detecting it. A major challenge lies in the\npotentially evasive nature of hate speech due to the ambiguity and fast\nevolution of natural language. To tackle this, we introduce a vectorisation\nbased on a crowd-sourced and continuously updated dictionary of hate words and\npropose fusing this approach with standard word embedding in order to improve\nthe classification performance of a CNN model. To train and test our model we\nuse a merge of two established datasets (110,748 tweets in total). By adding\nthe dictionary-enhanced input, we are able to increase the CNN model's\npredictive power and increase the F1 macro score by seven percentage points.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:27:33 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Kupi", "Maximilian", ""], ["Bodnar", "Michael", ""], ["Schmidt", "Nikolas", ""], ["Posada", "Carlos Eduardo", ""]]}, {"id": "2103.08784", "submitter": "Yen-Chun Chen", "authors": "Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang,\n  Jingjing Liu", "title": "LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time\n  Image-Text Retrieval", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal pre-training has propelled great advancement in\nvision-and-language research. These large-scale pre-trained models, although\nsuccessful, fatefully suffer from slow inference speed due to enormous\ncomputation cost mainly from cross-modal attention in Transformer architecture.\nWhen applied to real-life applications, such latency and computation demand\nseverely deter the practical use of pre-trained models. In this paper, we study\nImage-text retrieval (ITR), the most mature scenario of V+L application, which\nhas been widely studied even prior to the emergence of recent pre-trained\nmodels. We propose a simple yet highly effective approach, LightningDOT that\naccelerates the inference time of ITR by thousands of times, without\nsacrificing accuracy. LightningDOT removes the time-consuming cross-modal\nattention by pre-training on three novel learning objectives, extracting\nfeature indexes offline, and employing instant dot-product matching with\nfurther re-ranking, which significantly speeds up retrieval process. In fact,\nLightningDOT achieves new state of the art across multiple ITR benchmarks such\nas Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that\nconsume 1000x magnitude of computational hours. Code and pre-training\ncheckpoints are available at https://github.com/intersun/LightningDOT.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:35:28 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 21:53:08 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sun", "Siqi", ""], ["Chen", "Yen-Chun", ""], ["Li", "Linjie", ""], ["Wang", "Shuohang", ""], ["Fang", "Yuwei", ""], ["Liu", "Jingjing", ""]]}, {"id": "2103.08809", "submitter": "Stanislav Peshterliev", "authors": "Haytham ElFadeel and Stan Peshterliev", "title": "Robustly Optimized and Distilled Training for Natural Language\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore multi-task learning (MTL) as a second pretraining\nstep to learn enhanced universal language representation for transformer\nlanguage models. We use the MTL enhanced representation across several natural\nlanguage understanding tasks to improve performance and generalization.\nMoreover, we incorporate knowledge distillation (KD) in MTL to further boost\nperformance and devise a KD variant that learns effectively from multiple\nteachers. By combining MTL and KD, we propose Robustly Optimized and Distilled\n(ROaD) modeling framework. We use ROaD together with the ELECTRA model to\nobtain state-of-the-art results for machine reading comprehension and natural\nlanguage inference.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 02:35:22 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["ElFadeel", "Haytham", ""], ["Peshterliev", "Stan", ""]]}, {"id": "2103.08849", "submitter": "Po-Yao Huang", "authors": "Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze\n  and Alexander Hauptmann", "title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual\n  Transfer of Vision-Language Models", "comments": "accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies zero-shot cross-lingual transfer of vision-language\nmodels. Specifically, we focus on multilingual text-to-video search and propose\na Transformer-based model that learns contextualized multilingual multimodal\nembeddings. Under a zero-shot setting, we empirically demonstrate that\nperformance degrades significantly when we query the multilingual text-video\nmodel with non-English sentences. To address this problem, we introduce a\nmultilingual multimodal pre-training strategy, and collect a new multilingual\ninstructional video dataset (MultiHowTo100M) for pre-training. Experiments on\nVTT show that our method significantly improves video search in non-English\nlanguages without additional annotations. Furthermore, when multilingual\nannotations are available, our method outperforms recent baselines by a large\nmargin in multilingual text-to-video search on VTT and VATEX; as well as in\nmultilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is\navailable at http://github.com/berniebear/Multi-HT100M.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 04:37:40 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 17:40:09 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 02:01:38 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Huang", "Po-Yao", ""], ["Patrick", "Mandela", ""], ["Hu", "Junjie", ""], ["Neubig", "Graham", ""], ["Metze", "Florian", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2103.08862", "submitter": "Pengbo Liu", "authors": "Pengbo Liu, Hailong Cao, Tiejun Zhao", "title": "Gumbel-Attention for Multi-modal Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal machine translation (MMT) improves translation quality by\nintroducing visual information. However, the existing MMT model ignores the\nproblem that the image will bring information irrelevant to the text, causing\nmuch noise to the model and affecting the translation quality. In this paper,\nwe propose a novel Gumbel-Attention for multi-modal machine translation, which\nselects the text-related parts of the image features. Specifically, different\nfrom the previous attention-based method, we first use a differentiable method\nto select the image information and automatically remove the useless parts of\nthe image features. Through the score matrix of Gumbel-Attention and image\nfeatures, the image-aware text representation is generated. And then, we\nindependently encode the text representation and the image-aware text\nrepresentation with the multi-modal encoder. Finally, the final output of the\nencoder is obtained through multi-modal gated fusion. Experiments and case\nanalysis proves that our method retains the image features related to the text,\nand the remaining parts help the MMT model generates better translations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 05:44:01 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Liu", "Pengbo", ""], ["Cao", "Hailong", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2103.08886", "submitter": "Haiqin Yang", "authors": "Zengfeng Zeng, Dan Ma, Haiqin Yang, Zhen Gou and Jianping Shen", "title": "Automatic Intent-Slot Induction for Dialogue Systems", "comments": "12 pages, 11 figures, 6 tables, in WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatically and accurately identifying user intents and filling the\nassociated slots from their spoken language are critical to the success of\ndialogue systems. Traditional methods require manually defining the\nDOMAIN-INTENT-SLOT schema and asking many domain experts to annotate the\ncorresponding utterances, upon which neural models are trained. This procedure\nbrings the challenges of information sharing hindering, out-of-schema, or data\nsparsity in open-domain dialogue systems. To tackle these challenges, we\nexplore a new task of {\\em automatic intent-slot induction} and propose a novel\ndomain-independent tool. That is, we design a coarse-to-fine three-step\nprocedure including Role-labeling, Concept-mining, And Pattern-mining (RCAP):\n(1) role-labeling: extracting keyphrases from users' utterances and classifying\nthem into a quadruple of coarsely-defined intent-roles via sequence labeling;\n(2) concept-mining: clustering the extracted intent-role mentions and naming\nthem into abstract fine-grained concepts; (3) pattern-mining: applying the\nApriori algorithm to mine intent-role patterns and automatically inferring the\nintent-slot using these coarse-grained intent-role labels and fine-grained\nconcepts. Empirical evaluations on both real-world in-domain and out-of-domain\ndatasets show that: (1) our RCAP can generate satisfactory SLU schema and\noutperforms the state-of-the-art supervised learning method; (2) our RCAP can\nbe directly applied to out-of-domain datasets and gain at least 76\\%\nimprovement of F1-score on intent detection and 41\\% improvement of F1-score on\nslot filling; (3) our RCAP exhibits its power in generic intent-slot\nextractions with less manual effort, which opens pathways for schema induction\non new domains and unseen intent-slot discovery for generalizable dialogue\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:21:31 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zeng", "Zengfeng", ""], ["Ma", "Dan", ""], ["Yang", "Haiqin", ""], ["Gou", "Zhen", ""], ["Shen", "Jianping", ""]]}, {"id": "2103.08893", "submitter": "Haiqin Yang", "authors": "Yiying Yang, Xi Yin, Haiqin Yang, Xingjian Fei, Hao Peng, Kaijie Zhou,\n  Kunfeng Lai, and Jianping Shen", "title": "KGSynNet: A Novel Entity Synonyms Discovery Framework with Knowledge\n  Graph", "comments": "16 pages, 3 figures, 5 tables, in DASFAA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Entity synonyms discovery is crucial for entity-leveraging applications.\nHowever, existing studies suffer from several critical issues: (1) the input\nmentions may be out-of-vocabulary (OOV) and may come from a different semantic\nspace of the entities; (2) the connection between mentions and entities may be\nhidden and cannot be established by surface matching; and (3) some entities\nrarely appear due to the long-tail effect. To tackle these challenges, we\nfacilitate knowledge graphs and propose a novel entity synonyms discovery\nframework, named \\emph{KGSynNet}. Specifically, we pre-train subword embeddings\nfor mentions and entities using a large-scale domain-specific corpus while\nlearning the knowledge embeddings of entities via a joint TransC-TransE model.\nMore importantly, to obtain a comprehensive representation of entities, we\nemploy a specifically designed \\emph{fusion gate} to adaptively absorb the\nentities' knowledge information into their semantic features. We conduct\nextensive experiments to demonstrate the effectiveness of our \\emph{KGSynNet}\nin leveraging the knowledge graph. The experimental results show that the\n\\emph{KGSynNet} improves the state-of-the-art methods by 14.7\\% in terms of\nhits@3 in the offline evaluation and outperforms the BERT model by 8.3\\% in the\npositive feedback rate of an online A/B test on the entity linking module of a\nquestion answering system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:32:33 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 08:41:06 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yang", "Yiying", ""], ["Yin", "Xi", ""], ["Yang", "Haiqin", ""], ["Fei", "Xingjian", ""], ["Peng", "Hao", ""], ["Zhou", "Kaijie", ""], ["Lai", "Kunfeng", ""], ["Shen", "Jianping", ""]]}, {"id": "2103.08952", "submitter": "Philipp Wicke", "authors": "Philipp Wicke and Marianna M. Bolognesi", "title": "Covid-19 Discourse on Twitter: How the Topics, Sentiments, Subjectivity,\n  and Figurative Frames Changed Over Time", "comments": null, "journal-ref": "Frontiers in Communication, Volume: 6, Pages: 45, Year: 2021", "doi": "10.3389/fcomm.2021.651997", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The words we use to talk about the current epidemiological crisis on social\nmedia can inform us on how we are conceptualizing the pandemic and how we are\nreacting to its development. This paper provides an extensive explorative\nanalysis of how the discourse about Covid-19 reported on Twitter changes\nthrough time, focusing on the first wave of this pandemic. Based on an\nextensive corpus of tweets (produced between 20th March and 1st July 2020)\nfirst we show how the topics associated with the development of the pandemic\nchanged through time, using topic modeling. Second, we show how the sentiment\npolarity of the language used in the tweets changed from a relatively positive\nvalence during the first lockdown, toward a more negative valence in\ncorrespondence with the reopening. Third we show how the average subjectivity\nof the tweets increased linearly and fourth, how the popular and frequently\nused figurative frame of WAR changed when real riots and fights entered the\ndiscourse.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:22:39 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Wicke", "Philipp", ""], ["Bolognesi", "Marianna M.", ""]]}, {"id": "2103.08955", "submitter": "Stefan Gr\\\"unewald", "authors": "Stefan Gr\\\"unewald, Prisca Piccirilli, Annemarie Friedrich", "title": "Coordinate Constructions in English Enhanced Universal Dependencies:\n  Analysis and Computational Modeling", "comments": "15 pages, 2 figures; to be published at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the representation of coordinate constructions in\nEnhanced Universal Dependencies (UD), where relevant dependency links are\npropagated from conjunction heads to other conjuncts. English treebanks for\nenhanced UD have been created from gold basic dependencies using a heuristic\nrule-based converter, which propagates only core arguments. With the aim of\ndetermining which set of links should be propagated from a semantic\nperspective, we create a large-scale dataset of manually edited syntax graphs.\nWe identify several systematic errors in the original data, and propose to also\npropagate adjuncts. We observe high inter-annotator agreement for this semantic\nannotation task. Using our new manually verified dataset, we perform the first\nprincipled comparison of rule-based and (partially novel) machine-learning\nbased methods for conjunction propagation for English. We show that learning\npropagation rules is more effective than hand-designing heuristic rules. When\nusing automatic parses, our neural graph-parser based edge predictor\noutperforms the currently predominant pipelinesusing a basic-layer tree parser\nplus converters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:24:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Gr\u00fcnewald", "Stefan", ""], ["Piccirilli", "Prisca", ""], ["Friedrich", "Annemarie", ""]]}, {"id": "2103.08993", "submitter": "Laurent Besacier", "authors": "Jama Hussein Mohamud, Lloyd Acquaye Thompson, Aissatou Ndoye, and\n  Laurent Besacier", "title": "Fast Development of ASR in African Languages using Self Supervised\n  Speech Representation Learning", "comments": "Accepted at AfricaNLP2021 workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the results of an informal collaboration launched during\nthe African Master of Machine Intelligence (AMMI) in June 2020. After a series\nof lectures and labs on speech data collection using mobile applications and on\nself-supervised representation learning from speech, a small group of students\nand the lecturer continued working on automatic speech recognition (ASR)\nproject for three languages: Wolof, Ga, and Somali. This paper describes how\ndata was collected and ASR systems developed with a small amount (1h) of\ntranscribed speech as training data. In these low resource conditions,\npre-training a model on large amounts of raw speech was fundamental for the\nefficiency of ASR systems developed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:37:03 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mohamud", "Jama Hussein", ""], ["Thompson", "Lloyd Acquaye", ""], ["Ndoye", "Aissatou", ""], ["Besacier", "Laurent", ""]]}, {"id": "2103.09120", "submitter": "Leonardo F. R. Ribeiro", "authors": "Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych", "title": "Structural Adapters in Pretrained Language Models for AMR-to-text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on text generation from graph-structured data relies on\npretrained language models (PLMs) and utilizes graph linearization heuristics\nrather than explicitly considering the graph structure. Efficiently encoding\nthe graph structure in PLMs is challenging because they were pretrained on\nnatural language, and modeling structured data may lead to catastrophic\nforgetting of distributional knowledge. In this paper, we propose StructAdapt,\nan adapter method to encode graph structure into PLMs. Contrary to prior work,\nStructAdapt effectively models interactions among the nodes based on the graph\nconnectivity, only training graph structure-aware adapter parameters. In this\nway, we avoid catastrophic forgetting while maintaining the topological\nstructure of the graph. We empirically show the benefits of explicitly encoding\ngraph structure into PLMs using adapters and achieve state-of-the-art results\non two AMR-to-text datasets, training only 5.1% of the PLM parameters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:06:50 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ribeiro", "Leonardo F. R.", ""], ["Zhang", "Yue", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2103.09185", "submitter": "Hatem Haddad", "authors": "Aymen Ben Elhaj Mabrouk, Moez Ben Haj Hmida, Chayma Fourati, Hatem\n  Haddad, Abir Messaoudi", "title": "A Multilingual African Embedding for FAQ Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Searching for an available, reliable, official, and understandable\ninformation is not a trivial task due to scattered information across the\ninternet, and the availability lack of governmental communication channels\ncommunicating with African dialects and languages. In this paper, we introduce\nan Artificial Intelligence Powered chatbot for crisis communication that would\nbe omnichannel, multilingual and multi dialectal. We present our work on\nmodified StarSpace embedding tailored for African dialects for the\nquestion-answering task along with the architecture of the proposed chatbot\nsystem and a description of the different layers. English, French, Arabic,\nTunisian, Igbo,Yor\\`ub\\'a, and Hausa are used as languages and dialects.\nQuantitative and qualitative evaluation results are obtained for our real\ndeployed Covid-19 chatbot. Results show that users are satisfied and the\nconversation with the chatbot is meeting customer needs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:36:40 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mabrouk", "Aymen Ben Elhaj", ""], ["Hmida", "Moez Ben Haj", ""], ["Fourati", "Chayma", ""], ["Haddad", "Hatem", ""], ["Messaoudi", "Abir", ""]]}, {"id": "2103.09263", "submitter": "Maximilian Mozes", "authors": "Maximilian Mozes, Bennett Kleinberg", "title": "No Intruder, no Validity: Evaluation Criteria for Privacy-Preserving\n  Text Anonymization", "comments": "pre-print; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For sensitive text data to be shared among NLP researchers and practitioners,\nshared documents need to comply with data protection and privacy laws. There is\nhence a growing interest in automated approaches for text anonymization.\nHowever, measuring such methods' performance is challenging: missing a single\nidentifying attribute can reveal an individual's identity. In this paper, we\ndraw attention to this problem and argue that researchers and practitioners\ndeveloping automated text anonymization systems should carefully assess whether\ntheir evaluation methods truly reflect the system's ability to protect\nindividuals from being re-identified. We then propose TILD, a set of evaluation\ncriteria that comprises an anonymization method's technical performance, the\ninformation loss resulting from its anonymization, and the human ability to\nde-anonymize redacted documents. These criteria may facilitate progress towards\na standardized way for measuring anonymization performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 18:18:29 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Mozes", "Maximilian", ""], ["Kleinberg", "Bennett", ""]]}, {"id": "2103.09325", "submitter": "Alexandros Kastanos", "authors": "Alexandros Kastanos and Tyler Martin", "title": "Graph Convolutional Network for Swahili News Classification", "comments": "8 pages, accepted at EACL AfricaNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work empirically demonstrates the ability of Text Graph Convolutional\nNetwork (Text GCN) to outperform traditional natural language processing\nbenchmarks for the task of semi-supervised Swahili news classification. In\nparticular, we focus our experimentation on the sparsely-labelled\nsemi-supervised context which is representative of the practical constraints\nfacing low-resourced African languages. We follow up on this result by\nintroducing a variant of the Text GCN model which utilises a bag of words\nembedding rather than a naive one-hot encoding to reduce the memory footprint\nof Text GCN whilst demonstrating similar predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 21:03:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Kastanos", "Alexandros", ""], ["Martin", "Tyler", ""]]}, {"id": "2103.09330", "submitter": "Minh Nguyen", "authors": "Minh Van Nguyen, Viet Dac Lai and Thien Huu Nguyen", "title": "Cross-Task Instance Representation Interactions and Label Dependencies\n  for Joint Information Extraction with Graph Convolutional Networks", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Existing works on information extraction (IE) have mainly solved the four\nmain tasks separately (entity mention recognition, relation extraction, event\ntrigger detection, and argument extraction), thus failing to benefit from\ninter-dependencies between tasks. This paper presents a novel deep learning\nmodel to simultaneously solve the four tasks of IE in a single model (called\nFourIE). Compared to few prior work on jointly performing four IE tasks, FourIE\nfeatures two novel contributions to capture inter-dependencies between tasks.\nFirst, at the representation level, we introduce an interaction graph between\ninstances of the four tasks that is used to enrich the prediction\nrepresentation for one instance with those from related instances of other\ntasks. Second, at the label level, we propose a dependency graph for the\ninformation types in the four IE tasks that captures the connections between\nthe types expressed in an input sentence. A new regularization mechanism is\nintroduced to enforce the consistency between the golden and predicted type\ndependency graphs to improve representation learning. We show that the proposed\nmodel achieves the state-of-the-art performance for joint IE on both\nmonolingual and multilingual learning settings with three different languages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 21:23:50 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 01:22:46 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 23:53:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Van Nguyen", "Minh", ""], ["Lai", "Viet Dac", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2103.09474", "submitter": "Keon Lee", "authors": "Keon Lee, Kyumin Park, Daeyoung Kim", "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech\n  Decomposition for Expressive and Controllable Neural Text to Speech", "comments": "5 pages, 2 figures, Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on neural text-to-speech (TTS) have been addressed on limited\nspeed in training and inference time, robustness for difficult synthesis\nconditions, expressiveness, and controllability. Although several approaches\nresolve some limitations, there has been no attempt to solve all weaknesses at\nonce. In this paper, we propose STYLER, an expressive and controllable TTS\nframework with high-speed and robust synthesis. Our novel audio-text aligning\nmethod called Mel Calibrator and excluding autoregressive decoding enable rapid\ntraining and inference and robust synthesis on unseen data. Also, disentangled\nstyle factor modeling under supervision enlarges the controllability in\nsynthesizing process leading to expressive TTS. On top of it, a novel noise\nmodeling pipeline using domain adversarial training and Residual Decoding\nempowers noise-robust style transfer, decomposing the noise without any\nadditional label. Various experiments demonstrate that STYLER is more effective\nin speed and robustness than expressive TTS with autoregressive decoding and\nmore expressive and controllable than reading style non-autoregressive TTS.\nSynthesis samples and experiment results are provided via our demo page, and\ncode is available publicly.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:11:09 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 03:10:05 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 03:47:56 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 01:55:08 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Lee", "Keon", ""], ["Park", "Kyumin", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2103.09499", "submitter": "Hui Li", "authors": "Yanlin Wang, Hui Li", "title": "Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs", "comments": "Accepted in AAAI 2021. This version contains the appendix for the\n  derivation of Eq. 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code completion has become an essential component of integrated development\nenvironments. Contemporary code completion methods rely on the abstract syntax\ntree (AST) to generate syntactically correct code. However, they cannot fully\ncapture the sequential and repetitive patterns of writing code and the\nstructural information of the AST. To alleviate these problems, we propose a\nnew code completion approach named CCAG, which models the flattened sequence of\na partial AST as an AST graph. CCAG uses our proposed AST Graph Attention Block\nto capture different dependencies in the AST graph for representation learning\nin code completion. The sub-tasks of code completion are optimized via\nmulti-task learning in CCAG, and the task balance is automatically achieved\nusing uncertainty without the need to tune task weights. The experimental\nresults show that CCAG has superior performance than state-of-the-art\napproaches and it is able to provide intelligent code completion.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:11:09 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Yanlin", ""], ["Li", "Hui", ""]]}, {"id": "2103.09519", "submitter": "Thin Dang Van", "authors": "Dang Van Thin, Lac Si Le, Vu Xuan Hoang, Ngan Luu-Thuy Nguyen", "title": "Investigating Monolingual and Multilingual BERTModels for Vietnamese\n  Aspect Category Detection", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Aspect category detection (ACD) is one of the challenging tasks in the\nAspect-based sentiment Analysis problem. The purpose of this task is to\nidentify the aspect categories mentioned in user-generated reviews from a set\nof pre-defined categories. In this paper, we investigate the performance of\nvarious monolingual pre-trained language models compared with multilingual\nmodels on the Vietnamese aspect category detection problem. We conduct the\nexperiments on two benchmark datasets for the restaurant and hotel domain. The\nexperimental results demonstrated the effectiveness of the monolingual PhoBERT\nmodel than others on two datasets. We also evaluate the performance of the\nmultilingual model based on the combination of whole SemEval-2016 datasets in\nother languages with the Vietnamese dataset. To the best of our knowledge, our\nresearch study is the first attempt at performing various available pre-trained\nlanguage models on aspect category detection task and utilize the datasets from\nother languages based on multilingual models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:04:03 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Van Thin", "Dang", ""], ["Le", "Lac Si", ""], ["Hoang", "Vu Xuan", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2103.09534", "submitter": "Juntao Li", "authors": "Juntao Li, Chang Liu, Chongyang Tao, Zhangming Chan, Dongyan Zhao, Min\n  Zhang, Rui Yan", "title": "Dialogue History Matters! Personalized Response Selectionin Multi-turn\n  Retrieval-based Chatbots", "comments": "Accepted by ACM Transactions on Information Systems, 25 pages, 2\n  figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-turn context-response matching methods mainly concentrate on\nobtaining multi-level and multi-dimension representations and better\ninteractions between context utterances and response. However, in real-place\nconversation scenarios, whether a response candidate is suitable not only\ncounts on the given dialogue context but also other backgrounds, e.g., wording\nhabits, user-specific dialogue history content. To fill the gap between these\nup-to-date methods and the real-world applications, we incorporate\nuser-specific dialogue history into the response selection and propose a\npersonalized hybrid matching network (PHMN). Our contributions are two-fold: 1)\nour model extracts personalized wording behaviors from user-specific dialogue\nhistory as extra matching information; 2) we perform hybrid representation\nlearning on context-response utterances and explicitly incorporate a customized\nattention mechanism to extract vital information from context-response\ninteractions so as to improve the accuracy of matching. We evaluate our model\non two large datasets with user identification, i.e., personalized Ubuntu\ndialogue Corpus (P-Ubuntu) and personalized Weibo dataset (P-Weibo).\nExperimental results confirm that our method significantly outperforms several\nstrong models by combining personalized attention, wording behaviors, and\nhybrid representation learning.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:42:11 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Li", "Juntao", ""], ["Liu", "Chang", ""], ["Tao", "Chongyang", ""], ["Chan", "Zhangming", ""], ["Zhao", "Dongyan", ""], ["Zhang", "Min", ""], ["Yan", "Rui", ""]]}, {"id": "2103.09535", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Yejin Bang, Andrea Madotto, Madian Khabsa, Pascale Fung", "title": "Towards Few-Shot Fact-Checking via Perplexity", "comments": "Accpeted to NAACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning has drawn researchers' attention to overcome the problem of\ndata scarcity. Recently, large pre-trained language models have shown great\nperformance in few-shot learning for various downstream tasks, such as question\nanswering and machine translation. Nevertheless, little exploration has been\nmade to achieve few-shot learning for the fact-checking task. However,\nfact-checking is an important problem, especially when the amount of\ninformation online is growing exponentially every day. In this paper, we\npropose a new way of utilizing the powerful transfer learning ability of a\nlanguage model via a perplexity score. The most notable strength of our\nmethodology lies in its capability in few-shot learning. With only two training\nsamples, our methodology can already outperform the Major Class baseline by\nmore than absolute 10% on the F1-Macro metric across multiple datasets. Through\nexperiments, we empirically verify the plausibility of the rather surprising\nusage of the perplexity score in the context of fact-checking and highlight the\nstrength of our few-shot methodology by comparing it to strong\nfine-tuning-based baseline models. Moreover, we construct and publicly release\ntwo new fact-checking datasets related to COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:43:19 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Lee", "Nayeon", ""], ["Bang", "Yejin", ""], ["Madotto", "Andrea", ""], ["Khabsa", "Madian", ""], ["Fung", "Pascale", ""]]}, {"id": "2103.09548", "submitter": "Yang-Yin Lee", "authors": "Lee-Hsun Hsieh and Yang-Yin Lee and Ee-Peng Lim", "title": "ENCONTER: Entity Constrained Progressive Sequence Generation via\n  Insertion-based Transformer", "comments": "EACL 2021 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained using large amount of data, autoregressive language models are\nable to generate high quality sequences. However, these models do not perform\nwell under hard lexical constraints as they lack fine control of content\ngeneration process. Progressive insertion-based transformers can overcome the\nabove limitation and efficiently generate a sequence in parallel given some\ninput tokens as constraint. These transformers however may fail to support hard\nlexical constraints as their generation process is more likely to terminate\nprematurely. The paper analyses such early termination problems and proposes\nthe Entity-constrained insertion transformer (ENCONTER), a new insertion\ntransformer that addresses the above pitfall without compromising much\ngeneration efficiency. We introduce a new training strategy that considers\npredefined hard lexical constraints (e.g., entities to be included in the\ngenerated sequence). Our experiments show that ENCONTER outperforms other\nbaseline models in several performance metrics rendering it more suitable in\npractical applications. Our code is available at\nhttps://github.com/LARC-CMU-SMU/Enconter\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:24:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hsieh", "Lee-Hsun", ""], ["Lee", "Yang-Yin", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2103.09567", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen", "title": "Endangered Languages are not Low-Resourced!", "comments": "In Multilingual Facilitation (2021)", "journal-ref": null, "doi": "10.31885/9789515150257.1", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The term low-resourced has been tossed around in the field of natural\nlanguage processing to a degree that almost any language that is not English\ncan be called \"low-resourced\"; sometimes even just for the sake of making a\nmundane or mediocre paper appear more interesting and insightful. In a field\nwhere English is a synonym for language and low-resourced is a synonym for\nanything not English, calling endangered languages low-resourced is a bit of an\noverstatement. In this paper, I inspect the relation of the endangered with the\nlow-resourced from my own experiences.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:05:29 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""]]}, {"id": "2103.09591", "submitter": "Yonatan Bitton", "authors": "Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz, Michael Elhadad", "title": "Automatic Generation of Contrast Sets from Scene Graphs: Probing the\n  Compositional Consistency of GQA", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that supervised models often exploit data artifacts\nto achieve good test scores while their performance severely degrades on\nsamples outside their training distribution. Contrast sets (Gardneret al.,\n2020) quantify this phenomenon by perturbing test samples in a minimal way such\nthat the output label is modified. While most contrast sets were created\nmanually, requiring intensive annotation effort, we present a novel method\nwhich leverages rich semantic input representation to automatically generate\ncontrast sets for the visual question answering task. Our method computes the\nanswer of perturbed questions, thus vastly reducing annotation cost and\nenabling thorough evaluation of models' performance on various semantic aspects\n(e.g., spatial or relational reasoning). We demonstrate the effectiveness of\nour approach on the GQA dataset and its semantic scene graph image\nrepresentation. We find that, despite GQA's compositionality and carefully\nbalanced label distribution, two high-performing models drop 13-17% in accuracy\ncompared to the original test set. Finally, we show that our automatic\nperturbation can be applied to the training set to mitigate the degradation in\nperformance, opening the door to more robust models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:19:25 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bitton", "Yonatan", ""], ["Stanovsky", "Gabriel", ""], ["Schwartz", "Roy", ""], ["Elhadad", "Michael", ""]]}, {"id": "2103.09593", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty", "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots", "comments": "To be presented at NAACL-HLT 2021. Abstract also published in the\n  Rising Stars Track of the Workshop on Computational Approaches to Linguistic\n  Code-Switching (CALCS 2021)", "journal-ref": "2021.naacl-main.282", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual models have demonstrated impressive cross-lingual transfer\nperformance. However, test sets like XNLI are monolingual at the example level.\nIn multilingual communities, it is common for polyglots to code-mix when\nconversing with each other. Inspired by this phenomenon, we present two strong\nblack-box adversarial attacks (one word-level, one phrase-level) for\nmultilingual models that push their ability to handle code-mixed sentences to\nthe limit. The former uses bilingual dictionaries to propose perturbations and\ntranslations of the clean example for sense disambiguation. The latter directly\naligns the clean example with its translations before extracting phrases as\nperturbations. Our phrase-level attack has a success rate of 89.75% against\nXLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.\nFinally, we propose an efficient adversarial training scheme that trains in the\nsame number of steps as the original model and show that it improves model\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:20:53 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:30:27 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 02:02:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""]]}, {"id": "2103.09602", "submitter": "Gullal Singh Cheema", "authors": "Gullal S. Cheema and Sherzod Hakimov and Eric M\\\"uller-Budack and\n  Ralph Ewerth", "title": "On the Role of Images for Analyzing Claims in Social Media", "comments": "CLEOPATRA-2021 Workshop co-located with The Web Conf 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news is a severe problem in social media. In this paper, we present an\nempirical study on visual, textual, and multimodal models for the tasks of\nclaim, claim check-worthiness, and conspiracy detection, all of which are\nrelated to fake news detection. Recent work suggests that images are more\ninfluential than text and often appear alongside fake text. To this end,\nseveral multimodal models have been proposed in recent years that use images\nalong with text to detect fake news on social media sites like Twitter.\nHowever, the role of images is not well understood for claim detection,\nspecifically using transformer-based textual and multimodal models. We\ninvestigate state-of-the-art models for images, text (Transformer-based), and\nmultimodal information for four different datasets across two languages to\nunderstand the role of images in the task of claim and conspiracy detection.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:40:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Cheema", "Gullal S.", ""], ["Hakimov", "Sherzod", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2103.09606", "submitter": "Youri Van Der Zee", "authors": "Youri van der Zee, Jan C. Scholtes, Marcel Westerhoud, Julien Rossi", "title": "Code Word Detection in Fraud Investigations using a Deep-Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In modern litigation, fraud investigators often face an overwhelming number\nof documents that must be reviewed throughout a matter. In the majority of\nlegal cases, fraud investigators do not know beforehand, exactly what they are\nlooking for, nor where to find it. In addition, fraudsters may use deception to\nhide their behaviour and intentions by using code words. Effectively, this\nmeans fraud investigators are looking for a needle in the haystack without\nknowing what the needle looks like.\n  As part of a larger research program, we use a framework to expedite the\ninvestigation process applying text-mining and machine learning techniques. We\nstructure this framework using three well-known methods in fraud\ninvestigations: (i) the fraud triangle (ii) the golden (\"W\") investigation\nquestions, and (iii) the analysis of competing hypotheses. With this framework,\nit is possible to automatically organize investigative data, so it is easier\nfor investigators to find answers to typical investigative questions.\n  In this research, we focus on one of the components of this framework: the\nidentification of the usage of code words by fraudsters. Here for, a novel\n(annotated) synthetic data set is created containing such code words, hidden in\nnormal email communication. Subsequently, a range of machine learning\ntechniques are employed to detect such code words. We show that the\nstate-of-the-art BERT model significantly outperforms other methods on this\ntask. With this result, we demonstrate that deep neural language models can\nreliably (F1 score of 0.9) be applied in fraud investigations for the detection\nof code words.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:49:55 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["van der Zee", "Youri", ""], ["Scholtes", "Jan C.", ""], ["Westerhoud", "Marcel", ""], ["Rossi", "Julien", ""]]}, {"id": "2103.09635", "submitter": "Javier Huertas-Tato", "authors": "Javier Huertas-Tato and Alejandro Mart\\'in and David Camacho", "title": "SILT: Efficient transformer training for inter-lingual inference", "comments": "This research is funded by the project CIVIC: Intelligent\n  characterisation of the veracity of the information related to COVID-19,\n  granted by BBVA FOUNDATION GRANTS FOR SCIENTIFIC RESEARCH TEAMS SARS-CoV-2\n  and COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of transformers to perform precision tasks such as question\nanswering, Natural Language Inference (NLI) or summarising, have enabled them\nto be ranked as one of the best paradigm to address Natural Language Processing\n(NLP) tasks. NLI is one of the best scenarios to test these architectures, due\nto the knowledge required to understand complex sentences and established\nrelationships between a hypothesis and a premise. Nevertheless, these models\nsuffer from incapacity to generalise to other domains or difficulties to face\nmultilingual and interlingual scenarios. The leading pathway in the literature\nto address these issues involve designing and training extremely large\narchitectures, which leads to unpredictable behaviours and to establish\nbarriers which impede broad access and fine tuning. In this paper, we propose a\nnew architecture called Siamese Inter-Lingual Transformer (SILT), to\nefficiently align multilingual embeddings for Natural Language Inference,\nallowing for unmatched language pairs to be processed. SILT leverages siamese\npre-trained multi-lingual transformers with frozen weights where the two input\nsentences attend each other to later be combined through a matrix alignment\nmethod. The experimental results carried out in this paper evidence that SILT\nallows to reduce drastically the number of trainable parameters while allowing\nfor inter-lingual NLI and achieving state-of-the-art performance on common\nbenchmarks.\n  We make our code and dataset available at\nhttps://github.com/jahuerta92/siamese-inter-lingual-transformer.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:23:53 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 19:28:55 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 08:22:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Huertas-Tato", "Javier", ""], ["Mart\u00edn", "Alejandro", ""], ["Camacho", "David", ""]]}, {"id": "2103.09645", "submitter": "Akbar Karimi", "authors": "Akbar Karimi, Leonardo Rossi, Andrea Prati", "title": "UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using\n  CharacterBERT and Bag-of-Words Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ever-increasing availability of digital information, toxic content\nis also on the rise. Therefore, the detection of this type of language is of\nparamount importance. We tackle this problem utilizing a combination of a\nstate-of-the-art pre-trained language model (CharacterBERT) and a traditional\nbag-of-words technique. Since the content is full of toxic words that have not\nbeen written according to their dictionary spelling, attendance to individual\ncharacters is crucial. Therefore, we use CharacterBERT to extract features\nbased on the word characters. It consists of a CharacterCNN module that learns\ncharacter embeddings from the context. These are, then, fed into the well-known\nBERT architecture. The bag-of-words method, on the other hand, further improves\nupon that by making sure that some frequently used toxic words get labeled\naccordingly. With a 4 percent difference from the first team, our system ranked\n36th in the competition. The code is available for further re-search and\nreproduction of the results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:39:49 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 14:40:51 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Karimi", "Akbar", ""], ["Rossi", "Leonardo", ""], ["Prati", "Andrea", ""]]}, {"id": "2103.09666", "submitter": "Wenliang Dai", "authors": "Wenliang Dai, Samuel Cahyawijaya, Zihan Liu, Pascale Fung", "title": "Multimodal End-to-End Sparse Model for Emotion Recognition", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing works on multimodal affective computing tasks, such as emotion\nrecognition, generally adopt a two-phase pipeline, first extracting feature\nrepresentations for each single modality with hand-crafted algorithms and then\nperforming end-to-end learning with the extracted features. However, the\nextracted features are fixed and cannot be further fine-tuned on different\ntarget tasks, and manually finding feature extraction algorithms does not\ngeneralize or scale well to different tasks, which can lead to sub-optimal\nperformance. In this paper, we develop a fully end-to-end model that connects\nthe two phases and optimizes them jointly. In addition, we restructure the\ncurrent datasets to enable the fully end-to-end training. Furthermore, to\nreduce the computational overhead brought by the end-to-end model, we introduce\na sparse cross-modal attention mechanism for the feature extraction.\nExperimental results show that our fully end-to-end model significantly\nsurpasses the current state-of-the-art models based on the two-phase pipeline.\nMoreover, by adding the sparse cross-modal attention, our model can maintain\nperformance with around half the computation in the feature extraction part.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:05:05 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 16:21:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dai", "Wenliang", ""], ["Cahyawijaya", "Samuel", ""], ["Liu", "Zihan", ""], ["Fung", "Pascale", ""]]}, {"id": "2103.09687", "submitter": "Aissam Outchakoucht", "authors": "Aissam Outchakoucht, Hamza Es-Samaali", "title": "Moroccan Dialect -Darija- Open Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Darija Open Dataset (DODa) is an open-source project for the Moroccan\ndialect. With more than 10,000 entries DODa is arguably the largest open-source\ncollaborative project for Darija-English translation built for Natural Language\nProcessing purposes. In fact, besides semantic categorization, DODa also adopts\na syntactic one, presents words under different spellings, offers verb-to-noun\nand masculine-to-feminine correspondences, contains the conjugation of hundreds\nof verbs in different tenses, and many other subsets to help researchers better\nunderstand and study Moroccan dialect. This data paper presents a description\nof DODa, its features, how it was collected, as well as a first application in\nImage Classification using ImageNet labels translated to Darija. This\ncollaborative project is hosted on GitHub platform under MIT's Open-Source\nlicense and aims to be a standard resource for researchers, students, and\nanyone who is interested in Moroccan Dialect\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:37:59 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Outchakoucht", "Aissam", ""], ["Es-Samaali", "Hamza", ""]]}, {"id": "2103.09710", "submitter": "Anya Belz", "authors": "Anastasia Shimorina and Anya Belz", "title": "The Human Evaluation Datasheet 1.0: A Template for Recording Details of\n  Human Evaluation Experiments in NLP", "comments": "Unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the Human Evaluation Datasheet, a template for\nrecording the details of individual human evaluation experiments in Natural\nLanguage Processing (NLP). Originally taking inspiration from seminal papers by\nBender and Friedman (2018), Mitchell et al. (2019), and Gebru et al. (2020),\nthe Human Evaluation Datasheet is intended to facilitate the recording of\nproperties of human evaluations in sufficient detail, and with sufficient\nstandardisation, to support comparability, meta-evaluation, and reproducibility\ntests.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:08:50 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Shimorina", "Anastasia", ""], ["Belz", "Anya", ""]]}, {"id": "2103.09857", "submitter": "Ankit Gupta", "authors": "Ankit Gupta, Jonathan Berant", "title": "Value-aware Approximate Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the success of dot-product attention in Transformers, numerous\napproximations have been recently proposed to address its quadratic complexity\nwith respect to the input length. However, all approximations thus far have\nignored the contribution of the $\\textit{value vectors}$ to the quality of\napproximation. In this work, we argue that research efforts should be directed\ntowards approximating the true output of the attention sub-layer, which\nincludes the value vectors. We propose a value-aware objective, and show\ntheoretically and empirically that an optimal approximation of a value-aware\nobjective substantially outperforms an optimal approximation that ignores\nvalues, in the context of language modeling. Moreover, we show that the choice\nof kernel function for computing attention similarity can substantially affect\nthe quality of sparse approximations, where kernel functions that are less\nskewed are more affected by the value vectors.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:43:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gupta", "Ankit", ""], ["Berant", "Jonathan", ""]]}, {"id": "2103.09926", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Tanja S\\\"aily, Eetu M\\\"akel\\\"a, Mika H\\\"am\\\"al\\\"ainen", "title": "From Plenipotentiary to Puddingless: Users and Uses of New Words in\n  Early English Letters", "comments": "In Multilingual Facilitation (2021)", "journal-ref": null, "doi": "10.31885/9789515150257.15", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study neologism use in two samples of early English correspondence, from\n1640--1660 and 1760--1780. Of especial interest are the early adopters of new\nvocabulary, the social groups they represent, and the types and functions of\ntheir neologisms. We describe our computer-assisted approach and note the\ndifficulties associated with massive variation in the corpus. Our findings\ninclude that while male letter-writers tend to use neologisms more frequently\nthan women, the eighteenth century seems to have provided more opportunities\nfor women and the lower ranks to participate in neologism use as well. In both\nsamples, neologisms most frequently occur in letters written between close\nfriends, which could be due to this less stable relationship triggering more\ncreative language use. In the seventeenth-century sample, we observe the\ninfluence of the English Civil War, while the eighteenth-century sample appears\nto reflect the changing functions of letter-writing, as correspondence is\nincreasingly being used as a tool for building and maintaining social\nrelationships in addition to exchanging information.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 21:45:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["S\u00e4ily", "Tanja", ""], ["M\u00e4kel\u00e4", "Eetu", ""], ["H\u00e4m\u00e4l\u00e4inen", "Mika", ""]]}, {"id": "2103.09935", "submitter": "George Saon", "authors": "George Saon, Zoltan Tueske, Daniel Bolanos and Brian Kingsbury", "title": "Advancing RNN Transducer Technology for Speech Recognition", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a set of techniques for RNN Transducers (RNN-Ts) that were\ninstrumental in lowering the word error rate on three different tasks\n(Switchboard 300 hours, conversational Spanish 780 hours and conversational\nItalian 900 hours). The techniques pertain to architectural changes, speaker\nadaptation, language model fusion, model combination and general training\nrecipe. First, we introduce a novel multiplicative integration of the encoder\nand prediction network vectors in the joint network (as opposed to additive).\nSecond, we discuss the applicability of i-vector speaker adaptation to RNN-Ts\nin conjunction with data perturbation. Third, we explore the effectiveness of\nthe recently proposed density ratio language model fusion for these tasks. Last\nbut not least, we describe the other components of our training recipe and\ntheir effect on recognition performance. We report a 5.9% and 12.5% word error\nrate on the Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation\nand a 12.7% WER on the Mozilla CommonVoice Italian test set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 22:19:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Saon", "George", ""], ["Tueske", "Zoltan", ""], ["Bolanos", "Daniel", ""], ["Kingsbury", "Brian", ""]]}, {"id": "2103.09977", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu and Mark O. Riedl", "title": "Situated Language Learning via Interactive Narratives", "comments": "Preprint. Under journal review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a roadmap that explores the question of how to imbue\nlearning agents with the ability to understand and generate contextually\nrelevant natural language in service of achieving a goal. We hypothesize that\ntwo key components in creating such agents are interactivity and environment\ngrounding, shown to be vital parts of language learning in humans, and posit\nthat interactive narratives should be the environments of choice for such\ntraining these agents. These games are simulations in which an agent interacts\nwith the world through natural language -- \"perceiving\", \"acting upon\", and\n\"talking to\" the world using textual descriptions, commands, and dialogue --\nand as such exist at the intersection of natural language processing,\nstorytelling, and sequential decision making. We discuss the unique challenges\na text games' puzzle-like structure combined with natural language\nstate-and-action spaces provides: knowledge representation, commonsense\nreasoning, and exploration. Beyond the challenges described so far, progress in\nthe realm of interactive narratives can be applied in adjacent problem domains.\nThese applications provide interesting challenges of their own as well as\nextensions to those discussed so far. We describe three of them in detail: (1)\nevaluating AI system's commonsense understanding by automatically creating\ninteractive narratives; (2) adapting abstract text-based policies to include\nother modalities such as vision; and (3) enabling multi-agent and human-AI\ncollaboration in shared, situated worlds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 01:55:16 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2103.10013", "submitter": "Xuanli He", "authors": "Xuanli He and Lingjuan Lyu and Qiongkai Xu and Lichao Sun", "title": "Model Extraction and Adversarial Transferability, Your BERT is\n  Vulnerable!", "comments": "accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) tasks, ranging from text classification to\ntext generation, have been revolutionised by the pre-trained language models,\nsuch as BERT. This allows corporations to easily build powerful APIs by\nencapsulating fine-tuned BERT models for downstream tasks. However, when a\nfine-tuned BERT model is deployed as a service, it may suffer from different\nattacks launched by malicious users. In this work, we first present how an\nadversary can steal a BERT-based API service (the victim/target model) on\nmultiple benchmark datasets with limited prior knowledge and queries. We\nfurther show that the extracted model can lead to highly transferable\nadversarial attacks against the victim model. Our studies indicate that the\npotential vulnerabilities of BERT-based API services still hold, even when\nthere is an architectural mismatch between the victim model and the attack\nmodel. Finally, we investigate two defence strategies to protect the victim\nmodel and find that unless the performance of the victim model is sacrificed,\nboth model ex-traction and adversarial transferability can effectively\ncompromise the target models\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 04:23:21 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["He", "Xuanli", ""], ["Lyu", "Lingjuan", ""], ["Xu", "Qiongkai", ""], ["Sun", "Lichao", ""]]}, {"id": "2103.10069", "submitter": "Luan Thanh Nguyen", "authors": "Luan Thanh Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen", "title": "Constructive and Toxic Speech Detection for Open-domain Social Media\n  Comments in Vietnamese", "comments": "Accepted as a FULL PAPER for The 34th International Conference on\n  Industrial, Engineering & Other Applications of Applied Intelligent Systems\n  (IEA/AIE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The rise of social media has led to the increasing of comments on online\nforums. However, there still exists invalid comments which are not informative\nfor users. Moreover, those comments are also quite toxic and harmful to people.\nIn this paper, we create a dataset for constructive and toxic speech detection,\nnamed UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)\nwith 10,000 human-annotated comments. For these tasks, we propose a system for\nconstructive and toxic speech detection with the state-of-the-art transfer\nlearning model in Vietnamese NLP as PhoBERT. With this system, we obtain\nF1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,\nrespectively. Besides, we implement various baseline models as traditional\nMachine Learning and Deep Neural Network-Based models to evaluate the dataset.\nWith the results, we can solve several tasks on the online discussions and\ndevelop the framework for identifying constructiveness and toxicity of\nVietnamese social media comments automatically.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:04:12 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:55:12 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 14:49:19 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 14:50:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nguyen", "Luan Thanh", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2103.10121", "submitter": "Dmytro Kalpakchi", "authors": "Dmytro Kalpakchi and Johan Boye", "title": "Quinductor: a multilingual data-driven method for generating\n  reading-comprehension questions using Universal Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a multilingual data-driven method for generating reading\ncomprehension questions using dependency trees. Our method provides a strong,\nmostly deterministic, and inexpensive-to-train baseline for less-resourced\nlanguages. While a language-specific corpus is still required, its size is\nnowhere near those required by modern neural question generation (QG)\narchitectures. Our method surpasses QG baselines previously reported in the\nliterature and shows a good performance in terms of human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:49:56 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 10:02:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kalpakchi", "Dmytro", ""], ["Boye", "Johan", ""]]}, {"id": "2103.10133", "submitter": "Aili Shen", "authors": "Aili Shen, Meladel Mistica, Bahar Salehi, Hang Li, Timothy Baldwin,\n  and Jianzhong Qi", "title": "Evaluating Document Coherence Modelling", "comments": "accepted to TACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pretrained language models (\"LM\") have driven impressive gains over\nmorpho-syntactic and semantic tasks, their ability to model discourse and\npragmatic phenomena is less clear. As a step towards a better understanding of\ntheir discourse modelling capabilities, we propose a sentence intrusion\ndetection task. We examine the performance of a broad range of pretrained LMs\non this detection task for English. Lacking a dataset for the task, we\nintroduce INSteD, a novel intruder sentence detection dataset, containing\n170,000+ documents constructed from English Wikipedia and CNN news articles.\nOur experiments show that pretrained LMs perform impressively in in-domain\nevaluation, but experience a substantial drop in the cross-domain setting,\nindicating limited generalisation capacity. Further results over a novel\nlinguistic probe dataset show that there is substantial room for improvement,\nespecially in the cross-domain setting.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:05:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Shen", "Aili", ""], ["Mistica", "Meladel", ""], ["Salehi", "Bahar", ""], ["Li", "Hang", ""], ["Baldwin", "Timothy", ""], ["Qi", "Jianzhong", ""]]}, {"id": "2103.10195", "submitter": "Hala Mulki", "authors": "Hala Mulki, Bilal Ghanem", "title": "Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language", "comments": "10 pages, 2 figures, WANLP 2021 co-located with EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online misogyny has become an increasing worry for Arab women who experience\ngender-based online abuse on a daily basis. Misogyny automatic detection\nsystems can assist in the prohibition of anti-women Arabic toxic content.\nDeveloping such systems is hindered by the lack of the Arabic misogyny\nbenchmark datasets. In this paper, we introduce an Arabic Levantine Twitter\ndataset for Misogynistic language (LeT-Mi) to be the first benchmark dataset\nfor Arabic misogyny. We further provide a detailed review of the dataset\ncreation and annotation phases. The consistency of the annotations for the\nproposed dataset was emphasized through inter-rater agreement evaluation\nmeasures. Moreover, Let-Mi was used as an evaluation dataset through\nbinary/multi-/target classification tasks conducted by several state-of-the-art\nmachine learning systems along with Multi-Task Learning (MTL) configuration.\nThe obtained results indicated that the performances achieved by the used\nsystems are consistent with state-of-the-art results for languages other than\nArabic, while employing MTL improved the performance of the misogyny/target\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:01:13 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Mulki", "Hala", ""], ["Ghanem", "Bilal", ""]]}, {"id": "2103.10198", "submitter": "Gerhard J\\\"ager", "authors": "Gerhard J\\\"ager and Johannes Wahle", "title": "Phylogenetic typology", "comments": "32 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CL q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we propose a novel method to estimate the frequency\ndistribution of linguistic variables while controlling for statistical\nnon-independence due to shared ancestry. Unlike previous approaches, our\ntechnique uses all available data, from language families large and small as\nwell as from isolates, while controlling for different degrees of relatedness\non a continuous scale estimated from the data. Our approach involves three\nsteps: First, distributions of phylogenies are inferred from lexical data.\nSecond, these phylogenies are used as part of a statistical model to\nstatistically estimate transition rates between parameter states. Finally, the\nlong-term equilibrium of the resulting Markov process is computed. As a case\nstudy, we investigate a series of potential word-order correlations across the\nlanguages of the world.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:03:49 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:41:32 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["J\u00e4ger", "Gerhard", ""], ["Wahle", "Johannes", ""]]}, {"id": "2103.10282", "submitter": "Paul Michel", "authors": "Paul Michel, Tatsunori Hashimoto, Graham Neubig", "title": "Modeling the Second Player in Distributionally Robust Optimization", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributionally robust optimization (DRO) provides a framework for training\nmachine learning models that are able to perform well on a collection of\nrelated data distributions (the \"uncertainty set\"). This is done by solving a\nmin-max game: the model is trained to minimize its maximum expected loss among\nall distributions in the uncertainty set. While careful design of the\nuncertainty set is critical to the success of the DRO procedure, previous work\nhas been limited to relatively simple alternatives that keep the min-max\noptimization problem exactly tractable, such as $f$-divergence balls. In this\npaper, we argue instead for the use of neural generative models to characterize\nthe worst-case distribution, allowing for more flexible and problem-specific\nselection of the uncertainty set. However, while simple conceptually, this\napproach poses a number of implementation and optimization challenges. To\ncircumvent these issues, we propose a relaxation of the KL-constrained inner\nmaximization objective that makes the DRO problem more amenable to\ngradient-based optimization of large scale generative models, and develop model\nselection heuristics to guide hyper-parameter search. On both toy settings and\nrealistic NLP tasks, we find that the proposed approach yields models that are\nmore robust than comparable baselines.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:26:26 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 08:14:09 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Michel", "Paul", ""], ["Hashimoto", "Tatsunori", ""], ["Neubig", "Graham", ""]]}, {"id": "2103.10291", "submitter": "Ben Peters", "authors": "Ben Peters and Andr\\'e F. T. Martins", "title": "Smoothing and Shrinking the Sparse Seq2Seq Search Space", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current sequence-to-sequence models are trained to minimize cross-entropy and\nuse softmax to compute the locally normalized probabilities over target\nsequences. While this setup has led to strong results in a variety of tasks,\none unsatisfying aspect is its length bias: models give high scores to short,\ninadequate hypotheses and often make the empty string the argmax -- the\nso-called cat got your tongue problem. Recently proposed entmax-based sparse\nsequence-to-sequence models present a possible solution, since they can shrink\nthe search space by assigning zero probability to bad hypotheses, but their\nability to handle word-level tasks with transformers has never been tested. In\nthis work, we show that entmax-based models effectively solve the cat got your\ntongue problem, removing a major source of model error for neural machine\ntranslation. In addition, we generalize label smoothing, a critical\nregularization technique, to the broader family of Fenchel-Young losses, which\nincludes both cross-entropy and the entmax losses. Our resulting label-smoothed\nentmax loss models set a new state of the art on multilingual\ngrapheme-to-phoneme conversion and deliver improvements and better calibration\nproperties on cross-lingual morphological inflection and machine translation\nfor 6 language pairs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:45:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Peters", "Ben", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2103.10325", "submitter": "Ashish Shenoy", "authors": "Ashish Shenoy, Sravan Bodapati, Katrin Kirchhoff", "title": "Contextual Biasing of Language Models for Speech Recognition in\n  Goal-Oriented Conversational Agents", "comments": "Updated version with extensions are uploaded here arXiv:2104.11070", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Goal-oriented conversational interfaces are designed to accomplish specific\ntasks and typically have interactions that tend to span multiple turns adhering\nto a pre-defined structure and a goal. However, conventional neural language\nmodels (NLM) in Automatic Speech Recognition (ASR) systems are mostly trained\nsentence-wise with limited context. In this paper, we explore different ways to\nincorporate context into a LSTM based NLM in order to model long range\ndependencies and improve speech recognition. Specifically, we use context carry\nover across multiple turns and use lexical contextual cues such as system\ndialog act from Natural Language Understanding (NLU) models and the user\nprovided structure of the chatbot. We also propose a new architecture that\nutilizes context embeddings derived from BERT on sample utterances provided\nduring inference time. Our experiments show a word error rate (WER) relative\nreduction of 7% over non-contextual utterance-level NLM rescorers on\ngoal-oriented audio datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:38:08 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 00:44:17 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 22:04:26 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shenoy", "Ashish", ""], ["Bodapati", "Sravan", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "2103.10360", "submitter": "Zhengxiao Du", "authors": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin\n  Yang, Jie Tang", "title": "All NLP Tasks Are Generation Tasks: A General Pretraining Framework", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been various types of pretraining architectures including\nautoregressive models (e.g., GPT), autoencoding models (e.g., BERT), and\nencoder-decoder models (e.g., T5). On the other hand, NLP tasks are different\nin nature, with three main categories being classification, unconditional\ngeneration, and conditional generation. However, none of the pretraining\nframeworks performs the best for all tasks, which introduces inconvenience for\nmodel development and selection. We propose a novel pretraining framework GLM\n(General Language Model) to address this challenge. Compared to previous work,\nour architecture has three major benefits: (1) it performs well on\nclassification, unconditional generation, and conditional generation tasks with\none single pretrained model; (2) it outperforms BERT-like models on\nclassification due to improved pretrain-finetune consistency; (3) it naturally\nhandles variable-length blank filling which is crucial for many downstream\ntasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural\nlanguage understanding benchmark with the same amount of pre-training data.\nMoreover, GLM with 1.25x parameters of BERT-Large achieves the best performance\nin NLU, conditional and unconditional generation at the same time, which\ndemonstrates its generalizability to different downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:30:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Du", "Zhengxiao", ""], ["Qian", "Yujie", ""], ["Liu", "Xiao", ""], ["Ding", "Ming", ""], ["Qiu", "Jiezhong", ""], ["Yang", "Zhilin", ""], ["Tang", "Jie", ""]]}, {"id": "2103.10385", "submitter": "Xiao Liu", "authors": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin\n  Yang, Jie Tang", "title": "GPT Understands, Too", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While GPTs with traditional fine-tuning fail to achieve strong results on\nnatural language understanding (NLU), we show that GPTs can be better than or\ncomparable to similar-sized BERTs on NLU tasks with a novel method P-tuning --\nwhich employs trainable continuous prompt embeddings. On the knowledge probing\n(LAMA) benchmark, the best GPT recovers 64\\% (P@1) of world knowledge without\nany additional text provided during test time, which substantially improves the\nprevious best by 20+ percentage points. On the SuperGlue benchmark, GPTs\nachieve comparable and sometimes better performance to similar-sized BERTs in\nsupervised learning. Importantly, we find that P-tuning also improves BERTs'\nperformance in both few-shot and supervised settings while largely reducing the\nneed for prompt engineering. Consequently, P-tuning outperforms the\nstate-of-the-art approaches on the few-shot SuperGlue benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:13:50 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Xiao", ""], ["Zheng", "Yanan", ""], ["Du", "Zhengxiao", ""], ["Ding", "Ming", ""], ["Qian", "Yujie", ""], ["Yang", "Zhilin", ""], ["Tang", "Jie", ""]]}, {"id": "2103.10387", "submitter": "William Gantt", "authors": "William Gantt, Lelia Glass, and Aaron Steven White", "title": "Decomposing and Recomposing Event Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an event structure ontology empirically derived from inferential\nproperties annotated on sentence- and document-level semantic graphs. We induce\nthis ontology jointly with semantic role, entity type, and event-event relation\nontologies using a document-level generative model, identifying sets of types\nthat align closely with previous theoretically-motivated taxonomies.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:16:43 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gantt", "William", ""], ["Glass", "Lelia", ""], ["White", "Aaron Steven", ""]]}, {"id": "2103.10415", "submitter": "Qinyuan Ye", "authors": "Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, Xiang Ren", "title": "Refining Language Models with Compositional Explanations", "comments": "Code: https://github.com/INK-USC/expl-refinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior works\nreveal such spurious patterns via post-hoc explanation algorithms which compute\nthe importance of input features. Further, the model is regularized to align\nthe importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:48:54 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 07:06:15 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yao", "Huihan", ""], ["Chen", "Ying", ""], ["Ye", "Qinyuan", ""], ["Jin", "Xisen", ""], ["Ren", "Xiang", ""]]}, {"id": "2103.10480", "submitter": "David Noever", "authors": "David A. Noever, Samantha E. Miller Noever", "title": "Reading Isn't Believing: Adversarial Attacks On Multi-Modal Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With Open AI's publishing of their CLIP model (Contrastive Language-Image\nPre-training), multi-modal neural networks now provide accessible models that\ncombine reading with visual recognition. Their network offers novel ways to\nprobe its dual abilities to read text while classifying visual objects. This\npaper demonstrates several new categories of adversarial attacks, spanning\nbasic typographical, conceptual, and iconographic inputs generated to fool the\nmodel into making false or absurd classifications. We demonstrate that\ncontradictory text and image signals can confuse the model into choosing false\n(visual) options. Like previous authors, we show by example that the CLIP model\ntends to read first, look later, a phenomenon we describe as reading isn't\nbelieving.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:56:51 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Noever", "David A.", ""], ["Noever", "Samantha E. Miller", ""]]}, {"id": "2103.10489", "submitter": "Ivan Srba", "authors": "Ivan Srba, Gabriele Lenzini, Matus Pikuliak, Samuel Pecar", "title": "Addressing Hate Speech with Data Science: An Overview from Computer\n  Science Perspective", "comments": null, "journal-ref": "Wachs S., Koch-Priewe B., Zick A. (eds) Hate Speech -\n  Multidisziplinare Analysen und Handlungsoptionen. Springer VS, Wiesbaden.\n  2021", "doi": "10.1007/978-3-658-31793-5_14", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From a computer science perspective, addressing on-line hate speech is a\nchallenging task that is attracting the attention of both industry (mainly\nsocial media platform owners) and academia. In this chapter, we provide an\noverview of state-of-the-art data-science approaches - how they define hate\nspeech, which tasks they solve to mitigate the phenomenon, and how they address\nthese tasks. We limit our investigation mostly to (semi-)automatic detection of\nhate speech, which is the task that the majority of existing computer science\nworks focus on. Finally, we summarize the challenges and the open problems in\nthe current data-science research and the future directions in this field. Our\naim is to prepare an easily understandable report, capable to promote the\nmultidisciplinary character of hate speech research. Researchers from other\ndomains (e.g., psychology and sociology) can thus take advantage of the\nknowledge achieved in the computer science domain but also contribute back and\nhelp improve how computer science is addressing that urgent and socially\nrelevant issue which is the prevalence of hate speech in social media.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 19:19:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Srba", "Ivan", ""], ["Lenzini", "Gabriele", ""], ["Pikuliak", "Matus", ""], ["Pecar", "Samuel", ""]]}, {"id": "2103.10518", "submitter": "Qi Liu", "authors": "Qi Liu, Lei Yu, Laura Rimell, Phil Blunsom", "title": "Pretraining the Noisy Channel Model for Task-Oriented Dialogue", "comments": "Accepted to TACL, pre MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Direct decoding for task-oriented dialogue is known to suffer from the\nexplaining-away effect, manifested in models that prefer short and generic\nresponses. Here we argue for the use of Bayes' theorem to factorize the\ndialogue task into two models, the distribution of the context given the\nresponse, and the prior for the response itself. This approach, an\ninstantiation of the noisy channel model, both mitigates the explaining-away\neffect and allows the principled incorporation of large pretrained models for\nthe response prior. We present extensive experiments showing that a noisy\nchannel model decodes better responses compared to direct decoding and that a\ntwo stage pretraining strategy, employing both open-domain and task-oriented\ndialogue data, improves over randomly initialized models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:52:49 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Qi", ""], ["Yu", "Lei", ""], ["Rimell", "Laura", ""], ["Blunsom", "Phil", ""]]}, {"id": "2103.10531", "submitter": "Alexandra Chronopoulou", "authors": "Alexandra Chronopoulou, Dario Stojanovski and Alexander Fraser", "title": "Improving the Lexical Ability of Pretrained Language Models for\n  Unsupervised Neural Machine Translation", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful methods for unsupervised neural machine translation (UNMT) employ\ncrosslingual pretraining via self-supervision, often in the form of a masked\nlanguage modeling or a sequence generation task, which requires the model to\nalign the lexical- and high-level representations of the two languages. While\ncross-lingual pretraining works for similar languages with abundant corpora, it\nperforms poorly in low-resource and distant languages. Previous research has\nshown that this is because the representations are not sufficiently aligned. In\nthis paper, we enhance the bilingual masked language model pretraining with\nlexical-level information by using type-level cross-lingual subword embeddings.\nEmpirical results demonstrate improved performance both on UNMT (up to 4.5\nBLEU) and bilingual lexicon induction using our method compared to a UNMT\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:17:58 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:27:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chronopoulou", "Alexandra", ""], ["Stojanovski", "Dario", ""], ["Fraser", "Alexander", ""]]}, {"id": "2103.10550", "submitter": "Carlos A. Aguirre", "authors": "Carlos Aguirre, Keith Harrigian, Mark Dredze", "title": "Gender and Racial Fairness in Depression Research using Social Media", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple studies have demonstrated that behavior on internet-based social\nmedia platforms can be indicative of an individual's mental health status. The\nwidespread availability of such data has spurred interest in mental health\nresearch from a computational lens. While previous research has raised concerns\nabout possible biases in models produced from this data, no study has\nquantified how these biases actually manifest themselves with respect to\ndifferent demographic groups, such as gender and racial/ethnic groups. Here, we\nanalyze the fairness of depression classifiers trained on Twitter data with\nrespect to gender and racial demographic groups. We find that model performance\nsystematically differs for underrepresented groups and that these discrepancies\ncannot be fully explained by trivial data representation issues. Our study\nconcludes with recommendations on how to avoid these biases in future research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 22:34:41 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Aguirre", "Carlos", ""], ["Harrigian", "Keith", ""], ["Dredze", "Mark", ""]]}, {"id": "2103.10599", "submitter": "Pratik K. Biswas", "authors": "Pratik K. Biswas and Aleksandr Iakubovich", "title": "Extractive Summarization of Call Transcripts", "comments": "Journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization is the process of extracting the most important\ninformation from the text and presenting it concisely in fewer sentences. Call\ntranscript is a text that involves textual description of a phone conversation\nbetween a customer (caller) and agent(s) (customer representatives). This paper\npresents an indigenously developed method that combines topic modeling and\nsentence selection with punctuation restoration in condensing ill-punctuated or\nun-punctuated call transcripts to produce summaries that are more readable.\nExtensive testing, evaluation and comparisons have demonstrated the efficacy of\nthis summarizer for call transcript summarization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:40:59 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 18:48:02 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Biswas", "Pratik K.", ""], ["Iakubovich", "Aleksandr", ""]]}, {"id": "2103.10668", "submitter": "Ramin Shahbazi", "authors": "Ramin Shahbazi, Rishab Sharma, Fatemeh H. Fard", "title": "API2Com: On the Improvement of Automatically Generated Code Comments\n  Using API Documentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code comments can help in program comprehension and are considered as\nimportant artifacts to help developers in software maintenance. However, the\ncomments are mostly missing or are outdated, specially in complex software\nprojects. As a result, several automatic comment generation models are\ndeveloped as a solution. The recent models explore the integration of external\nknowledge resources such as Unified Modeling Language class diagrams to improve\nthe generated comments. In this paper, we propose API2Com, a model that\nleverages the Application Programming Interface Documentations (API Docs) as a\nknowledge resource for comment generation. The API Docs include the description\nof the methods in more details and therefore, can provide better context in the\ngenerated comments. The API Docs are used along with the code snippets and\nAbstract Syntax Trees in our model. We apply the model on a large Java dataset\nof over 130,000 methods and evaluate it using both Transformer and RNN-base\narchitectures. Interestingly, when API Docs are used, the performance increase\nis negligible. We therefore run different experiments to reason about the\nresults. For methods that only contain one API, adding API Docs improves the\nresults by 4% BLEU score on average (BLEU score is an automatic evaluation\nmetric used in machine translation). However, as the number of APIs that are\nused in a method increases, the performance of the model in generating comments\ndecreases due to long documentations used in the input. Our results confirm\nthat the API Docs can be useful in generating better comments, but, new\ntechniques are required to identify the most informative ones in a method\nrather than using all documentations simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 07:29:40 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Shahbazi", "Ramin", ""], ["Sharma", "Rishab", ""], ["Fard", "Fatemeh H.", ""]]}, {"id": "2103.10673", "submitter": "Marek \\v{S}uppa", "authors": "Katar\\'ina Bene\\v{s}ov\\'a, Andrej \\v{S}vec, Marek \\v{S}uppa", "title": "Cost-effective Deployment of BERT Models in Serverless Environment", "comments": "NAACL-HLT 2021 Industry Track Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this study we demonstrate the viability of deploying BERT-style models to\nserverless environments in a production setting. Since the freely available\npre-trained models are too large to be deployed in this way, we utilize\nknowledge distillation and fine-tune the models on proprietary datasets for two\nreal-world tasks: sentiment analysis and semantic textual similarity. As a\nresult, we obtain models that are tuned for a specific domain and deployable in\nserverless environments. The subsequent performance analysis shows that this\nsolution results in latency levels acceptable for production use and that it is\nalso a cost-effective approach for small-to-medium size deployments of BERT\nmodels, all without any infrastructure overhead.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 07:45:17 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 12:19:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bene\u0161ov\u00e1", "Katar\u00edna", ""], ["\u0160vec", "Andrej", ""], ["\u0160uppa", "Marek", ""]]}, {"id": "2103.10682", "submitter": "Tianwen Wei", "authors": "Tianwen Wei, Jianwei Qi, Shenghuan He, Songtao Sun", "title": "Masked Conditional Random Fields for Sequence Labeling", "comments": "accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conditional Random Field (CRF) based neural models are among the most\nperformant methods for solving sequence labeling problems. Despite its great\nsuccess, CRF has the shortcoming of occasionally generating illegal sequences\nof tags, e.g. sequences containing an \"I-\" tag immediately after an \"O\" tag,\nwhich is forbidden by the underlying BIO tagging scheme. In this work, we\npropose Masked Conditional Random Field (MCRF), an easy to implement variant of\nCRF that impose restrictions on candidate paths during both training and\ndecoding phases. We show that the proposed method thoroughly resolves this\nissue and brings consistent improvement over existing CRF-based models with\nnear zero additional cost.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 08:23:24 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wei", "Tianwen", ""], ["Qi", "Jianwei", ""], ["He", "Shenghuan", ""], ["Sun", "Songtao", ""]]}, {"id": "2103.10685", "submitter": "Xu Zou", "authors": "Xu Zou, Da Yin, Qingyang Zhong, Ming Ding, Zhilin Yang, Jie Tang", "title": "Controllable Generation from Pre-trained Language Models via Inverse\n  Prompting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale pre-trained language models have demonstrated strong capabilities\nof generating realistic text. However, it remains challenging to control the\ngeneration results. Previous approaches such as prompting are far from\nsufficient, which limits the usage of language models. To tackle this\nchallenge, we propose an innovative method, inverse prompting, to better\ncontrol text generation. The core idea of inverse prompting is to use generated\ntext to inversely predict the prompt during beam search, which enhances the\nrelevance between the prompt and the generated text and provides better\ncontrollability. Empirically, we pre-train a large-scale Chinese language model\nto perform a systematic study using human evaluation on the tasks of\nopen-domain poem generation and open-domain long-form question answering. Our\nresults show that our proposed method substantially outperforms the baselines\nand that our generation quality is close to human performance on some of the\ntasks.\n  Narrators can try our poem generation demo at\nhttps://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at\nhttps://pretrain.aminer.cn/app/qa. For researchers, the code is provided in\nhttps://github.com/THUDM/InversePrompting.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 08:36:52 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 17:51:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zou", "Xu", ""], ["Yin", "Da", ""], ["Zhong", "Qingyang", ""], ["Ding", "Ming", ""], ["Yang", "Zhilin", ""], ["Tang", "Jie", ""]]}, {"id": "2103.10730", "submitter": "Simran Khanuja", "authors": "Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee\n  Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu,\n  Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian,\n  Partha Talukdar", "title": "MuRIL: Multilingual Representations for Indian Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India is a multilingual society with 1369 rationalized languages and dialects\nbeing spoken across the country (INDIA, 2011). Of these, the 22 scheduled\nlanguages have a staggering total of 1.17 billion speakers and 121 languages\nhave more than 10,000 speakers (INDIA, 2011). India also has the second largest\n(and an ever growing) digital footprint (Statista, 2020). Despite this, today's\nstate-of-the-art multilingual systems perform suboptimally on Indian (IN)\nlanguages. This can be explained by the fact that multilingual language models\n(LMs) are often trained on 100+ languages together, leading to a small\nrepresentation of IN languages in their vocabulary and training data.\nMultilingual LMs are substantially less effective in resource-lean scenarios\n(Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn't help\ncapture the various nuances of a language. One also commonly observes IN\nlanguage text transliterated to Latin or code-mixed with English, especially in\ninformal settings (for example, on social media platforms) (Rijhwani et al.,\n2017). This phenomenon is not adequately handled by current state-of-the-art\nmultilingual LMs. To address the aforementioned gaps, we propose MuRIL, a\nmultilingual LM specifically built for IN languages. MuRIL is trained on\nsignificantly large amounts of IN text corpora only. We explicitly augment\nmonolingual text corpora with both translated and transliterated document\npairs, that serve as supervised cross-lingual signals in training. MuRIL\nsignificantly outperforms multilingual BERT (mBERT) on all tasks in the\nchallenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present\nresults on transliterated (native to Latin script) test sets of the chosen\ndatasets and demonstrate the efficacy of MuRIL in handling transliterated data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:06:37 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 13:23:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Khanuja", "Simran", ""], ["Bansal", "Diksha", ""], ["Mehtani", "Sarvesh", ""], ["Khosla", "Savya", ""], ["Dey", "Atreyee", ""], ["Gopalan", "Balaji", ""], ["Margam", "Dilip Kumar", ""], ["Aggarwal", "Pooja", ""], ["Nagipogu", "Rajiv Teja", ""], ["Dave", "Shachi", ""], ["Gupta", "Shruti", ""], ["Gali", "Subhash Chandra Bose", ""], ["Subramanian", "Vish", ""], ["Talukdar", "Partha", ""]]}, {"id": "2103.10731", "submitter": "Herman Kamper", "authors": "Christiaan Jacobs, Yevgen Matusevych, Herman Kamper", "title": "Acoustic word embeddings for zero-resource languages using\n  self-supervised contrastive learning and multilingual adaptation", "comments": "Accepted to SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Acoustic word embeddings (AWEs) are fixed-dimensional representations of\nvariable-length speech segments. For zero-resource languages where labelled\ndata is not available, one AWE approach is to use unsupervised\nautoencoder-based recurrent models. Another recent approach is to use\nmultilingual transfer: a supervised AWE model is trained on several\nwell-resourced languages and then applied to an unseen zero-resource language.\nWe consider how a recent contrastive learning loss can be used in both the\npurely unsupervised and multilingual transfer settings. Firstly, we show that\nterms from an unsupervised term discovery system can be used for contrastive\nself-supervision, resulting in improvements over previous unsupervised\nmonolingual AWE models. Secondly, we consider how multilingual AWE models can\nbe adapted to a specific zero-resource language using discovered terms. We find\nthat self-supervised contrastive adaptation outperforms adapted multilingual\ncorrespondence autoencoder and Siamese AWE models, giving the best overall\nresults in a word discrimination task on six zero-resource languages.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:08:35 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Jacobs", "Christiaan", ""], ["Matusevych", "Yevgen", ""], ["Kamper", "Herman", ""]]}, {"id": "2103.10734", "submitter": "Alp \\\"Oktem", "authors": "Alp \\\"Oktem, Eric DeLuca, Rodrigue Bashizi, Eric Paquin, Grace Tang", "title": "Congolese Swahili Machine Translation for Humanitarian Response", "comments": "Accepted to Africa NLP workshop organized within the 16th Conference\n  of the European Chapter of the Association for Computational Linguistics\n  (EACL2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our efforts to make a bidirectional Congolese\nSwahili (SWC) to French (FRA) neural machine translation system with the\nmotivation of improving humanitarian translation workflows. For training, we\ncreated a 25,302-sentence general domain parallel corpus and combined it with\npublicly available data. Experimenting with low-resource methodologies like\ncross-dialect transfer and semi-supervised learning, we recorded improvements\nof up to 2.4 and 3.5 BLEU points in the SWC-FRA and FRA-SWC directions,\nrespectively. We performed human evaluations to assess the usability of our\nmodels in a COVID-domain chatbot that operates in the Democratic Republic of\nCongo (DRC). Direct assessment in the SWC-FRA direction demonstrated an average\nquality ranking of 6.3 out of 10 with 75% of the target strings conveying the\nmain message of the source text. For the FRA-SWC direction, our preliminary\ntests on post-editing assessment showed its potential usefulness for\nmachine-assisted translation. We make our models, datasets containing up to 1\nmillion sentences, our development pipeline, and a translator web-app available\nfor public use.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:15:48 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["\u00d6ktem", "Alp", ""], ["DeLuca", "Eric", ""], ["Bashizi", "Rodrigue", ""], ["Paquin", "Eric", ""], ["Tang", "Grace", ""]]}, {"id": "2103.10763", "submitter": "Jiayan Pei", "authors": "Jiayan Pei, Yimin Wu, Zishan Qin, Yao Cong, Jingtao Guan", "title": "Attention-based model for predicting question relatedness on Stack\n  Overflow", "comments": "11 pages, 4 figures, IEEE/ACM MSR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack Overflow is one of the most popular Programming Community-based\nQuestion Answering (PCQA) websites that has attracted more and more users in\nrecent years. When users raise or inquire questions in Stack Overflow,\nproviding related questions can help them solve problems. Although there are\nmany approaches based on deep learning that can automatically predict the\nrelatedness between questions, those approaches are limited since interaction\ninformation between two questions may be lost. In this paper, we adopt the deep\nlearning technique, propose an Attention-based Sentence pair Interaction Model\n(ASIM) to predict the relatedness between questions on Stack Overflow\nautomatically. We adopt the attention mechanism to capture the semantic\ninteraction information between the questions. Besides, we have pre-trained and\nreleased word embeddings specific to the software engineering domain for this\ntask, which may also help other related tasks. The experiment results\ndemonstrate that ASIM has made significant improvement over the baseline\napproaches in Precision, Recall, and Micro-F1 evaluation metrics, achieving\nstate-of-the-art performance in this task. Our model also performs well in the\nduplicate question detection task of AskUbuntu, which is a similar but\ndifferent task, proving its generalization and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 12:18:03 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 09:12:02 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 13:06:57 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 06:44:49 GMT"}, {"version": "v5", "created": "Thu, 1 Apr 2021 11:57:51 GMT"}, {"version": "v6", "created": "Mon, 5 Apr 2021 10:37:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Pei", "Jiayan", ""], ["Wu", "Yimin", ""], ["Qin", "Zishan", ""], ["Cong", "Yao", ""], ["Guan", "Jingtao", ""]]}, {"id": "2103.10918", "submitter": "Nicholas Egan", "authors": "Nicholas Egan, Oleg Vasilyev, John Bohannon", "title": "Play the Shannon Game With Language Models: A Human-Free Approach to\n  Summary Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a summary is to concisely state the most important information in\na document. With this principle in mind, we introduce new reference-free\nsummary evaluation metrics that use a pretrained language model to estimate the\ninformation shared between a document and its summary. These metrics are a\nmodern take on the Shannon Game, a method for summary quality scoring proposed\ndecades ago, where we replace human annotators with language models. We also\nview these metrics as an extension of BLANC, a recently proposed approach to\nsummary quality measurement based on the performance of a language model with\nand without the help of a summary. Using GPT-2, we empirically verify that the\nintroduced metrics correlate with human judgement based on coverage, overall\nquality, and five summary dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:27:58 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Egan", "Nicholas", ""], ["Vasilyev", "Oleg", ""], ["Bohannon", "John", ""]]}, {"id": "2103.11011", "submitter": "Dani Kiyasseh", "authors": "Dani Kiyasseh, Tingting Zhu, David Clifton", "title": "Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of\n  Cardiac Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac signals, such as the electrocardiogram, convey a significant amount\nof information about the health status of a patient which is typically\nsummarized by a clinician in the form of a clinical report, a cumbersome\nprocess that is prone to errors. To streamline this routine process, we propose\na deep neural network capable of captioning cardiac signals; it receives a\ncardiac signal as input and generates a clinical report as output. We extend\nthis further to generate multilingual reports. To that end, we create and make\npublicly available a multilingual clinical report dataset. In the absence of\nsufficient labelled data, deep neural networks can benefit from a warm-start,\nor pre-training, procedure in which parameters are first learned in an\narbitrary task. We propose such a task in the form of discriminative\nmultilingual pre-training where tokens from clinical reports are randomly\nreplaced with those from other languages and the network is tasked with\npredicting the language of all tokens. We show that our method performs on par\nwith state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE,\nwhile simultaneously generating diverse and plausible clinical reports. We also\ndemonstrate that multilingual models can outperform their monolingual\ncounterparts, informally terming this beneficial phenomenon as the blessing of\nmultilinguality.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:30:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kiyasseh", "Dani", ""], ["Zhu", "Tingting", ""], ["Clifton", "David", ""]]}, {"id": "2103.11024", "submitter": "Andres Karjus", "authors": "Andres Karjus, Richard A. Blythe, Simon Kirby, Tianyu Wang, Kenny\n  Smith", "title": "Conceptual similarity and communicative need shape colexification: an\n  experimental study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colexification refers to the phenomenon of multiple meanings sharing one word\nin a language. Cross-linguistic lexification patterns have been shown to be\nlargely predictable, as similar concepts are often colexified. We test a recent\nclaim that, beyond this general tendency, communicative needs play an important\nrole in shaping colexification patterns. We approach this question by means of\na series of human experiments, using an artificial language communication game\nparadigm. Our results across four experiments match the previous\ncross-linguistic findings: all other things being equal, speakers do prefer to\ncolexify similar concepts. However, we also find evidence supporting the\ncommunicative need hypothesis: when faced with a frequent need to distinguish\nsimilar pairs of meanings, speakers adjust their colexification preferences to\nmaintain communicative efficiency, and avoid colexifying those similar meanings\nwhich need to be distinguished in communication. This research provides further\nevidence to support the argument that languages are shaped by the needs and\npreferences of their speakers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:18:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Karjus", "Andres", ""], ["Blythe", "Richard A.", ""], ["Kirby", "Simon", ""], ["Wang", "Tianyu", ""], ["Smith", "Kenny", ""]]}, {"id": "2103.11029", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis, Venkatesh Sivaraman, Adam Perer, Eric\n  Fosler-Lussier, Harry Hochheiser", "title": "TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between\n  Corpora", "comments": "Accepted as a Systems Demonstration at NAACL-HLT 2021. Video\n  demonstration at https://youtu.be/1xEEfsMwL0k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of words and concepts capture syntactic and semantic regularities\nof language; however, they have seen limited use as tools to study\ncharacteristics of different corpora and how they relate to one another. We\nintroduce TextEssence, an interactive system designed to enable comparative\nanalysis of corpora using embeddings. TextEssence includes visual,\nneighbor-based, and similarity-based modes of embedding analysis in a\nlightweight, web-based interface. We further propose a new measure of embedding\nconfidence based on nearest neighborhood overlap, to assist in identifying\nhigh-quality embeddings for corpus analysis. A case study on COVID-19\nscientific literature illustrates the utility of the system. TextEssence is\navailable from https://github.com/drgriffis/text-essence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:26:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Sivaraman", "Venkatesh", ""], ["Perer", "Adam", ""], ["Fosler-Lussier", "Eric", ""], ["Hochheiser", "Harry", ""]]}, {"id": "2103.11062", "submitter": "Kareem Ahmed", "authors": "Kareem Ahmed, Eric Wang, Guy Van den Broeck, Kai-Wei Chang", "title": "Leveraging Unlabeled Data for Entity-Relation Extraction through\n  Probabilistic Constraint Satisfaction", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of entity-relation extraction in the presence of\nsymbolic domain knowledge. Such knowledge takes the form of an ontology\ndefining relations and their permissible arguments. Previous approaches set out\nto integrate such knowledge in their learning approaches either through\nself-training, or through approximations that lose the precise meaning of the\nlogical expressions. By contrast, our approach employs semantic loss which\ncaptures the precise meaning of a logical sentence through maintaining a\nprobability distribution over all possible states, and guiding the model to\nsolutions which minimize any constraint violations. With a focus on low-data\nregimes, we show that semantic loss outperforms the baselines by a wide margin.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 00:16:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ahmed", "Kareem", ""], ["Wang", "Eric", ""], ["Broeck", "Guy Van den", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2103.11070", "submitter": "Dian Yu", "authors": "Dian Yu, Kenji Sagae, Zhou Yu", "title": "Attribute Alignment: Controlling Text Generation from Pre-trained\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models benefit from training with a large amount of unlabeled\ntext, which gives them increasingly fluent and diverse generation capabilities.\nHowever, using these models for text generation that takes into account target\nattributes, such as sentiment polarity or specific topics, remains a challenge.\nWe propose a simple and flexible method for controlling text generation by\naligning disentangled attribute representations. In contrast to recent efforts\non training a discriminator to perturb the token level distribution for an\nattribute, we use the same data to learn an alignment function to guide the\npre-trained, non-controlled language model to generate texts with the target\nattribute without changing the original language model parameters. We evaluate\nour method on sentiment- and topic-controlled generation, and show large\nperformance gains over previous methods while retaining fluency and diversity.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 01:51:32 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yu", "Dian", ""], ["Sagae", "Kenji", ""], ["Yu", "Zhou", ""]]}, {"id": "2103.11072", "submitter": "Siwen Luo", "authors": "Siwen Luo and Hamish Ivison and Caren Han and Josiah Poon", "title": "Local Interpretations for Explainable Natural Language Processing: A\n  Survey", "comments": "This work is an initial draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of deep learning techniques has grown across various fields over\nthe past decade, complaints about the opaqueness of the black-box models have\nincreased, resulting in an increased focus on transparency in deep learning\nmodels. This work investigates various methods to improve the interpretability\nof deep neural networks for natural language processing (NLP) tasks, including\nmachine translation and sentiment analysis. We provide a comprehensive\ndiscussion on the definition of the term \\textit{interpretability} and its\nvarious aspects at the beginning of this work. The methods collected and\nsummarised in this survey are only associated with local interpretation and are\ndivided into three categories: 1) explaining the model's predictions through\nrelated input features; 2) explaining through natural language explanation; 3)\nprobing the hidden states of models and word representations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 02:28:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luo", "Siwen", ""], ["Ivison", "Hamish", ""], ["Han", "Caren", ""], ["Poon", "Josiah", ""]]}, {"id": "2103.11088", "submitter": "Chen Liang", "authors": "Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Weizhu Chen,\n  Jianfeng Gao and Tuo Zhao", "title": "Token-wise Curriculum Learning for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing curriculum learning approaches to Neural Machine Translation (NMT)\nrequire sampling sufficient amounts of \"easy\" samples from training data at the\nearly training stage. This is not always achievable for low-resource languages\nwhere the amount of training data is limited. To address such limitation, we\npropose a novel token-wise curriculum learning approach that creates sufficient\namounts of easy samples. Specifically, the model learns to predict a short\nsub-sequence from the beginning part of each target sentence at the early stage\nof training, and then the sub-sequence is gradually expanded as the training\nprogresses. Such a new curriculum design is inspired by the cumulative effect\nof translation errors, which makes the latter tokens more difficult to predict\nthan the beginning ones. Extensive experiments show that our approach can\nconsistently outperform baselines on 5 language pairs, especially for\nlow-resource languages. Combining our approach with sentence-level methods\nfurther improves the performance on high-resource languages.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:57:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liang", "Chen", ""], ["Jiang", "Haoming", ""], ["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Chen", "Weizhu", ""], ["Gao", "Jianfeng", ""], ["Zhao", "Tuo", ""]]}, {"id": "2103.11089", "submitter": "Liangyou Li", "authors": "Liangyou Li and Andy Way and Qun Liu", "title": "Dependency Graph-to-String Statistical Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present graph-based translation models which translate source graphs into\ntarget strings. Source graphs are constructed from dependency trees with extra\nlinks so that non-syntactic phrases are connected. Inspired by phrase-based\nmodels, we first introduce a translation model which segments a graph into a\nsequence of disjoint subgraphs and generates a translation by combining\nsubgraph translations left-to-right using beam search. However, similar to\nphrase-based models, this model is weak at phrase reordering. Therefore, we\nfurther introduce a model based on a synchronous node replacement grammar which\nlearns recursive translation rules. We provide two implementations of the model\nwith different restrictions so that source graphs can be parsed efficiently.\nExperiments on Chinese--English and German--English show that our graph-based\nmodels are significantly better than corresponding sequence- and tree-based\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 04:20:56 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Liangyou", ""], ["Way", "Andy", ""], ["Liu", "Qun", ""]]}, {"id": "2103.11145", "submitter": "Alberto Testoni", "authors": "Alberto Testoni, Raffaella Bernardi", "title": "Overprotective Training Environments Fall Short at Testing Time: Let\n  Models Contribute to Their Own Training", "comments": "This paper has been published in the Proceedings of the Seventh\n  Italian Conference on Computational Linguistics, CLiC-it 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite important progress, conversational systems often generate dialogues\nthat sound unnatural to humans. We conjecture that the reason lies in their\ndifferent training and testing conditions: agents are trained in a controlled\n\"lab\" setting but tested in the \"wild\". During training, they learn to generate\nan utterance given the human dialogue history. On the other hand, during\ntesting, they must interact with each other, and hence deal with noisy data. We\npropose to fill this gap by training the model with mixed batches containing\nboth samples of human and machine-generated dialogues. We assess the validity\nof the proposed method on GuessWhat?!, a visual referential game.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 09:55:50 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:16:52 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Testoni", "Alberto", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "2103.11151", "submitter": "Alberto Testoni", "authors": "Alberto Testoni, Raffaella Bernardi", "title": "The Interplay of Task Success and Dialogue Quality: An in-depth\n  Evaluation in Task-Oriented Visual Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When training a model on referential dialogue guessing games, the best model\nis usually chosen based on its task success. We show that in the popular\nend-to-end approach, this choice prevents the model from learning to generate\nlinguistically richer dialogues, since the acquisition of language proficiency\ntakes longer than learning the guessing task. By comparing models playing\ndifferent games (GuessWhat, GuessWhich, and Mutual Friends), we show that this\ndiscrepancy is model- and task-agnostic. We investigate whether and when better\nlanguage quality could lead to higher task success. We show that in GuessWhat,\nmodels could increase their accuracy if they learn to ground, encode, and\ndecode also words that do not occur frequently in the training set.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 10:13:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Testoni", "Alberto", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "2103.11189", "submitter": "Jonne S\\\"alev\\\"a", "authors": "Jonne S\\\"alev\\\"a and Constantine Lignos", "title": "The Effectiveness of Morphology-aware Segmentation in Low-Resource\n  Neural Machine Translation", "comments": "EACL 2021 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the performance of several modern subword segmentation\nmethods in a low-resource neural machine translation setting. We compare\nsegmentations produced by applying BPE at the token or sentence level with\nmorphologically-based segmentations from LMVR and MORSEL. We evaluate\ntranslation tasks between English and each of Nepali, Sinhala, and Kazakh, and\npredict that using morphologically-based segmentation methods would lead to\nbetter performance in this setting. However, comparing to BPE, we find that no\nconsistent and reliable differences emerge between the segmentation methods.\nWhile morphologically-based methods outperform BPE in a few cases, what\nperforms best tends to vary across tasks, and the performance of segmentation\nmethods is often statistically indistinguishable.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 14:39:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["S\u00e4lev\u00e4", "Jonne", ""], ["Lignos", "Constantine", ""]]}, {"id": "2103.11263", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee, Tejas Gokhale, Chitta Baral", "title": "Self-Supervised Test-Time Learning for Reading Comprehension", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent work on unsupervised question answering has shown that models can be\ntrained with procedurally generated question-answer pairs and can achieve\nperformance competitive with supervised methods. In this work, we consider the\ntask of unsupervised reading comprehension and present a method that performs\n\"test-time learning\" (TTL) on a given context (text passage), without requiring\ntraining on large-scale human-authored datasets containing\n\\textit{context-question-answer} triplets. This method operates directly on a\nsingle test context, uses self-supervision to train models on synthetically\ngenerated question-answer pairs, and then infers answers to unseen\nhuman-authored questions for this context. Our method achieves accuracies\ncompetitive with fully supervised methods and significantly outperforms current\nunsupervised methods. TTL methods with a smaller model are also competitive\nwith the current state-of-the-art in unsupervised reading comprehension.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 23:24:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Gokhale", "Tejas", ""], ["Baral", "Chitta", ""]]}, {"id": "2103.11320", "submitter": "Ninareh Mehrabi", "authors": "Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren,\n  Aram Galstyan", "title": "Lawyers are Dishonest? Quantifying Representational Harms in Commonsense\n  Knowledge Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warning: this paper contains content that may be offensive or upsetting.\n  Numerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 06:59:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Mehrabi", "Ninareh", ""], ["Zhou", "Pei", ""], ["Morstatter", "Fred", ""], ["Pujara", "Jay", ""], ["Ren", "Xiang", ""], ["Galstyan", "Aram", ""]]}, {"id": "2103.11332", "submitter": "Tiezheng Yu", "authors": "Tiezheng Yu, Zihan Liu, Pascale Fung", "title": "AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive\n  Summarization", "comments": "The first two authors contributed equally. Accepted as a long paper\n  in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art abstractive summarization models generally rely on extensive\nlabeled data, which lowers their generalization ability on domains where such\ndata are not available. In this paper, we present a study of domain adaptation\nfor the abstractive summarization task across six diverse target domains in a\nlow-resource setting. Specifically, we investigate the second phase of\npre-training on large-scale generative models under three different settings:\n1) source domain pre-training; 2) domain-adaptive pre-training; and 3)\ntask-adaptive pre-training. Experiments show that the effectiveness of\npre-training is correlated with the similarity between the pre-training data\nand the target domain task. Moreover, we find that continuing pre-training\ncould lead to the pre-trained model's catastrophic forgetting, and a learning\nmethod with less forgetting can alleviate this issue. Furthermore, results\nillustrate that a huge gap still exists between the low-resource and\nhigh-resource settings, which highlights the need for more advanced domain\nadaptation methods for the abstractive summarization task.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 08:12:19 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 08:42:52 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 14:09:29 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yu", "Tiezheng", ""], ["Liu", "Zihan", ""], ["Fung", "Pascale", ""]]}, {"id": "2103.11356", "submitter": "Dongsheng Wang", "authors": "Dongsheng Wang, Prayag Tiwari, Sahil Garg, Hongyin Zhu, Peter Bruza", "title": "Structural block driven - enhanced convolutional neural representation\n  for relation extraction", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2019.105913", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel lightweight relation extraction approach of\nstructural block driven - convolutional neural learning. Specifically, we\ndetect the essential sequential tokens associated with entities through\ndependency analysis, named as a structural block, and only encode the block on\na block-wise and an inter-block-wise representation, utilizing multi-scale\nCNNs. This is to 1) eliminate the noisy from irrelevant part of a sentence;\nmeanwhile 2) enhance the relevant block representation with both block-wise and\ninter-block-wise semantically enriched representation. Our method has the\nadvantage of being independent of long sentence context since we only encode\nthe sequential tokens within a block boundary. Experiments on two datasets\ni.e., SemEval2010 and KBP37, demonstrate the significant advantages of our\nmethod. In particular, we achieve the new state-of-the-art performance on the\nKBP37 dataset; and comparable performance with the state-of-the-art on the\nSemEval2010 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:23:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Dongsheng", ""], ["Tiwari", "Prayag", ""], ["Garg", "Sahil", ""], ["Zhu", "Hongyin", ""], ["Bruza", "Peter", ""]]}, {"id": "2103.11360", "submitter": "Yimeng Dai", "authors": "Rui Zhang, Yimeng Dai, Shijie Liu", "title": "NameRec*: Highly Accurate and Fine-grained Person Name Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce the NameRec* task, which aims to do highly\naccurate and fine-grained person name recognition. Traditional Named Entity\nRecognition models have good performance in recognising well-formed person\nnames from text with consistent and complete syntax, such as news articles.\nHowever, there are rapidly growing scenarios where sentences are of incomplete\nsyntax and names are in various forms such as user-generated contents and\nacademic homepages. To address person name recognition in this context, we\npropose a fine-grained annotation scheme based on anthroponymy. To take full\nadvantage of the fine-grained annotations, we propose a Co-guided Neural\nNetwork (CogNN) for person name recognition. CogNN fully explores the\nintra-sentence context and rich training signals of name forms. To better\nutilize the inter-sentence context and implicit relations, which are extremely\nessential for recognizing person names in long documents, we further propose an\nInter-sentence BERT Model (IsBERT). IsBERT has an overlapped input processor,\nand an inter-sentence encoder with bidirectional overlapped contextual\nembedding learning and multi-hop inference mechanisms. To derive benefit from\ndifferent documents with a diverse abundance of context, we propose an advanced\nAdaptive Inter-sentence BERT Model (Ada-IsBERT) to dynamically adjust the\ninter-sentence overlapping ratio to different documents. We conduct extensive\nexperiments to demonstrate the superiority of the proposed methods on both\nacademic homepages and news articles.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:35:04 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 12:25:59 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhang", "Rui", ""], ["Dai", "Yimeng", ""], ["Liu", "Shijie", ""]]}, {"id": "2103.11367", "submitter": "Yuanxin Liu", "authors": "Yuanxin Liu and Zheng Lin and Fengcheng Yuan", "title": "ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques", "comments": "Published as a conference paper at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models of the BERT family have defined the\nstate-of-the-arts in a wide range of NLP tasks. However, the performance of\nBERT-based models is mainly driven by the enormous amount of parameters, which\nhinders their application to resource-limited scenarios. Faced with this\nproblem, recent studies have been attempting to compress BERT into a\nsmall-scale model. However, most previous work primarily focuses on a single\nkind of compression technique, and few attention has been paid to the\ncombination of different methods. When BERT is compressed with integrated\ntechniques, a critical question is how to design the entire compression\nframework to obtain the optimal performance. In response to this question, we\nintegrate three kinds of compression methods (weight pruning, low-rank\nfactorization and knowledge distillation (KD)) and explore a range of designs\nconcerning model architecture, KD strategy, pruning frequency and learning rate\nschedule. We find that a careful choice of the designs is crucial to the\nperformance of the compressed model. Based on the empirical findings, our best\ncompressed model, dubbed Refined BERT cOmpreSsion with InTegrAted techniques\n(ROSITA), is $7.5 \\times$ smaller than BERT while maintains $98.5\\%$ of the\nperformance on five tasks of the GLUE benchmark, outperforming the previous\nBERT compression methods with similar parameter budget. The code is available\nat https://github.com/llyx97/Rosita.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 11:33:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Yuanxin", ""], ["Lin", "Zheng", ""], ["Yuan", "Fengcheng", ""]]}, {"id": "2103.11401", "submitter": "Pelin Dogan", "authors": "Pelin Dogan-Sch\\\"onberger, Julian M\\\"ader, Thomas Hofmann", "title": "SwissDial: Parallel Multidialectal Corpus of Spoken Swiss German", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swiss German is a dialect continuum whose natively acquired dialects\nsignificantly differ from the formal variety of the language. These dialects\nare mostly used for verbal communication and do not have standard orthography.\nThis has led to a lack of annotated datasets, rendering the use of many NLP\nmethods infeasible. In this paper, we introduce the first annotated parallel\ncorpus of spoken Swiss German across 8 major dialects, plus a Standard German\nreference. Our goal has been to create and to make available a basic dataset\nfor employing data-driven NLP applications in Swiss German. We present our data\ncollection procedure in detail and validate the quality of our corpus by\nconducting experiments with the recent neural models for speech synthesis.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:00:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Dogan-Sch\u00f6nberger", "Pelin", ""], ["M\u00e4der", "Julian", ""], ["Hofmann", "Thomas", ""]]}, {"id": "2103.11405", "submitter": "Yu Bao", "authors": "Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen", "title": "Non-Autoregressive Translation by Learning Target Categorical Codes", "comments": "11 pages, 3 figures, 7 tables. Accepted by NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-autoregressive Transformer is a promising text generation model. However,\ncurrent non-autoregressive models still fall behind their autoregressive\ncounterparts in translation quality. We attribute this accuracy gap to the lack\nof dependency modeling among decoder inputs. In this paper, we propose CNAT,\nwhich learns implicitly categorical codes as latent variables into the\nnon-autoregressive decoding. The interaction among these categorical codes\nremedies the missing dependencies and improves the model capacity. Experiment\nresults show that our model achieves comparable or better performance in\nmachine translation tasks, compared with several strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:12:34 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bao", "Yu", ""], ["Huang", "Shujian", ""], ["Xiao", "Tong", ""], ["Wang", "Dongqi", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "2103.11408", "submitter": "Raviraj Joshi", "authors": "Atharva Kulkarni, Meet Mandhane, Manali Likhitkar, Gayatri Kshirsagar,\n  Raviraj Joshi", "title": "L3CubeMahaSent: A Marathi Tweet-based Sentiment Analysis Dataset", "comments": "Accepted at WASSA@EACL 2021", "journal-ref": "https://www.aclweb.org/anthology/2021.wassa-1.23/", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is one of the most fundamental tasks in Natural Language\nProcessing. Popular languages like English, Arabic, Russian, Mandarin, and also\nIndian languages such as Hindi, Bengali, Tamil have seen a significant amount\nof work in this area. However, the Marathi language which is the third most\npopular language in India still lags behind due to the absence of proper\ndatasets. In this paper, we present the first major publicly available Marathi\nSentiment Analysis Dataset - L3CubeMahaSent. It is curated using tweets\nextracted from various Maharashtrian personalities' Twitter accounts. Our\ndataset consists of ~16,000 distinct tweets classified in three broad classes\nviz. positive, negative, and neutral. We also present the guidelines using\nwhich we annotated the tweets. Finally, we present the statistics of our\ndataset and baseline classification results using CNN, LSTM, ULMFiT, and\nBERT-based deep learning models.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:22:13 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 07:15:12 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kulkarni", "Atharva", ""], ["Mandhane", "Meet", ""], ["Likhitkar", "Manali", ""], ["Kshirsagar", "Gayatri", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2103.11431", "submitter": "Rishabh Gupta", "authors": "Rishabh Gupta and Rajesh N Rao", "title": "SEMIE: SEMantically Infused Embeddings with Enhanced Interpretability\n  for Domain-specific Small Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Word embeddings are a basic building block of modern NLP pipelines. Efforts\nhave been made to learn rich, efficient, and interpretable embeddings for large\ngeneric datasets available in the public domain. However, these embeddings have\nlimited applicability for small corpora from specific domains such as\nautomotive, manufacturing, maintenance and support, etc. In this work, we\npresent a comprehensive notion of interpretability for word embeddings and\npropose a novel method to generate highly interpretable and efficient\nembeddings for a domain-specific small corpus. We report the evaluation results\nof our resulting word embeddings and demonstrate their novel features for\nenhanced interpretability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 16:28:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gupta", "Rishabh", ""], ["Rao", "Rajesh N", ""]]}, {"id": "2103.11441", "submitter": "Tao Gui", "authors": "Tao Gui, Xiao Wang, Qi Zhang, Qin Liu, Yicheng Zou, Xin Zhou, Rui\n  Zheng, Chong Zhang, Qinzhuo Wu, Jiacheng Ye, Zexiong Pang, Yongxin Zhang,\n  Zhengyan Li, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng\n  Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Bolin Zhu, Shan Qin,\n  Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou,\n  Zhongyu Wei, Xipeng Qiu and Xuanjing Huang", "title": "TextFlint: Unified Multilingual Robustness Evaluation Toolkit for\n  Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various robustness evaluation methodologies from different perspectives have\nbeen proposed for different natural language processing (NLP) tasks. These\nmethods have often focused on either universal or task-specific generalization\ncapabilities. In this work, we propose a multilingual robustness evaluation\nplatform for NLP tasks (TextFlint) that incorporates universal text\ntransformation, task-specific transformation, adversarial attack,\nsubpopulation, and their combinations to provide comprehensive robustness\nanalysis. TextFlint enables practitioners to automatically evaluate their\nmodels from all aspects or to customize their evaluations as desired with just\na few lines of code. To guarantee user acceptability, all the text\ntransformations are linguistically based, and we provide a human evaluation for\neach one. TextFlint generates complete analytical reports as well as targeted\naugmented data to address the shortcomings of the model's robustness. To\nvalidate TextFlint's utility, we performed large-scale empirical evaluations\n(over 67,000 evaluations) on state-of-the-art deep learning models, classic\nsupervised methods, and real-world systems. Almost all models showed\nsignificant performance degradation, including a decline of more than 50% of\nBERT's prediction accuracy on tasks such as aspect-level sentiment\nclassification, named entity recognition, and natural language inference.\nTherefore, we call for the robustness to be included in the model evaluation,\nso as to promote the healthy development of NLP technology.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:20:38 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 09:56:50 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 09:31:54 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gui", "Tao", ""], ["Wang", "Xiao", ""], ["Zhang", "Qi", ""], ["Liu", "Qin", ""], ["Zou", "Yicheng", ""], ["Zhou", "Xin", ""], ["Zheng", "Rui", ""], ["Zhang", "Chong", ""], ["Wu", "Qinzhuo", ""], ["Ye", "Jiacheng", ""], ["Pang", "Zexiong", ""], ["Zhang", "Yongxin", ""], ["Li", "Zhengyan", ""], ["Ma", "Ruotian", ""], ["Fei", "Zichu", ""], ["Cai", "Ruijian", ""], ["Zhao", "Jun", ""], ["Hu", "Xingwu", ""], ["Yan", "Zhiheng", ""], ["Tan", "Yiding", ""], ["Hu", "Yuan", ""], ["Bian", "Qiyuan", ""], ["Liu", "Zhihua", ""], ["Zhu", "Bolin", ""], ["Qin", "Shan", ""], ["Xing", "Xiaoyu", ""], ["Fu", "Jinlan", ""], ["Zhang", "Yue", ""], ["Peng", "Minlong", ""], ["Zheng", "Xiaoqing", ""], ["Zhou", "Yaqian", ""], ["Wei", "Zhongyu", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2103.11448", "submitter": "Rui Xie", "authors": "Rui Xie, Wei Ye, Jinan Sun, Shikun Zhang", "title": "Exploiting Method Names to Improve Code Summarization: A Deliberation\n  Multi-Task Learning Approach", "comments": "Accepted in ICPC\n  2021(https://conf.researchr.org/details/icpc-2021/icpc-2021-research/13/Exploiting-Method-Names-to-Improve-Code-Summarization-A-Deliberation-Multi-Task-Lear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code summaries are brief natural language descriptions of source code pieces.\nThe main purpose of code summarization is to assist developers in understanding\ncode and to reduce documentation workload. In this paper, we design a novel\nmulti-task learning (MTL) approach for code summarization through mining the\nrelationship between method code summaries and method names. More specifically,\nsince a method's name can be considered as a shorter version of its code\nsummary, we first introduce the tasks of generation and informativeness\nprediction of method names as two auxiliary training objectives for code\nsummarization. A novel two-pass deliberation mechanism is then incorporated\ninto our MTL architecture to generate more consistent intermediate states fed\ninto a summary decoder, especially when informative method names do not exist.\nTo evaluate our deliberation MTL approach, we carried out a large-scale\nexperiment on two existing datasets for Java and Python. The experiment results\nshow that our technique can be easily applied to many state-of-the-art neural\nmodels for code summarization and improve their performance. Meanwhile, our\napproach shows significant superiority when generating summaries for methods\nwith non-informative names.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:52:21 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:54:11 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xie", "Rui", ""], ["Ye", "Wei", ""], ["Sun", "Jinan", ""], ["Zhang", "Shikun", ""]]}, {"id": "2103.11474", "submitter": "Sandra Avila", "authors": "Gabriel Oliveira dos Santos and Esther Luna Colombini and Sandra Avila", "title": "#PraCegoVer: A Large Dataset for Image Captioning in Portuguese", "comments": "19 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatically describing images using natural sentences is an important task\nto support visually impaired people's inclusion onto the Internet. It is still\na big challenge that requires understanding the relation of the objects present\nin the image and their attributes and actions they are involved in. Then,\nvisual interpretation methods are needed, but linguistic models are also\nnecessary to verbally describe the semantic relations. This problem is known as\nImage Captioning. Although many datasets were proposed in the literature, the\nmajority contains only English captions, whereas datasets with captions\ndescribed in other languages are scarce. Recently, a movement called PraCegoVer\narose on the Internet, stimulating users from social media to publish images,\ntag #PraCegoVer and add a short description of their content. Thus, inspired by\nthis movement, we have proposed the #PraCegoVer, a multi-modal dataset with\nPortuguese captions based on posts from Instagram. It is the first large\ndataset for image captioning in Portuguese with freely annotated images.\nFurther, the captions in our dataset bring additional challenges to the\nproblem: first, in contrast to popular datasets such as MS COCO Captions,\n#PraCegoVer has only one reference to each image; also, both mean and variance\nof our reference sentence length are significantly greater than those in the MS\nCOCO Captions. These two characteristics contribute to making our dataset\ninteresting due to the linguistic aspect and the challenges that it introduces\nto the image captioning problem. We publicly-share the dataset at\nhttps://github.com/gabrielsantosrv/PraCegoVer.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 19:55:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Santos", "Gabriel Oliveira dos", ""], ["Colombini", "Esther Luna", ""], ["Avila", "Sandra", ""]]}, {"id": "2103.11528", "submitter": "Son T. Luu", "authors": "Son T. Luu, Kiet Van Nguyen and Ngan Luu-Thuy Nguyen", "title": "A Large-scale Dataset for Hate Speech Detection on Vietnamese Social\n  Media Texts", "comments": "IEA/AIE 2021: Advances and Trends in Artificial Intelligence.\n  Artificial Intelligence Practices, pp 415-426", "journal-ref": null, "doi": "10.1007/978-3-030-79457-6_35", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, Vietnam witnesses the mass development of social network\nusers on different social platforms such as Facebook, Youtube, Instagram, and\nTiktok. On social medias, hate speech has become a critical problem for social\nnetwork users. To solve this problem, we introduce the ViHSD - a\nhuman-annotated dataset for automatically detecting hate speech on the social\nnetwork. This dataset contains over 30,000 comments, each comment in the\ndataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, we\nintroduce the data creation process for annotating and evaluating the quality\nof the dataset. Finally, we evaluated the dataset by deep learning models and\ntransformer models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 00:55:47 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 02:46:47 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 09:29:18 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 06:22:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Luu", "Son T.", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2103.11576", "submitter": "Ying Xu", "authors": "Ying Xu, Xu Zhong, Antonio Jimeno Yepes, Jey Han Lau", "title": "Grey-box Adversarial Attack And Defence For Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a grey-box adversarial attack and defence framework for\nsentiment classification. We address the issues of differentiability, label\npreservation and input reconstruction for adversarial attack and defence in one\nunified framework. Our results show that once trained, the attacking model is\ncapable of generating high-quality adversarial examples substantially faster\n(one order of magnitude less in time) than state-of-the-art attacking methods.\nThese examples also preserve the original sentiment according to human\nevaluation. Additionally, our framework produces an improved classifier that is\nrobust in defending against multiple adversarial attacking methods. Code is\navailable at: https://github.com/ibm-aur-nlp/adv-def-text-dist.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 04:05:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xu", "Ying", ""], ["Zhong", "Xu", ""], ["Yepes", "Antonio Jimeno", ""], ["Lau", "Jey Han", ""]]}, {"id": "2103.11578", "submitter": "Liping Yuan", "authors": "Liping Yuan, Jiehang Zeng, Xiaoqing Zheng", "title": "SparseGAN: Sparse Generative Adversarial Network for Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is still a challenging task to learn a neural text generation model under\nthe framework of generative adversarial networks (GANs) since the entire\ntraining process is not differentiable. The existing training strategies either\nsuffer from unreliable gradient estimations or imprecise sentence\nrepresentations. Inspired by the principle of sparse coding, we propose a\nSparseGAN that generates semantic-interpretable, but sparse sentence\nrepresentations as inputs to the discriminator. The key idea is that we treat\nan embedding matrix as an over-complete dictionary, and use a linear\ncombination of very few selected word embeddings to approximate the output\nfeature representation of the generator at each time step. With such\nsemantic-rich representations, we not only reduce unnecessary noises for\nefficient adversarial training, but also make the entire training process fully\ndifferentiable. Experiments on multiple text generation datasets yield\nperformance improvements, especially in sequence-level metrics, such as BLEU.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 04:44:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yuan", "Liping", ""], ["Zeng", "Jiehang", ""], ["Zheng", "Xiaoqing", ""]]}, {"id": "2103.11596", "submitter": "Shweta Chauhan", "authors": "Shweta Chauhan, Shefali Saxena, Philemon Daniel", "title": "Monolingual and Parallel Corpora for Kangri Low Resource Language", "comments": "7 pages, 6 Tables, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we present the dataset of Himachali low resource endangered\nlanguage, Kangri (ISO 639-3xnr) listed in the United Nations Educational,\nScientific and Cultural Organization (UNESCO). The compilation of kangri corpus\nhas been a challenging task due to the non-availability of the digitalized\nresources. The corpus contains 1,81,552 Monolingual and 27,362 Hindi-Kangri\nParallel corpora. We shared pre-trained kangri word embeddings. We also\nreported the Bilingual Evaluation Understudy (BLEU) score and Metric for\nEvaluation of Translation with Explicit ORdering (METEOR) score of Statistical\nMachine Translation (SMT) and Neural Machine Translation (NMT) results for the\ncorpus. The corpus is freely available for non-commercial usages and research.\nTo the best of our knowledge, this is the first Himachali low resource\nendangered language corpus. The resources are available at\n(https://github.com/chauhanshweta/Kangri_corpus)\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:52:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chauhan", "Shweta", ""], ["Saxena", "Shefali", ""], ["Daniel", "Philemon", ""]]}, {"id": "2103.11603", "submitter": "Liping Yuan", "authors": "Liping Yuan, Jiangtao Feng, Xiaoqing Zheng, Xuanjing Huang", "title": "Alleviate Exposure Bias in Sequence Prediction \\\\ with Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A popular strategy to train recurrent neural networks (RNNs), known as\n``teacher forcing'' takes the ground truth as input at each time step and makes\nthe later predictions partly conditioned on those inputs. Such training\nstrategy impairs their ability to learn rich distributions over entire\nsequences because the chosen inputs hinders the gradients back-propagating to\nall previous states in an end-to-end manner. We propose a fully differentiable\ntraining algorithm for RNNs to better capture long-term dependencies by\nrecovering the probability of the whole sequence. The key idea is that at each\ntime step, the network takes as input a ``bundle'' of similar words predicted\nat the previous step instead of a single ground truth. The representations of\nthese similar words forms a convex hull, which can be taken as a kind of\nregularization to the input. Smoothing the inputs by this way makes the whole\nprocess trainable and differentiable. This design makes it possible for the\nmodel to explore more feasible combinations (possibly unseen sequences), and\ncan be interpreted as a computationally efficient approximation to the beam\nsearch. Experiments on multiple sequence generation tasks yield performance\nimprovements, especially in sequence-level metrics, such as BLUE or ROUGE-2.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 06:15:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yuan", "Liping", ""], ["Feng", "Jiangtao", ""], ["Zheng", "Xiaoqing", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2103.11643", "submitter": "Xiangyang Mou", "authors": "Xiangyang Mou, Mo Yu, Shiyu Chang, Yufei Feng, Li Zhang and Hui Su", "title": "Complementary Evidence Identification in Open-Domain Question Answering", "comments": "7 pages, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new problem of complementary evidence identification\nfor open-domain question answering (QA). The problem aims to efficiently find a\nsmall set of passages that covers full evidence from multiple aspects as to\nanswer a complex question. To this end, we proposes a method that learns vector\nrepresentations of passages and models the sufficiency and diversity within the\nselected set, in addition to the relevance between the question and passages.\nOur experiments demonstrate that our method considers the dependence within the\nsupporting evidence and significantly improves the accuracy of complementary\nevidence selection in QA domain.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:04:50 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 06:35:05 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 08:13:48 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mou", "Xiangyang", ""], ["Yu", "Mo", ""], ["Chang", "Shiyu", ""], ["Feng", "Yufei", ""], ["Zhang", "Li", ""], ["Su", "Hui", ""]]}, {"id": "2103.11647", "submitter": "Ning Ding", "authors": "Ning Ding, Xiaobin Wang, Yao Fu, Guangwei Xu, Rui Wang, Pengjun Xie,\n  Ying Shen, Fei Huang, Hai-Tao Zheng, Rui Zhang", "title": "Prototypical Representation Learning for Relation Extraction", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing relations between entities is a pivotal task of relational\nlearning. Learning relation representations from distantly-labeled datasets is\ndifficult because of the abundant label noise and complicated expressions in\nhuman language. This paper aims to learn predictive, interpretable, and robust\nrelation representations from distantly-labeled data that are effective in\ndifferent settings, including supervised, distantly supervised, and few-shot\nlearning. Instead of solely relying on the supervision from noisy labels, we\npropose to learn prototypes for each relation from contextual information to\nbest explore the intrinsic semantics of relations. Prototypes are\nrepresentations in the feature space abstracting the essential semantics of\nrelations between entities in sentences. We learn prototypes based on\nobjectives with clear geometric interpretation, where the prototypes are unit\nvectors uniformly dispersed in a unit ball, and statement embeddings are\ncentered at the end of their corresponding prototype vectors on the surface of\nthe ball. This approach allows us to learn meaningful, interpretable prototypes\nfor the final classification. Results on several relation learning tasks show\nthat our model significantly outperforms the previous state-of-the-art models.\nWe further demonstrate the robustness of the encoder and the interpretability\nof prototypes with extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:11:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ding", "Ning", ""], ["Wang", "Xiaobin", ""], ["Fu", "Yao", ""], ["Xu", "Guangwei", ""], ["Wang", "Rui", ""], ["Xie", "Pengjun", ""], ["Shen", "Ying", ""], ["Huang", "Fei", ""], ["Zheng", "Hai-Tao", ""], ["Zhang", "Rui", ""]]}, {"id": "2103.11761", "submitter": "Adrian Rebmann", "authors": "Adrian Rebmann and Han van der Aa", "title": "Extracting Semantic Process Information from the Natural Language in\n  Event Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining focuses on the analysis of recorded event data in order to\ngain insights about the true execution of business processes. While\nfoundational process mining techniques treat such data as sequences of abstract\nevents, more advanced techniques depend on the availability of specific kinds\nof information, such as resources in organizational mining and business objects\nin artifact-centric analysis. However, this information is generally not\nreadily available, but rather associated with events in an ad hoc manner, often\neven as part of unstructured textual attributes. Given the size and complexity\nof event logs, this calls for automated support to extract such process\ninformation and, thereby, enable advanced process mining techniques. In this\npaper, we present an approach that achieves this through so-called semantic\nrole labeling of event data. We combine the analysis of textual attribute\nvalues, based on a state-of-the-art language model, with a novel attribute\nclassification technique. In this manner, our approach extracts information\nabout up to eight semantic roles per event. We demonstrate the approach's\nefficacy through a quantitative evaluation using a broad range of event logs\nand demonstrate the usefulness of the extracted information in a case study.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:39:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rebmann", "Adrian", ""], ["van der Aa", "Han", ""]]}, {"id": "2103.11764", "submitter": "Sara Durrani", "authors": "Sara Durrani and Umair Arshad", "title": "Transfer learning from High-Resource to Low-Resource Language Improves\n  Speech Affect Recognition Classification Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Speech Affect Recognition is a problem of extracting emotional affects from\naudio data. Low resource languages corpora are rear and affect recognition is a\ndifficult task in cross-corpus settings. We present an approach in which the\nmodel is trained on high resource language and fine-tune to recognize affects\nin low resource language. We train the model in same corpus setting on SAVEE,\nEMOVO, Urdu, and IEMOCAP by achieving baseline accuracy of 60.45, 68.05, 80.34,\nand 56.58 percent respectively. For capturing the diversity of affects in\nlanguages cross-corpus evaluations are discussed in detail. We find that\naccuracy improves by adding the domain target data into the training data.\nFinally, we show that performance is improved for low resource language speech\naffect recognition by achieving the UAR OF 69.32 and 68.2 for Urdu and Italian\nspeech affects.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:17:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Durrani", "Sara", ""], ["Arshad", "Umair", ""]]}, {"id": "2103.11786", "submitter": "Changzeng Fu", "authors": "Changzeng Fu", "title": "JPS-daprinfo: A Dataset for Japanese Dialog Act Analysis and\n  People-related Information Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We conducted a labeling work on a spoken Japanese dataset (I-JAS) for the\ntext classification, which contains 50 interview dialogues of two-way Japanese\nconversation that discuss the participants' past present and future. Each\ndialogue is 30 minutes long. From this dataset, we selected the interview\ndialogues of native Japanese speakers as the samples. Given the dataset, we\nannotated sentences with 13 labels. The labeling work was conducted by native\nJapanese speakers who have experiences with data annotation. The total amount\nof the annotated samples is 20130.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 12:15:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Fu", "Changzeng", ""]]}, {"id": "2103.11790", "submitter": "Patrick Schramowski", "authors": "Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin Rothkopf,\n  Kristian Kersting", "title": "Language Models have a Moral Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial writing is permeating our lives due to recent advances in\nlarge-scale, transformer-based language models (LMs) such as BERT, its\nvariants, GPT-2/3, and others. Using them as pretrained models and fine-tuning\nthem for specific tasks, researchers have extended the state of the art for\nmany NLP tasks and shown that they not only capture linguistic knowledge but\nalso retain general knowledge implicitly present in the data. These and other\nsuccesses are exciting. Unfortunately, LMs trained on unfiltered text corpora\nsuffer from degenerate and biased behaviour. While this is well established, we\nshow that recent improvements of LMs also store ethical and moral values of the\nsociety and actually bring a ``moral dimension'' to surface: the values are\ncapture geometrically by a direction in the embedding space, reflecting well\nthe agreement of phrases to social norms implicitly expressed in the training\ntexts. This provides a path for attenuating or even preventing toxic\ndegeneration in LMs. Since one can now rate the (non-)normativity of arbitrary\nphrases without explicitly training the LM for this task, the moral dimension\ncan be used as ``moral compass'' guiding (even other) LMs towards producing\nnormative text, as we will show.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:59:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Schramowski", "Patrick", ""], ["Turan", "Cigdem", ""], ["Andersen", "Nico", ""], ["Rothkopf", "Constantin", ""], ["Kersting", "Kristian", ""]]}, {"id": "2103.11792", "submitter": "Muhammad Zohaib Khan", "authors": "Muhammad Zohaib Khan", "title": "Comparing the Performance of NLP Toolkits and Evaluation measures in\n  Legal Tech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in Natural Language Processing have led to the\nintroduction of state-of-the-art Neural Language Models, enabled with\nunsupervised transferable learning, using different pretraining objectives.\nWhile these models achieve excellent results on the downstream NLP tasks,\nvarious domain adaptation techniques can improve their performance on\ndomain-specific tasks. We compare and analyze the pretrained Neural Language\nModels, XLNet (autoregressive), and BERT (autoencoder) on the Legal Tasks.\nResults show that XLNet Model performs better on our Sequence Classification\ntask of Legal Opinions Classification, whereas BERT produces better results on\nthe NER task. We use domain-specific pretraining and additional legal\nvocabulary to adapt BERT Model further to the Legal Domain. We prepared\nmultiple variants of the BERT Model, using both methods and their combination.\nComparing our variants of the BERT Model, specializing in the Legal Domain, we\nconclude that both additional pretraining and vocabulary techniques enhance the\nBERT model's performance on the Legal Opinions Classification task. Additional\nlegal vocabulary improves BERT's performance on the NER task. Combining the\npretraining and vocabulary techniques further improves the final results. Our\nLegal-Vocab-BERT Model gives the best results on the Legal Opinions Task,\noutperforming the larger pretrained general Language Models, i.e., BERT-Base\nand XLNet-Base.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 11:06:32 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Khan", "Muhammad Zohaib", ""]]}, {"id": "2103.11794", "submitter": "Xiaochen Hou", "authors": "Xiaochen Hou, Peng Qi, Guangtao Wang, Rex Ying, Jing Huang, Xiaodong\n  He, Bowen Zhou", "title": "Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level\n  Sentiment Classification", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on aspect-level sentiment classification has demonstrated the\nefficacy of incorporating syntactic structures such as dependency trees with\ngraph neural networks(GNN), but these approaches are usually vulnerable to\nparsing errors. To better leverage syntactic information in the face of\nunavoidable errors, we propose a simple yet effective graph ensemble technique,\nGraphMerge, to make use of the predictions from differ-ent parsers. Instead of\nassigning one set of model parameters to each dependency tree, we first combine\nthe dependency relations from different parses before applying GNNs over the\nresulting graph. This allows GNN mod-els to be robust to parse errors at no\nadditional computational cost, and helps avoid overparameterization and\noverfitting from GNN layer stacking by introducing more connectivity into the\nensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter\ndatasets show that our GraphMerge model not only outperforms models with single\ndependency tree, but also beats other ensemble mod-els without adding model\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:27:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hou", "Xiaochen", ""], ["Qi", "Peng", ""], ["Wang", "Guangtao", ""], ["Ying", "Rex", ""], ["Huang", "Jing", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "2103.11795", "submitter": "Bojun Huang", "authors": "Fei Yuan, Longtu Zhang, Huang Bojun, Yaobo Liang", "title": "Simpson's Bias in NLP Training", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most machine learning tasks, we evaluate a model $M$ on a given data\npopulation $S$ by measuring a population-level metric $F(S;M)$. Examples of\nsuch evaluation metric $F$ include precision/recall for (binary) recognition,\nthe F1 score for multi-class classification, and the BLEU metric for language\ngeneration. On the other hand, the model $M$ is trained by optimizing a\nsample-level loss $G(S_t;M)$ at each learning step $t$, where $S_t$ is a subset\nof $S$ (a.k.a. the mini-batch). Popular choices of $G$ include cross-entropy\nloss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption\nbehind this paradigm is that the mean value of the sample-level loss $G$, if\naveraged over all possible samples, should effectively represent the\npopulation-level metric $F$ of the task, such as, that $\\mathbb{E}[ G(S_t;M) ]\n\\approx F(S;M)$.\n  In this paper, we systematically investigate the above assumption in several\nNLP tasks. We show, both theoretically and experimentally, that some popular\ndesigns of the sample-level loss $G$ may be inconsistent with the true\npopulation-level metric $F$ of the task, so that models trained to optimize the\nformer can be substantially sub-optimal to the latter, a phenomenon we call it,\nSimpson's bias, due to its deep connections with the classic paradox known as\nSimpson's reversal paradox in statistics and social sciences.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:19:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yuan", "Fei", ""], ["Zhang", "Longtu", ""], ["Bojun", "Huang", ""], ["Liang", "Yaobo", ""]]}, {"id": "2103.11798", "submitter": "Roy Ka-Wei Lee", "authors": "Zhiqiang Hu, Roy Ka-Wei Lee, Lei Wang, Ee-Peng Lim and Bo Dai", "title": "DeepStyle: User Style Embedding for Authorship Attribution of Short\n  Texts", "comments": "Paper accepted for 4th APWeb-WAIM Joint Conference on Web and Big\n  Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Authorship attribution (AA), which is the task of finding the owner of a\ngiven text, is an important and widely studied research topic with many\napplications. Recent works have shown that deep learning methods could achieve\nsignificant accuracy improvement for the AA task. Nevertheless, most of these\nproposed methods represent user posts using a single type of feature (e.g.,\nword bi-grams) and adopt a text classification approach to address the task.\nFurthermore, these methods offer very limited explainability of the AA results.\nIn this paper, we address these limitations by proposing DeepStyle, a novel\nembedding-based framework that learns the representations of users' salient\nwriting styles. We conduct extensive experiments on two real-world datasets\nfrom Twitter and Weibo. Our experiment results show that DeepStyle outperforms\nthe state-of-the-art baselines on the AA task.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 15:56:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hu", "Zhiqiang", ""], ["Lee", "Roy Ka-Wei", ""], ["Wang", "Lei", ""], ["Lim", "Ee-Peng", ""], ["Dai", "Bo", ""]]}, {"id": "2103.11799", "submitter": "Roy Ka-Wei Lee", "authors": "Rui Cao, Roy Ka-Wei Lee and Tuan-Anh Hoang", "title": "DeepHate: Hate Speech Detection via Multi-Faceted Text Representations", "comments": "Paper Accepted for 12th International ACM Conference on Web Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online hate speech is an important issue that breaks the cohesiveness of\nonline social communities and even raises public safety concerns in our\nsocieties. Motivated by this rising issue, researchers have developed many\ntraditional machine learning and deep learning methods to detect hate speech in\nonline social platforms automatically. However, most of these methods have only\nconsidered single type textual feature, e.g., term frequency, or using word\nembeddings. Such approaches neglect the other rich textual information that\ncould be utilized to improve hate speech detection. In this paper, we propose\nDeepHate, a novel deep learning model that combines multi-faceted text\nrepresentations such as word embeddings, sentiments, and topical information,\nto detect hate speech in online social platforms. We conduct extensive\nexperiments and evaluate DeepHate on three large publicly available real-world\ndatasets. Our experiment results show that DeepHate outperforms the\nstate-of-the-art baselines on the hate speech detection task. We also perform\ncase studies to provide insights into the salient features that best aid in\ndetecting hate speech in online social platforms.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:11:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cao", "Rui", ""], ["Lee", "Roy Ka-Wei", ""], ["Hoang", "Tuan-Anh", ""]]}, {"id": "2103.11800", "submitter": "Roy Ka-Wei Lee", "authors": "Md Rabiul Awal, Rui Cao, Roy Ka-Wei Lee, Sandra Mitrovic", "title": "AngryBERT: Joint Learning Target and Emotion for Hate Speech Detection", "comments": "Paper Accepted for 25th Pacific-Asia Conference on Knowledge\n  Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated hate speech detection in social media is a challenging task that\nhas recently gained significant traction in the data mining and Natural\nLanguage Processing community. However, most of the existing methods adopt a\nsupervised approach that depended heavily on the annotated hate speech\ndatasets, which are imbalanced and often lack training samples for hateful\ncontent. This paper addresses the research gaps by proposing a novel multitask\nlearning-based model, AngryBERT, which jointly learns hate speech detection\nwith sentiment classification and target identification as secondary relevant\ntasks. We conduct extensive experiments to augment three commonly-used hate\nspeech detection datasets. Our experiment results show that AngryBERT\noutperforms state-of-the-art single-task-learning and multitask learning\nbaselines. We conduct ablation studies and case studies to empirically examine\nthe strengths and characteristics of our AngryBERT model and show that the\nsecondary tasks are able to improve hate speech detection.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:17:26 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Awal", "Md Rabiul", ""], ["Cao", "Rui", ""], ["Lee", "Roy Ka-Wei", ""], ["Mitrovic", "Sandra", ""]]}, {"id": "2103.11804", "submitter": "Valeria Mazzeo", "authors": "V. Mazzeo, A. Rapisarda and G. Giuffrida", "title": "Detection of fake news on CoViD-19 on Web Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In early January 2020, after China reported the first cases of the new\ncoronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully\naccurate information has started spreading faster than the virus itself.\nAlongside this pandemic, people have experienced a parallel infodemic, i.e., an\noverabundance of information, some of which misleading or even harmful, that\nhas widely spread around the globe. Although Social Media are increasingly\nbeing used as information source, Web Search Engines, like Google or Yahoo!,\nstill represent a powerful and trustworthy resource for finding information on\nthe Web. This is due to their capability to capture the largest amount of\ninformation, helping users quickly identify the most relevant, useful, although\nnot always the most reliable, results for their search queries. This study aims\nto detect potential misleading and fake contents by capturing and analysing\ntextual information, which flow through Search Engines. By using a real-world\ndataset associated with recent CoViD-19 pandemic, we first apply re-sampling\ntechniques for class imbalance, then we use existing Machine Learning\nalgorithms for classification of not reliable news. By extracting lexical and\nhost-based features of associated Uniform Resource Locators (URLs) for news\narticles, we show that the proposed methods, so common in phishing and\nmalicious URLs detection, can improve the efficiency and performance of\nclassifiers. Based on these findings, we suggest that the use of both textual\nand URLs features can improve the effectiveness of fake news detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:07:26 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 20:42:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mazzeo", "V.", ""], ["Rapisarda", "A.", ""], ["Giuffrida", "G.", ""]]}, {"id": "2103.11811", "submitter": "David Adelani", "authors": "David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D'souza,\n  Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba,\n  Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime,\n  Shamsuddeen Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez\n  Ogayo, Anuoluwapo Aremu, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi,\n  Seid Muhie Yimam, Tajuddeen Gwadabe, Ignatius Ezeani, Rubungo Andre\n  Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba\n  Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki,\n  Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala,\n  Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede,\n  Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele\n  Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi\n  Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia,\n  Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye\n  Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei", "title": "MasakhaNER: Named Entity Recognition for African Languages", "comments": "Accepted to TACL 2021, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:12:44 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 15:14:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Adelani", "David Ifeoluwa", ""], ["Abbott", "Jade", ""], ["Neubig", "Graham", ""], ["D'souza", "Daniel", ""], ["Kreutzer", "Julia", ""], ["Lignos", "Constantine", ""], ["Palen-Michel", "Chester", ""], ["Buzaaba", "Happy", ""], ["Rijhwani", "Shruti", ""], ["Ruder", "Sebastian", ""], ["Mayhew", "Stephen", ""], ["Azime", "Israel Abebe", ""], ["Muhammad", "Shamsuddeen", ""], ["Emezue", "Chris Chinenye", ""], ["Nakatumba-Nabende", "Joyce", ""], ["Ogayo", "Perez", ""], ["Aremu", "Anuoluwapo", ""], ["Gitau", "Catherine", ""], ["Mbaye", "Derguene", ""], ["Alabi", "Jesujoba", ""], ["Yimam", "Seid Muhie", ""], ["Gwadabe", "Tajuddeen", ""], ["Ezeani", "Ignatius", ""], ["Niyongabo", "Rubungo Andre", ""], ["Mukiibi", "Jonathan", ""], ["Otiende", "Verrah", ""], ["Orife", "Iroro", ""], ["David", "Davis", ""], ["Ngom", "Samba", ""], ["Adewumi", "Tosin", ""], ["Rayson", "Paul", ""], ["Adeyemi", "Mofetoluwa", ""], ["Muriuki", "Gerald", ""], ["Anebi", "Emmanuel", ""], ["Chukwuneke", "Chiamaka", ""], ["Odu", "Nkiruka", ""], ["Wairagala", "Eric Peter", ""], ["Oyerinde", "Samuel", ""], ["Siro", "Clemencia", ""], ["Bateesa", "Tobius Saul", ""], ["Oloyede", "Temilola", ""], ["Wambui", "Yvonne", ""], ["Akinode", "Victor", ""], ["Nabagereka", "Deborah", ""], ["Katusiime", "Maurice", ""], ["Awokoya", "Ayodele", ""], ["MBOUP", "Mouhamadane", ""], ["Gebreyohannes", "Dibora", ""], ["Tilaye", "Henok", ""], ["Nwaike", "Kelechi", ""], ["Wolde", "Degaga", ""], ["Faye", "Abdoulaye", ""], ["Sibanda", "Blessing", ""], ["Ahia", "Orevaoghene", ""], ["Dossou", "Bonaventure F. P.", ""], ["Ogueji", "Kelechi", ""], ["DIOP", "Thierno Ibrahima", ""], ["Diallo", "Abdoulaye", ""], ["Akinfaderin", "Adewale", ""], ["Marengereke", "Tendai", ""], ["Osei", "Salomey", ""]]}, {"id": "2103.11835", "submitter": "Andrei Mircea Romascanu", "authors": "Mikael Brunila, Rosie Zhao, Andrei Mircea, Sam Lumley, Renee Sieber", "title": "Bridging the gap between supervised classification and unsupervised\n  topic modelling for social-media assisted crisis management", "comments": "Adapt-NLP @EACL2021; first three authors contributed equally; code\n  available at https://github.com/smacawi/bert-topics/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media such as Twitter provide valuable information to crisis managers\nand affected people during natural disasters. Machine learning can help\nstructure and extract information from the large volume of messages shared\nduring a crisis; however, the constantly evolving nature of crises makes\neffective domain adaptation essential. Supervised classification is limited by\nunchangeable class labels that may not be relevant to new events, and\nunsupervised topic modelling by insufficient prior knowledge. In this paper, we\nbridge the gap between the two and show that BERT embeddings finetuned on\ncrisis-related tweet classification can effectively be used to adapt to a new\ncrisis, discovering novel topics while preserving relevant classes from\nsupervised training, and leveraging bidirectional self-attention to extract\ntopic keywords. We create a dataset of tweets from a snowstorm to evaluate our\nmethod's transferability to new crises, and find that it outperforms\ntraditional topic models in both automatic, and human evaluations grounded in\nthe needs of crisis managers. More broadly, our method can be used for textual\ndomain adaptation where the latent classes are unknown but overlap with known\nclasses from other domains.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:30:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Brunila", "Mikael", ""], ["Zhao", "Rosie", ""], ["Mircea", "Andrei", ""], ["Lumley", "Sam", ""], ["Sieber", "Renee", ""]]}, {"id": "2103.11850", "submitter": "Abul Hasan", "authors": "Abul Hasan, Mark Levene, David Weston, Renate Fromson, Nicolas\n  Koslover, and Tamara Levene", "title": "Triage and diagnosis of COVID-19 from medical social media", "comments": "13 pages, 6 figrues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: This study aims to develop an end-to-end natural language\nprocessing pipeline for triage and diagnosis of COVID-19 from patient-authored\nsocial media posts, in order to provide researchers and other interested\nparties with additional information on the symptoms, severity and prevalence of\nthe disease. Materials and Methods: The text processing pipeline first extracts\nCOVID-19 symptoms and related concepts such as severity, duration, negations,\nand body parts from patients posts using conditional random fields. An\nunsupervised rule-based algorithm is then applied to establish relations\nbetween concepts in the next step of the pipeline. The extracted concepts and\nrelations are subsequently used to construct two different vector\nrepresentations of each post. These vectors are applied separately to build\nsupport vector machine learning models to triage patients into three categories\nand diagnose them for COVID-19. Results: We report that macro- and\nmicro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for\nthe triage and diagnosis of COVID-19, when the models are trained on human\nlabelled data. Our experimental results indicate that similar performance can\nbe achieved when the models are trained using predicted labels from concept\nextraction and rule-based classifiers, thus yielding end-to-end machine\nlearning. Also, we highlight important features uncovered by our diagnostic\nmachine learning models and compare them with the most frequent symptoms\nrevealed in another COVID-19 dataset. In particular, we found that the most\nimportant features are not always the most frequent ones.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:46:16 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 11:49:12 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 08:12:14 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Hasan", "Abul", ""], ["Levene", "Mark", ""], ["Weston", "David", ""], ["Fromson", "Renate", ""], ["Koslover", "Nicolas", ""], ["Levene", "Tamara", ""]]}, {"id": "2103.11859", "submitter": "Andrew A. Krizhanovsky", "authors": "Andrew Krizhanovsky, Natalia Krizhanovsky and Irina Novak", "title": "Part of speech and gramset tagging algorithms for unknown words based on\n  morphological dictionaries of the Veps and Karelian languages", "comments": "17 pages, 4 tables, 7 figures, published in the conference proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research devoted to the low-resource Veps and Karelian languages.\nAlgorithms for assigning part of speech tags to words and grammatical\nproperties to words are presented in the article. These algorithms use our\nmorphological dictionaries, where the lemma, part of speech and a set of\ngrammatical features (gramset) are known for each word form. The algorithms are\nbased on the analogy hypothesis that words with the same suffixes are likely to\nhave the same inflectional models, the same part of speech and gramset. The\naccuracy of these algorithms were evaluated and compared. 313 thousand Vepsian\nand 66 thousand Karelian words were used to verify the accuracy of these\nalgorithms. The special functions were designed to assess the quality of\nresults of the developed algorithms. 92.4% of Vepsian words and 86.8% of\nKarelian words were assigned a correct part of speech by the developed\nalgorithm. 95.3% of Vepsian words and 90.7% of Karelian words were assigned a\ncorrect gramset by our algorithm. Morphological and semantic tagging of texts,\nwhich are closely related and inseparable in our corpus processes, are\ndescribed in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:58:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Krizhanovsky", "Andrew", ""], ["Krizhanovsky", "Natalia", ""], ["Novak", "Irina", ""]]}, {"id": "2103.11865", "submitter": "Carola Carlino", "authors": "Carola Carlino, Gennaro Nolano, Maria Pia di Buono, Johanna Monti", "title": "#LaCulturaNonsiFerma: Report on Use and Diffusion of #Hashtags from the\n  Italian Cultural Institutions during the COVID-19 outbreak", "comments": "17 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report presents an analysis of #hashtags used by Italian Cultural\nHeritage institutions to promote and communicate cultural content during the\nCOVID-19 lock-down period in Italy. Several activities to support and engage\nusers' have been proposed using social media. Most of these activities present\none or more #hashtags which help to aggregate content and create a community on\nspecific topics. Results show that on one side Italian institutions have been\nvery proactive in adapting to the pandemic scenario and on the other side\nusers' reacted very positively increasing their participation in the proposed\nactivities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:02:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Carlino", "Carola", ""], ["Nolano", "Gennaro", ""], ["di Buono", "Maria Pia", ""], ["Monti", "Johanna", ""]]}, {"id": "2103.11878", "submitter": "Yuchen Jiang", "authors": "Yuchen Jiang, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang and\n  Ming Zhou", "title": "BlonD: An Automatic Evaluation Metric for Document-level\n  MachineTranslation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard automatic metrics (such as BLEU) are problematic for document-level\nMT evaluation. They can neither distinguish document-level improvements in\ntranslation quality from sentence-level ones nor can they identify the specific\ndiscourse phenomena that caused the translation errors. To address these\nproblems, we propose an automatic metric BlonD for document-level machine\ntranslation evaluation. BlonD takes discourse coherence into consideration by\ncalculating the recall and distance of check-pointing phrases and tags, and\nfurther provides comprehensive evaluation scores by combining with n-gram.\nExtensive comparisons between BlonD and existing evaluation metrics are\nconducted to illustrate their critical distinctions. Experimental results show\nthat BlonD has a much higher document-level sensitivity with respect to\nprevious metrics. The human evaluation also reveals high Pearson R correlation\nvalues between BlonD scores and manual quality judgments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:14:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jiang", "Yuchen", ""], ["Ma", "Shuming", ""], ["Zhang", "Dongdong", ""], ["Yang", "Jian", ""], ["Huang", "Haoyang", ""], ["Zhou", "Ming", ""]]}, {"id": "2103.11909", "submitter": "Jan Philip Wahle", "authors": "Jan Philip Wahle, Terry Ruas, Tom\\'a\\v{s} Folt\\'ynek, Norman Meuschke,\n  Bela Gipp", "title": "Identifying Machine-Paraphrased Plagiarism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Employing paraphrasing tools to conceal plagiarized text is a severe threat\nto academic integrity. To enable the detection of machine-paraphrased text, we\nevaluate the effectiveness of five pre-trained word embedding models combined\nwith machine learning classifiers and state-of-the-art neural language models.\nWe analyze preprints of research papers, graduation theses, and Wikipedia\narticles, which we paraphrased using different configurations of the tools\nSpinBot and SpinnerChief. The best performing technique, Longformer, achieved\nan average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for\nSpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and\nF1=65.6% for SpinnerChief cases. We show that the automated classification\nalleviates shortcomings of widely-used text-matching systems, such as Turnitin\nand PlagScan. To facilitate future research, all data, code, and two web\napplications showcasing our contributions are openly available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:54:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wahle", "Jan Philip", ""], ["Ruas", "Terry", ""], ["Folt\u00fdnek", "Tom\u00e1\u0161", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2103.11920", "submitter": "Jonas Pfeiffer", "authors": "Gregor Geigle, Jonas Pfeiffer, Nils Reimers, Ivan Vuli\\'c, Iryna\n  Gurevych", "title": "Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for\n  Improved Cross-Modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches to cross-modal retrieval process text and\nvisual input jointly, relying on Transformer-based architectures with\ncross-attention mechanisms that attend over all words and objects in an image.\nWhile offering unmatched retrieval performance, such models: 1) are typically\npretrained from scratch and thus less scalable, 2) suffer from huge retrieval\nlatency and inefficiency issues, which makes them impractical in realistic\napplications. To address these crucial gaps towards both improved and efficient\ncross-modal retrieval, we propose a novel fine-tuning framework which turns any\npretrained text-image multi-modal model into an efficient retrieval model. The\nframework is based on a cooperative retrieve-and-rerank approach which\ncombines: 1) twin networks to separately encode all items of a corpus, enabling\nefficient initial retrieval, and 2) a cross-encoder component for a more\nnuanced (i.e., smarter) ranking of the retrieved small set of items. We also\npropose to jointly fine-tune the two components with shared weights, yielding a\nmore parameter-efficient model. Our experiments on a series of standard\ncross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot\nsetups, demonstrate improved accuracy and huge efficiency benefits over the\nstate-of-the-art cross-encoders.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:08:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Geigle", "Gregor", ""], ["Pfeiffer", "Jonas", ""], ["Reimers", "Nils", ""], ["Vuli\u0107", "Ivan", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2103.11921", "submitter": "Darsh Shah", "authors": "Darsh J Shah, Lili Yu, Tao Lei and Regina Barzilay", "title": "Nutri-bullets: Summarizing Health Studies by Composing Segments", "comments": "12 pages", "journal-ref": "AAAI 2021 Camera Ready", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce \\emph{Nutri-bullets}, a multi-document summarization task for\nhealth and nutrition. First, we present two datasets of food and health\nsummaries from multiple scientific studies. Furthermore, we propose a novel\n\\emph{extract-compose} model to solve the problem in the regime of limited\nparallel data. We explicitly select key spans from several abstracts using a\npolicy network, followed by composing the selected spans to present a summary\nvia a task specific language model. Compared to state-of-the-art methods, our\napproach leads to more faithful, relevant and diverse summarization --\nproperties imperative to this application. For instance, on the BreastCancer\ndataset our approach gets a more than 50\\% improvement on relevance and\nfaithfulness.\\footnote{Our code and data is available at\n\\url{https://github.com/darsh10/Nutribullets.}}\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:08:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Shah", "Darsh J", ""], ["Yu", "Lili", ""], ["Lei", "Tao", ""], ["Barzilay", "Regina", ""]]}, {"id": "2103.11943", "submitter": "Mikhail Koroteev Mr.", "authors": "M. V. Koroteev", "title": "BERT: A Review of Applications in Natural Language Processing and\n  Understanding", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this review, we describe the application of one of the most popular deep\nlearning-based language models - BERT. The paper describes the mechanism of\noperation of this model, the main areas of its application to the tasks of text\nanalytics, comparisons with similar models in each task, as well as a\ndescription of some proprietary models. In preparing this review, the data of\nseveral dozen original scientific articles published over the past few years,\nwhich attracted the most attention in the scientific community, were\nsystematized. This survey will be useful to all students and researchers who\nwant to get acquainted with the latest advances in the field of natural\nlanguage text analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:34:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Koroteev", "M. V.", ""]]}, {"id": "2103.11955", "submitter": "Derek Tam", "authors": "Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, Colin\n  Raffel", "title": "Improving and Simplifying Pattern Exploiting Training", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few shot learning without any unlabeled data and introduce\nADAPET, which modifies PET's objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:52:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tam", "Derek", ""], ["Menon", "Rakesh R", ""], ["Bansal", "Mohit", ""], ["Srivastava", "Shashank", ""], ["Raffel", "Colin", ""]]}, {"id": "2103.12011", "submitter": "Jonathan Herzig", "authors": "Jonathan Herzig, Thomas M\\\"uller, Syrine Krichene, Julian Martin\n  Eisenschlos", "title": "Open Domain Question Answering over Tables via Dense Retrieval", "comments": "NAACL 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in open-domain QA have led to strong models based on dense\nretrieval, but only focused on retrieving textual passages. In this work, we\ntackle open-domain QA over tables for the first time, and show that retrieval\ncan be improved by a retriever designed to handle tabular context. We present\nan effective pre-training procedure for our retriever and improve retrieval\nquality with mined hard negatives. As relevant datasets are missing, we extract\na subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA\ndataset. We find that our retriever improves retrieval results from 72.0 to\n81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a\nBERT based retriever.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:01:04 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 09:39:24 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Herzig", "Jonathan", ""], ["M\u00fcller", "Thomas", ""], ["Krichene", "Syrine", ""], ["Eisenschlos", "Julian Martin", ""]]}, {"id": "2103.12028", "submitter": "Isaac Caswell", "authors": "Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch,\n  Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov,\n  Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb,\n  Beno\\^it Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey\n  Osei, Pedro Javier Ortiz Su\\'arez, Iroro Orife, Kelechi Ogueji, Rubungo Andre\n  Niyongabo, Toan Q. Nguyen, Mathias M\\\"uller, Andr\\'e M\\\"uller, Shamsuddeen\n  Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov,\n  Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine\n  Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile\n  Dlamini, Nisansa de Silva, Sakine \\c{C}abuk Ball{\\i}, Stella Biderman,\n  Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe\n  Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia,\n  Sweta Agrawal, Mofetoluwa Adeyemi", "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets", "comments": "10 pages paper; 10 pages appendix; AfricaNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the success of large-scale pre-training and multilingual modeling in\nNatural Language Processing (NLP), recent years have seen a proliferation of\nlarge, web-mined text datasets covering hundreds of languages. However, to date\nthere has been no systematic analysis of the quality of these publicly\navailable datasets, or whether the datasets actually contain content in the\nlanguages they claim to represent. In this work, we manually audit the quality\nof 205 language-specific corpora released with five major public datasets\n(CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4), and audit the correctness of\nlanguage codes in a sixth (JW300). We find that lower-resource corpora have\nsystematic issues: at least 15 corpora are completely erroneous, and a\nsignificant fraction contains less than 50% sentences of acceptable quality.\nSimilarly, we find 82 corpora that are mislabeled or use nonstandard/ambiguous\nlanguage codes. We demonstrate that these issues are easy to detect even for\nnon-speakers of the languages in question, and supplement the human judgements\nwith automatic analyses. Inspired by our analysis, we recommend techniques to\nevaluate and improve multilingual corpora and discuss the risks that come with\nlow-quality data releases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:30:33 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 19:38:25 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Caswell", "Isaac", ""], ["Kreutzer", "Julia", ""], ["Wang", "Lisa", ""], ["Wahab", "Ahsan", ""], ["van Esch", "Daan", ""], ["Ulzii-Orshikh", "Nasanbayar", ""], ["Tapo", "Allahsera", ""], ["Subramani", "Nishant", ""], ["Sokolov", "Artem", ""], ["Sikasote", "Claytone", ""], ["Setyawan", "Monang", ""], ["Sarin", "Supheakmungkol", ""], ["Samb", "Sokhar", ""], ["Sagot", "Beno\u00eet", ""], ["Rivera", "Clara", ""], ["Rios", "Annette", ""], ["Papadimitriou", "Isabel", ""], ["Osei", "Salomey", ""], ["Su\u00e1rez", "Pedro Javier Ortiz", ""], ["Orife", "Iroro", ""], ["Ogueji", "Kelechi", ""], ["Niyongabo", "Rubungo Andre", ""], ["Nguyen", "Toan Q.", ""], ["M\u00fcller", "Mathias", ""], ["M\u00fcller", "Andr\u00e9", ""], ["Muhammad", "Shamsuddeen Hassan", ""], ["Muhammad", "Nanda", ""], ["Mnyakeni", "Ayanda", ""], ["Mirzakhalov", "Jamshidbek", ""], ["Matangira", "Tapiwanashe", ""], ["Leong", "Colin", ""], ["Lawson", "Nze", ""], ["Kudugunta", "Sneha", ""], ["Jernite", "Yacine", ""], ["Jenny", "Mathias", ""], ["Firat", "Orhan", ""], ["Dossou", "Bonaventure F. P.", ""], ["Dlamini", "Sakhile", ""], ["de Silva", "Nisansa", ""], ["Ball\u0131", "Sakine \u00c7abuk", ""], ["Biderman", "Stella", ""], ["Battisti", "Alessia", ""], ["Baruwa", "Ahmed", ""], ["Bapna", "Ankur", ""], ["Baljekar", "Pallavi", ""], ["Azime", "Israel Abebe", ""], ["Awokoya", "Ayodele", ""], ["Ataman", "Duygu", ""], ["Ahia", "Orevaoghene", ""], ["Ahia", "Oghenefego", ""], ["Agrawal", "Sweta", ""], ["Adeyemi", "Mofetoluwa", ""]]}, {"id": "2103.12048", "submitter": "Ndapa Nakashole", "authors": "Ndapa Nakashole", "title": "Extracting the Unknown from Long Math Problems", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.HO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In problem solving, understanding the problem that one seeks to solve is an\nessential initial step. In this paper, we propose computational methods for\nfacilitating problem understanding through the task of recognizing the unknown\nin specifications of long Math problems. We focus on the topic of Probability.\nOur experimental results show that learning models yield strong results on the\ntask, a promising first step towards human interpretable, modular approaches to\nunderstanding long Math problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:51:10 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:20:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Nakashole", "Ndapa", ""]]}, {"id": "2103.12204", "submitter": "Long Chen", "authors": "Long Chen, Zhihong Jiang, Jun Xiao, Wei Liu", "title": "Human-like Controllable Image Captioning with Verb-specific Semantic\n  Roles", "comments": "Accepted by CVPR 2021. The code is available at:\n  https://github.com/mad-red/VSR-guided-CIC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllable Image Captioning (CIC) -- generating image descriptions\nfollowing designated control signals -- has received unprecedented attention\nover the last few years. To emulate the human ability in controlling caption\ngeneration, current CIC studies focus exclusively on control signals concerning\nobjective properties, such as contents of interest or descriptive patterns.\nHowever, we argue that almost all existing objective control signals have\noverlooked two indispensable characteristics of an ideal control signal: 1)\nEvent-compatible: all visual contents referred to in a single sentence should\nbe compatible with the described activity. 2) Sample-suitable: the control\nsignals should be suitable for a specific image sample. To this end, we propose\na new control signal for CIC: Verb-specific Semantic Roles (VSR). VSR consists\nof a verb and some semantic roles, which represents a targeted activity and the\nroles of entities involved in this activity. Given a designated VSR, we first\ntrain a grounded semantic role labeling (GSRL) model to identify and ground all\nentities for each role. Then, we propose a semantic structure planner (SSP) to\nlearn human-like descriptive semantic structures. Lastly, we use a role-shift\ncaptioning model to generate the captions. Extensive experiments and ablations\ndemonstrate that our framework can achieve better controllability than several\nstrong baselines on two challenging CIC benchmarks. Besides, we can generate\nmulti-level diverse captions easily. The code is available at:\nhttps://github.com/mad-red/VSR-guided-CIC.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:17:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Long", ""], ["Jiang", "Zhihong", ""], ["Xiao", "Jun", ""], ["Liu", "Wei", ""]]}, {"id": "2103.12235", "submitter": "Ansong Ni", "authors": "Ansong Ni, Matt Gardner, Pradeep Dasigi", "title": "Mitigating False-Negative Contexts in Multi-document QuestionAnswering\n  with Retrieval Marginalization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Question Answering (QA) tasks requiring information from multiple documents\noften rely on a retrieval model to identify relevant information from which the\nreasoning model can derive an answer. The retrieval model is typically trained\nto maximize the likelihood of the labeled supporting evidence. However, when\nretrieving from large text corpora such as Wikipedia, the correct answer can\noften be obtained from multiple evidence candidates, not all of them labeled as\npositive, thus rendering the training signal weak and noisy. The problem is\nexacerbated when the questions are unanswerable or the answers are boolean,\nsince the models cannot rely on lexical overlap to map answers to supporting\nevidences. We develop a new parameterization of set-valued retrieval that\nproperly handles unanswerable queries, and we show that marginalizing over this\nset during training allows a model to mitigate false negatives in annotated\nsupporting evidences. We test our method with two multi-document QA datasets,\nIIRC and HotpotQA. On IIRC, we show that joint modeling with marginalization on\nalternative contexts improves model performance by 5.5 F1 points and achieves a\nnew state-of-the-art performance of 50.6 F1. We also show that marginalization\nresults in 0.9 to 1.6 QA F1 improvement on HotpotQA in various settings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:44:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ni", "Ansong", ""], ["Gardner", "Matt", ""], ["Dasigi", "Pradeep", ""]]}, {"id": "2103.12248", "submitter": "Roozbeh Mottaghi", "authors": "Jialin Wu, Jiasen Lu, Ashish Sabharwal, Roozbeh Mottaghi", "title": "Multi-Modal Answer Validation for Knowledge-Based VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of knowledge-based visual question answering involves answering\nquestions that require external knowledge in addition to the content of the\nimage. Such knowledge typically comes in a variety of forms, including visual,\ntextual, and commonsense knowledge. The use of more knowledge sources, however,\nalso increases the chance of retrieving more irrelevant or noisy facts, making\nit difficult to comprehend the facts and find the answer. To address this\nchallenge, we propose Multi-modal Answer Validation using External knowledge\n(MAVEx), where the idea is to validate a set of promising answer candidates\nbased on answer-specific knowledge retrieval. This is in contrast to existing\napproaches that search for the answer in a vast collection of often irrelevant\nfacts. Our approach aims to learn which knowledge source should be trusted for\neach answer candidate and how to validate the candidate using that source. We\nconsider a multi-modal setting, relying on both textual and visual knowledge\nresources, including images searched using Google, sentences from Wikipedia\narticles, and concepts from ConceptNet. Our experiments with OK-VQA, a\nchallenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 00:49:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 19:57:46 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Wu", "Jialin", ""], ["Lu", "Jiasen", ""], ["Sabharwal", "Ashish", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2103.12258", "submitter": "Prashant Serai", "authors": "Prashant Serai and Vishal Sunder and Eric Fosler-Lussier", "title": "Hallucination of speech recognition errors with sequence to sequence\n  learning", "comments": "Submitted to IEEE/ACM Transactions on Audio Speech and Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) is an imperfect process that results in\ncertain mismatches in ASR output text when compared to plain written text or\ntranscriptions. When plain text data is to be used to train systems for spoken\nlanguage understanding or ASR, a proven strategy to reduce said mismatch and\nprevent degradations, is to hallucinate what the ASR outputs would be given a\ngold transcription. Prior work in this domain has focused on modeling errors at\nthe phonetic level, while using a lexicon to convert the phones to words,\nusually accompanied by an FST Language model. We present novel end-to-end\nmodels to directly predict hallucinated ASR word sequence outputs, conditioning\non an input word sequence as well as a corresponding phoneme sequence. This\nimproves prior published results for recall of errors from an in-domain ASR\nsystem's transcription of unseen data, as well as an out-of-domain ASR system's\ntranscriptions of audio from an unrelated task, while additionally exploring an\nin-between scenario when limited characterization data from the test ASR system\nis obtainable. To verify the extrinsic validity of the method, we also use our\nhallucinated ASR errors to augment training for a spoken question classifier,\nfinding that they enable robustness to real ASR errors in a downstream task,\nwhen scarce or even zero task-specific audio was available at train-time.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:09:39 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 16:13:35 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 20:34:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Serai", "Prashant", ""], ["Sunder", "Vishal", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "2103.12279", "submitter": "Dheeraj Rajagopal", "authors": "Dheeraj Rajagopal, Vidhisha Balachandran, Eduard Hovy, Yulia Tsvetkov", "title": "SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SelfExplain, a novel self-explaining framework that explains a\ntext classifier's predictions using phrase-based concepts. SelfExplain augments\nexisting neural classifiers by adding (1) a globally interpretable layer that\nidentifies the most influential concepts in the training set for a given sample\nand (2) a locally interpretable layer that quantifies the contribution of each\nlocal input concept by computing a relevance score relative to the predicted\nlabel. Experiments across five text-classification datasets show that\nSelfExplain facilitates interpretability without sacrificing performance. Most\nimportantly, explanations from SelfExplain are perceived as more\nunderstandable, adequately justifying and trustworthy by human judges compared\nto existing widely-used baselines.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:07:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rajagopal", "Dheeraj", ""], ["Balachandran", "Vidhisha", ""], ["Hovy", "Eduard", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2103.12280", "submitter": "Yanping Chen", "authors": "Yanping Chen and Wenfan Jin and Yongbin Qin and Ruizhang Huang and\n  Qinghua Zheng and Ping Chen", "title": "Annotation of Chinese Predicate Heads and Relevant Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A predicate head is a verbal expression that plays a role as the structural\ncenter of a sentence. Identifying predicate heads is critical to understanding\na sentence. It plays the leading role in organizing the relevant syntactic\nelements in a sentence, including subject elements, adverbial elements, etc.\nFor some languages, such as English, word morphologies are valuable for\nidentifying predicate heads. However, Chinese offers no morphological\ninformation to indicate words` grammatical roles. A Chinese sentence often\ncontains several verbal expressions; identifying the expression that plays the\nrole of the predicate head is not an easy task. Furthermore, Chinese sentences\nare inattentive to structure and provide no delimitation between words.\nTherefore, identifying Chinese predicate heads involves significant challenges.\nIn Chinese information extraction, little work has been performed in predicate\nhead recognition. No generally accepted evaluation dataset supports work in\nthis important area. This paper presents the first attempt to develop an\nannotation guideline for Chinese predicate heads and their relevant syntactic\nelements. This annotation guideline emphasizes the role of the predicate as the\nstructural center of a sentence. The design of relevant syntactic element\nannotation also follows this principle. Many considerations are proposed to\nachieve this goal, e.g., patterns of predicate heads, a flattened annotation\nstructure, and a simpler syntactic unit type. Based on the proposed annotation\nguideline, more than 1,500 documents were manually annotated. The corpus will\nbe available online for public access. With this guideline and annotated\ncorpus, our goal is to broadly impact and advance the research in the area of\nChinese information extraction and to provide the research community with a\ncritical resource that has been lacking for a long time.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:11:59 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 07:05:47 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chen", "Yanping", ""], ["Jin", "Wenfan", ""], ["Qin", "Yongbin", ""], ["Huang", "Ruizhang", ""], ["Zheng", "Qinghua", ""], ["Chen", "Ping", ""]]}, {"id": "2103.12312", "submitter": "Jingxuan Tu", "authors": "Jingxuan Tu and Constantine Lignos", "title": "TMR: Evaluating NER Recall on Tough Mentions", "comments": "To appear in the 2021 EACL Student Research Workshop (SRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Tough Mentions Recall (TMR) metrics to supplement traditional\nnamed entity recognition (NER) evaluation by examining recall on specific\nsubsets of \"tough\" mentions: unseen mentions, those whose tokens or token/type\ncombination were not observed in training, and type-confusable mentions, token\nsequences with multiple entity types in the test data. We demonstrate the\nusefulness of these metrics by evaluating corpora of English, Spanish, and\nDutch using five recent neural architectures. We identify subtle differences\nbetween the performance of BERT and Flair on two English NER corpora and\nidentify a weak spot in the performance of current models in Spanish. We\nconclude that the TMR metrics enable differentiation between otherwise\nsimilar-scoring systems and identification of patterns in performance that\nwould go unnoticed from overall precision, recall, and F1.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:04:14 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tu", "Jingxuan", ""], ["Lignos", "Constantine", ""]]}, {"id": "2103.12360", "submitter": "Shivani Kumar", "authors": "Shivani Kumar, Anubhav Shrimal, Md Shad Akhtar, Tanmoy Chakraborty", "title": "Discovering Emotion and Reasoning its Flip in Multi-Party Conversations\n  using Masked Memory Network and Transformer", "comments": "32 pages, 3 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient discovery of emotion states of speakers in a multi-party\nconversation is highly important to design human-like conversational agents.\nDuring the conversation, the cognitive state of a speaker often alters due to\ncertain past utterances, which may lead to a flip in her emotion state.\nTherefore, discovering the reasons (triggers) behind one's emotion flip during\nconversation is important to explain the emotion labels of individual\nutterances. In this paper, along with addressing the task of emotion\nrecognition in conversations (ERC), we introduce a novel task -- Emotion Flip\nReasoning (EFR) that aims to identify past utterances which have triggered\none's emotion state to flip at a certain time. We propose a masked memory\nnetwork to address the former and a Transformer-based network for the latter\ntask. To this end, we consider MELD, a benchmark emotion recognition dataset in\nmulti-party conversations for the task of ERC and augment it with new\nground-truth labels for EFR. An extensive comparison with four state-of-the-art\nmodels suggests improved performances of our models for both the tasks. We\nfurther present anecdotal evidences and both qualitative and quantitative error\nanalyses to support the superiority of our models compared to the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 07:42:09 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 09:17:35 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kumar", "Shivani", ""], ["Shrimal", "Anubhav", ""], ["Akhtar", "Md Shad", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2103.12377", "submitter": "Md Shad Akhtar Dr.", "authors": "Shraman Pramanick, Md Shad Akhtar, Tanmoy Chakraborty", "title": "Exercise? I thought you said 'Extra Fries': Leveraging Sentence\n  Demarcations and Multi-hop Attention for Meme Affect Analysis", "comments": "Accepted for publication in ICWSM-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Internet is awash in memes as they are humorous, satirical, or ironic\nwhich make people laugh. According to a survey, 33% of social media users in\nage bracket [13-35] send memes every day, whereas more than 50% send every\nweek. Some of these memes spread rapidly within a very short time-frame, and\ntheir virality depends on the novelty of their (textual and visual) content. A\nfew of them convey positive messages, such as funny or motivational quotes;\nwhile others are meant to mock/hurt someone's feelings through sarcastic or\noffensive messages. Despite the appealing nature of memes and their rapid\nemergence on social media, effective analysis of memes has not been adequately\nattempted to the extent it deserves.\n  In this paper, we attempt to solve the same set of tasks suggested in the\nSemEval'20-Memotion Analysis competition. We propose a multi-hop\nattention-based deep neural network framework, called MHA-MEME, whose prime\nobjective is to leverage the spatial-domain correspondence between the visual\nmodality (an image) and various textual segments to extract fine-grained\nfeature representations for classification. We evaluate MHA-MEME on the\n'Memotion Analysis' dataset for all three sub-tasks - sentiment classification,\naffect classification, and affect class quantification. Our comparative study\nshows sota performances of MHA-MEME for all three tasks compared to the top\nsystems that participated in the competition. Unlike all the baselines which\nperform inconsistently across all three tasks, MHA-MEME outperforms baselines\nin all the tasks on average. Moreover, we validate the generalization of\nMHA-MEME on another set of manually annotated test samples and observe it to be\nconsistent. Finally, we establish the interpretability of MHA-MEME.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:21:37 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pramanick", "Shraman", ""], ["Akhtar", "Md Shad", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2103.12407", "submitter": "Rohan Alexander", "authors": "Ke-Li Chiu and Rohan Alexander", "title": "Detecting Hate Speech with GPT-3", "comments": "15 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist? We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 is able to identify sexist or racist text with an\naccuracy between 48 per cent and 69 per cent. With few-shot learning and an\ninstruction included in the prompt, the model's accuracy can be as high as 78\nper cent. We conclude that large language models have a role to play in hate\nspeech detection, and that with further development language models could be\nused to counter hate speech and even self-police.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:17:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chiu", "Ke-Li", ""], ["Alexander", "Rohan", ""]]}, {"id": "2103.12412", "submitter": "Prashant Kapil", "authors": "Prashant Kapil, Asif Ekbal", "title": "Leveraging Multi-domain, Heterogeneous Data using Deep Multitask\n  Learning for Hate Speech Detection", "comments": "10 pages, 2 figures, 13 tables. Accepted at THE SEVENTEENTH\n  INTERNATIONAL CONFERENCE ON NATURAL LANGUAGE PROCESSING (ICON) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the exponential rise in user-generated web content on social media, the\nproliferation of abusive languages towards an individual or a group across the\ndifferent sections of the internet is also rapidly increasing. It is very\nchallenging for human moderators to identify the offensive contents and filter\nthose out. Deep neural networks have shown promise with reasonable accuracy for\nhate speech detection and allied applications. However, the classifiers are\nheavily dependent on the size and quality of the training data. Such a\nhigh-quality large data set is not easy to obtain. Moreover, the existing data\nsets that have emerged in recent times are not created following the same\nannotation guidelines and are often concerned with different types and\nsub-types related to hate. To solve this data sparsity problem, and to obtain\nmore global representative features, we propose a Convolution Neural Network\n(CNN) based multi-task learning models (MTLs)\\footnote{code is available at\nhttps://github.com/imprasshant/STL-MTL} to leverage information from multiple\nsources. Empirical analysis performed on three benchmark datasets shows the\nefficacy of the proposed approach with the significant improvement in accuracy\nand F-score to obtain state-of-the-art performance with respect to the existing\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:31:01 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kapil", "Prashant", ""], ["Ekbal", "Asif", ""]]}, {"id": "2103.12428", "submitter": "Mali Jin", "authors": "Mali Jin and Nikolaos Aletras", "title": "Modeling the Severity of Complaints in Social Media", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speech act of complaining is used by humans to communicate a negative\nmismatch between reality and expectations as a reaction to an unfavorable\nsituation. Linguistic theory of pragmatics categorizes complaints into various\nseverity levels based on the face-threat that the complainer is willing to\nundertake. This is particularly useful for understanding the intent of\ncomplainers and how humans develop suitable apology strategies. In this paper,\nwe study the severity level of complaints for the first time in computational\nlinguistics. To facilitate this, we enrich a publicly available data set of\ncomplaints with four severity categories and train different transformer-based\nnetworks combined with linguistic information achieving 55.7 macro F1. We also\njointly model binary complaint classification and complaint severity in a\nmulti-task setting achieving new state-of-the-art results on binary complaint\ndetection reaching up to 88.2 macro F1. Finally, we present a qualitative\nanalysis of the behavior of our models in predicting complaint severity levels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:13:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Jin", "Mali", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2103.12440", "submitter": "Florian Boudin", "authors": "Florian Boudin and Ygor Gallina", "title": "Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural keyphrase generation models have recently attracted much interest due\nto their ability to output absent keyphrases, that is, keyphrases that do not\nappear in the source text. In this paper, we discuss the usefulness of absent\nkeyphrases from an Information Retrieval (IR) perspective, and show that the\ncommonly drawn distinction between present and absent keyphrases is not made\nexplicit enough. We introduce a finer-grained categorization scheme that sheds\nmore light on the impact of absent keyphrases on scientific document retrieval.\nUnder this scheme, we find that only a fraction (around 20%) of the words that\nmake up keyphrases actually serves as document expansion, but that this small\nfraction of words is behind much of the gains observed in retrieval\neffectiveness. We also discuss how the proposed scheme can offer a new angle to\nevaluate the output of neural keyphrase generation models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:42:18 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 14:10:40 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Boudin", "Florian", ""], ["Gallina", "Ygor", ""]]}, {"id": "2103.12450", "submitter": "Jan Philip Wahle", "authors": "Jan Philip Wahle, Terry Ruas, Norman Meuschke, Bela Gipp", "title": "Are Neural Language Models Good Plagiarists? A Benchmark for Neural\n  Paraphrase Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:01:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wahle", "Jan Philip", ""], ["Ruas", "Terry", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2103.12506", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Husrev Taha Sencar, Jisun An, Haewoon Kwak", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "comments": "factuality of reporting, fact-checking, political ideology, media\n  bias, disinformation, propaganda, social media, news media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present level of proliferation of fake, biased, and propagandistic\ncontent online has made it impossible to fact-check every single suspicious\nclaim or article, either manually or automatically. Thus, many researchers are\nshifting their attention to higher granularity, aiming to profile entire news\noutlets, which makes it possible to detect likely \"fake news\" the moment it is\npublished, by simply checking the reliability of its source. Source factuality\nis also an important element of systems for automatic fact-checking and \"fake\nnews\" detection, as they need to assess the reliability of the evidence they\nretrieve online. Political bias detection, which in the Western political\nlandscape is about predicting left-center-right bias, is an equally important\ntopic, which has experienced a similar shift towards profiling entire news\noutlets. Moreover, there is a clear connection between the two, as highly\nbiased media are less likely to be factual; yet, the two problems have been\naddressed separately. In this survey, we review the state of the art on media\nprofiling for factuality and bias, arguing for the need to model them jointly.\nWe further discuss interesting recent advances in using different information\nsources and modalities, which go beyond the text of the articles the target\nnews outlet has published. Finally, we discuss current challenges and outline\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:11:54 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Nakov", "Preslav", ""], ["Sencar", "Husrev Taha", ""], ["An", "Jisun", ""], ["Kwak", "Haewoon", ""]]}, {"id": "2103.12528", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal,\n  Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, Fabio\n  Petroni", "title": "Multilingual Autoregressive Entity Linking", "comments": "20 pages, 8 figures, and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present mGENRE, a sequence-to-sequence system for the Multilingual Entity\nLinking (MEL) problem -- the task of resolving language-specific mentions to a\nmultilingual Knowledge Base (KB). For a mention in a given language, mGENRE\npredicts the name of the target entity left-to-right, token-by-token in an\nautoregressive fashion. The autoregressive formulation allows us to effectively\ncross-encode mention string and entity names to capture more interactions than\nthe standard dot product between mention and entity vectors. It also enables\nfast search within a large KB even for mentions that do not appear in mention\ntables and with no need for large-scale vector indices. While prior MEL works\nuse a single representation for each entity, we match against entity names of\nas many languages as possible, which allows exploiting language connections\nbetween source input and target name. Moreover, in a zero-shot setting on\nlanguages with no training data at all, mGENRE treats the target language as a\nlatent variable that is marginalized at prediction time. This leads to over 50%\nimprovements in average accuracy. We show the efficacy of our approach through\nextensive evaluation including experiments on three popular MEL benchmarks\nwhere mGENRE establishes new state-of-the-art results. Code and pre-trained\nmodels at https://github.com/facebookresearch/GENRE.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:25:55 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["De Cao", "Nicola", ""], ["Wu", "Ledell", ""], ["Popat", "Kashyap", ""], ["Artetxe", "Mikel", ""], ["Goyal", "Naman", ""], ["Plekhanov", "Mikhail", ""], ["Zettlemoyer", "Luke", ""], ["Cancedda", "Nicola", ""], ["Riedel", "Sebastian", ""], ["Petroni", "Fabio", ""]]}, {"id": "2103.12541", "submitter": "Preslav Nakov", "authors": "Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri,\n  Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed Firooz,\n  Preslav Nakov", "title": "A Survey on Multimodal Disinformation Detection", "comments": "disinformation, misinformation, factuality, harmfulness, fake news,\n  propaganda, multimodality, text, images, videos, network structure,\n  temporality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CL cs.CR cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the proliferation of fake news, propaganda,\nmisinformation, and disinformation online. While initially this was mostly\nabout textual content, over time images and videos gained popularity, as they\nare much easier to consume, attract much more attention, and spread further\nthan simple text. As a result, researchers started targeting different\nmodalities and combinations thereof. As different modalities are studied in\ndifferent research communities, with insufficient interaction, here we offer a\nsurvey that explores the state-of-the-art on multimodal disinformation\ndetection covering various combinations of modalities: text, images, audio,\nvideo, network structure, and temporal information. Moreover, while some\nstudies focused on factuality, others investigated how harmful the content is.\nWhile these two components in the definition of disinformation -- (i)\nfactuality and (ii) harmfulness, are equally important, they are typically\nstudied in isolation. Thus, we argue for the need to tackle disinformation\ndetection by taking into account multiple modalities as well as both factuality\nand harmfulness, in the same framework. Finally, we discuss current challenges\nand future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:04:17 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Alam", "Firoj", ""], ["Cresci", "Stefano", ""], ["Chakraborty", "Tanmoy", ""], ["Silvestri", "Fabrizio", ""], ["Dimitrov", "Dimiter", ""], ["Martino", "Giovanni Da San", ""], ["Shaar", "Shaden", ""], ["Firooz", "Hamed", ""], ["Nakov", "Preslav", ""]]}, {"id": "2103.12615", "submitter": "Hongru Liang", "authors": "Hongru Liang, Haozheng Wang, Qian Li, Jun Wang, Guandong Xu, Jiawei\n  Chen, Jin-Mao Wei, Zhenglu Yang", "title": "A General Framework for Learning Prosodic-Enhanced Representation of Rap\n  Lyrics", "comments": null, "journal-ref": null, "doi": "10.1007/s11280-019-00672-2", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning and analyzing rap lyrics is a significant basis for many web\napplications, such as music recommendation, automatic music categorization, and\nmusic information retrieval, due to the abundant source of digital music in the\nWorld Wide Web. Although numerous studies have explored the topic, knowledge in\nthis field is far from satisfactory, because critical issues, such as prosodic\ninformation and its effective representation, as well as appropriate\nintegration of various features, are usually ignored. In this paper, we propose\na hierarchical attention variational autoencoder framework (HAVAE), which\nsimultaneously consider semantic and prosodic features for rap lyrics\nrepresentation learning. Specifically, the representation of the prosodic\nfeatures is encoded by phonetic transcriptions with a novel and effective\nstrategy~(i.e., rhyme2vec). Moreover, a feature aggregation strategy is\nproposed to appropriately integrate various features and generate\nprosodic-enhanced representation. A comprehensive empirical evaluation\ndemonstrates that the proposed framework outperforms the state-of-the-art\napproaches under various metrics in different rap lyrics learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:13:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Liang", "Hongru", ""], ["Wang", "Haozheng", ""], ["Li", "Qian", ""], ["Wang", "Jun", ""], ["Xu", "Guandong", ""], ["Chen", "Jiawei", ""], ["Wei", "Jin-Mao", ""], ["Yang", "Zhenglu", ""]]}, {"id": "2103.12693", "submitter": "Jacopo Staiano", "authors": "Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier,\n  Benjamin Piwowarski, Jacopo Staiano, Alex Wang", "title": "QuestEval: Summarization Asks for Fact-based Evaluation", "comments": "project page: https://github.com/recitalAI/QuestEval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization evaluation remains an open research problem: current metrics\nsuch as ROUGE are known to be limited and to correlate poorly with human\njudgments. To alleviate this issue, recent work has proposed evaluation metrics\nwhich rely on question answering models to assess whether a summary contains\nall the relevant information in its source document. Though promising, the\nproposed approaches have so far failed to correlate better than ROUGE with\nhuman judgments.\n  In this paper, we extend previous approaches and propose a unified framework,\nnamed QuestEval. In contrast to established metrics such as ROUGE or BERTScore,\nQuestEval does not require any ground-truth reference. Nonetheless, QuestEval\nsubstantially improves the correlation with human judgments over four\nevaluation dimensions (consistency, coherence, fluency, and relevance), as\nshown in the extensive experiments we report.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:16:09 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:29:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Scialom", "Thomas", ""], ["Dray", "Paul-Alexis", ""], ["Gallinari", "Patrick", ""], ["Lamprier", "Sylvain", ""], ["Piwowarski", "Benjamin", ""], ["Staiano", "Jacopo", ""], ["Wang", "Alex", ""]]}, {"id": "2103.12703", "submitter": "Alexander Ku", "authors": "Alexander Ku and Peter Anderson and Jordi Pont-Tuset and Jason\n  Baldridge", "title": "PanGEA: The Panoramic Graph Environment Annotation Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight\ntoolkit for collecting speech and text annotations in photo-realistic 3D\nenvironments. PanGEA immerses annotators in a web-based simulation and allows\nthem to move around easily as they speak and/or listen. It includes database\nand cloud storage integration, plus utilities for automatically aligning\nrecorded speech with manual transcriptions and the virtual pose of the\nannotators. Out of the box, PanGEA supports two tasks -- collecting navigation\ninstructions and navigation instruction following -- and it could be easily\nadapted for annotating walking tours, finding and labeling landmarks or\nobjects, and similar tasks. We share best practices learned from using PanGEA\nin a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We\nhope that our open-source annotation toolkit and insights will both expedite\nfuture data collection efforts and spur innovation on the kinds of grounded\nlanguage tasks such environments can support.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:24:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ku", "Alexander", ""], ["Anderson", "Peter", ""], ["Pont-Tuset", "Jordi", ""], ["Baldridge", "Jason", ""]]}, {"id": "2103.12777", "submitter": "Sonal Garg", "authors": "Sonal Garg, Sumanth Prabhu, Hemant Misra, and G. Srinivasaraghavan", "title": "Unsupervised Contextual Paraphrase Generation using Lexical Control and\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer support via chat requires agents to resolve customer queries with\nminimum wait time and maximum customer satisfaction. Given that the agents as\nwell as the customers can have varying levels of literacy, the overall quality\nof responses provided by the agents tend to be poor if they are not predefined.\nBut using only static responses can lead to customer detraction as the\ncustomers tend to feel that they are no longer interacting with a human. Hence,\nit is vital to have variations of the static responses to reduce monotonicity\nof the responses. However, maintaining a list of such variations can be\nexpensive. Given the conversation context and the agent response, we propose an\nunsupervised frame-work to generate contextual paraphrases using autoregressive\nmodels. We also propose an automated metric based on Semantic Similarity,\nTextual Entailment, Expression Diversity and Fluency to evaluate the quality of\ncontextual paraphrases and demonstrate performance improvement with\nReinforcement Learning (RL) fine-tuning using the automated metric as the\nreward function.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:22:03 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Garg", "Sonal", ""], ["Prabhu", "Sumanth", ""], ["Misra", "Hemant", ""], ["Srinivasaraghavan", "G.", ""]]}, {"id": "2103.12801", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee, Kuntal Kumar Pal, Fish Wang, Chitta Baral", "title": "Variable Name Recovery in Decompiled Binary Code using Constrained\n  Masked Language Modeling", "comments": "Work In Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decompilation is the procedure of transforming binary programs into a\nhigh-level representation, such as source code, for human analysts to examine.\nWhile modern decompilers can reconstruct and recover much information that is\ndiscarded during compilation, inferring variable names is still extremely\ndifficult. Inspired by recent advances in natural language processing, we\npropose a novel solution to infer variable names in decompiled code based on\nMasked Language Modeling, Byte-Pair Encoding, and neural architectures such as\nTransformers and BERT. Our solution takes \\textit{raw} decompiler output, the\nless semantically meaningful code, as input, and enriches it using our proposed\n\\textit{finetuning} technique, Constrained Masked Language Modeling. Using\nConstrained Masked Language Modeling introduces the challenge of predicting the\nnumber of masked tokens for the original variable name. We address this\n\\textit{count of token prediction} challenge with our post-processing\nalgorithm. Compared to the state-of-the-art approaches, our trained VarBERT\nmodel is simpler and of much better performance. We evaluated our model on an\nexisting large-scale data set with 164,632 binaries and showed that it can\npredict variable names identical to the ones present in the original source\ncode up to 84.15\\% of the time.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 19:09:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Pal", "Kuntal Kumar", ""], ["Wang", "Fish", ""], ["Baral", "Chitta", ""]]}, {"id": "2103.12838", "submitter": "Reid Pryzant", "authors": "Reid Pryzant", "title": "Repairing Pronouns in Translation with BERT-Based Post-Editing", "comments": "Authors were unable to get full test set results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pronouns are important determinants of a text's meaning but difficult to\ntranslate. This is because pronoun choice can depend on entities described in\nprevious sentences, and in some languages pronouns may be dropped when the\nreferent is inferrable from the context. These issues can lead Neural Machine\nTranslation (NMT) systems to make critical errors on pronouns that impair\nintelligibility and even reinforce gender bias. We investigate the severity of\nthis pronoun issue, showing that (1) in some domains, pronoun choice can\naccount for more than half of a NMT systems' errors, and (2) pronouns have a\ndisproportionately large impact on perceived translation quality. We then\ninvestigate a possible solution: fine-tuning BERT on a pronoun prediction task\nusing chunks of source-side sentences, then using the resulting classifier to\nrepair the translations of an existing NMT model. We offer an initial case\nstudy of this approach for the Japanese-English language pair, observing that a\nsmall number of translations are significantly improved according to human\nevaluators.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:01:03 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 13:21:09 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 18:11:53 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 00:41:37 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pryzant", "Reid", ""]]}, {"id": "2103.12841", "submitter": "Arianna Betti", "authors": "Yvette Oortwijn, Hein van den Berg, and Arianna Betti", "title": "Ground Truths for the Humanities", "comments": "Provocation paper presented at the 5th Workshop on Visualization for\n  the Digital Humanities (VIS4DH), part of the 31st IEEE Visualization\n  Conference, IEEE VIS 2020, Virtual Event, Salt Lake City, USA, October 25-30,\n  2020. 1 page + 1 page of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring a faithful interaction with data and its representation for\nhumanities can and should depend on expert-constructed ground truths.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:05:42 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Oortwijn", "Yvette", ""], ["Berg", "Hein van den", ""], ["Betti", "Arianna", ""]]}, {"id": "2103.12872", "submitter": "Stella Biderman", "authors": "Louis Castricato and Stella Biderman and Rogelio E. Cardona-Rivera and\n  David Thue", "title": "Towards a Formal Model of Narratives", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the beginnings of a formal framework for modeling\nnarrative \\textit{qua} narrative. Our framework affords the ability to discuss\nkey qualities of stories and their communication, including the flow of\ninformation from a Narrator to a Reader, the evolution of a Reader's story\nmodel over time, and Reader uncertainty. We demonstrate its applicability to\ncomputational narratology by giving explicit algorithms for measuring the\naccuracy with which information was conveyed to the Reader and two novel\nmeasurements of story coherence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:33:23 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Castricato", "Louis", ""], ["Biderman", "Stella", ""], ["Cardona-Rivera", "Rogelio E.", ""], ["Thue", "David", ""]]}, {"id": "2103.12876", "submitter": "Chen Zhao", "authors": "Chen Zhao, Chenyan Xiong, Xin Qian and Jordan Boyd-Graber", "title": "Complex Factoid Question Answering with a Free-Text Knowledge Graph", "comments": "WWW2020", "journal-ref": null, "doi": "10.1145/3366423.3380197", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce DELFT, a factoid question answering system which combines the\nnuance and depth of knowledge graph question answering approaches with the\nbroader coverage of free-text. DELFT builds a free-text knowledge graph from\nWikipedia, with entities as nodes and sentences in which entities co-occur as\nedges. For each question, DELFT finds the subgraph linking question entity\nnodes to candidates using text sentences as edges, creating a dense and high\ncoverage semantic graph. A novel graph neural network reasons over the\nfree-text graph-combining evidence on the nodes via information along edge\nsentences-to select a final answer. Experiments on three question answering\ndatasets show DELFT can answer entity-rich questions better than machine\nreading based models, bert-based answer ranking and memory networks. DELFT's\nadvantage comes from both the high coverage of its free-text knowledge\ngraph-more than double that of dbpedia relations-and the novel graph neural\nnetwork which reasons on the rich but noisy free-text evidence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:53:09 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhao", "Chen", ""], ["Xiong", "Chenyan", ""], ["Qian", "Xin", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2103.12882", "submitter": "Andreas Hamm", "authors": "Andreas Hamm, Jana Thelen, Rasmus Beckmann, Simon Odrowski (German\n  Aerospace Center DLR)", "title": "TeCoMiner: Topic Discovery Through Term Community Detection", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This note is a short description of TeCoMiner, an interactive tool for\nexploring the topic content of text collections. Unlike other topic modeling\ntools, TeCoMiner is not based on some generative probabilistic model but on\ntopological considerations about co-occurrence networks of terms. We outline\nthe methods used for identifying topics, describe the features of the tool, and\nsketch an application, using a corpus of policy related scientific news on\nenvironmental issues published by the European Commission over the last decade.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 23:08:46 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hamm", "Andreas", "", "German\n  Aerospace Center DLR"], ["Thelen", "Jana", "", "German\n  Aerospace Center DLR"], ["Beckmann", "Rasmus", "", "German\n  Aerospace Center DLR"], ["Odrowski", "Simon", "", "German\n  Aerospace Center DLR"]]}, {"id": "2103.12906", "submitter": "Sheshera Mysore", "authors": "Sheshera Mysore, Tim O'Gorman, Andrew McCallum, Hamed Zamani", "title": "CSFCube -- A Test Collection of Computer Science Research Articles for\n  Faceted Query by Example", "comments": "Submitted for single-blind review at the CIKM 2021 Resource Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate backgrounds, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents, drawn from computational linguistics and machine learning\nvenues. We carefully followed the annotation guideline used by TREC for depth-k\npooling (k = 100 or 250) and the resulting data collection consists of graded\nrelevance scores with high annotation agreement. The data is freely available\nfor research purposes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:02:12 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 17:35:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mysore", "Sheshera", ""], ["O'Gorman", "Tim", ""], ["McCallum", "Andrew", ""], ["Zamani", "Hamed", ""]]}, {"id": "2103.12953", "submitter": "Dejiao Zhang", "authors": "Dejiao Zhang, Feng Nan, Xiaokai Wei, Shangwen Li, Henghui Zhu,\n  Kathleen McKeown, Ramesh Nallapati, Andrew Arnold, Bing Xiang", "title": "Supporting Clustering with Contrastive Learning", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised clustering aims at discovering the semantic categories of data\naccording to some distance measured in the representation space. However,\ndifferent categories often overlap with each other in the representation space\nat the beginning of the learning process, which poses a significant challenge\nfor distance-based clustering in achieving good separation between different\ncategories. To this end, we propose Supporting Clustering with Contrastive\nLearning (SCCL) -- a novel framework to leverage contrastive learning to\npromote better separation. We assess the performance of SCCL on short text\nclustering and show that SCCL significantly advances the state-of-the-art\nresults on most benchmark datasets with 3%-11% improvement on Accuracy and\n4%-15% improvement on Normalized Mutual Information. Furthermore, our\nquantitative analysis demonstrates the effectiveness of SCCL in leveraging the\nstrengths of both bottom-up instance discrimination and top-down clustering to\nachieve better intra-cluster and inter-cluster distances when evaluated with\nthe ground truth cluster labels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:05:17 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:26:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Dejiao", ""], ["Nan", "Feng", ""], ["Wei", "Xiaokai", ""], ["Li", "Shangwen", ""], ["Zhu", "Henghui", ""], ["McKeown", "Kathleen", ""], ["Nallapati", "Ramesh", ""], ["Arnold", "Andrew", ""], ["Xiang", "Bing", ""]]}, {"id": "2103.12975", "submitter": "Yining Hong", "authors": "Yining Hong, Qing Li, Song-Chun Zhu, Siyuan Huang", "title": "VLGrammar: Grounded Grammar Induction of Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive grammar suggests that the acquisition of language grammar is\ngrounded within visual structures. While grammar is an essential representation\nof natural language, it also exists ubiquitously in vision to represent the\nhierarchical part-whole structure. In this work, we study grounded grammar\ninduction of vision and language in a joint learning framework. Specifically,\nwe present VLGrammar, a method that uses compound probabilistic context-free\ngrammars (compound PCFGs) to induce the language grammar and the image grammar\nsimultaneously. We propose a novel contrastive learning framework to guide the\njoint learning of both modules. To provide a benchmark for the grounded grammar\ninduction task, we collect a large-scale dataset, \\textsc{PartIt}, which\ncontains human-written sentences that describe part-level semantics for 3D\nobjects. Experiments on the \\textsc{PartIt} dataset show that VLGrammar\noutperforms all baselines in image grammar induction and language grammar\ninduction. The learned VLGrammar naturally benefits related downstream tasks.\nSpecifically, it improves the image unsupervised clustering accuracy by 30\\%,\nand performs well in image retrieval and text retrieval. Notably, the induced\ngrammar shows superior generalizability by easily generalizing to unseen\ncategories.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:05:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hong", "Yining", ""], ["Li", "Qing", ""], ["Zhu", "Song-Chun", ""], ["Huang", "Siyuan", ""]]}, {"id": "2103.13009", "submitter": "Nicholas Lourie", "authors": "Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi", "title": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New\n  Multitask Benchmark", "comments": "27 pages, 19 figures, 34 tables. Accepted to AAAI 2021. For\n  associated code and data see https://github.com/allenai/rainbow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense AI has long been seen as a near impossible goal -- until\nrecently. Now, research interest has sharply increased with an influx of new\nbenchmarks and models.\n  We propose two new ways to evaluate commonsense models, emphasizing their\ngenerality on new tasks and building on diverse, recently introduced\nbenchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote\nresearch on commonsense models that generalize well over multiple tasks and\ndatasets. Second, we propose a novel evaluation, the cost equivalent curve,\nthat sheds new insight on how the choice of source datasets, pretrained\nlanguage models, and transfer learning methods impacts performance and data\nefficiency.\n  We perform extensive experiments -- over 200 experiments encompassing 4800\nmodels -- and report multiple valuable and sometimes surprising findings, e.g.,\nthat transfer almost always leads to better or equivalent performance if\nfollowing a particular recipe, that QA-based commonsense datasets transfer well\nwith each other, while commonsense knowledge graphs do not, and that perhaps\ncounter-intuitively, larger models benefit more from transfer than smaller\nones.\n  Last but not least, we introduce a new universal commonsense reasoning model,\nUNICORN, that establishes new state-of-the-art performance across 8 popular\ncommonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA\n(90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA\n(79.3%).\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:32:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lourie", "Nicholas", ""], ["Bras", "Ronan Le", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""]]}, {"id": "2103.13019", "submitter": "Christof Sch\\\"och", "authors": "Christof Sch\\\"och", "title": "Topic Modeling Genre: An Exploration of French Classical and\n  Enlightenment Drama", "comments": "11 figures", "journal-ref": "Digital Humanities Quarterly, 11.2, 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of literary genre is a highly complex one: not only are different\ngenres frequently defined on several, but not necessarily the same levels of\ndescription, but consideration of genres as cognitive, social, or scholarly\nconstructs with a rich history further complicate the matter. This contribution\nfocuses on thematic aspects of genre with a quantitative approach, namely Topic\nModeling. Topic Modeling has proven to be useful to discover thematic patterns\nand trends in large collections of texts, with a view to class or browse them\non the basis of their dominant themes. It has rarely if ever, however, been\napplied to collections of dramatic texts.\n  In this contribution, Topic Modeling is used to analyze a collection of\nFrench Drama of the Classical Age and the Enlightenment. The general aim of\nthis contribution is to discover what semantic types of topics are found in\nthis collection, whether different dramatic subgenres have distinctive dominant\ntopics and plot-related topic patterns, and inversely, to what extent\nclustering methods based on topic scores per play produce groupings of texts\nwhich agree with more conventional genre distinctions. This contribution shows\nthat interesting topic patterns can be detected which provide new insights into\nthe thematic, subgenre-related structure of French drama as well as into the\nhistory of French drama of the Classical Age and the Enlightenment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:57:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sch\u00f6ch", "Christof", ""]]}, {"id": "2103.13031", "submitter": "Jakub Sido", "authors": "Jakub Sido, Ond\\v{r}ej Pra\\v{z}\\'ak, Pavel P\\v{r}ib\\'a\\v{n}, Jan\n  Pa\\v{s}ek, Michal Sej\\'ak, Miloslav Konop\\'ik", "title": "Czert -- Czech BERT-like Model for Language Representation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the training process of the first Czech monolingual\nlanguage representation models based on BERT and ALBERT architectures. We\npre-train our models on more than 340K of sentences, which is 50 times more\nthan multilingual models that include Czech data. We outperform the\nmultilingual models on 9 out of 11 datasets. In addition, we establish the new\nstate-of-the-art results on nine datasets. At the end, we discuss properties of\nmonolingual and multilingual models based upon our results. We publish all the\npre-trained and fine-tuned models freely for the research community.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:27:28 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 14:49:45 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Sido", "Jakub", ""], ["Pra\u017e\u00e1k", "Ond\u0159ej", ""], ["P\u0159ib\u00e1\u0148", "Pavel", ""], ["Pa\u0161ek", "Jan", ""], ["Sej\u00e1k", "Michal", ""], ["Konop\u00edk", "Miloslav", ""]]}, {"id": "2103.13033", "submitter": "Gregor Betz", "authors": "Gregor Betz and Kyle Richardson and Christian Voigt", "title": "Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning\n  Performance of GPT-2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Thinking aloud is an effective meta-cognitive strategy human reasoners apply\nto solve difficult problems. We suggest to improve the reasoning ability of\npre-trained neural language models in a similar way, namely by expanding a\ntask's context with problem elaborations that are dynamically generated by the\nlanguage model itself. Our main result is that dynamic problem elaboration\nsignificantly improves the zero-shot performance of GPT-2 in a deductive\nreasoning and natural language inference task: While the model uses a syntactic\nheuristic for predicting an answer, it is capable (to some degree) of\ngenerating reasoned additional context which facilitates the successful\napplication of its heuristic. We explore different ways of generating\nelaborations, including fewshot learning, and find that their relative\nperformance varies with the specific problem characteristics (such as problem\ndifficulty). Moreover, the effectiveness of an elaboration can be explained in\nterms of the degree to which the elaboration semantically coheres with the\ncorresponding problem. In particular, elaborations that are most faithful to\nthe original problem description may boost accuracy by up to 24%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:33:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Betz", "Gregor", ""], ["Richardson", "Kyle", ""], ["Voigt", "Christian", ""]]}, {"id": "2103.13076", "submitter": "Jungo Kasai", "authors": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco,\n  Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith", "title": "Finetuning Pretrained Transformers into RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers have outperformed recurrent neural networks (RNNs) in natural\nlanguage generation. This comes with a significant computational overhead, as\nthe attention mechanism scales with a quadratic complexity in sequence length.\nEfficient transformer variants have received increasing interest from recent\nworks. Among them, a linear-complexity recurrent variant has proven well suited\nfor autoregressive generation. It approximates the softmax attention with\nrandomized or heuristic feature maps, but can be difficult to train or yield\nsuboptimal accuracy. This work aims to convert a pretrained transformer into\nits efficient recurrent counterpart, improving the efficiency while retaining\nthe accuracy. Specifically, we propose a swap-then-finetune procedure: in an\noff-the-shelf pretrained transformer, we replace the softmax attention with its\nlinear-complexity recurrent alternative and then finetune. With a learned\nfeature map, our approach provides an improved tradeoff between efficiency and\naccuracy over the standard transformer and other recurrent variants. We also\nshow that the finetuning process needs lower training cost than training these\nrecurrent variants from scratch. As many recent models for natural language\ntasks are increasingly dependent on large-scale pretrained transformers, this\nwork presents a viable approach to improving inference efficiency without\nrepeating the expensive pretraining process.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:50:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kasai", "Jungo", ""], ["Peng", "Hao", ""], ["Zhang", "Yizhe", ""], ["Yogatama", "Dani", ""], ["Ilharco", "Gabriel", ""], ["Pappas", "Nikolaos", ""], ["Mao", "Yi", ""], ["Chen", "Weizhu", ""], ["Smith", "Noah A.", ""]]}, {"id": "2103.13084", "submitter": "Ilias Chalkidis", "authors": "Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos\n  Aletras, Ion Androutsopoulos and Prodromos Malakasiotis", "title": "Paragraph-level Rationale Extraction through Regularization: A case\n  study on European Court of Human Rights Cases", "comments": "9 pages, long paper at NAACL 2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Interpretability or explainability is an emerging research field in NLP. From\na user-centric point of view, the goal is to build models that provide proper\njustification for their decisions, similar to those of humans, by requiring the\nmodels to satisfy additional constraints. To this end, we introduce a new\napplication on legal text where, contrary to mainstream literature targeting\nword-level rationales, we conceive rationales as selected paragraphs in\nmulti-paragraph structured court cases. We also release a new dataset\ncomprising European Court of Human Rights cases, including annotations for\nparagraph-level rationales. We use this dataset to study the effect of already\nproposed rationale constraints, i.e., sparsity, continuity, and\ncomprehensiveness, formulated as regularizers. Our findings indicate that some\nof these constraints are not beneficial in paragraph-level rationale\nextraction, while others need re-formulation to better handle the multi-label\nnature of the task we consider. We also introduce a new constraint,\nsingularity, which further improves the quality of rationales, even compared\nwith noisy rationale supervision. Experimental results indicate that the newly\nintroduced task is very challenging and there is a large scope for further\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:58:39 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chalkidis", "Ilias", ""], ["Fergadiotis", "Manos", ""], ["Tsarapatsanis", "Dimitrios", ""], ["Aletras", "Nikolaos", ""], ["Androutsopoulos", "Ion", ""], ["Malakasiotis", "Prodromos", ""]]}, {"id": "2103.13103", "submitter": "Filip Ginter", "authors": "Jenna Kanerva, Filip Ginter, Li-Hsin Chang, Iiro Rastas, Valtteri\n  Skantsi, Jemina Kilpel\\\"ainen, Hanna-Mari Kupari, Jenna Saarni, Maija\n  Sev\\'on, Otto Tarkka", "title": "Finnish Paraphrase Corpus", "comments": "Accepted to NoDaLiDa 2021, data:\n  https://github.com/TurkuNLP/Turku-paraphrase-corpus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we introduce the first fully manually annotated paraphrase\ncorpus for Finnish containing 53,572 paraphrase pairs harvested from\nalternative subtitles and news headings. Out of all paraphrase pairs in our\ncorpus 98% are manually classified to be paraphrases at least in their given\ncontext, if not in all contexts. Additionally, we establish a manual candidate\nselection method and demonstrate its feasibility in high quality paraphrase\nselection in terms of both cost and quality.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 11:24:10 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kanerva", "Jenna", ""], ["Ginter", "Filip", ""], ["Chang", "Li-Hsin", ""], ["Rastas", "Iiro", ""], ["Skantsi", "Valtteri", ""], ["Kilpel\u00e4inen", "Jemina", ""], ["Kupari", "Hanna-Mari", ""], ["Saarni", "Jenna", ""], ["Sev\u00f3n", "Maija", ""], ["Tarkka", "Otto", ""]]}, {"id": "2103.13136", "submitter": "Avijit Thawani", "authors": "Avijit Thawani, Jay Pujara, Pedro A. Szekely, Filip Ilievski", "title": "Representing Numbers in NLP: a Survey and a Vision", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  NLP systems rarely give special consideration to numbers found in text. This\nstarkly contrasts with the consensus in neuroscience that, in the brain,\nnumbers are represented differently from words. We arrange recent NLP work on\nnumeracy into a comprehensive taxonomy of tasks and methods. We break down the\nsubjective notion of numeracy into 7 subtasks, arranged along two dimensions:\ngranularity (exact vs approximate) and units (abstract vs grounded). We analyze\nthe myriad representational choices made by 18 previously published number\nencoders and decoders. We synthesize best practices for representing numbers in\ntext and articulate a vision for holistic numeracy in NLP, comprised of design\ntrade-offs and a unified evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:28:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Thawani", "Avijit", ""], ["Pujara", "Jay", ""], ["Szekely", "Pedro A.", ""], ["Ilievski", "Filip", ""]]}, {"id": "2103.13166", "submitter": "Fernando Alves", "authors": "Fernando C. Alves", "title": "Language learnability in the limit for general metrics: a Gold-Angluin\n  result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his pioneering work in the field of Inductive Inference, Gold (1967)\nproved that a set containing all finite languages and at least one infinite\nlanguage over the same fixed alphabet is not learnable in the exact sense.\nWithin the same framework, Angluin (1980) provided a complete characterization\nfor the learnability of language families. Mathematically, the concept of exact\nlearning in that classical setting can be seen as the use of a particular type\nof metric for learning in the limit. In this short research note we use\nNiyogi's extended version of a theorem by Blum and Blum (1975) on the existence\nof locking data sets to prove a necessary condition for learnability in the\nlimit of any family of languages in any given metric. This recovers Gold's\ntheorem as a special case. Moreover, when the language family is further\nassumed to contain all finite languages, the same condition also becomes\nsufficient for learnability in the limit.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:11:09 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Alves", "Fernando C.", ""]]}, {"id": "2103.13262", "submitter": "Jiaao He", "authors": "Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, Jie Tang", "title": "FastMoE: A Fast Mixture-of-Expert Training System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trillions of parameters. However, training trillion-scale MoE\nrequires algorithm and system co-design for a well-tuned high performance\ndistributed training system. Unfortunately, the only existing platform that\nmeets the requirements strongly depends on Google's hardware (TPU) and software\n(Mesh Tensorflow) stack, and is not open and available to the public,\nespecially GPU and PyTorch communities.\n  In this paper, we present FastMoE, a distributed MoE training system based on\nPyTorch with common accelerators. The system provides a hierarchical interface\nfor both flexible model design and easy adaption to different applications,\nsuch as Transformer-XL and Megatron-LM. Different from direct implementation of\nMoE models using PyTorch, the training speed is highly optimized in FastMoE by\nsophisticated high-performance acceleration skills. The system supports placing\ndifferent experts on multiple GPUs across multiple nodes, enabling enlarging\nthe number of experts linearly against the number of GPUs. The source of\nFastMoE is available at https://github.com/laekov/fastmoe under Apache-2\nlicense.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:27:15 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["He", "Jiaao", ""], ["Qiu", "Jiezhong", ""], ["Zeng", "Aohan", ""], ["Yang", "Zhilin", ""], ["Zhai", "Jidong", ""], ["Tang", "Jie", ""]]}, {"id": "2103.13272", "submitter": "Garry Kuwanto", "authors": "Garry Kuwanto, Afra Feyza Aky\\\"urek, Isidora Chara Tourni, Siyang Li,\n  Derry Wijaya", "title": "Low-Resource Machine Translation for Low-Resource Languages: Leveraging\n  Comparable Data, Code-Switching and Compute Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical study of unsupervised neural machine translation\n(NMT) for truly low resource languages, exploring the case when both parallel\ntraining data and compute resource are lacking, reflecting the reality of most\nof the world's languages and the researchers working on these languages. We\npropose a simple and scalable method to improve unsupervised NMT, showing how\nadding comparable data mined using a bilingual dictionary along with modest\nadditional compute resource to train the model can significantly improve its\nperformance. We also demonstrate how the use of the dictionary to code-switch\nmonolingual data to create more comparable data can further improve\nperformance. With this weak supervision, our best method achieves BLEU scores\nthat improve over supervised results for English$\\rightarrow$Gujarati (+18.88),\nEnglish$\\rightarrow$Kazakh (+5.84), and English$\\rightarrow$Somali (+1.16),\nshowing the promise of weakly-supervised NMT for many low resource languages\nwith modest compute resource in the world. To the best of our knowledge, our\nwork is the first to quantitatively showcase the impact of different modest\ncompute resource in low resource NMT.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:40:28 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kuwanto", "Garry", ""], ["Aky\u00fcrek", "Afra Feyza", ""], ["Tourni", "Isidora Chara", ""], ["Li", "Siyang", ""], ["Wijaya", "Derry", ""]]}, {"id": "2103.13275", "submitter": "Khalid Alnajjar", "authors": "Khalid Alnajjar", "title": "When Word Embeddings Become Endangered", "comments": null, "journal-ref": "In M. H\\\"am\\\"al\\\"ainen, N. Partanen, & K. Alnajjar (Eds.),\n  Multilingual Facilitation (pp. 275-288). University of Helsinki (2021)", "doi": "10.31885/9789515150257.24", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big languages such as English and Finnish have many natural language\nprocessing (NLP) resources and models, but this is not the case for\nlow-resourced and endangered languages as such resources are so scarce despite\nthe great advantages they would provide for the language communities. The most\ncommon types of resources available for low-resourced and endangered languages\nare translation dictionaries and universal dependencies. In this paper, we\npresent a method for constructing word embeddings for endangered languages\nusing existing word embeddings of different resource-rich languages and the\ntranslation dictionaries of resource-poor languages. Thereafter, the embeddings\nare fine-tuned using the sentences in the universal dependencies and aligned to\nmatch the semantic spaces of the big languages; resulting in cross-lingual\nembeddings. The endangered languages we work with here are Erzya, Moksha,\nKomi-Zyrian and Skolt Sami. Furthermore, we build a universal sentiment\nanalysis model for all the languages that are part of this study, whether\nendangered or not, by utilizing cross-lingual word embeddings. The evaluation\nconducted shows that our word embeddings for endangered languages are\nwell-aligned with the resource-rich languages, and they are suitable for\ntraining task-specific models as demonstrated by our sentiment analysis model\nwhich achieved a high accuracy. All our cross-lingual word embeddings and the\nsentiment analysis model have been released openly via an easy-to-use Python\nlibrary.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:42:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Alnajjar", "Khalid", ""]]}, {"id": "2103.13309", "submitter": "Genta Indra Winata", "authors": "Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin,\n  Andrea Madotto, Pascale Fung", "title": "Are Multilingual Models Effective in Code-Switching?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual language models have shown decent performance in multilingual\nand cross-lingual natural language understanding tasks. However, the power of\nthese multilingual models in code-switching tasks has not been fully explored.\nIn this paper, we study the effectiveness of multilingual language models to\nunderstand their capability and adaptability to the mixed-language setting by\nconsidering the inference speed, performance, and number of parameters to\nmeasure their practicality. We conduct experiments in three language pairs on\nnamed entity recognition and part-of-speech tagging and compare them with\nexisting methods, such as using bilingual embeddings and multilingual\nmeta-embeddings. Our findings suggest that pre-trained multilingual models do\nnot necessarily guarantee high-quality representations on code-switching, while\nusing meta-embeddings achieves similar results with significantly fewer\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:20:02 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Winata", "Genta Indra", ""], ["Cahyawijaya", "Samuel", ""], ["Liu", "Zihan", ""], ["Lin", "Zhaojiang", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2103.13329", "submitter": "Md. Akmal Haidar", "authors": "Md Akmal Haidar and Mehdi Rezagholizadeh", "title": "Fine-tuning of Pre-trained End-to-end Speech Recognition with Generative\n  Adversarial Networks", "comments": "Accepted in ICASSP 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adversarial training of end-to-end (E2E) ASR systems using generative\nadversarial networks (GAN) has recently been explored for low-resource ASR\ncorpora. GANs help to learn the true data representation through a two-player\nmin-max game. However, training an E2E ASR model using a large ASR corpus with\na GAN framework has never been explored, because it might take excessively long\ntime due to high-variance gradient updates and face convergence issues. In this\npaper, we introduce a novel framework for fine-tuning a pre-trained ASR model\nusing the GAN objective where the ASR model acts as a generator and a\ndiscriminator tries to distinguish the ASR output from the real data. Since the\nASR model is pre-trained, we hypothesize that the ASR model output (soft\ndistribution vectors) helps to get higher scores from the discriminator and\nmakes the task of the discriminator harder within our GAN framework, which in\nturn improves the performance of the ASR model in the fine-tuning stage. Here,\nthe pre-trained ASR model is fine-tuned adversarially against the discriminator\nusing an additional adversarial loss. Experiments on full LibriSpeech dataset\nshow that our proposed approach outperforms baselines and conventional\nGAN-based adversarial models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:40:48 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Haidar", "Md Akmal", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "2103.13426", "submitter": "Pengyu Nie", "authors": "Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Raymond J. Mooney,\n  Junyi Jessy Li, Milos Gligoric", "title": "Learning to Generate Code Comments from Class Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive code comments are essential for supporting code comprehension and\nmaintenance. We propose the task of automatically generating comments for\noverriding methods. We formulate a novel framework which accommodates the\nunique contextual and linguistic reasoning that is required for performing this\ntask. Our approach features: (1) incorporating context from the class\nhierarchy; (2) conditioning on learned, latent representations of specificity\nto generate comments that capture the more specialized behavior of the\noverriding method; and (3) unlikelihood training to discourage predictions\nwhich do not conform to invariant characteristics of the comment corresponding\nto the overridden method. Our experiments show that the proposed approach is\nable to generate comments for overriding methods of higher quality compared to\nprevailing comment generation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:12:52 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 15:36:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Jiyang", ""], ["Panthaplackel", "Sheena", ""], ["Nie", "Pengyu", ""], ["Mooney", "Raymond J.", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}, {"id": "2103.13439", "submitter": "Won Ik Cho", "authors": "Won Ik Cho, Sangwhan Moon, Jong In Kim, Seok Min Kim, Nam Soo Kim", "title": "StyleKQC: A Style-Variant Paraphrase Corpus for Korean Questions and\n  Commands", "comments": "6 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Paraphrasing is often performed with less concern for controlled style\nconversion. Especially for questions and commands, style-variant paraphrasing\ncan be crucial in tone and manner, which also matters with industrial\napplications such as dialog system. In this paper, we attack this issue with a\ncorpus construction scheme that simultaneously considers the core content and\nstyle of directives, namely intent and formality, for the Korean language.\nUtilizing manually generated natural language queries on six daily topics, we\nexpand the corpus to formal and informal sentences by human rewriting and\ntransferring. We verify the validity and industrial applicability of our\napproach by checking the adequate classification and inference performance that\nfit with the fine-tuning approaches, at the same time proposing a supervised\nformality transfer task.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:38:53 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Cho", "Won Ik", ""], ["Moon", "Sangwhan", ""], ["Kim", "Jong In", ""], ["Kim", "Seok Min", ""], ["Kim", "Nam Soo", ""]]}, {"id": "2103.13546", "submitter": "Carsten Eickhoff", "authors": "Abdullah Ahmed, Adeel Abbasi, Carsten Eickhoff", "title": "Benchmarking Modern Named Entity Recognition Techniques for Free-text\n  Health Record De-identification", "comments": "Presented at AMIA Informatics Summit 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records (EHRs) have become the primary form of medical\ndata-keeping across the United States. Federal law restricts the sharing of any\nEHR data that contains protected health information (PHI). De-identification,\nthe process of identifying and removing all PHI, is crucial for making EHR data\npublicly available for scientific research. This project explores several deep\nlearning-based named entity recognition (NER) methods to determine which\nmethod(s) perform better on the de-identification task. We trained and tested\nour models on the i2b2 training dataset, and qualitatively assessed their\nperformance using EHR data collected from a local hospital. We found that 1)\nBiLSTM-CRF represents the best-performing encoder/decoder combination, 2)\ncharacter-embeddings and CRFs tend to improve precision at the price of recall,\nand 3) transformers alone under-perform as context encoders. Future work\nfocused on structuring medical text may improve the extraction of semantic and\nsyntactic information for the purposes of EHR de-identification.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:26:58 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ahmed", "Abdullah", ""], ["Abbasi", "Adeel", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2103.13550", "submitter": "Andreas Hamm", "authors": "Andreas Hamm and Simon Odrowski (German Aerospace Center DLR)", "title": "Term-community-based topic detection with variable resolution", "comments": "31 pages, 6 figures", "journal-ref": "Information. 2021; 12(6):221", "doi": "10.3390/info12060221", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network-based procedures for topic detection in huge text collections offer\nan intuitive alternative to probabilistic topic models. We present in detail a\nmethod that is especially designed with the requirements of domain experts in\nmind. Like similar methods, it employs community detection in term\nco-occurrence graphs, but it is enhanced by including a resolution parameter\nthat can be used for changing the targeted topic granularity. We also establish\na term ranking and use semantic word-embedding for presenting term communities\nin a way that facilitates their interpretation. We demonstrate the application\nof our method with a widely used corpus of general news articles and show the\nresults of detailed social-sciences expert evaluations of detected topics at\nvarious resolutions. A comparison with topics detected by Latent Dirichlet\nAllocation is also included. Finally, we discuss factors that influence topic\ninterpretation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:29:39 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 23:26:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hamm", "Andreas", "", "German Aerospace Center DLR"], ["Odrowski", "Simon", "", "German Aerospace Center DLR"]]}, {"id": "2103.13552", "submitter": "Shunyu Yao", "authors": "Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht", "title": "Reading and Acting while Blindfolded: The Need for Semantics in Text\n  Game Agents", "comments": "NAACL 2021. Project page: https://blindfolded.cs.princeton.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-based games simulate worlds and interact with players using natural\nlanguage. Recent work has used them as a testbed for autonomous\nlanguage-understanding agents, with the motivation being that understanding the\nmeanings of words or semantics is a key component of how humans understand,\nreason, and act in these worlds. However, it remains unclear to what extent\nartificial agents utilize semantic understanding of the text. To this end, we\nperform experiments to systematically reduce the amount of semantic information\navailable to a learning agent. Surprisingly, we find that an agent is capable\nof achieving high scores even in the complete absence of language semantics,\nindicating that the currently popular experimental setup and models may be\npoorly designed to understand and leverage game texts. To remedy this\ndeficiency, we propose an inverse dynamics decoder to regularize the\nrepresentation space and encourage exploration, which shows improved\nperformance on several games including Zork I. We discuss the implications of\nour findings for designing future agents with stronger semantic understanding.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:35:27 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 18:41:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yao", "Shunyu", ""], ["Narasimhan", "Karthik", ""], ["Hausknecht", "Matthew", ""]]}, {"id": "2103.13584", "submitter": "Yutao Zhu", "authors": "Yutao Zhu, Jian-Yun Nie, Kun Zhou, Shengchao Liu, Yabo Ling, Pan Du", "title": "BERT4SO: Neural Sentence Ordering by Fine-tuning BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence ordering aims to arrange the sentences of a given text in the\ncorrect order. Recent work frames it as a ranking problem and applies deep\nneural networks to it. In this work, we propose a new method, named BERT4SO, by\nfine-tuning BERT for sentence ordering. We concatenate all sentences and\ncompute their representations by using multiple special tokens and carefully\ndesigned segment (interval) embeddings. The tokens across multiple sentences\ncan attend to each other which greatly enhances their interactions. We also\npropose a margin-based listwise ranking loss based on ListMLE to facilitate the\noptimization process. Experimental results on five benchmark datasets\ndemonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:32:32 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 02:15:47 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 03:43:59 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhu", "Yutao", ""], ["Nie", "Jian-Yun", ""], ["Zhou", "Kun", ""], ["Liu", "Shengchao", ""], ["Ling", "Yabo", ""], ["Du", "Pan", ""]]}, {"id": "2103.13587", "submitter": "Sansiri Tarnpradab", "authors": "Sansiri Tarnpradab, Fereshteh Jafariakinabad and Kien A. Hua", "title": "Improving Online Forums Summarization via Hierarchical Unified Deep\n  Neural Network", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online discussion forums are prevalent and easily accessible, thus allowing\npeople to share ideas and opinions by posting messages in the discussion\nthreads. Forum threads that significantly grow in length can become difficult\nfor participants, both newcomers and existing, to grasp main ideas. To mitigate\nthis problem, this study aims to create an automatic text summarizer for online\nforums. We present Hierarchical Unified Deep Neural Network to build sentence\nand thread representations for the forum summarization. In this scheme, Bi-LSTM\nderives a representation that comprises information of the whole sentence and\nwhole thread; whereas, CNN captures most informative features with respect to\ncontext from sentence and thread. Attention mechanism is applied on top of CNN\nto further highlight high-level representations that carry important\ninformation contributing to a desirable summary. Extensive performance\nevaluation has been conducted on three datasets, two of which are real-life\nonline forums and one is news dataset. The results reveal that the proposed\nmodel outperforms several competitive baselines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:38:56 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 20:53:03 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Tarnpradab", "Sansiri", ""], ["Jafariakinabad", "Fereshteh", ""], ["Hua", "Kien A.", ""]]}, {"id": "2103.13597", "submitter": "Zhihao Fan", "authors": "Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian\n  Jiao, Nan Duan, Ruofei Zhang, Xuanjing Huang", "title": "Mask Attention Networks: Rethinking and Strengthen Transformer", "comments": "Accepted as a long paper to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer is an attention-based neural network, which consists of two\nsublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN).\nExisting research explores to enhance the two sublayers separately to improve\nthe capability of Transformer for text representation. In this paper, we\npresent a novel understanding of SAN and FFN as Mask Attention Networks (MANs)\nand show that they are two special cases of MANs with static mask matrices.\nHowever, their static mask matrices limit the capability for localness modeling\nin text representation learning. We therefore introduce a new layer named\ndynamic mask attention network (DMAN) with a learnable mask matrix which is\nable to model localness adaptively. To incorporate advantages of DMAN, SAN, and\nFFN, we propose a sequential layered structure to combine the three types of\nlayers. Extensive experiments on various tasks, including neural machine\ntranslation and text summarization demonstrate that our model outperforms the\noriginal Transformer.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:07:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Fan", "Zhihao", ""], ["Gong", "Yeyun", ""], ["Liu", "Dayiheng", ""], ["Wei", "Zhongyu", ""], ["Wang", "Siyuan", ""], ["Jiao", "Jian", ""], ["Duan", "Nan", ""], ["Zhang", "Ruofei", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2103.13606", "submitter": "Pedram Hosseini", "authors": "Pedram Hosseini, David A. Broniatowski, Mona Diab", "title": "Predicting Directionality in Causal Relations in Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we test the performance of two bidirectional transformer-based\nlanguage models, BERT and SpanBERT, on predicting directionality in causal\npairs in the textual content. Our preliminary results show that predicting\ndirection for inter-sentence and implicit causal relations is more challenging.\nAnd, SpanBERT performs better than BERT on causal samples with longer span\nlength. We also introduce CREST which is a framework for unifying a collection\nof scattered datasets of causal relations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:49:01 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hosseini", "Pedram", ""], ["Broniatowski", "David A.", ""], ["Diab", "Mona", ""]]}, {"id": "2103.13610", "submitter": "Tong Cui", "authors": "Tong Cui, Jinghui Xiao, Liangyou Li, Xin Jiang, Qun Liu", "title": "An Approach to Improve Robustness of NLP Systems against ASR Errors", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-enabled systems typically first convert audio to text through an\nautomatic speech recognition (ASR) model and then feed the text to downstream\nnatural language processing (NLP) modules. The errors of the ASR system can\nseriously downgrade the performance of the NLP modules. Therefore, it is\nessential to make them robust to the ASR errors. Previous work has shown it is\neffective to employ data augmentation methods to solve this problem by\ninjecting ASR noise during the training process. In this paper, we utilize the\nprevalent pre-trained language model to generate training samples with\nASR-plausible noise. Compare to the previous methods, our approach generates\nASR noise that better fits the real-world error distribution. Experimental\nresults on spoken language translation(SLT) and spoken language understanding\n(SLU) show that our approach effectively improves the system robustness against\nthe ASR errors and achieves state-of-the-art results on both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 05:15:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Cui", "Tong", ""], ["Xiao", "Jinghui", ""], ["Li", "Liangyou", ""], ["Jiang", "Xin", ""], ["Liu", "Qun", ""]]}, {"id": "2103.13678", "submitter": "Shuhao Gu", "authors": "Shuhao Gu, Yang Feng, Wanying Xie", "title": "Pruning-then-Expanding Model for Domain Adaptation of Neural Machine\n  Translation", "comments": "NAACL 2021 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain Adaptation is widely used in practical applications of neural machine\ntranslation, which aims to achieve good performance on both the general-domain\nand in-domain. However, the existing methods for domain adaptation usually\nsuffer from catastrophic forgetting, domain divergence, and model explosion. To\naddress these three problems, we propose a method of \"divide and conquer\" which\nis based on the importance of neurons or parameters in the translation model.\nIn our method, we first prune the model and only keep the important neurons or\nparameters, making them responsible for both general-domain and in-domain\ntranslation. Then we further train the pruned model supervised by the original\nunpruned model with the knowledge distillation method. Last we expand the model\nto the original size and fine-tune the added parameters for the in-domain\ntranslation. We conduct experiments on different languages and domains and the\nresults show that our method can achieve significant improvements compared with\nseveral strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:57:09 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:20:18 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Gu", "Shuhao", ""], ["Feng", "Yang", ""], ["Xie", "Wanying", ""]]}, {"id": "2103.13799", "submitter": "David Vilares", "authors": "David Vilares and Marcos Garcia and Carlos G\\'omez-Rodr\\'iguez", "title": "Bertinho: Galician BERT Representations", "comments": "Accepted in the journal Procesamiento del Lenguaje Natural", "journal-ref": "Procesamiento del Lenguaje Natural. 66 (2021) 13-26", "doi": "10.26342/2021-66-1", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a monolingual BERT model for Galician. We follow the\nrecent trend that shows that it is feasible to build robust monolingual BERT\nmodels even for relatively low-resource languages, while performing better than\nthe well-known official multilingual BERT (mBERT). More particularly, we\nrelease two monolingual Galician BERT models, built using 6 and 12 transformer\nlayers, respectively; trained with limited resources (~45 million tokens on a\nsingle GPU of 24GB). We then provide an exhaustive evaluation on a number of\ntasks such as POS-tagging, dependency parsing and named entity recognition. For\nthis purpose, all these tasks are cast in a pure sequence labeling setup in\norder to run BERT without the need to include any additional layers on top of\nit (we only use an output classification layer to map the contextualized\nrepresentations into the predicted label). The experiments show that our\nmodels, especially the 12-layer one, outperform the results of mBERT in most\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 12:51:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Vilares", "David", ""], ["Garcia", "Marcos", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2103.13868", "submitter": "Yuzhong Wang", "authors": "Yuzhong Wang, Chaojun Xiao, Shirong Ma, Haoxi Zhong, Cunchao Tu,\n  Tianyang Zhang, Zhiyuan Liu, Maosong Sun", "title": "Equality before the Law: Legal Judgment Consistency Analysis for\n  Fairness", "comments": "15 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a legal system, judgment consistency is regarded as one of the most\nimportant manifestations of fairness. However, due to the complexity of factual\nelements that impact sentencing in real-world scenarios, few works have been\ndone on quantitatively measuring judgment consistency towards real-world data.\nIn this paper, we propose an evaluation metric for judgment inconsistency,\nLegal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency\nbetween data groups divided by specific features (e.g., gender, region, race).\nWe propose to simulate judges from different groups with legal judgment\nprediction (LJP) models and measure the judicial inconsistency with the\ndisagreement of the judgment results given by LJP models trained on different\ngroups. Experimental results on the synthetic data verify the effectiveness of\nLInCo. We further employ LInCo to explore the inconsistency in real cases and\ncome to the following observations: (1) Both regional and gender inconsistency\nexist in the legal system, but gender inconsistency is much less than regional\ninconsistency; (2) The level of regional inconsistency varies little across\ndifferent time periods; (3) In general, judicial inconsistency is negatively\ncorrelated with the severity of the criminal charges. Besides, we use LInCo to\nevaluate the performance of several de-bias methods, such as adversarial\nlearning, and find that these mechanisms can effectively help LJP models to\navoid suffering from data bias.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:28:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wang", "Yuzhong", ""], ["Xiao", "Chaojun", ""], ["Ma", "Shirong", ""], ["Zhong", "Haoxi", ""], ["Tu", "Cunchao", ""], ["Zhang", "Tianyang", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2103.13871", "submitter": "Gianpaolo Zammarchi", "authors": "Gianpaolo Zammarchi, Francesco Mola, Claudio Conversano", "title": "Impact of the COVID-19 outbreak on Italy's country reputation and stock\n  market performance: a sentiment analysis approach", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  During the recent Coronavirus disease 2019 (COVID-19) outbreak, the\nmicroblogging service Twitter has been widely used to share opinions and\nreactions to events. Italy was one of the first European countries to be\nseverely affected by the outbreak and to establish lockdown and stay-at-home\norders, potentially leading to country reputation damage. We resort to\nsentiment analysis to investigate changes in opinions about Italy reported on\nTwitter before and after the COVID-19 outbreak. Using different lexicons-based\nmethods, we find a breakpoint corresponding to the date of the first\nestablished case of COVID-19 in Italy that causes a relevant change in\nsentiment scores used as proxy of the country reputation. Next, we demonstrate\nthat sentiment scores about Italy are strongly associated with the levels of\nthe FTSE-MIB index, the Italian Stock Exchange main index, as they serve as\nearly detection signals of changes in the values of FTSE-MIB. Finally, we make\na content-based classification of tweets into positive and negative and use two\nmachine learning classifiers to validate the assigned polarity of tweets posted\nbefore and after the outbreak.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 14:03:11 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zammarchi", "Gianpaolo", ""], ["Mola", "Francesco", ""], ["Conversano", "Claudio", ""]]}, {"id": "2103.13942", "submitter": "Damien Sileo", "authors": "Damien Sileo", "title": "Visual Grounding Strategies for Text-Only Natural Language Processing", "comments": "Accepted at LANTERN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual grounding is a promising path toward more robust and accurate Natural\nLanguage Processing (NLP) models. Many multimodal extensions of BERT (e.g.,\nVideoBERT, LXMERT, VL-BERT) allow a joint modeling of texts and images that\nlead to state-of-the-art results on multimodal tasks such as Visual Question\nAnswering. Here, we leverage multimodal modeling for purely textual tasks\n(language modeling and classification) with the expectation that the multimodal\npretraining provides a grounding that can improve text processing accuracy. We\npropose possible strategies in this respect. A first type of strategy, referred\nto as {\\it transferred grounding} consists in applying multimodal models to\ntext-only tasks using a placeholder to replace image input. The second one,\nwhich we call {\\it associative grounding}, harnesses image retrieval to match\ntexts with related images during both pretraining and text-only downstream\ntasks. We draw further distinctions into both strategies and then compare them\naccording to their impact on language modeling and commonsense-related\ndownstream tasks, showing improvement over text-only baselines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 16:03:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sileo", "Damien", ""]]}, {"id": "2103.13997", "submitter": "Yonatan Alon", "authors": "Yonatan Alon", "title": "Real-time low-resource phoneme recognition on edge devices", "comments": "The model and code described in this paper are publicly available at\n  https://github.com/yonatankarimish/YonaVox", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While speech recognition has seen a surge in interest and research over the\nlast decade, most machine learning models for speech recognition either require\nlarge training datasets or lots of storage and memory. Combined with the\nprominence of English as the number one language in which audio data is\navailable, this means most other languages currently lack good speech\nrecognition models.\n  The method presented in this paper shows how to create and train models for\nspeech recognition in any language which are not only highly accurate, but also\nrequire very little storage, memory and training data when compared with\ntraditional models. This allows training models to recognize any language and\ndeploying them on edge devices such as mobile phones or car displays for fast\nreal-time speech recognition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:34:59 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Alon", "Yonatan", ""]]}, {"id": "2103.14152", "submitter": "Qiujia Li", "authors": "Qiujia Li, Yu Zhang, Bo Li, Liangliang Cao, Philip C. Woodland", "title": "Residual Energy-Based Models for End-to-End Speech Recognition", "comments": "To appear in Proc. Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 22:08:00 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 17:26:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Li", "Qiujia", ""], ["Zhang", "Yu", ""], ["Li", "Bo", ""], ["Cao", "Liangliang", ""], ["Woodland", "Philip C.", ""]]}, {"id": "2103.14245", "submitter": "Yi Shi", "authors": "Congyi Wang, Yu Chen, Bin Wang, Yi Shi", "title": "Improve GAN-based Neural Vocoder using Pointwise Relativistic\n  LeastSquare GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GAN-based neural vocoders, such as Parallel WaveGAN and MelGAN have attracted\ngreat interest due to their lightweight and parallel structures, enabling them\nto generate high fidelity waveform in a real-time manner. In this paper,\ninspired by Relativistic GAN, we introduce a novel variant of the LSGAN\nframework under the context of waveform synthesis, named Pointwise Relativistic\nLSGAN (PRLSGAN). In this approach, we take the truism score distribution into\nconsideration and combine the original MSE loss with the proposed pointwise\nrelative discrepancy loss to increase the difficulty of the generator to fool\nthe discriminator, leading to improved generation quality. Moreover, PRLSGAN is\na general-purposed framework that can be combined with any GAN-based neural\nvocoder to enhance its generation quality. Experiments have shown a consistent\nperformance boost based on Parallel WaveGAN and MelGAN, demonstrating the\neffectiveness and strong generalization ability of our proposed PRLSGAN neural\nvocoders.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:35:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:00:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Congyi", ""], ["Chen", "Yu", ""], ["Wang", "Bin", ""], ["Shi", "Yi", ""]]}, {"id": "2103.14302", "submitter": "Jaeyun Song", "authors": "Jaeyun Song, Hajin Shim, Eunho Yang", "title": "Mutually-Constrained Monotonic Multihead Attention for Online ASR", "comments": "Accepted at IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the feature of real-time decoding, Monotonic Multihead Attention\n(MMA) shows comparable performance to the state-of-the-art offline methods in\nmachine translation and automatic speech recognition (ASR) tasks. However, the\nlatency of MMA is still a major issue in ASR and should be combined with a\ntechnique that can reduce the test latency at inference time, such as\nhead-synchronous beam search decoding, which forces all non-activated heads to\nactivate after a small fixed delay from the first head activation. In this\npaper, we remove the discrepancy between training and test phases by\nconsidering, in the training of MMA, the interactions across multiple heads\nthat will occur in the test time. Specifically, we derive the expected\nalignments from monotonic attention by considering the boundaries of other\nheads and reflect them in the learning process. We validate our proposed method\non the two standard benchmark datasets for ASR and show that our approach, MMA\nwith the mutually-constrained heads from the training stage, provides better\nperformance than baselines.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 07:33:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Song", "Jaeyun", ""], ["Shim", "Hajin", ""], ["Yang", "Eunho", ""]]}, {"id": "2103.14349", "submitter": "Yinya Huang", "authors": "Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang", "title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent QA with logical reasoning questions requires passage-level relations\namong the sentences. However, current approaches still focus on sentence-level\nrelations interacting among tokens. In this work, we explore aggregating\npassage-level clues for solving logical reasoning QA by using discourse-based\ninformation. We propose a discourse-aware graph network (DAGN) that reasons\nrelying on the discourse structure of the texts. The model encodes discourse\ninformation as a graph with elementary discourse units (EDUs) and discourse\nrelations, and learns the discourse-aware features via a graph network for\ndownstream QA tasks. Experiments are conducted on two logical reasoning QA\ndatasets, ReClor and LogiQA, and our proposed DAGN achieves competitive\nresults. The source code is available at https://github.com/Eleanor-H/DAGN.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:41:56 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 09:01:59 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Huang", "Yinya", ""], ["Fang", "Meng", ""], ["Cao", "Yu", ""], ["Wang", "Liwei", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2103.14410", "submitter": "Tanmoy Chakraborty", "authors": "Ayan Sengupta, William Scott Paka, Suman Roy, Gaurav Ranjan, Tanmoy\n  Chakraborty", "title": "An Embedding-based Joint Sentiment-Topic Model for Short Texts", "comments": "Accepted in International AAAI Conference on Web and Social Media\n  (ICWSM), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Short text is a popular avenue of sharing feedback, opinions and reviews on\nsocial media, e-commerce platforms, etc. Many companies need to extract\nmeaningful information (which may include thematic content as well as semantic\npolarity) out of such short texts to understand users' behaviour. However,\nobtaining high quality sentiment-associated and human interpretable themes\nstill remains a challenge for short texts. In this paper we develop ELJST, an\nembedding enhanced generative joint sentiment-topic model that can discover\nmore coherent and diverse topics from short texts. It uses Markov Random Field\nRegularizer that can be seen as a generalisation of skip-gram based models.\nFurther, it can leverage higher-order semantic information appearing in word\nembedding, such as self-attention weights in graphical models. Our results show\nan average improvement of 10% in topic coherence and 5% in topic\ndiversification over baselines. Finally, ELJST helps understand users'\nbehaviour at more granular levels which can be explained. All these can bring\nsignificant values to the service and healthcare industries often dealing with\ncustomers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:41:21 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sengupta", "Ayan", ""], ["Paka", "William Scott", ""], ["Roy", "Suman", ""], ["Ranjan", "Gaurav", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2103.14411", "submitter": "Alexis Toumi", "authors": "Alexis Toumi, Alex Koziell-Pipe", "title": "Functorial Language Models", "comments": "Submitted to SemSpace 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce functorial language models: a principled way to compute\nprobability distributions over word sequences given a monoidal functor from\ngrammar to meaning. This yields a method for training categorical compositional\ndistributional (DisCoCat) models on raw text data. We provide a\nproof-of-concept implementation in DisCoPy, the Python toolbox for monoidal\ncategories.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:41:52 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Toumi", "Alexis", ""], ["Koziell-Pipe", "Alex", ""]]}, {"id": "2103.14423", "submitter": "Karl-Heinz Zimmermann", "authors": "Merve Nur Cakir, Mehwish Saleemi, Karl-Heinz Zimmermann", "title": "On the Theory of Stochastic Automata", "comments": "50 pages, 11 figures, index included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The theory of discrete stochastic systems has been initiated by the work of\nShannon and von Neumann. While Shannon has considered memory-less communication\nchannels and their generalization by introducing states, von Neumann has\nstudied the synthesis of reliable systems from unreliable components. The\nfundamental work of Rabin and Scott about deterministic finite-state automata\nhas led to two generalizations. First, the generalization of transition\nfunctions to conditional distributions studied by Carlyle and Starke. This in\nturn has led to a generalization of time-discrete Markov chains in which the\nchains are governed by more than one transition probability matrix. Second, the\ngeneralization of regular sets by introducing stochastic automata as described\nby Rabin. Stochastic automata are well-investigated. This report provides a\nshort introduction to stochastic automata based on the valuable book of Claus.\nThis includes the basic topics of the theory of stochastic automata:\nequivalence, minimization, reduction, covering, observability, and determinism.\nThen stochastic versions of Mealy and Moore automata are studied and finally\nstochastic language acceptors are considered as a generalization of\nnondeterministic finite-state acceptors.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:05:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Cakir", "Merve Nur", ""], ["Saleemi", "Mehwish", ""], ["Zimmermann", "Karl-Heinz", ""]]}, {"id": "2103.14443", "submitter": "Damai Dai", "authors": "Damai Dai, Hua Zheng, Zhifang Sui, Baobao Chang", "title": "Incorporating Connections Beyond Knowledge Embeddings: A Plug-and-Play\n  Module to Enhance Commonsense Reasoning in Machine Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Machine Reading Comprehension (MRC) has been well-addressed by\npattern matching, but the ability of commonsense reasoning remains a gap\nbetween humans and machines. Previous methods tackle this problem by enriching\nword representations via pre-trained Knowledge Graph Embeddings (KGE). However,\nthey make limited use of a large number of connections between nodes in\nKnowledge Graphs (KG), which could be pivotal cues to build the commonsense\nreasoning chains. In this paper, we propose a Plug-and-play module to\nIncorporatE Connection information for commonsEnse Reasoning (PIECER). Beyond\nenriching word representations with knowledge embeddings, PIECER constructs a\njoint query-passage graph to explicitly guide commonsense reasoning by the\nknowledge-oriented connections between words. Further, PIECER has high\ngeneralizability since it can be plugged into suitable positions in any MRC\nmodel. Experimental results on ReCoRD, a large-scale public MRC dataset\nrequiring commonsense reasoning, show that PIECER introduces stable performance\nimprovements for four representative base MRC models, especially in\nlow-resource settings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:55:19 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Dai", "Damai", ""], ["Zheng", "Hua", ""], ["Sui", "Zhifang", ""], ["Chang", "Baobao", ""]]}, {"id": "2103.14453", "submitter": "Markus Bayer", "authors": "Markus Bayer, Marc-Andr\\'e Kaufhold, Bj\\\"orn Buchhold, Marcel Keller,\n  J\\\"org Dallmeyer and Christian Reuter", "title": "Data Augmentation in Natural Language Processing: A Novel Text\n  Generation Approach for Long and Short Text Classifiers", "comments": "20 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases of machine learning, research suggests that the development of\ntraining data might have a higher relevance than the choice and modelling of\nclassifiers themselves. Thus, data augmentation methods have been developed to\nimprove classifiers by artificially created training data. In NLP, there is the\nchallenge of establishing universal rules for text transformations which\nprovide new linguistic patterns. In this paper, we present and evaluate a text\ngeneration method suitable to increase the performance of classifiers for long\nand short texts. We achieved promising improvements when evaluating short as\nwell as long text tasks with the enhancement by our text generation method. In\na simulated low data regime additive accuracy gains of up to 15.53% are\nachieved. As the current track of these constructed regimes is not universally\napplicable, we also show major improvements in several real world low data\ntasks (up to +4.84 F1 score). Since we are evaluating the method from many\nperspectives, we also observe situations where the method might not be\nsuitable. We discuss implications and patterns for the successful application\nof our approach on different types of datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:16:07 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bayer", "Markus", ""], ["Kaufhold", "Marc-Andr\u00e9", ""], ["Buchhold", "Bj\u00f6rn", ""], ["Keller", "Marcel", ""], ["Dallmeyer", "J\u00f6rg", ""], ["Reuter", "Christian", ""]]}, {"id": "2103.14465", "submitter": "Kamil Bujel", "authors": "Kamil Bujel, Helen Yannakoudakis, Marek Rei", "title": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate how sentence-level transformers can be modified into effective\nsequence labelers at the token level without any direct supervision. Existing\napproaches to zero-shot sequence labeling do not perform well when applied on\ntransformer-based architectures. As transformers contain multiple layers of\nmulti-head self-attention, information in the sentence gets distributed between\nmany tokens, negatively affecting zero-shot token-level performance. We find\nthat a soft attention module which explicitly encourages sharpness of attention\nweights can significantly outperform existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:35:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 19:03:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bujel", "Kamil", ""], ["Yannakoudakis", "Helen", ""], ["Rei", "Marek", ""]]}, {"id": "2103.14512", "submitter": "Hamed Hemati", "authors": "Hamed Hemati, Damian Borth", "title": "Continual Speaker Adaptation for Text-to-Speech Synthesis", "comments": "submitted to INTERSPEECH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a multi-speaker Text-to-Speech (TTS) model from scratch is\ncomputationally expensive and adding new speakers to the dataset requires the\nmodel to be re-trained. The naive solution of sequential fine-tuning of a model\nfor new speakers can cause the model to have poor performance on older\nspeakers. This phenomenon is known as catastrophic forgetting. In this paper,\nwe look at TTS modeling from a continual learning perspective where the goal is\nto add new speakers without forgetting previous speakers. Therefore, we first\npropose an experimental setup and show that serial fine-tuning for new speakers\ncan result in the forgetting of the previous speakers. Then we exploit two\nwell-known techniques for continual learning namely experience replay and\nweight regularization and we reveal how one can mitigate the effect of\ndegradation in speech synthesis diversity in sequential training of new\nspeakers using these methods. Finally, we present a simple extension to improve\nthe results in extreme setups.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:14:20 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hemati", "Hamed", ""], ["Borth", "Damian", ""]]}, {"id": "2103.14540", "submitter": "Ahmed Elgohary", "authors": "Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney,\n  Gonzalo Ramos and Ahmed Hassan Awadallah", "title": "NL-EDIT: Correcting semantic parse errors through natural language\n  interaction", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study semantic parsing in an interactive setting in which users correct\nerrors with natural language feedback. We present NL-EDIT, a model for\ninterpreting natural language feedback in the interaction context to generate a\nsequence of edits that can be applied to the initial parse to correct its\nerrors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL\nparsers by up to 20% with only one turn of correction. We analyze the\nlimitations of the model and discuss directions for improvement and evaluation.\nThe code and datasets used in this paper are publicly available at\nhttp://aka.ms/NLEdit.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:45:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Elgohary", "Ahmed", ""], ["Meek", "Christopher", ""], ["Richardson", "Matthew", ""], ["Fourney", "Adam", ""], ["Ramos", "Gonzalo", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2103.14542", "submitter": "Dongsheng Luo", "authors": "Dongsheng Luo, Wei Cheng, Jingchao Ni, Wenchao Yu, Xuchao Zhang, Bo\n  Zong, Yanchi Liu, Zhengzhang Chen, Dongjin Song, Haifeng Chen, Xiang Zhang", "title": "Unsupervised Document Embedding via Contrastive Augmentation", "comments": "13 pages; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a contrasting learning approach with data augmentation techniques\nto learn document representations in an unsupervised manner. Inspired by recent\ncontrastive self-supervised learning algorithms used for image and NLP\npretraining, we hypothesize that high-quality document embedding should be\ninvariant to diverse paraphrases that preserve the semantics of the original\ndocument. With different backbones and contrastive learning frameworks, our\nstudy reveals the enormous benefits of contrastive augmentation for document\nrepresentation learning with two additional insights: 1) including data\naugmentation in a contrastive way can substantially improve the embedding\nquality in unsupervised document representation learning, and 2) in general,\nstochastic augmentations generated by simple word-level manipulation work much\nbetter than sentence-level and document-level ones. We plug our method into a\nclassifier and compare it with a broad range of baseline methods on six\nbenchmark datasets. Our method can decrease the classification error rate by up\nto 6.4% over the SOTA approaches on the document classification task, matching\nor even surpassing fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:48:52 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Luo", "Dongsheng", ""], ["Cheng", "Wei", ""], ["Ni", "Jingchao", ""], ["Yu", "Wenchao", ""], ["Zhang", "Xuchao", ""], ["Zong", "Bo", ""], ["Liu", "Yanchi", ""], ["Chen", "Zhengzhang", ""], ["Song", "Dongjin", ""], ["Chen", "Haifeng", ""], ["Zhang", "Xiang", ""]]}, {"id": "2103.14580", "submitter": "Mahdi Namazifar", "authors": "Mahdi Namazifar, John Malik, Li Erran Li, Gokhan Tur, Dilek Hakkani\n  T\\\"ur", "title": "Correcting Automated and Manual Speech Transcription Errors using Warped\n  Language Models", "comments": "Submitted to INTERSPEECH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Masked language models have revolutionized natural language processing\nsystems in the past few years. A recently introduced generalization of masked\nlanguage models called warped language models are trained to be more robust to\nthe types of errors that appear in automatic or manual transcriptions of spoken\nlanguage by exposing the language model to the same types of errors during\ntraining. In this work we propose a novel approach that takes advantage of the\nrobustness of warped language models to transcription noise for correcting\ntranscriptions of spoken language. We show that our proposed approach is able\nto achieve up to 10% reduction in word error rates of both automatic and manual\ntranscriptions of spoken language.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:43:23 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Namazifar", "Mahdi", ""], ["Malik", "John", ""], ["Li", "Li Erran", ""], ["Tur", "Gokhan", ""], ["T\u00fcr", "Dilek Hakkani", ""]]}, {"id": "2103.14583", "submitter": "Nay San", "authors": "Nay San, Martijn Bartelds, Mitchell Browne, Lily Clifford, Fiona\n  Gibson, John Mansfield, David Nash, Jane Simpson, Myfany Turpin, Maria\n  Vollmer, Sasha Wilmoth, Dan Jurafsky", "title": "Leveraging neural representations for facilitating access to\n  untranscribed speech from endangered languages", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For languages with insufficient resources to train speech recognition\nsystems, query-by-example spoken term detection (QbE-STD) offers a way of\naccessing an untranscribed speech corpus by helping identify regions where\nspoken query terms occur. Yet retrieval performance can be poor when the query\nand corpus are spoken by different speakers and produced in different recording\nconditions. Using data selected from a variety of speakers and recording\nconditions from 7 Australian Aboriginal languages and a regional variety of\nDutch, all of which are endangered or vulnerable, we evaluated whether QbE-STD\nperformance on these languages could be improved by leveraging representations\nextracted from the pre-trained English wav2vec 2.0 model. Compared to the use\nof Mel-frequency cepstral coefficients and bottleneck features, we find that\nrepresentations from the middle layers of the wav2vec 2.0 Transformer offer\nlarge gains in task performance (between 56% and 86%). While features extracted\nusing the pre-trained English model yielded improved detection on all the\nevaluation languages, better detection performance was associated with the\nevaluation language's phonological similarity to English.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:44:08 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["San", "Nay", ""], ["Bartelds", "Martijn", ""], ["Browne", "Mitchell", ""], ["Clifford", "Lily", ""], ["Gibson", "Fiona", ""], ["Mansfield", "John", ""], ["Nash", "David", ""], ["Simpson", "Jane", ""], ["Turpin", "Myfany", ""], ["Vollmer", "Maria", ""], ["Wilmoth", "Sasha", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2103.14620", "submitter": "Irene Li", "authors": "Irene Li, Tianxiao Li, Yixin Li, Ruihai Dong, and Toyotaro Suzumura", "title": "Heterogeneous Graph Neural Networks for Multi-label Text Classification", "comments": "8 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-label text classification (MLTC) is an attractive and challenging task\nin natural language processing (NLP). Compared with single-label text\nclassification, MLTC has a wider range of applications in practice. In this\npaper, we propose a heterogeneous graph convolutional network model to solve\nthe MLTC problem by modeling tokens and labels as nodes in a heterogeneous\ngraph. In this way, we are able to take into account multiple relationships\nincluding token-level relationships. Besides, the model allows a good\nexplainability as the token-label edges are exposed. We evaluate our method on\nthree real-world datasets and the experimental results show that it achieves\nsignificant improvements and outperforms state-of-the-art comparison methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:33:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Irene", ""], ["Li", "Tianxiao", ""], ["Li", "Yixin", ""], ["Dong", "Ruihai", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "2103.14625", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Robert Turko, Duen Horng Chau", "title": "Dodrio: Exploring Transformer Models with Interactive Visualization", "comments": "10 pages, 8 figures, Accepted to ACL 2021. For a demo video, see\n  https://youtu.be/qB-T9j7UTgE . For a live demo, see\n  https://poloclub.github.io/dodrio/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do large pre-trained transformer-based models perform so well across a\nwide variety of NLP tasks? Recent research suggests the key may lie in\nmulti-headed attention mechanism's ability to learn and represent linguistic\ninformation. Understanding how these models represent both syntactic and\nsemantic knowledge is vital to investigate why they succeed and fail, what they\nhave learned, and how they can improve. We present Dodrio, an open-source\ninteractive visualization tool to help NLP researchers and practitioners\nanalyze attention mechanisms in transformer-based models with linguistic\nknowledge. Dodrio tightly integrates an overview that summarizes the roles of\ndifferent attention heads, and detailed views that help users compare attention\nweights with the syntactic structure and semantic information in the input\ntext. To facilitate the visual comparison of attention weights and linguistic\nknowledge, Dodrio applies different graph visualization techniques to represent\nattention weights scalable to longer input text. Case studies highlight how\nDodrio provides insights into understanding the attention mechanism in\ntransformer-based models. Dodrio is available at\nhttps://poloclub.github.io/dodrio/.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:39:37 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:42:50 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 14:51:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Zijie J.", ""], ["Turko", "Robert", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2103.14736", "submitter": "Hiromasa Fujihara", "authors": "Shintaro Ando, Hiromasa Fujihara", "title": "Construction of a Large-scale Japanese ASR Corpus on TV Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new large-scale Japanese speech corpus for training\nautomatic speech recognition (ASR) systems. This corpus contains over 2,000\nhours of speech with transcripts built on Japanese TV recordings and their\nsubtitles. We develop herein an iterative workflow to extract matching audio\nand subtitle segments from TV recordings based on a conventional method for\nlightly-supervised audio-to-text alignment. We evaluate a model trained with\nour corpus using an evaluation dataset built on Japanese TEDx presentation\nvideos and confirm that the performance is better than that trained with the\nCorpus of Spontaneous Japanese (CSJ). The experiment results show the\nusefulness of our corpus for training ASR systems. This corpus is made public\nfor the research community along with Kaldi scripts for training the models\nreported in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 21:14:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ando", "Shintaro", ""], ["Fujihara", "Hiromasa", ""]]}, {"id": "2103.14757", "submitter": "Ikechukwu Onyenwe", "authors": "Chidinma A. Nwafor and Ikechukwu E. Onyenwe", "title": "An Automated Multiple-Choice Question Generation Using Natural Language\n  Processing Techniques", "comments": "Recently accepted by the International Journal on Natural Language\n  Computing (IJNLC) awaiting publication, 11 pages, 4 figures, 5 tables", "journal-ref": "International Journal on Natural Language Computing(IJNLC), April\n  2021", "doi": "10.5121/ijnlc.2021.10201", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic multiple-choice question generation (MCQG) is a useful yet\nchallenging task in Natural Language Processing (NLP). It is the task of\nautomatic generation of correct and relevant questions from textual data.\nDespite its usefulness, manually creating sizeable, meaningful and relevant\nquestions is a time-consuming and challenging task for teachers. In this paper,\nwe present an NLP-based system for automatic MCQG for Computer-Based Testing\nExamination (CBTE).We used NLP technique to extract keywords that are important\nwords in a given lesson material. To validate that the system is not perverse,\nfive lesson materials were used to check the effectiveness and efficiency of\nthe system. The manually extracted keywords by the teacher were compared to the\nauto-generated keywords and the result shows that the system was capable of\nextracting keywords from lesson materials in setting examinable questions. This\noutcome is presented in a user-friendly interface for easy accessibility.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 22:39:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nwafor", "Chidinma A.", ""], ["Onyenwe", "Ikechukwu E.", ""]]}, {"id": "2103.14785", "submitter": "Jesus Perez-Martin", "authors": "Jesus Perez-Martin and Benjamin Bustos and Silvio Jamil F. Guimar\\~aes\n  and Ivan Sipiran and Jorge P\\'erez and Grethel Coello Said", "title": "Bridging Vision and Language from the Video-to-Text Perspective: A\n  Comprehensive Review", "comments": "66 pages, 5 figures. Submitted to Artificial Intelligence Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the area of Vision and Language encompasses challenging topics\nthat seek to connect visual and textual information. The video-to-text problem\nis one of these topics, in which the goal is to connect an input video with its\ntextual description. This connection can be mainly made by retrieving the most\nsignificant descriptions from a corpus or generating a new one given a context\nvideo. These two ways represent essential tasks for Computer Vision and Natural\nLanguage Processing communities, called text retrieval from video task and\nvideo captioning/description task. These two tasks are substantially more\ncomplex than predicting or retrieving a single sentence from an image. The\nspatiotemporal information present in videos introduces diversity and\ncomplexity regarding the visual content and the structure of associated\nlanguage descriptions. This review categorizes and describes the\nstate-of-the-art techniques for the video-to-text problem. It covers the main\nvideo-to-text methods and the ways to evaluate their performance. We analyze\nhow the most reported benchmark datasets have been created, showing their\ndrawbacks and strengths for the problem requirements. We also show the\nimpressive progress that researchers have made on each dataset, and we analyze\nwhy, despite this progress, the video-to-text conversion is still unsolved.\nState-of-the-art techniques are still a long way from achieving human-like\nperformance in generating or retrieving video descriptions. We cover several\nsignificant challenges in the field and discuss future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 02:12:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Perez-Martin", "Jesus", ""], ["Bustos", "Benjamin", ""], ["Guimar\u00e3es", "Silvio Jamil F.", ""], ["Sipiran", "Ivan", ""], ["P\u00e9rez", "Jorge", ""], ["Said", "Grethel Coello", ""]]}, {"id": "2103.14797", "submitter": "Akshat Gupta", "authors": "Akshat Gupta, Sargam Menghani, Sai Krishna Rallabandi, Alan W Black", "title": "Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is an important task in understanding social media content\nlike customer reviews, Twitter and Facebook feeds etc. In multilingual\ncommunities around the world, a large amount of social media text is\ncharacterized by the presence of Code-Switching. Thus, it has become important\nto build models that can handle code-switched data. However, annotated\ncode-switched data is scarce and there is a need for unsupervised models and\nalgorithms. We propose a general framework called Unsupervised Self-Training\nand show its applications for the specific use case of sentiment analysis of\ncode-switched data. We use the power of pre-trained BERT models for\ninitialization and fine-tune them in an unsupervised manner, only using pseudo\nlabels produced by zero-shot transfer. We test our algorithm on multiple\ncode-switched languages and provide a detailed analysis of the learning\ndynamics of the algorithm with the aim of answering the question - `Does our\nunsupervised model understand the Code-Switched languages or does it just learn\nits representations?'. Our unsupervised models compete well with their\nsupervised counterparts, with their performance reaching within 1-7\\% (weighted\nF1 scores) when compared to supervised models trained for a two class problem.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 03:23:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gupta", "Akshat", ""], ["Menghani", "Sargam", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan W", ""]]}, {"id": "2103.14804", "submitter": "Xuejiao Tang", "authors": "Xin Huang, Wenbin Zhang, Yiyi Huang, Xuejiao Tang, Mingli Zhang,\n  Jayachander Surbiryala, Vasileios Iosifidis, Zhen Liu and Ji Zhang", "title": "LSTM Based Sentiment Analysis for Cryptocurrency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent studies in big data analytics and natural language processing develop\nautomatic techniques in analyzing sentiment in the social media information. In\naddition, the growing user base of social media and the high volume of posts\nalso provide valuable sentiment information to predict the price fluctuation of\nthe cryptocurrency. This research is directed to predicting the volatile price\nmovement of cryptocurrency by analyzing the sentiment in social media and\nfinding the correlation between them. While previous work has been developed to\nanalyze sentiment in English social media posts, we propose a method to\nidentify the sentiment of the Chinese social media posts from the most popular\nChinese social media platform Sina-Weibo. We develop the pipeline to capture\nWeibo posts, describe the creation of the crypto-specific sentiment dictionary,\nand propose a long short-term memory (LSTM) based recurrent neural network\nalong with the historical cryptocurrency price movement to predict the price\ntrend for future time frames. The conducted experiments demonstrate the\nproposed approach outperforms the state of the art auto regressive based model\nby 18.5% in precision and 15.4% in recall.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 04:08:37 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 03:22:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Huang", "Xin", ""], ["Zhang", "Wenbin", ""], ["Huang", "Yiyi", ""], ["Tang", "Xuejiao", ""], ["Zhang", "Mingli", ""], ["Surbiryala", "Jayachander", ""], ["Iosifidis", "Vasileios", ""], ["Liu", "Zhen", ""], ["Zhang", "Ji", ""]]}, {"id": "2103.14916", "submitter": "Stefano Menini", "authors": "Stefano Menini, Alessio Palmero Aprosio, Sara Tonelli", "title": "Abuse is Contextual, What about NLP? The Role of Context in Abusive\n  Language Annotation and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The datasets most widely used for abusive language detection contain lists of\nmessages, usually tweets, that have been manually judged as abusive or not by\none or more annotators, with the annotation performed at message level. In this\npaper, we investigate what happens when the hateful content of a message is\njudged also based on the context, given that messages are often ambiguous and\nneed to be interpreted in the context of occurrence. We first re-annotate part\nof a widely used dataset for abusive language detection in English in two\nconditions, i.e. with and without context. Then, we compare the performance of\nthree classification algorithms obtained on these two types of dataset, arguing\nthat a context-aware classification is more challenging but also more similar\nto a real application scenario.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:31:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Menini", "Stefano", ""], ["Aprosio", "Alessio Palmero", ""], ["Tonelli", "Sara", ""]]}, {"id": "2103.14919", "submitter": "Dongfang Li", "authors": "Dongfang Li, Jingcong Tao, Qingcai Chen, Baotian Hu", "title": "You Can Do Better! If You Elaborate the Reason When Making Prediction", "comments": "14 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural predictive models have achieved remarkable performance improvements in\nvarious natural language processing tasks. However, most neural predictive\nmodels suffer from the lack of explainability of predictions, limiting their\npractical utility. This paper proposes a neural predictive approach to make a\nprediction and generate its corresponding explanation simultaneously. It\nleverages the knowledge entailed in explanations as an additional distillation\nsignal for more efficient learning. We conduct a preliminary study on Chinese\nmedical multiple-choice question answering, English natural language inference,\nand commonsense question answering tasks. The experimental results show that\nthe proposed approach can generate reasonable explanations for its predictions\neven with a small-scale training corpus. The proposed method also achieves\nimproved prediction accuracy on three datasets, which indicates that making\npredictions can benefit from generating the explanation in the decision\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:55:19 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 15:53:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Dongfang", ""], ["Tao", "Jingcong", ""], ["Chen", "Qingcai", ""], ["Hu", "Baotian", ""]]}, {"id": "2103.14961", "submitter": "Luke Gessler", "authors": "Luke Gessler, Shira Wein, Nathan Schneider", "title": "Supersense and Sensibility: Proxy Tasks for Semantic Annotation of\n  Prepositions", "comments": "Presented at LAW XIV in 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prepositional supersense annotation is time-consuming and requires expert\ntraining. Here, we present two sensible methods for obtaining prepositional\nsupersense annotations by eliciting surface substitution and similarity\njudgments. Four pilot studies suggest that both methods have potential for\nproducing prepositional supersense annotations that are comparable in quality\nto expert annotations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:28:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gessler", "Luke", ""], ["Wein", "Shira", ""], ["Schneider", "Nathan", ""]]}, {"id": "2103.14972", "submitter": "Francielle Alves Vargas", "authors": "Francielle Alves Vargas, Isabelle Carvalho, Fabiana Rodrigues de\n  G\\'oes, Fabr\\'icio Benevenuto, Thiago Alexandre Salgueiro Pardo", "title": "Building an Expert Annotated Corpus of Brazilian Instagram Comments for\n  Hate Speech and Offensive Language Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The understanding of an offense is subjective and people may have different\nopinions about the offensiveness of a comment. Moreover, offenses and hate\nspeech may occur through sarcasm, which hides the real intention of the comment\nand makes the decision of the annotators more confusing. Therefore, providing a\nwell-structured annotation process is crucial to a better understanding of hate\nspeech and offensive language phenomena, as well as supplying better\nperformance for machine learning classifiers. In this paper, we describe a\ncorpus annotation process proposed by a linguist, a hate speech specialist, and\nmachine learning engineers in order to support the identification of hate\nspeech and offensive language on social media. In addition, we provide the\nfirst robust dataset of this kind for the Brazilian Portuguese language. The\ncorpus was collected from Instagram posts of political personalities and\nmanually annotated, being composed by 7,000 annotated documents according to\nthree different layers: a binary classification (offensive versus non-offensive\nlanguage), the level of offense (highly offensive, moderately offensive, and\nslightly offensive messages), and the identification regarding the target of\nthe discriminatory content (xenophobia, racism, homophobia, sexism, religious\nintolerance, partyism, apology to the dictatorship, antisemitism, and\nfatphobia). Each comment was annotated by three different annotators and\nachieved high inter-annotator agreement. The proposed annotation approach is\nalso language and domain-independent nevertheless it is currently customized\nfor Brazilian Portuguese.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:43:16 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 22:15:40 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 10:02:52 GMT"}, {"version": "v4", "created": "Sun, 2 May 2021 20:58:41 GMT"}, {"version": "v5", "created": "Sun, 9 May 2021 16:41:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vargas", "Francielle Alves", ""], ["Carvalho", "Isabelle", ""], ["de G\u00f3es", "Fabiana Rodrigues", ""], ["Benevenuto", "Fabr\u00edcio", ""], ["Pardo", "Thiago Alexandre Salgueiro", ""]]}, {"id": "2103.14973", "submitter": "Hua Shen", "authors": "Hua Shen, Ting-Hao 'Kenneth' Huang", "title": "Explaining the Road Not Taken", "comments": "Accepted by The 2021 ACM CHI Workshop on Operationalizing\n  Human-Centered Perspectives in Explainable AI (CHI 2021 HCXAI Workshop). For\n  associated website, see https://human-centered-exnlp.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is unclear if existing interpretations of deep neural network models\nrespond effectively to the needs of users. This paper summarizes the common\nforms of explanations (such as feature attribution, decision rules, or probes)\nused in over 200 recent papers about natural language processing (NLP), and\ncompares them against user questions collected in the XAI Question Bank. We\nfound that although users are interested in explanations for the road not taken\n-- namely, why the model chose one result and not a well-defined, seemly\nsimilar legitimate counterpart -- most model interpretations cannot answer\nthese questions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:47:06 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 04:51:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shen", "Hua", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "2103.15022", "submitter": "Man Luo", "authors": "Man Luo, Shailaja Keyur Sampat, Riley Tallman, Yankai Zeng, Manuha\n  Vancha, Akarshan Sajja, Chitta Baral", "title": "'Just because you are right, doesn't mean I am wrong': Overcoming a\n  Bottleneck in the Development and Evaluation of Open-Ended Visual Question\n  Answering (VQA) Tasks", "comments": "accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GQA (Hudson and Manning, 2019) is a dataset for real-world visual reasoning\nand compositional question answering. We found that many answers predicted by\nthe best visionlanguage models on the GQA dataset do not match the ground-truth\nanswer but still are semantically meaningful and correct in the given context.\nIn fact, this is the case with most existing visual question answering (VQA)\ndatasets where they assume only one ground-truth answer for each question. We\npropose Alternative Answer Sets (AAS) of ground-truth answers to address this\nlimitation, which is created automatically using off-the-shelf NLP tools. We\nintroduce a semantic metric based on AAS and modify top VQA solvers to support\nmultiple plausible answers for a question. We implement this approach on the\nGQA dataset and show the performance improvements.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 00:07:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Luo", "Man", ""], ["Sampat", "Shailaja Keyur", ""], ["Tallman", "Riley", ""], ["Zeng", "Yankai", ""], ["Vancha", "Manuha", ""], ["Sajja", "Akarshan", ""], ["Baral", "Chitta", ""]]}, {"id": "2103.15025", "submitter": "Yijun Xiao", "authors": "Yijun Xiao, William Yang Wang", "title": "On Hallucination and Predictive Uncertainty in Conditional Language\n  Generation", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite improvements in performances on different natural language generation\ntasks, deep neural models are prone to hallucinating facts that are incorrect\nor nonexistent. Different hypotheses are proposed and examined separately for\ndifferent tasks, but no systematic explanations are available across these\ntasks. In this study, we draw connections between hallucinations and predictive\nuncertainty in conditional language generation. We investigate their\nrelationship in both image captioning and data-to-text generation and propose a\nsimple extension to beam search to reduce hallucination. Our analysis shows\nthat higher predictive uncertainty corresponds to a higher chance of\nhallucination. Epistemic uncertainty is more indicative of hallucination than\naleatoric or total uncertainties. It helps to achieve better results of trading\nperformance in standard metric for less hallucination with the proposed beam\nsearch variant.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 00:32:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xiao", "Yijun", ""], ["Wang", "William Yang", ""]]}, {"id": "2103.15060", "submitter": "Ye Jia", "authors": "Ye Jia, Heiga Zen, Jonathan Shen, Yu Zhang, Yonghui Wu", "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces PnG BERT, a new encoder model for neural TTS. This\nmodel is augmented from the original BERT model, by taking both phoneme and\ngrapheme representations of text as input, as well as the word-level alignment\nbetween them. It can be pre-trained on a large text corpus in a self-supervised\nmanner, and fine-tuned in a TTS task. Experimental results show that a neural\nTTS model using a pre-trained PnG BERT as its encoder yields more natural\nprosody and more accurate pronunciation than a baseline model using only\nphoneme input with no pre-training. Subjective side-by-side preference\nevaluations show that raters have no statistically significant preference\nbetween the speech synthesized using a PnG BERT and ground truth recordings\nfrom professional speakers.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 06:24:00 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 06:02:15 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 04:24:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jia", "Ye", ""], ["Zen", "Heiga", ""], ["Shen", "Jonathan", ""], ["Zhang", "Yu", ""], ["Wu", "Yonghui", ""]]}, {"id": "2103.15066", "submitter": "Fang Wu", "authors": "Fang Wu and Xiang Bai", "title": "InsertGNN: Can Graph Neural Networks Outperform Humans in TOEFL Sentence\n  Insertion Problem?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence insertion is a delicate but fundamental NLP problem. Current\napproaches in sentence ordering, text coherence, and question answering (QA)\nare neither suitable nor good at solving it. In this paper, We propose\nInsertGNN, a simple yet effective model that represents the problem as a graph\nand adopts the graph Neural Network (GNN) to learn the connection between\nsentences. It is also supervised by both the local and global information that\nthe local interactions of neighboring sentences can be considered. To the best\nof our knowledge, this is the first recorded attempt to apply a supervised\ngraph-structured model in sentence insertion. We evaluate our method in our\nnewly collected TOEFL dataset and further verify its effectiveness on the\nlarger arXivdataset using cross-domain learning. The experiments show that\nInsertGNN outperforms the unsupervised text coherence method, the topological\nsentence ordering approach, and the QA architecture. Specifically, It achieves\nan accuracy of 70%, rivaling the average human test scores.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 06:50:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wu", "Fang", ""], ["Bai", "Xiang", ""]]}, {"id": "2103.15075", "submitter": "Dimitris Papadopoulos", "authors": "Dimitris Papadopoulos, Nikolaos Papadakis and Nikolaos Matsatsinis", "title": "PENELOPIE: Enabling Open Information Extraction for the Greek Language\n  through Machine Translation", "comments": "16th conference of the European Chapter of the Association for\n  Computational Linguistics Student Research Workshop (EACL 2021 SRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present our submission for the EACL 2021 SRW; a methodology\nthat aims at bridging the gap between high and low-resource languages in the\ncontext of Open Information Extraction, showcasing it on the Greek language.\nThe goals of this paper are twofold: First, we build Neural Machine Translation\n(NMT) models for English-to-Greek and Greek-to-English based on the Transformer\narchitecture. Second, we leverage these NMT models to produce English\ntranslations of Greek text as input for our NLP pipeline, to which we apply a\nseries of pre-processing and triple extraction tasks. Finally, we\nback-translate the extracted triples to Greek. We conduct an evaluation of both\nour NMT and OIE methods on benchmark datasets and demonstrate that our approach\noutperforms the current state-of-the-art for the Greek natural language.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 08:01:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Papadopoulos", "Dimitris", ""], ["Papadakis", "Nikolaos", ""], ["Matsatsinis", "Nikolaos", ""]]}, {"id": "2103.15122", "submitter": "Siyuan Feng", "authors": "Siyuan Feng, Olya Kudina, Bence Mark Halpern and Odette Scharenborg", "title": "Quantifying Bias in Automatic Speech Recognition", "comments": "Submitted to INTERSPEECH (IS) 2021. This preprint version differs\n  slightly from the version submitted to IS 2021: Figure 1 is not included in\n  IS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) systems promise to deliver objective\ninterpretation of human speech. Practice and recent evidence suggests that the\nstate-of-the-art (SotA) ASRs struggle with the large variation in speech due to\ne.g., gender, age, speech impairment, race, and accents. Many factors can cause\nthe bias of an ASR system. Our overarching goal is to uncover bias in ASR\nsystems to work towards proactive bias mitigation in ASR. This paper is a first\nstep towards this goal and systematically quantifies the bias of a Dutch SotA\nASR system against gender, age, regional accents and non-native accents. Word\nerror rates are compared, and an in-depth phoneme-level error analysis is\nconducted to understand where bias is occurring. We primarily focus on bias due\nto articulation differences in the dataset. Based on our findings, we suggest\nbias mitigation strategies for ASR development.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 12:52:03 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:12:22 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Feng", "Siyuan", ""], ["Kudina", "Olya", ""], ["Halpern", "Bence Mark", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2103.15125", "submitter": "Lester Beltran", "authors": "Lester Beltran", "title": "Quantum Bose-Einstein Statistics for Indistinguishable Concepts in Human\n  Language", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the hypothesis that within a combination of a 'number concept'\nplus a 'substantive concept', such as 'eleven animals,' the identity and\nindistinguishability present on the level of the concepts, i.e., all eleven\nanimals are identical and indistinguishable, gives rise to a statistical\nstructure of the Bose-Einstein type similar to how Bose-Einstein statistics is\npresent for identical and indistinguishable quantum particles. We proceed by\nidentifying evidence for this hypothesis by extracting the statistical data\nfrom the World-Wide-Web utilizing the Google Search tool. By using the\nKullback-Leibler divergence method, we then compare the obtained distribution\nwith the Maxwell-Boltzmann as well as with the Bose-Einstein distributions and\nshow that the Bose-Einstein's provides a better fit as compared to the\nMaxwell-Boltzmanns.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 13:07:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Beltran", "Lester", ""]]}, {"id": "2103.15255", "submitter": "Yuncong Li", "authors": "Fang Wang, Yuncong Li, Wenjun Zhang, Sheng-hua Zhong", "title": "A More Fine-Grained Aspect-Sentiment-Opinion Triplet Extraction Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term,\nsentiment and opinion term triplets from sentences and tries to provide a\ncomplete solution for aspect-based sentiment analysis (ABSA). However, some\ntriplets extracted by ASTE are confusing, since the sentiment in a triplet\nextracted by ASTE is the sentiment that the sentence expresses toward the\naspect term rather than the sentiment of the aspect term and opinion term pair.\nIn this paper, we introduce a more fine-grained Aspect-Sentiment-Opinion\nTriplet Extraction (ASOTE) Task. ASOTE also extracts aspect term, sentiment and\nopinion term triplets. However, the sentiment in a triplet extracted by ASOTE\nis the sentiment of the aspect term and opinion term pair. We build four\ndatasets for ASOTE based on several popular ABSA benchmarks. We propose two\nmethods for ASOTE. The first method extracts the opinion terms of an aspect\nterm and predicts the sentiments of the aspect term and opinion term pairs\njointly with a unified tag schema. The second method is based on multiple\ninstance learning, which is trained on ASTE datasets, but can also perform the\nASOTE task. Experimental results on the four datasets demonstrate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 00:42:51 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 06:52:17 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 10:49:44 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 06:46:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Fang", ""], ["Li", "Yuncong", ""], ["Zhang", "Wenjun", ""], ["Zhong", "Sheng-hua", ""]]}, {"id": "2103.15316", "submitter": "Jianlin Su", "authors": "Jianlin Su, Jiarun Cao, Weijie Liu, Yangyiwen Ou", "title": "Whitening Sentence Representations for Better Semantics and Faster\n  Retrieval", "comments": "The source code of this paper is available at\n  https://github.com/bojone/BERT-whitening", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pre-training models such as BERT have achieved great success in many natural\nlanguage processing tasks. However, how to obtain better sentence\nrepresentation through these pre-training models is still worthy to exploit.\nPrevious work has shown that the anisotropy problem is an critical bottleneck\nfor BERT-based sentence representation which hinders the model to fully utilize\nthe underlying semantic features. Therefore, some attempts of boosting the\nisotropy of sentence distribution, such as flow-based model, have been applied\nto sentence representations and achieved some improvement. In this paper, we\nfind that the whitening operation in traditional machine learning can similarly\nenhance the isotropy of sentence representations and achieve competitive\nresults. Furthermore, the whitening technique is also capable of reducing the\ndimensionality of the sentence representation. Our experimental results show\nthat it can not only achieve promising performance but also significantly\nreduce the storage cost and accelerate the model retrieval speed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:51:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Su", "Jianlin", ""], ["Cao", "Jiarun", ""], ["Liu", "Weijie", ""], ["Ou", "Yangyiwen", ""]]}, {"id": "2103.15327", "submitter": "Haopeng Zhang", "authors": "Haopeng Zhang and Jiawei Zhang", "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised\n  Document Summarization", "comments": "We found some error in Table 2. We need some time to update the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised document summarization has re-acquired lots of attention in\nrecent years thanks to its simplicity and data independence. In this paper, we\npropose a graph-based unsupervised approach for extractive document\nsummarization. Instead of ranking sentences by salience and extracting\nsentences one by one, our approach works at a summary-level by utilizing graph\ncentrality and centroid. We first extract summary candidates as subgraphs based\non centrality from the sentence graph and then select from the summary\ncandidates by matching to the centroid. We perform extensive experiments on two\nbench-marked summarization datasets, and the results demonstrate the\neffectiveness of our model compared to state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:35:33 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 17:19:04 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhang", "Haopeng", ""], ["Zhang", "Jiawei", ""]]}, {"id": "2103.15330", "submitter": "Haw-Shiuan Chang", "authors": "Haw-Shiuan Chang, Amol Agrawal, Andrew McCallum", "title": "Extending Multi-Sense Word Embedding to Phrases and Sentences for\n  Unsupervised Semantic Applications", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most unsupervised NLP models represent each word with a single point or\nsingle region in semantic space, while the existing multi-sense word embeddings\ncannot represent longer word sequences like phrases or sentences. We propose a\nnovel embedding method for a text sequence (a phrase or a sentence) where each\nsequence is represented by a distinct set of multi-mode codebook embeddings to\ncapture different semantic facets of its meaning. The codebook embeddings can\nbe viewed as the cluster centers which summarize the distribution of possibly\nco-occurring words in a pre-trained word embedding space. We introduce an\nend-to-end trainable neural model that directly predicts the set of cluster\ncenters from the input text sequence during test time. Our experiments show\nthat the per-sentence codebook embeddings significantly improve the\nperformances in unsupervised sentence similarity and extractive summarization\nbenchmarks. In phrase similarity experiments, we discover that the multi-facet\nembeddings provide an interpretable semantic representation but do not\noutperform the single-facet baseline.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:54:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chang", "Haw-Shiuan", ""], ["Agrawal", "Amol", ""], ["McCallum", "Andrew", ""]]}, {"id": "2103.15335", "submitter": "Haw-Shiuan Chang", "authors": "Haw-Shiuan Chang, Jiaming Yuan, Mohit Iyyer, Andrew McCallum", "title": "Changing the Mind of Transformers for Topically-Controllable Language\n  Generation", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Transformer-based language models can aid human authors by suggesting\nplausible continuations of text written so far. However, current interactive\nwriting assistants do not allow authors to guide text generation in desired\ntopical directions. To address this limitation, we design a framework that\ndisplays multiple candidate upcoming topics, of which a user can select a\nsubset to guide the generation. Our framework consists of two components: (1) a\nmethod that produces a set of candidate topics by predicting the centers of\nword clusters in the possible continuations, and (2) a text generation model\nwhose output adheres to the chosen topics. The training of both components is\nself-supervised, using only unlabeled text. Our experiments demonstrate that\nour topic options are better than those of standard clustering approaches, and\nour framework often generates fluent sentences related to the chosen topics, as\njudged by automated metrics and crowdsourced workers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:02:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chang", "Haw-Shiuan", ""], ["Yuan", "Jiaming", ""], ["Iyyer", "Mohit", ""], ["McCallum", "Andrew", ""]]}, {"id": "2103.15339", "submitter": "Haw-Shiuan Chang", "authors": "Rohan Paul, Haw-Shiuan Chang, Andrew McCallum", "title": "Multi-facet Universal Schema", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Universal schema (USchema) assumes that two sentence patterns that share the\nsame entity pairs are similar to each other. This assumption is widely adopted\nfor solving various types of relation extraction (RE) tasks. Nevertheless, each\nsentence pattern could contain multiple facets, and not every facet is similar\nto all the facets of another sentence pattern co-occurring with the same entity\npair. To address the violation of the USchema assumption, we propose\nmulti-facet universal schema that uses a neural model to represent each\nsentence pattern as multiple facet embeddings and encourage one of these facet\nembeddings to be close to that of another sentence pattern if they co-occur\nwith the same entity pair. In our experiments, we demonstrate that multi-facet\nembeddings significantly outperform their single-facet embedding counterpart,\ncompositional universal schema (CUSchema) (Verga et al., 2016), in distantly\nsupervised relation extraction tasks. Moreover, we can also use multiple\nembeddings to detect the entailment relation between two sentence patterns when\nno manual label is available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:10:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Paul", "Rohan", ""], ["Chang", "Haw-Shiuan", ""], ["McCallum", "Andrew", ""]]}, {"id": "2103.15429", "submitter": "Robert Schwarzenberg", "authors": "Robert Schwarzenberg, Nils Feldhus, Sebastian M\\\"oller", "title": "Efficient Explanations from Empirical Explainers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Amid a discussion about Green AI in which we see explainability neglected, we\nexplore the possibility to efficiently approximate computationally expensive\nexplainers. To this end, we propose the task of feature attribution modelling\nthat we address with Empirical Explainers. Empirical Explainers learn from data\nto predict the attribution maps of expensive explainers. We train and test\nEmpirical Explainers in the language domain and find that they model their\nexpensive counterparts well, at a fraction of the cost. They could thus\nmitigate the computational burden of neural explanations significantly, in\napplications that tolerate an approximation error.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:54:55 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schwarzenberg", "Robert", ""], ["Feldhus", "Nils", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2103.15475", "submitter": "Salomey Osei", "authors": "Paul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi,\n  Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel\n  Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante\n  Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin\n  Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo,\n  Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel,\n  Anokye Acheampong Amponsah, James Ben Hayfron-Acquah", "title": "NLP for Ghanaian Languages", "comments": "6 pages paper; Accepted at AfricaNLP @EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP Ghana is an open-source non-profit organization aiming to advance the\ndevelopment and adoption of state-of-the-art NLP techniques and digital\nlanguage tools to Ghanaian languages and problems. In this paper, we first\npresent the motivation and necessity for the efforts of the organization; by\nintroducing some popular Ghanaian languages while presenting the state of NLP\nin Ghana. We then present the NLP Ghana organization and outline its aims,\nscope of work, some of the methods employed and contributions made thus far in\nthe NLP community in Ghana.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:16:52 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:01:29 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Azunre", "Paul", ""], ["Osei", "Salomey", ""], ["Addo", "Salomey", ""], ["Adu-Gyamfi", "Lawrence Asamoah", ""], ["Moore", "Stephen", ""], ["Adabankah", "Bernard", ""], ["Opoku", "Bernard", ""], ["Asare-Nyarko", "Clara", ""], ["Nyarko", "Samuel", ""], ["Amoaba", "Cynthia", ""], ["Appiah", "Esther Dansoa", ""], ["Akwerh", "Felix", ""], ["Lawson", "Richard Nii Lante", ""], ["Budu", "Joel", ""], ["Debrah", "Emmanuel", ""], ["Boateng", "Nana", ""], ["Ofori", "Wisdom", ""], ["Buabeng-Munkoh", "Edwin", ""], ["Adjei", "Franklin", ""], ["Ampomah", "Isaac Kojo Essel", ""], ["Otoo", "Joseph", ""], ["Borkor", "Reindorf", ""], ["Mensah", "Standylove Birago", ""], ["Mensah", "Lucien", ""], ["Marcel", "Mark Amoako", ""], ["Amponsah", "Anokye Acheampong", ""], ["Hayfron-Acquah", "James Ben", ""]]}, {"id": "2103.15500", "submitter": "Zehong Cao Dr.", "authors": "Xinping Liu, Zehong Cao", "title": "Retrieving Event-related Human Brain Dynamics from Natural Sentence\n  Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electroencephalography (EEG) signals recordings when people reading natural\nlanguages are commonly used as a cognitive method to interpret human language\nunderstanding in neuroscience and psycholinguistics. Previous studies have\ndemonstrated that the human fixation and activation in word reading associated\nwith some brain regions, but it is not clear when and how to measure the brain\ndynamics across time and frequency domains. In this study, we propose the first\nanalysis of event-related brain potentials (ERPs), and event-related spectral\nperturbations (ERSPs) on benchmark datasets which consist of sentence-level\nsimultaneous EEG and related eye-tracking recorded from human natural reading\nexperiment tasks. Our results showed peaks evoked at around 162 ms after the\nstimulus (starting to read each sentence) in the occipital area, indicating the\nbrain retriving lexical and semantic visual information processing approaching\n200 ms from the sentence onset. Furthermore, the occipital ERP around 200ms\npresents negative power and positive power in short and long reaction times. In\naddition, the occipital ERSP around 200ms demonstrated increased high gamma and\ndecreased low beta and low gamma power, relative to the baseline. Our results\nimplied that most of the semantic-perception responses occurred around the\n200ms in alpha, beta and gamma bands of EEG signals. Our findings also provide\npotential impacts on promoting cognitive natural language processing models\nevaluation from EEG dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:11:34 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Xinping", ""], ["Cao", "Zehong", ""]]}, {"id": "2103.15515", "submitter": "Cong-Thanh Do", "authors": "Cong-Thanh Do, Rama Doddipatla, Thomas Hain", "title": "Multiple-hypothesis CTC-based semi-supervised adaptation of end-to-end\n  speech recognition", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an adaptation method for end-to-end speech recognition.\nIn this method, multiple automatic speech recognition (ASR) 1-best hypotheses\nare integrated in the computation of the connectionist temporal classification\n(CTC) loss function. The integration of multiple ASR hypotheses helps\nalleviating the impact of errors in the ASR hypotheses to the computation of\nthe CTC loss when ASR hypotheses are used. When being applied in\nsemi-supervised adaptation scenarios where part of the adaptation data do not\nhave labels, the CTC loss of the proposed method is computed from different ASR\n1-best hypotheses obtained by decoding the unlabeled adaptation data.\nExperiments are performed in clean and multi-condition training scenarios where\nthe CTC-based end-to-end ASR systems are trained on Wall Street Journal (WSJ)\nclean training data and CHiME-4 multi-condition training data, respectively,\nand tested on Aurora-4 test data. The proposed adaptation method yields 6.6%\nand 5.8% relative word error rate (WER) reductions in clean and multi-condition\ntraining scenarios, respectively, compared to a baseline system which is\nadapted with part of the adaptation data having manual transcriptions using\nback-propagation fine-tuning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:38:35 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:30:35 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Do", "Cong-Thanh", ""], ["Doddipatla", "Rama", ""], ["Hain", "Thomas", ""]]}, {"id": "2103.15543", "submitter": "Wenkai Yang", "authors": "Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, Bin He", "title": "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability\n  of the Embedding Layers in NLP Models", "comments": "NAACL-HLT 2021, Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have revealed a security threat to natural language processing\n(NLP) models, called the Backdoor Attack. Victim models can maintain\ncompetitive performance on clean samples while behaving abnormally on samples\nwith a specific trigger word inserted. Previous backdoor attacking methods\nusually assume that attackers have a certain degree of data knowledge, either\nthe dataset which users would use or proxy datasets for a similar task, for\nimplementing the data poisoning procedure. However, in this paper, we find that\nit is possible to hack the model in a data-free way by modifying one single\nword embedding vector, with almost no accuracy sacrificed on clean samples.\nExperimental results on sentiment analysis and sentence-pair classification\ntasks show that our method is more efficient and stealthier. We hope this work\ncan raise the awareness of such a critical security risk hidden in the\nembedding layers of NLP models. Our code is available at\nhttps://github.com/lancopku/Embedding-Poisoning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:19:45 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Wenkai", ""], ["Li", "Lei", ""], ["Zhang", "Zhiyuan", ""], ["Ren", "Xuancheng", ""], ["Sun", "Xu", ""], ["He", "Bin", ""]]}, {"id": "2103.15625", "submitter": "Salomey Osei", "authors": "Paul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi,\n  Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel\n  Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante\n  Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin\n  Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo,\n  Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel,\n  Anokye Acheampong Amponsah, James Ben Hayfron-Acquah", "title": "English-Twi Parallel Corpus for Machine Translation", "comments": "9 pages paper, Accepted at African NLP workshop @EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel machine translation training corpus for English and\nAkuapem Twi of 25,421 sentence pairs. We used a transformer-based translator to\ngenerate initial translations in Akuapem Twi, which were later verified and\ncorrected where necessary by native speakers to eliminate any occurrence of\ntranslationese. In addition, 697 higher quality crowd-sourced sentences are\nprovided for use as an evaluation set for downstream Natural Language\nProcessing (NLP) tasks. The typical use case for the larger human-verified\ndataset is for further training of machine translation models in Akuapem Twi.\nThe higher quality 697 crowd-sourced dataset is recommended as a testing\ndataset for machine translation of English to Twi and Twi to English models.\nFurthermore, the Twi part of the crowd-sourced data may also be used for other\ntasks, such as representation learning, classification, etc. We fine-tune the\ntransformer translation model on the training corpus and report benchmarks on\nthe crowd-sourced test set.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:04:57 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:12:46 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 15:31:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Azunre", "Paul", ""], ["Osei", "Salomey", ""], ["Addo", "Salomey", ""], ["Adu-Gyamfi", "Lawrence Asamoah", ""], ["Moore", "Stephen", ""], ["Adabankah", "Bernard", ""], ["Opoku", "Bernard", ""], ["Asare-Nyarko", "Clara", ""], ["Nyarko", "Samuel", ""], ["Amoaba", "Cynthia", ""], ["Appiah", "Esther Dansoa", ""], ["Akwerh", "Felix", ""], ["Lawson", "Richard Nii Lante", ""], ["Budu", "Joel", ""], ["Debrah", "Emmanuel", ""], ["Boateng", "Nana", ""], ["Ofori", "Wisdom", ""], ["Buabeng-Munkoh", "Edwin", ""], ["Adjei", "Franklin", ""], ["Ampomah", "Isaac Kojo Essel", ""], ["Otoo", "Joseph", ""], ["Borkor", "Reindorf", ""], ["Mensah", "Standylove Birago", ""], ["Mensah", "Lucien", ""], ["Marcel", "Mark Amoako", ""], ["Amponsah", "Anokye Acheampong", ""], ["Hayfron-Acquah", "James Ben", ""]]}, {"id": "2103.15721", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May,\n  Jonathan Gratch", "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic\n  Negotiation Systems", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated systems that negotiate with humans have broad applications in\npedagogy and conversational AI. To advance the development of practical\nnegotiation systems, we present CaSiNo: a novel corpus of over a thousand\nnegotiation dialogues in English. Participants take the role of campsite\nneighbors and negotiate for food, water, and firewood packages for their\nupcoming trip. Our design results in diverse and linguistically rich\nnegotiations while maintaining a tractable, closed-domain environment. Inspired\nby the literature in human-human negotiations, we annotate persuasion\nstrategies and perform correlation analysis to understand how the dialogue\nbehaviors are associated with the negotiation performance. We further propose\nand evaluate a multi-task framework to recognize these strategies in a given\nutterance. We find that multi-task learning substantially improves the\nperformance for all strategy labels, especially for the ones that are the most\nskewed. We release the dataset, annotations, and the code to propel future work\nin human-machine negotiations: https://github.com/kushalchawla/CaSiNo\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:07:25 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 02:36:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chawla", "Kushal", ""], ["Ramirez", "Jaysa", ""], ["Clever", "Rene", ""], ["Lucas", "Gale", ""], ["May", "Jonathan", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2103.15722", "submitter": "Chengdong Liang", "authors": "Chengdong Liang, Menglong Xu, Xiao-Lei Zhang", "title": "Transformer-based end-to-end speech recognition with residual\n  Gaussian-based self-attention", "comments": "Accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:09:00 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 16:09:10 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 01:49:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liang", "Chengdong", ""], ["Xu", "Menglong", ""], ["Zhang", "Xiao-Lei", ""]]}, {"id": "2103.15737", "submitter": "Arpit Sharma", "authors": "Pratik Jayarao and Arpit Sharma", "title": "Retraining DistilBERT for a Voice Shopping Assistant by Using Universal\n  Dependencies", "comments": "Published in the Proceedings of The Fourth Workshop on Reasoning and\n  Learning for Human-Machine Dialogues at the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we retrained the distilled BERT language model for Walmart's\nvoice shopping assistant on retail domain-specific data. We also injected\nuniversal syntactic dependencies to improve the performance of the model\nfurther. The Natural Language Understanding (NLU) components of the voice\nassistants available today are heavily dependent on language models for various\ntasks. The generic language models such as BERT and RoBERTa are useful for\ndomain-independent assistants but have limitations when they cater to a\nspecific domain. For example, in the shopping domain, the token 'horizon' means\na brand instead of its literal meaning. Generic models are not able to capture\nsuch subtleties. So, in this work, we retrained a distilled version of the BERT\nlanguage model on retail domain-specific data for Walmart's voice shopping\nassistant. We also included universal dependency-based features in the\nretraining process further to improve the performance of the model on\ndownstream tasks. We evaluated the performance of the retrained language model\non four downstream tasks, including intent-entity detection, sentiment\nanalysis, voice title shortening and proactive intent suggestion. We observed\nan increase in the performance of all the downstream tasks of up to 1.31% on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:24:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jayarao", "Pratik", ""], ["Sharma", "Arpit", ""]]}, {"id": "2103.15760", "submitter": "Zilun Peng", "authors": "Zilun Peng, Akshay Budhkar, Ilana Tuil, Jason Levy, Parinaz Sobhani,\n  Raphael Cohen, Jumana Nassour", "title": "Shrinking Bigfoot: Reducing wav2vec 2.0 footprint", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wav2vec 2.0 is a state-of-the-art speech recognition model which maps speech\naudio waveforms into latent representations. The largest version of wav2vec 2.0\ncontains 317 million parameters. Hence, the inference latency of wav2vec 2.0\nwill be a bottleneck in production, leading to high costs and a significant\nenvironmental footprint. To improve wav2vec's applicability to a production\nsetting, we explore multiple model compression methods borrowed from the domain\nof large language models. Using a teacher-student approach, we distilled the\nknowledge from the original wav2vec 2.0 model into a student model, which is 2\ntimes faster and 4.8 times smaller than the original model. This increase in\nperformance is accomplished with only a 7% degradation in word error rate\n(WER). Our quantized model is 3.6 times smaller than the original model, with\nonly a 0.1% degradation in WER. To the best of our knowledge, this is the first\nwork that compresses wav2vec 2.0.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:50:28 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 14:57:08 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Peng", "Zilun", ""], ["Budhkar", "Akshay", ""], ["Tuil", "Ilana", ""], ["Levy", "Jason", ""], ["Sobhani", "Parinaz", ""], ["Cohen", "Raphael", ""], ["Nassour", "Jumana", ""]]}, {"id": "2103.15845", "submitter": "Andrew Zupon", "authors": "Andrew Zupon, Evan Crew, Sandy Ritchie", "title": "Text Normalization for Low-Resource Languages of Africa", "comments": "to be presented at AfricaNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training data for machine learning models can come from many different\nsources, which can be of dubious quality. For resource-rich languages like\nEnglish, there is a lot of data available, so we can afford to throw out the\ndubious data. For low-resource languages where there is much less data\navailable, we can't necessarily afford to throw out the dubious data, in case\nwe end up with a training set which is too small to train a model. In this\nstudy, we examine the effects of text normalization and data set quality for a\nset of low-resource languages of Africa -- Afrikaans, Amharic, Hausa, Igbo,\nMalagasy, Somali, Swahili, and Zulu. We describe our text normalizer which we\nbuilt in the Pynini framework, a Python library for finite state transducers,\nand our experiments in training language models for African languages using the\nNatural Language Toolkit (NLTK), an open-source Python library for NLP.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:00:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zupon", "Andrew", ""], ["Crew", "Evan", ""], ["Ritchie", "Sandy", ""]]}, {"id": "2103.15871", "submitter": "Varun Kumar", "authors": "Luoxin Chen, Francisco Garcia, Varun Kumar, He Xie, Jianhua Lu", "title": "Industry Scale Semi-Supervised Learning for Natural Language\n  Understanding", "comments": "NAACL 2021 Industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a production Semi-Supervised Learning (SSL) pipeline\nbased on the student-teacher framework, which leverages millions of unlabeled\nexamples to improve Natural Language Understanding (NLU) tasks. We investigate\ntwo questions related to the use of unlabeled data in production SSL context:\n1) how to select samples from a huge unlabeled data pool that are beneficial\nfor SSL training, and 2) how do the selected data affect the performance of\ndifferent state-of-the-art SSL techniques. We compare four widely used SSL\ntechniques, Pseudo-Label (PL), Knowledge Distillation (KD), Virtual Adversarial\nTraining (VAT) and Cross-View Training (CVT) in conjunction with two data\nselection methods including committee-based selection and submodular\noptimization based selection. We further examine the benefits and drawbacks of\nthese techniques when applied to intent classification (IC) and named entity\nrecognition (NER) tasks, and provide guidelines specifying when each of these\nmethods might be beneficial to improve large scale NLU systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:24:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Luoxin", ""], ["Garcia", "Francisco", ""], ["Kumar", "Varun", ""], ["Xie", "He", ""], ["Lu", "Jianhua", ""]]}, {"id": "2103.15877", "submitter": "Sai Koneru", "authors": "Sai Koneru, Danni Liu and Jan Niehues", "title": "Unsupervised Machine Translation On Dravidian Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised neural machine translation (UNMT) is beneficial especially for\nlow resource languages such as those from the Dravidian family. However, UNMT\nsystems tend to fail in realistic scenarios involving actual low resource\nlanguages. Recent works propose to utilize auxiliary parallel data and have\nachieved state-of-the-art results. In this work, we focus on unsupervised\ntranslation between English and Kannada, a low resource Dravidian language. We\nadditionally utilize a limited amount of auxiliary data between English and\nother related Dravidian languages. We show that unifying the writing systems is\nessential in unsupervised translation between the Dravidian languages. We\nexplore several model architectures that use the auxiliary data in order to\nmaximize knowledge sharing and enable UNMT for distant language pairs. Our\nexperiments demonstrate that it is crucial to include auxiliary languages that\nare similar to our focal language, Kannada. Furthermore, we propose a metric to\nmeasure language similarity and show that it serves as a good indicator for\nselecting the auxiliary languages.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:33:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Koneru", "Sai", ""], ["Liu", "Danni", ""], ["Niehues", "Jan", ""]]}, {"id": "2103.15909", "submitter": "Massimo Stella", "authors": "Massimo Stella, Michael S. Vitevitch and Federico Botta", "title": "Cognitive networks identify the content of English and Italian popular\n  posts about COVID-19 vaccines: Anticipation, logistics, conspiracy and loss\n  of trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monitoring social discourse about COVID-19 vaccines is key to understanding\nhow large populations perceive vaccination campaigns. We focus on 4765 unique\npopular tweets in English or Italian about COVID-19 vaccines between 12/2020\nand 03/2021. One popular English tweet was liked up to 495,000 times, stressing\nhow popular tweets affected cognitively massive populations. We investigate\nboth text and multimedia in tweets, building a knowledge graph of\nsyntactic/semantic associations in messages including visual features and\nindicating how online users framed social discourse mostly around the logistics\nof vaccine distribution. The English semantic frame of \"vaccine\" was highly\npolarised between trust/anticipation (towards the vaccine as a scientific asset\nsaving lives) and anger/sadness (mentioning critical issues with dose\nadministering). Semantic associations with \"vaccine,\" \"hoax\" and conspiratorial\njargon indicated the persistence of conspiracy theories and vaccines in\nmassively read English posts (absent in Italian messages). The image analysis\nfound that popular tweets with images of people wearing face masks used\nlanguage lacking the trust and joy found in tweets showing people with no\nmasks, indicating a negative affect attributed to face covering in social\ndiscourse. A behavioural analysis revealed a tendency for users to share\ncontent eliciting joy, sadness and disgust and to like less sad messages,\nhighlighting an interplay between emotions and content diffusion beyond\nsentiment. With the AstraZeneca vaccine being suspended in mid March 2021,\n\"Astrazeneca\" was associated with trustful language driven by experts, but\npopular Italian tweets framed \"vaccine\" by crucially replacing earlier levels\nof trust with deep sadness. Our results stress how cognitive networks and\ninnovative multimedia processing open new ways for reconstructing online\nperceptions about vaccines and trust.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:38:13 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Stella", "Massimo", ""], ["Vitevitch", "Michael S.", ""], ["Botta", "Federico", ""]]}, {"id": "2103.15912", "submitter": "Maria Mihaela Trusca", "authors": "Tomas Liesting, Flavius Frasincar, Maria Mihaela Trusca", "title": "Data Augmentation in a Hybrid Approach for Aspect-Based Sentiment\n  Analysis", "comments": "The 36th ACM/SIGAPP Symposium On Applied Computing, Virtual\n  Conference, March 22-March 26, 202", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a way to increase the diversity of available data by\napplying constrained transformations on the original data. This strategy has\nbeen widely used in image classification but has to the best of our knowledge\nnot yet been used in aspect-based sentiment analysis (ABSA). ABSA is a text\nanalysis technique that determines aspects and their associated sentiment in\nopinionated text. In this paper, we investigate the effect of data augmentation\non a state-of-the-art hybrid approach for aspect-based sentiment analysis\n(HAABSA). We apply modified versions of easy data augmentation (EDA),\nbacktranslation, and word mixup. We evaluate the proposed techniques on the\nSemEval 2015 and SemEval 2016 datasets. The best result is obtained with the\nadjusted version of EDA, which yields a 0.5 percentage point improvement on the\nSemEval 2016 dataset and 1 percentage point increase on the SemEval 2015\ndataset compared to the original HAABSA model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:43:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liesting", "Tomas", ""], ["Frasincar", "Flavius", ""], ["Trusca", "Maria Mihaela", ""]]}, {"id": "2103.15927", "submitter": "Maria Mihaela Trusca", "authors": "Lisa Meijer, Flavius Frasincar, Maria Mihaela Trusca", "title": "Explaining a Neural Attention Model for Aspect-Based Sentiment\n  Classification Using Diagnostic Classification", "comments": "The 36th ACM/SIGAPP Symposium On Applied Computing, Virtual\n  Conference, March 22-March 26, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high performance machine learning models for Aspect-Based Sentiment\nClassification (ABSC) produce black box models, and therefore barely explain\nhow they classify a certain sentiment value towards an aspect. In this paper,\nwe propose explanation models, that inspect the internal dynamics of a\nstate-of-the-art neural attention model, the LCR-Rot-hop, by using a technique\ncalled Diagnostic Classification. Our diagnostic classifier is a simple neural\nnetwork, which evaluates whether the internal layers of the LCR-Rot-hop model\nencode useful word information for classification, i.e., the part of speech,\nthe sentiment value, the presence of aspect relation, and the aspect-related\nsentiment value of words. We conclude that the lower layers in the LCR-Rot-hop\nmodel encode the part of speech and the sentiment value, whereas the higher\nlayers represent the presence of a relation with the aspect and the\naspect-related sentiment value of words.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:59:34 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Meijer", "Lisa", ""], ["Frasincar", "Flavius", ""], ["Trusca", "Maria Mihaela", ""]]}, {"id": "2103.15949", "submitter": "Yubei Chen", "authors": "Zeyu Yun, Yubei Chen, Bruno A Olshausen, Yann LeCun", "title": "Transformer visualization via dictionary learning: contextualized\n  embedding as a linear superposition of transformer factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer networks have revolutionized NLP representation learning since\nthey were introduced. Though a great effort has been made to explain the\nrepresentation in transformers, it is widely recognized that our understanding\nis not sufficient. One important reason is that there lack enough visualization\ntools for detailed analysis. In this paper, we propose to use dictionary\nlearning to open up these `black boxes' as linear superpositions of transformer\nfactors. Through visualization, we demonstrate the hierarchical semantic\nstructures captured by the transformer factors, e.g. word-level polysemy\ndisambiguation, sentence-level pattern formation, and long-range dependency.\nWhile some of these patterns confirm the conventional prior linguistic\nknowledge, the rest are relatively unexpected, which may provide new insights.\nWe hope this visualization tool can bring further knowledge and a better\nunderstanding of how transformer networks work.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:51:33 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yun", "Zeyu", ""], ["Chen", "Yubei", ""], ["Olshausen", "Bruno A", ""], ["LeCun", "Yann", ""]]}, {"id": "2103.15950", "submitter": "Kalpa Gunaratna", "authors": "Kalpa Gunaratna, Yu Wang, Hongxia Jin", "title": "Entity Context Graph: Learning Entity Representations\n  fromSemi-Structured Textual Sources on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge is captured in the form of entities and their relationships and\nstored in knowledge graphs. Knowledge graphs enhance the capabilities of\napplications in many different areas including Web search, recommendation, and\nnatural language understanding. This is mainly because, entities enable\nmachines to understand things that go beyond simple tokens. Many modern\nalgorithms use learned entity embeddings from these structured representations.\nHowever, building a knowledge graph takes time and effort, hence very costly\nand nontrivial. On the other hand, many Web sources describe entities in some\nstructured format and therefore, finding ways to get them into useful entity\nknowledge is advantageous. We propose an approach that processes entity centric\ntextual knowledge sources to learn entity embeddings and in turn avoids the\nneed for a traditional knowledge graph. We first extract triples into the new\nrepresentation format that does not use traditional complex triple extraction\nmethods defined by pre-determined relationship labels. Then we learn entity\nembeddings through this new type of triples. We show that the embeddings\nlearned from our approach are: (i) high quality and comparable to a known\nknowledge graph-based embeddings and can be used to improve them further, (ii)\nbetter than a contextual language model-based entity embeddings, and (iii) easy\nto compute and versatile in domain-specific applications where a knowledge\ngraph is not readily available\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:52:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gunaratna", "Kalpa", ""], ["Wang", "Yu", ""], ["Jin", "Hongxia", ""]]}, {"id": "2103.15953", "submitter": "Jussi Karlgren", "authors": "Rosie Jones, Ben Carterette, Ann Clifton, Maria Eskevich, Gareth J. F.\n  Jones, Jussi Karlgren, Aasish Pappu, Sravana Reddy, Yongze Yu", "title": "TREC 2020 Podcasts Track Overview", "comments": null, "journal-ref": "The Proceedings of the Twenty-Ninth Text REtrieval Conference\n  Proceedings (TREC 2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Podcast Track is new at the Text Retrieval Conference (TREC) in 2020. The\npodcast track was designed to encourage research into podcasts in the\ninformation retrieval and NLP research communities. The track consisted of two\nshared tasks: segment retrieval and summarization, both based on a dataset of\nover 100,000 podcast episodes (metadata, audio, and automatic transcripts)\nwhich was released concurrently with the track. The track generated\nconsiderable interest, attracted hundreds of new registrations to TREC and\nfifteen teams, mostly disjoint between search and summarization, made final\nsubmissions for assessment. Deep learning was the dominant experimental\napproach for both search experiments and summarization. This paper gives an\noverview of the tasks and the results of the participants' experiments. The\ntrack will return to TREC 2021 with the same two tasks, incorporating slight\nmodifications in response to participant feedback.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:58:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jones", "Rosie", ""], ["Carterette", "Ben", ""], ["Clifton", "Ann", ""], ["Eskevich", "Maria", ""], ["Jones", "Gareth J. F.", ""], ["Karlgren", "Jussi", ""], ["Pappu", "Aasish", ""], ["Reddy", "Sravana", ""], ["Yu", "Yongze", ""]]}, {"id": "2103.15963", "submitter": "Salomey Osei", "authors": "Paul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi,\n  Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel\n  Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante\n  Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin\n  Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo,\n  Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel,\n  Anokye Acheampong Amponsah, James Ben Hayfron-Acquah", "title": "Contextual Text Embeddings for Twi", "comments": "10 pages paper; Accepted at African NLP Workshop @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based language models have been changing the modern Natural\nLanguage Processing (NLP) landscape for high-resource languages such as\nEnglish, Chinese, Russian, etc. However, this technology does not yet exist for\nany Ghanaian language. In this paper, we introduce the first of such models for\nTwi or Akan, the most widely spoken Ghanaian language. The specific\ncontribution of this research work is the development of several pretrained\ntransformer language models for the Akuapem and Asante dialects of Twi, paving\nthe way for advances in application areas such as Named Entity Recognition\n(NER), Neural Machine Translation (NMT), Sentiment Analysis (SA) and\nPart-of-Speech (POS) tagging. Specifically, we introduce four different\nflavours of ABENA -- A BERT model Now in Akan that is fine-tuned on a set of\nAkan corpora, and BAKO - BERT with Akan Knowledge only, which is trained from\nscratch. We open-source the model through the Hugging Face model hub and\ndemonstrate its use via a simple sentiment classification example.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:36:44 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:03:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Azunre", "Paul", ""], ["Osei", "Salomey", ""], ["Addo", "Salomey", ""], ["Adu-Gyamfi", "Lawrence Asamoah", ""], ["Moore", "Stephen", ""], ["Adabankah", "Bernard", ""], ["Opoku", "Bernard", ""], ["Asare-Nyarko", "Clara", ""], ["Nyarko", "Samuel", ""], ["Amoaba", "Cynthia", ""], ["Appiah", "Esther Dansoa", ""], ["Akwerh", "Felix", ""], ["Lawson", "Richard Nii Lante", ""], ["Budu", "Joel", ""], ["Debrah", "Emmanuel", ""], ["Boateng", "Nana", ""], ["Ofori", "Wisdom", ""], ["Buabeng-Munkoh", "Edwin", ""], ["Adjei", "Franklin", ""], ["Ampomah", "Isaac Kojo Essel", ""], ["Otoo", "Joseph", ""], ["Borkor", "Reindorf", ""], ["Mensah", "Standylove Birago", ""], ["Mensah", "Lucien", ""], ["Marcel", "Mark Amoako", ""], ["Amponsah", "Anokye Acheampong", ""], ["Hayfron-Acquah", "James Ben", ""]]}, {"id": "2103.16002", "submitter": "Madeleine Grunde-McLaughlin", "authors": "Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala", "title": "AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning", "comments": "8 pages, 15 pages supplementary, 12 figures. To be published in CVPR\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual events are a composition of temporal actions involving actors\nspatially interacting with objects. When developing computer vision models that\ncan reason about compositional spatio-temporal events, we need benchmarks that\ncan analyze progress and uncover shortcomings. Existing video question\nanswering benchmarks are useful, but they often conflate multiple sources of\nerror into one accuracy metric and have strong biases that models can exploit,\nmaking it difficult to pinpoint model weaknesses. We present Action Genome\nQuestion Answering (AGQA), a new benchmark for compositional spatio-temporal\nreasoning. AGQA contains $192M$ unbalanced question answer pairs for $9.6K$\nvideos. We also provide a balanced subset of $3.9M$ question answer pairs, $3$\norders of magnitude larger than existing benchmarks, that minimizes bias by\nbalancing the answer distributions and types of question structures. Although\nhuman evaluators marked $86.02\\%$ of our question-answer pairs as correct, the\nbest model achieves only $47.74\\%$ accuracy. In addition, AGQA introduces\nmultiple training/test splits to test for various reasoning abilities,\nincluding generalization to novel compositions, to indirect references, and to\nmore compositional steps. Using AGQA, we evaluate modern visual reasoning\nsystems, demonstrating that the best models barely perform better than\nnon-visual baselines exploiting linguistic biases and that none of the existing\nmodels generalize to novel compositions unseen during training.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:24:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Grunde-McLaughlin", "Madeleine", ""], ["Krishna", "Ranjay", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "2103.16057", "submitter": "Nancy Xu", "authors": "Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck,\n  James Landay, Monica S Lam", "title": "Grounding Open-Domain Instructions to Automate Web Support Tasks", "comments": "To be published in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding natural language instructions on the web to perform previously\nunseen tasks enables accessibility and automation. We introduce a task and\ndataset to train AI agents from open-domain, step-by-step instructions\noriginally written for people. We build RUSS (Rapid Universal Support Service)\nto tackle this problem. RUSS consists of two models: First, a BERT-LSTM with\npointers parses instructions to ThingTalk, a domain-specific language we design\nfor grounding natural language on the web. Then, a grounding model retrieves\nthe unique IDs of any webpage elements requested in ThingTalk. RUSS may\ninteract with the user through a dialogue (e.g. ask for an address) or execute\na web operation (e.g. click a button) inside the web runtime. To augment\ntraining, we synthesize natural language instructions mapped to ThingTalk. Our\ndataset consists of 80 different customer service problems from help websites,\nwith a total of 741 step-by-step instructions and their corresponding actions.\nRUSS achieves 76.7% end-to-end accuracy predicting agent actions from single\ninstructions. It outperforms state-of-the-art models that directly map\ninstructions to actions without ThingTalk. Our user study shows that RUSS is\npreferred by actual users over web navigation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:02:34 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:51:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xu", "Nancy", ""], ["Masling", "Sam", ""], ["Du", "Michael", ""], ["Campagna", "Giovanni", ""], ["Heck", "Larry", ""], ["Landay", "James", ""], ["Lam", "Monica S", ""]]}, {"id": "2103.16102", "submitter": "Yuxin Jiang", "authors": "Yuxin Jiang, Ziyi Shou, Qijun Wang, Hao Wu and Fangzhen Lin", "title": "XRJL-HKUST at SemEval-2021 Task 4: WordNet-Enhanced Dual Multi-head\n  Co-Attention for Reading Comprehension of Abstract Meaning", "comments": "6 pages, 5 figures, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our submitted system to SemEval 2021 Task 4: Reading\nComprehension of Abstract Meaning. Our system uses a large pre-trained language\nmodel as the encoder and an additional dual multi-head co-attention layer to\nstrengthen the relationship between passages and question-answer pairs,\nfollowing the current state-of-the-art model DUMA. The main difference is that\nwe stack the passage-question and question-passage attention modules instead of\ncalculating parallelly to simulate re-considering process. We also add a layer\nnormalization module to improve the performance of our model. Furthermore, to\nincorporate our known knowledge about abstract concepts, we retrieve the\ndefinitions of candidate answers from WordNet and feed them to the model as\nextra inputs. Our system, called WordNet-enhanced DUal Multi-head Co-Attention\n(WN-DUMA), achieves 86.67% and 89.99% accuracy on the official blind test set\nof subtask 1 and subtask 2 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:22:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jiang", "Yuxin", ""], ["Shou", "Ziyi", ""], ["Wang", "Qijun", ""], ["Wu", "Hao", ""], ["Lin", "Fangzhen", ""]]}, {"id": "2103.16189", "submitter": "Tao Wang", "authors": "Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, Deyi Xiong", "title": "Autocorrect in the Process of Translation -- Multi-task Learning\n  Improves Dialogue Machine Translation", "comments": "8 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Automatic translation of dialogue texts is a much needed demand in many real\nlife scenarios. However, the currently existing neural machine translation\ndelivers unsatisfying results. In this paper, we conduct a deep analysis of a\ndialogue corpus and summarize three major issues on dialogue translation,\nincluding pronoun dropping (\\droppro), punctuation dropping (\\droppun), and\ntypos (\\typo). In response to these challenges, we propose a joint learning\nmethod to identify omission and typo, and utilize context to translate dialogue\nutterances. To properly evaluate the performance, we propose a manually\nannotated dataset with 1,931 Chinese-English parallel utterances from 300\ndialogues as a benchmark testbed for dialogue translation. Our experiments show\nthat the proposed method improves translation quality by 3.2 BLEU over the\nbaselines. It also elevates the recovery rate of omitted pronouns from 26.09%\nto 47.16%. We will publish the code and dataset publicly at\nhttps://github.com/rgwt123/DialogueMT.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:12:47 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 08:04:28 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Tao", ""], ["Zhao", "Chengqi", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""], ["Xiong", "Deyi", ""]]}, {"id": "2103.16190", "submitter": "Anil Bas", "authors": "Imke van Heerden and Anil Bas", "title": "AfriKI: Machine-in-the-Loop Afrikaans Poetry Generation", "comments": "Accepted to EACL 2021 Workshop on AfricaNLP (non-archival)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generative language model called AfriKI. Our approach\nis based on an LSTM architecture trained on a small corpus of contemporary\nfiction. With the aim of promoting human creativity, we use the model as an\nauthoring tool to explore machine-in-the-loop Afrikaans poetry generation. To\nour knowledge, this is the first study to attempt creative text generation in\nAfrikaans.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:17:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["van Heerden", "Imke", ""], ["Bas", "Anil", ""]]}, {"id": "2103.16210", "submitter": "Harshil Shah", "authors": "Harshil Shah, Tim Xiao, David Barber", "title": "Locally-Contextual Nonlinear CRFs for Sequence Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear chain conditional random fields (CRFs) combined with contextual word\nembeddings have achieved state of the art performance on sequence labeling\ntasks. In many of these tasks, the identity of the neighboring words is often\nthe most useful contextual information when predicting the label of a given\nword. However, contextual embeddings are usually trained in a task-agnostic\nmanner. This means that although they may encode information about the\nneighboring words, it is not guaranteed. It can therefore be beneficial to\ndesign the sequence labeling architecture to directly extract this information\nfrom the embeddings. We propose locally-contextual nonlinear CRFs for sequence\nlabeling. Our approach directly incorporates information from the neighboring\nembeddings when predicting the label for a given word, and parametrizes the\npotential functions using deep neural networks. Our model serves as a drop-in\nreplacement for the linear chain CRF, consistently outperforming it in our\nablation study. On a variety of tasks, our results are competitive with those\nof the best published methods. In particular, we outperform the previous state\nof the art on chunking on CoNLL 2000 and named entity recognition on OntoNotes\n5.0 English.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:43:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shah", "Harshil", ""], ["Xiao", "Tim", ""], ["Barber", "David", ""]]}, {"id": "2103.16289", "submitter": "Md Rashad Al Hasan Rony", "authors": "Debanjan Chaudhuri, Md Rashad Al Hasan Rony, Jens Lehmann", "title": "Grounding Dialogue Systems via Knowledge Graph Aware Decoding with\n  Pre-trained Transformers", "comments": "16 pages, 3 figures, accepted at ESWC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating knowledge grounded responses in both goal and non-goal oriented\ndialogue systems is an important research challenge. Knowledge Graphs (KG) can\nbe viewed as an abstraction of the real world, which can potentially facilitate\na dialogue system to produce knowledge grounded responses. However, integrating\nKGs into the dialogue generation process in an end-to-end manner is a\nnon-trivial task. This paper proposes a novel architecture for integrating KGs\ninto the response generation process by training a BERT model that learns to\nanswer using the elements of the KG (entities and relations) in a multi-task,\nend-to-end setting. The k-hop subgraph of the KG is incorporated into the model\nduring training and inference using Graph Laplacian. Empirical evaluation\nsuggests that the model achieves better knowledge groundedness (measured via\nEntity F1 score) compared to other state-of-the-art models for both goal and\nnon-goal oriented dialogues.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:36:00 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chaudhuri", "Debanjan", ""], ["Rony", "Md Rashad Al Hasan", ""], ["Lehmann", "Jens", ""]]}, {"id": "2103.16325", "submitter": "Samuel Sep\\'ulveda", "authors": "Samuel Sep\\'ulveda, Marcelo Esperguel", "title": "Systematic Mapping Protocol: Reasoning Algorithms on Feature Model", "comments": "Systematic mapping protocol. Keywords: Reasoning algorithm, Feature\n  model, Software product lines, systematic mapping. 13 pages, 2 figures, 6\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Context: The importance of the feature modeling for the software product\nlines considering the modeling and management of the variability. Objective:\nDefine a protocol to conduct a systematic mapping study to summarize and\nsynthesize the evidence on reasoning algorithms for feature modeling. Method:\nApplication the protocol to conduct a systematic mapping study according the\nguidelines of K. Petersen. Results: A validated protocol to conduct a\nsystematic mapping study. Conclusions: Initial findings show that a more\ndetailed review for the different reasoning algorithms for feature modeling is\nneeded.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 23:34:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sep\u00falveda", "Samuel", ""], ["Esperguel", "Marcelo", ""]]}, {"id": "2103.16387", "submitter": "Carlo Romano Marcello Alessandro Santagiustina", "authors": "Carlo Santagiustina and Massimo Warglien", "title": "The Unfolding Structure of Arguments in Online Debates: The case of a\n  No-Deal Brexit", "comments": "Main article (18 pages, 7 figures) & Supplementary material (25\n  pages, 7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the last decade, political debates have progressively shifted to social\nmedia. Rhetorical devices employed by online actors and factions that operate\nin these debating arenas can be captured and analysed to conduct a statistical\nreading of societal controversies and their argumentation dynamics. In this\npaper, we propose a five-step methodology, to extract, categorize and explore\nthe latent argumentation structures of online debates. Using Twitter data about\na \"no-deal\" Brexit, we focus on the expected effects in case of materialisation\nof this event. First, we extract cause-effect claims contained in tweets using\nRegEx that exploit verbs related to Creation, Destruction and Causation.\nSecond, we categorise extracted \"no-deal\" effects using a Structural Topic\nModel estimated on unigrams and bigrams. Third, we select controversial effect\ntopics and explore within-topic argumentation differences between self-declared\npartisan user factions. We hence type topics using estimated covariate effects\non topic propensities, then, using the topics correlation network, we study the\ntopological structure of the debate to identify coherent topical\nconstellations. Finally, we analyse the debate time dynamics and infer\nlead/follow relations among factions. Results show that the proposed\nmethodology can be employed to perform a statistical rhetorics analysis of\ndebates, and map the architecture of controversies across time. In particular,\nthe \"no-deal\" Brexit debate is shown to have an assortative argumentation\nstructure heavily characterized by factional constellations of arguments, as\nwell as by polarized narrative frames invoked through verbs related to Creation\nand Destruction. Our findings highlight the benefits of implementing a systemic\napproach to the analysis of debates, which allows the unveiling of topical and\nfactional dependencies between arguments employed in online debates.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:29:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Santagiustina", "Carlo", ""], ["Warglien", "Massimo", ""]]}, {"id": "2103.16414", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov and Elizaveta Kuzmenko", "title": "Representing ELMo embeddings as two-dimensional text online", "comments": "EACL'2021 demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a new addition to the WebVectors toolkit which is used to serve\nword embedding models over the Web. The new ELMoViz module adds support for\ncontextualized embedding architectures, in particular for ELMo models. The\nprovided visualizations follow the metaphor of `two-dimensional text' by\nshowing lexical substitutes: words which are most semantically similar in\ncontext to the words of the input sentence. The system allows the user to\nchange the ELMo layers from which token embeddings are inferred. It also\nconveys corpus information about the query words and their lexical substitutes\n(namely their frequency tiers and parts of speech). The module is well\nintegrated into the rest of the WebVectors toolkit, providing lexical\nhyperlinks to word representations in static embedding models. Two web services\nhave already implemented the new functionality with pre-trained ELMo models for\nRussian, Norwegian and English.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:12:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Kuzmenko", "Elizaveta", ""]]}, {"id": "2103.16429", "submitter": "Hsuan Su", "authors": "Hsuan Su, Jiun-Hao Jhan, Fan-yun Sun, Saurav Sahay, Hung-yi Lee", "title": "Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn\n  Chatbot Responding with Intention", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most chatbot literature that focuses on improving the fluency and coherence\nof a chatbot, is dedicated to making chatbots more human-like. However, very\nlittle work delves into what really separates humans from chatbots -- humans\nintrinsically understand the effect their responses have on the interlocutor\nand often respond with an intention such as proposing an optimistic view to\nmake the interlocutor feel better. This paper proposes an innovative framework\nto train chatbots to possess human-like intentions. Our framework includes a\nguiding chatbot and an interlocutor model that plays the role of humans. The\nguiding chatbot is assigned an intention and learns to induce the interlocutor\nto reply with responses matching the intention, for example, long responses,\njoyful responses, responses with specific words, etc. We examined our framework\nusing three experimental setups and evaluated the guiding chatbot with four\ndifferent metrics to demonstrate flexibility and performance advantages.\nAdditionally, we performed trials with human interlocutors to substantiate the\nguiding chatbot's effectiveness in influencing the responses of humans to a\ncertain extent. Code will be made available to the public.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:24:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 17:39:23 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 03:42:16 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 15:58:42 GMT"}, {"version": "v5", "created": "Fri, 23 Apr 2021 14:45:14 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Su", "Hsuan", ""], ["Jhan", "Jiun-Hao", ""], ["Sun", "Fan-yun", ""], ["Sahay", "Saurav", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2103.16561", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Yuankai Qi, Pradyumna Narayana, Kazoo Sone, Sugato Basu,\n  Xin Eric Wang, Qi Wu, Miguel Eckstein, William Yang Wang", "title": "Diagnosing Vision-and-Language Navigation: What Really Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language navigation (VLN) is a multimodal task where an agent\nfollows natural language instructions and navigates in visual environments.\nMultiple setups have been proposed, and researchers apply new model\narchitectures or training techniques to boost navigation performance. However,\nrecent studies witness a slow-down in the performance improvements in both\nindoor and outdoor VLN tasks, and the agents' inner mechanisms for making\nnavigation decisions remain unclear. To the best of our knowledge, the way the\nagents perceive the multimodal input is under-studied and clearly needs\ninvestigations. In this work, we conduct a series of diagnostic experiments to\nunveil agents' focus during navigation. Results show that indoor navigation\nagents refer to both object tokens and direction tokens in the instruction when\nmaking decisions. In contrast, outdoor navigation agents heavily rely on\ndirection tokens and have a poor understanding of the object tokens.\nFurthermore, instead of merely staring at surrounding objects, indoor\nnavigation agents can set their sights on objects further from the current\nviewpoint. When it comes to vision-and-language alignments, many models claim\nthat they are able to align object tokens with certain visual targets, but we\ncast doubt on the reliability of such alignments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhu", "Wanrong", ""], ["Qi", "Yuankai", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""], ["Wang", "Xin Eric", ""], ["Wu", "Qi", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "2103.16564", "submitter": "Chuang Gan", "authors": "Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua\n  B. Tenenbaum, Chuang Gan", "title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual\n  Reasoning", "comments": "ICLR 2021. Project page: http://dcl.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.SC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of dynamic visual reasoning on raw videos. This is a\nchallenging problem; currently, state-of-the-art models often require dense\nsupervision on physical object properties and events from simulation, which are\nimpractical to obtain in real life. In this paper, we present the Dynamic\nConcept Learner (DCL), a unified framework that grounds physical objects and\nevents from video and language. DCL first adopts a trajectory extractor to\ntrack each object over time and to represent it as a latent, object-centric\nfeature vector. Building upon this object-centric representation, DCL learns to\napproximate the dynamic interaction among objects using graph networks. DCL\nfurther incorporates a semantic parser to parse questions into semantic\nprograms and, finally, a program executor to run the program to answer the\nquestion, levering the learned dynamics model. After training, DCL can detect\nand associate objects across the frames, ground visual properties, and physical\nevents, understand the causal relationship between events, make future and\ncounterfactual predictions, and leverage these extracted presentations for\nanswering queries. DCL achieves state-of-the-art performance on CLEVRER, a\nchallenging causal video reasoning dataset, even without using ground-truth\nattributes and collision labels from simulations for training. We further test\nDCL on a newly proposed video-retrieval and event localization dataset derived\nfrom CLEVRER, showing its strong generalization capacity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:48 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Zhenfang", ""], ["Mao", "Jiayuan", ""], ["Wu", "Jiajun", ""], ["Wong", "Kwan-Yee Kenneth", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2103.16590", "submitter": "Adithya Pratapa", "authors": "Adithya Pratapa, Antonios Anastasopoulos, Shruti Rijhwani, Aditi\n  Chaudhary, David R. Mortensen, Graham Neubig, Yulia Tsvetkov", "title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation systems are ubiquitous in natural language processing\napplications. However, evaluation of these systems remains a challenge,\nespecially in multilingual settings. In this paper, we propose L'AMBRE -- a\nmetric to evaluate the morphosyntactic well-formedness of text using its\ndependency parse and morphosyntactic rules of the language. We present a way to\nautomatically extract various rules governing morphosyntax directly from\ndependency treebanks. To tackle the noisy outputs from text generation systems,\nwe propose a simple methodology to train robust parsers. We show the\neffectiveness of our metric on the task of machine translation through a\ndiachronic study of systems translating into morphologically-rich languages.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:02:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Pratapa", "Adithya", ""], ["Anastasopoulos", "Antonios", ""], ["Rijhwani", "Shruti", ""], ["Chaudhary", "Aditi", ""], ["Mortensen", "David R.", ""], ["Neubig", "Graham", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2103.16624", "submitter": "Ikechukwu Onyenwe", "authors": "D.C. Asogwa, S.O. Anigbogu, I.E. Onyenwe, F.A. Sani", "title": "Text Classification Using Hybrid Machine Learning Algorithms on Big Data", "comments": "8 pages, 2 figures, 8 tables, Journal", "journal-ref": "International Journal of Trend in Research and Development, Volume\n  6(5), ISSN: 2394-9333, 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there are unprecedented data growth originating from different\nonline platforms which contribute to big data in terms of volume, velocity,\nvariety and veracity (4Vs). Given this nature of big data which is\nunstructured, performing analytics to extract meaningful information is\ncurrently a great challenge to big data analytics. Collecting and analyzing\nunstructured textual data allows decision makers to study the escalation of\ncomments/posts on our social media platforms. Hence, there is need for\nautomatic big data analysis to overcome the noise and the non-reliability of\nthese unstructured dataset from the digital media platforms. However, current\nmachine learning algorithms used are performance driven focusing on the\nclassification/prediction accuracy based on known properties learned from the\ntraining samples. With the learning task in a large dataset, most machine\nlearning models are known to require high computational cost which eventually\nleads to computational complexity. In this work, two supervised machine\nlearning algorithms are combined with text mining techniques to produce a\nhybrid model which consists of Na\\\"ive Bayes and support vector machines (SVM).\nThis is to increase the efficiency and accuracy of the results obtained and\nalso to reduce the computational cost and complexity. The system also provides\nan open platform where a group of persons with a common interest can share\ntheir comments/messages and these comments classified automatically as legal or\nillegal. This improves the quality of conversation among users. The hybrid\nmodel was developed using WEKA tools and Java programming language. The result\nshows that the hybrid model gave 96.76% accuracy as against the 61.45% and\n69.21% of the Na\\\"ive Bayes and SVM models respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:02:48 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Asogwa", "D. C.", ""], ["Anigbogu", "S. O.", ""], ["Onyenwe", "I. E.", ""], ["Sani", "F. A.", ""]]}, {"id": "2103.16674", "submitter": "Pu Wang", "authors": "Pu Wang, Hugo Van hamme", "title": "Pre-training for low resource speech-to-intent applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a speech-to-intent (S2I) agent which maps the users' spoken\ncommands to the agents' desired task actions can be challenging due to the\ndiverse grammatical and lexical preference of different users. As a remedy, we\ndiscuss a user-taught S2I system in this paper. The user-taught system learns\nfrom scratch from the users' spoken input with action demonstration, which\nensure it is fully matched to the users' way of formulating intents and their\narticulation habits. The main issue is the scarce training data due to the user\neffort involved. Existing state-of-art approaches in this setting are based on\nnon-negative matrix factorization (NMF) and capsule networks. In this paper we\ncombine the encoder of an end-to-end ASR system with the prior NMF/capsule\nnetwork-based user-taught decoder, and investigate whether pre-training\nmethodology can reduce training data requirements for the NMF and capsule\nnetwork. Experimental results show the pre-trained ASR-NMF framework\nsignificantly outperforms other models, and also, we discuss limitations of\npre-training with different types of command-and-control(C&C) applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 20:44:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Pu", ""], ["Van hamme", "Hugo", ""]]}, {"id": "2103.16710", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Ralf Schl\\\"uter, Hermann Ney", "title": "A study of latent monotonic attention variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end models reach state-of-the-art performance for speech recognition,\nbut global soft attention is not monotonic, which might lead to convergence\nproblems, to instability, to bad generalisation, cannot be used for online\nstreaming, and is also inefficient in calculation. Monotonicity can potentially\nfix all of this. There are several ad-hoc solutions or heuristics to introduce\nmonotonicity, but a principled introduction is rarely found in literature so\nfar. In this paper, we present a mathematically clean solution to introduce\nmonotonicity, by introducing a new latent variable which represents the audio\nposition or segment boundaries. We compare several monotonic latent models to\nour global soft attention baseline such as a hard attention model, a local\nwindowed soft attention model, and a segmental soft attention model. We can\nshow that our monotonic models perform as good as the global soft attention\nmodel. We perform our experiments on Switchboard 300h. We carefully outline the\ndetails of our training and release our code and configs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:35:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2103.16712", "submitter": "Mboning Tchiaze Elvis", "authors": "Elvis Mboning Tchiaze", "title": "Collaborative construction of lexicographic and parallel datasets for\n  African languages: first assessment", "comments": "EACL 2021 - AfricaNLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with a considerable lack of resources in African languages to carry out\nwork in Natural Language Processing (NLP), Natural Language Understanding (NLU)\nand artificial intelligence, the research teams of NTeALan association has set\nitself the objective of building open-source platforms for the collaborative\nconstruction of lexicographic data in African languages. In this article, we\npresent our first reports after 2 years of collaborative construction of\nlexicographic resources useful for African NLP tools.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:43:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tchiaze", "Elvis Mboning", ""]]}, {"id": "2103.16716", "submitter": "Mike Lewis", "authors": "Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, Luke\n  Zettlemoyer", "title": "BASE Layers: Simplifying Training of Large, Sparse Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new balanced assignment of experts (BASE) layer for large\nlanguage models that greatly simplifies existing high capacity sparse layers.\nSparse layers can dramatically improve the efficiency of training and inference\nby routing each token to specialized expert modules that contain only a small\nfraction of the model parameters. However, it can be difficult to learn\nbalanced routing functions that make full use of the available experts;\nexisting approaches typically use routing heuristics or auxiliary\nexpert-balancing loss functions. In contrast, we formulate token-to-expert\nallocation as a linear assignment problem, allowing an optimal assignment in\nwhich each expert receives an equal number of tokens. This optimal assignment\nscheme improves efficiency by guaranteeing balanced compute loads, and also\nsimplifies training by not requiring any new hyperparameters or auxiliary\nlosses. Code is publicly released at https://github.com/pytorch/fairseq/\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:08:32 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Lewis", "Mike", ""], ["Bhosale", "Shruti", ""], ["Dettmers", "Tim", ""], ["Goyal", "Naman", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2103.16750", "submitter": "Tyler Weitzman", "authors": "Tyler Weitzman and Hoon Pyo (Tim) Jeon", "title": "CloneBot: Personalized Dialogue-Response Predictions", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our project task was to create a model that, given a speaker ID, chat\nhistory, and an utterance query, can predict the response utterance in a\nconversation. The model is personalized for each speaker. This task can be a\nuseful tool for building speech bots that talk in a human-like manner in a live\nconversation. Further, we succeeded at using dense-vector encoding clustering\nto be able to retrieve relevant historical dialogue context, a useful strategy\nfor overcoming the input limitations of neural-based models when predictions\nrequire longer-term references from the dialogue history. In this paper, we\nhave implemented a state-of-the-art model using pre-training and fine-tuning\ntechniques built on transformer architecture and multi-headed attention blocks\nfor the Switchboard corpus. We also show how efficient vector clustering\nalgorithms can be used for real-time utterance predictions that require no\ntraining and therefore work on offline and encrypted message histories.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:15:37 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Weitzman", "Tyler", "", "Tim"], ["Pyo", "Hoon", "", "Tim"], ["Jeon", "", ""]]}, {"id": "2103.16776", "submitter": "Naoyuki Kanda", "authors": "Naoyuki Kanda, Guoli Ye, Yu Wu, Yashesh Gaur, Xiaofei Wang, Zhong\n  Meng, Zhuo Chen, Takuya Yoshioka", "title": "Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting\n  Transcription with Single Distant Microphone", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcribing meetings containing overlapped speech with only a single distant\nmicrophone (SDM) has been one of the most challenging problems for automatic\nspeech recognition (ASR). While various approaches have been proposed, all\nprevious studies on the monaural overlapped speech recognition problem were\nbased on either simulation data or small-scale real data. In this paper, we\nextensively investigate a two-step approach where we first pre-train a\nserialized output training (SOT)-based multi-talker ASR by using large-scale\nsimulation data and then fine-tune the model with a small amount of real\nmeeting data. Experiments are conducted by utilizing 75 thousand (K) hours of\nour internal single-talker recording to simulate a total of 900K hours of\nmulti-talker audio segments for supervised pre-training. With fine-tuning on\nthe 70 hours of the AMI-SDM training data, our SOT ASR model achieves a word\nerror rate (WER) of 21.2% for the AMI-SDM evaluation set while automatically\ncounting speakers in each test segment. This result is not only significantly\nbetter than the previous state-of-the-art WER of 36.4% with oracle utterance\nboundary information but also better than a result by a similarly fine-tuned\nsingle-talker ASR model applied to beamformed audio.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:43:32 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 21:02:43 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 21:46:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Kanda", "Naoyuki", ""], ["Ye", "Guoli", ""], ["Wu", "Yu", ""], ["Gaur", "Yashesh", ""], ["Wang", "Xiaofei", ""], ["Meng", "Zhong", ""], ["Chen", "Zhuo", ""], ["Yoshioka", "Takuya", ""]]}, {"id": "2103.16789", "submitter": "Sachin Kumar", "authors": "Lidia Kidane, Sachin Kumar, Yulia Tsvetkov", "title": "An Exploration of Data Augmentation Techniques for Improving English to\n  Tigrinya Translation", "comments": "Accepted at AfricaNLP Workshop, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that the performance of neural machine translation (NMT)\ndrops starkly in low-resource conditions, often requiring large amounts of\nauxiliary data to achieve competitive results. An effective method of\ngenerating auxiliary data is back-translation of target language sentences. In\nthis work, we present a case study of Tigrinya where we investigate several\nback-translation methods to generate synthetic source sentences. We find that\nin low-resource conditions, back-translation by pivoting through a\nhigher-resource language related to the target language proves most effective\nresulting in substantial improvements over baselines.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 03:31:09 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:48:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kidane", "Lidia", ""], ["Kumar", "Sachin", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2103.16801", "submitter": "Rina Buoy", "authors": "Rina Buoy and Nguonly Taing and Sokchea Kor", "title": "Joint Khmer Word Segmentation and Part-of-Speech Tagging Using Deep\n  Learning", "comments": "12 pages, 6 tables, and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Khmer text is written from left to right with optional space. Space is not\nserved as a word boundary but instead, it is used for readability or other\nfunctional purposes. Word segmentation is a prior step for downstream tasks\nsuch as part-of-speech (POS) tagging and thus, the robustness of POS tagging\nhighly depends on word segmentation. The conventional Khmer POS tagging is a\ntwo-stage process that begins with word segmentation and then actual tagging of\neach word, afterward. In this work, a joint word segmentation and POS tagging\napproach using a single deep learning model is proposed so that word\nsegmentation and POS tagging can be performed spontaneously. The proposed model\nwas trained and tested using the publicly available Khmer POS dataset. The\nvalidation suggested that the performance of the joint model is on par with the\nconventional two-stage POS tagging.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:26:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Buoy", "Rina", ""], ["Taing", "Nguonly", ""], ["Kor", "Sokchea", ""]]}, {"id": "2103.16808", "submitter": "Wanzheng Zhu", "authors": "Wanzheng Zhu, Hongyu Gong, Rohan Bansal, Zachary Weinberg, Nicolas\n  Christin, Giulia Fanti, Suma Bhat", "title": "Self-Supervised Euphemism Detection and Identification for Content\n  Moderation", "comments": "18 pages, 5 figures, 10 tables, 42nd IEEE Symposium on Security &\n  Privacy (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fringe groups and organizations have a long history of using\neuphemisms--ordinary-sounding words with a secret meaning--to conceal what they\nare discussing. Nowadays, one common use of euphemisms is to evade content\nmoderation policies enforced by social media platforms. Existing tools for\nenforcing policy automatically rely on keyword searches for words on a \"ban\nlist\", but these are notoriously imprecise: even when limited to swearwords,\nthey can still cause embarrassing false positives. When a commonly used\nordinary word acquires a euphemistic meaning, adding it to a keyword-based ban\nlist is hopeless: consider \"pot\" (storage container or marijuana?) or \"heater\"\n(household appliance or firearm?) The current generation of social media\ncompanies instead hire staff to check posts manually, but this is expensive,\ninhumane, and not much more effective. It is usually apparent to a human\nmoderator that a word is being used euphemistically, but they may not know what\nthe secret meaning is, and therefore whether the message violates policy. Also,\nwhen a euphemism is banned, the group that used it need only invent another\none, leaving moderators one step behind.\n  This paper will demonstrate unsupervised algorithms that, by analyzing words\nin their sentence-level context, can both detect words being used\neuphemistically, and identify the secret meaning of each word. Compared to the\nexisting state of the art, which uses context-free word embeddings, our\nalgorithm for detecting euphemisms achieves 30-400% higher detection accuracies\nof unlabeled euphemisms in a text corpus. Our algorithm for revealing\neuphemistic meanings of words is the first of its kind, as far as we are aware.\nIn the arms race between content moderators and policy evaders, our algorithms\nmay help shift the balance in the direction of the moderators.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:52:38 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhu", "Wanzheng", ""], ["Gong", "Hongyu", ""], ["Bansal", "Rohan", ""], ["Weinberg", "Zachary", ""], ["Christin", "Nicolas", ""], ["Fanti", "Giulia", ""], ["Bhat", "Suma", ""]]}, {"id": "2103.16809", "submitter": "Kun Zhou", "authors": "Kun Zhou, Berrak Sisman, Haizhou Li", "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech:\n  Two-stage Sequence-to-Sequence Training", "comments": "Accepted by Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotional voice conversion (EVC) aims to change the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In this\npaper, we propose a novel 2-stage training strategy for sequence-to-sequence\nemotional voice conversion with a limited amount of emotional speech data. We\nnote that the proposed EVC framework leverages text-to-speech (TTS) as they\nshare a common goal that is to generate high-quality expressive voice. In stage\n1, we perform style initialization with a multi-speaker TTS corpus, to\ndisentangle speaking style and linguistic content. In stage 2, we perform\nemotion training with a limited amount of emotional speech data, to learn how\nto disentangle emotional style and linguistic information from the speech. The\nproposed framework can perform both spectrum and prosody conversion and\nachieves significant improvement over the state-of-the-art baselines in both\nobjective and subjective evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:56:14 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:17:55 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhou", "Kun", ""], ["Sisman", "Berrak", ""], ["Li", "Haizhou", ""]]}, {"id": "2103.16827", "submitter": "Sehoon Kim", "authors": "Sehoon Kim, Amir Gholami, Zhewei Yao, Anirudda Nrusimha, Bohan Zhai,\n  Tianren Gao, Michael W. Mahoney, Kurt Keutzer", "title": "Q-ASR: Integer-only Zero-shot Quantization for Efficient Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end neural network models achieve improved performance on various\nautomatic speech recognition (ASR) tasks. However, these models perform poorly\non edge hardware due to large memory and computation requirements. While\nquantizing model weights and/or activations to low-precision can be a promising\nsolution, previous research on quantizing ASR models is limited. Most\nquantization approaches use floating-point arithmetic during inference; and\nthus they cannot fully exploit integer processing units, which use less power\nthan their floating-point counterparts. Moreover, they require\ntraining/validation data during quantization for finetuning or calibration;\nhowever, this data may not be available due to security/privacy concerns. To\naddress these limitations, we propose Q-ASR, an integer-only, zero-shot\nquantization scheme for ASR models. In particular, we generate synthetic data\nwhose runtime statistics resemble the real data, and we use it to calibrate\nmodels during quantization. We then apply Q-ASR to quantize QuartzNet-15x5 and\nJasperDR-10x5 without any training data, and we show negligible WER change as\ncompared to the full-precision baseline models. For INT8-only quantization, we\nobserve a very modest WER degradation of up to 0.29%, while we achieve up to\n2.44x speedup on a T4 GPU. Furthermore, Q-ASR exhibits a large compression rate\nof more than 4x with small WER degradation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:05:40 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kim", "Sehoon", ""], ["Gholami", "Amir", ""], ["Yao", "Zhewei", ""], ["Nrusimha", "Anirudda", ""], ["Zhai", "Bohan", ""], ["Gao", "Tianren", ""], ["Mahoney", "Michael W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2103.16911", "submitter": "Farid Arthaud", "authors": "Farid Arthaud, Rachel Bawden and Alexandra Birch", "title": "Few-shot learning through contextual data augmentation", "comments": "14 pages includince 3 of appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine translation (MT) models used in industries with constantly changing\ntopics, such as translation or news agencies, need to adapt to new data to\nmaintain their performance over time. Our aim is to teach a pre-trained MT\nmodel to translate previously unseen words accurately, based on very few\nexamples. We propose (i) an experimental setup allowing us to simulate novel\nvocabulary appearing in human-submitted translations, and (ii) corresponding\nevaluation metrics to compare our approaches. We extend a data augmentation\napproach using a pre-trained language model to create training examples with\nsimilar contexts for novel words. We compare different fine-tuning and data\naugmentation approaches and show that adaptation on the scale of one to five\nexamples is possible. Combining data augmentation with randomly selected\ntraining sentences leads to the highest BLEU score and accuracy improvements.\nImpressively, with only 1 to 5 examples, our model reports better accuracy\nscores than a reference system trained with on average 313 parallel examples.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:05:43 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Arthaud", "Farid", ""], ["Bawden", "Rachel", ""], ["Birch", "Alexandra", ""]]}, {"id": "2103.16929", "submitter": "Tapas Nayak", "authors": "Tapas Nayak and Navonil Majumder and Pawan Goyal and Soujanya Poria", "title": "Deep Neural Approaches to Relation Triplets Extraction: A Comprehensive\n  Survey", "comments": "A survey paper for relation extraction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the advances made in continuous representation of words (word\nembeddings) and deep neural architectures, many research works are published in\nthe area of relation extraction and it is very difficult to keep track of so\nmany papers. To help future research, we present a comprehensive review of the\nrecently published research works in relation extraction. We mostly focus on\nrelation extraction using deep neural networks which have achieved\nstate-of-the-art performance on publicly available datasets. In this survey, we\ncover sentence-level relation extraction to document-level relation extraction,\npipeline-based approaches to joint extraction approaches, annotated datasets to\ndistantly supervised datasets along with few very recent research directions\nsuch as zero-shot or few-shot relation extraction, noise mitigation in\ndistantly supervised datasets. Regarding neural architectures, we cover\nconvolutional models, recurrent network models, attention network models, and\ngraph convolutional models in this survey.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:27:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Nayak", "Tapas", ""], ["Majumder", "Navonil", ""], ["Goyal", "Pawan", ""], ["Poria", "Soujanya", ""]]}, {"id": "2103.16997", "submitter": "Oleksiy Syvokon", "authors": "Oleksiy Syvokon and Olena Nahorna", "title": "UA-GEC: Grammatical Error Correction and Fluency Corpus for the\n  Ukrainian Language", "comments": "See https://github.com/grammarly/ua-gec for the dataset. Version 2 of\n  the data is in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a corpus professionally annotated for grammatical error correction\n(GEC) and fluency edits in the Ukrainian language. To the best of our\nknowledge, this is the first GEC corpus for the Ukrainian language. We\ncollected texts with errors (20,715 sentences) from a diverse pool of\ncontributors, including both native and non-native speakers. The data cover a\nwide variety of writing domains, from text chats and essays to formal writing.\nProfessional proofreaders corrected and annotated the corpus for errors\nrelating to fluency, grammar, punctuation, and spelling. This corpus can be\nused for developing and evaluating GEC systems in Ukrainian. More generally, it\ncan be used for researching multilingual and low-resource NLP, morphologically\nrich languages, document-level GEC, and fluency correction. The corpus is\npublicly available at https://github.com/grammarly/ua-gec\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:18:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Syvokon", "Oleksiy", ""], ["Nahorna", "Olena", ""]]}, {"id": "2103.17055", "submitter": "Isabelle Augenstein", "authors": "Sheikh Muhammad Sarwar, Dimitrina Zlatkova, Momchil Hardalov, Yoan\n  Dinkov, Isabelle Augenstein, Preslav Nakov", "title": "A Neighbourhood Framework for Resource-Lean Content Flagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel interpretable framework for cross-lingual content\nflagging, which significantly outperforms prior work both in terms of\npredictive performance and average inference time. The framework is based on a\nnearest-neighbour architecture and is interpretable by design. Moreover, it can\neasily adapt to new instances without the need to retrain it from scratch.\nUnlike prior work, (i) we encode not only the texts, but also the labels in the\nneighbourhood space (which yields better accuracy), and (ii) we use a\nbi-encoder instead of a cross-encoder (which saves computation time). Our\nevaluation results on ten different datasets for abusive language detection in\neight languages shows sizable improvements over the state of the art, as well\nas a speed-up at inference time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:22:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Zlatkova", "Dimitrina", ""], ["Hardalov", "Momchil", ""], ["Dinkov", "Yoan", ""], ["Augenstein", "Isabelle", ""], ["Nakov", "Preslav", ""]]}, {"id": "2103.17090", "submitter": "Leonhard Hennig", "authors": "Marc H\\\"ubner, Christoph Alt, Robert Schwarzenberg, Leonhard Hennig", "title": "Defx at SemEval-2020 Task 6: Joint Extraction of Concepts and Relations\n  for Definition Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Definition Extraction systems are a valuable knowledge source for both humans\nand algorithms. In this paper we describe our submissions to the DeftEval\nshared task (SemEval-2020 Task 6), which is evaluated on an English textbook\ncorpus. We provide a detailed explanation of our system for the joint\nextraction of definition concepts and the relations among them. Furthermore we\nprovide an ablation study of our model variations and describe the results of\nan error analysis.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:01:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["H\u00fcbner", "Marc", ""], ["Alt", "Christoph", ""], ["Schwarzenberg", "Robert", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2103.17114", "submitter": "Vaclav Cvrcek", "authors": "V\\'aclav Cvr\\v{c}ek, Masako Ueda Fidler", "title": "No Keyword is an Island: In search of covert associations", "comments": "28 pages, 6 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes how corpus-assisted discourse analysis based on keyword\n(KW) identification and interpretation can benefit from employing Market basket\nanalysis (MBA) after KW extraction. MBA is a data mining technique used\noriginally in marketing that can reveal consistent associations between items\nin a shopping cart, but also between keywords in a corpus of many texts. By\nidentifying recurring associations between KWs we can compensate for the lack\nof wider context which is a major issue impeding the interpretation of isolated\nKWs (esp. when analyzing large data). To showcase the advantages of MBA in\n\"re-contextualizing\" keywords within the discourse, a pilot study on the topic\nof migration was conducted contrasting anti-system and center-right Czech\ninternet media. was conducted. The results show that MBA is useful in\nidentifying the dominant strategy of anti-system news portals: to weave in a\nconfounding ideological undercurrent and connect the concept of migrants to a\nmultitude of other topics (i.e., flooding the discourse).\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:33:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Cvr\u010dek", "V\u00e1clav", ""], ["Fidler", "Masako Ueda", ""]]}, {"id": "2103.17151", "submitter": "Lorenzo Lupo", "authors": "Lorenzo Lupo, Marco Dinarelli, Laurent Besacier", "title": "Divide and Rule: Training Context-Aware Multi-Encoder Translation Models\n  with Little Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-encoder models are a broad family of context-aware Neural Machine\nTranslation (NMT) systems that aim to improve translation quality by encoding\ndocument-level contextual information alongside the current sentence. The\ncontext encoding is undertaken by contextual parameters, trained on\ndocument-level data. In this work, we show that training these parameters takes\nlarge amount of data, since the contextual training signal is sparse. We\npropose an efficient alternative, based on splitting sentence pairs, that\nallows to enrich the training signal of a set of parallel sentences by breaking\nintra-sentential syntactic links, and thus frequently pushing the model to\nsearch the context for disambiguating clues. We evaluate our approach with BLEU\nand contrastive test sets, showing that it allows multi-encoder models to\nachieve comparable performances to a setting where they are trained with\n$\\times10$ document-level data. We also show that our approach is a viable\noption to context-aware NMT for language pairs with zero document-level\nparallel data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:15:32 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Lupo", "Lorenzo", ""], ["Dinarelli", "Marco", ""], ["Besacier", "Laurent", ""]]}, {"id": "2103.17191", "submitter": "Pushkar Mishra", "authors": "Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova", "title": "Modeling Users and Online Communities for Abuse Detection: A Position on\n  Ethics and Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abuse on the Internet is an important societal problem of our time. Millions\nof Internet users face harassment, racism, personal attacks, and other types of\nabuse across various platforms. The psychological effects of abuse on\nindividuals can be profound and lasting. Consequently, over the past few years,\nthere has been a substantial research effort towards automated abusive language\ndetection in the field of NLP. In this position paper, we discuss the role that\nmodeling of users and online communities plays in abuse detection.\nSpecifically, we review and analyze the state of the art methods that leverage\nuser or community information to enhance the understanding and detection of\nabusive language. We then explore the ethical challenges of incorporating user\nand community information, laying out considerations to guide future research.\nFinally, we address the topic of explainability in abusive language detection,\nproposing properties that an explainable method should aim to exhibit. We\ndescribe how user and community information can facilitate the realization of\nthese properties and discuss the effective operationalization of explainability\nin view of the properties.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:20:37 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:35:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mishra", "Pushkar", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2103.17205", "submitter": "David Uthus", "authors": "David Uthus, Maria Voitovich, R.J. Mical", "title": "Augmenting Poetry Composition with Verse by Verse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe Verse by Verse, our experiment in augmenting the creative process\nof writing poetry with an AI. We have created a group of AI poets, styled after\nvarious American classic poets, that are able to offer as suggestions generated\nlines of verse while a user is composing a poem. In this paper, we describe the\nunderlying system to offer these suggestions. This includes a generative model,\nwhich is tasked with generating a large corpus of lines of verse offline and\nwhich are then stored in an index, and a dual-encoder model that is tasked with\nrecommending the next possible set of verses from our index given the previous\nline of verse.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:31:57 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Uthus", "David", ""], ["Voitovich", "Maria", ""], ["Mical", "R. J.", ""]]}, {"id": "2103.17249", "submitter": "Dani Lischinski", "authors": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani\n  Lischinski", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "comments": "18 pages, 24 figures, code and video may be found here:\n  https://github.com/orpatashnik/StyleCLIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by the ability of StyleGAN to generate highly realistic images in a\nvariety of domains, much recent work has focused on understanding how to use\nthe latent spaces of StyleGAN to manipulate generated and real images. However,\ndiscovering semantically meaningful latent manipulations typically involves\npainstaking human examination of the many degrees of freedom, or an annotated\ncollection of images for each desired manipulation. In this work, we explore\nleveraging the power of recently introduced Contrastive Language-Image\nPre-training (CLIP) models in order to develop a text-based interface for\nStyleGAN image manipulation that does not require such manual effort. We first\nintroduce an optimization scheme that utilizes a CLIP-based loss to modify an\ninput latent vector in response to a user-provided text prompt. Next, we\ndescribe a latent mapper that infers a text-guided latent manipulation step for\na given input image, allowing faster and more stable text-based manipulation.\nFinally, we present a method for mapping a text prompts to input-agnostic\ndirections in StyleGAN's style space, enabling interactive text-driven image\nmanipulation. Extensive results and comparisons demonstrate the effectiveness\nof our approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:51:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Patashnik", "Or", ""], ["Wu", "Zongze", ""], ["Shechtman", "Eli", ""], ["Cohen-Or", "Daniel", ""], ["Lischinski", "Dani", ""]]}, {"id": "2103.17250", "submitter": "Vil\\'em Zouhar", "authors": "Vil\\'em Zouhar and Daria Pylypenko", "title": "Leveraging Neural Machine Translation for Word Alignment", "comments": "16 pages (without references). To be published in PBML 116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most common tools for word-alignment rely on a large amount of parallel\nsentences, which are then usually processed according to one of the IBM model\nalgorithms. The training data is, however, the same as for machine translation\n(MT) systems, especially for neural MT (NMT), which itself is able to produce\nword-alignments using the trained attention heads. This is convenient because\nword-alignment is theoretically a viable byproduct of any attention-based NMT,\nwhich is also able to provide decoder scores for a translated sentence pair.\n  We summarize different approaches on how word-alignment can be extracted from\nalignment scores and then explore ways in which scores can be extracted from\nNMT, focusing on inferring the word-alignment scores based on output sentence\nand token probabilities. We compare this to the extraction of alignment scores\nfrom attention. We conclude with aggregating all of the sources of alignment\nscores into a simple feed-forward network which achieves the best results when\ncombined alignment extractors are used.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:51:35 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zouhar", "Vil\u00e9m", ""], ["Pylypenko", "Daria", ""]]}]