[{"id": "0911.0894", "submitter": "Vishal Goyal", "authors": "N. Rama and Meenakshi Lakshmanan", "title": "A New Computational Schema for Euphonic Conjunctions in Sanskrit\n  Processing", "comments": "International Journal of Computer Science Issues, IJCSI Volume 5,\n  pp43-51, October 2009", "journal-ref": "Rama N. and M. Lakshmanan, \"A New Computational Schema for\n  Euphonic Conjunctions in Sanskrit Processing\", International Journal of\n  Computer Science Issues, IJCSI, Volume 5, pp43-51, October 2009", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated language processing is central to the drive to enable facilitated\nreferencing of increasingly available Sanskrit E texts. The first step towards\nprocessing Sanskrit text involves the handling of Sanskrit compound words that\nare an integral part of Sanskrit texts. This firstly necessitates the\nprocessing of euphonic conjunctions or sandhis, which are points in words or\nbetween words, at which adjacent letters coalesce and transform. The ancient\nSanskrit grammarian Panini's codification of the Sanskrit grammar is the\naccepted authority in the subject. His famed sutras or aphorisms, numbering\napproximately four thousand, tersely, precisely and comprehensively codify the\nrules of the grammar, including all the rules pertaining to sandhis. This work\npresents a fresh new approach to processing sandhis in terms of a computational\nschema. This new computational model is based on Panini's complex codification\nof the rules of grammar. The model has simple beginnings and is yet powerful,\ncomprehensive and computationally lean.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2009 19:33:21 GMT"}], "update_date": "2009-11-05", "authors_parsed": [["Rama", "N.", ""], ["Lakshmanan", "Meenakshi", ""]]}, {"id": "0911.0907", "submitter": "Vishal Goyal", "authors": "Kaustubh Bhattacharyya and Kandarpa Kumar Sarma", "title": "ANN-based Innovative Segmentation Method for Handwritten text in\n  Assamese", "comments": "International Journal of Computer Science Issues, Volume 5, pp9-16,\n  October 2009", "journal-ref": "K. Bhattacharyya and K. K. Sarma, \"ANN-based Innovative\n  Segmentation Method for Handwritten text in Assamese\", International Journal\n  of Computer Science Issues, Volume 5, pp9-16, October 2009", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Network (ANN) s has widely been used for recognition of\noptically scanned character, which partially emulates human thinking in the\ndomain of the Artificial Intelligence. But prior to recognition, it is\nnecessary to segment the character from the text to sentences, words etc.\nSegmentation of words into individual letters has been one of the major\nproblems in handwriting recognition. Despite several successful works all over\nthe work, development of such tools in specific languages is still an ongoing\nprocess especially in the Indian context. This work explores the application of\nANN as an aid to segmentation of handwritten characters in Assamese- an\nimportant language in the North Eastern part of India. The work explores the\nperformance difference obtained in applying an ANN-based dynamic segmentation\nalgorithm compared to projection- based static segmentation. The algorithm\ninvolves, first training of an ANN with individual handwritten characters\nrecorded from different individuals. Handwritten sentences are separated out\nfrom text using a static segmentation method. From the segmented line,\nindividual characters are separated out by first over segmenting the entire\nline. Each of the segments thus obtained, next, is fed to the trained ANN. The\npoint of segmentation at which the ANN recognizes a segment or a combination of\nseveral segments to be similar to a handwritten character, a segmentation\nboundary for the character is assumed to exist and segmentation performed. The\nsegmented character is next compared to the best available match and the\nsegmentation boundary confirmed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2009 18:32:08 GMT"}], "update_date": "2009-11-05", "authors_parsed": [["Bhattacharyya", "Kaustubh", ""], ["Sarma", "Kandarpa Kumar", ""]]}, {"id": "0911.1451", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff, Ping Zhou", "title": "Co-word Analysis using the Chinese Character Set", "comments": null, "journal-ref": "Co-Word Analysis using the Chinese Character Set, Journal of the\n  American Society for Information Science and Technology 59(9), 1528-1530,\n  2008", "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, Chinese texts could not be studied using co-word analysis\nbecause the words are not separated by spaces in Chinese (and Japanese). A word\ncan be composed of one or more characters. The online availability of programs\nthat separate Chinese texts makes it possible to analyze them using semantic\nmaps. Chinese characters contain not only information, but also meaning. This\nmay enhance the readability of semantic maps. In this study, we analyze 58\nwords which occur ten or more times in the 1652 journal titles of the China\nScientific and Technical Papers and Citations Database. The word occurrence\nmatrix is visualized and factor-analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2009 19:15:33 GMT"}], "update_date": "2009-11-10", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Zhou", "Ping", ""]]}, {"id": "0911.1516", "submitter": "Sana Ullah", "authors": "Sana Ullah, M.A. Khan, Kyung Sup Kwak", "title": "A Discourse-based Approach in Text-based Machine Translation", "comments": "1 figure, 3 tables, ICCIT 07, Vol. 3, pp 1073-1074, Busan, June 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a theoretical research based approach to ellipsis\nresolution in machine translation. The formula of discourse is applied in order\nto resolve ellipses. The validity of the discourse formula is analyzed by\napplying it to the real world text, i.e., newspaper fragments. The source text\nis converted into mono-sentential discourses where complex discourses require\nfurther dissection either directly into primitive discourses or first into\ncompound discourses and later into primitive ones. The procedure of dissection\nneeds further improvement, i.e., discovering as many primitive discourse forms\nas possible. An attempt has been made to investigate new primitive discourses\nor patterns from the given text.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2009 10:57:04 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2010 08:49:34 GMT"}], "update_date": "2010-07-29", "authors_parsed": [["Ullah", "Sana", ""], ["Khan", "M. A.", ""], ["Kwak", "Kyung Sup", ""]]}, {"id": "0911.1517", "submitter": "Sana Ullah", "authors": "Sana Ullah, M.Asdaque Hussain, and Kyung Sup Kwak", "title": "Resolution of Unidentified Words in Machine Translation", "comments": "4 pages, 2 figures, 2 tables, The 4th annual International New\n  Exploratory Technologies Conference 2007 (NEXT 2007), Seoul, South Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a mechanism of resolving unidentified lexical units in\nText-based Machine Translation (TBMT). In a Machine Translation (MT) system it\nis unlikely to have a complete lexicon and hence there is intense need of a new\nmechanism to handle the problem of unidentified words. These unknown words\ncould be abbreviations, names, acronyms and newly introduced terms. We have\nproposed an algorithm for the resolution of the unidentified words. This\nalgorithm takes discourse unit (primitive discourse) as a unit of analysis and\nprovides real time updates to the lexicon. We have manually applied the\nalgorithm to news paper fragments. Along with anaphora and cataphora\nresolution, many unknown words especially names and abbreviations were updated\nto the lexicon.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2009 13:52:05 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2010 10:45:45 GMT"}], "update_date": "2010-07-29", "authors_parsed": [["Ullah", "Sana", ""], ["Hussain", "M. Asdaque", ""], ["Kwak", "Kyung Sup", ""]]}, {"id": "0911.1842", "submitter": "Laurent Romary", "authors": "Nancy Ide (INRIA Lorraine - LORIA), Laurent Romary (INRIA Lorraine -\n  LORIA)", "title": "Standards for Language Resources", "comments": "Colloque avec actes et comit\\'e de lecture. internationale", "journal-ref": "IRCS Workshop on Linguistic Databases, Philadelphia : United\n  States (2001)", "doi": null, "report-no": "A01-R-287 || ide01b", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is two-fold: to present an abstract data model for\nlinguistic annotations and its implementation using XML, RDF and related\nstandards; and to outline the work of a newly formed committee of the\nInternational Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource\nManagement, which will use this work as its starting point.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2009 07:12:03 GMT"}], "update_date": "2009-11-11", "authors_parsed": [["Ide", "Nancy", "", "INRIA Lorraine - LORIA"], ["Romary", "Laurent", "", "INRIA Lorraine -\n  LORIA"]]}, {"id": "0911.1965", "submitter": "Nitin Madnani", "authors": "Nitin Madnani, Hongyan Jing, Nanda Kambhatla and Salim Roukos", "title": "Active Learning for Mention Detection: A Comparison of Sentence\n  Selection Strategies", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and compare various sentence selection strategies for active\nlearning for the task of detecting mentions of entities. The best strategy\nemploys the sum of confidences of two statistical classifiers trained on\ndifferent views of the data. Our experimental results show that, compared to\nthe random selection strategy, this strategy reduces the amount of required\nlabeled training data by over 50% while achieving the same performance. The\neffect is even more significant when only named mentions are considered: the\nsystem achieves the same performance by using only 42% of the training data\nrequired by the random selection strategy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2009 19:52:01 GMT"}], "update_date": "2009-11-11", "authors_parsed": [["Madnani", "Nitin", ""], ["Jing", "Hongyan", ""], ["Kambhatla", "Nanda", ""], ["Roukos", "Salim", ""]]}, {"id": "0911.2284", "submitter": "Fabio G. Guerrero-Moreno", "authors": "Fabio G. Guerrero", "title": "A New Look at the Classical Entropy of Written English", "comments": "Submitted to the IEEE Transactions on Information Theory. Reference\n  on page 5 corrected. Weighted average of HL on page 12 corrected (average\n  redundancy R has therefore been updated). Abstract slightly changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple method for finding the entropy and redundancy of a reasonable long\nsample of English text by direct computer processing and from first principles\naccording to Shannon theory is presented. As an example, results on the entropy\nof the English language have been obtained based on a total of 20.3 million\ncharacters of written English, considering symbols from one to five hundred\ncharacters in length. Besides a more realistic value of the entropy of English,\na new perspective on some classic entropy-related concepts is presented. This\nmethod can also be extended to other Latin languages. Some implications for\npractical applications such as plagiarism-detection software, and the minimum\nnumber of words that should be used in social Internet network messaging, are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2009 01:48:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2009 03:15:30 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Guerrero", "Fabio G.", ""]]}, {"id": "0911.3280", "submitter": "Maurizio Serva", "authors": "Maurizio Serva", "title": "Automated languages phylogeny from Levenshtein distance", "comments": "Ideas and results in this paper were presented at the International\n  Conference: Visitas Internationais, Instituto de Estudos Avancados\n  Transdisciplinares, (Belo Horizonte, 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages evolve over time in a process in which reproduction, mutation and\nextinction are all possible, similar to what happens to living organisms. Using\nthis similarity it is possible, in principle, to build family trees which show\nthe degree of relatedness between languages.\n  The method used by modern glottochronology, developed by Swadesh in the\n1950s, measures distances from the percentage of words with a common historical\norigin. The weak point of this method is that subjective judgment plays a\nrelevant role.\n  Recently we proposed an automated method that avoids the subjectivity, whose\nresults can be replicated by studies that use the same database and that\ndoesn't require a specific linguistic knowledge. Moreover, the method allows a\nquick comparison of a large number of languages.\n  We applied our method to the Indo-European and Austronesian families,\nconsidering in both cases, fifty different languages. The resulting trees are\nsimilar to those of previous studies, but with some important differences in\nthe position of few languages and subgroups. We believe that these differences\ncarry new information on the structure of the tree and on the phylogenetic\nrelationships within families.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 12:17:22 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2011 22:14:52 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2011 18:48:20 GMT"}, {"version": "v4", "created": "Fri, 29 Apr 2011 20:24:40 GMT"}, {"version": "v5", "created": "Tue, 3 May 2011 21:19:29 GMT"}, {"version": "v6", "created": "Fri, 6 May 2011 20:41:08 GMT"}, {"version": "v7", "created": "Mon, 2 Jul 2012 09:54:50 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Serva", "Maurizio", ""]]}, {"id": "0911.3292", "submitter": "Maurizio Serva", "authors": "Filippo Petroni and Maurizio Serva", "title": "Automated words stability and languages phylogeny", "comments": "XI International Conference \"Cognitive Modeling in Linguistics-2009\"\n  Constanca, Romania, September, 7-14, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of measuring distance between languages seems to have its roots in\nthe work of the French explorer Dumont D'Urville (D'Urville 1832). He collected\ncomparative words lists of various languages during his voyages aboard the\nAstrolabe from 1826 to1829 and, in his work about the geographical division of\nthe Pacific, he proposed a method to measure the degree of relation among\nlanguages. The method used by modern glottochronology, developed by Morris\nSwadesh in the 1950s (Swadesh 1952), measures distances from the percentage of\nshared cognates, which are words with a common historical origin. Recently, we\nproposed a new automated method which uses normalized Levenshtein distance\namong words with the same meaning and averages on the words contained in a\nlist. Another classical problem in glottochronology is the study of the\nstability of words corresponding to different meanings. Words, in fact, evolve\nbecause of lexical changes, borrowings and replacement at a rate which is not\nthe same for all of them. The speed of lexical evolution is different for\ndifferent meanings and it is probably related to the frequency of use of the\nassociated words (Pagel et al. 2007). This problem is tackled here by an\nautomated methodology only based on normalized Levenshtein distance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 12:47:44 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2009 17:08:06 GMT"}], "update_date": "2009-12-07", "authors_parsed": [["Petroni", "Filippo", ""], ["Serva", "Maurizio", ""]]}, {"id": "0911.3411", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff and Iina Hellsten", "title": "Measuring the Meaning of Words in Contexts: An automated analysis of\n  controversies about Monarch butterflies, Frankenfoods, and stem cells", "comments": null, "journal-ref": "Scientometrics 67(2), 2006, 231-258", "doi": null, "report-no": null, "categories": "cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-words have been considered as carriers of meaning across different domains\nin studies of science, technology, and society. Words and co-words, however,\nobtain meaning in sentences, and sentences obtain meaning in their contexts of\nuse. At the science/society interface, words can be expected to have different\nmeanings: the codes of communication that provide meaning to words differ on\nthe varying sides of the interface. Furthermore, meanings and interfaces may\nchange over time. Given this structuring of meaning across interfaces and over\ntime, we distinguish between metaphors and diaphors as reflexive mechanisms\nthat facilitate the translation between contexts. Our empirical focus is on\nthree recent scientific controversies: Monarch butterflies, Frankenfoods, and\nstem-cell therapies. This study explores new avenues that relate the study of\nco-word analysis in context with the sociological quest for the analysis and\nprocessing of meaning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 22:03:56 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Hellsten", "Iina", ""]]}, {"id": "0911.3944", "submitter": "Patrick J. Wolfe", "authors": "Christopher M. White, Sanjeev P. Khudanpur, and Patrick J. Wolfe", "title": "Likelihood-based semi-supervised model selection with applications to\n  speech processing", "comments": "11 pages, 2 figures; submitted for publication", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.\n  1016-1026, 2010", "doi": "10.1109/JSTSP.2010.2076050", "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional supervised pattern recognition tasks, model selection is\ntypically accomplished by minimizing the classification error rate on a set of\nso-called development data, subject to ground-truth labeling by human experts\nor some other means. In the context of speech processing systems and other\nlarge-scale practical applications, however, such labeled development data are\ntypically costly and difficult to obtain. This article proposes an alternative\nsemi-supervised framework for likelihood-based model selection that leverages\nunlabeled data by using trained classifiers representing each model to\nautomatically generate putative labels. The errors that result from this\nautomatic labeling are shown to be amenable to results from robust statistics,\nwhich in turn provide for minimax-optimal censored likelihood ratio tests that\nrecover the nonparametric sign test as a limiting case. This approach is then\nvalidated experimentally using a state-of-the-art automatic speech recognition\nsystem to select between candidate word pronunciations using unlabeled speech\ndata that only potentially contain instances of the words under test. Results\nprovide supporting evidence for the utility of this approach, and suggest that\nit may also find use in other applications of machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 01:30:36 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["White", "Christopher M.", ""], ["Khudanpur", "Sanjeev P.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0911.5116", "submitter": "Laurent Romary", "authors": "Laurent Romary (INRIA Saclay - Ile de France, IDSL)", "title": "Standardization of the formal representation of lexical information for\n  NLP", "comments": null, "journal-ref": "Dictionarie. An International Encyclopedia of Lexicography.\n  Supplementary volume: Recent developments with special focus on computational\n  lexicography (2010) -", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey of dictionary models and formats is presented as well as a\npresentation of corresponding recent standardisation activities.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2009 16:27:58 GMT"}], "update_date": "2009-11-30", "authors_parsed": [["Romary", "Laurent", "", "INRIA Saclay - Ile de France, IDSL"]]}, {"id": "0911.5568", "submitter": "Cedric Messiant", "authors": "C\\'edric Messiant (LIPN), Thierry Poibeau (LIPN)", "title": "Acquisition d'informations lexicales \\`a partir de corpus C\\'edric\n  Messiant et Thierry Poibeau", "comments": "3 pages", "journal-ref": "Troisi\\`eme colloque international de l'Association Fran\\c{c}aise\n  de Linguistique Cognitive (AFLICO), Nanterre : France (2009)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about automatic acquisition of lexical information from\ncorpora, especially subcategorization acquisition.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2009 08:07:16 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Messiant", "C\u00e9dric", "", "LIPN"], ["Poibeau", "Thierry", "", "LIPN"]]}, {"id": "0911.5703", "submitter": "Stevan Harnad", "authors": "Olivier Picard, Alexandre Blondin-Masse, Stevan Harnad, Odile\n  Marcotte, Guillaume Chicoisne and Yassine Gargouri", "title": "Hierarchies in Dictionary Definition Space", "comments": "9 pages, 5 figues, 2 tables, 12 references, 23rd Annual Conference on\n  Neural Information Processing Systems (NIPS): Workshop on Analyzing Networks\n  and Learning With Graphs\n  http://nips.cc/Conferences/2009/Program/event.php?ID=1504", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dictionary defines words in terms of other words. Definitions can tell you\nthe meanings of words you don't know, but only if you know the meanings of the\ndefining words. How many words do you need to know (and which ones) in order to\nbe able to learn all the rest from definitions? We reduced dictionaries to\ntheir \"grounding kernels\" (GKs), about 10% of the dictionary, from which all\nthe other words could be defined. The GK words turned out to have\npsycholinguistic correlates: they were learned at an earlier age and more\nconcrete than the rest of the dictionary. But one can compress still more: the\nGK turns out to have internal structure, with a strongly connected \"kernel\ncore\" (KC) and a surrounding layer, from which a hierarchy of definitional\ndistances can be derived, all the way out to the periphery of the full\ndictionary. These definitional distances, too, are correlated with\npsycholinguistic variables (age of acquisition, concreteness, imageability,\noral and written frequency) and hence perhaps with the \"mental lexicon\" in each\nof our heads.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2009 18:15:35 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Picard", "Olivier", ""], ["Blondin-Masse", "Alexandre", ""], ["Harnad", "Stevan", ""], ["Marcotte", "Odile", ""], ["Chicoisne", "Guillaume", ""], ["Gargouri", "Yassine", ""]]}]