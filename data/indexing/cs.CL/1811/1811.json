[{"id": "1811.00051", "submitter": "Gerasimos Lampouras", "authors": "Gerasimos Lampouras and Ion Androutsopoulos", "title": "Generating Texts with Integer Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-to-text generation typically employs a pipeline architecture, which\noften leads to suboptimal texts. Content selection, for example, may greedily\nselect the most important facts, which may require, however, too many words to\nexpress, and this may be undesirable when space is limited or expensive.\nSelecting other facts, possibly only slightly less important, may allow the\nlexicalization stage to use much fewer words, or to report more facts in the\nsame space. Decisions made during content selection and lexicalization may also\nlead to more or fewer sentence aggregation opportunities, affecting the length\nand readability of the resulting texts. Building upon on a publicly available\nstate of the art natural language generator for Semantic Web ontologies, this\narticle presents an Integer Linear Programming model that, unlike pipeline\narchitectures, jointly considers choices available in content selection,\nlexicalization, and sentence aggregation to avoid greedy local decisions and\nproduce more compact texts, i.e., texts that report more facts per word.\nCompact texts are desirable, for example, when generating advertisements to be\nincluded in Web search results, or when summarizing structured information in\nlimited space. An extended version of the proposed model also considers a\nlimited form of referring expression generation and avoids redundant sentences.\nAn approximation of the two models can be used when longer texts need to be\ngenerated. Experiments with three ontologies confirm that the proposed models\nlead to more compact texts, compared to pipeline systems, with no deterioration\nor with improvements in the perceived quality of the generated texts.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:24:32 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Lampouras", "Gerasimos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1811.00062", "submitter": "Niek Tax", "authors": "Niek Tax, Irene Teinemaa, Sebastiaan J. van Zelst", "title": "An Interdisciplinary Comparison of Sequence Modeling Methods for\n  Next-Element Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of sequential nature arise in many application domains in forms of, e.g.\ntextual data, DNA sequences, and software execution traces. Different research\ndisciplines have developed methods to learn sequence models from such datasets:\n(i) in the machine learning field methods such as (hidden) Markov models and\nrecurrent neural networks have been developed and successfully applied to a\nwide-range of tasks, (ii) in process mining process discovery techniques aim to\ngenerate human-interpretable descriptive models, and (iii) in the grammar\ninference field the focus is on finding descriptive models in the form of\nformal grammars. Despite their different focuses, these fields share a common\ngoal - learning a model that accurately describes the behavior in the\nunderlying data. Those sequence models are generative, i.e, they can predict\nwhat elements are likely to occur after a given unfinished sequence. So far,\nthese fields have developed mainly in isolation from each other and no\ncomparison exists. This paper presents an interdisciplinary experimental\nevaluation that compares sequence modeling techniques on the task of\nnext-element prediction on four real-life sequence datasets. The results\nindicate that machine learning techniques that generally have no aim at\ninterpretability in terms of accuracy outperform techniques from the process\nmining and grammar inference fields that aim to yield interpretable models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:54:27 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tax", "Niek", ""], ["Teinemaa", "Irene", ""], ["van Zelst", "Sebastiaan J.", ""]]}, {"id": "1811.00066", "submitter": "Nina Poerner", "authors": "Nina Poerner, Masoud Jalili Sabet, Benjamin Roth and Hinrich Sch\\\"utze", "title": "Aligning Very Small Parallel Corpora Using Cross-Lingual Word Embeddings\n  and a Monogamy Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count-based word alignment methods, such as the IBM models or fast-align,\nstruggle on very small parallel corpora. We therefore present an alternative\napproach based on cross-lingual word embeddings (CLWEs), which are trained on\npurely monolingual data. Our main contribution is an unsupervised objective to\nadapt CLWEs to parallel corpora. In experiments on between 25 and 500\nsentences, our method outperforms fast-align. We also show that our fine-tuning\nobjective consistently improves a CLWE-only baseline.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:58:22 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Poerner", "Nina", ""], ["Sabet", "Masoud Jalili", ""], ["Roth", "Benjamin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1811.00070", "submitter": "Yifeng Tao", "authors": "Yifeng Tao, Bruno Godefroy, Guillaume Genthial, and Christopher Potts", "title": "Effective Feature Representation for Clinical Text Concept Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crucial information about the practice of healthcare is recorded only in\nfree-form text, which creates an enormous opportunity for high-impact NLP.\nHowever, annotated healthcare datasets tend to be small and expensive to\nobtain, which raises the question of how to make maximally efficient uses of\nthe available data. To this end, we develop an LSTM-CRF model for combining\nunsupervised word representations and hand-built feature representations\nderived from publicly available healthcare ontologies. We show that this\ncombined model yields superior performance on five datasets of diverse kinds of\nhealthcare text (clinical, social, scientific, commercial). Each involves the\nlabeling of complex, multi-word spans that pick out different healthcare\nconcepts. We also introduce a new labeled dataset for identifying the treatment\nrelations between drugs and diseases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 19:06:50 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 20:16:49 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tao", "Yifeng", ""], ["Godefroy", "Bruno", ""], ["Genthial", "Guillaume", ""], ["Potts", "Christopher", ""]]}, {"id": "1811.00119", "submitter": "Su Wang", "authors": "Su Wang, Rahul Gupta, Nancy Chang, Jason Baldridge", "title": "A task in a suit and a tie: paraphrase generation with semantic\n  augmentation", "comments": null, "journal-ref": "Association for the Advancement of Artificial Intelligence (AAAI)\n  2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paraphrasing is rooted in semantics. We show the effectiveness of\ntransformers (Vaswani et al. 2017) for paraphrase generation and further\nimprovements by incorporating PropBank labels via a multi-encoder. Evaluating\non MSCOCO and WikiAnswers, we find that transformers are fast and effective,\nand that semantic augmentation for both transformers and LSTMs leads to sizable\n2-3 point gains in BLEU, METEOR and TER. More importantly, we find surprisingly\nlarge gains on human evaluations compared to previous models. Nevertheless,\nmanual inspection of generated paraphrases reveals ample room for improvement:\neven our best model produces human-acceptable paraphrases for only 28% of\ncaptions from the CHIA dataset (Sharma et al. 2018), and it fails spectacularly\non sentences from Wikipedia. Overall, these results point to the potential for\nincorporating semantics in the task while highlighting the need for stronger\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 21:04:04 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:24:51 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Wang", "Su", ""], ["Gupta", "Rahul", ""], ["Chang", "Nancy", ""], ["Baldridge", "Jason", ""]]}, {"id": "1811.00127", "submitter": "Amaru Cuba Gyllensten", "authors": "Amaru Cuba Gyllensten and Magnus Sahlgren", "title": "Measuring Issue Ownership using Word Embeddings", "comments": "Accepted to the 9th Workshop on Computational Approaches to\n  Subjectivity, Sentiment & Social Media Analysis (WASSA), held in conjunction\n  with the EMNLP 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment and topic analysis are common methods used for social media\nmonitoring. Essentially, these methods answers questions such as, \"what is\nbeing talked about, regarding X\", and \"what do people feel, regarding X\". In\nthis paper, we investigate another venue for social media monitoring, namely\nissue ownership and agenda setting, which are concepts from political science\nthat have been used to explain voter choice and electoral outcomes. We argue\nthat issue alignment and agenda setting can be seen as a kind of semantic\nsource similarity of the kind \"how similar is source A to issue owner P, when\ntalking about issue X\", and as such can be measured using word/document\nembedding techniques. We present work in progress towards measuring that kind\nof conditioned similarity, and introduce a new notion of similarity for\npredictive embeddings. We then test this method by measuring the similarity\nbetween politically aligned media and political parties, conditioned on\nbloc-specific issues.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 21:31:08 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Gyllensten", "Amaru Cuba", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "1811.00135", "submitter": "Yijun Xiao", "authors": "Yijun Xiao, Tiancheng Zhao, William Yang Wang", "title": "Dirichlet Variational Autoencoder for Text Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an improved variational autoencoder (VAE) for text modeling with\ntopic information explicitly modeled as a Dirichlet latent variable. By\nproviding the proposed model topic awareness, it is more superior at\nreconstructing input texts. Furthermore, due to the inherent interactions\nbetween the newly introduced Dirichlet variable and the conventional\nmultivariate Gaussian variable, the model is less prone to KL divergence\nvanishing. We derive the variational lower bound for the new model and conduct\nexperiments on four different data sets. The results show that the proposed\nmodel is superior at text reconstruction across the latent space and\nclassifications on learned representations have higher test accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:04:22 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Xiao", "Yijun", ""], ["Zhao", "Tiancheng", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00146", "submitter": "Maarten Sap", "authors": "Maarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula,\n  Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, Yejin Choi", "title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning", "comments": "AAAI 2019 CR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ATOMIC, an atlas of everyday commonsense reasoning, organized\nthrough 877k textual descriptions of inferential knowledge. Compared to\nexisting resources that center around taxonomic knowledge, ATOMIC focuses on\ninferential knowledge organized as typed if-then relations with variables\n(e.g., \"if X pays Y a compliment, then Y will likely return the compliment\").\nWe propose nine if-then relation types to distinguish causes vs. effects,\nagents vs. themes, voluntary vs. involuntary events, and actions vs. mental\nstates. By generatively training on the rich inferential knowledge described in\nATOMIC, we show that neural models can acquire simple commonsense capabilities\nand reason about previously unseen events. Experimental results demonstrate\nthat multitask models that incorporate the hierarchical structure of if-then\nrelation types lead to more accurate inference compared to models trained in\nisolation, as measured by both automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:57:51 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 20:12:03 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 19:52:21 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Sap", "Maarten", ""], ["LeBras", "Ronan", ""], ["Allaway", "Emily", ""], ["Bhagavatula", "Chandra", ""], ["Lourie", "Nicholas", ""], ["Rashkin", "Hannah", ""], ["Roof", "Brendan", ""], ["Smith", "Noah A.", ""], ["Choi", "Yejin", ""]]}, {"id": "1811.00147", "submitter": "Haoyu Wang", "authors": "Haoyu Wang, Vivek Kulkarni, William Yang Wang", "title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings", "comments": "10 pages, 6 figures", "journal-ref": "Automated Knowledge Base Construction (AKBC), 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%).\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:59:57 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Haoyu", ""], ["Kulkarni", "Vivek", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00185", "submitter": "Haojie Pan", "authors": "Haojie Pan, Junpei Zhou, Zhou Zhao, Yan Liu, Deng Cai, Min Yang", "title": "Dial2Desc: End-to-end Dialogue Description Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first propose a new task named Dialogue Description (Dial2Desc). Unlike\nother existing dialogue summarization tasks such as meeting summarization, we\ndo not maintain the natural flow of a conversation but describe an object or an\naction of what people are talking about. The Dial2Desc system takes a dialogue\ntext as input, then outputs a concise description of the object or the action\ninvolved in this conversation. After reading this short description, one can\nquickly extract the main topic of a conversation and build a clear picture in\nhis mind, without reading or listening to the whole conversation. Based on the\nexisting dialogue dataset, we build a new dataset, which has more than one\nhundred thousand dialogue-description pairs. As a step forward, we demonstrate\nthat one can get more accurate and descriptive results using a new neural\nattentive model that exploits the interaction between utterances from different\nspeakers, compared with other baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:10:50 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Pan", "Haojie", ""], ["Zhou", "Junpei", ""], ["Zhao", "Zhou", ""], ["Liu", "Yan", ""], ["Cai", "Deng", ""], ["Yang", "Min", ""]]}, {"id": "1811.00196", "submitter": "Qingyu Yin", "authors": "Hui Liu, Qingyu Yin, William Yang Wang", "title": "Towards Explainable NLP: A Generative Explanation Framework for Text\n  Classification", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building explainable systems is a critical problem in the field of Natural\nLanguage Processing (NLP), since most machine learning models provide no\nexplanations for the predictions. Existing approaches for explainable machine\nlearning systems tend to focus on interpreting the outputs or the connections\nbetween inputs and outputs. However, the fine-grained information is often\nignored, and the systems do not explicitly generate the human-readable\nexplanations. To better alleviate this problem, we propose a novel generative\nexplanation framework that learns to make classification decisions and generate\nfine-grained explanations at the same time. More specifically, we introduce the\nexplainable factor and the minimum risk training approach that learn to\ngenerate more reasonable explanations. We construct two new datasets that\ncontain summaries, rating scores, and fine-grained reasons. We conduct\nexperiments on both datasets, comparing with several strong neural network\nbaseline systems. Experimental results show that our method surpasses all\nbaselines on both datasets, and is able to generate concise explanations at the\nsame time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:45:57 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 13:12:58 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Liu", "Hui", ""], ["Yin", "Qingyu", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00198", "submitter": "Yu Hao", "authors": "Hao Yu, Vivek Kulkarni, William Wang", "title": "MOHONE: Modeling Higher Order Network Effects in KnowledgeGraphs via\n  Network Infused Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many knowledge graph embedding methods operate on triples and are therefore\nimplicitly limited by a very local view of the entire knowledge graph. We\npresent a new framework MOHONE to effectively model higher order network\neffects in knowledge-graphs, thus enabling one to capture varying degrees of\nnetwork connectivity (from the local to the global). Our framework is generic,\nexplicitly models the network scale, and captures two different aspects of\nsimilarity in networks: (a) shared local neighborhood and (b) structural\nrole-based similarity. First, we introduce methods that learn network\nrepresentations of entities in the knowledge graph capturing these varied\naspects of similarity. We then propose a fast, efficient method to incorporate\nthe information captured by these network representations into existing\nknowledge graph embeddings. We show that our method consistently and\nsignificantly improves the performance on link prediction of several different\nknowledge-graph embedding methods including TRANSE, TRANSD, DISTMULT, and\nCOMPLEX(by at least 4 points or 17% in some cases).\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:04:09 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yu", "Hao", ""], ["Kulkarni", "Vivek", ""], ["Wang", "William", ""]]}, {"id": "1811.00207", "submitter": "Eric Smith", "authors": "Hannah Rashkin, Eric Michael Smith, Margaret Li and Y-Lan Boureau", "title": "Towards Empathetic Open-domain Conversation Models: a New Benchmark and\n  Dataset", "comments": "accepted to ACL 2019 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge for dialogue agents is recognizing feelings in the conversation\npartner and replying accordingly, a key communicative skill. While it is\nstraightforward for humans to recognize and acknowledge others' feelings in a\nconversation, this is a significant challenge for AI systems due to the paucity\nof suitable publicly-available datasets for training and evaluation. This work\nproposes a new benchmark for empathetic dialogue generation and\nEmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional\nsituations. Our experiments indicate that dialogue models that use our dataset\nare perceived to be more empathetic by human evaluators, compared to models\nmerely trained on large-scale Internet conversation data. We also present\nempirical comparisons of dialogue model adaptations for empathetic responding,\nleveraging existing models or datasets without requiring lengthy re-training of\nthe full model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:43:10 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 18:33:17 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 04:09:22 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 18:24:13 GMT"}, {"version": "v5", "created": "Wed, 28 Aug 2019 21:35:38 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Rashkin", "Hannah", ""], ["Smith", "Eric Michael", ""], ["Li", "Margaret", ""], ["Boureau", "Y-Lan", ""]]}, {"id": "1811.00225", "submitter": "Naomi Saphra", "authors": "Naomi Saphra and Adam Lopez", "title": "Understanding Learning Dynamics Of Language Models with SVCCA", "comments": "Accepted for publication in NAACL 2019", "journal-ref": null, "doi": "10.18653/v1/N19-1329", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research has shown that neural models implicitly encode linguistic features,\nbut there has been no research showing \\emph{how} these encodings arise as the\nmodels are trained. We present the first study on the learning dynamics of\nneural language models, using a simple and flexible analysis method called\nSingular Vector Canonical Correlation Analysis (SVCCA), which enables us to\ncompare learned representations across time and across models, without the need\nto evaluate directly on annotated data. We probe the evolution of syntactic,\nsemantic, and topic representations and find that part-of-speech is learned\nearlier than topic; that recurrent layers become more similar to those of a\ntagger during training; and embedding layers less similar. Our results and\nmethods could inform better learning algorithms for NLP models, possibly to\nincorporate linguistic information more effectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 04:51:20 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 10:15:06 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 14:45:07 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Saphra", "Naomi", ""], ["Lopez", "Adam", ""]]}, {"id": "1811.00228", "submitter": "Daouda Sow", "authors": "Daouda Sow and Zengchang Qin and Mouhamed Niasse and Tao Wan", "title": "A sequential guiding network with attention for image captioning", "comments": "5 pages, 2 figures, 1 table, IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances of deep learning in both computer vision (CV) and natural\nlanguage processing (NLP) provide us a new way of understanding semantics, by\nwhich we can deal with more challenging tasks such as automatic description\ngeneration from natural images. In this challenge, the encoder-decoder\nframework has achieved promising performance when a convolutional neural\nnetwork (CNN) is used as image encoder and a recurrent neural network (RNN) as\ndecoder. In this paper, we introduce a sequential guiding network that guides\nthe decoder during word generation. The new model is an extension of the\nencoder-decoder framework with attention that has an additional guiding long\nshort-term memory (LSTM) and can be trained in an end-to-end manner by using\nimage/descriptions pairs. We validate our approach by conducting extensive\nexperiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model\nachieves significant improvement comparing to the other state-of-the-art deep\nlearning models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:03:26 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 07:06:03 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 22:35:58 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Sow", "Daouda", ""], ["Qin", "Zengchang", ""], ["Niasse", "Mouhamed", ""], ["Wan", "Tao", ""]]}, {"id": "1811.00232", "submitter": "Daesik Kim", "authors": "Daesik Kim, Seonhoon Kim and Nojun Kwak", "title": "Textbook Question Answering with Multi-modal Context Graph Understanding\n  and Self-supervised Open-set Comprehension", "comments": "ACL2019 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel algorithm for solving the textbook\nquestion answering (TQA) task which describes more realistic QA problems\ncompared to other recent tasks. We mainly focus on two related issues with\nanalysis of the TQA dataset. First, solving the TQA problems requires to\ncomprehend multi-modal contexts in complicated input data. To tackle this issue\nof extracting knowledge features from long text lessons and merging them with\nvisual features, we establish a context graph from texts and images, and\npropose a new module f-GCN based on graph convolutional networks (GCN). Second,\nscientific terms are not spread over the chapters and subjects are split in the\nTQA dataset. To overcome this so called \"out-of-domain\" issue, before learning\nQA problems, we introduce a novel self-supervised open-set learning process\nwithout any annotations. The experimental results show that our model\nsignificantly outperforms prior state-of-the-art methods. Moreover, ablation\nstudies validate that both methods of incorporating f-GCN for extracting\nknowledge from multi-modal contexts and our newly proposed self-supervised\nlearning process are effective for TQA problems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:10:44 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 10:38:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kim", "Daesik", ""], ["Kim", "Seonhoon", ""], ["Kwak", "Nojun", ""]]}, {"id": "1811.00238", "submitter": "Hao Li", "authors": "Hao Li, Yang Wang, Xinyu Liu, Zhichao Sheng, Si Wei", "title": "Spelling Error Correction Using a Nested RNN Model and Pseudo Training\n  Data", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nested recurrent neural network (nested RNN) model for English\nspelling error correction and generate pseudo data based on phonetic similarity\nto train it. The model fuses orthographic information and context as a whole\nand is trained in an end-to-end fashion. This avoids feature engineering and\ndoes not rely on a noisy channel model as in traditional methods. Experiments\nshow that the proposed method is superior to existing systems in correcting\nspelling errors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:21:47 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Li", "Hao", ""], ["Wang", "Yang", ""], ["Liu", "Xinyu", ""], ["Sheng", "Zhichao", ""], ["Wei", "Si", ""]]}, {"id": "1811.00239", "submitter": "Lili Mou", "authors": "Nabiha Asghar, Lili Mou, Kira A. Selby, Kevin D. Pantasdo, Pascal\n  Poupart, Xin Jiang", "title": "Progressive Memory Banks for Incremental Domain Adaptation", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of incremental domain adaptation (IDA) in\nnatural language processing (NLP). We assume each domain comes one after\nanother, and that we could only access data in the current domain. The goal of\nIDA is to build a unified model performing well on all the domains that we have\nencountered. We adopt the recurrent neural network (RNN) widely used in NLP,\nbut augment it with a directly parameterized memory bank, which is retrieved by\nan attention mechanism at each step of RNN transition. The memory bank provides\na natural way of IDA: when adapting our model to a new domain, we progressively\nadd new slots to the memory bank, which increases the number of parameters, and\nthus the model capacity. We learn the new memory slots and fine-tune existing\nparameters by back-propagation. Experimental results show that our approach\nachieves significantly better performance than fine-tuning alone. Compared with\nexpanding hidden states, our approach is more robust for old domains, shown by\nboth empirical and theoretical results. Our model also outperforms previous\nwork of IDA including elastic weight consolidation and progressive neural\nnetworks in the experiments.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:22:01 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 04:04:14 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Asghar", "Nabiha", ""], ["Mou", "Lili", ""], ["Selby", "Kira A.", ""], ["Pantasdo", "Kevin D.", ""], ["Poupart", "Pascal", ""], ["Jiang", "Xin", ""]]}, {"id": "1811.00240", "submitter": "Farhad Bin Siddique", "authors": "Farhad Bin Siddique, Dario Bertero, Pascale Fung", "title": "GlobalTrait: Personality Alignment of Multilingual Word Embeddings", "comments": "Submitted and accepted to AAAI 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multilingual model to recognize Big Five Personality traits from\ntext data in four different languages: English, Spanish, Dutch and Italian. Our\nanalysis shows that words having a similar semantic meaning in different\nlanguages do not necessarily correspond to the same personality traits.\nTherefore, we propose a personality alignment method, GlobalTrait, which has a\nmapping for each trait from the source language to the target language\n(English), such that words that correlate positively to each trait are close\ntogether in the multilingual vector space. Using these aligned embeddings for\ntraining, we can transfer personality related training features from\nhigh-resource languages such as English to other low-resource languages, and\nget better multilingual results, when compared to using simple monolingual and\nunaligned multilingual embeddings. We achieve an average F-score increase\n(across all three languages except English) from 65 to 73.4 (+8.4), when\ncomparing our monolingual model to multilingual using CNN with personality\naligned embeddings. We also show relatively good performance in the regression\ntasks, and better classification results when evaluating our model on a\nseparate Chinese dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:26:27 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 04:47:57 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Siddique", "Farhad Bin", ""], ["Bertero", "Dario", ""], ["Fung", "Pascale", ""]]}, {"id": "1811.00241", "submitter": "Zhiping Zeng", "authors": "Zhiping Zeng, Yerbolat Khassanov, Van Tung Pham, Haihua Xu, Eng Siong\n  Chng, Haizhou Li", "title": "On the End-to-End Solution to Mandarin-English Code-switching Speech\n  Recognition", "comments": "Accepted for Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-switching (CS) refers to a linguistic phenomenon where a speaker uses\ndifferent languages in an utterance or between alternating utterances. In this\nwork, we study end-to-end (E2E) approaches to the Mandarin-English\ncode-switching speech recognition (CSSR) task. We first examine the\neffectiveness of using data augmentation and byte-pair encoding (BPE) subword\nunits. More importantly, we propose a multitask learning recipe, where a\nlanguage identification task is explicitly learned in addition to the E2E\nspeech recognition task. Furthermore, we introduce an efficient word vocabulary\nexpansion method for language modeling to alleviate data sparsity issues under\nthe code-switching scenario. Experimental results on the SEAME data, a\nMandarin-English CS corpus, demonstrate the effectiveness of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:29:02 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 10:25:52 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zeng", "Zhiping", ""], ["Khassanov", "Yerbolat", ""], ["Pham", "Van Tung", ""], ["Xu", "Haihua", ""], ["Chng", "Eng Siong", ""], ["Li", "Haizhou", ""]]}, {"id": "1811.00253", "submitter": "Kaitao Song", "authors": "Kaitao Song, Xu Tan, Furong Peng and Jianfeng Lu", "title": "Hybrid Self-Attention Network for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoder-decoder is the typical framework for Neural Machine Translation\n(NMT), and different structures have been developed for improving the\ntranslation performance. Transformer is one of the most promising structures,\nwhich can leverage the self-attention mechanism to capture the semantic\ndependency from global view. However, it cannot distinguish the relative\nposition of different tokens very well, such as the tokens located at the left\nor right of the current token, and cannot focus on the local information around\nthe current token either. To alleviate these problems, we propose a novel\nattention mechanism named Hybrid Self-Attention Network (HySAN) which\naccommodates some specific-designed masks for self-attention network to extract\nvarious semantic, such as the global/local information, the left/right part\ncontext. Finally, a squeeze gate is introduced to combine different kinds of\nSANs for fusion. Experimental results on three machine translation tasks show\nthat our proposed framework outperforms the Transformer baseline significantly\nand achieves superior results over state-of-the-art NMT systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 06:35:21 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 03:57:51 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 11:50:42 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Song", "Kaitao", ""], ["Tan", "Xu", ""], ["Peng", "Furong", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1811.00258", "submitter": "Long Zhou", "authors": "Long Zhou, Yuchen Liu, Jiajun Zhang, Chengqing Zong, Guoping Huang", "title": "Language-Independent Representor for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current Neural Machine Translation (NMT) employs a language-specific encoder\nto represent the source sentence and adopts a language-specific decoder to\ngenerate target translation. This language-dependent design leads to\nlarge-scale network parameters and makes the duality of the parallel data\nunderutilized. To address the problem, we propose in this paper a\nlanguage-independent representor to replace the encoder and decoder by using\nweight sharing. This shared representor can not only reduce large portion of\nnetwork parameters, but also facilitate us to fully explore the language\nduality by jointly training source-to-target, target-to-source, left-to-right\nand right-to-left translations within a multi-task learning framework.\nExperiments show that our proposed framework can obtain significant\nimprovements over conventional NMT models on resource-rich and low-resource\ntranslation tasks with only a quarter of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 06:47:23 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zhou", "Long", ""], ["Liu", "Yuchen", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""], ["Huang", "Guoping", ""]]}, {"id": "1811.00266", "submitter": "Shonosuke Ishiwatari", "authors": "Shonosuke Ishiwatari, Hiroaki Hayashi, Naoki Yoshinaga, Graham Neubig,\n  Shoetsu Sato, Masashi Toyoda and Masaru Kitsuregawa", "title": "Learning to Describe Phrases with Local and Global Contexts", "comments": "Accepted to NAACL-HLT2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When reading a text, it is common to become stuck on unfamiliar words and\nphrases, such as polysemous words with novel senses, rarely used idioms,\ninternet slang, or emerging entities. If we humans cannot figure out the\nmeaning of those expressions from the immediate local context, we consult\ndictionaries for definitions or search documents or the web to find other\nglobal context to help in interpretation. Can machines help us do this work?\nWhich type of context is more important for machines to solve the problem? To\nanswer these questions, we undertake a task of describing a given phrase in\nnatural language based on its local and global contexts. To solve this task, we\npropose a neural description model that consists of two context encoders and a\ndescription decoder. In contrast to the existing methods for non-standard\nEnglish explanation [Ni+ 2017] and definition generation [Noraset+ 2017;\nGadetsky+ 2018], our model appropriately takes important clues from both local\nand global contexts. Experimental results on three existing datasets (including\nWordNet, Oxford and Urban Dictionaries) and a dataset newly created from\nWikipedia demonstrate the effectiveness of our method over previous work.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:18:33 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 06:10:51 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Ishiwatari", "Shonosuke", ""], ["Hayashi", "Hiroaki", ""], ["Yoshinaga", "Naoki", ""], ["Neubig", "Graham", ""], ["Sato", "Shoetsu", ""], ["Toyoda", "Masashi", ""], ["Kitsuregawa", "Masaru", ""]]}, {"id": "1811.00275", "submitter": "Pengcheng Yang", "authors": "Pengcheng Yang, Fuli Luo, Shuangzhi Wu, Jingjing Xu, Dongdong Zhang,\n  Xu Sun", "title": "Learning Unsupervised Word Mapping by Maximizing Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual word embeddings aim to capture common linguistic regularities\nof different languages, which benefit various downstream tasks ranging from\nmachine translation to transfer learning. Recently, it has been shown that\nthese embeddings can be effectively learned by aligning two disjoint\nmonolingual vector spaces through a linear transformation (word mapping). In\nthis work, we focus on learning such a word mapping without any supervision\nsignal. Most previous work of this task adopts parametric metrics to measure\ndistribution differences, which typically requires a sophisticated alternate\noptimization process, either in the form of \\emph{minmax game} or intermediate\n\\emph{density estimation}. This alternate optimization process is relatively\nhard and unstable. In order to avoid such sophisticated alternate optimization,\nwe propose to learn unsupervised word mapping by directly maximizing the mean\ndiscrepancy between the distribution of transferred embedding and target\nembedding. Extensive experimental results show that our proposed model\noutperforms competitive baselines by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:54:31 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yang", "Pengcheng", ""], ["Luo", "Fuli", ""], ["Wu", "Shuangzhi", ""], ["Xu", "Jingjing", ""], ["Zhang", "Dongdong", ""], ["Sun", "Xu", ""]]}, {"id": "1811.00287", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang, Jun Xie, Zhixing Tan, Jinsong Su, Deyi Xiong and Lei Li", "title": "Towards Linear Time Neural Machine Translation with Capsule Networks", "comments": "Accepted as EMNLP2019", "journal-ref": null, "doi": "10.18653/v1/D19-1074", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we first investigate a novel capsule network with dynamic\nrouting for linear time Neural Machine Translation (NMT), referred as\n\\textsc{CapsNMT}. \\textsc{CapsNMT} uses an aggregation mechanism to map the\nsource sentence into a matrix with pre-determined size, and then applys a deep\nLSTM network to decode the target sequence from the source representation.\nUnlike the previous work \\cite{sutskever2014sequence} to store the source\nsentence with a passive and bottom-up way, the dynamic routing policy encodes\nthe source sentence with an iterative process to decide the credit attribution\nbetween nodes from lower and higher layers. \\textsc{CapsNMT} has two core\nproperties: it runs in time that is linear in the length of the sequences and\nprovides a more flexible way to select, represent and aggregates the part-whole\ninformation of the source sentence. On WMT14 English-German task and a larger\nWMT14 English-French task, \\textsc{CapsNMT} achieves comparable results with\nthe state-of-the-art NMT systems. To the best of our knowledge, this is the\nfirst work that capsule networks have been empirically investigated for\nsequence to sequence problems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 09:37:48 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 13:32:33 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 06:11:14 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 03:41:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Mingxuan", ""], ["Xie", "Jun", ""], ["Tan", "Zhixing", ""], ["Su", "Jinsong", ""], ["Xiong", "Deyi", ""], ["Li", "Lei", ""]]}, {"id": "1811.00347", "submitter": "Ramon Sanabria", "authors": "Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott,\n  Lo\\\"ic Barrault, Lucia Specia and Florian Metze", "title": "How2: A Large-scale Dataset for Multimodal Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce How2, a multimodal collection of instructional\nvideos with English subtitles and crowdsourced Portuguese translations. We also\npresent integrated sequence-to-sequence baselines for machine translation,\nautomatic speech recognition, spoken language translation, and multimodal\nsummarization. By making available data and code for several multimodal natural\nlanguage tasks, we hope to stimulate more research on these and similar\nchallenges, to obtain a deeper understanding of multimodality in language\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 12:47:11 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 07:03:52 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Sanabria", "Ramon", ""], ["Caglayan", "Ozan", ""], ["Palaskar", "Shruti", ""], ["Elliott", "Desmond", ""], ["Barrault", "Lo\u00efc", ""], ["Specia", "Lucia", ""], ["Metze", "Florian", ""]]}, {"id": "1811.00357", "submitter": "Iacer Calixto", "authors": "Iacer Calixto and Miguel Rios and Wilker Aziz", "title": "Latent Variable Model for Multi-modal Translation", "comments": "Paper accepted at ACL 2019. Contains 8 pages (11 including\n  references, 13 including appendix), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose to model the interaction between visual and textual\nfeatures for multi-modal neural machine translation (MMT) through a latent\nvariable model. This latent variable can be seen as a multi-modal stochastic\nembedding of an image and its description in a foreign language. It is used in\na target-language decoder and also to predict image features. Importantly, our\nmodel formulation utilises visual and textual inputs during training but does\nnot require that images be available at test time. We show that our latent\nvariable MMT formulation improves considerably over strong baselines, including\na multi-task learning approach (Elliott and K\\'ad\\'ar, 2017) and a conditional\nvariational auto-encoder approach (Toyama et al., 2016). Finally, we show\nimprovements due to (i) predicting image features in addition to only\nconditioning on them, (ii) imposing a constraint on the minimum amount of\ninformation encoded in the latent variable, and (iii) by training on additional\ntarget-language image descriptions (i.e. synthetic data).\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 13:19:27 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 16:56:25 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Calixto", "Iacer", ""], ["Rios", "Miguel", ""], ["Aziz", "Wilker", ""]]}, {"id": "1811.00379", "submitter": "Deepak Gupta", "authors": "Hitesh Golchha, Deepak Gupta, Asif Ekbal, Pushpak Bhattacharyya", "title": "Helping each Other: A Framework for Customer-to-Customer Suggestion\n  Mining using a Semi-supervised Deep Neural Network", "comments": "To be appear in the proceedings of ICON 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suggestion mining is increasingly becoming an important task along with\nsentiment analysis. In today's cyberspace world, people not only express their\nsentiments and dispositions towards some entities or services, but they also\nspend considerable time sharing their experiences and advice to fellow\ncustomers and the product/service providers with two-fold agenda: helping\nfellow customers who are likely to share a similar experience, and motivating\nthe producer to bring specific changes in their offerings which would be more\nappreciated by the customers. In our current work, we propose a hybrid deep\nlearning model to identify whether a review text contains any suggestion. The\nmodel employs semi-supervised learning to leverage the useful information from\nthe large amount of unlabeled data. We evaluate the performance of our proposed\nmodel on a benchmark customer review dataset, comprising of the reviews of\nHotel and Electronics domains. Our proposed approach shows the F-scores of\n65.6% and 65.5% for the Hotel and Electronics review datasets, respectively.\nThese performances are significantly better compared to the existing\nstate-of-the-art system.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 13:49:58 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Golchha", "Hitesh", ""], ["Gupta", "Deepak", ""], ["Ekbal", "Asif", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1811.00383", "submitter": "Rudra Murthy V", "authors": "Rudra Murthy V, Anoop Kunchukuttan, Pushpak Bhattacharyya", "title": "Addressing word-order Divergence in Multilingual Neural Machine\n  Translation for extremely Low Resource Languages", "comments": "Accepted as Short Paper at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning approaches for Neural Machine Translation (NMT) train a NMT\nmodel on the assisting-target language pair (parent model) which is later\nfine-tuned for the source-target language pair of interest (child model), with\nthe target language being the same. In many cases, the assisting language has a\ndifferent word order from the source language. We show that divergent word\norder adversely limits the benefits from transfer learning when little to no\nparallel corpus between the source and target language is available. To bridge\nthis divergence, We propose to pre-order the assisting language sentence to\nmatch the word order of the source language and train the parent model. Our\nexperiments on many language pairs show that bridging the word order gap leads\nto significant improvement in the translation quality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 13:53:27 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 05:15:55 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Murthy", "Rudra", "V"], ["Kunchukuttan", "Anoop", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1811.00403", "submitter": "Herman Kamper", "authors": "Herman Kamper", "title": "Truly unsupervised acoustic word embeddings using weak top-down\n  constraints in encoder-decoder models", "comments": "5 pages, 3 figures, 2 tables; accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate unsupervised models that can map a variable-duration speech\nsegment to a fixed-dimensional representation. In settings where unlabelled\nspeech is the only available resource, such acoustic word embeddings can form\nthe basis for \"zero-resource\" speech search, discovery and indexing systems.\nMost existing unsupervised embedding methods still use some supervision, such\nas word or phoneme boundaries. Here we propose the encoder-decoder\ncorrespondence autoencoder (EncDec-CAE), which, instead of true word segments,\nuses automatically discovered segments: an unsupervised term discovery system\nfinds pairs of words of the same unknown type, and the EncDec-CAE is trained to\nreconstruct one word given the other as input. We compare it to a standard\nencoder-decoder autoencoder (AE), a variational AE with a prior over its latent\nembedding, and downsampling. EncDec-CAE outperforms its closest competitor by\n24% relative in average precision on two languages in a word discrimination\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 14:17:01 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 14:28:07 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kamper", "Herman", ""]]}, {"id": "1811.00405", "submitter": "Soujanya Poria", "authors": "Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea,\n  Alexander Gelbukh, Erik Cambria", "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotion detection in conversations is a necessary step for a number of\napplications, including opinion mining over chat history, social media threads,\ndebates, argumentation mining, understanding consumer feedback in live\nconversations, etc. Currently, systems do not treat the parties in the\nconversation individually by adapting to the speaker of each utterance. In this\npaper, we describe a new method based on recurrent neural networks that keeps\ntrack of the individual party states throughout the conversation and uses this\ninformation for emotion classification. Our model outperforms the state of the\nart by a significant margin on two different datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 14:27:19 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 15:50:06 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 09:56:20 GMT"}, {"version": "v4", "created": "Sat, 25 May 2019 09:42:37 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Majumder", "Navonil", ""], ["Poria", "Soujanya", ""], ["Hazarika", "Devamanyu", ""], ["Mihalcea", "Rada", ""], ["Gelbukh", "Alexander", ""], ["Cambria", "Erik", ""]]}, {"id": "1811.00436", "submitter": "Haggai Roitman", "authors": "Haggai Roitman, Guy Feigenblat, David Konopnicki, Doron Cohen, Odellia\n  Boni", "title": "Unsupervised Dual-Cascade Learning with Pseudo-Feedback Distillation for\n  Query-based Extractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dual-CES -- a novel unsupervised, query-focused, multi-document\nextractive summarizer. Dual-CES is designed to better handle the tradeoff\nbetween saliency and focus in summarization. To this end, Dual-CES employs a\ntwo-step dual-cascade optimization approach with saliency-based pseudo-feedback\ndistillation. Overall, Dual-CES significantly outperforms all other\nstate-of-the-art unsupervised alternatives. Dual-CES is even shown to be able\nto outperform strong supervised summarizers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:32:38 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Roitman", "Haggai", ""], ["Feigenblat", "Guy", ""], ["Konopnicki", "David", ""], ["Cohen", "Doron", ""], ["Boni", "Odellia", ""]]}, {"id": "1811.00491", "submitter": "Alane Suhr", "authors": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav\n  Artzi", "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs", "comments": "ACL 2019 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset for joint reasoning about natural language and\nimages, with a focus on semantic diversity, compositionality, and visual\nreasoning challenges. The data contains 107,292 examples of English sentences\npaired with web photographs. The task is to determine whether a natural\nlanguage caption is true about a pair of photographs. We crowdsource the data\nusing sets of visually rich images and a compare-and-contrast task to elicit\nlinguistically diverse language. Qualitative analysis shows the data requires\ncompositional joint reasoning, including about quantities, comparisons, and\nrelations. Evaluation using state-of-the-art visual reasoning methods shows the\ndata presents a strong challenge.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:47:44 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 16:13:16 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 05:26:36 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Suhr", "Alane", ""], ["Zhou", "Stephanie", ""], ["Zhang", "Ally", ""], ["Zhang", "Iris", ""], ["Bai", "Huajun", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.00498", "submitter": "Ra\\'ul V\\'azquez", "authors": "Ra\\'ul V\\'azquez, Alessandro Raganato, J\\\"org Tiedemann, Mathias\n  Creutz", "title": "Multilingual NMT with a language-independent attention bridge", "comments": null, "journal-ref": "Proceedings of the 4th Workshop on Representation Learning for NLP\n  (RepL4NLP-2019) Pages 33-39", "doi": "10.18653/v1/W19-4305", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multilingual encoder-decoder architecture capable\nof obtaining multilingual sentence representations by means of incorporating an\nintermediate {\\em attention bridge} that is shared across all languages. That\nis, we train the model with language-specific encoders and decoders that are\nconnected via self-attention with a shared layer that we call attention bridge.\nThis layer exploits the semantics from each language for performing translation\nand develops into a language-independent meaning representation that can\nefficiently be used for transfer learning. We present a new framework for the\nefficient development of multilingual NMT using this model and scheduled\ntraining. We have tested the approach in a systematic way with a multi-parallel\ndata set. We show that the model achieves substantial improvements over strong\nbilingual models and that it also works well for zero-shot translation, which\ndemonstrates its ability of abstraction and transfer learning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:06:09 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["V\u00e1zquez", "Ra\u00fal", ""], ["Raganato", "Alessandro", ""], ["Tiedemann", "J\u00f6rg", ""], ["Creutz", "Mathias", ""]]}, {"id": "1811.00511", "submitter": "Woon Sang Cho", "authors": "Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley,\n  Chris Brockett, Mengdi Wang, Jianfeng Gao", "title": "Towards Coherent and Cohesive Long-form Text Generation", "comments": "Selected for spotlight oral presentation at NAACL-HLT 2019 Workshop\n  on Narrative Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating coherent and cohesive long-form texts is a challenging task.\nPrevious works relied on large amounts of human-generated texts to train neural\nlanguage models. However, few attempted to explicitly improve neural language\nmodels from the perspectives of coherence and cohesion. In this work, we\npropose a new neural language model that is equipped with two neural\ndiscriminators which provide feedback signals at the levels of sentence\n(cohesion) and paragraph (coherence). Our model is trained using a simple yet\nefficient variant of policy gradient, called negative-critical sequence\ntraining, which is proposed to eliminate the need of training a separate critic\nfor estimating baseline. Results demonstrate the effectiveness of our approach,\nshowing improvements over the strong baseline -- recurrent attention-based\nbidirectional MLE-trained neural language model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:30:50 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 15:56:31 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Cho", "Woon Sang", ""], ["Zhang", "Pengchuan", ""], ["Zhang", "Yizhe", ""], ["Li", "Xiujun", ""], ["Galley", "Michel", ""], ["Brockett", "Chris", ""], ["Wang", "Mengdi", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1811.00513", "submitter": "Congzheng Song", "authors": "Congzheng Song, Vitaly Shmatikov", "title": "Auditing Data Provenance in Text-Generation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help enforce data-protection regulations such as GDPR and detect\nunauthorized uses of personal data, we develop a new \\emph{model auditing}\ntechnique that helps users check if their data was used to train a machine\nlearning model. We focus on auditing deep-learning models that generate\nnatural-language text, including word prediction and dialog generation. These\nmodels are at the core of popular online services and are often trained on\npersonal data such as users' messages, searches, chats, and comments.\n  We design and evaluate a black-box auditing method that can detect, with very\nfew queries to a model, if a particular user's texts were used to train it\n(among thousands of other users). We empirically show that our method can\nsuccessfully audit well-generalized models that are not overfitted to the\ntraining data. We also analyze how text-generation models memorize word\nsequences and explain why this memorization makes them amenable to auditing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:32:44 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 18:47:05 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Song", "Congzheng", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "1811.00552", "submitter": "Sandeep Subramanian", "authors": "Sandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic\n  Denoyer, Marc'Aurelio Ranzato, Y-Lan Boureau", "title": "Multiple-Attribute Text Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach to unsupervised \"style transfer\" in text is based on\nthe idea of learning a latent representation, which is independent of the\nattributes specifying its \"style\". In this paper, we show that this condition\nis not necessary and is not always met in practice, even with domain\nadversarial training that explicitly aims at learning such disentangled\nrepresentations. We thus propose a new model that controls several factors of\nvariation in textual data where this condition on disentanglement is replaced\nwith a simpler mechanism based on back-translation. Our method allows control\nover multiple attributes, like gender, sentiment, product type, etc., and a\nmore fine-grained control on the trade-off between content preservation and\nchange of style with a pooling operator in the latent space. Our experiments\ndemonstrate that the fully entangled model produces better generations, even\nwhen tested on new and more challenging benchmarks comprising reviews with\nmultiple sentences and multiple attributes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:00:00 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 04:13:22 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Subramanian", "Sandeep", ""], ["Lample", "Guillaume", ""], ["Smith", "Eric Michael", ""], ["Denoyer", "Ludovic", ""], ["Ranzato", "Marc'Aurelio", ""], ["Boureau", "Y-Lan", ""]]}, {"id": "1811.00570", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei\n  Chang, Nanyun Peng", "title": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\n  Study on Dependency Parsing", "comments": "Accepted by NAACL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different languages might have different word orders. In this paper, we\ninvestigate cross-lingual transfer and posit that an order-agnostic model will\nperform better when transferring to distant foreign languages. To test our\nhypothesis, we train dependency parsers on an English corpus and evaluate their\ntransfer performance on 30 other languages. Specifically, we compare encoders\nand decoders based on Recurrent Neural Networks (RNNs) and modified\nself-attentive architectures. The former relies on sequential information while\nthe latter is more flexible at modeling word order. Rigorous experiments and\ndetailed analysis shows that RNN-based architectures transfer well to languages\nthat are close to English, while self-attentive models have better overall\ncross-lingual transferability and perform especially well on distant languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:11:01 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 19:45:14 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 01:20:22 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Zhang", "Zhisong", ""], ["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""], ["Chang", "Kai-Wei", ""], ["Peng", "Nanyun", ""]]}, {"id": "1811.00586", "submitter": "Philipp Dufter", "authors": "Philipp Dufter, Mengjie Zhao, Hinrich Sch\\\"utze", "title": "Multilingual Embeddings Jointly Induced from Contexts and Concepts:\n  Simple, Strong and Scalable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings induced from local context are prevalent in NLP. A simple and\neffective context-based multilingual embedding learner is Levy et al. (2017)'s\nS-ID (sentence ID) method. Another line of work induces high-performing\nmultilingual embeddings from concepts (Dufter et al., 2018). In this paper, we\npropose Co+Co, a simple and scalable method that combines context-based and\nconcept-based learning. From a sentence aligned corpus, concepts are extracted\nvia sampling; words are then associated with their concept ID and sentence ID\nin embedding learning. This is the first work that successfully combines\ncontext-based and concept-based embedding learning. We show that Co+Co performs\nwell for two different application scenarios: the Parallel Bible Corpus (1000+\nlanguages, low-resource) and EuroParl (12 languages, high-resource). Among\nmethods applicable to both corpora, Co+Co performs best in our evaluation setup\nof six tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:48:57 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 19:09:51 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Dufter", "Philipp", ""], ["Zhao", "Mengjie", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1811.00606", "submitter": "Grace Hui Yang", "authors": "Zhiwen Tang and Grace Hui Yang", "title": "DeepTileBars: Visualizing Term Distribution for Neural Information\n  Retrieval", "comments": null, "journal-ref": "Proceedings of the AAAI 2019 Conference on Artificial\n  Intelligence, 33(01), 289-296", "doi": "10.1609/aaai.v33i01.3301289", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most neural Information Retrieval (Neu-IR) models derive query-to-document\nranking scores based on term-level matching. Inspired by TileBars, a classical\nterm distribution visualization method, in this paper, we propose a novel\nNeu-IR model that handles query-to-document matching at the subtopic and higher\nlevels. Our system first splits the documents into topical segments,\n\"visualizes\" the matchings between the query and the segments, and then feeds\nan interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final\nranking scores. DeepTileBars models the relevance signals occurring at\ndifferent granularities in a document's topic hierarchy. It better captures the\ndiscourse structure of a document and thus the matching patterns. Although its\ndesign and implementation are light-weight, DeepTileBars outperforms other\nstate-of-the-art Neu-IR models on benchmark datasets including the Text\nREtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:39:14 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 21:44:21 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 13:58:39 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tang", "Zhiwen", ""], ["Yang", "Grace Hui", ""]]}, {"id": "1811.00613", "submitter": "Jesse Thomason", "authors": "Jesse Thomason, Daniel Gordon and Yonatan Bisk", "title": "Shifting the Baseline: Single Modality Performance on Visual Navigation\n  & QA", "comments": "Published at The Conference of the North American Chapter of the\n  Association for Computational Linguistics (NAACL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the surprising strength of unimodal baselines in multimodal\ndomains, and make concrete recommendations for best practices in future\nresearch. Where existing work often compares against random or majority class\nbaselines, we argue that unimodal approaches better capture and reflect dataset\nbiases and therefore provide an important comparison when assessing the\nperformance of multimodal techniques. We present unimodal ablations on three\nrecent datasets in visual navigation and QA, seeing an up to 29% absolute gain\nin performance over published baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:54:58 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 01:32:55 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 20:17:47 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Thomason", "Jesse", ""], ["Gordon", "Daniel", ""], ["Bisk", "Yonatan", ""]]}, {"id": "1811.00614", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Mehrnoosh Sadrzadeh, Matthew Purver, Julian Hough, Ruth Kempson", "title": "Exploring Semantic Incrementality with Dynamic Syntax and Vector Space\n  Semantics", "comments": "accepted in SemDial 2018:\n  https://semdial.hypotheses.org/program/accepted-papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental requirements for models of semantic processing in\ndialogue is incrementality: a model must reflect how people interpret and\ngenerate language at least on a word-by-word basis, and handle phenomena such\nas fragments, incomplete and jointly-produced utterances. We show that the\nincremental word-by-word parsing process of Dynamic Syntax (DS) can be assigned\na compositional distributional semantics, with the composition operator of DS\ncorresponding to the general operation of tensor contraction from multilinear\nalgebra. We provide abstract semantic decorations for the nodes of DS trees, in\nterms of vectors, tensors, and sums thereof; using the latter to model the\nunderspecified elements crucial to assigning partial representations during\nincremental processing. As a working example, we give an instantiation of this\ntheory using plausibility tensors of compositional distributional semantics,\nand show how our framework can incrementally assign a semantic plausibility\nmeasure as it parses phrases and sentences.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:59:06 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Sadrzadeh", "Mehrnoosh", ""], ["Purver", "Matthew", ""], ["Hough", "Julian", ""], ["Kempson", "Ruth", ""]]}, {"id": "1811.00625", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Jianshu Chen, Zhou Yu", "title": "Incorporating Structured Commonsense Knowledge in Story Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to select an appropriate story ending is the first step towards\nperfect narrative comprehension. Story ending prediction requires not only the\nexplicit clues within the context, but also the implicit knowledge (such as\ncommonsense) to construct a reasonable and consistent story. However, most\nprevious approaches do not explicitly use background commonsense knowledge. We\npresent a neural story ending selection model that integrates three types of\ninformation: narrative sequence, sentiment evolution and commonsense knowledge.\nExperiments show that our model outperforms state-of-the-art approaches on a\npublic dataset, ROCStory Cloze Task , and the performance gain from adding the\nadditional commonsense knowledge is significant.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:30:25 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chen", "Jiaao", ""], ["Chen", "Jianshu", ""], ["Yu", "Zhou", ""]]}, {"id": "1811.00633", "submitter": "Bojan Petrovski", "authors": "Bojan Petrovski, Ignacio Aguado, Andreea Hossmann, Michael Baeriswyl,\n  Claudiu Musat", "title": "Embedding Individual Table Columns for Resilient SQL Chatbots", "comments": "SCAI, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the world's data is stored in relational databases. Accessing these\nrequires specialized knowledge of the Structured Query Language (SQL), putting\nthem out of the reach of many people. A recent research thread in Natural\nLanguage Processing (NLP) aims to alleviate this problem by automatically\ntranslating natural language questions into SQL queries. While the proposed\nsolutions are a great start, they lack robustness and do not easily generalize:\nthe methods require high quality descriptions of the database table columns,\nand the most widely used training dataset, WikiSQL, is heavily biased towards\nusing those descriptions as part of the questions.\n  In this work, we propose solutions to both problems: we entirely eliminate\nthe need for column descriptions, by relying solely on their contents, and we\naugment the WikiSQL dataset by paraphrasing column names to reduce bias. We\nshow that the accuracy of existing methods drops when trained on our augmented,\ncolumn-agnostic dataset, and that our own method reaches state of the art\naccuracy, while relying on column contents only.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:03:41 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Petrovski", "Bojan", ""], ["Aguado", "Ignacio", ""], ["Hossmann", "Andreea", ""], ["Baeriswyl", "Michael", ""], ["Musat", "Claudiu", ""]]}, {"id": "1811.00641", "submitter": "Anish Acharya", "authors": "Anish Acharya, Rahul Goel, Angeliki Metallinou, Inderjit Dhillon", "title": "Online Embedding Compression for Text Classification using Low Rank\n  Matrix Factorization", "comments": "Accepted in Thirty-Third AAAI Conference on Artificial Intelligence\n  (AAAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have become state of the art for natural language\nprocessing (NLP) tasks, however deploying these models in production system\nposes significant memory constraints. Existing compression methods are either\nlossy or introduce significant latency. We propose a compression method that\nleverages low rank matrix factorization during training,to compress the word\nembedding layer which represents the size bottleneck for most NLP models. Our\nmodels are trained, compressed and then further re-trained on the downstream\ntask to recover accuracy while maintaining the reduced size. Empirically, we\nshow that the proposed method can achieve 90% compression with minimal impact\nin accuracy for sentence classification tasks, and outperforms alternative\nmethods like fixed-point quantization or offline word embedding compression. We\nalso analyze the inference time and storage space for our method through FLOP\ncalculations, showing that we can compress DNN models by a configurable ratio\nand regain accuracy loss without introducing additional latency compared to\nfixed point quantization. Finally, we introduce a novel learning rate schedule,\nthe Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate\nto outperform other popular adaptive learning rate algorithms on a sentence\nclassification benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:38:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Acharya", "Anish", ""], ["Goel", "Rahul", ""], ["Metallinou", "Angeliki", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1811.00644", "submitter": "Mohammadreza Rezvan", "authors": "Mohammadreza Rezvan, Saeedeh Shekarpour, Faisal Alshargi,\n  Krishnaprasad Thirunarayan, Valerie L. Shalin, Amit Sheth", "title": "Analyzing and learning the language for different types of harassment", "comments": "Submitted for PLOS ONE Journal 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disclaimer: This paper is concerned with violent online harassment. To\ndescribe the subject at an adequate level of realism, examples of our collected\ntweets involve violent, threatening, vulgar and hateful speech language in the\ncontext of racial, sexual, political, appearance and intellectual harassment.\n  The presence of a significant amount of harassment in user-generated content\nand its negative impact calls for robust automatic detection approaches. This\nrequires that we can identify different forms or types of harassment. Earlier\nwork has classified harassing language in terms of hurtfulness, abusiveness,\nsentiment, and profanity. However, to identify and understand harassment more\naccurately, it is essential to determine the context that represents the\ninterrelated conditions in which they occur. In this paper, we introduce the\nnotion of contextual type to harassment involving five categories: (i) sexual,\n(ii) racial, (iii) appearance-related, (iv) intellectual and (v) political. We\nutilize an annotated corpus from Twitter distinguishing these types of\nharassment. To study the context for each type that sheds light on the\nlinguistic meaning, interpretation, and distribution, we conduct two lines of\ninvestigation: an extensive linguistic analysis, and a statistical distribution\nof unigrams. We then build type-ware classifiers to automate the identification\nof type-specific harassment. Our experiments demonstrate that these classifiers\nprovide competitive accuracy for identifying and analyzing harassment on social\nmedia. We present extensive discussion and major observations about the\neffectiveness of type-aware classifiers using a detailed comparison setup\nproviding insight into the role of type-dependent features.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:40:26 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 15:06:49 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Rezvan", "Mohammadreza", ""], ["Shekarpour", "Saeedeh", ""], ["Alshargi", "Faisal", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Shalin", "Valerie L.", ""], ["Sheth", "Amit", ""]]}, {"id": "1811.00659", "submitter": "Deren Lei", "authors": "Deren Lei, Zichen Sun, Yijun Xiao, William Yang Wang", "title": "Implicit Regularization of Stochastic Gradient Descent in Natural\n  Language Processing: Observations and Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with remarkably strong generalization performances are\nusually over-parameterized. Despite explicit regularization strategies are used\nfor practitioners to avoid over-fitting, the impacts are often small. Some\ntheoretical studies have analyzed the implicit regularization effect of\nstochastic gradient descent (SGD) on simple machine learning models with\ncertain assumptions. However, how it behaves practically in state-of-the-art\nmodels and real-world datasets is still unknown. To bridge this gap, we study\nthe role of SGD implicit regularization in deep learning systems. We show pure\nSGD tends to converge to minimas that have better generalization performances\nin multiple natural language processing (NLP) tasks. This phenomenon coexists\nwith dropout, an explicit regularizer. In addition, neural network's finite\nlearning capability does not impact the intrinsic nature of SGD's implicit\nregularization effect. Specifically, under limited training samples or with\ncertain corrupted labels, the implicit regularization effect remains strong. We\nfurther analyze the stability by varying the weight initialization range. We\ncorroborate these experimental findings with a decision boundary visualization\nusing a 3-layer neural network for interpretation. Altogether, our work enables\na deepened understanding on how implicit regularization affects the deep\nlearning model and sheds light on the future study of the over-parameterized\nmodel's generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:24:25 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lei", "Deren", ""], ["Sun", "Zichen", ""], ["Xiao", "Yijun", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00671", "submitter": "Sean Welleck", "authors": "Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho", "title": "Dialogue Natural Language Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency is a long standing issue faced by dialogue models. In this paper,\nwe frame the consistency of dialogue agents as natural language inference (NLI)\nand create a new natural language inference dataset called Dialogue NLI. We\npropose a method which demonstrates that a model trained on Dialogue NLI can be\nused to improve the consistency of a dialogue model, and evaluate the method\nwith human evaluation and with automatic metrics on a suite of evaluation sets\ndesigned to measure a dialogue model's consistency.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:10:03 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 01:08:01 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Welleck", "Sean", ""], ["Weston", "Jason", ""], ["Szlam", "Arthur", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1811.00681", "submitter": "Yaliang Li", "authors": "Sheng Shen, Yaliang Li, Nan Du, Xian Wu, Yusheng Xie, Shen Ge, Tao\n  Yang, Kai Wang, Xingzheng Liang, Wei Fan", "title": "On the Generation of Medical Question-Answer Pairs", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) has achieved promising progress recently. However,\nanswering a question in real-world scenarios like the medical domain is still\nchallenging, due to the requirement of external knowledge and the insufficient\nquantity of high-quality training data. In the light of these challenges, we\nstudy the task of generating medical QA pairs in this paper. With the insight\nthat each medical question can be considered as a sample from the latent\ndistribution of questions given answers, we propose an automated medical QA\npair generation framework, consisting of an unsupervised key phrase detector\nthat explores unstructured material for validity, and a generator that involves\na multi-pass decoder to integrate structural knowledge for diversity. A series\nof experiments have been conducted on a real-world dataset collected from the\nNational Medical Licensing Examination of China. Both automatic evaluation and\nhuman annotation demonstrate the effectiveness of the proposed method. Further\ninvestigation shows that, by incorporating the generated QA pairs for training,\nsignificant improvement in terms of accuracy can be achieved for the\nexamination QA system.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:50:43 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 23:58:54 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Shen", "Sheng", ""], ["Li", "Yaliang", ""], ["Du", "Nan", ""], ["Wu", "Xian", ""], ["Xie", "Yusheng", ""], ["Ge", "Shen", ""], ["Yang", "Tao", ""], ["Wang", "Kai", ""], ["Liang", "Xingzheng", ""], ["Fan", "Wei", ""]]}, {"id": "1811.00692", "submitter": "Yuanpeng Li", "authors": "Yuanpeng Li, Yi Yang, Jianyu Wang, Wei Xu", "title": "Zero-Shot Transfer VQA Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring a large vocabulary is an important aspect of human intelligence.\nOnecommon approach for human to populating vocabulary is to learn words\nduringreading or listening, and then use them in writing or speaking. This\nability totransfer from input to output is natural for human, but it is\ndifficult for machines.Human spontaneously performs this knowledge transfer in\ncomplicated multimodaltasks, such as Visual Question Answering (VQA). In order\nto approach human-levelArtificial Intelligence, we hope to equip machines with\nsuch ability. Therefore, toaccelerate this research, we propose a newzero-shot\ntransfer VQA(ZST-VQA)dataset by reorganizing the existing VQA v1.0 dataset in\nthe way that duringtraining, some words appear only in one module (i.e.\nquestions) but not in theother (i.e. answers). In this setting, an intelligent\nmodel should understand andlearn the concepts from one module (i.e. questions),\nand at test time, transfer themto the other (i.e. predict the concepts as\nanswers). We conduct evaluation on thisnew dataset using three existing\nstate-of-the-art VQA neural models. Experimentalresults show a significant drop\nin performance on this dataset, indicating existingmethods do not address the\nzero-shot transfer problem. Besides, our analysis findsthat this may be caused\nby the implicit bias learned during training.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 01:02:49 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Li", "Yuanpeng", ""], ["Yang", "Yi", ""], ["Wang", "Jianyu", ""], ["Xu", "Wei", ""]]}, {"id": "1811.00693", "submitter": "Yanran Li", "authors": "Yanran Li and Wenjie Li", "title": "Meta-path Augmented Response Generation", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a chatbot, namely Mocha to make good use of relevant entities when\ngenerating responses. Augmented with meta-path information, Mocha is able to\nmention proper entities following the conversation flow.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 01:06:14 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Li", "Yanran", ""], ["Li", "Wenjie", ""]]}, {"id": "1811.00696", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Liqun Chen, Dinghan\n  Shen, Guoyin Wang, Lawrence Carin", "title": "Sequence Generation with Guider Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence generation with reinforcement learning (RL) has received significant\nattention recently. However, a challenge with such methods is the sparse-reward\nproblem in the RL training process, in which a scalar guiding signal is often\nonly available after an entire sequence has been generated. This type of sparse\nreward tends to ignore the global structural information of a sequence, causing\ngeneration of sequences that are semantically inconsistent. In this paper, we\npresent a model-based RL approach to overcome this issue. Specifically, we\npropose a novel guider network to model the sequence-generation environment,\nwhich can assist next-word prediction and provide intermediate rewards for\ngenerator optimization. Extensive experiments show that the proposed method\nleads to improved performance for both unconditional and conditional\nsequence-generation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 01:21:17 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Chen", "Changyou", ""], ["Gan", "Zhe", ""], ["Wang", "Wenlin", ""], ["Chen", "Liqun", ""], ["Shen", "Dinghan", ""], ["Wang", "Guoyin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1811.00707", "submitter": "Boris Ginsburg", "authors": "Jason Li, Ravi Gadde, Boris Ginsburg, Vitaly Lavrukhin", "title": "Training Neural Speech Recognition Systems with Synthetic Speech\n  Augmentation", "comments": "Pre-print. Work in progress, 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building an accurate automatic speech recognition (ASR) system requires a\nlarge dataset that contains many hours of labeled speech samples produced by a\ndiverse set of speakers. The lack of such open free datasets is one of the main\nissues preventing advancements in ASR research. To address this problem, we\npropose to augment a natural speech dataset with synthetic speech. We train\nvery large end-to-end neural speech recognition models using the LibriSpeech\ndataset augmented with synthetic speech. These new models achieve state of the\nart Word Error Rate (WER) for character-level based models without an external\nlanguage model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:15:46 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Li", "Jason", ""], ["Gadde", "Ravi", ""], ["Ginsburg", "Boris", ""], ["Lavrukhin", "Vitaly", ""]]}, {"id": "1811.00717", "submitter": "He Zhao", "authors": "He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou", "title": "Dirichlet belief networks for topic structure learning", "comments": "accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, considerable research effort has been devoted to developing deep\narchitectures for topic models to learn topic structures. Although several deep\nmodels have been proposed to learn better topic proportions of documents, how\nto leverage the benefits of deep structures for learning word distributions of\ntopics has not yet been rigorously studied. Here we propose a new multi-layer\ngenerative process on word distributions of topics, where each layer consists\nof a set of topics and each topic is drawn from a mixture of the topics of the\nlayer above. As the topics in all layers can be directly interpreted by words,\nthe proposed model is able to discover interpretable topic hierarchies. As a\nself-contained module, our model can be flexibly adapted to different kinds of\ntopic models to improve their modelling accuracy and interpretability.\nExtensive experiments on text corpora demonstrate the advantages of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:54:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhao", "He", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1811.00720", "submitter": "Ting-Rui Chiang", "authors": "Ting-Rui Chiang and Yun-Nung Chen", "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math\n  Word Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving math word problems is a challenging task that requires accurate\nnatural language understanding to bridge natural language texts and math\nexpressions. Motivated by the intuition about how human generates the equations\ngiven the problem texts, this paper presents a neural approach to automatically\nsolve math word problems by operating symbols according to their semantic\nmeanings in texts. This paper views the process of generating equation as a\nbridge between the semantic world and the symbolic world, where the proposed\nneural math solver is based on an encoder-decoder framework. In the proposed\nmodel, the encoder is designed to understand the semantics of problems, and the\ndecoder focuses on tracking semantic meanings of the generated symbols and then\ndeciding which symbol to generate next. The preliminary experiments are\nconducted in a dataset Math23K, and our model significantly outperforms both\nthe state-of-the-art single model and the best non-retrieval-based model over\nabout 10% accuracy, demonstrating the effectiveness of bridging the symbolic\nand semantic worlds from math word problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 03:04:21 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 04:18:33 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chiang", "Ting-Rui", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1811.00728", "submitter": "Xiang Li", "authors": "Xiang Li, Haiyang Xue, Wei Chen, Yang Liu, Yang Feng and Qun Liu", "title": "Improving the Robustness of Speech Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural machine translation (NMT) has achieved impressive progress\nrecently, it is usually trained on the clean parallel data set and hence cannot\nwork well when the input sentence is the production of the automatic speech\nrecognition (ASR) system due to the enormous errors in the source. To solve\nthis problem, we propose a simple but effective method to improve the\nrobustness of NMT in the case of speech translation. We simulate the noise\nexisting in the realistic output of the ASR system and inject them into the\nclean parallel data so that NMT can work under similar word distributions\nduring training and testing. Besides, we also incorporate the Chinese Pinyin\nfeature which is easy to get in speech translation to further improve the\ntranslation performance. Experiment results show that our method has a more\nstable performance and outperforms the baseline by an average of 3.12 BLEU on\nmultiple noisy test sets, even while achieves a generalization improvement on\nthe WMT'17 Chinese-English test set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 03:56:42 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Li", "Xiang", ""], ["Xue", "Haiyang", ""], ["Chen", "Wei", ""], ["Liu", "Yang", ""], ["Feng", "Yang", ""], ["Liu", "Qun", ""]]}, {"id": "1811.00739", "submitter": "Xuan Zhang", "authors": "Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy\n  Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, Marine Carpuat", "title": "An Empirical Exploration of Curriculum Learning for Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation systems based on deep neural networks are expensive to\ntrain. Curriculum learning aims to address this issue by choosing the order in\nwhich samples are presented during training to help train better models faster.\nWe adopt a probabilistic view of curriculum learning, which lets us flexibly\nevaluate the impact of curricula design, and perform an extensive exploration\non a German-English translation task. Results show that it is possible to\nimprove convergence time at no loss in translation quality. However, results\nare highly sensitive to the choice of sample difficulty criteria, curriculum\nschedule and other hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:05:26 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhang", "Xuan", ""], ["Kumar", "Gaurav", ""], ["Khayrallah", "Huda", ""], ["Murray", "Kenton", ""], ["Gwinnup", "Jeremy", ""], ["Martindale", "Marianna J", ""], ["McNamee", "Paul", ""], ["Duh", "Kevin", ""], ["Carpuat", "Marine", ""]]}, {"id": "1811.00770", "submitter": "Ray Oshikawa", "authors": "Ray Oshikawa, Jing Qian, William Yang Wang", "title": "A Survey on Natural Language Processing for Fake News Detection", "comments": "11 pages, no figure, Accepted to LREC 2020", "journal-ref": "Proceedings of the 12th Language Resources and Evaluation\n  Conference (LREC 2020) pp. 6086-6093", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news detection is a critical yet challenging problem in Natural Language\nProcessing (NLP). The rapid rise of social networking platforms has not only\nyielded a vast increase in information accessibility but has also accelerated\nthe spread of fake news. Thus, the effect of fake news has been growing,\nsometimes extending to the offline world and threatening public safety. Given\nthe massive amount of Web content, automatic fake news detection is a practical\nNLP problem useful to all online content providers, in order to reduce the\nhuman time and effort to detect and prevent the spread of fake news. In this\npaper, we describe the challenges involved in fake news detection and also\ndescribe related tasks. We systematically review and compare the task\nformulations, datasets and NLP solutions that have been developed for this\ntask, and also discuss the potentials and limitations of them. Based on our\ninsights, we outline promising research directions, including more\nfine-grained, detailed, fair, and practical detection models. We also highlight\nthe difference between fake news detection and other related tasks, and the\nimportance of NLP solutions for fake news detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:10:21 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 01:40:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Oshikawa", "Ray", ""], ["Qian", "Jing", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00780", "submitter": "Edwin D. Simpson", "authors": "Edwin Simpson and Iryna Gurevych", "title": "A Bayesian Approach for Sequence Tagging with Crowds", "comments": "Accepted for EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current methods for sequence tagging, a core task in NLP, are data hungry,\nwhich motivates the use of crowdsourcing as a cheap way to obtain labelled\ndata. However, annotators are often unreliable and current aggregation methods\ncannot capture common types of span annotation errors. To address this, we\npropose a Bayesian method for aggregating sequence tags that reduces errors by\nmodelling sequential dependencies between the annotations as well as the\nground-truth labels. By taking a Bayesian approach, we account for uncertainty\nin the model due to both annotator errors and the lack of data for modelling\nannotators who complete few tasks. We evaluate our model on crowdsourced data\nfor named entity recognition, information extraction and argument mining,\nshowing that our sequential model outperforms the previous state of the art. We\nalso find that our approach can reduce crowdsourcing costs through more\neffective active learning, as it better captures uncertainty in the sequence\nlabels when there are few annotations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:50:11 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 13:14:02 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 07:31:01 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1811.00783", "submitter": "Byeongchang Kim", "authors": "Byeongchang Kim, Hyunwoo Kim and Gunhee Kim", "title": "Abstractive Summarization of Reddit Posts with Multi-level Memory\n  Networks", "comments": "Published in NAACL-HLT 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of abstractive summarization in two directions:\nproposing a novel dataset and a new model. First, we collect Reddit TIFU\ndataset, consisting of 120K posts from the online discussion forum Reddit. We\nuse such informal crowd-generated posts as text source, in contrast with\nexisting datasets that mostly use formal documents as source such as news\narticles. Thus, our dataset could less suffer from some biases that key\nsentences usually locate at the beginning of the text and favorable summary\ncandidates are already inside the text in similar forms. Second, we propose a\nnovel abstractive summarization model named multi-level memory networks (MMN),\nequipped with multi-level memory to store the information of text from\ndifferent levels of abstraction. With quantitative evaluation and user studies\nvia Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly\nabstractive and the MMN outperforms the state-of-the-art summarization models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:59:19 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 02:33:59 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kim", "Byeongchang", ""], ["Kim", "Hyunwoo", ""], ["Kim", "Gunhee", ""]]}, {"id": "1811.00787", "submitter": "Alexander H. Liu", "authors": "Alexander H. Liu, Hung-yi Lee, Lin-shan Lee", "title": "Adversarial Training of End-to-end Speech Recognition Using a\n  Criticizing Language Model", "comments": "under review ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we proposed a novel Adversarial Training (AT) approach for\nend-to-end speech recognition using a Criticizing Language Model (CLM). In this\nway the CLM and the automatic speech recognition (ASR) model can challenge and\nlearn from each other iteratively to improve the performance. Since the CLM\nonly takes the text as input, huge quantities of unpaired text data can be\nutilized in this approach within end-to-end training. Moreover, AT can be\napplied to any end-to-end ASR model using any deep-learning-based language\nmodeling frameworks, and compatible with any existing end-to-end decoding\nmethod. Initial results with an example experimental setup demonstrated the\nproposed approach is able to gain consistent improvements efficiently from\nauxiliary text data under different scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 09:10:24 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liu", "Alexander H.", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1811.00845", "submitter": "Angrosh Mandya Dr.", "authors": "Angrosh Mandya, Danushka Bollegala, Frans Coenen and Katie Atkinson", "title": "Combining Long Short Term Memory and Convolutional Neural Network for\n  Cross-Sentence n-ary Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose in this paper a combined model of Long Short Term Memory and\nConvolutional Neural Networks (LSTM-CNN) that exploits word embeddings and\npositional embeddings for cross-sentence n-ary relation extraction. The\nproposed model brings together the properties of both LSTMs and CNNs, to\nsimultaneously exploit long-range sequential information and capture most\ninformative features, essential for cross-sentence n-ary relation extraction.\nThe LSTM-CNN model is evaluated on standard dataset on cross-sentence n-ary\nrelation extraction, where it significantly outperforms baselines such as CNNs,\nLSTMs and also a combined CNN-LSTM model. The paper also shows that the\nLSTM-CNN model outperforms the current state-of-the-art methods on\ncross-sentence n-ary relation extraction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 13:22:19 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Mandya", "Angrosh", ""], ["Bollegala", "Danushka", ""], ["Coenen", "Frans", ""], ["Atkinson", "Katie", ""]]}, {"id": "1811.00854", "submitter": "Adel Rahimi", "authors": "Adel Rahimi, Mohammad Bahrani", "title": "Improving Information Retrieval Results for Persian Documents using\n  FarsNet", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method for query expansion, which uses\nFarsNet (Persian WordNet) to find similar tokens related to the query and\nexpand the semantic meaning of the query. For this purpose, we use synonymy\nrelations in FarsNet and extract the related synonyms to query words. This\nalgorithm is used to enhance information retrieval systems and improve search\nresults. The overall evaluation of this system in comparison to the baseline\nmethod (without using query expansion) shows an improvement of about 9 percent\nin Mean Average Precision (MAP).\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:48:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Rahimi", "Adel", ""], ["Bahrani", "Mohammad", ""]]}, {"id": "1811.00869", "submitter": "Durmus Sahin", "authors": "Durmus Ozkan Sahin, Erdal Kilic", "title": "Comparison of Classification Algorithms Used Medical Documents\n  Categorization", "comments": "International Conference on Computer Science and Engineering 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume of text based documents have been increasing day by day. Medical\ndocuments are located within this growing text documents. In this study, the\ntechniques used for text classification applied on medical documents and\nevaluated classification performance. Used data sets are multi class and multi\nlabelled. Chi Square (CHI) technique was used for feature selection also SMO,\nNB, C4.5, RF and KNN algorithms was used for classification. The aim of this\nstudy, success of various classifiers is evaluated on multi class and multi\nlabel data sets consisting of medical documents. The first 400 features, while\nthe most successful in the KNN classifier, feature number 400 and after the SMO\nhas become the most successful classifier.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:08:53 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Sahin", "Durmus Ozkan", ""], ["Kilic", "Erdal", ""]]}, {"id": "1811.00907", "submitter": "Ilia Kulikov", "authors": "Ilia Kulikov, Alexander H. Miller, Kyunghyun Cho, Jason Weston", "title": "Importance of Search and Evaluation Strategies in Neural Dialogue\n  Modeling", "comments": "iNLG 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the impact of search strategies in neural dialogue modeling.\nWe first compare two standard search algorithms, greedy and beam search, as\nwell as our newly proposed iterative beam search which produces a more diverse\nset of candidate responses. We evaluate these strategies in realistic full\nconversations with humans and propose a model-based Bayesian calibration to\naddress annotator bias. These conversations are analyzed using two automatic\nmetrics: log-probabilities assigned by the model and utterance diversity. Our\nexperiments reveal that better search algorithms lead to higher rated\nconversations. However, finding the optimal selection mechanism to choose from\na more diverse set of candidates is still an open question.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:54:50 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 10:11:54 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 11:21:56 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kulikov", "Ilia", ""], ["Miller", "Alexander H.", ""], ["Cho", "Kyunghyun", ""], ["Weston", "Jason", ""]]}, {"id": "1811.00937", "submitter": "Alon Talmor", "authors": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge", "comments": "accepted as a long paper at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When answering a question, people often draw upon their rich world knowledge\nin addition to the particular context. Recent work has focused primarily on\nanswering questions given some relevant document or context, and required very\nlittle general background. To investigate question answering with prior\nknowledge, we present CommonsenseQA: a challenging new dataset for commonsense\nquestion answering. To capture common sense beyond associations, we extract\nfrom ConceptNet (Speer et al., 2017) multiple target concepts that have the\nsame semantic relation to a single source concept. Crowd-workers are asked to\nauthor multiple-choice questions that mention the source concept and\ndiscriminate in turn between each of the target concepts. This encourages\nworkers to create questions with complex semantics that often require prior\nknowledge. We create 12,247 questions through this procedure and demonstrate\nthe difficulty of our task with a large number of strong baselines. Our best\nbaseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,\nwell below human performance, which is 89%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:34:29 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 18:02:58 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Talmor", "Alon", ""], ["Herzig", "Jonathan", ""], ["Lourie", "Nicholas", ""], ["Berant", "Jonathan", ""]]}, {"id": "1811.00942", "submitter": "Raphael Tang", "authors": "Raphael Tang, Jimmy Lin", "title": "Progress and Tradeoffs in Neural Language Models", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have witnessed a dramatic shift towards techniques driven\nby neural networks for a variety of NLP tasks. Undoubtedly, neural language\nmodels (NLMs) have reduced perplexity by impressive amounts. This progress,\nhowever, comes at a substantial cost in performance, in terms of inference\nlatency and energy consumption, which is particularly of concern in deployments\non mobile devices. This paper, which examines the quality-performance tradeoff\nof various language modeling techniques, represents to our knowledge the first\nto make this observation. We compare state-of-the-art NLMs with \"classic\"\nKneser-Ney (KN) LMs in terms of energy usage, latency, perplexity, and\nprediction accuracy using two standard benchmarks. On a Raspberry Pi, we find\nthat orders of increase in latency and energy usage correspond to less change\nin perplexity, while the difference is much less pronounced on a desktop.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:46:52 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Tang", "Raphael", ""], ["Lin", "Jimmy", ""]]}, {"id": "1811.00945", "submitter": "Kurt Shuster", "authors": "Kurt Shuster, Samuel Humeau, Antoine Bordes, Jason Weston", "title": "Image Chat: Engaging Grounded Conversations", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve the long-term goal of machines being able to engage humans in\nconversation, our models should captivate the interest of their speaking\npartners. Communication grounded in images, whereby a dialogue is conducted\nbased on a given photo, is a setup naturally appealing to humans (Hu et al.,\n2014). In this work we study large-scale architectures and datasets for this\ngoal. We test a set of neural architectures using state-of-the-art image and\ntext representations, considering various ways to fuse the components. To test\nsuch models, we collect a dataset of grounded human-human conversations, where\nspeakers are asked to play roles given a provided emotional mood or style, as\nthe use of such traits is also a key factor in engagingness (Guo et al., 2019).\nOur dataset, Image-Chat, consists of 202k dialogues over 202k images using 215\npossible style traits. Automatic metrics and human evaluations of engagingness\nshow the efficacy of our approach; in particular, we obtain state-of-the-art\nperformance on the existing IGC task, and our best performing model is almost\non par with humans on the Image-Chat test set (preferred 47.7% of the time).\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:54:15 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 21:44:44 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Shuster", "Kurt", ""], ["Humeau", "Samuel", ""], ["Bordes", "Antoine", ""], ["Weston", "Jason", ""]]}, {"id": "1811.00960", "submitter": "Lo\\\"ic Vial", "authors": "Lo\\\"ic Vial, Benjamin Lecouteux and Didier Schwab", "title": "Improving the Coverage and the Generalization Ability of Neural Word\n  Sense Disambiguation through Hypernymy and Hyponymy Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Word Sense Disambiguation (WSD), the predominant approach generally\ninvolves a supervised system trained on sense annotated corpora. The limited\nquantity of such corpora however restricts the coverage and the performance of\nthese systems. In this article, we propose a new method that solves these\nissues by taking advantage of the knowledge present in WordNet, and especially\nthe hypernymy and hyponymy relationships between synsets, in order to reduce\nthe number of different sense tags that are necessary to disambiguate all words\nof the lexical database. Our method leads to state of the art results on most\nWSD evaluation tasks, while improving the coverage of supervised systems,\nreducing the training time and the size of the models, without additional\ntraining data. In addition, we exhibit results that significantly outperform\nthe state of the art when our method is combined with an ensembling technique\nand the addition of the WordNet Gloss Tagged as training corpus.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:20:13 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Vial", "Lo\u00efc", ""], ["Lecouteux", "Benjamin", ""], ["Schwab", "Didier", ""]]}, {"id": "1811.00967", "submitter": "Igor Shalyminov", "authors": "Igor Shalyminov, Ond\\v{r}ej Du\\v{s}ek, and Oliver Lemon", "title": "Neural Response Ranking for Social Conversation: A Data-Efficient\n  Approach", "comments": "2018 EMNLP Workshop SCAI: The 2nd International Workshop on\n  Search-Oriented Conversational AI. Brussels, Belgium, October 31, 2018", "journal-ref": "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International\n  Workshop on Search-Oriented Conversational AI, pages 1-8. ISBN\n  978-1-948087-75-9", "doi": "10.18653/v1/W18-5701", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overall objective of 'social' dialogue systems is to support engaging,\nentertaining, and lengthy conversations on a wide variety of topics, including\nsocial chit-chat. Apart from raw dialogue data, user-provided ratings are the\nmost common signal used to train such systems to produce engaging responses. In\nthis paper we show that social dialogue systems can be trained effectively from\nraw unannotated data. Using a dataset of real conversations collected in the\n2017 Alexa Prize challenge, we developed a neural ranker for selecting 'good'\nsystem responses to user utterances, i.e. responses which are likely to lead to\nlong and engaging conversations. We show that (1) our neural ranker\nconsistently outperforms several strong baselines when trained to optimise for\nuser ratings; (2) when trained on larger amounts of data and only using\nconversation length as the objective, the ranker performs better than the one\ntrained using ratings -- ultimately reaching a Precision@1 of 0.87. This\nadvance will make data collection for social conversational agents simpler and\nless expensive in the future.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:31:22 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Shalyminov", "Igor", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Lemon", "Oliver", ""]]}, {"id": "1811.00998", "submitter": "James O' Neill", "authors": "James O' Neill, Danushka Bollegala", "title": "Analysing Dropout and Compounding Errors in Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper carries out an empirical analysis of various dropout techniques\nfor language modelling, such as Bernoulli dropout, Gaussian dropout, Curriculum\nDropout, Variational Dropout and Concrete Dropout. Moreover, we propose an\nextension of variational dropout to concrete dropout and curriculum dropout\nwith varying schedules. We find these extensions to perform well when compared\nto standard dropout approaches, particularly variational curriculum dropout\nwith a linear schedule. Largest performance increases are made when applying\ndropout on the decoder layer. Lastly, we analyze where most of the errors occur\nat test time as a post-analysis step to determine if the well-known problem of\ncompounding errors is apparent and to what end do the proposed methods mitigate\nthis issue for each dataset. We report results on a 2-hidden layer LSTM, GRU\nand Highway network with embedding dropout, dropout on the gated hidden layers\nand the output projection layer for each model. We report our results on\nPenn-TreeBank and WikiText-2 word-level language modelling datasets, where the\nformer reduces the long-tail distribution through preprocessing and one which\npreserves rare words in the training and test set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:31:53 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1811.01001", "submitter": "Mirac Suzgun", "authors": "Mirac Suzgun, Yonatan Belinkov, Stuart M. Shieber", "title": "On Evaluating the Generalization of LSTM Models in Formal Languages", "comments": "Proceedings of the Society for Computation in Linguistics (SCiL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are theoretically Turing-complete and\nestablished themselves as a dominant model for language processing. Yet, there\nstill remains an uncertainty regarding their language learning capabilities. In\nthis paper, we empirically evaluate the inductive learning capabilities of Long\nShort-Term Memory networks, a popular extension of simple RNNs, to learn simple\nformal languages, in particular $a^nb^n$, $a^nb^nc^n$, and $a^nb^nc^nd^n$. We\ninvestigate the influence of various aspects of learning, such as training data\nregimes and model capacity, on the generalization to unobserved samples. We\nfind striking differences in model performances under different training\nsettings and highlight the need for careful analysis and assessment when making\nclaims about the learning capabilities of neural network models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:37:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Suzgun", "Mirac", ""], ["Belinkov", "Yonatan", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1811.01012", "submitter": "Dhiraj Madan", "authors": "Dhiraj Madan, Dinesh Raghu, Gaurav Pandey and Sachindra Joshi", "title": "Unsupervised Learning of Interpretable Dialog Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently several deep learning based models have been proposed for end-to-end\nlearning of dialogs. While these models can be trained from data without the\nneed for any additional annotations, it is hard to interpret them. On the other\nhand, there exist traditional state based dialog systems, where the states of\nthe dialog are discrete and hence easy to interpret. However these states need\nto be handcrafted and annotated in the data. To achieve the best of both\nworlds, we propose Latent State Tracking Network (LSTN) using which we learn an\ninterpretable model in unsupervised manner. The model defines a discrete latent\nvariable at each turn of the conversation which can take a finite set of\nvalues. Since these discrete variables are not present in the training data, we\nuse EM algorithm to train our model in unsupervised manner. In the experiments,\nwe show that LSTN can help achieve interpretability in dialog models without\nmuch decrease in performance compared to end-to-end approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:56:23 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Madan", "Dhiraj", ""], ["Raghu", "Dinesh", ""], ["Pandey", "Gaurav", ""], ["Joshi", "Sachindra", ""]]}, {"id": "1811.01013", "submitter": "Jimmy Lin", "authors": "Peng Shi, Jinfeng Rao, Jimmy Lin", "title": "Simple Attention-Based Representation Learning for Ranking Short Social\n  Media Posts", "comments": "Published in the Proceedings of the 2019 Conference of the North\n  American Chapter of the Association for Computational Linguistics: Human\n  Language Technologies, Volume 1 (Long and Short Papers)", "journal-ref": null, "doi": "10.18653/v1/N19-1229", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of ranking short social media posts with\nrespect to user queries using neural networks. Instead of starting with a\ncomplex architecture, we proceed from the bottom up and examine the\neffectiveness of a simple, word-level Siamese architecture augmented with\nattention-based mechanisms for capturing semantic \"soft\" matches between query\nand post tokens. Extensive experiments on datasets from the TREC Microblog\nTracks show that our simple models not only achieve better effectiveness than\nexisting approaches that are far more complex or exploit a more diverse set of\nrelevance signals, but are also much faster. Implementations of our samCNN\n(Simple Attention-based Matching CNN) models are shared with the community to\nsupport future work.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:57:42 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 20:15:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Shi", "Peng", ""], ["Rao", "Jinfeng", ""], ["Lin", "Jimmy", ""]]}, {"id": "1811.01062", "submitter": "Matthias Lalisse", "authors": "Matthias Lalisse and Paul Smolensky", "title": "Augmenting Compositional Models for Knowledge Base Completion Using\n  Gradient Representations", "comments": "10 pages, 2 figures, To appear in proceedings of the Society for\n  Computation in Linguistics (SCIL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural models of Knowledge Base data have typically employed compositional\nrepresentations of graph objects: entity and relation embeddings are\nsystematically combined to evaluate the truth of a candidate Knowedge Base\nentry. Using a model inspired by Harmonic Grammar, we propose to tokenize\ntriplet embeddings by subjecting them to a process of optimization with respect\nto learned well-formedness conditions on Knowledge Base triplets. The resulting\nmodel, known as Gradient Graphs, leads to sizable improvements when implemented\nas a companion to compositional models. Also, we show that the\n\"supracompositional\" triplet token embeddings it produces have interpretable\nproperties that prove helpful in performing inference on the resulting triplet\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 19:20:53 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 19:12:01 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Lalisse", "Matthias", ""], ["Smolensky", "Paul", ""]]}, {"id": "1811.01063", "submitter": "Ehsan Kamalloo", "authors": "Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane", "title": "Augmenting Neural Response Generation with Context-Aware Topical\n  Attention", "comments": "Accepted at ACL 2019 Workshop on NLP for ConvAI (NLP4ConvAI). 8 pages\n  + 4 appendix pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in\ngenerating natural conversational exchanges. Notwithstanding the syntactically\nwell-formed responses generated by these neural network models, they are prone\nto be acontextual, short and generic. In this work, we introduce a Topical\nHierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven,\nmulti-turn response generation system intended to produce contextual and\ntopic-aware responses. Our model is built upon the basic Seq2Seq model by\naugmenting it with a hierarchical joint attention mechanism that incorporates\ntopical concepts and previous interactions into the response generation. To\ntrain our model, we provide a clean and high-quality conversational dataset\nmined from Reddit comments. We evaluate THRED on two novel automated metrics,\ndubbed Semantic Similarity and Response Echo Index, as well as with human\nevaluation. Our experiments demonstrate that the proposed model is able to\ngenerate more diverse and contextually relevant responses compared to the\nstrong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 19:38:18 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 06:35:51 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Dziri", "Nouha", ""], ["Kamalloo", "Ehsan", ""], ["Mathewson", "Kory W.", ""], ["Zaiane", "Osmar", ""]]}, {"id": "1811.01064", "submitter": "Surafel Melaku Lakew Mr.", "authors": "Surafel M. Lakew, Aliia Erofeeva, Marcello Federico", "title": "Neural Machine Translation into Language Varieties", "comments": "Published at EMNLP 2018: third conference on machine translation (WMT\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both research and commercial machine translation have so far neglected the\nimportance of properly handling the spelling, lexical and grammar divergences\noccurring among language varieties. Notable cases are standard national\nvarieties such as Brazilian and European Portuguese, and Canadian and European\nFrench, which popular online machine translation services are not keeping\ndistinct. We show that an evident side effect of modeling such varieties as\nunique classes is the generation of inconsistent translations. In this work, we\ninvestigate the problem of training neural machine translation from English to\nspecific pairs of language varieties, assuming both labeled and unlabeled\nparallel texts, and low-resource conditions. We report experiments from English\nto two pairs of dialects, EuropeanBrazilian Portuguese and European-Canadian\nFrench, and two pairs of standardized varieties, Croatian-Serbian and\nIndonesian-Malay. We show significant BLEU score improvements over baseline\nsystems when translation into similar languages is learned as a multilingual\ntask with shared representations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 19:41:44 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lakew", "Surafel M.", ""], ["Erofeeva", "Aliia", ""], ["Federico", "Marcello", ""]]}, {"id": "1811.01088", "submitter": "Jason Phang", "authors": "Jason Phang, Thibault F\\'evry, Samuel R. Bowman", "title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate\n  Labeled-data Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining sentence encoders with language modeling and related unsupervised\ntasks has recently been shown to be very effective for language understanding\ntasks. By supplementing language model-style pretraining with further training\non data-rich supervised tasks, such as natural language inference, we obtain\nadditional performance improvements on the GLUE benchmark. Applying\nsupplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of\n81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over\nBERT. We also observe reduced variance across random restarts in this setting.\nOur approach yields similar improvements when applied to ELMo (Peters et al.,\n2018a) and Radford et al. (2018)'s model. In addition, the benefits of\nsupplementary training are particularly pronounced in data-constrained regimes,\nas we show in experiments with artificially limited training data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:04:24 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 19:07:16 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Phang", "Jason", ""], ["F\u00e9vry", "Thibault", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1811.01090", "submitter": "Dor Muhlgay", "authors": "Dor Muhlgay, Jonathan Herzig and Jonathan Berant", "title": "Value-based Search in Execution Space for Mapping Instructions to\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training models to map natural language instructions to programs given target\nworld supervision only requires searching for good programs at training time.\nSearch is commonly done using beam search in the space of partial programs or\nprogram trees, but as the length of the instructions grows finding a good\nprogram becomes difficult. In this work, we propose a search algorithm that\nuses the target world state, known at training time, to train a critic network\nthat predicts the expected reward of every search state. We then score search\nstates on the beam by interpolating their expected reward with the likelihood\nof programs represented by the search state. Moreover, we search not in the\nspace of programs but in a more compressed state of program executions,\naugmented with recent entities and actions. On the SCONE dataset, we show that\nour algorithm dramatically improves performance on all three domains compared\nto standard beam search and other baselines.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:14:46 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 19:42:05 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Muhlgay", "Dor", ""], ["Herzig", "Jonathan", ""], ["Berant", "Jonathan", ""]]}, {"id": "1811.01100", "submitter": "Jiacheng Zhang", "authors": "Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu and Maosong Sun", "title": "Prior Knowledge Integration for Neural Machine Translation using\n  Posterior Regularization", "comments": "ACL 2017 (modified)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural machine translation has made significant progress recently,\nhow to integrate multiple overlapping, arbitrary prior knowledge sources\nremains a challenge. In this work, we propose to use posterior regularization\nto provide a general framework for integrating prior knowledge into neural\nmachine translation. We represent prior knowledge sources as features in a\nlog-linear model, which guides the learning process of the neural translation\nmodel. Experiments on Chinese-English translation show that our approach leads\nto significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:33:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Jiacheng", ""], ["Liu", "Yang", ""], ["Luan", "Huanbo", ""], ["Xu", "Jingfang", ""], ["Sun", "Maosong", ""]]}, {"id": "1811.01115", "submitter": "Sujay Kumar Jauhar", "authors": "Sujay Kumar Jauhar, Michael Gamon and Patrick Pantel", "title": "Neural Task Representations as Weak Supervision for Model Agnostic\n  Cross-Lingual Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Natural language processing is heavily Anglo-centric, while the demand for\nmodels that work in languages other than English is greater than ever. Yet, the\ntask of transferring a model from one language to another can be expensive in\nterms of annotation costs, engineering time and effort. In this paper, we\npresent a general framework for easily and effectively transferring neural\nmodels from English to other languages. The framework, which relies on task\nrepresentations as a form of weak supervision, is model and task agnostic,\nmeaning that many existing neural architectures can be ported to other\nlanguages with minimal effort. The only requirement is unlabeled parallel data,\nand a loss defined over task representations. We evaluate our framework by\ntransferring an English sentiment classifier to three different languages. On a\nbattery of tests, we show that our models outperform a number of strong\nbaselines and rival state-of-the-art results, which rely on more complex\napproaches and significantly more resources and data. Additionally, we find\nthat the framework proposed in this paper is able to capture semantically rich\nand meaningful representations across languages, despite the lack of direct\nsupervision.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 22:52:22 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jauhar", "Sujay Kumar", ""], ["Gamon", "Michael", ""], ["Pantel", "Patrick", ""]]}, {"id": "1811.01116", "submitter": "Xing Niu", "authors": "Xing Niu, Weijia Xu, Marine Carpuat", "title": "Bi-Directional Differentiable Input Reconstruction for Low-Resource\n  Neural Machine Translation", "comments": "Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to better exploit the limited amounts of parallel text available in\nlow-resource settings by introducing a differentiable reconstruction loss for\nneural machine translation (NMT). This loss compares original inputs to\nreconstructed inputs, obtained by back-translating translation hypotheses into\nthe input language. We leverage differentiable sampling and bi-directional NMT\nto train models end-to-end, without introducing additional parameters. This\napproach achieves small but consistent BLEU improvements on four language pairs\nin both translation directions, and outperforms an alternative differentiable\nreconstruction strategy based on hidden states.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 22:55:04 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 22:55:33 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Niu", "Xing", ""], ["Xu", "Weijia", ""], ["Carpuat", "Marine", ""]]}, {"id": "1811.01124", "submitter": "Marco Cuturi", "authors": "Jean Alaux, Edouard Grave, Marco Cuturi, Armand Joulin", "title": "Unsupervised Hyperalignment for Multilingual Word Embeddings", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of aligning continuous word representations, learned\nin multiple languages, to a common space. It was recently shown that, in the\ncase of two languages, it is possible to learn such a mapping without\nsupervision. This paper extends this line of work to the problem of aligning\nmultiple languages to a common space. A solution is to independently map all\nlanguages to a pivot language. Unfortunately, this degrades the quality of\nindirect word translation. We thus propose a novel formulation that ensures\ncomposable mappings, leading to better alignments. We evaluate our method by\njointly aligning word vectors in eleven languages, showing consistent\nimprovement with indirect mappings while maintaining competitive performance on\ndirect word translation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 23:30:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 11:21:19 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 20:58:06 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Alaux", "Jean", ""], ["Grave", "Edouard", ""], ["Cuturi", "Marco", ""], ["Joulin", "Armand", ""]]}, {"id": "1811.01127", "submitter": "Tushar Khot", "authors": "Souvik Kundu and Tushar Khot and Ashish Sabharwal and Peter Clark", "title": "Exploiting Explicit Paths for Multi-hop Reading Comprehension", "comments": null, "journal-ref": "ACL 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, path-based reasoning approach for the multi-hop reading\ncomprehension task where a system needs to combine facts from multiple passages\nto answer a question. Although inspired by multi-hop reasoning over knowledge\ngraphs, our proposed approach operates directly over unstructured text. It\ngenerates potential paths through passages and scores them without any direct\npath supervision. The proposed model, named PathNet, attempts to extract\nimplicit relations from text through entity pair representations, and compose\nthem to encode each path. To capture additional context, PathNet also composes\nthe passage representations along each path to compute a passage-based\nrepresentation. Unlike previous approaches, our model is then able to explain\nits reasoning via these explicit paths through the passages. We show that our\napproach outperforms prior models on the multi-hop Wikihop dataset, and also\ncan be generalized to apply to the OpenBookQA dataset, matching\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 23:48:46 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 22:01:59 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kundu", "Souvik", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Clark", "Peter", ""]]}, {"id": "1811.01135", "submitter": "Lajanugen Logeswaran", "authors": "Lajanugen Logeswaran, Honglak Lee, Samy Bengio", "title": "Content preserving text generation with attribute controls", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of modifying textual attributes of\nsentences. Given an input sentence and a set of attribute labels, we attempt to\ngenerate sentences that are compatible with the conditioning information. To\nensure that the model generates content compatible sentences, we introduce a\nreconstruction loss which interpolates between auto-encoding and\nback-translation loss components. We propose an adversarial loss to enforce\ngenerated samples to be attribute compatible and realistic. Through\nquantitative, qualitative and human evaluations we demonstrate that our model\nis capable of generating fluent sentences that better reflect the conditioning\ninformation compared to prior methods. We further demonstrate that the model is\ncapable of simultaneously controlling multiple attributes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:29:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Logeswaran", "Lajanugen", ""], ["Lee", "Honglak", ""], ["Bengio", "Samy", ""]]}, {"id": "1811.01136", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Holger Schwenk", "title": "Margin-based Parallel Corpus Mining with Multilingual Sentence\n  Embeddings", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:34:05 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 22:17:09 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Artetxe", "Mikel", ""], ["Schwenk", "Holger", ""]]}, {"id": "1811.01137", "submitter": "Surafel Melaku Lakew Mr.", "authors": "Surafel M. Lakew, Aliia Erofeeva, Matteo Negri, Marcello Federico,\n  Marco Turchi", "title": "Transfer Learning in Multilingual Neural Machine Translation with\n  Dynamic Vocabulary", "comments": "Published at the International Workshop on Spoken Language\n  Translation (IWSLT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to transfer knowledge across neural machine translation\n(NMT) models by means of a shared dynamic vocabulary. Our approach allows to\nextend an initial model for a given language pair to cover new languages by\nadapting its vocabulary as long as new data become available (i.e., introducing\nnew vocabulary items if they are not included in the initial model). The\nparameter transfer mechanism is evaluated in two scenarios: i) to adapt a\ntrained single language NMT system to work with a new language pair and ii) to\ncontinuously add new language pairs to grow to a multilingual NMT system. In\nboth the scenarios our goal is to improve the translation performance, while\nminimizing the training convergence time. Preliminary experiments spanning five\nlanguages with different training data sizes (i.e., 5k and 50k parallel\nsentences) show a significant performance gain ranging from +3.85 up to +13.63\nBLEU in different language directions. Moreover, when compared with training an\nNMT model from scratch, our transfer-learning approach allows us to reach\nhigher performance after training up to 4% of the total training steps.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:38:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lakew", "Surafel M.", ""], ["Erofeeva", "Aliia", ""], ["Negri", "Matteo", ""], ["Federico", "Marcello", ""], ["Turchi", "Marco", ""]]}, {"id": "1811.01157", "submitter": "Yonatan Belinkov", "authors": "Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim\n  Dalvi, James Glass", "title": "Identifying and Controlling Important Neurons in Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) models learn representations containing\nsubstantial linguistic information. However, it is not clear if such\ninformation is fully distributed or if some of it can be attributed to\nindividual neurons. We develop unsupervised methods for discovering important\nneurons in NMT models. Our methods rely on the intuition that different models\nlearn similar properties, and do not require any costly external supervision.\nWe show experimentally that translation quality depends on the discovered\nneurons, and find that many of them capture common linguistic phenomena.\nFinally, we show how to control NMT translations in predictable ways, by\nmodifying activations of individual neurons.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 04:22:52 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Bau", "Anthony", ""], ["Belinkov", "Yonatan", ""], ["Sajjad", "Hassan", ""], ["Durrani", "Nadir", ""], ["Dalvi", "Fahim", ""], ["Glass", "James", ""]]}, {"id": "1811.01183", "submitter": "Drahomira Herrmannova", "authors": "Drahomira Herrmannova, Steven R. Young, Robert M. Patton, Christopher\n  G. Stahl, Nicole C. Kleinstreuer, Mary S. Wolfe", "title": "Unsupervised Identification of Study Descriptors in Toxicology Research:\n  An Experimental Study", "comments": "Ninth International Workshop on Health Text Mining and Information\n  Analysis at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and extracting data elements such as study descriptors in\npublication full texts is a critical yet manual and labor-intensive step\nrequired in a number of tasks. In this paper we address the question of\nidentifying data elements in an unsupervised manner. Specifically, provided a\nset of criteria describing specific study parameters, such as species, route of\nadministration, and dosing regimen, we develop an unsupervised approach to\nidentify text segments (sentences) relevant to the criteria. A binary\nclassifier trained to identify publications that met the criteria performs\nbetter when trained on the candidate sentences than when trained on sentences\nrandomly picked from the text, supporting the intuition that our method is able\nto accurately identify study descriptors.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 09:29:36 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Herrmannova", "Drahomira", ""], ["Young", "Steven R.", ""], ["Patton", "Robert M.", ""], ["Stahl", "Christopher G.", ""], ["Kleinstreuer", "Nicole C.", ""], ["Wolfe", "Mary S.", ""]]}, {"id": "1811.01237", "submitter": "Jun Feng", "authors": "Jun Feng, Minlie Huang, Yijie Zhang, Yang Yang and Xiaoyan Zhu", "title": "Relation Mention Extraction from Noisy Data with Hierarchical\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address a task of relation mention extraction from noisy\ndata: extracting representative phrases for a particular relation from noisy\nsentences that are collected via distant supervision. Despite its significance\nand value in many downstream applications, this task is less studied on noisy\ndata. The major challenges exists in 1) the lack of annotation on mention\nphrases, and more severely, 2) handling noisy sentences which do not express a\nrelation at all. To address the two challenges, we formulate the task as a\nsemi-Markov decision process and propose a novel hierarchical reinforcement\nlearning model. Our model consists of a top-level sentence selector to remove\nnoisy sentences, a low-level mention extractor to extract relation mentions,\nand a reward estimator to provide signals to guide data denoising and mention\nextraction without explicit annotations. Experimental results show that our\nmodel is effective to extract relation mentions from noisy data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 15:50:27 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Feng", "Jun", ""], ["Huang", "Minlie", ""], ["Zhang", "Yijie", ""], ["Yang", "Yang", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1811.01241", "submitter": "Jason  Weston", "authors": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli,\n  Jason Weston", "title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open-domain dialogue intelligent agents should exhibit the use of\nknowledge, however there are few convincing demonstrations of this to date. The\nmost popular sequence to sequence models typically \"generate and hope\" generic\nutterances that can be memorized in the weights of the model when mapping from\ninput utterance(s) to output, rather than employing recalled knowledge as\ncontext. Use of knowledge has so far proved difficult, in part because of the\nlack of a supervised learning benchmark task which exhibits knowledgeable open\ndialogue with clear grounding. To that end we collect and release a large\ndataset with conversations directly grounded with knowledge retrieved from\nWikipedia. We then design architectures capable of retrieving knowledge,\nreading and conditioning on it, and finally generating natural responses. Our\nbest performing dialogue models are able to conduct knowledgeable discussions\non open-domain topics as evaluated by automatic metrics and human evaluations,\nwhile our new benchmark allows for measuring further improvements in this\nimportant research direction.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 16:11:29 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 22:23:34 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Dinan", "Emily", ""], ["Roller", "Stephen", ""], ["Shuster", "Kurt", ""], ["Fan", "Angela", ""], ["Auli", "Michael", ""], ["Weston", "Jason", ""]]}, {"id": "1811.01275", "submitter": "Andres Karjus", "authors": "Andres Karjus, Richard A. Blythe, Simon Kirby, Kenny Smith", "title": "Challenges in detecting evolutionary forces in language change using\n  diachronic corpora", "comments": null, "journal-ref": "Glossa: a journal of general linguistics, 5(1) (2020), p.45", "doi": "10.5334/gjgl.909", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newberry et al. (Detecting evolutionary forces in language change, Nature\n551, 2017) tackle an important but difficult problem in linguistics, the\ntesting of selective theories of language change against a null model of drift.\nHaving applied a test from population genetics (the Frequency Increment Test)\nto a number of relevant examples, they suggest stochasticity has a previously\nunder-appreciated role in language evolution. We replicate their results and\nfind that while the overall observation holds, results produced by this\napproach on individual time series can be sensitive to how the corpus is\norganized into temporal segments (binning). Furthermore, we use a large set of\nsimulations in conjunction with binning to systematically explore the range of\napplicability of the Frequency Increment Test. We conclude that care should be\nexercised with interpreting results of tests like the Frequency Increment Test\non individual series, given the researcher degrees of freedom available when\napplying the test to corpus data, and fundamental differences between genetic\nand linguistic data. Our findings have implications for selection testing and\ntemporal binning in general, as well as demonstrating the usefulness of\nsimulations for evaluating methods newly introduced to the field.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 20:02:17 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 13:23:09 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Karjus", "Andres", ""], ["Blythe", "Richard A.", ""], ["Kirby", "Simon", ""], ["Smith", "Kenny", ""]]}, {"id": "1811.01294", "submitter": "Brett Beaulieu-Jones", "authors": "Brett K. Beaulieu-Jones, Isaac S. Kohane and Andrew L. Beam", "title": "Learning Contextual Hierarchical Structure of Medical Concepts with\n  Poincair\\'e Embeddings to Clarify Phenotypes", "comments": "To appear in 2019 Pacific Symposium on Biocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical association studies are increasingly done using clinical concepts,\nand in particular diagnostic codes from clinical data repositories as\nphenotypes. Clinical concepts can be represented in a meaningful, vector space\nusing word embedding models. These embeddings allow for comparison between\nclinical concepts or for straightforward input to machine learning models.\nUsing traditional approaches, good representations require high dimensionality,\nmaking downstream tasks such as visualization more difficult. We applied\nPoincar\\'e embeddings in a 2-dimensional hyperbolic space to a large-scale\nadministrative claims database and show performance comparable to\n100-dimensional embeddings in a euclidean space. We then examine disease\nrelationships under different disease contexts to better understand potential\nphenotypes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 22:47:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Beaulieu-Jones", "Brett K.", ""], ["Kohane", "Isaac S.", ""], ["Beam", "Andrew L.", ""]]}, {"id": "1811.01299", "submitter": "Minh Nguyen", "authors": "Minh N. B. Nguyen, Samuel Thomas, Anne E. Gattiker, Sujatha Kashyap,\n  Kush R. Varshney", "title": "SimplerVoice: A Key Message & Visual Description Generator System for\n  Illiteracy", "comments": null, "journal-ref": "Data For Good Exchange 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SimplerVoice: a key message and visual description generator\nsystem to help low-literate adults navigate the information-dense world with\nconfidence, on their own. SimplerVoice can automatically generate sensible\nsentences describing an unknown object, extract semantic meanings of the object\nusage in the form of a query string, then, represent the string as multiple\ntypes of visual guidance (pictures, pictographs, etc.). We demonstrate\nSimplerVoice system in a case study of generating grocery products' manuals\nthrough a mobile application. To evaluate, we conducted a user study on\nSimplerVoice's generated description in comparison to the information\ninterpreted by users from other methods: the original product package and\nsearch engines' top result, in which SimplerVoice achieved the highest\nperformance score: 4.82 on 5-point mean opinion score scale. Our result shows\nthat SimplerVoice is able to provide low-literate end-users with simple yet\ninformative components to help them understand how to use the grocery products,\nand that the system may potentially provide benefits in other real-world use\ncases\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 23:43:38 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Nguyen", "Minh N. B.", ""], ["Thomas", "Samuel", ""], ["Gattiker", "Anne E.", ""], ["Kashyap", "Sujatha", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1811.01302", "submitter": "Peter Henderson", "authors": "Peter Henderson, Koustuv Sinha, Rosemary Nan Ke, Joelle Pineau", "title": "Adversarial Gain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples can be defined as inputs to a model which induce a\nmistake - where the model output is different than that of an oracle, perhaps\nin surprising or malicious ways. Original models of adversarial attacks are\nprimarily studied in the context of classification and computer vision tasks.\nWhile several attacks have been proposed in natural language processing (NLP)\nsettings, they often vary in defining the parameters of an attack and what a\nsuccessful attack would look like. The goal of this work is to propose a\nunifying model of adversarial examples suitable for NLP tasks in both\ngenerative and classification settings. We define the notion of adversarial\ngain: based in control theory, it is a measure of the change in the output of a\nsystem relative to the perturbation of the input (caused by the so-called\nadversary) presented to the learner. This definition, as we show, can be used\nunder different feature spaces and distance conditions to determine attack or\ndefense effectiveness across different intuitive manifolds. This notion of\nadversarial gain not only provides a useful way for evaluating adversaries and\ndefenses, but can act as a building block for future work in robustness under\nadversaries due to its rooted nature in stability and manifold theory.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 00:02:42 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Henderson", "Peter", ""], ["Sinha", "Koustuv", ""], ["Ke", "Rosemary Nan", ""], ["Pineau", "Joelle", ""]]}, {"id": "1811.01304", "submitter": "Jiaoyan Chen", "authors": "Jiaoyan Chen and Ernesto Jimenez-Ruiz and Ian Horrocks and Charles\n  Sutton", "title": "ColNet: Embedding the Semantics of Web Tables for Column Type Prediction", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically annotating column types with knowledge base (KB) concepts is a\ncritical task to gain a basic understanding of web tables. Current methods rely\non either table metadata like column name or entity correspondences of cells in\nthe KB, and may fail to deal with growing web tables with incomplete meta\ninformation. In this paper we propose a neural network based column type\nannotation framework named ColNet which is able to integrate KB reasoning and\nlookup with machine learning and can automatically train Convolutional Neural\nNetworks for prediction. The prediction model not only considers the contextual\nsemantics within a cell using word representation, but also embeds the\nsemantics of a column by learning locality features from multiple cells. The\nmethod is evaluated with DBPedia and two different web table datasets, T2Dv2\nfrom the general Web and Limaye from Wikipedia pages, and achieves higher\nperformance than the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 00:26:00 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:30:31 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chen", "Jiaoyan", ""], ["Jimenez-Ruiz", "Ernesto", ""], ["Horrocks", "Ian", ""], ["Sutton", "Charles", ""]]}, {"id": "1811.01307", "submitter": "Yu-An Chung", "authors": "Yu-An Chung and Wei-Hung Weng and Schrasing Tong and James Glass", "title": "Towards Unsupervised Speech-to-Text Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for building speech-to-text translation (ST) systems\nusing only monolingual speech and text corpora, in other words, speech\nutterances from a source language and independent text from a target language.\nAs opposed to traditional cascaded systems and end-to-end architectures, our\nsystem does not require any labeled data (i.e., transcribed source audio or\nparallel source and target text corpora) during training, making it especially\napplicable to language pairs with very few or even zero bilingual resources.\nThe framework initializes the ST system with a cross-modal bilingual dictionary\ninferred from the monolingual corpora, that maps every source speech segment\ncorresponding to a spoken word to its target text translation. For unseen\nsource speech utterances, the system first performs word-by-word translation on\neach speech segment in the utterance. The translation is improved by leveraging\na language model and a sequence denoising autoencoder to provide prior\nknowledge about the target language. Experimental results show that our\nunsupervised system achieves comparable BLEU scores to supervised end-to-end\nmodels despite the lack of supervision. We also provide an ablation analysis to\nexamine the utility of each component in our system.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 01:23:47 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Chung", "Yu-An", ""], ["Weng", "Wei-Hung", ""], ["Tong", "Schrasing", ""], ["Glass", "James", ""]]}, {"id": "1811.01331", "submitter": "Yinpei Dai", "authors": "Yinpei Dai, Yichi Zhang, Zhijian Ou, Yanmeng Wang, Junlan Feng", "title": "Elastic CRFs for Open-ontology Slot Filling", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slot filling is a crucial component in task-oriented dialog systems, which is\nto parse (user) utterances into semantic concepts called slots. An ontology is\ndefined by the collection of slots and the values that each slot can take. The\nwidely-used practice of treating slot filling as a sequence labeling task\nsuffers from two drawbacks. First, the ontology is usually pre-defined and\nfixed. Most current methods are unable to predict new labels for unseen slots.\nSecond, the one-hot encoding of slot labels ignores the semantic meanings and\nrelations for slots, which are implicit in their natural language descriptions.\nThese observations motivate us to propose a novel model called elastic\nconditional random field (eCRF), for open-ontology slot filling. eCRFs can\nleverage the neural features of both the utterance and the slot descriptions,\nand are able to model the interactions between different slots. Experimental\nresults show that eCRFs outperforms existing models on both the in-domain and\nthe cross-doamin tasks, especially in predictions of unseen slots and values.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 07:38:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Dai", "Yinpei", ""], ["Zhang", "Yichi", ""], ["Ou", "Zhijian", ""], ["Wang", "Yanmeng", ""], ["Feng", "Junlan", ""]]}, {"id": "1811.01355", "submitter": "Amrith Setlur", "authors": "Amrith Rajagopal Setlur", "title": "Semi-Supervised Confidence Network aided Gated Attention based Recurrent\n  Neural Network for Clickbait Detection", "comments": "Oral Presentation, ICON 2018 [Proceedings in ACL Anthology]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clickbaits are catchy headlines that are frequently used by social media\noutlets in order to allure its viewers into clicking them and thus leading them\nto dubious content. Such venal schemes thrive on exploiting the curiosity of\nnaive social media users, directing traffic to web pages that won't be visited\notherwise. In this paper, we propose a novel, semi-supervised classification\nbased approach, that employs attentions sampled from a Gumbel-Softmax\ndistribution to distill contexts that are fairly important in clickbait\ndetection. An additional loss over the attention weights is used to encode\nprior knowledge. Furthermore, we propose a confidence network that enables\nlearning over weak labels and improves robustness to noisy labels. We show that\nwith merely 30% of strongly labeled samples we can achieve over 97% of the\naccuracy, of current state of the art methods in clickbait detection.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 12:00:37 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Setlur", "Amrith Rajagopal", ""]]}, {"id": "1811.01382", "submitter": "Kai Hu", "authors": "Kai Hu, Zhijian Ou, Min Hu, Junlan Feng", "title": "Neural CRF transducers for sequence labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional random fields (CRFs) have been shown to be one of the most\nsuccessful approaches to sequence labeling. Various linear-chain neural CRFs\n(NCRFs) are developed to implement the non-linear node potentials in CRFs, but\nstill keeping the linear-chain hidden structure. In this paper, we propose NCRF\ntransducers, which consists of two RNNs, one extracting features from\nobservations and the other capturing (theoretically infinite) long-range\ndependencies between labels. Different sequence labeling methods are evaluated\nover POS tagging, chunking and NER (English, Dutch). Experiment results show\nthat NCRF transducers achieve consistent improvements over linear-chain NCRFs\nand RNN transducers across all the four tasks, and can improve state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 14:45:50 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hu", "Kai", ""], ["Ou", "Zhijian", ""], ["Hu", "Min", ""], ["Feng", "Junlan", ""]]}, {"id": "1811.01389", "submitter": "Surafel Melaku Lakew Mr.", "authors": "Surafel M. Lakew, Quintino F. Lotito, Matteo Negri, Marco Turchi,\n  Marcello Federico", "title": "Improving Zero-Shot Translation of Low-Resource Languages", "comments": "Published at the International Workshop on Spoken Language\n  Translation (IWSLT), Tokyo, Japan, December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on multilingual neural machine translation reported competitive\nperformance with respect to bilingual models and surprisingly good performance\neven on (zeroshot) translation directions not observed at training time. We\ninvestigate here a zero-shot translation in a particularly lowresource\nmultilingual setting. We propose a simple iterative training procedure that\nleverages a duality of translations directly generated by the system for the\nzero-shot directions. The translations produced by the system (sub-optimal\nsince they contain mixed language from the shared vocabulary), are then used\ntogether with the original parallel data to feed and iteratively re-train the\nmultilingual network. Over time, this allows the system to learn from its own\ngenerated and increasingly better output. Our approach shows to be effective in\nimproving the two zero-shot directions of our multilingual model. In\nparticular, we observed gains of about 9 BLEU points over a baseline\nmultilingual model and up to 2.08 BLEU over a pivoting mechanism using two\nbilingual models. Further analysis shows that there is also a slight\nimprovement in the non-zero-shot language directions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 15:41:42 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lakew", "Surafel M.", ""], ["Lotito", "Quintino F.", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""], ["Federico", "Marcello", ""]]}, {"id": "1811.01399", "submitter": "Peifeng Wang", "authors": "Peifeng Wang, Jialong Han, Chenliang Li, Rong Pan", "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge\n  Graph Embedding", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding aims at modeling entities and relations with\nlow-dimensional vectors. Most previous methods require that all entities should\nbe seen during training, which is unpractical for real-world knowledge graphs\nwith new entities emerging on a daily basis. Recent efforts on this issue\nsuggest training a neighborhood aggregator in conjunction with the conventional\nentity and relation embeddings, which may help embed new entities inductively\nvia their existing neighbors. However, their neighborhood aggregators neglect\nthe unordered and unequal natures of an entity's neighbors. To this end, we\nsummarize the desired properties that may lead to effective neighborhood\naggregators. We also introduce a novel aggregator, namely, Logic Attention\nNetwork (LAN), which addresses the properties by aggregating neighbors with\nboth rules- and network-based attention weights. By comparing with conventional\naggregators on two knowledge graph completion tasks, we experimentally validate\nLAN's superiority in terms of the desired properties.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 16:39:28 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 04:06:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Peifeng", ""], ["Han", "Jialong", ""], ["Li", "Chenliang", ""], ["Pan", "Rong", ""]]}, {"id": "1811.01409", "submitter": "Mehwish Alam Miss", "authors": "Mehwish Alam, Aldo Gangemi, Valentina Presutti, Diego Reforgiato\n  Recupero", "title": "Semantic Role Labeling for Knowledge Graph Extraction from Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces TakeFive, a new semantic role labeling method that\ntransforms a text into a frame-oriented knowledge graph. It performs dependency\nparsing, identifies the words that evoke lexical frames, locates the roles and\nfillers for each frame, runs coercion techniques, and formalises the results as\na knowledge graph. This formal representation complies with the frame semantics\nused in Framester, a factual-linguistic linked data resource. The obtained\nprecision, recall and F1 values indicate that TakeFive is competitive with\nother existing methods such as SEMAFOR, Pikes, PathLSTM and FRED. We finally\ndiscuss how to combine TakeFive and FRED, obtaining higher values of precision,\nrecall and F1.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 17:57:07 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Alam", "Mehwish", ""], ["Gangemi", "Aldo", ""], ["Presutti", "Valentina", ""], ["Recupero", "Diego Reforgiato", ""]]}, {"id": "1811.01468", "submitter": "Najmeh Sadoughi", "authors": "Najmeh Sadoughi, Greg P. Finley, James Fone, Vignesh Murali, Maxim\n  Korenevski, Slava Baryshnikov, Nico Axtmann, Mark Miller, David\n  Suendermann-Oeft", "title": "Medical code prediction with multi-view convolution and\n  description-regularized label-dependent attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A ubiquitous task in processing electronic medical data is the assignment of\nstandardized codes representing diagnoses and/or procedures to free-text\ndocuments such as medical reports. This is a difficult natural language\nprocessing task that requires parsing long, heterogeneous documents and\nselecting a set of appropriate codes from tens of thousands of\npossibilities---many of which have very few positive training samples. We\npresent a deep learning system that advances the state of the art for the\nMIMIC-III dataset, achieving a new best micro F1-measure of 55.85\\%,\nsignificantly outperforming the previous best result (Mullenbach et al. 2018).\nWe achieve this through a number of enhancements, including two major novel\ncontributions: multi-view convolutional channels, which effectively learn to\nadjust kernel sizes throughout the input; and attention regularization,\nmediated by natural-language code descriptions, which helps overcome sparsity\nfor thousands of uncommon codes. These and other modifications are selected to\naddress difficulties inherent to both automated coding specifically and deep\nlearning generally. Finally, we investigate our accuracy results in detail to\nindividually measure the impact of these contributions and point the way\ntowards future algorithmic improvements.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 00:54:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sadoughi", "Najmeh", ""], ["Finley", "Greg P.", ""], ["Fone", "James", ""], ["Murali", "Vignesh", ""], ["Korenevski", "Maxim", ""], ["Baryshnikov", "Slava", ""], ["Axtmann", "Nico", ""], ["Miller", "Mark", ""], ["Suendermann-Oeft", "David", ""]]}, {"id": "1811.01690", "submitter": "Ram\\'on Fernandez Astudillo", "authors": "Takaaki Hori, Ramon Astudillo, Tomoki Hayashi, Yu Zhang, Shinji\n  Watanabe, Jonathan Le Roux", "title": "Cycle-consistency training for end-to-end speech recognition", "comments": "Submitted to ICASSP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to train end-to-end automatic speech recognition\n(ASR) models using unpaired data. Although the end-to-end approach can\neliminate the need for expert knowledge such as pronunciation dictionaries to\nbuild ASR systems, it still requires a large amount of paired data, i.e.,\nspeech utterances and their transcriptions. Cycle-consistency losses have been\nrecently proposed as a way to mitigate the problem of limited paired data.\nThese approaches compose a reverse operation with a given transformation, e.g.,\ntext-to-speech (TTS) with ASR, to build a loss that only requires unsupervised\ndata, speech in this example. Applying cycle consistency to ASR models is not\ntrivial since fundamental information, such as speaker traits, are lost in the\nintermediate text bottleneck. To solve this problem, this work presents a loss\nthat is based on the speech encoder state sequence instead of the raw speech\nsignal. This is achieved by training a Text-To-Encoder model and defining a\nloss based on the encoder reconstruction error. Experimental results on the\nLibriSpeech corpus show that the proposed cycle-consistency training reduced\nthe word error rate by 14.7% from an initial model trained with 100-hour paired\ndata, using an additional 360 hours of audio data without transcriptions. We\nalso investigate the use of text-only data mainly for language modeling to\nfurther improve the performance in the unpaired data training scenario.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 06:32:58 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 01:58:48 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Hori", "Takaaki", ""], ["Astudillo", "Ramon", ""], ["Hayashi", "Tomoki", ""], ["Zhang", "Yu", ""], ["Watanabe", "Shinji", ""], ["Roux", "Jonathan Le", ""]]}, {"id": "1811.01697", "submitter": "Wei Shi", "authors": "Wei Shi and Vera Demberg", "title": "Learning to Explicitate Connectives with Seq2Seq Network for Implicit\n  Discourse Relation Classification", "comments": "to appear on IWCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Implicit discourse relation classification is one of the most difficult steps\nin discourse parsing. The difficulty stems from the fact that the coherence\nrelation must be inferred based on the content of the discourse relational\narguments. Therefore, an effective encoding of the relational arguments is of\ncrucial importance. We here propose a new model for implicit discourse relation\nclassification, which consists of a classifier, and a sequence-to-sequence\nmodel which is trained to generate a representation of the discourse relational\narguments by trying to predict the relational arguments including a suitable\nimplicit connective. Training is possible because such implicit connectives\nhave been annotated as part of the PDTB corpus. Along with a memory network,\nour model could generate more refined representations for the task. And on the\nnow standard 11-way classification, our method outperforms previous state of\nthe art systems on the PDTB benchmark on multiple settings including cross\nvalidation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:09:53 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 11:31:35 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Shi", "Wei", ""], ["Demberg", "Vera", ""]]}, {"id": "1811.01710", "submitter": "Shankar Kumar", "authors": "Jared Lichtarge, Christopher Alberti, Shankar Kumar, Noam Shazeer,\n  Niki Parmar", "title": "Weakly Supervised Grammatical Error Correction using Iterative Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to Grammatical Error Correction (GEC) that is\neffective at making use of models trained on large amounts of weakly supervised\nbitext. We train the Transformer sequence-to-sequence model on 4B tokens of\nWikipedia revisions and employ an iterative decoding strategy that is tailored\nto the loosely-supervised nature of the Wikipedia training corpus. Finetuning\non the Lang-8 corpus and ensembling yields an F0.5 of 58.3 on the CoNLL'14\nbenchmark and a GLEU of 62.4 on JFLEG. The combination of weakly supervised\ntraining and iterative decoding obtains an F0.5 of 48.2 on CoNLL'14 even\nwithout using any labeled GEC data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 01:31:10 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lichtarge", "Jared", ""], ["Alberti", "Christopher", ""], ["Kumar", "Shankar", ""], ["Shazeer", "Noam", ""], ["Parmar", "Niki", ""]]}, {"id": "1811.01713", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Ian E.H. Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan,\n  Pin-Yu Chen, Pradeep Ravikumar, Michael J. Witbrock", "title": "Word Mover's Embedding: From Word2Vec to Document Embedding", "comments": "EMNLP'18 Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the celebrated Word2Vec technique yields semantically rich\nrepresentations for individual words, there has been relatively less success in\nextending to generate unsupervised sentences or documents embeddings. Recent\nwork has demonstrated that a distance measure between documents called\n\\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,\nyields unprecedented KNN classification accuracy. However, WMD is expensive to\ncompute, and it is hard to extend its use beyond a KNN classifier. In this\npaper, we propose the \\emph{Word Mover's Embedding } (WME), a novel approach to\nbuilding an unsupervised document (sentence) embedding from pre-trained word\nembeddings. In our experiments on 9 benchmark text classification datasets and\n22 textual similarity tasks, the proposed technique consistently matches or\noutperforms state-of-the-art techniques, with significantly higher accuracy on\nproblems of short length.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 19:43:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Wu", "Lingfei", ""], ["Yen", "Ian E. H.", ""], ["Xu", "Kun", ""], ["Xu", "Fangli", ""], ["Balakrishnan", "Avinash", ""], ["Chen", "Pin-Yu", ""], ["Ravikumar", "Pradeep", ""], ["Witbrock", "Michael J.", ""]]}, {"id": "1811.01727", "submitter": "Ronghui You", "authors": "Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka and\n  Shanfeng Zhu", "title": "AttentionXML: Label Tree-based Attention-Aware Deep Model for\n  High-Performance Extreme Multi-Label Text Classification", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label text classification (XMTC) is an important problem in the\nera of big data, for tagging a given text with the most relevant multiple\nlabels from an extremely large-scale label set. XMTC can be found in many\napplications, such as item categorization, web page tagging, and news\nannotation. Traditionally most methods used bag-of-words (BOW) as inputs,\nignoring word context as well as deep semantic information. Recent attempts to\novercome the problems of BOW by deep learning still suffer from 1) failing to\ncapture the important subtext for each label and 2) lack of scalability against\nthe huge number of labels. We propose a new label tree-based deep learning\nmodel for XMTC, called AttentionXML, with two unique features: 1) a multi-label\nattention mechanism with raw text as input, which allows to capture the most\nrelevant part of text to each label; and 2) a shallow and wide probabilistic\nlabel tree (PLT), which allows to handle millions of labels, especially for\n\"tail labels\". We empirically compared the performance of AttentionXML with\nthose of eight state-of-the-art methods over six benchmark datasets, including\nAmazon-3M with around 3 million labels. AttentionXML outperformed all competing\nmethods under all experimental settings. Experimental results also show that\nAttentionXML achieved the best performance against tail labels among label\ntree-based methods. The code and datasets are available at\nhttp://github.com/yourh/AttentionXML .\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:30:33 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 13:29:08 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 15:51:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["You", "Ronghui", ""], ["Zhang", "Zihan", ""], ["Wang", "Ziye", ""], ["Dai", "Suyang", ""], ["Mamitsuka", "Hiroshi", ""], ["Zhu", "Shanfeng", ""]]}, {"id": "1811.01734", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu and Andrei M. Butnaru", "title": "Transductive Learning with String Kernels for Cross-Domain Text\n  Classification", "comments": "Accepted at ICONIP 2018. arXiv admin note: substantial text overlap\n  with arXiv:1808.08409", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many text classification tasks, there is a major problem posed by the\nlack of labeled data in a target domain. Although classifiers for a target\ndomain can be trained on labeled text data from a related source domain, the\naccuracy of such classifiers is usually lower in the cross-domain setting.\nRecently, string kernels have obtained state-of-the-art results in various text\nclassification tasks such as native language identification or automatic essay\nscoring. Moreover, classifiers based on string kernels have been found to be\nrobust to the distribution gap between different domains. In this paper, we\nformally describe an algorithm composed of two simple yet effective\ntransductive learning approaches to further improve the results of string\nkernels in cross-domain settings. By adapting string kernels to the test set\nwithout using the ground-truth test labels, we report significantly better\naccuracy rates in cross-domain English polarity classification.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:05:36 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Butnaru", "Andrei M.", ""]]}, {"id": "1811.01747", "submitter": "Ali Emami Mr.", "authors": "Ali Emami, Paul Trichelair, Adam Trischler, Kaheer Suleman, Hannes\n  Schulz and Jackie Chi Kit Cheung", "title": "The Knowref Coreference Corpus: Removing Gender and Number Cues for\n  Difficult Pronominal Anaphora Resolution", "comments": "9 pages (excluding references), accepted for ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new benchmark for coreference resolution and NLI, Knowref,\nthat targets common-sense understanding and world knowledge. Previous\ncoreference resolution tasks can largely be solved by exploiting the number and\ngender of the antecedents, or have been handcrafted and do not reflect the\ndiversity of naturally occurring text. We present a corpus of over 8,000\nannotated text passages with ambiguous pronominal anaphora. These instances are\nboth challenging and realistic. We show that various coreference systems,\nwhether rule-based, feature-rich, or neural, perform significantly worse on the\ntask than humans, who display high inter-annotator agreement. To explain this\nperformance gap, we show empirically that state-of-the art models often fail to\ncapture context, instead relying on the gender or number of candidate\nantecedents to make a decision. We then use problem-specific insights to\npropose a data-augmentation trick called antecedent switching to alleviate this\ntendency in models. Finally, we show that antecedent switching yields promising\nresults on other tasks as well: we use it to achieve state-of-the-art results\non the GAP coreference task.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:41:26 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 22:16:35 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 20:06:32 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Emami", "Ali", ""], ["Trichelair", "Paul", ""], ["Trischler", "Adam", ""], ["Suleman", "Kaheer", ""], ["Schulz", "Hannes", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "1811.01778", "submitter": "Paul Trichelair", "authors": "Paul Trichelair and Ali Emami and Adam Trischler and Kaheer Suleman\n  and Jackie Chi Kit Cheung", "title": "How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the\n  Winograd Schema Challenge and SWAG", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have significantly improved the state-of-the-art on\ncommon-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge\n(WSC) and SWAG. The question we ask in this paper is whether improved\nperformance on these benchmarks represents genuine progress towards\ncommon-sense-enabled systems. We make case studies of both benchmarks and\ndesign protocols that clarify and qualify the results of previous work by\nanalyzing threats to the validity of previous experimental designs. Our\nprotocols account for several properties prevalent in common-sense benchmarks\nincluding size limitations, structural regularities, and variable instance\ndifficulty.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:11:24 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 06:28:44 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Trichelair", "Paul", ""], ["Emami", "Ali", ""], ["Trischler", "Adam", ""], ["Suleman", "Kaheer", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "1811.01786", "submitter": "Michael Filhol", "authors": "Michael Filhol", "title": "A human-editable Sign Language representation for software editing---and\n  a writing system?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To equip SL with software properly, we need an input system to represent and\nmanipulate signed contents in the same way that every day software allows to\nprocess written text. Refuting the claim that video is good enough a medium to\nserve the purpose, we propose to build a representation that is: editable,\nqueryable, synthesisable and user-friendly---we define those terms upfront. The\nissue being functionally and conceptually linked to that of writing, we study\nexisting writing systems, namely those in use for vocal languages, those\ndesigned and proposed for SLs, and more spontaneous ways in which SL users put\ntheir language in writing. Observing each paradigm in turn, we move on to\npropose a new approach to satisfy our goals of integration in software. We\nfinally open the prospect of our proposition being used outside of this\nrestricted scope, as a writing system in itself, and compare its properties to\nthe other writing systems presented.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:21:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Filhol", "Michael", ""]]}, {"id": "1811.01824", "submitter": "Patrick Fernandes", "authors": "Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt", "title": "Structured Neural Summarization", "comments": "Published in ICLR 2019 https://openreview.net/forum?id=H1ersoRqtm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization of long sequences into a concise statement is a core problem in\nnatural language processing, requiring non-trivial understanding of the input.\nBased on the promising results of graph neural networks on highly structured\ndata, we develop a framework to extend existing sequence encoders with a graph\ncomponent that can reason about long-distance relationships in weakly\nstructured data such as text. In an extensive evaluation, we show that the\nresulting hybrid sequence-graph models outperform both pure sequence models as\nwell as pure graph models on a range of summarization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:12:04 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 13:22:59 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 11:47:10 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 12:43:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fernandes", "Patrick", ""], ["Allamanis", "Miltiadis", ""], ["Brockschmidt", "Marc", ""]]}, {"id": "1811.01833", "submitter": "Guofu Li", "authors": "Guofu Li, Pengjia Zhu, and Zhiyi Chen", "title": "Accelerating System Log Processing by Semi-supervised Learning: A\n  Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing need for more automated system-log analysis tools for\nlarge scale online system in a timely manner. However, conventional way to\nmonitor and classify the log output based on keyword list does not scale well\nfor complex system in which codes contributed by a large group of developers,\nwith diverse ways of encoding the error messages, often with misleading pre-set\nlabels. In this paper, we propose that the design of a large scale online log\nanalysis should follow the \"Least Prior Knowledge Principle\", in which\nunsupervised or semi-supervised solution with the minimal prior knowledge of\nthe log should be encoded directly. Thereby, we report our experience in\ndesigning a two-stage machine learning based method, in which the system logs\nare regarded as the output of a quasi-natural language, pre-filtered by a\nperplexity score threshold, and then undergo a fine-grained classification\nprocedure. Tests on empirical data show that our method has obvious advantage\nregarding to the processing speed and classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 00:28:26 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Guofu", ""], ["Zhu", "Pengjia", ""], ["Chen", "Zhiyi", ""]]}, {"id": "1811.01866", "submitter": "Richard Futrell", "authors": "Richard Futrell and Roger P. Levy", "title": "Do RNNs learn human-like abstract word order preferences?", "comments": "To be presented at SCiL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNN language models have achieved state-of-the-art results on various tasks,\nbut what exactly they are representing about syntax is as yet unclear. Here we\ninvestigate whether RNN language models learn humanlike word order preferences\nin syntactic alternations. We collect language model surprisal scores for\ncontrolled sentence stimuli exhibiting major syntactic alternations in English:\nheavy NP shift, particle shift, the dative alternation, and the genitive\nalternation. We show that RNN language models reproduce human preferences in\nthese alternations based on NP length, animacy, and definiteness. We collect\nhuman acceptability ratings for our stimuli, in the first acceptability\njudgment experiment directly manipulating the predictors of syntactic\nalternations. We show that the RNNs' performance is similar to the human\nacceptability ratings and is not matched by an n-gram baseline model. Our\nresults show that RNNs learn the abstract features of weight, animacy, and\ndefiniteness which underlie soft constraints on syntactic alternations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 17:32:14 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Futrell", "Richard", ""], ["Levy", "Roger P.", ""]]}, {"id": "1811.01910", "submitter": "Nikolai Rozanov", "authors": "Edward Collins, Nikolai Rozanov, Bingbing Zhang", "title": "Evolutionary Data Measures: Understanding the Difficulty of Text\n  Classification Tasks", "comments": "27 pages, 6 tables, 3 figures (submitted for publication in June\n  2018), CoNLL 2018", "journal-ref": "ACL, CoNLL(K18-1037), 22, 380--391, (2018)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification tasks are usually analysed and improved through new model\narchitectures or hyperparameter optimisation but the underlying properties of\ndatasets are discovered on an ad-hoc basis as errors occur. However,\nunderstanding the properties of the data is crucial in perfecting models. In\nthis paper we analyse exactly which characteristics of a dataset best determine\nhow difficult that dataset is for the task of text classification. We then\npropose an intuitive measure of difficulty for text classification datasets\nwhich is simple and fast to calculate. We show that this measure generalises to\nunseen data by comparing it to state-of-the-art datasets and results. This\nmeasure can be used to analyse the precise source of errors in a dataset and\nallows fast estimation of how difficult a dataset is to learn. We searched for\nthis measure by training 12 classical and neural network based models on 78\nreal-world datasets, then use a genetic algorithm to discover the best measure\nof difficulty. Our difficulty-calculating code ( https://github.com/Wluper/edm\n) and datasets ( http://data.wluper.com ) are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:39:54 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 10:07:20 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Collins", "Edward", ""], ["Rozanov", "Nikolai", ""], ["Zhang", "Bingbing", ""]]}, {"id": "1811.01931", "submitter": "Robert Giaquinto", "authors": "Robert Giaquinto and Arindam Banerjee", "title": "DAPPER: Scaling Dynamic Author Persona Topic Model to Billion Word\n  Corpora", "comments": "Published in IEEE International Conference on Data Mining, November\n  2018, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting common narratives from multi-author dynamic text corpora requires\ncomplex models, such as the Dynamic Author Persona (DAP) topic model. However,\nsuch models are complex and can struggle to scale to large corpora, often\nbecause of challenging non-conjugate terms. To overcome such challenges, in\nthis paper we adapt new ideas in approximate inference to the DAP model,\nresulting in the DAP Performed Exceedingly Rapidly (DAPPER) topic model.\nSpecifically, we develop Conjugate-Computation Variational Inference (CVI)\nbased variational Expectation-Maximization (EM) for learning the model,\nyielding fast, closed form updates for each document, replacing iterative\noptimization in earlier work. Our results show significant improvements in\nmodel fit and training time without needing to compromise the model's temporal\nstructure or the application of Regularized Variation Inference (RVI). We\ndemonstrate the scalability and effectiveness of the DAPPER model by extracting\nhealth journeys from the CaringBridge corpus --- a collection of 9 million\njournals written by 200,000 authors during health crises.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 21:27:56 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Giaquinto", "Robert", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1811.01938", "submitter": "Sophie Van Der Zee", "authors": "Sophie van der Zee, Ronald Poppe, Alice Havrileck, and Aurelien\n  Baillon", "title": "A personal model of trumpery: Deception detection in a real-world\n  high-stakes setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language use reveals information about who we are and how we feel1-3. One of\nthe pioneers in text analysis, Walter Weintraub, manually counted which types\nof words people used in medical interviews and showed that the frequency of\nfirst-person singular pronouns (i.e., I, me, my) was a reliable indicator of\ndepression, with depressed people using I more often than people who are not\ndepressed4. Several studies have demonstrated that language use also differs\nbetween truthful and deceptive statements5-7, but not all differences are\nconsistent across people and contexts, making prediction difficult8. Here we\nshow how well linguistic deception detection performs at the individual level\nby developing a model tailored to a single individual: the current US\npresident. Using tweets fact-checked by an independent third party (Washington\nPost), we found substantial linguistic differences between factually correct\nand incorrect tweets and developed a quantitative model based on these\ndifferences. Next, we predicted whether out-of-sample tweets were either\nfactually correct or incorrect and achieved a 73% overall accuracy. Our results\ndemonstrate the power of linguistic analysis in real-world deception research\nwhen applied at the individual level and provide evidence that factually\nincorrect tweets are not random mistakes of the sender.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:46:45 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["van der Zee", "Sophie", ""], ["Poppe", "Ronald", ""], ["Havrileck", "Alice", ""], ["Baillon", "Aurelien", ""]]}, {"id": "1811.01990", "submitter": "Joern Wuebker", "authors": "Joern Wuebker, Patrick Simianer, John DeNero", "title": "Compact Personalized Models for Neural Machine Translation", "comments": "Published at the 2018 Conference on Empirical Methods in Natural\n  Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and compare methods for gradient-based domain adaptation of\nself-attentive neural machine translation models. We demonstrate that a large\nproportion of model parameters can be frozen during adaptation with minimal or\nno reduction in translation quality by encouraging structured sparsity in the\nset of offset tensors during learning via group lasso regularization. We\nevaluate this technique for both batch and incremental adaptation across\nmultiple data sets and language pairs. Our system architecture - combining a\nstate-of-the-art self-attentive model with compact domain adaptation - provides\nhigh quality personalized machine translation that is both space and time\nefficient.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:29:56 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Wuebker", "Joern", ""], ["Simianer", "Patrick", ""], ["DeNero", "John", ""]]}, {"id": "1811.02050", "submitter": "Ye Jia", "authors": "Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J. Weiss, Yuan Cao,\n  Chung-Cheng Chiu, Naveen Ari, Stella Laurenzo, Yonghui Wu", "title": "Leveraging Weakly Supervised Data to Improve End-to-End Speech-to-Text\n  Translation", "comments": "ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end Speech Translation (ST) models have many potential advantages when\ncompared to the cascade of Automatic Speech Recognition (ASR) and text Machine\nTranslation (MT) models, including lowered inference latency and the avoidance\nof error compounding. However, the quality of end-to-end ST is often limited by\na paucity of training data, since it is difficult to collect large parallel\ncorpora of speech and translated transcript pairs. Previous studies have\nproposed the use of pre-trained components and multi-task learning in order to\nbenefit from weakly supervised training data, such as speech-to-transcript or\ntext-to-foreign-text pairs. In this paper, we demonstrate that using\npre-trained MT or text-to-speech (TTS) synthesis models to convert weakly\nsupervised data into speech-to-translation pairs for ST training can be more\neffective than multi-task learning. Furthermore, we demonstrate that a high\nquality end-to-end ST model can be trained using only weakly supervised\ndatasets, and that synthetic data sourced from unlabeled monolingual text or\nspeech can be used to improve performance. Finally, we discuss methods for\navoiding overfitting to synthetic speech with a quantitative ablation study.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:57:09 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 19:47:34 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jia", "Ye", ""], ["Johnson", "Melvin", ""], ["Macherey", "Wolfgang", ""], ["Weiss", "Ron J.", ""], ["Cao", "Yuan", ""], ["Chiu", "Chung-Cheng", ""], ["Ari", "Naveen", ""], ["Laurenzo", "Stella", ""], ["Wu", "Yonghui", ""]]}, {"id": "1811.02058", "submitter": "Seongjun Hahm", "authors": "Seongjun Hahm, Iroro Orife, Shane Walker and Jason Flaks", "title": "The Marchex 2018 English Conversational Telephone Speech Recognition\n  System", "comments": "5 pages, 1 figure. Submitted to INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe recent performance improvements to the production\nMarchex speech recognition system for our spontaneous customer-to-business\ntelephone conversations. In our previous work, we focused on in-domain language\nand acoustic model training. In this work we employ state-of-the-art\nsemi-supervised lattice-free maximum mutual information (LF-MMI) training\nprocess which can supervise over full lattices from unlabeled audio. On Marchex\nEnglish (ME), a modern evaluation set of conversational North American English,\nwe observed a 3.3% (3.2% for agent, 3.6% for caller) reduction in absolute word\nerror rate (WER) with 3x faster decoding speed over the performance of the 2017\nproduction system. We expect this improvement boost Marchex Call Analytics\nsystem performance especially for natural language processing pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:18:31 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 19:54:42 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Hahm", "Seongjun", ""], ["Orife", "Iroro", ""], ["Walker", "Shane", ""], ["Flaks", "Jason", ""]]}, {"id": "1811.02062", "submitter": "Xuankai Chang", "authors": "Xuankai Chang and Yanmin Qian and Kai Yu and Shinji Watanabe", "title": "End-to-End Monaural Multi-speaker ASR System without Pretraining", "comments": "submitted to ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end models have become a popular approach as an alternative\nto traditional hybrid models in automatic speech recognition (ASR). The\nmulti-speaker speech separation and recognition task is a central task in\ncocktail party problem. In this paper, we present a state-of-the-art monaural\nmulti-speaker end-to-end automatic speech recognition model. In contrast to\nprevious studies on the monaural multi-speaker speech recognition, this\nend-to-end framework is trained to recognize multiple label sequences\ncompletely from scratch. The system only requires the speech mixture and\ncorresponding label sequences, without needing any indeterminate supervisions\nobtained from non-mixture speech or corresponding labels/alignments. Moreover,\nwe exploited using the individual attention module for each separated speaker\nand the scheduled sampling to further improve the performance. Finally, we\nevaluate the proposed model on the 2-speaker mixed speech generated from the\nWSJ corpus and the wsj0-2mix dataset, which is a speech separation and\nrecognition benchmark. The experiments demonstrate that the proposed methods\ncan improve the performance of the end-to-end model in separating the\noverlapping speech and recognizing the separated streams. From the results, the\nproposed model leads to ~10.0% relative performance gains in terms of CER and\nWER respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:21:51 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Chang", "Xuankai", ""], ["Qian", "Yanmin", ""], ["Yu", "Kai", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1811.02066", "submitter": "Hossein Zeinali", "authors": "Hossein Zeinali, Lukas Burget, Johan Rohdin, Themos Stafylakis, Jan\n  Cernocky", "title": "How to Improve Your Speaker Embeddings Extractor in Generic Toolkits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, speaker embeddings extracted with deep neural networks became the\nstate-of-the-art method for speaker verification. In this paper we aim to\nfacilitate its implementation on a more generic toolkit than Kaldi, which we\nanticipate to enable further improvements on the method. We examine several\ntricks in training, such as the effects of normalizing input features and\npooled statistics, different methods for preventing overfitting as well as\nalternative non-linearities that can be used instead of Rectifier Linear Units.\nIn addition, we investigate the difference in performance between TDNN and CNN,\nand between two types of attention mechanism. Experimental results on Speaker\nin the Wild, SRE 2016 and SRE 2018 datasets demonstrate the effectiveness of\nthe proposed implementation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:31:00 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Zeinali", "Hossein", ""], ["Burget", "Lukas", ""], ["Rohdin", "Johan", ""], ["Stafylakis", "Themos", ""], ["Cernocky", "Jan", ""]]}, {"id": "1811.02076", "submitter": "Hao Cheng", "authors": "Hao Cheng, Ming-Wei Chang, Kenton Lee, Ankur Parikh, Michael Collins,\n  Kristina Toutanova", "title": "Improving Span-based Question Answering Systems with Coarsely Labeled\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approaches to improve fine-grained short answer Question Answering\nmodels by integrating coarse-grained data annotated for paragraph-level\nrelevance and show that coarsely annotated data can bring significant\nperformance gains. Experiments demonstrate that the standard multi-task\nlearning approach of sharing representations is not the most effective way to\nleverage coarse-grained annotations. Instead, we can explicitly model the\nlatent fine-grained short answer variables and optimize the marginal\nlog-likelihood directly or use a newly proposed \\emph{posterior distillation}\nlearning objective. Since these latent-variable methods have explicit access to\nthe relationship between the fine and coarse tasks, they result in\nsignificantly larger improvements from coarse supervision.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:03:02 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Cheng", "Hao", ""], ["Chang", "Ming-Wei", ""], ["Lee", "Kenton", ""], ["Parikh", "Ankur", ""], ["Collins", "Michael", ""], ["Toutanova", "Kristina", ""]]}, {"id": "1811.02122", "submitter": "Younggun Lee", "authors": "Younggun Lee and Taesu Kim", "title": "Robust and fine-grained prosody control of end-to-end speech synthesis", "comments": "ICASSP 2019, best viewed in color", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose prosody embeddings for emotional and expressive speech synthesis\nnetworks. The proposed methods introduce temporal structures in the embedding\nnetworks, thus enabling fine-grained control of the speaking style of the\nsynthesized speech. The temporal structures can be designed either on the\nspeech side or the text side, leading to different control resolutions in time.\nThe prosody embedding networks are plugged into end-to-end speech synthesis\nnetworks and trained without any other supervision except for the target speech\nfor synthesizing. It is demonstrated that the prosody embedding networks\nlearned to extract prosodic features. By adjusting the learned prosody\nfeatures, we could change the pitch and amplitude of the synthesized speech\nboth at the frame level and the phoneme level. We also introduce the temporal\nnormalization of prosody embeddings, which shows better robustness against\nspeaker perturbations during prosody transfer tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 01:54:22 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 06:33:37 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Lee", "Younggun", ""], ["Kim", "Taesu", ""]]}, {"id": "1811.02134", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Jaejin Cho, Murali Karthick Baskar, Tatsuya\n  Kawahara, Shinji Watanabe", "title": "Transfer learning of language-independent end-to-end ASR with language\n  model fusion", "comments": "Accepted at ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores better adaptation methods to low-resource languages using\nan external language model (LM) under the framework of transfer learning. We\nfirst build a language-independent ASR system in a unified sequence-to-sequence\n(S2S) architecture with a shared vocabulary among all languages. During\nadaptation, we perform LM fusion transfer, where an external LM is integrated\ninto the decoder network of the attention-based S2S model in the whole\nadaptation stage, to effectively incorporate linguistic context of the target\nlanguage. We also investigate various seed models for transfer learning.\nExperimental evaluations using the IARPA BABEL data set show that LM fusion\ntransfer improves performances on all target five languages compared with\nsimple transfer learning when the external text data is available. Our final\nsystem drastically reduces the performance gap from the hybrid systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 02:46:23 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 08:49:21 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Cho", "Jaejin", ""], ["Baskar", "Murali Karthick", ""], ["Kawahara", "Tatsuya", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1811.02166", "submitter": "Shun Zheng", "authors": "Shun Zheng, Xu Han, Yankai Lin, Peilin Yu, Lu Chen, Ling Huang,\n  Zhiyuan Liu and Wei Xu", "title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised\n  Neural Relation Extraction", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-based labeling methods have achieved promising results in alleviating\nthe inevitable labeling noises of distantly supervised neural relation\nextraction. However, these methods require significant expert labor to write\nrelation-specific patterns, which makes them too sophisticated to generalize\nquickly.To ease the labor-intensive workload of pattern writing and enable the\nquick generalization to new relation types, we propose a neural pattern\ndiagnosis framework, DIAG-NRE, that can automatically summarize and refine\nhigh-quality relational patterns from noise data with human experts in the\nloop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two\nreal-world datasets and present both significant and interpretable improvements\nover state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 05:08:59 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 09:57:21 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zheng", "Shun", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Yu", "Peilin", ""], ["Chen", "Lu", ""], ["Huang", "Ling", ""], ["Liu", "Zhiyuan", ""], ["Xu", "Wei", ""]]}, {"id": "1811.02172", "submitter": "Chong Wang", "authors": "Jiangtao Feng, Lingpeng Kong, Po-Sen Huang, Chong Wang, Da Huang,\n  Jiayuan Mao, Kan Qiao, Dengyong Zhou", "title": "Neural Phrase-to-Phrase Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Neural Phrase-to-Phrase Machine Translation\n(NP$^2$MT). Our model uses a phrase attention mechanism to discover relevant\ninput (source) segments that are used by a decoder to generate output (target)\nphrases. We also design an efficient dynamic programming algorithm to decode\nsegments that allows the model to be trained faster than the existing neural\nphrase-based machine translation method by Huang et al. (2018). Furthermore,\nour method can naturally integrate with external phrase dictionaries during\ndecoding. Empirical experiments show that our method achieves comparable\nperformance with the state-of-the art methods on benchmark datasets. However,\nwhen the training and testing data are from different distributions or domains,\nour method performs better.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 05:47:52 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Feng", "Jiangtao", ""], ["Kong", "Lingpeng", ""], ["Huang", "Po-Sen", ""], ["Wang", "Chong", ""], ["Huang", "Da", ""], ["Mao", "Jiayuan", ""], ["Qiao", "Kan", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1811.02182", "submitter": "Geonmin Kim", "authors": "Geonmin Kim, Hwaran Lee, Bo-Kyeong Kim, Sang-Hoon Oh, and Soo-Young\n  Lee", "title": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for\n  Speech Recognition", "comments": "will be published in IEEE Signal Processing Letter", "journal-ref": null, "doi": "10.1109/LSP.2018.2880285", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many speech enhancement methods try to learn the relationship between noisy\nand clean speech, obtained using an acoustic room simulator. We point out\nseveral limitations of enhancement methods relying on clean speech targets; the\ngoal of this work is proposing an alternative learning algorithm, called\nacoustic and adversarial supervision (AAS). AAS makes the enhanced output both\nmaximizing the likelihood of transcription on the pre-trained acoustic model\nand having general characteristics of clean speech, which improve\ngeneralization on unseen noisy speeches. We employ the connectionist temporal\nclassification and the unpaired conditional boundary equilibrium generative\nadversarial network as the loss function of AAS. AAS is tested on two datasets\nincluding additive noise without and with reverberation, Librispeech + DEMAND\nand CHiME-4. By visualizing the enhanced speech with different loss\ncombinations, we demonstrate the role of each supervision. AAS achieves a lower\nword error rate than other state-of-the-art methods using the clean speech\ntarget in both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 06:23:57 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Kim", "Geonmin", ""], ["Lee", "Hwaran", ""], ["Kim", "Bo-Kyeong", ""], ["Oh", "Sang-Hoon", ""], ["Lee", "Soo-Young", ""]]}, {"id": "1811.02230", "submitter": "Heike Adel", "authors": "Heike Adel and Hinrich Sch\\\"utze", "title": "CIS at TAC Cold Start 2015: Neural Networks and Coreference Resolution\n  for Slot Filling", "comments": "TAC KBP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the CIS slot filling system for the TAC Cold Start\nevaluations 2015. It extends and improves the system we have built for the\nevaluation last year. This paper mainly describes the changes to our last\nyear's system. Especially, it focuses on the coreference and classification\ncomponent. For coreference, we have performed several analysis and prepared a\nresource to simplify our end-to-end system and improve its runtime. For\nclassification, we propose to use neural networks. We have trained\nconvolutional and recurrent neural networks and combined them with traditional\nevaluation methods, namely patterns and support vector machines. Our runs for\nthe 2015 evaluation have been designed to directly assess the effect of each\nnetwork on the end-to-end performance of the system. The CIS system achieved\nrank 3 of all slot filling systems participating in the task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:57:53 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Adel", "Heike", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1811.02278", "submitter": "Chris Hokamp", "authors": "Chris Hokamp and Sebastian Ruder and John Glover", "title": "Off-the-Shelf Unsupervised NMT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame unsupervised machine translation (MT) in the context of multi-task\nlearning (MTL), combining insights from both directions. We leverage\noff-the-shelf neural MT architectures to train unsupervised MT models with no\nparallel data and show that such models can achieve reasonably good\nperformance, competitive with models purpose-built for unsupervised MT.\nFinally, we propose improvements that allow us to apply our models to\nEnglish-Turkish, a truly low-resource language pair.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 10:50:04 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Hokamp", "Chris", ""], ["Ruder", "Sebastian", ""], ["Glover", "John", ""]]}, {"id": "1811.02318", "submitter": "Lingbing Guo", "authors": "Lingbing Guo, Zequn Sun, Ermei Cao, Wei Hu", "title": "Recurrent Skipping Networks for Entity Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning knowledge graph (KG) embeddings for\nentity alignment (EA). Current methods use the embedding models mainly focusing\non triple-level learning, which lacks the ability of capturing long-term\ndependencies existing in KGs. Consequently, the embedding-based EA methods\nheavily rely on the amount of prior (known) alignment, due to the identity\ninformation in the prior alignment cannot be efficiently propagated from one KG\nto another. In this paper, we propose RSN4EA (recurrent skipping networks for\nEA), which leverages biased random walk sampling for generating long paths\nacross KGs and models the paths with a novel recurrent skipping network (RSN).\nRSN integrates the conventional recurrent neural network (RNN) with residual\nlearning and can largely improve the convergence speed and performance with\nonly a few more parameters. We evaluated RSN4EA on a series of datasets\nconstructed from real-world KGs. Our experimental results showed that it\noutperformed a number of state-of-the-art embedding-based EA methods and also\nachieved comparable performance for KG completion.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:28:58 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Guo", "Lingbing", ""], ["Sun", "Zequn", ""], ["Cao", "Ermei", ""], ["Hu", "Wei", ""]]}, {"id": "1811.02320", "submitter": "Sihao Xue", "authors": "Yixiao Qu, Sihao Xue, Zhenyi Ying, Hang Zhou, Jue Sun", "title": "Hierarchical Neural Network Architecture In Keyword Spotting", "comments": "To be submitted in part to IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it\nis essential to ensure a high recall rate. However, its real-time property\nrequires low computation complexity. This contradiction inspires people to find\na suitable model which is small enough to perform well in multi environments.\nTo deal with this contradiction, we implement the Hierarchical Neural\nNetwork(HNN), which is proved to be effective in many speech recognition\nproblems. HNN outperforms traditional DNN and CNN even though its model size\nand computation complexity are slightly less. Also, its simple topology\nstructure makes easy to deploy on any device.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:32:27 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Qu", "Yixiao", ""], ["Xue", "Sihao", ""], ["Ying", "Zhenyi", ""], ["Zhou", "Hang", ""], ["Sun", "Jue", ""]]}, {"id": "1811.02338", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Lei Hou, Juanzi Li, Zhiyuan Liu, Hanwang Zhang", "title": "Learning to Embed Sentences Using Attentive Recursive Trees", "comments": "AAAI Conference of Artificial Intelligence, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence embedding is an effective feature representation for most deep\nlearning-based NLP tasks. One prevailing line of methods is using recursive\nlatent tree-structured networks to embed sentences with task-specific\nstructures. However, existing models have no explicit mechanism to emphasize\ntask-informative words in the tree structure. To this end, we propose an\nAttentive Recursive Tree model (AR-Tree), where the words are dynamically\nlocated according to their importance in the task. Specifically, we construct\nthe latent tree for a sentence in a proposed important-first strategy, and\nplace more attentive words nearer to the root; thus, AR-Tree can inherently\nemphasize important words during the bottom-up composition of the sentence\nembedding. We propose an end-to-end reinforced training strategy for AR-Tree,\nwhich is demonstrated to consistently outperform, or be at least comparable to,\nthe state-of-the-art sentence embedding methods on three sentence understanding\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 13:12:22 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 03:18:06 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Shi", "Jiaxin", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""], ["Zhang", "Hanwang", ""]]}, {"id": "1811.02356", "submitter": "Ching-Ting Chang", "authors": "Ching-Ting Chang, Shun-Po Chuang, Hung-Yi Lee", "title": "Code-switching Sentence Generation by Generative Adversarial Networks\n  and its Application to Data Augmentation", "comments": "Accepted by Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-switching is about dealing with alternative languages in speech or text.\nIt is partially speaker-depend and domain-related, so completely explaining the\nphenomenon by linguistic rules is challenging. Compared to most monolingual\ntasks, insufficient data is an issue for code-switching. To mitigate the issue\nwithout expensive human annotation, we proposed an unsupervised method for\ncode-switching data augmentation. By utilizing a generative adversarial\nnetwork, we can generate intra-sentential code-switching sentences from\nmonolingual sentences. We applied proposed method on two corpora, and the\nresult shows that the generated code-switching sentences improve the\nperformance of code-switching language models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:07:15 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 00:40:54 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 06:00:01 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 13:31:35 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chang", "Ching-Ting", ""], ["Chuang", "Shun-Po", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1811.02364", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Hai Zhao, Kangwei Ling, Jiangtong Li, Zuchao Li,\n  Shexia He, Guohong Fu", "title": "Effective Subword Segmentation for Text Comprehension", "comments": "Accepted by IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is the foundation of machine reading comprehension\nand inference. In state-of-the-art models, character-level representations have\nbeen broadly adopted to alleviate the problem of effectively representing rare\nor complex words. However, character itself is not a natural minimal linguistic\nunit for representation or word embedding composing due to ignoring the\nlinguistic coherence of consecutive characters inside word. This paper presents\na general subword-augmented embedding framework for learning and composing\ncomputationally-derived subword-level representations. We survey a series of\nunsupervised segmentation methods for subword acquisition and different\nsubword-augmented strategies for text understanding, showing that\nsubword-augmented embedding significantly improves our baselines in various\ntypes of text understanding tasks on both English and Chinese benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:24:37 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 00:47:05 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""], ["Ling", "Kangwei", ""], ["Li", "Jiangtong", ""], ["Li", "Zuchao", ""], ["He", "Shexia", ""], ["Fu", "Guohong", ""]]}, {"id": "1811.02394", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Chen Liang, Lei Hou, Juanzi Li, Zhiyuan Liu, Hanwang Zhang", "title": "DeepChannel: Salience Estimation by Contrastive Learning for Extractive\n  Document Summarization", "comments": "AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepChannel, a robust, data-efficient, and interpretable neural\nmodel for extractive document summarization. Given any document-summary pair,\nwe estimate a salience score, which is modeled using an attention-based deep\nneural network, to represent the salience degree of the summary for yielding\nthe document. We devise a contrastive training strategy to learn the salience\nestimation network, and then use the learned salience score as a guide and\niteratively extract the most salient sentences from the document as our\ngenerated summary. In experiments, our model not only achieves state-of-the-art\nROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the\nout-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1\nF-1 score of 39.41 on CNN/Daily Mail test set with merely $1 / 100$ training\nset, demonstrating a tremendous data efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:06:44 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 02:11:01 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Shi", "Jiaxin", ""], ["Liang", "Chen", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""], ["Zhang", "Hanwang", ""]]}, {"id": "1811.02435", "submitter": "Advaith Siddharthan", "authors": "Advaith Siddharthan, Nicolas Cherbuin, Paul J. Eslinger, Kasia\n  Kozlowska, Nora A. Murphy, Leroy Lowe", "title": "WordNet-feelings: A linguistic categorisation of human feelings", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present the first in depth linguistic study of human\nfeelings. While there has been substantial research on incorporating some\naffective categories into linguistic analysis (e.g. sentiment, and to a lesser\nextent, emotion), the more diverse category of human feelings has thus far not\nbeen investigated. We surveyed the extensive interdisciplinary literature\naround feelings to construct a working definition of what constitutes a feeling\nand propose 9 broad categories of feeling. We identified potential feeling\nwords based on their pointwise mutual information with morphological variants\nof the word `feel' in the Google n-gram corpus, and present a manual annotation\nexercise where 317 WordNet senses of one hundred of these words were\ncategorised as `not a feeling' or as one of the 9 proposed categories of\nfeeling. We then proceeded to annotate 11386 WordNet senses of all these words\nto create WordNet-feelings, a new affective dataset that identifies 3664 word\nsenses as feelings, and associates each of these with one of the 9 categories\nof feeling. WordNet-feelings can be used in conjunction with other datasets\nsuch as SentiWordNet that annotate word senses with complementary affective\nproperties such as valence and intensity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:42:17 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Siddharthan", "Advaith", ""], ["Cherbuin", "Nicolas", ""], ["Eslinger", "Paul J.", ""], ["Kozlowska", "Kasia", ""], ["Murphy", "Nora A.", ""], ["Lowe", "Leroy", ""]]}, {"id": "1811.02456", "submitter": "Robert Martorano", "authors": "Robert Frank Martorano III", "title": "Semantic Term \"Blurring\" and Stochastic \"Barcoding\" for Improved\n  Unsupervised Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of text data being produced in the modern age makes it\nincreasingly important to intuitively group, categorize, or classify text data\nby theme for efficient retrieval and search. Yet, the high dimensionality and\nimprecision of text data, or more generally language as a whole, prove to be\nchallenging when attempting to perform unsupervised document clustering. In\nthis thesis, we present two novel methods for improving unsupervised document\nclustering/classification by theme. The first is to improve document\nrepresentations. We look to exploit \"term neighborhoods\" and \"blur\" semantic\nweight across neighboring terms. These neighborhoods are located in the\nsemantic space afforded by \"word embeddings.\" The second method is for cluster\nrevision, based on what we deem as \"stochastic barcoding\", or \"S- Barcode\"\npatterns. Text data is inherently high dimensional, yet clustering typically\ntakes place in a low dimensional representation space. Our method utilizes\nlower dimension clustering results as initial cluster configurations, and\niteratively revises the configuration in the high dimensional space. We show\nwith experimental results how both of the two methods improve the quality of\ndocument clustering. While this thesis elaborates on the two new conceptual\ncontributions, a joint thesis by David Yan details the feature transformation\nand software architecture we developed for unsupervised document\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:08:31 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Martorano", "Robert Frank", "III"]]}, {"id": "1811.02480", "submitter": "Giovanni Morrone", "authors": "Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi,\n  Luciano Fadiga, Leonardo Badino", "title": "Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement\n  in Multi-Talker Environments", "comments": "Proceedings of 2019 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682061", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of enhancing the speech of a speaker of\ninterest in a cocktail party scenario when visual information of the speaker of\ninterest is available. Contrary to most previous studies, we do not learn\nvisual features on the typically small audio-visual datasets, but use an\nalready available face landmark detector (trained on a separate image dataset).\nThe landmarks are used by LSTM-based models to generate time-frequency masks\nwhich are applied to the acoustic mixed-speech spectrogram. Results show that:\n(i) landmark motion features are very effective features for this task, (ii)\nsimilarly to previous work, reconstruction of the target speaker's spectrogram\nmediated by masking is significantly more accurate than direct spectrogram\nreconstruction, and (iii) the best masks depend on both motion landmark\nfeatures and the input mixed-speech spectrogram. To the best of our knowledge,\nour proposed models are the first models trained and evaluated on the limited\nsize GRID and TCD-TIMIT datasets, that achieve speaker-independent speech\nenhancement in a multi-talker setting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:35:01 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 10:40:59 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 16:27:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Morrone", "Giovanni", ""], ["Pasa", "Luca", ""], ["Tikhanoff", "Vadim", ""], ["Bergamaschi", "Sonia", ""], ["Fadiga", "Luciano", ""], ["Badino", "Leonardo", ""]]}, {"id": "1811.02507", "submitter": "Takashi Morita", "authors": "Takashi Morita, Hiroki Koda", "title": "Superregular grammars do not provide additional explanatory power but\n  allow for a compact analysis of animal song", "comments": "Accepted for publication by Royal Society Open Science", "journal-ref": null, "doi": "10.1098/rsos.190139", "report-no": null, "categories": "q-bio.NC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A pervasive belief with regard to the differences between human language and\nanimal vocal sequences (song) is that they belong to different classes of\ncomputational complexity, with animal song belonging to regular languages,\nwhereas human language is superregular. This argument, however, lacks empirical\nevidence since superregular analyses of animal song are understudied. The goal\nof this paper is to perform a superregular analysis of animal song, using data\nfrom gibbons as a case study, and demonstrate that a superregular analysis can\nbe effectively used with non-human data. A key finding is that a superregular\nanalysis does not increase explanatory power but rather provides for compact\nanalysis: Fewer grammatical rules are necessary once superregularity is\nallowed. This pattern is analogous to a previous computational analysis of\nhuman language, and accordingly, the null hypothesis, that human language and\nanimal song are governed by the same type of grammatical systems, cannot be\nrejected.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 05:07:37 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 12:06:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Morita", "Takashi", ""], ["Koda", "Hiroki", ""]]}, {"id": "1811.02510", "submitter": "Mikel Forcada Dr.", "authors": "Miquel Espl\\`a-Gomis, Felipe S\\'anchez-Mart\\'inez, Mikel L. Forcada", "title": "UAlacant machine translation quality estimation at WMT 2018: a simple\n  approach using phrase tables and feed-forward neural networks", "comments": "10 pages, 1 table, Proceedings of the 3rd Conference on Machine\n  Translation (WMT18), Brussels 31.10.2018--01.11.2018, pp. 814--821", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Universitat d'Alacant submissions to the word- and\nsentence-level machine translation (MT) quality estimation (QE) shared task at\nWMT 2018. Our approach to word-level MT QE builds on previous work to mark the\nwords in the machine-translated sentence as \\textit{OK} or \\textit{BAD}, and is\nextended to determine if a word or sequence of words need to be inserted in the\ngap after each word. Our sentence-level submission simply uses the edit\noperations predicted by the word-level approach to approximate TER. The method\npresented ranked first in the sub-task of identifying insertions in gaps for\nthree out of the six datasets, and second in the rest of them.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:25:21 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Espl\u00e0-Gomis", "Miquel", ""], ["S\u00e1nchez-Mart\u00ednez", "Felipe", ""], ["Forcada", "Mikel L.", ""]]}, {"id": "1811.02528", "submitter": "R\\'emi Francis", "authors": "R\\'emi Francis, Tom Ash, Will Williams", "title": "Discriminative training of RNNLMs with the average word error criterion", "comments": "Sumbitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic speech recognition (ASR), recurrent neural language models\n(RNNLM) are typically used to refine hypotheses in the form of lattices or\nn-best lists, which are generated by a beam search decoder with a weaker\nlanguage model. The RNNLMs are usually trained generatively using the\nperplexity (PPL) criterion on large corpora of grammatically correct text.\nHowever, the hypotheses are noisy, and the RNNLM doesn't always make the\nchoices that minimise the metric we optimise for, the word error rate (WER). To\naddress this mismatch we propose to use a task specific loss to train an RNNLM\nto discriminate between multiple hypotheses within lattice rescoring scenario.\nBy fine-tuning the RNNLM on lattices with the average edit distance loss, we\nshow that we obtain a 1.9% relative improvement in word error rate over a\npurely generatively trained model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:53:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 16:50:45 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Francis", "R\u00e9mi", ""], ["Ash", "Tom", ""], ["Williams", "Will", ""]]}, {"id": "1811.02549", "submitter": "Massimo Caccia", "authors": "Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle\n  Pineau, Laurent Charlin", "title": "Language GANs Falling Short", "comments": null, "journal-ref": "ICLR 2020 - Proceedings of the Seventh International Conference on\n  Learning Representation", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating high-quality text with sufficient diversity is essential for a\nwide range of Natural Language Generation (NLG) tasks. Maximum-Likelihood (MLE)\nmodels trained with teacher forcing have consistently been reported as weak\nbaselines, where poor performance is attributed to exposure bias (Bengio et\nal., 2015; Ranzato et al., 2015); at inference time, the model is fed its own\nprediction instead of a ground-truth token, which can lead to accumulating\nerrors and poor samples. This line of reasoning has led to an outbreak of\nadversarial based approaches for NLG, on the account that GANs do not suffer\nfrom exposure bias. In this work, we make several surprising observations which\ncontradict common beliefs. First, we revisit the canonical evaluation framework\nfor NLG, and point out fundamental flaws with quality-only evaluation: we show\nthat one can outperform such metrics using a simple, well-known temperature\nparameter to artificially reduce the entropy of the model's conditional\ndistributions. Second, we leverage the control over the quality / diversity\ntrade-off given by this parameter to evaluate models over the whole\nquality-diversity spectrum and find MLE models constantly outperform the\nproposed GAN variants over the whole quality-diversity space. Our results have\nseveral implications: 1) The impact of exposure bias on sample quality is less\nsevere than previously thought, 2) temperature tuning provides a better quality\n/ diversity trade-off than adversarial training while being easier to train,\neasier to cross-validate, and less computationally expensive. Code to reproduce\nthe experiments is available at github.com/pclucas14/GansFallingShort\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:44:11 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 05:29:16 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 04:40:20 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 19:55:38 GMT"}, {"version": "v5", "created": "Sat, 17 Aug 2019 00:01:05 GMT"}, {"version": "v6", "created": "Wed, 19 Feb 2020 22:44:37 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Caccia", "Massimo", ""], ["Caccia", "Lucas", ""], ["Fedus", "William", ""], ["Larochelle", "Hugo", ""], ["Pineau", "Joelle", ""], ["Charlin", "Laurent", ""]]}, {"id": "1811.02602", "submitter": "Sufeng Duan", "authors": "Sufeng Duan, Jiangtong Li, Hai Zhao", "title": "Fast Neural Chinese Word Segmentation for Long Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly developed neural models have achieved competitive performance in\nChinese word segmentation (CWS) as their traditional counterparts. However,\nmost of methods encounter the computational inefficiency especially for long\nsentences because of the increasing model complexity and slower decoders. This\npaper presents a simple neural segmenter which directly labels the gap\nexistence between adjacent characters to alleviate the existing drawback. Our\nsegmenter is fully end-to-end and capable of performing segmentation very fast.\nWe also show a performance difference with different tag sets. The experiments\nshow that our segmenter can provide comparable performance with\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:19:49 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 14:59:36 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Duan", "Sufeng", ""], ["Li", "Jiangtong", ""], ["Zhao", "Hai", ""]]}, {"id": "1811.02611", "submitter": "Luzi Sennhauser", "authors": "Luzi Sennhauser, Robert C. Berwick", "title": "Evaluating the Ability of LSTMs to Learn Context-Free Grammars", "comments": null, "journal-ref": "Proceedings of the EMNLP Workshop BlackboxNLP (2018) 115-124", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While long short-term memory (LSTM) neural net architectures are designed to\ncapture sequence information, human language is generally composed of\nhierarchical structures. This raises the question as to whether LSTMs can learn\nhierarchical structures. We explore this question with a well-formed bracket\nprediction task using two types of brackets modeled by an LSTM. Demonstrating\nthat such a system is learnable by an LSTM is the first step in demonstrating\nthat the entire class of CFLs is also learnable. We observe that the model\nrequires exponential memory in terms of the number of characters and embedded\ndepth, where a sub-linear memory should suffice. Still, the model does more\nthan memorize the training input. It learns how to distinguish between relevant\nand irrelevant information. On the other hand, we also observe that the model\ndoes not generalize well. We conclude that LSTMs do not learn the relevant\nunderlying context-free rules, suggesting the good overall performance is\nattained rather by an efficient way of evaluating nuisance variables. LSTMs are\na way to quickly reach good results for many natural language tasks, but to\nunderstand and generate natural language one has to investigate other concepts\nthat can make more direct use of natural language's structural nature.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:07:47 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Sennhauser", "Luzi", ""], ["Berwick", "Robert C.", ""]]}, {"id": "1811.02641", "submitter": "Matthew Maciejewski", "authors": "Matthew Maciejewski, Gregory Sell, Leibny Paola Garcia-Perera, Shinji\n  Watanabe, Sanjeev Khudanpur", "title": "Building Corpora for Single-Channel Speech Separation Across Multiple\n  Domains", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, the bulk of research on single-channel speech separation has been\nconducted using clean, near-field, read speech, which is not representative of\nmany modern applications. In this work, we develop a procedure for constructing\nhigh-quality synthetic overlap datasets, necessary for most deep learning-based\nseparation frameworks. We produced datasets that are more representative of\nrealistic applications using the CHiME-5 and Mixer 6 corpora and evaluate\nstandard methods on this data to demonstrate the shortcomings of current\nsource-separation performance. We also demonstrate the value of a wide variety\nof data in training robust models that generalize well to multiple conditions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:59:54 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Maciejewski", "Matthew", ""], ["Sell", "Gregory", ""], ["Garcia-Perera", "Leibny Paola", ""], ["Watanabe", "Shinji", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1811.02701", "submitter": "EPTCS", "authors": "Martha Lewis, Bob Coecke, Jules Hedges, Dimitri Kartsaklis, Dan\n  Marsden", "title": "Proceedings of the 2018 Workshop on Compositional Approaches in Physics,\n  NLP, and Social Sciences", "comments": null, "journal-ref": "EPTCS 283, 2018", "doi": "10.4204/EPTCS.283", "report-no": null, "categories": "cs.CL cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to compose parts to form a more complex whole, and to analyze a\nwhole as a combination of elements, is desirable across disciplines. This\nworkshop bring together researchers applying compositional approaches to\nphysics, NLP, cognitive science, and game theory. Within NLP, a long-standing\naim is to represent how words can combine to form phrases and sentences. Within\nthe framework of distributional semantics, words are represented as vectors in\nvector spaces. The categorical model of Coecke et al. [2010], inspired by\nquantum protocols, has provided a convincing account of compositionality in\nvector space models of NLP. There is furthermore a history of vector space\nmodels in cognitive science. Theories of categorization such as those developed\nby Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between\nfeature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model\nof concepts in which conceptual spaces provide geometric structures, and\ninformation is represented by points, vectors and regions in vector spaces. The\nsame compositional approach has been applied to this formalism, giving\nconceptual spaces theory a richer model of compositionality than previously\n[Bolt et al., 2018]. Compositional approaches have also been applied in the\nstudy of strategic games and Nash equilibria. In contrast to classical game\ntheory, where games are studied monolithically as one global object,\ncompositional game theory works bottom-up by building large and complex games\nfrom smaller components. Such an approach is inherently difficult since the\ninteraction between games has to be considered. Research into categorical\ncompositional methods for this field have recently begun [Ghani et al., 2018].\nMoreover, the interaction between the three disciplines of cognitive science,\nlinguistics and game theory is a fertile ground for research. Game theory in\ncognitive science is a well-established area [Camerer, 2011]. Similarly game\ntheoretic approaches have been applied in linguistics [J\\\"ager, 2008]. Lastly,\nthe study of linguistics and cognitive science is intimately intertwined\n[Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies\ncompositional approaches via vector spaces and categorical quantum theory,\nallowing the interplay between the three disciplines to be examined.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 23:25:45 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Lewis", "Martha", ""], ["Coecke", "Bob", ""], ["Hedges", "Jules", ""], ["Kartsaklis", "Dimitri", ""], ["Marsden", "Dan", ""]]}, {"id": "1811.02714", "submitter": "Nicolas Angelard-Gontier", "authors": "Nicolas Gontier, Koustuv Sinha, Peter Henderson, Iulian Serban,\n  Michael Noseworthy, Prasanna Parthasarathi, Joelle Pineau", "title": "The RLLChatbot: a solution to the ConvAI challenge", "comments": "46 pages including references and appendix, 14 figures, 12 tables;\n  Under review for the Dialogue & Discourse journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current conversational systems can follow simple commands and answer basic\nquestions, but they have difficulty maintaining coherent and open-ended\nconversations about specific topics. Competitions like the Conversational\nIntelligence (ConvAI) challenge are being organized to push the research\ndevelopment towards that goal. This article presents in detail the RLLChatbot\nthat participated in the 2017 ConvAI challenge. The goal of this research is to\nbetter understand how current deep learning and reinforcement learning tools\ncan be used to build a robust yet flexible open domain conversational agent. We\nprovide a thorough description of how a dialog system can be built and trained\nfrom mostly public-domain datasets using an ensemble model. The first\ncontribution of this work is a detailed description and analysis of different\ntext generation models in addition to novel message ranking and selection\nmethods. Moreover, a new open-source conversational dataset is presented.\nTraining on this data significantly improves the Recall@k score of the ranking\nand selection mechanisms compared to our baseline model responsible for\nselecting the message returned at each interaction.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 01:19:05 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 14:33:41 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Gontier", "Nicolas", ""], ["Sinha", "Koustuv", ""], ["Henderson", "Peter", ""], ["Serban", "Iulian", ""], ["Noseworthy", "Michael", ""], ["Parthasarathi", "Prasanna", ""], ["Pineau", "Joelle", ""]]}, {"id": "1811.02735", "submitter": "Nelson Yalta", "authors": "Nelson Yalta, Shinji Watanabe, Takaaki Hori, Kazuhiro Nakadai, Tetsuya\n  Ogata", "title": "CNN-based MultiChannel End-to-End Speech Recognition for everyday home\n  environments", "comments": "5 pages, 1 figure, EUSIPCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casual conversations involving multiple speakers and noises from surrounding\ndevices are common in everyday environments, which degrades the performances of\nautomatic speech recognition systems. These challenging characteristics of\nenvironments are the target of the CHiME-5 challenge. By employing a\nconvolutional neural network (CNN)-based multichannel end-to-end speech\nrecognition system, this study attempts to overcome the presents difficulties\nin everyday environments. The system comprises of an attention-based\nencoder-decoder neural network that directly generates a text as an output from\na sound input. The multichannel CNN encoder, which uses residual connections\nand batch renormalization, is trained with augmented data, including white\nnoise injection. The experimental results show that the word error rate is\nreduced by 8.5% and 0.6% absolute from a single channel end-to-end and the best\nbaseline (LF-MMI TDNN) on the CHiME-5 corpus, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 02:38:13 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 12:36:00 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 00:52:53 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Yalta", "Nelson", ""], ["Watanabe", "Shinji", ""], ["Hori", "Takaaki", ""], ["Nakadai", "Kazuhiro", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "1811.02736", "submitter": "Hyungjun Lim", "authors": "Hyungjun Lim, Younggwan Kim, Youngmoon Jung, Myunghun Jung, and Hoirin\n  Kim", "title": "Learning acoustic word embeddings with phonetically associated triplet\n  network", "comments": "5 pages, 4 figures, submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous researches on acoustic word embeddings used in query-by-example\nspoken term detection have shown remarkable performance improvements when using\na triplet network. However, the triplet network is trained using only a limited\ninformation about acoustic similarity between words. In this paper, we propose\na novel architecture, phonetically associated triplet network (PATN), which\naims at increasing discriminative power of acoustic word embeddings by\nutilizing phonetic information as well as word identity. The proposed model is\nlearned to minimize a combined loss function that was made by introducing a\ncross entropy loss to the lower layer of LSTM-based triplet network. We\nobserved that the proposed method performs significantly better than the\nbaseline triplet network on a word discrimination task with the WSJ dataset\nresulting in over 20% relative improvement in recall rate at 1.0 false alarm\nper hour. Finally, we examined the generalization ability by conducting the\nout-of-domain test on the RM dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 02:38:49 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 05:25:53 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 01:35:25 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Lim", "Hyungjun", ""], ["Kim", "Younggwan", ""], ["Jung", "Youngmoon", ""], ["Jung", "Myunghun", ""], ["Kim", "Hoirin", ""]]}, {"id": "1811.02750", "submitter": "Bridianne O'Dea", "authors": "B. ODea, T.W. Boonstra, M.E. Larsen, T. Nguyen, S. Venkatesh, H.\n  Christensen", "title": "The relationship between linguistic expression and symptoms of\n  depression, anxiety, and suicidal thoughts: A longitudinal study of blog\n  content", "comments": "29 pages, 6 figures", "journal-ref": "PLoS ONE 16(5): e0251787, 2021", "doi": "10.1371/journal.pone.0251787", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its popularity and availability, social media data may present a new\nway to identify individuals who are experiencing mental illness. By analysing\nblog content, this study aimed to investigate the associations between\nlinguistic features and symptoms of depression, generalised anxiety, and\nsuicidal ideation. This study utilised a longitudinal study design. Individuals\nwho blogged were invited to participate in a study in which they completed\nfortnightly mental health questionnaires including the PHQ9 and GAD7 for a\nperiod of 36 weeks. Linguistic features were extracted from blog data using the\nLIWC tool. Bivariate and multivariate analyses were performed to investigate\nthe correlations between the linguistic features and mental health scores\nbetween subjects. We then used the multivariate regression model to predict\nlongitudinal changes in mood within subjects. A total of 153 participants\nconsented to taking part, with 38 participants completing the required number\nof questionnaires and blog posts during the study period. Between-subject\nanalysis revealed that several linguistic features, including tentativeness and\nnon-fluencies, were significantly associated with depression and anxiety\nsymptoms, but not suicidal thoughts. Within-subject analysis showed no robust\ncorrelations between linguistic features and changes in mental health score.\nThis study provides further support for the relationship between linguistic\nfeatures within social media data and symptoms of depression and anxiety. The\nlack of robust within-subject correlations indicate that the relationship\nobserved at the group level may not generalise to individual changes over time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:11:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["ODea", "B.", ""], ["Boonstra", "T. W.", ""], ["Larsen", "M. E.", ""], ["Nguyen", "T.", ""], ["Venkatesh", "S.", ""], ["Christensen", "H.", ""]]}, {"id": "1811.02765", "submitter": "Xin Wang", "authors": "Xin Wang, Jiawei Wu, Da Zhang, Yu Su, William Yang Wang", "title": "Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video\n  Captioning", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although promising results have been achieved in video captioning, existing\nmodels are limited to the fixed inventory of activities in the training corpus,\nand do not generalize to open vocabulary scenarios. Here we introduce a novel\ntask, zero-shot video captioning, that aims at describing out-of-domain videos\nof unseen activities. Videos of different activities usually require different\ncaptioning strategies in many aspects, i.e. word selection, semantic\nconstruction, and style expression etc, which poses a great challenge to depict\nnovel activities without paired training data. But meanwhile, similar\nactivities share some of those aspects in common. Therefore, We propose a\nprincipled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video\ncaptioning, which learns to compose different experts based on different topic\nembeddings, implicitly transferring the knowledge learned from seen activities\nto unseen ones. Besides, we leverage external topic-related text corpus to\nconstruct the topic embedding for each activity, which embodies the most\nrelevant semantic vectors within the topic. Empirical results not only validate\nthe effectiveness of our method in utilizing semantic knowledge for video\ncaptioning, but also show its strong generalization ability when describing\nnovel activities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:33:07 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 23:22:19 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Xin", ""], ["Wu", "Jiawei", ""], ["Zhang", "Da", ""], ["Su", "Yu", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.02770", "submitter": "Murali Karthick Baskar", "authors": "Murali Karthick Baskar, Luk\\'a\\v{s} Burget, Shinji Watanabe, Martin\n  Karafi\\'at, Takaaki Hori, Jan Honza \\v{C}ernock\\'y", "title": "Promising Accurate Prefix Boosting for sequence-to-sequence ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present promising accurate prefix boosting (PAPB), a\ndiscriminative training technique for attention based sequence-to-sequence\n(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an\neffective manner. The training procedure involves maximizing the score of each\npartial correct sequence obtained during beam search compared to other\nhypotheses. The training objective also includes minimization of token\n(character) error rate. PAPB shows its efficacy by achieving 10.8\\% and 3.8\\%\nWER with and without RNNLM respectively on Wall Street Journal dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:53:21 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Baskar", "Murali Karthick", ""], ["Burget", "Luk\u00e1\u0161", ""], ["Watanabe", "Shinji", ""], ["Karafi\u00e1t", "Martin", ""], ["Hori", "Takaaki", ""], ["\u010cernock\u00fd", "Jan Honza", ""]]}, {"id": "1811.02775", "submitter": "Sung-Feng Huang", "authors": "Sung-Feng Huang, Yi-Chen Chen, Hung-yi Lee, Lin-shan Lee", "title": "Improved Audio Embeddings by Adjacency-Based Clustering with\n  Applications in Spoken Term Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding audio signal segments into vectors with fixed dimensionality is\nattractive because all following processing will be easier and more efficient,\nfor example modeling, classifying or indexing. Audio Word2Vec previously\nproposed was shown to be able to represent audio segments for spoken words as\nsuch vectors carrying information about the phonetic structures of the signal\nsegments. However, each linguistic unit (word, syllable, phoneme in text form)\ncorresponds to unlimited number of audio segments with vector representations\ninevitably spread over the embedding space, which causes some confusion. It is\ntherefore desired to better cluster the audio embeddings such that those\ncorresponding to the same linguistic unit can be more compactly distributed. In\nthis paper, inspired by Siamese networks, we propose some approaches to achieve\nthe above goal. This includes identifying positive and negative pairs from\nunlabeled data for Siamese style training, disentangling acoustic factors such\nas speaker characteristics from the audio embedding, handling unbalanced data\ndistribution, and having the embedding processes learn from the adjacency\nrelationships among data points. All these can be done in an unsupervised way.\nImproved performance was obtained in preliminary experiments on the LibriSpeech\ndata set, including clustering characteristics analysis and applications of\nspoken term detection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 06:18:14 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Huang", "Sung-Feng", ""], ["Chen", "Yi-Chen", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1811.02902", "submitter": "Gregor Wiedemann", "authors": "Gregor Wiedemann, Raghav Jindal, Chris Biemann", "title": "microNER: A Micro-Service for German Named Entity Recognition based on\n  BiLSTM-CRF", "comments": "7 pages, 1 figure", "journal-ref": "Proceedings of the 14th Conference on Natural Language Processing\n  / Konferenz zur Verarbeitung nat\\\"urlicher Sprache (KONVENS 2018)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For named entity recognition (NER), bidirectional recurrent neural networks\nbecame the state-of-the-art technology in recent years. Competing approaches\nvary with respect to pre-trained word embeddings as well as models for\ncharacter embeddings to represent sequence information most effectively. For\nNER in German language texts, these model variations have not been studied\nextensively. We evaluate the performance of different word and character\nembeddings on two standard German datasets and with a special focus on\nout-of-vocabulary words. With F-Scores above 82% for the GermEval'14 dataset\nand above 85% for the CoNLL'03 dataset, we achieve (near) state-of-the-art\nperformance for this task. We publish several pre-trained models wrapped into a\nmicro-service based on Docker to allow for easy integration of German NER into\nother applications via a JSON API.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 14:31:13 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Wiedemann", "Gregor", ""], ["Jindal", "Raghav", ""], ["Biemann", "Chris", ""]]}, {"id": "1811.02906", "submitter": "Gregor Wiedemann", "authors": "Gregor Wiedemann, Eugen Ruppert, Raghav Jindal, Chris Biemann", "title": "Transfer Learning from LDA to BiLSTM-CNN for Offensive Language\n  Detection in Twitter", "comments": "10 pages, 1 figure", "journal-ref": "Proceedings of GermEval 2018, 14th Conference on Natural Language\n  Processing (KONVENS 2018)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different strategies for automatic offensive language\nclassification on German Twitter data. For this, we employ a sequentially\ncombined BiLSTM-CNN neural network. Based on this model, three transfer\nlearning tasks to improve the classification performance with background\nknowledge are tested. We compare 1. Supervised category transfer: social media\ndata annotated with near-offensive language categories, 2. Weakly-supervised\ncategory transfer: tweets annotated with emojis they contain, 3. Unsupervised\ncategory transfer: tweets annotated with topic clusters obtained by Latent\nDirichlet Allocation (LDA). Further, we investigate the effect of three\ndifferent strategies to mitigate negative effects of 'catastrophic forgetting'\nduring transfer learning. Our results indicate that transfer learning in\ngeneral improves offensive language detection. Best results are achieved from\npre-training our model on the unsupervised topic clustering of tweets in\ncombination with thematic user cluster information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 14:40:10 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Wiedemann", "Gregor", ""], ["Ruppert", "Eugen", ""], ["Jindal", "Raghav", ""], ["Biemann", "Chris", ""]]}, {"id": "1811.02959", "submitter": "Koustuv Sinha", "authors": "Koustuv Sinha, Shagun Sodhani, William L. Hamilton and Joelle Pineau", "title": "Compositional Language Understanding with Text-based Relational\n  Reasoning", "comments": "4 pages of main content, to be presented at Relational Representation\n  Learning Workshop, NIPS 2018, Montreal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks for natural language reasoning have largely focused on\nextractive, fact-based question-answering (QA) and common-sense inference.\nHowever, it is also crucial to understand the extent to which neural networks\ncan perform relational reasoning and combinatorial generalization from natural\nlanguage---abilities that are often obscured by annotation artifacts and the\ndominance of language modeling in standard QA benchmarks. In this work, we\npresent a novel benchmark dataset for language understanding that isolates\nperformance on relational reasoning. We also present a neural message-passing\nbaseline and show that this model, which incorporates a relational inductive\nbias, is superior at combinatorial generalization compared to a traditional\nrecurrent neural network approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:17:48 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:32:05 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Sinha", "Koustuv", ""], ["Sodhani", "Shagun", ""], ["Hamilton", "William L.", ""], ["Pineau", "Joelle", ""]]}, {"id": "1811.03036", "submitter": "Agnieszka Fale\\'nska", "authors": "Agnieszka Falenska, Anders Bj\\\"orkelund, Xiang Yu, Jonas Kuhn", "title": "IMS at the PolEval 2018: A Bulky Ensemble Depedency Parser meets 12\n  Simple Rules for Predicting Enhanced Dependencies in Polish", "comments": null, "journal-ref": "Proceedings of the PolEval 2018 Workshop, 2018, 25-39", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the IMS contribution to the PolEval 2018 Shared Task. We\nsubmitted systems for both of the Subtasks of Task 1. In Subtask (A), which was\nabout dependency parsing, we used our ensemble system from the CoNLL 2017 UD\nShared Task. The system first preprocesses the sentences with a CRF\nPOS/morphological tagger and predicts supertags with a neural tagger. Then, it\nemploys multiple instances of three different parsers and merges their outputs\nby applying blending. The system achieved the second place out of four\nparticipating teams. In this paper we show which components of the system were\nthe most responsible for its final performance.\n  The goal of Subtask (B) was to predict enhanced graphs. Our approach\nconsisted of two steps: parsing the sentences with our ensemble system from\nSubtask (A), and applying 12 simple rules to obtain the final dependency\ngraphs. The rules introduce additional enhanced arcs only for tokens with\n\"conj\" heads (conjuncts). They do not predict semantic relations at all. The\nsystem ranked first out of three participating teams. In this paper we show\nexamples of rules we designed and analyze the relation between the quality of\nautomatically parsed trees and the accuracy of the enhanced graphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:40:03 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Falenska", "Agnieszka", ""], ["Bj\u00f6rkelund", "Anders", ""], ["Yu", "Xiang", ""], ["Kuhn", "Jonas", ""]]}, {"id": "1811.03039", "submitter": "Alberto Poncelas", "authors": "Alberto Poncelas, Gideon Maillette de Buy Wenniger, Andy Way", "title": "Data Selection with Feature Decay Algorithms Using an Approximated\n  Target Side", "comments": null, "journal-ref": "Proceedings of the 15th International Workshop on Spoken Language\n  Translation (2018) 173-180", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data selection techniques applied to neural machine translation (NMT) aim to\nincrease the performance of a model by retrieving a subset of sentences for use\nas training data.\n  One of the possible data selection techniques are transductive learning\nmethods, which select the data based on the test set, i.e. the document to be\ntranslated. A limitation of these methods to date is that using the source-side\ntest set does not by itself guarantee that sentences are selected with correct\ntranslations, or translations that are suitable given the test-set domain. Some\ncorpora, such as subtitle corpora, may contain parallel sentences with\ninaccurate translations caused by localization or length restrictions.\n  In order to try to fix this problem, in this paper we propose to use an\napproximated target-side in addition to the source-side when selecting suitable\nsentence-pairs for training a model. This approximated target-side is built by\npre-translating the source-side.\n  In this work, we explore the performance of this general idea for one\nspecific data selection approach called Feature Decay Algorithms (FDA).\n  We train German-English NMT models on data selected by using the test set\n(source), the approximated target side, and a mixture of both. Our findings\nreveal that models built using a combination of outputs of FDA (using the test\nset and an approximated target side) perform better than those solely using the\ntest set. We obtain a statistically significant improvement of more than 1.5\nBLEU points over a model trained with all data, and more than 0.5 BLEU points\nover a strong FDA baseline that uses source-side information only.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:50:05 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Poncelas", "Alberto", ""], ["Wenniger", "Gideon Maillette de Buy", ""], ["Way", "Andy", ""]]}, {"id": "1811.03115", "submitter": "Mitchell Stern", "authors": "Mitchell Stern, Noam Shazeer, Jakob Uszkoreit", "title": "Blockwise Parallel Decoding for Deep Autoregressive Models", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive sequence-to-sequence models have demonstrated impressive\nperformance across a wide variety of tasks in recent years. While common\narchitecture classes such as recurrent, convolutional, and self-attention\nnetworks make different trade-offs between the amount of computation needed per\nlayer and the length of the critical path at training time, generation still\nremains an inherently sequential process. To overcome this limitation, we\npropose a novel blockwise parallel decoding scheme in which we make predictions\nfor multiple time steps in parallel then back off to the longest prefix\nvalidated by a scoring model. This allows for substantial theoretical\nimprovements in generation speed when applied to architectures that can process\noutput sequences in parallel. We verify our approach empirically through a\nseries of experiments using state-of-the-art self-attention models for machine\ntranslation and image super-resolution, achieving iteration reductions of up to\n2x over a baseline greedy decoder with no loss in quality, or up to 7x in\nexchange for a slight decrease in performance. In terms of wall-clock time, our\nfastest models exhibit real-time speedups of up to 4x over standard greedy\ndecoding.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 19:09:40 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Stern", "Mitchell", ""], ["Shazeer", "Noam", ""], ["Uszkoreit", "Jakob", ""]]}, {"id": "1811.03169", "submitter": "Stephane Fotso", "authors": "Stephane Fotso, Philip Spanoudes, Benjamin C. Ponedel, Brian Reynoso,\n  Janet Ko", "title": "Attention Fusion Networks: Combining Behavior and E-mail Content to\n  Improve Customer Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer support is a central objective at Square as it helps us build and\nmaintain great relationships with our sellers. In order to provide the best\nexperience, we strive to deliver the most accurate and quasi-instantaneous\nresponses to questions regarding our products.\n  In this work, we introduce the Attention Fusion Network model which combines\nsignals extracted from seller interactions on the Square product ecosystem,\nalong with submitted email questions, to predict the most relevant solution to\na seller's inquiry. We show that the innovative combination of two very\ndifferent data sources that are rarely used together, using state-of-the-art\ndeep learning systems outperforms, candidate models that are trained only on a\nsingle source.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 22:14:32 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 22:32:10 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Fotso", "Stephane", ""], ["Spanoudes", "Philip", ""], ["Ponedel", "Benjamin C.", ""], ["Reynoso", "Brian", ""], ["Ko", "Janet", ""]]}, {"id": "1811.03189", "submitter": "Elizabeth Salesky", "authors": "Elizabeth Salesky, Susanne Burger, Jan Niehues, and Alex Waibel", "title": "Towards Fluent Translations from Disfluent Speech", "comments": "To appear at SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When translating from speech, special consideration for conversational speech\nphenomena such as disfluencies is necessary. Most machine translation training\ndata consists of well-formed written texts, causing issues when translating\nspontaneous speech. Previous work has introduced an intermediate step between\nspeech recognition (ASR) and machine translation (MT) to remove disfluencies,\nmaking the data better-matched to typical translation text and significantly\nimproving performance. However, with the rise of end-to-end speech translation\nsystems, this intermediate step must be incorporated into the\nsequence-to-sequence architecture. Further, though translated speech datasets\nexist, they are typically news or rehearsed speech without many disfluencies\n(e.g. TED), or the disfluencies are translated into the references (e.g.\nFisher). To generate clean translations from disfluent speech, cleaned\nreferences are necessary for evaluation. We introduce a corpus of cleaned\ntarget data for the Fisher Spanish-English dataset for this task. We compare\nhow different architectures handle disfluencies and provide a baseline for\nremoving disfluencies in end-to-end translation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:47:01 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Salesky", "Elizabeth", ""], ["Burger", "Susanne", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1811.03199", "submitter": "Panayiotis Georgiou", "authors": "Prashanth Gurunath Shivakumar and Panayiotis Georgiou", "title": "Confusion2Vec: Towards Enriching Vector Space Word Representations with\n  Representational Ambiguities", "comments": null, "journal-ref": "PeerJ Computer Science 5:e195, 2019", "doi": "10.7717/peerj-cs.195", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word vector representations are a crucial part of Natural Language Processing\n(NLP) and Human Computer Interaction. In this paper, we propose a novel word\nvector representation, Confusion2Vec, motivated from the human speech\nproduction and perception that encodes representational ambiguity. Humans\nemploy both acoustic similarity cues and contextual cues to decode information\nand we focus on a model that incorporates both sources of information. The\nrepresentational ambiguity of acoustics, which manifests itself in word\nconfusions, is often resolved by both humans and machines through contextual\ncues. A range of representational ambiguities can emerge in various domains\nfurther to acoustic perception, such as morphological transformations,\nparaphrasing for NLP tasks like machine translation etc. In this work, we\npresent a case study in application to Automatic Speech Recognition (ASR),\nwhere the word confusions are related to acoustic similarity. We present\nseveral techniques to train an acoustic perceptual similarity representation\nambiguity. We term this Confusion2Vec and learn on unsupervised-generated data\nfrom ASR confusion networks or lattice-like structures. Appropriate evaluations\nfor the Confusion2Vec are formulated for gauging acoustic similarity in\naddition to semantic-syntactic and word similarity evaluations. The\nConfusion2Vec is able to model word confusions efficiently, without\ncompromising on the semantic-syntactic word relations, thus effectively\nenriching the word vector space with extra task relevant ambiguity information.\nWe provide an intuitive exploration of the 2-dimensional Confusion2Vec space\nusing Principal Component Analysis of the embedding and relate to semantic,\nsyntactic and acoustic relationships. The potential of Confusion2Vec in the\nutilization of uncertainty present in lattices is demonstrated through small\nexamples relating to ASR error correction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 00:40:25 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 23:01:50 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Shivakumar", "Prashanth Gurunath", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "1811.03245", "submitter": "Roger Granada", "authors": "Roger Granada, Renata Vieira, Cassia Trojahn and Nathalie\n  Aussenac-Gilles", "title": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods\n  Across Different Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information systems are changing the idea of \"data processing\" to the\nidea of \"concept processing\", meaning that instead of processing words, such\nsystems process semantic concepts which carry meaning and share contexts with\nother concepts. Ontology is commonly used as a structure that captures the\nknowledge about a certain area via providing concepts and relations between\nthem. Traditionally, concept hierarchies have been built manually by knowledge\nengineers or domain experts. However, the manual construction of a concept\nhierarchy suffers from several limitations such as its coverage and the\nenormous costs of its extension and maintenance. Ontology learning, usually\nreferred to the (semi-)automatic support in ontology development, is usually\ndivided into steps, going from concepts identification, passing through\nhierarchy and non-hierarchy relations detection and, seldom, axiom extraction.\nIt is reasonable to say that among these steps the current frontier is in the\nestablishment of concept hierarchies, since this is the backbone of ontologies\nand, therefore, a good concept hierarchy is already a valuable resource for\nmany ontology applications. The automatic construction of concept hierarchies\nfrom texts is a complex task and much work have been proposing approaches to\nbetter extract relations between concepts. These different proposals have never\nbeen contrasted against each other on the same set of data and across different\nlanguages. Such comparison is important to see whether they are complementary\nor incremental. Also, we can see whether they present different tendencies\ntowards recall and precision. This paper evaluates these different methods on\nthe basis of hierarchy metrics such as density and depth, and evaluation\nmetrics such as Recall and Precision. Results shed light over the comprehensive\nset of methods according to the literature in the area.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:19:59 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Granada", "Roger", ""], ["Vieira", "Renata", ""], ["Trojahn", "Cassia", ""], ["Aussenac-Gilles", "Nathalie", ""]]}, {"id": "1811.03255", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Zhiyuan Tang, Ying Shi, Dong Wang", "title": "Phonetic-attention scoring for deep speaker features in speaker\n  verification", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that frame-level deep speaker features can be\nderived from a deep neural network with the training target set to discriminate\nspeakers by a short speech segment. By pooling the frame-level features,\nutterance-level representations, called d-vectors, can be derived and used in\nthe automatic speaker verification (ASV) task. This simple average pooling,\nhowever, is inherently sensitive to the phonetic content of the utterance. An\ninteresting idea borrowed from machine translation is the attention-based\nmechanism, where the contribution of an input word to the translation at a\nparticular time is weighted by an attention score. This score reflects the\nrelevance of the input word and the present translation. We can use the same\nidea to align utterances with different phonetic contents. This paper proposes\na phonetic-attention scoring approach for d-vector systems. By this approach,\nan attention score is computed for each frame pair. This score reflects the\nsimilarity of the two frames in phonetic content, and is used to weigh the\ncontribution of this frame pair in the utterance-based scoring. This new\nscoring approach emphasizes the frame pairs with similar phonetic contents,\nwhich essentially provides a soft alignment for utterances with any phonetic\ncontents. Experimental results show that compared with the naive average\npooling, this phonetic-attention scoring approach can deliver consistent\nperformance improvement in ASV tasks of both text-dependent and\ntext-independent.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:54:55 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Li", "Lantian", ""], ["Tang", "Zhiyuan", ""], ["Shi", "Ying", ""], ["Wang", "Dong", ""]]}, {"id": "1811.03258", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Zhiyuan Tang, Ying Shi, Dong Wang", "title": "Gaussian-Constrained training for speaker verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models, in particular the d-vector and x-vector architectures, have\nproduced state-of-the-art performance on many speaker verification tasks.\nHowever, two potential problems of these neural models deserve more\ninvestigation. Firstly, both models suffer from `information leak', which means\nthat some parameters participating in model training will be discarded during\ninference, i.e, the layers that are used as the classifier. Secondly, these\nmodels do not regulate the distribution of the derived speaker vectors. This\n`unconstrained distribution' may degrade the performance of the subsequent\nscoring component, e.g., PLDA. This paper proposes a Gaussian-constrained\ntraining approach that (1) discards the parametric classifier, and (2) enforces\nthe distribution of the derived speaker vectors to be Gaussian. Our experiments\non the VoxCeleb and SITW databases demonstrated that this new training approach\nproduced more representative and regular speaker embeddings, leading to\nconsistent performance improvement.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 04:03:09 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 12:58:11 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Li", "Lantian", ""], ["Tang", "Zhiyuan", ""], ["Shi", "Ying", ""], ["Wang", "Dong", ""]]}, {"id": "1811.03273", "submitter": "EPTCS", "authors": "Peter M. Hines (University of York)", "title": "Information Flow in Pregroup Models of Natural Language", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 13-27", "doi": "10.4204/EPTCS.283.2", "report-no": null, "categories": "cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about pregroup models of natural languages, and how they relate\nto the explicitly categorical use of pregroups in Compositional Distributional\nSemantics and Natural Language Processing. These categorical interpretations\nmake certain assumptions about the nature of natural languages that, when\nstated formally, may be seen to impose strong restrictions on pregroup grammars\nfor natural languages.\n  We formalize this as a hypothesis about the form that pregroup models of\nnatural languages must take, and demonstrate by an artificial language example\nthat these restrictions are not imposed by the pregroup axioms themselves. We\ncompare and contrast the artificial language examples with natural languages\n(using Welsh, a language where the 'noun' type cannot be taken as primitive, as\nan illustrative example).\n  The hypothesis is simply that there must exist a causal connection, or\ninformation flow, between the words of a sentence in a language whose purpose\nis to communicate information. This is not necessarily the case with formal\nlanguages that are simply generated by a series of 'meaning-free' rules. This\nimposes restrictions on the types of pregroup grammars that we expect to find\nin natural languages; we formalize this in algebraic, categorical, and\ngraphical terms.\n  We take some preliminary steps in providing conditions that ensure pregroup\nmodels satisfy these conjectured properties, and discuss the more general forms\nthis hypothesis may take.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:10:34 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Hines", "Peter M.", "", "University of York"]]}, {"id": "1811.03274", "submitter": "EPTCS", "authors": "Brian Tyrrell (University of Oxford)", "title": "Applying Distributional Compositional Categorical Models of Meaning to\n  Language Translation", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 28-49", "doi": "10.4204/EPTCS.283.3", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is twofold: first we will use vector space\ndistributional compositional categorical models of meaning to compare the\nmeaning of sentences in Irish and in English (and thus ascertain when a\nsentence is the translation of another sentence) using the cosine similarity\nscore. Then we shall outline a procedure which translates nouns by\nunderstanding their context, using a conceptual space model of cognition. We\nshall use metrics on the category ConvexRel to determine the distance between\nconcepts (and determine when a noun is the translation of another noun). This\npaper will focus on applications to Irish, a member of the Gaelic family of\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:10:51 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Tyrrell", "Brian", "", "University of Oxford"]]}, {"id": "1811.03275", "submitter": "EPTCS", "authors": "Francesco Galofaro (POLIMI, UNIBZ), Zeno Toffano (Centralesup\\'elec),\n  Bich-Li\\^en Doan (Centralesup\\'elec)", "title": "Quantum Semantic Correlations in Hate and Non-Hate Speeches", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 62-74", "doi": "10.4204/EPTCS.283.5", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to apply the notions of quantum geometry and correlation to\nthe typification of semantic relations between couples of keywords in different\ndocuments. In particular we analysed texts classified as hate / non hate\nspeeches, containing the keywords \"women\", \"white\", and \"black\". The paper\ncompares this approach to cosine similarity, a classical methodology, to cast\nlight on the notion of \"similar meaning\".\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:11:37 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Galofaro", "Francesco", "", "POLIMI, UNIBZ"], ["Toffano", "Zeno", "", "Centralesup\u00e9lec"], ["Doan", "Bich-Li\u00ean", "", "Centralesup\u00e9lec"]]}, {"id": "1811.03276", "submitter": "EPTCS", "authors": "Gijs Wijnholds (Queen Mary University of London), Mehrnoosh Sadrzadeh\n  (Queen Mary University of London)", "title": "Classical Copying versus Quantum Entanglement in Natural Language: The\n  Case of VP-ellipsis", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 103-119", "doi": "10.4204/EPTCS.283.8", "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares classical copying and quantum entanglement in natural\nlanguage by considering the case of verb phrase (VP) ellipsis. VP ellipsis is a\nnon-linear linguistic phenomenon that requires the reuse of resources, making\nit the ideal test case for a comparative study of different copying behaviours\nin compositional models of natural language. Following the line of research in\ncompositional distributional semantics set out by (Coecke et al., 2010) we\ndevelop an extension of the Lambek calculus which admits a controlled form of\ncontraction to deal with the copying of linguistic resources. We then develop\ntwo different compositional models of distributional meaning for this calculus.\nIn the first model, we follow the categorical approach of (Coecke et al., 2013)\nin which a functorial passage sends the proofs of the grammar to linear maps on\nvector spaces and we use Frobenius algebras to allow for copying. In the second\ncase, we follow the more traditional approach that one finds in categorial\ngrammars, whereby an intermediate step interprets proofs as non-linear lambda\nterms, using multiple variable occurrences that model classical copying. As a\ncase study, we apply the models to derive different readings of ambiguous\nelliptical phrases and compare the analyses that each model provides.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:12:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Wijnholds", "Gijs", "", "Queen Mary University of London"], ["Sadrzadeh", "Mehrnoosh", "", "Queen Mary University of London"]]}, {"id": "1811.03277", "submitter": "EPTCS", "authors": "Bob Coecke (University of Oxford), Giovanni de Felice (University of\n  Oxford), Dan Marsden (University of Oxford), Alexis Toumi (University of\n  Oxford)", "title": "Towards Compositional Distributional Discourse Analysis", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 1-12", "doi": "10.4204/EPTCS.283.1", "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical compositional distributional semantics provide a method to derive\nthe meaning of a sentence from the meaning of its individual words: the\ngrammatical reduction of a sentence automatically induces a linear map for\ncomposing the word vectors obtained from distributional semantics. In this\npaper, we extend this passage from word-to-sentence to sentence-to-discourse\ncomposition. To achieve this we introduce a notion of basic anaphoric\ndiscourses as a mid-level representation between natural language discourse\nformalised in terms of basic discourse representation structures (DRS); and\nknowledge base queries over the Semantic Web as described by basic graph\npatterns in the Resource Description Framework (RDF). This provides a\nhigh-level specification for compositional algorithms for question answering\nand anaphora resolution, and allows us to give a picture of natural language\nunderstanding as a process involving both statistical and logical resources.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:14:19 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Coecke", "Bob", "", "University of Oxford"], ["de Felice", "Giovanni", "", "University of\n  Oxford"], ["Marsden", "Dan", "", "University of Oxford"], ["Toumi", "Alexis", "", "University of\n  Oxford"]]}, {"id": "1811.03291", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta", "title": "Doc2Im: document to image conversion through self-attentive embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is a fundamental task in NLP applications. Latest\nresearch in this field has largely been divided into two major sub-fields.\nLearning representations is one sub-field and learning deeper models, both\nsequential and convolutional, which again connects back to the representation\nis the other side. We posit the idea that the stronger the representation is,\nthe simpler classifier models are needed to achieve higher performance. In this\npaper we propose a completely novel direction to text classification research,\nwherein we convert text to a representation very similar to images, such that\nany deep network able to handle images is equally able to handle text. We take\na deeper look at the representation of documents as an image and subsequently\nutilize very simple convolution based models taken as is from computer vision\ndomain. This image can be cropped, re-scaled, re-sampled and augmented just\nlike any other image to work with most of the state-of-the-art large\nconvolution based models which have been designed to handle large image\ndatasets. We show impressive results with some of the latest benchmarks in the\nrelated fields. We perform transfer learning experiments, both from text to\ntext domain and also from image to text domain. We believe this is a paradigm\nshift from the way document understanding and text classification has been\ntraditionally done, and will drive numerous novel research ideas in the\ncommunity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 06:51:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Gupta", "Mithun Das", ""]]}, {"id": "1811.03325", "submitter": "Xiaoshi Zhong", "authors": "Xiaoshi Zhong and Erik Cambria and Jagath C. Rajapakse", "title": "Discovering Power Laws in Entity Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a discovery that the length of the entities in various\ndatasets follows a family of scale-free power law distributions. The concept of\nentity here broadly includes the named entity, entity mention, time expression,\naspect term, and domain-specific entity that are well investigated in natural\nlanguage processing and related areas. The entity length denotes the number of\nwords in an entity. The power law distributions in entity length possess the\nscale-free property and have well-defined means and finite variances. We\nexplain the phenomenon of power laws in entity length by the principle of least\neffort in communication and the preferential mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 09:16:19 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 14:23:31 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 15:27:40 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhong", "Xiaoshi", ""], ["Cambria", "Erik", ""], ["Rajapakse", "Jagath C.", ""]]}, {"id": "1811.03399", "submitter": "Stefanie Rinderle-Ma", "authors": "Karolin Winter and Stefanie Rinderle-Ma", "title": "Untangling the GDPR Using ConRelMiner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Data Protection Regulation (GDPR) poses enormous challenges on\ncompanies and organizations with respect to understanding, implementing, and\nmaintaining the contained constraints. We report on how the ConRelMiner method\ncan be used for untangling the GDPR. For this, the GDPR is filtered and grouped\nalong the roles mentioned by the GDPR and the reduction of sentences to be read\nby analysts is shown. Moreover, the output of the ConRelMiner - a cluster graph\nwith relations between the sentences - is displayed and interpreted. Overall\nthe goal is to illustrate how the effort for implementing the GDPR can be\nreduced and a structured and meaningful representation of the relevant GDPR\nsentences can be found.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:28:01 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Winter", "Karolin", ""], ["Rinderle-Ma", "Stefanie", ""]]}, {"id": "1811.03423", "submitter": "Kory W Mathewson", "authors": "Markus Eger and Kory W. Mathewson", "title": "dAIrector: Automatic Story Beat Generation through Knowledge Synthesis", "comments": "10 pages with references, 1 figure. Accepted at Joint Workshop on\n  Intelligent Narrative Technologies and Intelligent Cinematography and Editing\n  at AAAI Conference on Artificial Intelligence and Interactive Digital\n  Entertainment (AIIDE'18). Edmonton, Alberta, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  dAIrector is an automated director which collaborates with humans\nstorytellers for live improvisational performances and writing assistance.\ndAIrector can be used to create short narrative arcs through contextual plot\ngeneration. In this work, we present the system architecture, a quantitative\nevaluation of design choices, and a case-study usage of the system which\nprovides qualitative feedback from a professional improvisational performer. We\npresent relevant metrics for the understudied domain of human-machine creative\ngeneration, specifically long-form narrative creation. We include, alongside\npublication, open-source code so that others may test, evaluate, and run the\ndAIrector.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:41:22 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Eger", "Markus", ""], ["Mathewson", "Kory W.", ""]]}, {"id": "1811.03451", "submitter": "Murali Karthick Baskar", "authors": "Martin Karafi\\'at, Murali Karthick Baskar, Shinji Watanabe, Takaaki\n  Hori, Matthew Wiesner and Jan \"Honza'' \\v{C}ernock\\'y", "title": "Analysis of Multilingual Sequence-to-Sequence speech recognition systems", "comments": "arXiv admin note: text overlap with arXiv:1810.03459", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the applications of various multilingual approaches\ndeveloped in conventional hidden Markov model (HMM) systems to\nsequence-to-sequence (seq2seq) automatic speech recognition (ASR). On a set\ncomposed of Babel data, we first show the effectiveness of multi-lingual\ntraining with stacked bottle-neck (SBN) features. Then we explore various\narchitectures and training strategies of multi-lingual seq2seq models based on\nCTC-attention networks including combinations of output layer, CTC and/or\nattention component re-training. We also investigate the effectiveness of\nlanguage-transfer learning in a very low resource scenario when the target\nlanguage is not included in the original multi-lingual training data.\nInterestingly, we found multilingual features superior to multilingual models,\nand this finding suggests that we can efficiently combine the benefits of the\nHMM system with the seq2seq system through these multilingual feature\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 09:59:24 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Karafi\u00e1t", "Martin", ""], ["Baskar", "Murali Karthick", ""], ["Watanabe", "Shinji", ""], ["Hori", "Takaaki", ""], ["Wiesner", "Matthew", ""], ["\u010cernock\u00fd", "Jan \"Honza''", ""]]}, {"id": "1811.03511", "submitter": "Jiaxun Cai", "authors": "Zuchao Li, Jiaxun Cai, Hai Zhao", "title": "Effective Representation for Easy-First Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Easy-first parsing relies on subtree re-ranking to build the complete parse\ntree. Whereas the intermediate state of parsing processing is represented by\nvarious subtrees, whose internal structural information is the key lead for\nlater parsing action decisions, we explore a better representation for such\nsubtrees. In detail, this work introduces a bottom-up subtree encoding method\nbased on the child-sum tree-LSTM. Starting from an easy-first dependency parser\nwithout other handcraft features, we show that the effective subtree encoder\ndoes promote the parsing process, and can make a greedy search easy-first\nparser achieve promising results on benchmark treebanks compared to\nstate-of-the-art baselines. Furthermore, with the help of the current\npre-training language model, we further improve the state-of-the-art results of\nthe easy-first approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:59:11 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 01:10:30 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 02:06:16 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Li", "Zuchao", ""], ["Cai", "Jiaxun", ""], ["Zhao", "Hai", ""]]}, {"id": "1811.03514", "submitter": "Ayyoob Imani Googhari", "authors": "Ayyoob Imani, Amir Vakili, Ali Montazer and Azadeh Shakery", "title": "Deep Neural Networks for Query Expansion using Word Embeddings", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query expansion is a method for alleviating the vocabulary mismatch problem\npresent in information retrieval tasks. Previous works have shown that terms\nselected for query expansion by traditional methods such as pseudo-relevance\nfeedback are not always helpful to the retrieval process. In this paper, we\nshow that this is also true for more recently proposed embedding-based query\nexpansion methods. We then introduce an artificial neural network classifier to\npredict the usefulness of query expansion terms. This classifier uses term word\nembeddings as inputs. We perform experiments on four TREC newswire and web\ncollections show that using terms selected by the classifier for expansion\nsignificantly improves retrieval performance when compared to competitive\nbaselines. The results are also shown to be more robust than the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:01:35 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Imani", "Ayyoob", ""], ["Vakili", "Amir", ""], ["Montazer", "Ali", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1811.03519", "submitter": "Bertrand Higy", "authors": "Bertrand Higy and Peter Bell", "title": "Few-shot learning with attention-based sequence-to-sequence models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches have recently become popular as a means of simplifying\nthe training and deployment of speech recognition systems. However, they often\nrequire large amounts of data to perform well on large vocabulary tasks. With\nthe aim of making end-to-end approaches usable by a broader range of\nresearchers, we explore the potential to use end-to-end methods in small\nvocabulary contexts where smaller datasets may be used. A significant drawback\nof small-vocabulary systems is the difficulty of expanding the vocabulary\nbeyond the original training samples -- therefore we also study strategies to\nextend the vocabulary with only few examples per new class (few-shot learning).\n  Our results show that an attention-based encoder-decoder can be competitive\nagainst a strong baseline on a small vocabulary keyword classification task,\nreaching 97.5% of accuracy on Tensorflow's Speech Commands dataset. It also\nshows promising results on the few-shot learning problem where a simple\nstrategy achieved 68.8\\% of accuracy on new keywords with only 10 examples for\neach new class. This score goes up to 88.4\\% with a larger set of 100 examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:05:50 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 11:15:52 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Higy", "Bertrand", ""], ["Bell", "Peter", ""]]}, {"id": "1811.03554", "submitter": "Pengxiang Cheng", "authors": "Pengxiang Cheng, Katrin Erk", "title": "Implicit Argument Prediction as Reading Comprehension", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit arguments, which cannot be detected solely through syntactic cues,\nmake it harder to extract predicate-argument tuples. We present a new model for\nimplicit argument prediction that draws on reading comprehension, casting the\npredicate-argument tuple with the missing argument as a query. We also draw on\npointer networks and multi-hop computation. Our model shows good performance on\nan argument cloze task as well as on a nominal implicit argument prediction\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:13:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Cheng", "Pengxiang", ""], ["Erk", "Katrin", ""]]}, {"id": "1811.03604", "submitter": "Andrew Hard", "authors": "Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy,\n  Fran\\c{c}oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo\\'e Kiddon,\n  Daniel Ramage", "title": "Federated Learning for Mobile Keyboard Prediction", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We train a recurrent neural network language model using a distributed,\non-device learning framework called federated learning for the purpose of\nnext-word prediction in a virtual keyboard for smartphones. Server-based\ntraining using stochastic gradient descent is compared with training on client\ndevices using the Federated Averaging algorithm. The federated algorithm, which\nenables training on a higher-quality dataset for this use case, is shown to\nachieve better prediction recall. This work demonstrates the feasibility and\nbenefit of training language models on client devices without exporting\nsensitive user data to servers. The federated learning environment gives users\ngreater control over the use of their data and simplifies the task of\nincorporating privacy by default with distributed training and aggregation\nacross a population of client devices.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:37:03 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 21:07:51 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Hard", "Andrew", ""], ["Rao", "Kanishka", ""], ["Mathews", "Rajiv", ""], ["Ramaswamy", "Swaroop", ""], ["Beaufays", "Fran\u00e7oise", ""], ["Augenstein", "Sean", ""], ["Eichner", "Hubert", ""], ["Kiddon", "Chlo\u00e9", ""], ["Ramage", "Daniel", ""]]}, {"id": "1811.03700", "submitter": "Chao Weng", "authors": "Chao Weng, Dong Yu", "title": "A Comparison of Lattice-free Discriminative Training Criteria for Purely\n  Sequence-Trained Neural Network Acoustic Models", "comments": "under review ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, three lattice-free (LF) discriminative training criteria for\npurely sequence-trained neural network acoustic models are compared on LVCSR\ntasks, namely maximum mutual information (MMI), boosted maximum mutual\ninformation (bMMI) and state-level minimum Bayes risk (sMBR). We demonstrate\nthat, analogous to LF-MMI, a neural network acoustic model can also be trained\nfrom scratch using LF-bMMI or LF-sMBR criteria respectively without the need of\ncross-entropy pre-training. Furthermore, experimental results on\nSwitchboard-300hrs and Switchboard+Fisher-2100hrs datasets show that models\ntrained with LF-bMMI consistently outperform those trained with plain LF-MMI\nand achieve a relative word error rate (WER) reduction of 5% over competitive\ntemporal convolution projected LSTM (TDNN-LSTMP) LF-MMI baselines.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:37:55 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 10:55:10 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Weng", "Chao", ""], ["Yu", "Dong", ""]]}, {"id": "1811.03729", "submitter": "Yanran Li", "authors": "Yanran Li and Wenjie Li and Ziqiang Cao and Chengyao Chen", "title": "Incorporating Relevant Knowledge in Context Modeling and Response\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To sustain engaging conversation, it is critical for chatbots to make good\nuse of relevant knowledge. Equipped with a knowledge base, chatbots are able to\nextract conversation-related attributes and entities to facilitate context\nmodeling and response generation. In this work, we distinguish the uses of\nattribute and entity and incorporate them into the encoder-decoder architecture\nin different manners. Based on the augmented architecture, our chatbot, namely\nMike, is able to generate responses by referring to proper entities from the\ncollected knowledge. To validate the proposed approach, we build a movie\nconversation corpus on which the proposed approach significantly outperforms\nother four knowledge-grounded models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 01:18:27 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Li", "Yanran", ""], ["Li", "Wenjie", ""], ["Cao", "Ziqiang", ""], ["Chen", "Chengyao", ""]]}, {"id": "1811.03754", "submitter": "Duong Nguyen", "authors": "Duong Nguyen Anh, Hieu Nguyen Kiem, Vi Ngo Van", "title": "Neural sequence labeling for Vietnamese POS Tagging and NER", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a neural architecture for Vietnamese sequence labeling\ntasks including part-of-speech (POS) tagging and named entity recognition\n(NER). We applied the model described in \\cite{lample-EtAl:2016:N16-1} that is\na combination of bidirectional Long-Short Term Memory and Conditional Random\nFields, which rely on two sources of information about words: character-based\nword representations learned from the supervised corpus and pre-trained word\nembeddings learned from other unannotated corpora. Experiments on benchmark\ndatasets show that this work achieves state-of-the-art performances on both\ntasks - 93.52\\% accuracy for POS tagging and 94.88\\% F1 for NER. Our sourcecode\nis available at here.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:15:23 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 13:23:15 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Anh", "Duong Nguyen", ""], ["Kiem", "Hieu Nguyen", ""], ["Van", "Vi Ngo", ""]]}, {"id": "1811.03790", "submitter": "Tomi Kinnunen", "authors": "Tomi Kinnunen and Rosa Gonz\\'alez Hautam\\\"aki and Ville Vestman and Md\n  Sahidullah", "title": "Can We Use Speaker Recognition Technology to Attack Itself? Enhancing\n  Mimicry Attacks Using Automatic Target Speaker Selection", "comments": "(A slightly shorter version) has been submitted to IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider technology-assisted mimicry attacks in the context of automatic\nspeaker verification (ASV). We use ASV itself to select targeted speakers to be\nattacked by human-based mimicry. We recorded 6 naive mimics for whom we select\ntarget celebrities from VoxCeleb1 and VoxCeleb2 corpora (7,365 potential\ntargets) using an i-vector system. The attacker attempts to mimic the selected\ntarget, with the utterances subjected to ASV tests using an independently\ndeveloped x-vector system. Our main finding is negative: even if some of the\nattacker scores against the target speakers were slightly increased, our mimics\ndid not succeed in spoofing the x-vector system. Interestingly, however, the\nrelative ordering of the selected targets (closest, furthest, median) are\nconsistent between the systems, which suggests some level of transferability\nbetween the systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 06:15:08 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Kinnunen", "Tomi", ""], ["Hautam\u00e4ki", "Rosa Gonz\u00e1lez", ""], ["Vestman", "Ville", ""], ["Sahidullah", "Md", ""]]}, {"id": "1811.03796", "submitter": "Yansong Feng", "authors": "Liwei Chen, Yansong Feng, Songfang Huang, Bingfeng Luo, Dongyan Zhao", "title": "Encoding Implicit Relation Requirements for Relation Extraction: A Joint\n  Inference Approach", "comments": "to appear in Artificial Intelligence", "journal-ref": "https://doi.org/10.1016/j.artint.2018.08.004", "doi": "10.1016/j.artint.2018.08.004", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction is the task of identifying predefined relationship\nbetween entities, and plays an essential role in information extraction,\nknowledge base construction, question answering and so on. Most existing\nrelation extractors make predictions for each entity pair locally and\nindividually, while ignoring implicit global clues available across different\nentity pairs and in the knowledge base, which often leads to conflicts among\nlocal predictions from different entity pairs. This paper proposes a joint\ninference framework that employs such global clues to resolve disagreements\namong local predictions. We exploit two kinds of clues to generate constraints\nwhich can capture the implicit type and cardinality requirements of a relation.\nThose constraints can be examined in either hard style or soft style, both of\nwhich can be effectively explored in an integer linear program formulation.\nExperimental results on both English and Chinese datasets show that our\nproposed framework can effectively utilize those two categories of global clues\nand resolve the disagreements among local predictions, thus improve various\nrelation extractors when such clues are applicable to the datasets. Our\nexperiments also indicate that the clues learnt automatically from existing\nknowledge bases perform comparably to or better than those refined by human.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 07:06:46 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Chen", "Liwei", ""], ["Feng", "Yansong", ""], ["Huang", "Songfang", ""], ["Luo", "Bingfeng", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1811.03865", "submitter": "Ozan Caglayan", "authors": "Ozan Caglayan, Ramon Sanabria, Shruti Palaskar, Lo\\\"ic Barrault,\n  Florian Metze", "title": "Multimodal Grounding for Sequence-to-Sequence Speech Recognition", "comments": "ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of processing speech by making use of multiple sensory\nmodalities. For example, the environment where a conversation takes place\ngenerally provides semantic and/or acoustic context that helps us to resolve\nambiguities or to recall named entities. Motivated by this, there have been\nmany works studying the integration of visual information into the speech\nrecognition pipeline. Specifically, in our previous work, we propose a\nmultistep visual adaptive training approach which improves the accuracy of an\naudio-based Automatic Speech Recognition (ASR) system. This approach, however,\nis not end-to-end as it requires fine-tuning the whole model with an adaptation\nlayer. In this paper, we propose novel end-to-end multimodal ASR systems and\ncompare them to the adaptive approach by using a range of visual\nrepresentations obtained from state-of-the-art convolutional neural networks.\nWe show that adaptive training is effective for S2S models leading to an\nabsolute improvement of 1.4% in word error rate. As for the end-to-end systems,\nalthough they perform better than baseline, the improvements are slightly less\nthan adaptive training, 0.8 absolute WER reduction in single-best models. Using\nensemble decoding, end-to-end models reach a WER of 15% which is the lowest\nscore among all systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 11:30:11 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 22:32:29 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Caglayan", "Ozan", ""], ["Sanabria", "Ramon", ""], ["Palaskar", "Shruti", ""], ["Barrault", "Lo\u00efc", ""], ["Metze", "Florian", ""]]}, {"id": "1811.03866", "submitter": "Timo Schick", "authors": "Timo Schick, Hinrich Sch\\\"utze", "title": "Learning Semantic Representations for Novel Words: Leveraging Both Form\n  and Context", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a key component of high-performing natural language\nprocessing (NLP) systems, but it remains a challenge to learn good\nrepresentations for novel words on the fly, i.e., for words that did not occur\nin the training data. The general problem setting is that word embeddings are\ninduced on an unlabeled training corpus and then a model is trained that embeds\nnovel words into this induced embedding space. Currently, two approaches for\nlearning embeddings of novel words exist: (i) learning an embedding from the\nnovel word's surface-form (e.g., subword n-grams) and (ii) learning an\nembedding from the context in which it occurs. In this paper, we propose an\narchitecture that leverages both sources of information - surface-form and\ncontext - and show that it results in large increases in embedding quality. Our\narchitecture obtains state-of-the-art results on the Definitional Nonce and\nContextual Rare Words datasets. As input, we only require an embedding set and\nan unlabeled corpus for training our architecture to produce embeddings\nappropriate for the induced embedding space. Thus, our model can easily be\nintegrated into any existing NLP system and enhance its capability to handle\nnovel words.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 11:44:05 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Schick", "Timo", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1811.03873", "submitter": "Tao Gui", "authors": "Tao Gui, Qi Zhang, Lujun Zhao, Yaosong Lin, Minlong Peng, Jingjing\n  Gong, Xuanjing Huang", "title": "Long Short-Term Memory with Dynamic Skip Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, long short-term memory (LSTM) has been successfully used to\nmodel sequential data of variable length. However, LSTM can still experience\ndifficulty in capturing long-term dependencies. In this work, we tried to\nalleviate this problem by introducing a dynamic skip connection, which can\nlearn to directly connect two dependent words. Since there is no dependency\ninformation in the training data, we propose a novel reinforcement\nlearning-based method to model the dependency relationship and connect\ndependent words. The proposed model computes the recurrent transition functions\nbased on the skip connections, which provides a dynamic skipping advantage over\nRNNs that always tackle entire sentences sequentially. Our experimental results\non three natural language processing tasks demonstrate that the proposed method\ncan achieve better performance than existing methods. In the number prediction\nexperiment, the proposed model outperformed LSTM with respect to accuracy by\nnearly 20%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:11:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Gui", "Tao", ""], ["Zhang", "Qi", ""], ["Zhao", "Lujun", ""], ["Lin", "Yaosong", ""], ["Peng", "Minlong", ""], ["Gong", "Jingjing", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1811.03875", "submitter": "Herman Kamper", "authors": "Ryan Eloff, Herman A. Engelbrecht, Herman Kamper", "title": "Multimodal One-Shot Learning of Speech and Images", "comments": "5 pages, 1 figure, 3 tables; accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a robot is shown new concepts visually together with spoken tags,\ne.g. \"milk\", \"eggs\", \"butter\". After seeing one paired audio-visual example per\nclass, it is shown a new set of unseen instances of these objects, and asked to\npick the \"milk\". Without receiving any hard labels, could it learn to match the\nnew continuous speech input to the correct visual instance? Although unimodal\none-shot learning has been studied, where one labelled example in a single\nmodality is given per class, this example motivates multimodal one-shot\nlearning. Our main contribution is to formally define this task, and to propose\nseveral baseline and advanced models. We use a dataset of paired spoken and\nvisual digits to specifically investigate recent advances in Siamese\nconvolutional neural networks. Our best Siamese model achieves twice the\naccuracy of a nearest neighbour model using pixel-distance over images and\ndynamic time warping over speech in 11-way cross-modal matching.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:14:20 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:08:03 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Eloff", "Ryan", ""], ["Engelbrecht", "Herman A.", ""], ["Kamper", "Herman", ""]]}, {"id": "1811.03925", "submitter": "Ryuichi Takanobu", "authors": "Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, Minlie Huang", "title": "A Hierarchical Framework for Relation Extraction with Reinforcement\n  Learning", "comments": "To appear in AAAI 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods determine relation types only after all the entities\nhave been recognized, thus the interaction between relation types and entity\nmentions is not fully modeled. This paper presents a novel paradigm to deal\nwith relation extraction by regarding the related entities as the arguments of\na relation. We apply a hierarchical reinforcement learning (HRL) framework in\nthis paradigm to enhance the interaction between entity mentions and relation\ntypes. The whole extraction process is decomposed into a hierarchy of two-level\nRL policies for relation detection and entity extraction respectively, so that\nit is more feasible and natural to deal with overlapping relations. Our model\nwas evaluated on public datasets collected via distant supervision, and results\nshow that it gains better performance than existing methods and is more\npowerful for extracting overlapping relations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:33:29 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Takanobu", "Ryuichi", ""], ["Zhang", "Tianyang", ""], ["Liu", "Jiexi", ""], ["Huang", "Minlie", ""]]}, {"id": "1811.04154", "submitter": "Shruti Rijhwani", "authors": "Shruti Rijhwani and Jiateng Xie and Graham Neubig and Jaime Carbonell", "title": "Zero-shot Neural Transfer for Cross-lingual Entity Linking", "comments": "To appear in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual entity linking maps an entity mention in a source language to\nits corresponding entry in a structured knowledge base that is in a different\n(target) language. While previous work relies heavily on bilingual lexical\nresources to bridge the gap between the source and the target languages, these\nresources are scarce or unavailable for many low-resource languages. To address\nthis problem, we investigate zero-shot cross-lingual entity linking, in which\nwe assume no bilingual lexical resources are available in the source\nlow-resource language. Specifically, we propose pivot-based entity linking,\nwhich leverages information from a high-resource \"pivot\" language to train\ncharacter-level neural entity linking models that are transferred to the source\nlow-resource language in a zero-shot manner. With experiments on 9 low-resource\nlanguages and transfer through a total of 54 languages, we show that our\nproposed pivot-based framework improves entity linking accuracy 17% (absolute)\non average over the baseline systems, for the zero-shot scenario. Further, we\nalso investigate the use of language-universal phonological representations\nwhich improves average accuracy (absolute) by 36% when transferring between\nlanguages that use different scripts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 22:50:50 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Rijhwani", "Shruti", ""], ["Xie", "Jiateng", ""], ["Neubig", "Graham", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1811.04164", "submitter": "Van-Khanh Tran", "authors": "Van-Khanh Tran and Le-Minh Nguyen", "title": "Dual Latent Variable Model for Low-Resource Natural Language Generation\n  in Dialogue Systems", "comments": "CoNLL 2018, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning models have shown improving results to natural language\ngeneration (NLG) irrespective of providing sufficient annotated data. However,\na modest training data may harm such models performance. Thus, how to build a\ngenerator that can utilize as much of knowledge from a low-resource setting\ndata is a crucial issue in NLG. This paper presents a variational neural-based\ngeneration model to tackle the NLG problem of having limited labeled dataset,\nin which we integrate a variational inference into an encoder-decoder generator\nand introduce a novel auxiliary autoencoding with an effective training\nprocedure. Experiments showed that the proposed methods not only outperform the\nprevious models when having sufficient training dataset but also show strong\nability to work acceptably well when the training data is scarce.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 00:12:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Tran", "Van-Khanh", ""], ["Nguyen", "Le-Minh", ""]]}, {"id": "1811.04179", "submitter": "Valts Blukis", "authors": "Valts Blukis, Dipendra Misra, Ross A. Knepper, Yoav Artzi", "title": "Mapping Navigation Instructions to Continuous Control Actions with\n  Position-Visitation Prediction", "comments": "Appeared in Conference on Robot Learning 2018", "journal-ref": "In Conference on Robot Learning (pp. 505-518) (2018)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for mapping natural language instructions and raw\nobservations to continuous control of a quadcopter drone. Our model predicts\ninterpretable position-visitation distributions indicating where the agent\nshould go during execution and where it should stop, and uses the predicted\ndistributions to select the actions to execute. This two-step model\ndecomposition allows for simple and efficient training using a combination of\nsupervised learning and imitation learning. We evaluate our approach with a\nrealistic drone simulator, and demonstrate absolute task-completion accuracy\nimprovements of 16.85% over two state-of-the-art instruction-following methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 02:57:38 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 18:37:30 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Blukis", "Valts", ""], ["Misra", "Dipendra", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.04201", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Yann LeCun", "title": "Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes Adversarially-Trained Normalized Noisy-Feature\nAuto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of\nan auto-encoder where the internal code is normalized on the unit sphere and\ncorrupted by additive noise. Simultaneously, a replica of the decoder (sharing\nthe same parameters as the AE decoder) is used as the generator and fed with\nrandom latent vectors. An adversarial discriminator is trained to distinguish\ntraining samples reconstructed from the AE from samples produced through the\nrandom-input generator, making the entire generator-discriminator path\ndifferentiable for discrete data like text. The combined effect of noise\ninjection in the code and shared weights between the decoder and the generator\ncan prevent the mode collapsing phenomenon commonly observed in GANs. Since\nperplexity cannot be applied to non-sequential text generation, we propose a\nnew evaluation method using the total variance distance between frequencies of\nhash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can\ncharacterize both the quality and the diversity of the generated texts.\nExperiments are offered in 6 large-scale datasets in Arabic, Chinese and\nEnglish, with comparisons against n-gram baselines and recurrent neural\nnetworks (RNNs). Ablation study on both the noise level and the discriminator\nis performed. We find that RNNs have trouble competing with the n-gram\nbaselines, and the ATNNFAE results are generally competitive.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 06:05:53 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhang", "Xiang", ""], ["LeCun", "Yann", ""]]}, {"id": "1811.04210", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui, Jian Su", "title": "Densely Connected Attention Propagation for Reading Comprehension", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DecaProp (Densely Connected Attention Propagation), a new densely\nconnected neural architecture for reading comprehension (RC). There are two\ndistinct characteristics of our model. Firstly, our model densely connects all\npairwise layers of the network, modeling relationships between passage and\nquery across all hierarchical levels. Secondly, the dense connectors in our\nnetwork are learned via attention instead of standard residual skip-connectors.\nTo this end, we propose novel Bidirectional Attention Connectors (BAC) for\nefficiently forging connections throughout the network. We conduct extensive\nexperiments on four challenging RC benchmarks. Our proposed approach achieves\nstate-of-the-art results on all four, outperforming existing baselines by up to\n$2.6\\%-14.2\\%$ in absolute F1 score.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 07:54:13 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 11:19:54 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""], ["Su", "Jian", ""]]}, {"id": "1811.04231", "submitter": "Won Ik Cho", "authors": "Won Ik Cho, Hyeon Seung Lee, Ji Won Yoon, Seok Min Kim, Nam Soo Kim", "title": "Speech Intention Understanding in a Head-final Language: A\n  Disambiguation Utilizing Intonation-dependency", "comments": "14 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a large portion of real-life utterances, the intention cannot be solely\ndecided by either their semantic or syntactic characteristics. Although not all\nthe sociolinguistic and pragmatic information can be digitized, at least\nphonetic features are indispensable in understanding the spoken language.\nEspecially in head-final languages such as Korean, sentence-final prosody has\ngreat importance in identifying the speaker's intention. This paper suggests a\nsystem which identifies the inherent intention of a spoken utterance given its\ntranscript, in some cases using auxiliary acoustic features. The main point\nhere is a separate distinction for cases where discrimination of intention\nrequires an acoustic cue. Thus, the proposed classification system decides\nwhether the given utterance is a fragment, statement, question, command, or a\nrhetorical question/command, utilizing the intonation-dependency coming from\nthe head-finality. Based on an intuitive understanding of the Korean language\nthat is engaged in the data annotation, we construct a network which identifies\nthe intention of a speech, and validate its utility with the test sentences.\nThe system, if combined with up-to-date speech recognizers, is expected to be\nflexibly inserted into various language understanding modules.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 10:41:27 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 05:45:59 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Cho", "Won Ik", ""], ["Lee", "Hyeon Seung", ""], ["Yoon", "Ji Won", ""], ["Kim", "Seok Min", ""], ["Kim", "Nam Soo", ""]]}, {"id": "1811.04284", "submitter": "Hainan Xu", "authors": "Hainan Xu, Shuoyang Ding, Shinji Watanabe", "title": "Improving End-to-end Speech Recognition with Pronunciation-assisted\n  Sub-word Modeling", "comments": null, "journal-ref": "ICASSP 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most end-to-end speech recognition systems model text directly as a sequence\nof characters or sub-words. Current approaches to sub-word extraction only\nconsider character sequence frequencies, which at times produce inferior\nsub-word segmentation that might lead to erroneous speech recognition output.\nWe propose pronunciation-assisted sub-word modeling (PASM), a sub-word\nextraction method that leverages the pronunciation information of a word.\nExperiments show that the proposed method can greatly improve upon the\ncharacter-based baseline, and also outperform commonly used byte-pair encoding\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 17:07:44 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 18:37:35 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Xu", "Hainan", ""], ["Ding", "Shuoyang", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1811.04319", "submitter": "Ronen Tamari", "authors": "Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, Yuji Matsumoto", "title": "Playing by the Book: An Interactive Game Approach for Action Graph\n  Extraction from Text", "comments": "Accepted to NAACL 2019 ESSP workshop\n  (https://scientific-knowledge.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding procedural text requires tracking entities, actions and effects\nas the narrative unfolds. We focus on the challenging real-world problem of\naction-graph extraction from material science papers, where language is highly\nspecialized and data annotation is expensive and scarce. We propose a novel\napproach, Text2Quest, where procedural text is interpreted as instructions for\nan interactive game. A learning agent completes the game by executing the\nprocedure correctly in a text-based simulated lab environment. The framework\ncan complement existing approaches and enables richer forms of learning\ncompared to static texts. We discuss potential limitations and advantages of\nthe approach, and release a prototype proof-of-concept, hoping to encourage\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 21:45:07 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 16:59:00 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 19:19:05 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tamari", "Ronen", ""], ["Shindo", "Hiroyuki", ""], ["Shahaf", "Dafna", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1811.04352", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang and Yafang Huang and Hai Zhao", "title": "Open Vocabulary Learning for Neural Chinese Pinyin IME", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pinyin-to-character (P2C) conversion is the core component of pinyin-based\nChinese input method engine (IME). However, the conversion is seriously\ncompromised by the ambiguities of Chinese characters corresponding to pinyin as\nwell as the predefined fixed vocabularies. To alleviate such inconveniences, we\npropose a neural P2C conversion model augmented by an online updated vocabulary\nwith a sampling mechanism to support open vocabulary learning during IME\nworking. Our experiments show that the proposed method outperforms commercial\nIMEs and state-of-the-art traditional models on standard corpus and true\ninputting history dataset in terms of multiple metrics and thus the online\nupdated vocabulary indeed helps our IME effectively follows user inputting\nbehavior.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 05:07:25 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 13:02:18 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 07:11:16 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 12:54:31 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Huang", "Yafang", ""], ["Zhao", "Hai", ""]]}, {"id": "1811.04354", "submitter": "Xinsong Zhang", "authors": "Xinsong Zhang, Pengshuai Li, Weijia Jia and Hai Zhao", "title": "Multi-labeled Relation Extraction with Attentive Capsule Network", "comments": "To be published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To disclose overlapped multiple relations from a sentence still keeps\nchallenging. Most current works in terms of neural models inconveniently\nassuming that each sentence is explicitly mapped to a relation label, cannot\nhandle multiple relations properly as the overlapped features of the relations\nare either ignored or very difficult to identify. To tackle with the new issue,\nwe propose a novel approach for multi-labeled relation extraction with capsule\nnetwork which acts considerably better than current convolutional or recurrent\nnet in identifying the highly overlapped relations within an individual\nsentence. To better cluster the features and precisely extract the relations,\nwe further devise attention-based routing algorithm and sliding-margin loss\nfunction, and embed them into our capsule network. The experimental results\nshow that the proposed approach can indeed extract the highly overlapped\nfeatures and achieve significant performance improvement for relation\nextraction comparing to the state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 05:29:17 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhang", "Xinsong", ""], ["Li", "Pengshuai", ""], ["Jia", "Weijia", ""], ["Zhao", "Hai", ""]]}, {"id": "1811.04369", "submitter": "Izzeddin Gur", "authors": "Izzeddin Gur, Dilek Hakkani-Tur, Gokhan Tur, Pararth Shah", "title": "User Modeling for Task Oriented Dialogues", "comments": "Accepted at SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce end-to-end neural network based models for simulating users of\ntask-oriented dialogue systems. User simulation in dialogue systems is crucial\nfrom two different perspectives: (i) automatic evaluation of different dialogue\nmodels, and (ii) training task-oriented dialogue systems. We design a\nhierarchical sequence-to-sequence model that first encodes the initial user\ngoal and system turns into fixed length representations using Recurrent Neural\nNetworks (RNN). It then encodes the dialogue history using another RNN layer.\nAt each turn, user responses are decoded from the hidden representations of the\ndialogue level RNN. This hierarchical user simulator (HUS) approach allows the\nmodel to capture undiscovered parts of the user goal without the need of an\nexplicit dialogue state tracking. We further develop several variants by\nutilizing a latent variable model to inject random variations into user\nresponses to promote diversity in simulated user responses and a novel goal\nregularization mechanism to penalize divergence of user responses from the\ninitial user goal. We evaluate the proposed models on movie ticket booking\ndomain by systematically interacting each user simulator with various dialogue\nsystem policies trained with different objectives and users.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 08:21:55 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Gur", "Izzeddin", ""], ["Hakkani-Tur", "Dilek", ""], ["Tur", "Gokhan", ""], ["Shah", "Pararth", ""]]}, {"id": "1811.04441", "submitter": "Chao Shang", "authors": "Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, Bowen Zhou", "title": "End-to-end Structure-Aware Convolutional Networks for Knowledge Base\n  Completion", "comments": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding has been an active research topic for knowledge\nbase completion, with progressive improvement from the initial TransE, TransH,\nDistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolution\nover embeddings and multiple layers of nonlinear features to model knowledge\ngraphs. The model can be efficiently trained and scalable to large knowledge\ngraphs. However, there is no structure enforcement in the embedding space of\nConvE. The recent graph convolutional network (GCN) provides another way of\nlearning graph node embedding by successfully utilizing graph connectivity\nstructure. In this work, we propose a novel end-to-end Structure-Aware\nConvolutional Network (SACN) that takes the benefit of GCN and ConvE together.\nSACN consists of an encoder of a weighted graph convolutional network (WGCN),\nand a decoder of a convolutional network called Conv-TransE. WGCN utilizes\nknowledge graph node structure, node attributes and edge relation types. It has\nlearnable weights that adapt the amount of information from neighbors used in\nlocal aggregation, leading to more accurate embeddings of graph nodes. Node\nattributes in the graph are represented as additional nodes in the WGCN. The\ndecoder Conv-TransE enables the state-of-the-art ConvE to be translational\nbetween entities and relations while keeps the same link prediction performance\nas ConvE. We demonstrate the effectiveness of the proposed SACN on standard\nFB15k-237 and WN18RR datasets, and it gives about 10% relative improvement over\nthe state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 18:07:44 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 01:14:53 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Shang", "Chao", ""], ["Tang", "Yun", ""], ["Huang", "Jing", ""], ["Bi", "Jinbo", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "1811.04454", "submitter": "Milan Aggarwal", "authors": "Milan Aggarwal, Nupur Kumari, Ayush Bansal, Balaji Krishnamurthy", "title": "ReDecode Framework for Iterative Improvement in Paraphrase Generation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating paraphrases, that is, different variations of a sentence conveying\nthe same meaning, is an important yet challenging task in NLP. Automatically\ngenerating paraphrases has its utility in many NLP tasks like question\nanswering, information retrieval, conversational systems to name a few. In this\npaper, we introduce iterative refinement of generated paraphrases within VAE\nbased generation framework. Current sequence generation models lack the\ncapability to (1) make improvements once the sentence is generated; (2) rectify\nerrors made while decoding. We propose a technique to iteratively refine the\noutput using multiple decoders, each one attending on the output sentence\ngenerated by the previous decoder. We improve current state of the art results\nsignificantly - with over 9% and 28% absolute increase in METEOR scores on\nQuora question pairs and MSCOCO datasets respectively. We also show\nqualitatively through examples that our re-decoding approach generates better\nparaphrases compared to a single decoder by rectifying errors and making\nimprovements in paraphrase structure, inducing variations and introducing new\nbut semantically coherent information.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:02:50 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Aggarwal", "Milan", ""], ["Kumari", "Nupur", ""], ["Bansal", "Ayush", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1811.04498", "submitter": "Jianguo Zhang", "authors": "Jianguo Zhang, Pengcheng Zou, Zhao Li, Yao Wan, Ye Liu, Xiuming Pan,\n  Yu Gong, Philip S. Yu", "title": "Product Title Refinement via Multi-Modal Generative Adversarial Learning", "comments": "Workshop on Visually Grounded Interaction and Language, NIPS, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, an increasing number of customers are in favor of using E-commerce\nApps to browse and purchase products. Since merchants are usually inclined to\nemploy redundant and over-informative product titles to attract customers'\nattention, it is of great importance to concisely display short product titles\non limited screen of cell phones. Previous researchers mainly consider textual\ninformation of long product titles and lack of human-like view during training\nand evaluation procedure. In this paper, we propose a Multi-Modal Generative\nAdversarial Network (MM-GAN) for short product title generation, which\ninnovatively incorporates image information, attribute tags from the product\nand the textual information from original long titles. MM-GAN treats short\ntitles generation as a reinforcement learning process, where the generated\ntitles are evaluated by the discriminator in a human-like view.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 22:37:38 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhang", "Jianguo", ""], ["Zou", "Pengcheng", ""], ["Li", "Zhao", ""], ["Wan", "Yao", ""], ["Liu", "Ye", ""], ["Pan", "Xiuming", ""], ["Gong", "Yu", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.04531", "submitter": "Raden Mu'az Mun'im", "authors": "Raden Mu'az Mun'im, Nakamasa Inoue, Koichi Shinoda", "title": "Sequence-Level Knowledge Distillation for Model Compression of\n  Attention-based Sequence-to-Sequence Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the feasibility of sequence-level knowledge distillation of\nSequence-to-Sequence (Seq2Seq) models for Large Vocabulary Continuous Speech\nRecognition (LVSCR). We first use a pre-trained larger teacher model to\ngenerate multiple hypotheses per utterance with beam search. With the same\ninput, we then train the student model using these hypotheses generated from\nthe teacher as pseudo labels in place of the original ground truth labels. We\nevaluate our proposed method using Wall Street Journal (WSJ) corpus. It\nachieved up to $ 9.8 \\times$ parameter reduction with accuracy loss of up to\n7.0\\% word-error rate (WER) increase\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 02:55:55 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Mun'im", "Raden Mu'az", ""], ["Inoue", "Nakamasa", ""], ["Shinoda", "Koichi", ""]]}, {"id": "1811.04568", "submitter": "Hiroshi Seki", "authors": "Hiroshi Seki, Takaaki Hori, Shinji Watanabe", "title": "Vectorization of hypotheses and speech for faster beam search in encoder\n  decoder-based speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder decoder network uses a left-to-right beam search\nalgorithm in the inference step. The current beam search expands hypotheses and\ntraverses the expanded hypotheses at the next time step. This traversal is\nimplemented using a for-loop program in general, and it leads to speed down of\nthe recognition process. In this paper, we propose a parallelism technique for\nbeam search, which accelerates the search process by vectorizing multiple\nhypotheses to eliminate the for-loop program. We also propose a technique to\nbatch multiple speech utterances for off-line recognition use, which reduces\nthe for-loop program with regard to the traverse of multiple utterances. This\nextension is not trivial during beam search unlike during training due to\nseveral pruning and thresholding techniques for efficient decoding. In\naddition, our method can combine scores of external modules, RNNLM and CTC, in\na batch as shallow fusion. We achieved 3.7 x speedup compared with the original\nbeam search algorithm by vectoring hypotheses, and achieved 10.5 x speedup by\nfurther changing processing unit to GPU.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 06:02:19 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Seki", "Hiroshi", ""], ["Hori", "Takaaki", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1811.04577", "submitter": "Long Nguyen", "authors": "Long Nguyen, Zhou Yang, Jia Li, Guofeng Cao, Fang Jin", "title": "Forecasting People's Needs in Hurricane Events from Social Network", "comments": null, "journal-ref": null, "doi": "10.1109/TBDATA.2019.2941887", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks can serve as a valuable communication channel for calls for\nhelp, offering assistance, and coordinating rescue activities in disaster.\nSocial networks such as Twitter allow users to continuously update relevant\ninformation, which is especially useful during a crisis, where the rapidly\nchanging conditions make it crucial to be able to access accurate information\npromptly. Social media helps those directly affected to inform others of\nconditions on the ground in real time and thus enables rescue workers to\ncoordinate their efforts more effectively, better meeting the survivors' need.\nThis paper presents a new sequence to sequence based framework for forecasting\npeople's needs during disasters using social media and weather data. It\nconsists of two Long Short-Term Memory (LSTM) models, one of which encodes\ninput sequences of weather information and the other plays as a conditional\ndecoder that decodes the encoded vector and forecasts the survivors' needs.\nCase studies utilizing data collected during Hurricane Sandy in 2012, Hurricane\nHarvey and Hurricane Irma in 2017 were analyzed and the results compared with\nthose obtained using a statistical language model n-gram and an LSTM generative\nmodel. Our proposed sequence to sequence method forecast people's needs more\nsuccessfully than either of the other models. This new approach shows great\npromise for enhancing disaster management activities such as evacuation\nplanning and commodity flow management.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 06:37:20 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 17:31:44 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Nguyen", "Long", ""], ["Yang", "Zhou", ""], ["Li", "Jia", ""], ["Cao", "Guofeng", ""], ["Jin", "Fang", ""]]}, {"id": "1811.04588", "submitter": "Xin Lv", "authors": "Xin Lv, Lei Hou, Juanzi Li, Zhiyuan Liu", "title": "Differentiating Concepts and Instances for Knowledge Graph Embedding", "comments": null, "journal-ref": "Proceedings of the 2018 Conference on Empirical Methods in Natural\n  Language Processing. 2018: 1971-1979", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concepts, which represent a group of different instances sharing common\nproperties, are essential information in knowledge representation. Most\nconventional knowledge embedding methods encode both entities (concepts and\ninstances) and relations as vectors in a low dimensional semantic space\nequally, ignoring the difference between concepts and instances. In this paper,\nwe propose a novel knowledge graph embedding model named TransC by\ndifferentiating concepts and instances. Specifically, TransC encodes each\nconcept in knowledge graph as a sphere and each instance as a vector in the\nsame semantic space. We use the relative positions to model the relations\nbetween concepts and instances (i.e., instanceOf), and the relations between\nconcepts and sub-concepts (i.e., subClassOf). We evaluate our model on both\nlink prediction and triple classification tasks on the dataset based on YAGO.\nExperimental results show that TransC outperforms state-of-the-art methods, and\ncaptures the semantic transitivity for instanceOf and subClassOf relation. Our\ncodes and datasets can be obtained from https:// github.com/davidlvxin/TransC.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 07:09:36 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Lv", "Xin", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1811.04599", "submitter": "Huimin Xu", "authors": "Huimin Xu, Zhang Zhang, Lingfei Wu, Cheng-Jun Wang", "title": "The Cinderella Complex: Word Embeddings Reveal Gender Stereotypes in\n  Movies and Books", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0225385", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our analysis of thousands of movies and books reveals how these cultural\nproducts weave stereotypical gender roles into morality tales and perpetuate\ngender inequality through storytelling. Using the word embedding techniques, we\nreveal the constructed emotional dependency of female characters on male\ncharacters in stories.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:26:57 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 09:24:05 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 12:43:01 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Xu", "Huimin", ""], ["Zhang", "Zhang", ""], ["Wu", "Lingfei", ""], ["Wang", "Cheng-Jun", ""]]}, {"id": "1811.04604", "submitter": "Liangchen Luo", "authors": "Liangchen Luo, Wenhao Huang, Qi Zeng, Zaiqing Nie, Xu Sun", "title": "Learning Personalized End-to-End Goal-Oriented Dialog", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing works on dialog systems only consider conversation content\nwhile neglecting the personality of the user the bot is interacting with, which\nbegets several unsolved issues. In this paper, we present a personalized\nend-to-end model in an attempt to leverage personalization in goal-oriented\ndialogs. We first introduce a Profile Model which encodes user profiles into\ndistributed embeddings and refers to conversation history from other similar\nusers. Then a Preference Model captures user preferences over knowledge base\nentities to handle the ambiguity in user requests. The two models are combined\ninto the Personalized MemN2N. Experiments show that the proposed model achieves\nqualitative performance improvements over state-of-the-art methods. As for\nhuman evaluation, it also outperforms other approaches in terms of task\ncompletion rate and user satisfaction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:45:25 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Luo", "Liangchen", ""], ["Huang", "Wenhao", ""], ["Zeng", "Qi", ""], ["Nie", "Zaiqing", ""], ["Sun", "Xu", ""]]}, {"id": "1811.04623", "submitter": "Mikhail Kudinov", "authors": "Vadim Popov and Mikhail Kudinov", "title": "Fine-tuning of Language Models with Discriminator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-entropy loss is a common choice when it comes to multiclass\nclassification tasks and language modeling in particular. Minimizing this loss\nresults in language models of very good quality. We show that it is possible to\nfine-tune these models and make them perform even better if they are fine-tuned\nwith sum of cross-entropy loss and reverse Kullback-Leibler divergence. The\nlatter is estimated using discriminator network that we train in advance.\nDuring fine-tuning probabilities of rare words that are usually underestimated\nby language models become bigger. The novel approach that we propose allows us\nto reach state-of-the-art quality on Penn Treebank: perplexity decreases from\n52.4 to 52.1. Our fine-tuning algorithm is rather fast, scales well to\ndifferent architectures and datasets and requires almost no hyperparameter\ntuning: the only hyperparameter that needs to be tuned is learning rate.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:43:24 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 05:09:22 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Popov", "Vadim", ""], ["Kudinov", "Mikhail", ""]]}, {"id": "1811.04655", "submitter": "Matej Gjurkovic", "authors": "Ivan Sekuli\\'c, Matej Gjurkovi\\'c, Jan \\v{S}najder", "title": "Not Just Depressed: Bipolar Disorder Prediction on Reddit", "comments": "WASSA at EMNLP 2018", "journal-ref": "WASSA@EMNLP 2018: 72-78", "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipolar disorder, an illness characterized by manic and depressive episodes,\naffects more than 60 million people worldwide. We present a preliminary study\non bipolar disorder prediction from user-generated text on Reddit, which relies\non users' self-reported labels. Our benchmark classifiers for bipolar disorder\nprediction outperform the baselines and reach accuracy and F1-scores of above\n86%. Feature analysis shows interesting differences in language use between\nusers with bipolar disorders and the control group, including differences in\nthe use of emotion-expressive words.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 10:55:11 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:45:43 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Sekuli\u0107", "Ivan", ""], ["Gjurkovi\u0107", "Matej", ""], ["\u0160najder", "Jan", ""]]}, {"id": "1811.04670", "submitter": "Arjun Roy", "authors": "Arjun Roy, Kingshuk Basak, Asif Ekbal, Pushpak Bhattacharyya", "title": "A Deep Ensemble Framework for Fake News Detection and Classification", "comments": "6 pages, 1 figure, accepted as a short paper in Web Intelligence 2018\n  (https://webintelligence2018.com/accepted-papers.html), title changed from\n  {\"Going Deep to Detect Liars\" Detecting Fake News using Deep Learning} to {A\n  Deep Ensemble Framework for Fake News Detection and Classification} as per\n  reviewers suggestion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news, rumor, incorrect information, and misinformation detection are\nnowadays crucial issues as these might have serious consequences for our social\nfabrics. The rate of such information is increasing rapidly due to the\navailability of enormous web information sources including social media feeds,\nnews blogs, online newspapers etc.\n  In this paper, we develop various deep learning models for detecting fake\nnews and classifying them into the pre-defined fine-grained categories.\n  At first, we develop models based on Convolutional Neural Network (CNN) and\nBi-directional Long Short Term Memory (Bi-LSTM) networks. The representations\nobtained from these two models are fed into a Multi-layer Perceptron Model\n(MLP) for the final classification. Our experiments on a benchmark dataset show\npromising results with an overall accuracy of 44.87\\%, which outperforms the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 11:40:09 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Roy", "Arjun", ""], ["Basak", "Kingshuk", ""], ["Ekbal", "Asif", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1811.04695", "submitter": "Marcos Zampieri", "authors": "Fernando Benites, Shervin Malmasi, Marcos Zampieri", "title": "Classifying Patent Applications with Ensemble Methods", "comments": "Proceedings of ALTA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for the automatic classification of patent applications\nusing an annotated dataset provided by the organizers of the ALTA 2018 shared\ntask - Classifying Patent Applications. The goal of the task is to use\ncomputational methods to categorize patent applications according to a\ncoarse-grained taxonomy of eight classes based on the International Patent\nClassification (IPC). We tested a variety of approaches for this task and the\nbest results, 0.778 micro-averaged F1-Score, were achieved by SVM ensembles\nusing a combination of words and characters as features. Our team, BMZ, was\nranked first among 14 teams in the competition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 12:49:42 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Benites", "Fernando", ""], ["Malmasi", "Shervin", ""], ["Zampieri", "Marcos", ""]]}, {"id": "1811.04697", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Helcl and Jind\\v{r}ich Libovick\\'y and Du\\v{s}an\n  Vari\\v{s}", "title": "CUNI System for the WMT18 Multimodal Translation Task", "comments": "Published at WMT18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our submission to the WMT18 Multimodal Translation Task. The main\nfeature of our submission is applying a self-attentive network instead of a\nrecurrent neural network. We evaluate two methods of incorporating the visual\nfeatures in the model: first, we include the image representation as another\ninput to the network; second, we train the model to predict the visual features\nand use it as an auxiliary objective. For our submission, we acquired both\ntextual and multimodal additional data. Both of the proposed methods yield\nsignificant improvements over recurrent networks and self-attentive textual\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 12:52:03 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Helcl", "Jind\u0159ich", ""], ["Libovick\u00fd", "Jind\u0159ich", ""], ["Vari\u0161", "Du\u0161an", ""]]}, {"id": "1811.04708", "submitter": "Joanna Rownicka", "authors": "Joanna Rownicka, Peter Bell, Steve Renals", "title": "Analyzing deep CNN-based utterance embeddings for acoustic model\n  adaptation", "comments": "accepted to SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore why deep convolutional neural networks (CNNs) with small\ntwo-dimensional kernels, primarily used for modeling spatial relations in\nimages, are also effective in speech recognition. We analyze the\nrepresentations learned by deep CNNs and compare them with deep neural network\n(DNN) representations and i-vectors, in the context of acoustic model\nadaptation. To explore whether interpretable information can be decoded from\nthe learned representations we evaluate their ability to discriminate between\nspeakers, acoustic conditions, noise type, and gender using the Aurora-4\ndataset. We extract both whole model embeddings (to capture the information\nlearned across the whole network) and layer-specific embeddings which enable\nunderstanding of the flow of information across the network. We also use\nlearned representations as the additional input for a time-delay neural network\n(TDNN) for the Aurora-4 and MGB-3 English datasets. We find that deep CNN\nembeddings outperform DNN embeddings for acoustic model adaptation and\nauxiliary features based on deep CNN embeddings result in similar word error\nrates to i-vectors.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 13:10:25 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Rownicka", "Joanna", ""], ["Bell", "Peter", ""], ["Renals", "Steve", ""]]}, {"id": "1811.04716", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Libovick\\'y and Jind\\v{r}ich Helcl and David Mare\\v{c}ek", "title": "Input Combination Strategies for Multi-Source Transformer Decoder", "comments": "Published at WMT18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-source sequence-to-sequence tasks, the attention mechanism can be\nmodeled in several ways. This topic has been thoroughly studied on recurrent\narchitectures. In this paper, we extend the previous work to the\nencoder-decoder attention in the Transformer architecture. We propose four\ndifferent input combination strategies for the encoder-decoder attention:\nserial, parallel, flat, and hierarchical. We evaluate our methods on tasks of\nmultimodal translation and translation with multiple source languages. The\nexperiments show that the models are able to use multiple sources and improve\nover single source baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 13:33:35 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Helcl", "Jind\u0159ich", ""], ["Mare\u010dek", "David", ""]]}, {"id": "1811.04719", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Libovick\\'y and Jind\\v{r}ich Helcl", "title": "End-to-End Non-Autoregressive Neural Machine Translation with\n  Connectionist Temporal Classification", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive decoding is the only part of sequence-to-sequence models that\nprevents them from massive parallelization at inference time.\nNon-autoregressive models enable the decoder to generate all output symbols\nindependently in parallel. We present a novel non-autoregressive architecture\nbased on connectionist temporal classification and evaluate it on the task of\nneural machine translation. Unlike other non-autoregressive methods which\noperate in several steps, our model can be trained end-to-end. We conduct\nexperiments on the WMT English-Romanian and English-German datasets. Our models\nachieve a significant speedup over the autoregressive models, keeping the\ntranslation quality comparable to other non-autoregressive models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 13:40:04 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Helcl", "Jind\u0159ich", ""]]}, {"id": "1811.04773", "submitter": "Emma Strubell", "authors": "Emma Strubell and Andrew McCallum", "title": "Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a\n  Deep Neural Architecture for SRL?", "comments": "In Proceedings of the Workshop on the Relevance of Linguistic\n  Structure in Neural Architectures for NLP, ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do unsupervised methods for learning rich, contextualized token\nrepresentations obviate the need for explicit modeling of linguistic structure\nin neural network models for semantic role labeling (SRL)? We address this\nquestion by incorporating the massively successful ELMo embeddings (Peters et\nal., 2018) into LISA (Strubell et al., 2018), a strong, linguistically-informed\nneural network architecture for SRL. In experiments on the CoNLL-2005 shared\ntask we find that though ELMo out-performs typical word embeddings, beginning\nto close the gap in F1 between LISA with predicted and gold syntactic parses,\nsyntactically-informed models still out-perform syntax-free models when both\nuse ELMo, especially on out-of-domain data. Our results suggest that linguistic\nstructures are indeed still relevant in this golden age of deep learning for\nNLP.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:16:12 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Strubell", "Emma", ""], ["McCallum", "Andrew", ""]]}, {"id": "1811.04791", "submitter": "Enno Hermann", "authors": "Enno Hermann, Herman Kamper, Sharon Goldwater", "title": "Multilingual and Unsupervised Subword Modeling for Zero-Resource\n  Languages", "comments": "17 pages, 6 figures, 7 tables. Accepted for publication in Computer\n  Speech and Language. arXiv admin note: text overlap with arXiv:1803.08863", "journal-ref": null, "doi": "10.1016/j.csl.2020.101098", "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subword modeling for zero-resource languages aims to learn low-level\nrepresentations of speech audio without using transcriptions or other resources\nfrom the target language (such as text corpora or pronunciation dictionaries).\nA good representation should capture phonetic content and abstract away from\nother types of variability, such as speaker differences and channel noise.\nPrevious work in this area has primarily focused unsupervised learning from\ntarget language data only, and has been evaluated only intrinsically. Here we\ndirectly compare multiple methods, including some that use only target language\nspeech data and some that use transcribed speech from other (non-target)\nlanguages, and we evaluate using two intrinsic measures as well as on a\ndownstream unsupervised word segmentation and clustering task. We find that\ncombining two existing target-language-only methods yields better features than\neither method alone. Nevertheless, even better results are obtained by\nextracting target language bottleneck features using a model trained on other\nlanguages. Cross-lingual training using just one other language is enough to\nprovide this benefit, but multilingual training helps even more. In addition to\nthese results, which hold across both intrinsic measures and the extrinsic\ntask, we discuss the qualitative differences between the different types of\nlearned features.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 11:04:40 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 20:15:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hermann", "Enno", ""], ["Kamper", "Herman", ""], ["Goldwater", "Sharon", ""]]}, {"id": "1811.04860", "submitter": "Genevieve Gorrell", "authors": "Genevieve Gorrell, Xingyi Song, Angus Roberts", "title": "Bio-YODIE: A Named Entity Linking System for Biomedical Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ever-expanding volumes of biomedical text require automated semantic\nannotation techniques to curate and put to best use. An established field of\nresearch seeks to link mentions in text to knowledge bases such as those\nincluded in the UMLS (Unified Medical Language System), in order to enable a\nmore sophisticated understanding. This work has yielded good results for tasks\nsuch as curating literature, but increasingly, annotation systems are more\nbroadly applied. Medical vocabularies are expanding in size, and with them the\nextent of term ambiguity. Document collections are increasing in size and\ncomplexity, creating a greater need for speed and robustness. Furthermore, as\nthe technologies are turned to new tasks, requirements change; for example\ngreater coverage of expressions may be required in order to annotate patient\nrecords, and greater accuracy may be needed for applications that affect\npatients. This places new demands on the approaches currently in use. In this\nwork, we present a new system, Bio-YODIE, and compare it to two other popular\nsystems in order to give guidance about suitable approaches in different\nscenarios and how systems might be designed to accommodate future needs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:06:53 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Gorrell", "Genevieve", ""], ["Song", "Xingyi", ""], ["Roberts", "Angus", ""]]}, {"id": "1811.04884", "submitter": "Tanya Chowdhury", "authors": "Tanya Chowdhury and Tanmoy Chakraborty", "title": "CQASUMM: Building References for Community Question Answering\n  Summarization Corpora", "comments": "Accepted in CODS-COMAD'19 , Jan 3-5, WB, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering forums such as Quora, Stackoverflow are rich\nknowledge resources, often catering to information on topics overlooked by\nmajor search engines. Answers submitted to these forums are often elaborated,\ncontain spam, are marred by slurs and business promotions. It is difficult for\na reader to go through numerous such answers to gauge community opinion. As a\nresult summarization becomes a prioritized task for CQA forums. While a number\nof efforts have been made to summarize factoid CQA, little work exists in\nsummarizing non-factoid CQA. We believe this is due to the lack of a\nconsiderably large, annotated dataset for CQA summarization. We create CQASUMM,\nthe first huge annotated CQA summarization dataset by filtering the 4.4 million\nYahoo! Answers L6 dataset. We sample threads where the best answer can double\nup as a reference summary and build hundred word summaries from them. We treat\nother answers as candidates documents for summarization. We provide a script to\ngenerate the dataset and introduce the new task of Community Question Answering\nSummarization. Multi document summarization has been widely studied with news\narticle datasets, especially in the DUC and TAC challenges using news corpora.\nHowever documents in CQA have higher variance, contradicting opinion and lesser\namount of overlap. We compare the popular multi document summarization\ntechniques and evaluate their performance on our CQA corpora. We look into the\nstate-of-the-art and understand the cases where existing multi document\nsummarizers (MDS) fail. We find that most MDS workflows are built for the\nentirely factual news corpora, whereas our corpus has a fair share of opinion\nbased instances too. We therefore introduce OpinioSumm, a new MDS which\noutperforms the best baseline by 4.6% w.r.t ROUGE-1 score.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:03:45 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chowdhury", "Tanya", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "1811.04890", "submitter": "Zhao Wang", "authors": "Zhao Wang and Aron Culotta", "title": "When do Words Matter? Understanding the Impact of Lexical Choice on\n  Audience Perception using Individual Treatment Effect Estimation", "comments": "AAAI_2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies across many disciplines have shown that lexical choice can affect\naudience perception. For example, how users describe themselves in a social\nmedia profile can affect their perceived socio-economic status. However, we\nlack general methods for estimating the causal effect of lexical choice on the\nperception of a specific sentence. While randomized controlled trials may\nprovide good estimates, they do not scale to the potentially millions of\ncomparisons necessary to consider all lexical choices. Instead, in this paper,\nwe first offer two classes of methods to estimate the effect on perception of\nchanging one word to another in a given sentence. The first class of algorithms\nbuilds upon quasi-experimental designs to estimate individual treatment effects\nfrom observational data. The second class treats treatment effect estimation as\na classification problem. We conduct experiments with three data sources (Yelp,\nTwitter, and Airbnb), finding that the algorithmic estimates align well with\nthose produced by randomized-control trials. Additionally, we find that it is\npossible to transfer treatment effect classifiers across domains and still\nmaintain high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:13:40 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 02:53:17 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 01:25:30 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 03:49:36 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Wang", "Zhao", ""], ["Culotta", "Aron", ""]]}, {"id": "1811.04897", "submitter": "Ruizhi Li", "authors": "Ruizhi Li, Xiaofei Wang, Sri Harish Mallidi, Takaaki Hori, Shinji\n  Watanabe, Hynek Hermansky", "title": "Multi-encoder multi-resolution framework for end-to-end speech\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based methods and Connectionist Temporal Classification (CTC)\nnetwork have been promising research directions for end-to-end Automatic Speech\nRecognition (ASR). The joint CTC/Attention model has achieved great success by\nutilizing both architectures during multi-task training and joint decoding. In\nthis work, we present a novel Multi-Encoder Multi-Resolution (MEMR) framework\nbased on the joint CTC/Attention model. Two heterogeneous encoders with\ndifferent architectures, temporal resolutions and separate CTC networks work in\nparallel to extract complimentary acoustic information. A hierarchical\nattention mechanism is then used to combine the encoder-level information. To\ndemonstrate the effectiveness of the proposed model, experiments are conducted\non Wall Street Journal (WSJ) and CHiME-4, resulting in relative Word Error Rate\n(WER) reduction of 18.0-32.1%. Moreover, the proposed MEMR model achieves 3.6%\nWER in the WSJ eval92 test set, which is the best WER reported for an\nend-to-end system on this benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:33:21 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Li", "Ruizhi", ""], ["Wang", "Xiaofei", ""], ["Mallidi", "Sri Harish", ""], ["Hori", "Takaaki", ""], ["Watanabe", "Shinji", ""], ["Hermansky", "Hynek", ""]]}, {"id": "1811.04903", "submitter": "Xiaofei Wang", "authors": "Xiaofei Wang, Ruizhi Li, Sri Harish Mallid, Takaaki Hori, Shinji\n  Watanabe, Hynek Hermansky", "title": "Stream attention-based multi-array end-to-end speech recognition", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) using multiple microphone arrays has\nachieved great success in the far-field robustness. Taking advantage of all the\ninformation that each array shares and contributes is crucial in this task.\nMotivated by the advances of joint Connectionist Temporal Classification\n(CTC)/attention mechanism in the End-to-End (E2E) ASR, a stream attention-based\nmulti-array framework is proposed in this work. Microphone arrays, acting as\ninformation streams, are activated by separate encoders and decoded under the\ninstruction of both CTC and attention networks. In terms of attention, a\nhierarchical structure is adopted. On top of the regular attention networks,\nstream attention is introduced to steer the decoder toward the most informative\nencoders. Experiments have been conducted on AMI and DIRHA multi-array corpora\nusing the encoder-decoder architecture. Compared with the best single-array\nresults, the proposed framework has achieved relative Word Error Rates (WERs)\nreduction of 3.7% and 9.7% in the two datasets, respectively, which is better\nthan conventional strategies as well.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:42:33 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 21:28:34 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Wang", "Xiaofei", ""], ["Li", "Ruizhi", ""], ["Mallid", "Sri Harish", ""], ["Hori", "Takaaki", ""], ["Watanabe", "Shinji", ""], ["Hermansky", "Hynek", ""]]}, {"id": "1811.04983", "submitter": "Dimitri Kartsaklis", "authors": "Victor Prokhorov, Mohammad Taher Pilehvar, Dimitri Kartsaklis, Pietro\n  Lio, Nigel Collier", "title": "Unseen Word Representation by Aligning Heterogeneous Lexical Semantic\n  Spaces", "comments": "Accepted for presentation at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding techniques heavily rely on the abundance of training data for\nindividual words. Given the Zipfian distribution of words in natural language\ntexts, a large number of words do not usually appear frequently or at all in\nthe training data. In this paper we put forward a technique that exploits the\nknowledge encoded in lexical resources, such as WordNet, to induce embeddings\nfor unseen words. Our approach adapts graph embedding and cross-lingual vector\nspace transformation techniques in order to merge lexical knowledge encoded in\nontologies with that derived from corpus statistics. We show that the approach\ncan provide consistent performance improvements across multiple evaluation\nbenchmarks: in-vitro, on multiple rare word similarity datasets, and in-vivo,\nin two downstream text classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 20:02:00 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Prokhorov", "Victor", ""], ["Pilehvar", "Mohammad Taher", ""], ["Kartsaklis", "Dimitri", ""], ["Lio", "Pietro", ""], ["Collier", "Nigel", ""]]}, {"id": "1811.05013", "submitter": "Ankesh Anand", "authors": "Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron\n  Courville", "title": "Blindfold Baselines for Embodied QA", "comments": "NIPS 2018 Visually-Grounded Interaction and Language (ViGilL)\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:45:41 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Anand", "Ankesh", ""], ["Belilovsky", "Eugene", ""], ["Kastner", "Kyle", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.05021", "submitter": "Yao Wan", "authors": "Yao Wan, Wenqiang Yan, Jianwei Gao, Zhou Zhao, Jian Wu, Philip S. Yu", "title": "Improved Dynamic Memory Network for Dialogue Act Classification with\n  Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue Act (DA) classification is a challenging problem in dialogue\ninterpretation, which aims to attach semantic labels to utterances and\ncharacterize the speaker's intention. Currently, many existing approaches\nformulate the DA classification problem ranging from multi-classification to\nstructured prediction, which suffer from two limitations: a) these methods are\neither handcrafted feature-based or have limited memories. b) adversarial\nexamples can't be correctly classified by traditional training methods. To\naddress these issues, in this paper we first cast the problem into a question\nand answering problem and proposed an improved dynamic memory networks with\nhierarchical pyramidal utterance encoder. Moreover, we apply adversarial\ntraining to train our proposed model. We evaluate our model on two public\ndatasets, i.e., Switchboard dialogue act corpus and the MapTask corpus.\nExtensive experiments show that our proposed model is not only robust, but also\nachieves better performance when compared with some state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:05:25 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Wan", "Yao", ""], ["Yan", "Wenqiang", ""], ["Gao", "Jianwei", ""], ["Zhao", "Zhou", ""], ["Wu", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.05067", "submitter": "Peter Hase", "authors": "John Benhardt, Peter Hase, Liuyi Zhu, Cynthia Rudin", "title": "Shall I Compare Thee to a Machine-Written Sonnet? An Approach to\n  Algorithmic Sonnet Generation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an approach for generating beautiful poetry. Our sonnet-generation\nalgorithm includes several novel elements that improve over the state of the\nart, leading to metrical, rhyming poetry with many human-like qualities. These\nnovel elements include in-line punctuation, part of speech restrictions, and\nmore appropriate training corpora. Our work is the winner of the 2018 PoetiX\nLiterary Turing Test Award for computer-generated poetry.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:04:42 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 22:52:16 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Benhardt", "John", ""], ["Hase", "Peter", ""], ["Zhu", "Liuyi", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1811.05082", "submitter": "Xin Li", "authors": "Xin Li, Lidong Bing, Piji Li, Wai Lam", "title": "A Unified Model for Opinion Target Extraction and Target Sentiment\n  Prediction", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target-based sentiment analysis involves opinion target extraction and target\nsentiment classification. However, most of the existing works usually studied\none of these two sub-tasks alone, which hinders their practical use. This paper\naims to solve the complete task of target-based sentiment analysis in an\nend-to-end fashion, and presents a novel unified model which applies a unified\ntagging scheme. Our framework involves two stacked recurrent neural networks:\nThe upper one predicts the unified tags to produce the final output results of\nthe primary target-based sentiment analysis; The lower one performs an\nauxiliary target boundary prediction aiming at guiding the upper network to\nimprove the performance of the primary task. To explore the inter-task\ndependency, we propose to explicitly model the constrained transitions from\ntarget boundaries to target sentiment polarities. We also propose to maintain\nthe sentiment consistency within an opinion target via a gate mechanism which\nmodels the relation between the features for the current word and the previous\nword. We conduct extensive experiments on three benchmark datasets and our\nframework achieves consistently superior results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 03:04:41 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 14:26:26 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Li", "Xin", ""], ["Bing", "Lidong", ""], ["Li", "Piji", ""], ["Lam", "Wai", ""]]}, {"id": "1811.05085", "submitter": "Wei-Jen Ko", "authors": "Wei-Jen Ko, Greg Durrett and Junyi Jessy Li", "title": "Domain Agnostic Real-Valued Specificity Prediction", "comments": "AAAI 2019 camera ready", "journal-ref": "The AAAI Conference on Artificial Intelligence 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence specificity quantifies the level of detail in a sentence,\ncharacterizing the organization of information in discourse. While this\ninformation is useful for many downstream applications, specificity prediction\nsystems predict very coarse labels (binary or ternary) and are trained on and\ntailored toward specific domains (e.g., news). The goal of this work is to\ngeneralize specificity prediction to domains where no labeled data is available\nand output more nuanced real-valued specificity ratings.\n  We present an unsupervised domain adaptation system for sentence specificity\nprediction, specifically designed to output real-valued estimates from binary\ntraining labels. To calibrate the values of these predictions appropriately, we\nregularize the posterior distribution of the labels towards a reference\ndistribution. We show that our framework generalizes well to three different\ndomains with 50%~68% mean absolute error reduction than the current\nstate-of-the-art system trained for news sentence specificity. We also\ndemonstrate the potential of our work in improving the quality and\ninformativeness of dialogue generation systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 03:16:29 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 19:14:20 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ko", "Wei-Jen", ""], ["Durrett", "Greg", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "1811.05097", "submitter": "Pan Zhou", "authors": "Senmao Wang, Pan Zhou, Wei Chen, Jia Jia, Lei Xie", "title": "Exploring RNN-Transducer for Chinese Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches have drawn much attention recently for significantly\nsimplifying the construction of an automatic speech recognition (ASR) system.\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\nstudies have shown that RNN-T is difficult to train and a very complex training\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\naim to simplify the training process while maintaining performance. First, a\nnew strategy of learning rate decay is proposed to accelerate the model\nconvergence. Second, we find that adding convolutional layers at the beginning\nof the network and using ordered data can discard the pre-training process of\nthe encoder without loss of performance. Besides, we design experiments to find\na balance among the usage of GPU memory, training circle and model performance.\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\n2% absolute improvement from a strong BLSTM CE system with language model\ntrained on the same text corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:37:11 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 03:39:24 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Wang", "Senmao", ""], ["Zhou", "Pan", ""], ["Chen", "Wei", ""], ["Jia", "Jia", ""], ["Xie", "Lei", ""]]}, {"id": "1811.05106", "submitter": "David Keetae Park", "authors": "Sungmin Kang, David Keetae Park, Jaehyuk Chang, Jaegul Choo", "title": "Interpreting Models by Allowing to Ask", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Questions convey information about the questioner, namely what one does not\nknow. In this paper, we propose a novel approach to allow a learning agent to\nask what it considers as tricky to predict, in the course of producing a final\noutput. By analyzing when and what it asks, we can make our model more\ntransparent and interpretable. We first develop this idea to propose a general\nframework of deep neural networks that can ask questions, which we call asking\nnetworks. A specific architecture and training process for an asking network is\nproposed for the task of colorization, which is an exemplar one-to-many task\nand thus a task where asking questions is helpful in performing the task\naccurately. Our results show that the model learns to generate meaningful\nquestions, asks difficult questions first, and utilizes the provided hint more\nefficiently than baseline models. We conclude that the proposed asking\nframework makes the learning agent reveal its weaknesses, which poses a\npromising new direction in developing interpretable and interactive models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:57:48 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kang", "Sungmin", ""], ["Park", "David Keetae", ""], ["Chang", "Jaehyuk", ""], ["Choo", "Jaegul", ""]]}, {"id": "1811.05121", "submitter": "Chang Xu", "authors": "Chang Xu, Weiran Huang, Hongwei Wang, Gang Wang and Tie-Yan Liu", "title": "Modeling Local Dependence in Natural Language with Multi-channel\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been widely used in processing natural\nlanguage tasks and achieve huge success. Traditional RNNs usually treat each\ntoken in a sentence uniformly and equally. However, this may miss the rich\nsemantic structure information of a sentence, which is useful for understanding\nnatural languages. Since semantic structures such as word dependence patterns\nare not parameterized, it is a challenge to capture and leverage structure\ninformation. In this paper, we propose an improved variant of RNN,\nMulti-Channel RNN (MC-RNN), to dynamically capture and leverage local semantic\nstructure information. Concretely, MC-RNN contains multiple channels, each of\nwhich represents a local dependence pattern at a time. An attention mechanism\nis introduced to combine these patterns at each step, according to the semantic\ninformation. Then we parameterize structure information by adaptively selecting\nthe most appropriate connection structures among channels. In this way, diverse\nlocal structures and dependence patterns in sentences can be well captured by\nMC-RNN. To verify the effectiveness of MC-RNN, we conduct extensive experiments\non typical natural language processing tasks, including neural machine\ntranslation, abstractive summarization, and language modeling. Experimental\nresults on these tasks all show significant improvements of MC-RNN over current\ntop systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 06:22:02 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Xu", "Chang", ""], ["Huang", "Weiran", ""], ["Wang", "Hongwei", ""], ["Wang", "Gang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1811.05145", "submitter": "Aditya Joshi", "authors": "Satyajit Kamble and Aditya Joshi", "title": "Hate Speech Detection from Code-mixed Hindi-English Tweets Using Deep\n  Learning Models", "comments": "This paper will appear at the 15th International Conference on\n  Natural Language Processing (ICON-2018) in India in December 2018. ICON is a\n  premier NLP conference in India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports an increment to the state-of-the-art in hate speech\ndetection for English-Hindi code-mixed tweets. We compare three typical deep\nlearning models using domain-specific embeddings. On experimenting with a\nbenchmark dataset of English-Hindi code-mixed tweets, we observe that using\ndomain-specific embeddings results in an improved representation of target\ngroups, and an improved F-score.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 07:49:02 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kamble", "Satyajit", ""], ["Joshi", "Aditya", ""]]}, {"id": "1811.05242", "submitter": "Martino Mensio", "authors": "Martino Mensio, Emanuele Bastianelli, Ilaria Tiddi, Giuseppe Rizzo", "title": "A Multi-layer LSTM-based Approach for Robot Command Interaction Modeling", "comments": "Workshop on Language and Robotics, IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first robotic platforms slowly approach our everyday life, we can\nimagine a near future where service robots will be easily accessible by\nnon-expert users through vocal interfaces. The capability of managing natural\nlanguage would indeed speed up the process of integrating such platform in the\nordinary life. Semantic parsing is a fundamental task of the Natural Language\nUnderstanding process, as it allows extracting the meaning of a user utterance\nto be used by a machine. In this paper, we present a preliminary study to\nsemantically parse user vocal commands for a House Service robot, using a\nmulti-layer Long-Short Term Memory neural network with attention mechanism. The\nsystem is trained on the Human Robot Interaction Corpus, and it is\npreliminarily compared with previous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:10:29 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Mensio", "Martino", ""], ["Bastianelli", "Emanuele", ""], ["Tiddi", "Ilaria", ""], ["Rizzo", "Giuseppe", ""]]}, {"id": "1811.05247", "submitter": "Pan Zhou", "authors": "Ruchao Fan, Pan Zhou, Wei Chen, Jia Jia, Gang Liu", "title": "An Online Attention-based Model for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based end-to-end models such as Listen, Attend and Spell (LAS),\nsimplify the whole pipeline of traditional automatic speech recognition (ASR)\nsystems and become popular in the field of speech recognition. In previous\nwork, researchers have shown that such architectures can acquire comparable\nresults to state-of-the-art ASR systems, especially when using a bidirectional\nencoder and global soft attention (GSA) mechanism. However, bidirectional\nencoder and GSA are two obstacles for real-time speech recognition. In this\nwork, we aim to stream LAS baseline by removing the above two obstacles. On the\nencoder side, we use a latency-controlled (LC) bidirectional structure to\nreduce the delay of forward computation. Meanwhile, an adaptive monotonic\nchunk-wise attention (AMoChA) mechanism is proposed to replace GSA for the\ncalculation of attention weight distribution. Furthermore, we propose two\nmethods to alleviate the huge performance degradation when combining LC and\nAMoChA. Finally, we successfully acquire an online LAS model, LC-AMoChA, which\nhas only 3.5% relative performance reduction to LAS baseline on our internal\nMandarin corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:23:37 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 09:13:17 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Fan", "Ruchao", ""], ["Zhou", "Pan", ""], ["Chen", "Wei", ""], ["Jia", "Jia", ""], ["Liu", "Gang", ""]]}, {"id": "1811.05250", "submitter": "Pan Zhou", "authors": "Pan Zhou, Wenwen Yang, Wei Chen, Yanfeng Wang, Jia Jia", "title": "Modality Attention for End-to-End Audio-visual Speech Recognition", "comments": "accepted by ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual speech recognition (AVSR) system is thought to be one of the\nmost promising solutions for robust speech recognition, especially in noisy\nenvironment. In this paper, we propose a novel multimodal attention based\nmethod for audio-visual speech recognition which could automatically learn the\nfused representation from both modalities based on their importance. Our method\nis realized using state-of-the-art sequence-to-sequence (Seq2seq)\narchitectures. Experimental results show that relative improvements from 2% up\nto 36% over the auditory modality alone are obtained depending on the different\nsignal-to-noise-ratio (SNR). Compared to the traditional feature concatenation\nmethods, our proposed approach can achieve better recognition performance under\nboth clean and noisy conditions. We believe modality attention based end-to-end\nmethod can be easily generalized to other multimodal tasks with correlated\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:28:03 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 04:21:06 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Zhou", "Pan", ""], ["Yang", "Wenwen", ""], ["Chen", "Wei", ""], ["Wang", "Yanfeng", ""], ["Jia", "Jia", ""]]}, {"id": "1811.05270", "submitter": "Rastin Matin", "authors": "Rastin Matin, Casper Hansen, Christian Hansen and Pia M{\\o}lgaard", "title": "Predicting Distresses using Deep Learning of Text Segments in Annual\n  Reports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporate distress models typically only employ the numerical financial\nvariables in the firms' annual reports. We develop a model that employs the\nunstructured textual data in the reports as well, namely the auditors' reports\nand managements' statements. Our model consists of a convolutional recurrent\nneural network which, when concatenated with the numerical financial variables,\nlearns a descriptive representation of the text that is suited for corporate\ndistress prediction. We find that the unstructured data provides a\nstatistically significant enhancement of the distress prediction performance,\nin particular for large firms where accurate predictions are of the utmost\nimportance. Furthermore, we find that auditors' reports are more informative\nthan managements' statements and that a joint model including both managements'\nstatements and auditors' reports displays no enhancement relative to a model\nincluding only auditors' reports. Our model demonstrates a direct improvement\nover existing state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 13:09:58 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Matin", "Rastin", ""], ["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["M\u00f8lgaard", "Pia", ""]]}, {"id": "1811.05303", "submitter": "Denis Lukovnikov", "authors": "Denis Lukovnikov, Nilesh Chakraborty, Jens Lehmann, Asja Fischer", "title": "Translating Natural Language to SQL using Pointer-Generator Networks and\n  How Decoding Order Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating natural language to SQL queries for table-based question\nanswering is a challenging problem and has received significant attention from\nthe research community. In this work, we extend a pointer-generator and\ninvestigate the order-matters problem in semantic parsing for SQL. Even though\nour model is a straightforward extension of a general-purpose\npointer-generator, it outperforms early works for WikiSQL and remains\ncompetitive to concurrently introduced, more complex models. Moreover, we\nprovide a deeper investigation of the potential order-matters problem that\ncould arise due to having multiple correct decoding paths, and investigate the\nuse of REINFORCE as well as a dynamic oracle in this context.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:06:58 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Lukovnikov", "Denis", ""], ["Chakraborty", "Nilesh", ""], ["Lehmann", "Jens", ""], ["Fischer", "Asja", ""]]}, {"id": "1811.05370", "submitter": "Aditya Siddhant", "authors": "Aditya Siddhant, Anuj Goyal, Angeliki Metallinou", "title": "Unsupervised Transfer Learning for Spoken Language Understanding in\n  Intelligent Agents", "comments": "To appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User interaction with voice-powered agents generates large amounts of\nunlabeled utterances. In this paper, we explore techniques to efficiently\ntransfer the knowledge from these unlabeled utterances to improve model\nperformance on Spoken Language Understanding (SLU) tasks. We use Embeddings\nfrom Language Model (ELMo) to take advantage of unlabeled data by learning\ncontextualized word representations. Additionally, we propose ELMo-Light\n(ELMoL), a faster and simpler unsupervised pre-training method for SLU. Our\nfindings suggest unsupervised pre-training on a large corpora of unlabeled\nutterances leads to significantly better SLU performance compared to training\nfrom scratch and it can even outperform conventional supervised transfer.\nAdditionally, we show that the gains from unsupervised transfer techniques can\nbe further improved by supervised transfer. The improvements are more\npronounced in low resource settings and when using only 1000 labeled in-domain\nsamples, our techniques match the performance of training from scratch on\n10-15x more labeled in-domain data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:44:31 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Siddhant", "Aditya", ""], ["Goyal", "Anuj", ""], ["Metallinou", "Angeliki", ""]]}, {"id": "1811.05402", "submitter": "Carsten Eickhoff", "authors": "Xing Wei, Carsten Eickhoff", "title": "Embedding Electronic Health Records for Clinical Information Retrieval", "comments": "Published in AMIA Annual Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network representation learning frameworks have recently shown to be\nhighly effective at a wide range of tasks ranging from radiography\ninterpretation via data-driven diagnostics to clinical decision support. This\noften superior performance comes at the price of dramatically increased\ntraining data requirements that cannot be satisfied in every given institution\nor scenario. As a means of countering such data sparsity effects, distant\nsupervision alleviates the need for scarce in-domain data by relying on a\nrelated, resource-rich, task for training.\n  This study presents an end-to-end neural clinical decision support system\nthat recommends relevant literature for individual patients (few available\nresources) via distant supervision on the well-known MIMIC-III collection\n(abundant resource). Our experiments show significant improvements in retrieval\neffectiveness over traditional statistical as well as purely locally supervised\nretrieval models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:55:52 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Wei", "Xing", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "1811.05408", "submitter": "Abhinav Rastogi", "authors": "Abhinav Rastogi, Raghav Gupta and Dilek Hakkani-Tur", "title": "Multi-task learning for Joint Language Understanding and Dialogue State\n  Tracking", "comments": "Published at SIGdial 2018; 9 pages", "journal-ref": "Proceedings of the 19th Annual SIGdial Meeting on Discourse and\n  Dialogue 2018 (pp. 376-384)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel approach for multi-task learning of language\nunderstanding (LU) and dialogue state tracking (DST) in task-oriented dialogue\nsystems. Multi-task training enables the sharing of the neural network layers\nresponsible for encoding the user utterance for both LU and DST and improves\nperformance while reducing the number of network parameters. In our proposed\nframework, DST operates on a set of candidate values for each slot that has\nbeen mentioned so far. These candidate sets are generated using LU slot\nannotations for the current user utterance, dialogue acts corresponding to the\npreceding system utterance and the dialogue state estimated for the previous\nturn, enabling DST to handle slots with a large or unbounded set of possible\nvalues and deal with slot values not seen during training. Furthermore, to\nbridge the gap between training and inference, we investigate the use of\nscheduled sampling on LU output for the current user utterance as well as the\nDST output for the preceding turn.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:01:50 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Rastogi", "Abhinav", ""], ["Gupta", "Raghav", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1811.05467", "submitter": "Laura Martinus", "authors": "Jade Z. Abbott and Laura Martinus", "title": "Towards Neural Machine Translation for African Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that South African education is in crisis, strategies for improvement\nand sustainability of high-quality, up-to-date education must be explored. In\nthe migration of education online, inclusion of machine translation for\nlow-resourced local languages becomes necessary. This paper aims to spur the\nuse of current neural machine translation (NMT) techniques for low-resourced\nlocal languages. The paper demonstrates state-of-the-art performance on\nEnglish-to-Setswana translation using the Autshumato dataset. The use of the\nTransformer architecture beat previous techniques by 5.33 BLEU points. This\ndemonstrates the promise of using current NMT techniques for African languages.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 06:49:08 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Abbott", "Jade Z.", ""], ["Martinus", "Laura", ""]]}, {"id": "1811.05468", "submitter": "Maximilian Hofer", "authors": "Maximilian Hofer, Andrey Kormilitzin, Paul Goldberg, Alejo\n  Nevado-Holgado", "title": "Few-shot Learning for Named Entity Recognition in Medical Text", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models have recently achieved state-of-the-art\nperformance gains in a variety of natural language processing (NLP) tasks\n(Young, Hazarika, Poria, & Cambria, 2017). However, these gains rely on the\navailability of large amounts of annotated examples, without which\nstate-of-the-art performance is rarely achievable. This is especially\ninconvenient for the many NLP fields where annotated examples are scarce, such\nas medical text. To improve NLP models in this situation, we evaluate five\nimprovements on named entity recognition (NER) tasks when only ten annotated\nexamples are available: (1) layer-wise initialization with pre-trained weights,\n(2) hyperparameter tuning, (3) combining pre-training data, (4) custom word\nembeddings, and (5) optimizing out-of-vocabulary (OOV) words. Experimental\nresults show that the F1 score of 69.3% achievable by state-of-the-art models\ncan be improved to 78.87%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 13:12:02 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hofer", "Maximilian", ""], ["Kormilitzin", "Andrey", ""], ["Goldberg", "Paul", ""], ["Nevado-Holgado", "Alejo", ""]]}, {"id": "1811.05475", "submitter": "Jingcheng Du", "authors": "Jingcheng Du, Qingyu Chen, Yifan Peng, Yang Xiang, Cui Tao, Zhiyong Lu", "title": "ML-Net: multi-label classification of biomedical texts with deep neural\n  networks", "comments": null, "journal-ref": null, "doi": "10.1093/jamia/ocz085", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-label text classification, each textual document can be assigned\nwith one or more labels. Due to this nature, the multi-label text\nclassification task is often considered to be more challenging compared to the\nbinary or multi-class text classification problems. As an important task with\nbroad applications in biomedicine such as assigning diagnosis codes, a number\nof different computational methods (e.g. training and combining binary\nclassifiers for each label) have been proposed in recent years. However, many\nsuffered from modest accuracy and efficiency, with only limited success in\npractical use. We propose ML-Net, a novel deep learning framework, for\nmulti-label classification of biomedical texts. As an end-to-end system, ML-Net\ncombines a label prediction network with an automated label count prediction\nmechanism to output an optimal set of labels by leveraging both predicted\nconfidence score of each label and the contextual information in the target\ndocument. We evaluate ML-Net on three independent, publicly-available corpora\nin two kinds of text genres: biomedical literature and clinical notes. For\nevaluation, example-based measures such as precision, recall and f-measure are\nused. ML-Net is compared with several competitive machine learning baseline\nmodels. Our benchmarking results show that ML-Net compares favorably to the\nstate-of-the-art methods in multi-label classification of biomedical texts.\nML-NET is also shown to be robust when evaluated on different text genres in\nbiomedicine. Unlike traditional machine learning methods, ML-Net does not\nrequire human efforts in feature engineering and is highly efficient and\nscalable approach to tasks with a large set of labels (no need to build\nindividual classifiers for each separate label). Finally, ML-NET is able to\ndynamically estimate the label count based on the document context in a more\nsystematic and accurate manner.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:31:49 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 16:02:52 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Du", "Jingcheng", ""], ["Chen", "Qingyu", ""], ["Peng", "Yifan", ""], ["Xiang", "Yang", ""], ["Tao", "Cui", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1811.05536", "submitter": "Saverio Perugini", "authors": "Brandon M. Williams and Saverio Perugini", "title": "Staging Human-computer Dialogs: An Application of the Futamura\n  Projections", "comments": "13 pages, 6 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.HC cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an application of the Futamura Projections to human-computer\ninteraction, and particularly to staging human-computer dialogs. Specifically,\nby providing staging analogs to the classical Futamura Projections, we\ndemonstrate that the Futamura Projections can be applied to the staging of\nhuman-computer dialogs in addition to the execution of programs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 21:45:20 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Williams", "Brandon M.", ""], ["Perugini", "Saverio", ""]]}, {"id": "1811.05540", "submitter": "Ahmed Nazim Uddin", "authors": "Ahmed Nazim Uddin, Md Ashequr Rahman, Md. Rafidul Islam, Mohammad\n  Ariful Haque", "title": "Native Language Identification using i-vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of determining a speaker's native language based only on his\nspeeches in a second language is known as Native Language Identification or\nNLI. Due to its increasing applications in various domains of speech signal\nprocessing, this has emerged as an important research area in recent times. In\nthis paper we have proposed an i-vector based approach to develop an automatic\nNLI system using MFCC and GFCC features. For evaluation of our approach, we\nhave tested our framework on the 2016 ComParE Native language sub-challenge\ndataset which has English language speakers from 11 different native language\nbackgrounds. Our proposed method outperforms the baseline system with an\nimprovement in accuracy by 21.95% for the MFCC feature based i-vector framework\nand 22.81% for the GFCC feature based i-vector framework.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:12:47 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Uddin", "Ahmed Nazim", ""], ["Rahman", "Md Ashequr", ""], ["Islam", "Md. Rafidul", ""], ["Haque", "Mohammad Ariful", ""]]}, {"id": "1811.05542", "submitter": "Aran Komatsuzaki", "authors": "Aran Komatsuzaki", "title": "Extractive Summary as Discrete Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare various methods to compress a text using a neural\nmodel. We find that extracting tokens as latent variables significantly\noutperforms the state-of-the-art discrete latent variable models such as\nVQ-VAE. Furthermore, we compare various extractive compression schemes. There\nare two best-performing methods that perform equally. One method is to simply\nchoose the tokens with the highest tf-idf scores. Another is to train a\nbidirectional language model similar to ELMo and choose the tokens with the\nhighest loss. If we consider any subsequence of a text to be a text in a\nbroader sense, we conclude that language is a strong compression code of\nitself. Our finding justifies the high quality of generation achieved with\nhierarchical method, as their latent variables are nothing but natural language\nsummary. We also conclude that there is a hierarchy in language such that an\nentire text can be predicted much more easily based on a sequence of a small\nnumber of keywords, which can be easily found by classical methods as tf-idf.\nWe speculate that this extraction process may be useful for unsupervised\nhierarchical text generation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 02:02:18 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 22:56:42 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Komatsuzaki", "Aran", ""]]}, {"id": "1811.05544", "submitter": "Dichao Hu", "authors": "Dichao Hu", "title": "An Introductory Survey on Attention Mechanisms in NLP Problems", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First derived from human intuition, later adapted to machine translation for\nautomatic token alignment, attention mechanism, a simple method that can be\nused for encoding sequence data based on the importance score each element is\nassigned, has been widely applied to and attained significant improvement in\nvarious tasks in natural language processing, including sentiment\nclassification, text summarization, question answering, dependency parsing,\netc. In this paper, we survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP problems, aiming to provide\nour readers with basic knowledge on this widely used method, discuss its\ndifferent variants for different tasks, explore its association with other\ntechniques in machine learning, and examine methods for evaluating its\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:19:22 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hu", "Dichao", ""]]}, {"id": "1811.05546", "submitter": "Mrinmaya Sachan", "authors": "Mrinmaya Sachan and Kumar Avinava Dubey and Eduard H. Hovy and Tom M.\n  Mitchell and Dan Roth and Eric P. Xing", "title": "Discourse in Multimedia: A Case Study in Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure readability, text is often written and presented with due\nformatting. These text formatting devices help the writer to effectively convey\nthe narrative. At the same time, these help the readers pick up the structure\nof the discourse and comprehend the conveyed information. There have been a\nnumber of linguistic theories on discourse structure of text. However, these\ntheories only consider unformatted text. Multimedia text contains rich\nformatting features which can be leveraged for various NLP tasks. In this\npaper, we study some of these discourse features in multimedia text and what\ncommunicative function they fulfil in the context. We examine how these\nmultimedia discourse features can be used to improve an information extraction\nsystem. We show that the discourse and text layout features provide information\nthat is complementary to lexical semantic information commonly used for\ninformation extraction. As a case study, we use these features to harvest\nstructured subject knowledge of geometry from textbooks. We show that the\nharvested structured knowledge can be used to improve an existing solver for\ngeometry problems, making it more accurate as well as more explainable.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:08:39 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Sachan", "Mrinmaya", ""], ["Dubey", "Kumar Avinava", ""], ["Hovy", "Eduard H.", ""], ["Mitchell", "Tom M.", ""], ["Roth", "Dan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1811.05549", "submitter": "Michel Gagnon", "authors": "Gagnon Michel, Zouaq Amal, Aranha Francisco, Ensan Faezeh and\n  Jean-Louis Ludovic", "title": "An Analysis of the Semantic Annotation Task on the Linked Data Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic annotation, the process of identifying key-phrases in texts and\nlinking them to concepts in a knowledge base, is an important basis for\nsemantic information retrieval and the Semantic Web uptake. Despite the\nemergence of semantic annotation systems, very few comparative studies have\nbeen published on their performance. In this paper, we provide an evaluation of\nthe performance of existing systems over three tasks: full semantic annotation,\nnamed entity recognition, and keyword detection. More specifically, the\nspotting capability (recognition of relevant surface forms in text) is\nevaluated for all three tasks, whereas the disambiguation (correctly\nassociating an entity from Wikipedia or DBpedia to the spotted surface forms)\nis evaluated only for the first two tasks. Our evaluation is twofold: First, we\ncompute standard precision and recall on the output of semantic annotators on\ndiverse datasets, each best suited for one of the identified tasks. Second, we\nbuild a statistical model using logistic regression to identify significant\nperformance differences. Our results show that systems that provide full\nannotation perform better than named entities annotators and keyword\nextractors, for all three tasks. However, there is still much room for\nimprovement for the identification of the most relevant entities described in a\ntext.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:24:29 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Michel", "Gagnon", ""], ["Amal", "Zouaq", ""], ["Francisco", "Aranha", ""], ["Faezeh", "Ensan", ""], ["Ludovic", "Jean-Louis", ""]]}, {"id": "1811.05553", "submitter": "Eleanor Chodroff", "authors": "Eleanor Chodroff", "title": "Corpus Phonetics Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corpus phonetics has become an increasingly popular method of research in\nlinguistic analysis. With advances in speech technology and computational\npower, large scale processing of speech data has become a viable technique.\nThis tutorial introduces the speech scientist and engineer to various automatic\nspeech processing tools. These include acoustic model creation and forced\nalignment using the Kaldi Automatic Speech Recognition Toolkit (Povey et al.,\n2011), forced alignment using FAVE-align (Rosenfelder et al., 2014), the\nMontreal Forced Aligner (McAuliffe et al., 2017), and the Penn Phonetics Lab\nForced Aligner (Yuan & Liberman, 2008), as well as stop consonant burst\nalignment using AutoVOT (Keshet et al., 2014). The tutorial provides a general\noverview of each program, step-by-step instructions for running the program, as\nwell as several tips and tricks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:42:10 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chodroff", "Eleanor", ""]]}, {"id": "1811.05563", "submitter": "Liangchen Luo", "authors": "Qi Zeng, Liangchen Luo, Wenhao Huang, Yang Tang", "title": "Text Assisted Insight Ranking Using Context-Aware Memory Network", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting valuable facts or informative summaries from multi-dimensional\ntables, i.e. insight mining, is an important task in data analysis and business\nintelligence. However, ranking the importance of insights remains a challenging\nand unexplored task. The main challenge is that explicitly scoring an insight\nor giving it a rank requires a thorough understanding of the tables and costs a\nlot of manual efforts, which leads to the lack of available training data for\nthe insight ranking problem. In this paper, we propose an insight ranking model\nthat consists of two parts: A neural ranking model explores the data\ncharacteristics, such as the header semantics and the data statistical\nfeatures, and a memory network model introduces table structure and context\ninformation into the ranking process. We also build a dataset with text\nassistance. Experimental results show that our approach largely improves the\nranking precision as reported in multi evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 23:11:26 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zeng", "Qi", ""], ["Luo", "Liangchen", ""], ["Huang", "Wenhao", ""], ["Tang", "Yang", ""]]}, {"id": "1811.05569", "submitter": "Asmelash Teka Hadgu", "authors": "Asmelash Teka Hadgu", "title": "Cross-lingual Short-text Matching with Deep Learning", "comments": "5 pages, CIKM AnlytiCup 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of short text matching is formulated as follows: given a pair of\nsentences or questions, a matching model determines whether the input pair mean\nthe same or not. Models that can automatically identify questions with the same\nmeaning have a wide range of applications in question answering sites and\nmodern chatbots. In this article, we describe the approach by team hahu to\nsolve this problem in the context of the \"CIKM AnalytiCup 2018 - Cross-lingual\nShort-text Matching of Question Pairs\" that is sponsored by Alibaba. Our\nsolution is an end-to-end system based on current advances in deep learning\nwhich avoids heavy feature-engineering and achieves improved performance over\ntraditional machine-learning approaches. The log-loss scores for the first and\nsecond rounds of the contest are 0.35 and 0.39 respectively. The team was\nranked 7th from 1027 teams in the overall ranking scheme by the organizers that\nconsisted of the two contest scores as well as: innovation and system\nintegrity, understanding data as well as practicality of the solution for\nbusiness.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 23:27:06 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hadgu", "Asmelash Teka", ""]]}, {"id": "1811.05616", "submitter": "Shanchan Wu", "authors": "Shanchan Wu, Kai Fan, Qiong Zhang", "title": "Improving Distantly Supervised Relation Extraction with Neural Noise\n  Converter and Conditional Optimal Selector", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervised relation extraction has been successfully applied to large\ncorpus with thousands of relations. However, the inevitable wrong labeling\nproblem by distant supervision will hurt the performance of relation\nextraction. In this paper, we propose a method with neural noise converter to\nalleviate the impact of noisy data, and a conditional optimal selector to make\nproper prediction. Our noise converter learns the structured transition matrix\non logit level and captures the property of distant supervised relation\nextraction dataset. The conditional optimal selector on the other hand helps to\nmake proper prediction decision of an entity pair even if the group of\nsentences is overwhelmed by no-relation sentences. We conduct experiments on a\nwidely used dataset and the results show significant improvement over\ncompetitive baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 03:02:12 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Wu", "Shanchan", ""], ["Fan", "Kai", ""], ["Zhang", "Qiong", ""]]}, {"id": "1811.05632", "submitter": "Lei Wang", "authors": "Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, Xiaojiang Liu", "title": "Translating a Math Word Problem to an Expression Tree", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to\nautomatic math word problem solving. Despite its simplicity, a drawback still\nremains: a math word problem can be correctly solved by more than one\nequations. This non-deterministic transduction harms the performance of maximum\nlikelihood estimation. In this paper, by considering the uniqueness of\nexpression tree, we propose an equation normalization method to normalize the\nduplicated equations. Moreover, we analyze the performance of three popular\nSEQ2SEQ models on the math word problem solving. We find that each model has\nits own specialty in solving problems, consequently an ensemble model is then\nproposed to combine their advantages. Experiments on dataset Math23K show that\nthe ensemble model with equation normalization significantly outperforms the\nprevious state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 04:18:00 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 03:05:27 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Wang", "Lei", ""], ["Wang", "Yan", ""], ["Cai", "Deng", ""], ["Zhang", "Dongxiang", ""], ["Liu", "Xiaojiang", ""]]}, {"id": "1811.05683", "submitter": "Hao Xiong", "authors": "Hao Xiong, Zhongjun He, Hua Wu and Haifeng Wang", "title": "Modeling Coherence for Discourse Neural Machine Translation", "comments": "Accepted by AAAI2019", "journal-ref": "AAAI2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse coherence plays an important role in the translation of one text.\nHowever, the previous reported models most focus on improving performance over\nindividual sentence while ignoring cross-sentence links and dependencies, which\naffects the coherence of the text. In this paper, we propose to use discourse\ncontext and reward to refine the translation quality from the discourse\nperspective. In particular, we generate the translation of individual sentences\nat first. Next, we deliberate the preliminary produced translations, and train\nthe model to learn the policy that produces discourse coherent text by a reward\nteacher. Practical results on multiple discourse test datasets indicate that\nour model significantly improves the translation quality over the\nstate-of-the-art baseline system by +1.23 BLEU score. Moreover, our model\ngenerates more discourse coherent text and obtains +2.2 BLEU improvements when\nevaluated by discourse metrics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:22:27 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Xiong", "Hao", ""], ["He", "Zhongjun", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "1811.05689", "submitter": "Aiqi Jiang", "authors": "Aiqi Jiang, Arkaitz Zubiaga", "title": "Leveraging Aspect Phrase Embeddings for Cross-Domain Review Rating\n  Prediction", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online review platforms are a popular way for users to post reviews by\nexpressing their opinions towards a product or service, as well as they are\nvaluable for other users and companies to find out the overall opinions of\ncustomers. These reviews tend to be accompanied by a rating, where the star\nrating has become the most common approach for users to give their feedback in\na quantitative way, generally as a likert scale of 1-5 stars. In other social\nmedia platforms like Facebook or Twitter, an automated review rating prediction\nsystem can be useful to determine the rating that a user would have given to\nthe product or service. Existing work on review rating prediction focuses on\nspecific domains, such as restaurants or hotels. This, however, ignores the\nfact that some review domains which are less frequently rated, such as\ndentists, lack sufficient data to build a reliable prediction model. In this\npaper, we experiment on 12 datasets pertaining to 12 different review domains\nof varying level of popularity to assess the performance of predictions across\ndifferent domains. We introduce a model that leverages aspect phrase embeddings\nextracted from the reviews, which enables the development of both in-domain and\ncross-domain review rating prediction systems. Our experiments show that both\nof our review rating prediction systems outperform all other baselines. The\ncross-domain review rating prediction system is particularly significant for\nthe least popular review domains, where leveraging training data from other\ndomains leads to remarkable improvements in performance. The in-domain review\nrating prediction system is instead more suitable for popular review domains,\nprovided that a model built from training data pertaining to the target domain\nis more suitable when this data is abundant.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:43:07 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Jiang", "Aiqi", ""], ["Zubiaga", "Arkaitz", ""]]}, {"id": "1811.05696", "submitter": "Jun Gao", "authors": "Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Shuming Shi", "title": "Generating Multiple Diverse Responses for Short-Text Conversation", "comments": "Accepted for publication at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural generative models have become popular and achieved promising\nperformance on short-text conversation tasks. They are generally trained to\nbuild a 1-to-1 mapping from the input post to its output response. However, a\ngiven post is often associated with multiple replies simultaneously in real\napplications. Previous research on this task mainly focuses on improving the\nrelevance and informativeness of the top one generated response for each post.\nVery few works study generating multiple accurate and diverse responses for the\nsame post. In this paper, we propose a novel response generation model, which\nconsiders a set of responses jointly and generates multiple diverse responses\nsimultaneously. A reinforcement learning algorithm is designed to solve our\nmodel. Experiments on two short-text conversation tasks validate that the\nmultiple responses generated by our model obtain higher quality and larger\ndiversity compared with various state-of-the-art generative models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:20:46 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 03:34:30 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 09:19:19 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Gao", "Jun", ""], ["Bi", "Wei", ""], ["Liu", "Xiaojiang", ""], ["Li", "Junhui", ""], ["Shi", "Shuming", ""]]}, {"id": "1811.05701", "submitter": "Lili Yao", "authors": "Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao\n  and Rui Yan", "title": "Plan-And-Write: Towards Better Automatic Storytelling", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic storytelling is challenging since it requires generating long,\ncoherent natural language to describes a sensible sequence of events. Despite\nconsiderable efforts on automatic story generation in the past, prior work\neither is restricted in plot planning, or can only generate stories in a narrow\ndomain. In this paper, we explore open-domain story generation that writes\nstories given a title (topic) as input. We propose a plan-and-write\nhierarchical generation framework that first plans a storyline, and then\ngenerates a story based on the storyline. We compare two planning strategies.\nThe dynamic schema interweaves story planning and its surface realization in\ntext, while the static schema plans out the entire storyline before generating\nstories. Experiments show that with explicit storyline planning, the generated\nstories are more diverse, coherent, and on topic than those generated without\ncreating a full plan, according to both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:37:40 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 02:48:19 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 06:43:42 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Yao", "Lili", ""], ["Peng", "Nanyun", ""], ["Weischedel", "Ralph", ""], ["Knight", "Kevin", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1811.05711", "submitter": "Muhammed Tarik Altuncu", "authors": "M. Tarik Altuncu, Erik Mayer, Sophia N. Yaliraki, Mauricio Barahona", "title": "From Free Text to Clusters of Content in Health Records: An Unsupervised\n  Graph Partitioning Approach", "comments": "25 pages, 2 tables, 8 figures and 5 supplementary figures", "journal-ref": null, "doi": "10.1007/s41109-018-0109-9", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Healthcare records contain large volumes of unstructured data in\ndifferent forms. Free text constitutes a large portion of such data, yet this\nsource of richly detailed information often remains under-used in practice\nbecause of a lack of suitable methodologies to extract interpretable content in\na timely manner. Here we apply network-theoretical tools to the analysis of\nfree text in Hospital Patient Incident reports in the English National Health\nService, to find clusters of reports in an unsupervised manner and at different\nlevels of resolution based directly on the free text descriptions contained\nwithin them. To do so, we combine recently developed deep neural network\ntext-embedding methodologies based on paragraph vectors with multi-scale Markov\nStability community detection applied to a similarity graph of documents\nobtained from sparsified text vector similarities. We showcase the approach\nwith the analysis of incident reports submitted in Imperial College Healthcare\nNHS Trust, London. The multiscale community structure reveals levels of meaning\nwith different resolution in the topics of the dataset, as shown by relevant\ndescriptive terms extracted from the groups of records, as well as by comparing\na posteriori against hand-coded categories assigned by healthcare personnel.\nOur content communities exhibit good correspondence with well-defined\nhand-coded categories, yet our results also provide further medical detail in\ncertain areas as well as revealing complementary descriptors of incidents\nbeyond the external classification. We also discuss how the method can be used\nto monitor reports over time and across different healthcare providers, and to\ndetect emerging trends that fall outside of pre-existing categories.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 10:08:19 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Altuncu", "M. Tarik", ""], ["Mayer", "Erik", ""], ["Yaliraki", "Sophia N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1811.05721", "submitter": "Yufang Hou", "authors": "Yufang Hou", "title": "A Deterministic Algorithm for Bridging Anaphora Resolution", "comments": "11 pages", "journal-ref": "EMNLP 2018", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et\nal., 2013b) use syntactic preposition patterns to calculate word relatedness.\nHowever, such patterns only consider NPs' head nouns and hence do not fully\ncapture the semantics of NPs. Recently, Hou (2018) created word embeddings\n(embeddings_PP) to capture associative similarity (ie, relatedness) between\nnouns by exploring the syntactic structure of noun phrases. But embeddings_PP\nonly contains word representations for nouns. In this paper, we create new word\nvectors by combining embeddings_PP with GloVe. This new word embeddings\n(embeddings_bridging) are a more general lexical knowledge resource for\nbridging and allow us to represent the meaning of an NP beyond its head easily.\nWe therefore develop a deterministic approach for bridging anaphora resolution,\nwhich represents the semantics of an NP based on its head noun and\nmodifications. We show that this simple approach achieves the competitive\nresults compared to the best system in Hou et al.(2013b) which explores Markov\nLogic Networks to model the problem. Additionally, we further improve the\nresults for bridging anaphora resolution reported in Hou (2018) by combining\nour simple deterministic approach with Hou et al.(2013b)'s best system MLN II.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 10:49:39 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hou", "Yufang", ""]]}, {"id": "1811.05740", "submitter": "Besnik Fetahu", "authors": "Christoph Hube and Besnik Fetahu", "title": "Neural Based Statement Classification for Biased Language", "comments": "The Twelfth ACM International Conference on Web Search and Data\n  Mining, February 11--15, 2019, Melbourne, VIC, Australia", "journal-ref": null, "doi": "10.1145/3289600.3291018", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biased language commonly occurs around topics which are of controversial\nnature, thus, stirring disagreement between the different involved parties of a\ndiscussion. This is due to the fact that for language and its use,\nspecifically, the understanding and use of phrases, the stances are cohesive\nwithin the particular groups. However, such cohesiveness does not hold across\ngroups.\n  In collaborative environments or environments where impartial language is\ndesired (e.g. Wikipedia, news media), statements and the language therein\nshould represent equally the involved parties and be neutrally phrased. Biased\nlanguage is introduced through the presence of inflammatory words or phrases,\nor statements that may be incorrect or one-sided, thus violating such\nconsensus.\n  In this work, we focus on the specific case of phrasing bias, which may be\nintroduced through specific inflammatory words or phrases in a statement. For\nthis purpose, we propose an approach that relies on a recurrent neural networks\nin order to capture the inter-dependencies between words in a phrase that\nintroduced bias.\n  We perform a thorough experimental evaluation, where we show the advantages\nof a neural based approach over competitors that rely on word lexicons and\nother hand-crafted features in detecting biased language. We are able to\ndistinguish biased statements with a precision of P=0.92, thus significantly\noutperforming baseline models with an improvement of over 30%. Finally, we\nrelease the largest corpus of statements annotated for biased language.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 11:55:51 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hube", "Christoph", ""], ["Fetahu", "Besnik", ""]]}, {"id": "1811.05757", "submitter": "Tharindu Bandaragoda", "authors": "Tharindu Rukshan Bandaragoda, Daswin De Silva, Damminda Alahakoon", "title": "Automatic event detection in microblogs using incremental machine\n  learning", "comments": null, "journal-ref": "Journal of the Association for Information Science and Technology,\n  2017, 68(10), 2394-2411", "doi": "10.1002/asi.23896", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global popularity of microblogs has led to an increasing accumulation of\nlarge volumes of text data on microblogging platforms such as Twitter. These\ncorpora are untapped resources to understand social expressions on diverse\nsubjects. Microblog analysis aims to unlock the value of such expressions by\ndiscovering insights and events of significance hidden among swathes of text.\nBesides velocity; diversity of content, brevity, absence of structure and\ntime-sensitivity are key challenges in microblog analysis. In this paper, we\npropose an unsupervised incremental machine learning and event detection\ntechnique to address these challenges. The proposed technique separates a\nmicroblog discussion into topics to address the key problem of diversity. It\nmaintains a record of the evolution of each topic over time. Brevity,\ntime-sensitivity and unstructured nature are addressed by these individual\ntopic pathways which contribute to generate a temporal, topic-driven structure\nof a microblog discussion. The proposed event detection method continuously\nmonitors these topic pathways using multiple domain-independent event\nindicators for events of significance. The autonomous nature of topic\nseparation, topic pathway generation, new topic identification and event\ndetection, appropriates the proposed technique for extensive applications in\nmicroblog analysis. We demonstrate these capabilities on tweets containing\n#microsoft and tweets containing #obama.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 21:56:25 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Bandaragoda", "Tharindu Rukshan", ""], ["De Silva", "Daswin", ""], ["Alahakoon", "Damminda", ""]]}, {"id": "1811.05760", "submitter": "Aniruddha Bhattacharya", "authors": "Aniruddha Bhattacharya and K.V. Kadambari", "title": "A Multimodal Approach towards Emotion Recognition of Music using Audio\n  and Lyrical Content", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MoodNet - A Deep Convolutional Neural Network based architecture\nto effectively predict the emotion associated with a piece of music given its\naudio and lyrical content.We evaluate different architectures consisting of\nvarying number of two-dimensional convolutional and subsampling layers,followed\nby dense layers.We use Mel-Spectrograms to represent the audio content and word\nembeddings-specifically 100 dimensional word vectors, to represent the textual\ncontent represented by the lyrics.We feed input data from both modalities to\nour MoodNet architecture.The output from both the modalities are then fused as\na fully connected layer and softmax classfier is used to predict the category\nof emotion.Using F1-score as our metric,our results show excellent performance\nof MoodNet over the two datasets we experimented on-The MIREX Multimodal\ndataset and the Million Song Dataset.Our experiments reflect the hypothesis\nthat more complex models perform better with more training data.We also observe\nthat lyrics outperform audio as a better expressed modality and conclude that\ncombining and using features from multiple modalities for prediction tasks\nresult in superior performance in comparison to using a single modality as\ninput.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 20:51:03 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Bhattacharya", "Aniruddha", ""], ["Kadambari", "K. V.", ""]]}, {"id": "1811.05768", "submitter": "Shaun D'Souza", "authors": "Shaun D'Souza", "title": "Parser Extraction of Triples in Unstructured Text", "comments": null, "journal-ref": "IAES International Journal of Artificial Intelligence (IJ-AI),\n  5(4):143-148, 2017", "doi": "10.11591/ij-ai.v5.i4.pp143-148", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web contains vast repositories of unstructured text. We investigate the\nopportunity for building a knowledge graph from these text sources. We generate\na set of triples which can be used in knowledge gathering and integration. We\ndefine the architecture of a language compiler for processing\nsubject-predicate-object triples using the OpenNLP parser. We implement a\ndepth-first search traversal on the POS tagged syntactic tree appending\npredicate and object information. A parser enables higher precision and higher\nrecall extractions of syntactic relationships across conjunction boundaries. We\nare able to extract 2-2.5 times the correct extractions of ReVerb. The\nextractions are used in a variety of semantic web applications and question\nanswering. We verify extraction of 50,000 triples on the ClueWeb dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:12:00 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["D'Souza", "Shaun", ""]]}, {"id": "1811.05770", "submitter": "EPTCS", "authors": "Bob Coecke (University of Oxford), Martha Lewis (University of\n  Amsterdam), Dan Marsden (University of Oxford)", "title": "Internal Wiring of Cartesian Verbs and Prepositions", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 75-88", "doi": "10.4204/EPTCS.283.6", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical compositional distributional semantics (CCDS) allows one to\ncompute the meaning of phrases and sentences from the meaning of their\nconstituent words. A type-structure carried over from the traditional\ncategorial model of grammar a la Lambek becomes a 'wire-structure' that\nmediates the interaction of word meanings. However, CCDS has a much richer\nlogical structure than plain categorical semantics in that certain words can\nalso be given an 'internal wiring' that either provides their entire meaning or\nreduces the size their meaning space. Previous examples of internal wiring\ninclude relative pronouns and intersective adjectives. Here we establish the\nsame for a large class of well-behaved transitive verbs to which we refer as\nCartesian verbs, and reduce the meaning space from a ternary tensor to a unary\none. Some experimental evidence is also provided.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:11:59 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Coecke", "Bob", "", "University of Oxford"], ["Lewis", "Martha", "", "University of\n  Amsterdam"], ["Marsden", "Dan", "", "University of Oxford"]]}, {"id": "1811.05825", "submitter": "Chang Su", "authors": "Su Chang, Xu Zhenzhong, Gao Xuan", "title": "Fake Comment Detection Based on Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the E-commerce and reviews website, the comment\ninformation is influencing people's life. More and more users share their\nconsumption experience and evaluate the quality of commodity by comment. When\npeople make a decision, they will refer these comments. The dependency of the\ncomments make the fake comment appear. The fake comment is that for profit and\nother bad motivation, business fabricate untrue consumption experience and they\npreach or slander some products. The fake comment is easy to mislead users'\nopinion and decision. The accuracy of humans identifying fake comment is low.\nIt's meaningful to detect fake comment using natural language processing\ntechnology for people getting true comment information. This paper uses the\nsentimental analysis to detect fake comment.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 11:28:19 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chang", "Su", ""], ["Zhenzhong", "Xu", ""], ["Xuan", "Gao", ""]]}, {"id": "1811.05826", "submitter": "Shubham Agarwal", "authors": "Shubham Agarwal, Marc Dymetman and Eric Gaussier", "title": "Char2char Generation with Reranking for the E2E NLG Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to the E2E NLG Challenge. Recently,\nneural seq2seq approaches have become mainstream in NLG, often resorting to\npre- (respectively post-) processing delexicalization (relexicalization) steps\nat the word-level to handle rare words. By contrast, we train a simple\ncharacter level seq2seq model, which requires no pre/post-processing\n(delexicalization, tokenization or even lowercasing), with surprisingly good\nresults. For further improvement, we explore two re-ranking approaches for\nscoring candidates. We also introduce a synthetic dataset creation procedure,\nwhich opens up a new way of creating artificial datasets for Natural Language\nGeneration.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 22:56:50 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Agarwal", "Shubham", ""], ["Dymetman", "Marc", ""], ["Gaussier", "Eric", ""]]}, {"id": "1811.05827", "submitter": "Vincent Cheutet", "authors": "Haiqing Zhang (DISP), Aicha Sekhari (DISP), Yacine Ouzrout (DISP),\n  Abdelaziz Bouras (DISP)", "title": "Jointly identifying opinion mining elements and fuzzy measurement of\n  opinion intensity to analyze product features", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence, Elsevier,\n  2016, 47, pp.122--139", "doi": "10.1016/j.engappai.2015.06.007", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining mainly involves three elements: feature and feature-of\nrelations, opinion expressions and the related opinion attributes (e.g.\nPolarity), and feature-opinion relations. Although many works have emerged to\nachieve its aim of gaining information, the previous researches typically\nhandled each of the three elements in isolation, which cannot give sufficient\ninformation extraction results; hence, the complexity and the running time of\ninformation extraction is increased. In this paper, we propose an opinion\nmining extraction algorithm to jointly discover the main opinion mining\nelements. Specifically, the algorithm automatically builds kernels to combine\nclosely related words into new terms from word level to phrase level based on\ndependency relations; and we ensure the accuracy of opinion expressions and\npolarity based on: fuzzy measurements, opinion degree intensifiers, and opinion\npatterns. The 3458 analyzed reviews show that the proposed algorithm can\neffectively identify the main elements simultaneously and outperform the\nbaseline methods. The proposed algorithm is used to analyze the features among\nheterogeneous products in the same category. The feature-by-feature comparison\ncan help to select the weaker features and recommend the correct specifications\nfrom the beginning life of a product. From this comparison, some interesting\nobservations are revealed. For example, the negative polarity of video\ndimension is higher than the product usability dimension for a product. Yet,\nenhancing the dimension of product usability can more effectively improve the\nproduct (C) 2015 Elsevier Ltd. All rights reserved.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:27:24 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zhang", "Haiqing", "", "DISP"], ["Sekhari", "Aicha", "", "DISP"], ["Ouzrout", "Yacine", "", "DISP"], ["Bouras", "Abdelaziz", "", "DISP"]]}, {"id": "1811.05889", "submitter": "Bowen Li", "authors": "Bowen Li, Jianpeng Cheng, Yang Liu and Frank Keller", "title": "Dependency Grammar Induction with a Neural Variational Transition-based\n  Parser", "comments": "AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency grammar induction is the task of learning dependency syntax\nwithout annotated training data. Traditional graph-based models with global\ninference achieve state-of-the-art results on this task but they require\n$O(n^3)$ run time. Transition-based models enable faster inference with $O(n)$\ntime complexity, but their performance still lags behind. In this work, we\npropose a neural transition-based parser for dependency grammar induction,\nwhose inference procedure utilizes rich neural features with $O(n)$ time\ncomplexity. We train the parser with an integration of variational inference,\nposterior regularization and variance reduction techniques. The resulting\nframework outperforms previous unsupervised transition-based dependency parsers\nand achieves performance comparable to graph-based models, both on the English\nPenn Treebank and on the Universal Dependency Treebank. In an empirical\ncomparison, we show that our approach substantially increases parsing speed\nover graph-based models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 16:30:22 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Li", "Bowen", ""], ["Cheng", "Jianpeng", ""], ["Liu", "Yang", ""], ["Keller", "Frank", ""]]}, {"id": "1811.05909", "submitter": "Alberto Poncelas", "authors": "Alberto Poncelas, Andy Way, Kepa Sarasola", "title": "The ADAPT System Description for the IWSLT 2018 Basque to English\n  Translation Task", "comments": null, "journal-ref": "Proceedings of the 15th International Workshop on Spoken Language\n  Translation (2018) 76-82", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the ADAPT system built for the Basque to English Low\nResource MT Evaluation Campaign. Basque is a low-resourced,\nmorphologically-rich language. This poses a challenge for Neural Machine\nTranslation models which usually achieve better performance when trained with\nlarge sets of data.\n  Accordingly, we used synthetic data to improve the translation quality\nproduced by a model built using only authentic data. Our proposal uses\nback-translated data to: (a) create new sentences, so the system can be trained\nwith more data; and (b) translate sentences that are close to the test set, so\nthe model can be fine-tuned to the document to be translated.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:06:33 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Poncelas", "Alberto", ""], ["Way", "Andy", ""], ["Sarasola", "Kepa", ""]]}, {"id": "1811.05949", "submitter": "Marek Rei", "authors": "Marek Rei, Anders S{\\o}gaard", "title": "Jointly Learning to Label Sentences and Tokens", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to construct text representations in end-to-end systems can be\ndifficult, as natural languages are highly compositional and task-specific\nannotated datasets are often limited in size. Methods for directly supervising\nlanguage composition can allow us to guide the models based on existing\nknowledge, regularizing them towards more robust and interpretable\nrepresentations. In this paper, we investigate how objectives at different\ngranularities can be used to learn better language representations and we\npropose an architecture for jointly learning to label sentences and tokens. The\npredictions at each level are combined together using an attention mechanism,\nwith token-level labels also acting as explicit supervision for composing\nsentence-level representations. Our experiments show that by learning to\nperform these tasks jointly on multiple levels, the model achieves substantial\nimprovements for both sentence classification and sequence labeling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:32:18 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Rei", "Marek", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1811.06031", "submitter": "Victor Sanh", "authors": "Victor Sanh, Thomas Wolf and Sebastian Ruder", "title": "A Hierarchical Multi-task Approach for Learning Embeddings from Semantic\n  Tasks", "comments": "8 pages, 1 figure, To appear in Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much effort has been devoted to evaluate whether multi-task learning can be\nleveraged to learn rich representations that can be used in various Natural\nLanguage Processing (NLP) down-stream applications. However, there is still a\nlack of understanding of the settings in which multi-task learning has a\nsignificant effect. In this work, we introduce a hierarchical model trained in\na multi-task learning setup on a set of carefully selected semantic tasks. The\nmodel is trained in a hierarchical fashion to introduce an inductive bias by\nsupervising a set of low level tasks at the bottom layers of the model and more\ncomplex tasks at the top layers of the model. This model achieves\nstate-of-the-art results on a number of tasks, namely Named Entity Recognition,\nEntity Mention Detection and Relation Extraction without hand-engineered\nfeatures or external NLP tools like syntactic parsers. The hierarchical\ntraining supervision induces a set of shared semantic representations at lower\nlayers of the model. We show that as we move from the bottom to the top layers\nof the model, the hidden states of the layers tend to represent more complex\nsemantic information.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:42:03 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 06:15:05 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Sanh", "Victor", ""], ["Wolf", "Thomas", ""], ["Ruder", "Sebastian", ""]]}, {"id": "1811.06096", "submitter": "Anusha Lalitha", "authors": "Yang Yang, Anusha Lalitha, Jinwon Lee, Chris Lott", "title": "Automatic Grammar Augmentation for Robust Voice Command Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel pipeline for automatic grammar augmentation that\nprovides a significant improvement in the voice command recognition accuracy\nfor systems with small footprint acoustic model (AM). The improvement is\nachieved by augmenting the user-defined voice command set, also called grammar\nset, with alternate grammar expressions. For a given grammar set, a set of\npotential grammar expressions (candidate set) for augmentation is constructed\nfrom an AM-specific statistical pronunciation dictionary that captures the\nconsistent patterns and errors in the decoding of AM induced by variations in\npronunciation, pitch, tempo, accent, ambiguous spellings, and noise conditions.\nUsing this candidate set, greedy optimization based and cross-entropy-method\n(CEM) based algorithms are considered to search for an augmented grammar set\nwith improved recognition accuracy utilizing a command-specific dataset. Our\nexperiments show that the proposed pipeline along with algorithms considered in\nthis paper significantly reduce the mis-detection and mis-classification rate\nwithout increasing the false-alarm rate. Experiments also demonstrate the\nconsistent superior performance of CEM method over greedy-based algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:19:03 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Yang", "Yang", ""], ["Lalitha", "Anusha", ""], ["Lee", "Jinwon", ""], ["Lott", "Chris", ""]]}, {"id": "1811.06156", "submitter": "Yu Hao", "authors": "Yu Hao, Xien Liu, Ji Wu, Ping Lv", "title": "Exploiting Sentence Embedding for Medical Question Answering", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of word embedding, sentence embedding remains a\nnot-well-solved problem. In this paper, we present a supervised learning\nframework to exploit sentence embedding for the medical question answering\ntask. The learning framework consists of two main parts: 1) a sentence\nembedding producing module, and 2) a scoring module. The former is developed\nwith contextual self-attention and multi-scale techniques to encode a sentence\ninto an embedding tensor. This module is shortly called Contextual\nself-Attention Multi-scale Sentence Embedding (CAMSE). The latter employs two\nscoring strategies: Semantic Matching Scoring (SMS) and Semantic Association\nScoring (SAS). SMS measures similarity while SAS captures association between\nsentence pairs: a medical question concatenated with a candidate choice, and a\npiece of corresponding supportive evidence. The proposed framework is examined\nby two Medical Question Answering(MedicalQA) datasets which are collected from\nreal-world applications: medical exam and clinical diagnosis based on\nelectronic medical records (EMR). The comparison results show that our proposed\nframework achieved significant improvements compared to competitive baseline\napproaches. Additionally, a series of controlled experiments are also conducted\nto illustrate that the multi-scale strategy and the contextual self-attention\nlayer play important roles for producing effective sentence embedding, and the\ntwo kinds of scoring strategies are highly complementary to each other for\nquestion answering problems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 03:38:20 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Hao", "Yu", ""], ["Liu", "Xien", ""], ["Wu", "Ji", ""], ["Lv", "Ping", ""]]}, {"id": "1811.06179", "submitter": "Yuan Luo", "authors": "Yuan Luo, Peter Szolovits", "title": "Implementing a Portable Clinical NLP System with a Common Data Model - a\n  Lisp Perspective", "comments": "6 pages, accepted by IEEE BIBM 2018 as regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Lisp architecture for a portable NLP system, termed\nLAPNLP, for processing clinical notes. LAPNLP integrates multiple standard,\ncustomized and in-house developed NLP tools. Our system facilitates portability\nacross different institutions and data systems by incorporating an enriched\nCommon Data Model (CDM) to standardize necessary data elements. It utilizes\nUMLS to perform domain adaptation when integrating generic domain NLP tools. It\nalso features stand-off annotations that are specified by positional reference\nto the original document. We built an interval tree based search engine to\nefficiently query and retrieve the stand-off annotations by specifying\npositional requirements. We also developed a utility to convert an inline\nannotation format to stand-off annotations to enable the reuse of clinical text\ndatasets with inline annotations. We experimented with our system on several\nNLP facilitated tasks including computational phenotyping for lymphoma patients\nand semantic relation extraction for clinical notes. These experiments\nshowcased the broader applicability and utility of LAPNLP.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 04:58:21 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Luo", "Yuan", ""], ["Szolovits", "Peter", ""]]}, {"id": "1811.06183", "submitter": "Yuan Luo", "authors": "Yizhen Zhong, Luke Rasmussen, Yu Deng, Jennifer Pacheco, Maureen\n  Smith, Justin Starren, Wei-Qi Wei, Peter Speltz, Joshua Denny, Nephi Walton,\n  George Hripcsak, Christopher G Chute, Yuan Luo", "title": "Characterizing Design Patterns of EHR-Driven Phenotype Extraction\n  Algorithms", "comments": "4 pages, accepted by IEEE BIBM 2018 as short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic development of phenotype algorithms from Electronic Health\nRecord data with machine learning (ML) techniques is of great interest given\nthe current practice is very time-consuming and resource intensive. The\nextraction of design patterns from phenotype algorithms is essential to\nunderstand their rationale and standard, with great potential to automate the\ndevelopment process. In this pilot study, we perform network visualization on\nthe design patterns and their associations with phenotypes and sites. We\nclassify design patterns using the fragments from previously annotated\nphenotype algorithms as the ground truth. The classification performance is\nused as a proxy for coherence at the attribution level. The bag-of-words\nrepresentation with knowledge-based features generated a good performance in\nthe classification task (0.79 macro-f1 scores). Good classification accuracy\nwith simple features demonstrated the attribution coherence and the feasibility\nof automatic identification of design patterns. Our results point to both the\nfeasibility and challenges of automatic identification of phenotyping design\npatterns, which would power the automatic development of phenotype algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 05:12:33 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhong", "Yizhen", ""], ["Rasmussen", "Luke", ""], ["Deng", "Yu", ""], ["Pacheco", "Jennifer", ""], ["Smith", "Maureen", ""], ["Starren", "Justin", ""], ["Wei", "Wei-Qi", ""], ["Speltz", "Peter", ""], ["Denny", "Joshua", ""], ["Walton", "Nephi", ""], ["Hripcsak", "George", ""], ["Chute", "Christopher G", ""], ["Luo", "Yuan", ""]]}, {"id": "1811.06203", "submitter": "Masashi Yoshikawa", "authors": "Masashi Yoshikawa, Koji Mineshima, Hiroshi Noji, Daisuke Bekki", "title": "Combining Axiom Injection and Knowledge Base Completion for Efficient\n  Natural Language Inference", "comments": "9 pages, accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In logic-based approaches to reasoning tasks such as Recognizing Textual\nEntailment (RTE), it is important for a system to have a large amount of\nknowledge data. However, there is a tradeoff between adding more knowledge data\nfor improved RTE performance and maintaining an efficient RTE system, as such a\nbig database is problematic in terms of the memory usage and computational\ncomplexity. In this work, we show the processing time of a state-of-the-art\nlogic-based RTE system can be significantly reduced by replacing its\nsearch-based axiom injection (abduction) mechanism by that based on Knowledge\nBase Completion (KBC). We integrate this mechanism in a Coq plugin that\nprovides a proof automation tactic for natural language inference.\nAdditionally, we show empirically that adding new knowledge data contributes to\nbetter RTE performance while not harming the processing speed in this\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 06:40:39 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Yoshikawa", "Masashi", ""], ["Mineshima", "Koji", ""], ["Noji", "Hiroshi", ""], ["Bekki", "Daisuke", ""]]}, {"id": "1811.06278", "submitter": "Nina Tahmasebi", "authors": "Nina Tahmasebi, Lars Borin and Adam Jatowt", "title": "Survey of Computational Approaches to Lexical Semantic Change", "comments": "This survey is an extended version of Survey of Computational\n  Approaches to Diachronic Conceptual Change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our languages are in constant flux driven by external factors such as\ncultural, societal and technological changes, as well as by only partially\nunderstood internal motivations. Words acquire new meanings and lose old\nsenses, new words are coined or borrowed from other languages and obsolete\nwords slide into obscurity. Understanding the characteristics of shifts in the\nmeaning and in the use of words is useful for those who work with the content\nof historical texts, the interested general public, but also in and of itself.\nThe findings from automatic lexical semantic change detection, and the models\nof diachronic conceptual change are currently being incorporated in approaches\nfor measuring document across-time similarity, information retrieval from\nlong-term document archives, the design of OCR algorithms, and so on. In recent\nyears we have seen a surge in interest in the academic community in\ncomputational methods and tools supporting inquiry into diachronic conceptual\nchange and lexical replacement. This article is an extract of a survey of\nrecent computational techniques to tackle lexical semantic change currently\nunder review. In this article we focus on diachronic conceptual change as an\nextension of semantic change.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:26:16 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 08:14:49 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Tahmasebi", "Nina", ""], ["Borin", "Lars", ""], ["Jatowt", "Adam", ""]]}, {"id": "1811.06303", "submitter": "Paul Groth", "authors": "Paul Groth and Antony Scerri and Ron Daniel, Jr. and Bradley P. Allen", "title": "End-to-End Learning for Answering Structured Queries Directly over Text", "comments": "18 pages, 6 figures", "journal-ref": "Proceedings of the Workshop on Deep Learning for Knowledge Graphs\n  (DL4KG2019)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured queries expressed in languages (such as SQL, SPARQL, or XQuery)\noffer a convenient and explicit way for users to express their information\nneeds for a number of tasks. In this work, we present an approach to answer\nthese directly over text data without storing results in a database. We\nspecifically look at the case of knowledge bases where queries are over\nentities and the relations between them. Our approach combines distributed\nquery answering (e.g. Triple Pattern Fragments) with models built for\nextractive question answering. Importantly, by applying distributed querying\nanswering we are able to simplify the model learning problem. We train models\nfor a large portion (572) of the relations within Wikidata and achieve an\naverage 0.70 F1 measure across all models. We also present a systematic method\nto construct the necessary training data for this task from knowledge graphs\nand describe a prototype implementation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 11:46:14 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 13:10:02 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Groth", "Paul", ""], ["Scerri", "Antony", ""], ["Daniel,", "Ron", "Jr."], ["Allen", "Bradley P.", ""]]}, {"id": "1811.06315", "submitter": "Javier Latorre", "authors": "Javier Latorre, Jakub Lachowicz, Jaime Lorenzo-Trueba, Thomas Merritt,\n  Thomas Drugman, Srikanth Ronanki, Klimkov Viacheslav", "title": "Effect of data reduction on sequence-to-sequence neural TTS", "comments": "4 pages, 1 extra for references. Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent speech synthesis systems based on sampling from autoregressive neural\nnetworks models can generate speech almost undistinguishable from human\nrecordings. However, these models require large amounts of data. This paper\nshows that the lack of data from one speaker can be compensated with data from\nother speakers. The naturalness of Tacotron2-like models trained on a blend of\n5k utterances from 7 speakers is better than that of speaker dependent models\ntrained on 15k utterances, but in terms of stability multi-speaker models are\nalways more stable. We also demonstrate that models mixing only 1250 utterances\nfrom a target speaker with 5k utterances from another 6 speakers can produce\nsignificantly better quality than state-of-the-art DNN-guided unit selection\nsystems trained on more than 10 times the data from the target speaker.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 12:31:12 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 13:53:58 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Latorre", "Javier", ""], ["Lachowicz", "Jakub", ""], ["Lorenzo-Trueba", "Jaime", ""], ["Merritt", "Thomas", ""], ["Drugman", "Thomas", ""], ["Ronanki", "Srikanth", ""], ["Viacheslav", "Klimkov", ""]]}, {"id": "1811.06439", "submitter": "David Ramsay", "authors": "Ishwarya Ananthabhotla, David B. Ramsay, and Joseph A. Paradiso", "title": "HCU400: An Annotated Dataset for Exploring Aural Phenomenology Through\n  Causal Uncertainty", "comments": null, "journal-ref": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP). IEEE, 2019", "doi": "10.1109/ICASSP.2019.8683147", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way we perceive a sound depends on many aspects-- its ecological\nfrequency, acoustic features, typicality, and most notably, its identified\nsource. In this paper, we present the HCU400: a dataset of 402 sounds ranging\nfrom easily identifiable everyday sounds to intentionally obscured artificial\nones. It aims to lower the barrier for the study of aural phenomenology as the\nlargest available audio dataset to include an analysis of causal attribution.\nEach sample has been annotated with crowd-sourced descriptions, as well as\nfamiliarity, imageability, arousal, and valence ratings. We extend existing\ncalculations of causal uncertainty, automating and generalizing them with word\nembeddings. Upon analysis we find that individuals will provide less polarized\nemotion ratings as a sound's source becomes increasingly ambiguous; individual\nratings of familiarity and imageability, on the other hand, diverge as\nuncertainty increases despite a clear negative trend on average.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:50:29 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 03:35:50 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Ananthabhotla", "Ishwarya", ""], ["Ramsay", "David B.", ""], ["Paradiso", "Joseph A.", ""]]}, {"id": "1811.06477", "submitter": "Thomas Cherian", "authors": "Thomas Cherian, Akshay Badola and Vineet Padmanabhan", "title": "Multi-cell LSTM Based Neural Language Model", "comments": "7 pages including 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models, being at the heart of many NLP problems, are always of great\ninterest to researchers. Neural language models come with the advantage of\ndistributed representations and long range contexts. With its particular\ndynamics that allow the cycling of information within the network, `Recurrent\nneural network' (RNN) becomes an ideal paradigm for neural language modeling.\nLong Short-Term Memory (LSTM) architecture solves the inadequacies of the\nstandard RNN in modeling long-range contexts. In spite of a plethora of RNN\nvariants, possibility to add multiple memory cells in LSTM nodes was seldom\nexplored. Here we propose a multi-cell node architecture for LSTMs and study\nits applicability for neural language modeling. The proposed multi-cell LSTM\nlanguage models outperform the state-of-the-art results on well-known Penn\nTreebank (PTB) setup.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:09:53 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Cherian", "Thomas", ""], ["Badola", "Akshay", ""], ["Padmanabhan", "Vineet", ""]]}, {"id": "1811.06567", "submitter": "Chandra Yadav Shekhar", "authors": "Chandra Shekhar Yadav", "title": "Automatic Text Document Summarization using Semantic-based Analysis", "comments": "six chapters, 32 figures, 25 tables, 167 pages, phd thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of the web, the amount of data on wen has been increased\nseveral million folds. In recent years web data generated is more than data\nstored for years. One important data format is text. To answer user queries\nover the internet, and to overcome the problem of information overload one\npossible solution is text document summarization. This not only reduces query\naccess time, but also optimize the document results according to specific users\nrequirements. Summarization of text document can be categorized as abstractive\nand extractive. Most of the work has been done in the direction of Extractive\nsummarization. Extractive summarized result is a subset of original documents\nwith the objective of more content coverage and lea redundancy. Our work is\nbased on Extractive approaches. In the first approach, we are using some\nstatistical features and semantic-based features. To include sentiment as a\nfeature is an idea cached from a view that emotion plays an important role. It\neffectively conveys a message. So, it may play a vital role in text document\nsummarization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 19:29:26 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Yadav", "Chandra Shekhar", ""]]}, {"id": "1811.06596", "submitter": "M. Shazan Mohomed Jabbar", "authors": "Mohomed Shazan Mohomed Jabbar, Luke Kumar, Hamman Samuel, Mi-Young\n  Kim, Sankalp Prabhakar, Randy Goebel, Osmar Za\\\"iane", "title": "On Generality and Knowledge Transferability in Cross-Domain Duplicate\n  Question Detection for Heterogeneous Community Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Duplicate question detection is an ongoing challenge in community question\nanswering because semantically equivalent questions can have significantly\ndifferent words and structures. In addition, the identification of duplicate\nquestions can reduce the resources required for retrieval, when the same\nquestions are not repeated. This study compares the performance of deep neural\nnetworks and gradient tree boosting, and explores the possibility of domain\nadaptation with transfer learning to improve the under-performing target\ndomains for the text-pair duplicates classification task, using three\nheterogeneous datasets: general-purpose Quora, technical Ask Ubuntu, and\nacademic English Stack Exchange. Ultimately, our study exposes the alternative\nhypothesis that the meaning of a \"duplicate\" is not inherently general-purpose,\nbut rather is dependent on the domain of learning, hence reducing the chance of\ntransfer learning through adapting to the domain.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:29:26 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Jabbar", "Mohomed Shazan Mohomed", ""], ["Kumar", "Luke", ""], ["Samuel", "Hamman", ""], ["Kim", "Mi-Young", ""], ["Prabhakar", "Sankalp", ""], ["Goebel", "Randy", ""], ["Za\u00efane", "Osmar", ""]]}, {"id": "1811.06621", "submitter": "Ian McGraw", "authors": "Yanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel\n  Alvarez, Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang,\n  Qiao Liang, Deepti Bhatia, Yuan Shangguan, Bo Li, Golan Pundak, Khe Chai Sim,\n  Tom Bagby, Shuo-yiin Chang, Kanishka Rao, Alexander Gruenstein", "title": "Streaming End-to-end Speech Recognition For Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) models, which directly predict output character sequences\ngiven input speech, are good candidates for on-device speech recognition. E2E\nmodels, however, present numerous challenges: In order to be truly useful, such\nmodels must decode speech utterances in a streaming fashion, in real time; they\nmust be robust to the long tail of use cases; they must be able to leverage\nuser-specific context (e.g., contact lists); and above all, they must be\nextremely accurate. In this work, we describe our efforts at building an E2E\nspeech recognizer using a recurrent neural network transducer. In experimental\nevaluations, we find that the proposed approach can outperform a conventional\nCTC-based model in terms of both latency and accuracy in a number of evaluation\ncategories.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:09:44 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["He", "Yanzhang", ""], ["Sainath", "Tara N.", ""], ["Prabhavalkar", "Rohit", ""], ["McGraw", "Ian", ""], ["Alvarez", "Raziel", ""], ["Zhao", "Ding", ""], ["Rybach", "David", ""], ["Kannan", "Anjuli", ""], ["Wu", "Yonghui", ""], ["Pang", "Ruoming", ""], ["Liang", "Qiao", ""], ["Bhatia", "Deepti", ""], ["Shangguan", "Yuan", ""], ["Li", "Bo", ""], ["Pundak", "Golan", ""], ["Sim", "Khe Chai", ""], ["Bagby", "Tom", ""], ["Chang", "Shuo-yiin", ""], ["Rao", "Kanishka", ""], ["Gruenstein", "Alexander", ""]]}, {"id": "1811.06630", "submitter": "Sungjin Lee", "authors": "Sungjin Lee", "title": "Nudging Neural Conversational Model with Domain Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural conversation models are attractive because one can train a model\ndirectly on dialog examples with minimal labeling. With a small amount of data,\nhowever, they often fail to generalize over test data since they tend to\ncapture spurious features instead of semantically meaningful domain knowledge.\nTo address this issue, we propose a novel approach that allows any human\nteachers to transfer their domain knowledge to the conversation model in the\nform of natural language rules. We tested our method with three different\ndialog datasets. The improved performance across all domains demonstrates the\nefficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:47:39 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Lee", "Sungjin", ""]]}, {"id": "1811.06943", "submitter": "Shintaro Yamamoto", "authors": "Shintaro Yamamoto, Yoshihiro Fukuhara, Ryota Suzuki, Shigeo Morishima,\n  Hirokatsu Kataoka", "title": "Automatic Paper Summary Generation from Visual and Textual Information", "comments": "International Conference on Machine Vision 2018, Munich, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent boom in artificial intelligence (AI) research, including\ncomputer vision (CV), it has become impossible for researchers in these fields\nto keep up with the exponentially increasing number of manuscripts. In response\nto this situation, this paper proposes the paper summary generation (PSG) task\nusing a simple but effective method to automatically generate an academic paper\nsummary from raw PDF data. We realized PSG by combination of vision-based\nsupervised components detector and language-based unsupervised important\nsentence extractor, which is applicable for a trained format of manuscripts. We\nshow the quantitative evaluation of ability of simple vision-based components\nextraction, and the qualitative evaluation that our system can extract both\nvisual item and sentence that are helpful for understanding. After processing\nvia our PSG, the 979 manuscripts accepted by the Conference on Computer Vision\nand Pattern Recognition (CVPR) 2018 are available. It is believed that the\nproposed method will provide a better way for researchers to stay caught with\nimportant academic papers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:52:25 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Yamamoto", "Shintaro", ""], ["Fukuhara", "Yoshihiro", ""], ["Suzuki", "Ryota", ""], ["Morishima", "Shigeo", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1811.07021", "submitter": "Rohit Voleti", "authors": "Rohit Voleti, Julie M. Liss, Visar Berisha", "title": "Investigating the Effects of Word Substitution Errors on Sentence\n  Embeddings", "comments": "4 Pages, 2 figures. Copyright IEEE 2019. Accepted and to appear in\n  the Proceedings of the 44th International Conference on Acoustics, Speech,\n  and Signal Processing 2019 (IEEE-ICASSP-2019), May 12-17 in Brighton, U.K.\n  Personal use of this material is permitted. However, permission to\n  reprint/republish this material must be obtained from the IEEE", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683367", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key initial step in several natural language processing (NLP) tasks\ninvolves embedding phrases of text to vectors of real numbers that preserve\nsemantic meaning. To that end, several methods have been recently proposed with\nimpressive results on semantic similarity tasks. However, all of these\napproaches assume that perfect transcripts are available when generating the\nembeddings. While this is a reasonable assumption for analysis of written text,\nit is limiting for analysis of transcribed text. In this paper we investigate\nthe effects of word substitution errors, such as those coming from automatic\nspeech recognition errors (ASR), on several state-of-the-art sentence embedding\nmethods. To do this, we propose a new simulator that allows the experimenter to\ninduce ASR-plausible word substitution errors in a corpus at a desired word\nerror rate. We use this simulator to evaluate the robustness of several\nsentence embedding methods. Our results show that pre-trained neural sentence\nencoders are both robust to ASR errors and perform well on textual similarity\ntasks after errors are introduced. Meanwhile, unweighted averages of word\nvectors perform well with perfect transcriptions, but their performance\ndegrades rapidly on textual similarity tasks for text with word substitution\nerrors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:25:23 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 23:53:01 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Voleti", "Rohit", ""], ["Liss", "Julie M.", ""], ["Berisha", "Visar", ""]]}, {"id": "1811.07032", "submitter": "Jiaming Shen", "authors": "Jiaming Shen, Ruiliang Lyu, Xiang Ren, Michelle Vanni, Brian Sadler,\n  Jiawei Han", "title": "Mining Entity Synonyms with Efficient Neural Set Generation", "comments": "AAAI 2019 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining entity synonym sets (i.e., sets of terms referring to the same entity)\nis an important task for many entity-leveraging applications. Previous work\neither rank terms based on their similarity to a given query term, or treats\nthe problem as a two-phase task (i.e., detecting synonymy pairs, followed by\norganizing these pairs into synonym sets). However, these approaches fail to\nmodel the holistic semantics of a set and suffer from the error propagation\nissue. Here we propose a new framework, named SynSetMine, that efficiently\ngenerates entity synonym sets from a given vocabulary, using example sets from\nexternal knowledge bases as distant supervision. SynSetMine consists of two\nnovel modules: (1) a set-instance classifier that jointly learns how to\nrepresent a permutation invariant synonym set and whether to include a new\ninstance (i.e., a term) into the set, and (2) a set generation algorithm that\nenumerates the vocabulary only once and applies the learned set-instance\nclassifier to detect all entity synonym sets in it. Experiments on three real\ndatasets from different domains demonstrate both effectiveness and efficiency\nof SynSetMine for mining entity synonym sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 21:08:50 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Shen", "Jiaming", ""], ["Lyu", "Ruiliang", ""], ["Ren", "Xiang", ""], ["Vanni", "Michelle", ""], ["Sadler", "Brian", ""], ["Han", "Jiawei", ""]]}, {"id": "1811.07033", "submitter": "Yixin Nie", "authors": "Yixin Nie, Yicheng Wang, Mohit Bansal", "title": "Analyzing Compositionality-Sensitivity of NLI Models", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Success in natural language inference (NLI) should require a model to\nunderstand both lexical and compositional semantics. However, through\nadversarial evaluation, we find that several state-of-the-art models with\ndiverse architectures are over-relying on the former and fail to use the\nlatter. Further, this compositionality unawareness is not reflected via\nstandard evaluation on current datasets. We show that removing RNNs in existing\nmodels or shuffling input words during training does not induce large\nperformance loss despite the explicit removal of compositional information.\nTherefore, we propose a compositionality-sensitivity testing setup that\nanalyzes models on natural examples from existing datasets that cannot be\nsolved via lexical features alone (i.e., on which a bag-of-words model gives a\nhigh probability to one wrong label), hence revealing the models' actual\ncompositionality awareness. We show that this setup not only highlights the\nlimited compositional ability of current NLI models, but also differentiates\nmodel performance based on design, e.g., separating shallow bag-of-words models\nfrom deeper, linguistically-grounded tree-based models. Our evaluation setup is\nan important analysis tool: complementing currently existing adversarial and\nlinguistically driven diagnostic evaluations, and exposing opportunities for\nfuture work on evaluating models' compositional understanding.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 21:12:24 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Nie", "Yixin", ""], ["Wang", "Yicheng", ""], ["Bansal", "Mohit", ""]]}, {"id": "1811.07039", "submitter": "Yixin Nie", "authors": "Yixin Nie, Haonan Chen, Mohit Bansal", "title": "Combining Fact Extraction and Verification with Neural Semantic Matching\n  Networks", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing concern with misinformation has stimulated research efforts on\nautomatic fact checking. The recently-released FEVER dataset introduced a\nbenchmark fact-verification task in which a system is asked to verify a claim\nusing evidential sentences from Wikipedia documents. In this paper, we present\na connected system consisting of three homogeneous neural semantic matching\nmodels that conduct document retrieval, sentence selection, and claim\nverification jointly for fact extraction and verification. For evidence\nretrieval (document retrieval and sentence selection), unlike traditional\nvector space IR models in which queries and sources are matched in some\npre-designed term vector space, we develop neural models to perform deep\nsemantic matching from raw textual input, assuming no intermediate term\nrepresentation and no access to structured external knowledge bases. We also\nshow that Pageview frequency can also help improve the performance of evidence\nretrieval results, that later can be matched by using our neural semantic\nmatching network. For claim verification, unlike previous approaches that\nsimply feed upstream retrieved evidence and the claim to a natural language\ninference (NLI) model, we further enhance the NLI model by providing it with\ninternal semantic relatedness scores (hence integrating it with the evidence\nretrieval modules) and ontological WordNet features. Experiments on the FEVER\ndataset indicate that (1) our neural semantic matching method outperforms\npopular TF-IDF and encoder models, by significant margins on all evidence\nretrieval metrics, (2) the additional relatedness score and WordNet features\nimprove the NLI model via better semantic awareness, and (3) by formalizing all\nthree subtasks as a similar semantic matching problem and improving on all\nthree stages, the complete model is able to achieve the state-of-the-art\nresults on the FEVER test set.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 21:37:59 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Nie", "Yixin", ""], ["Chen", "Haonan", ""], ["Bansal", "Mohit", ""]]}, {"id": "1811.07061", "submitter": "Li Lucy", "authors": "Li Lucy and Julia Mendelsohn", "title": "Using Sentiment Induction to Understand Variation in Gendered Online\n  Communities", "comments": "11 pages, 4 figures, to appear in proceedings of the Society for\n  Computation in Linguistics (SCIL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze gendered communities defined in three different ways: text, users,\nand sentiment. Differences across these representations reveal facets of\ncommunities' distinctive identities, such as social group, topic, and\nattitudes. Two communities may have high text similarity but not user\nsimilarity or vice versa, and word usage also does not vary according to a\nclearcut, binary perspective of gender. Community-specific sentiment lexicons\ndemonstrate that sentiment can be a useful indicator of words' social meaning\nand community values, especially in the context of discussion content and user\ndemographics. Our results show that social platforms such as Reddit are active\nsettings for different constructions of gender.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 23:17:33 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Lucy", "Li", ""], ["Mendelsohn", "Julia", ""]]}, {"id": "1811.07066", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Kunwoo Park, Joongbo Shin, Hongjun Lim, Seungpil Won,\n  Meeyoung Cha, Kyomin Jung", "title": "Detecting Incongruity Between News Headline and Body Text via a Deep\n  Hierarchical Encoder", "comments": "10 pages, Accepted as a conference paper at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some news headlines mislead readers with overrated or false information, and\nidentifying them in advance will better assist readers in choosing proper news\nstories to consume. This research introduces million-scale pairs of news\nheadline and body text dataset with incongruity label, which can uniquely be\nutilized for detecting news stories with misleading headlines. On this dataset,\nwe develop two neural networks with hierarchical architectures that model a\ncomplex textual representation of news articles and measure the incongruity\nbetween the headline and the body text. We also present a data augmentation\nmethod that dramatically reduces the text input size a model handles by\nindependently investigating each paragraph of news stories, which further\nboosts the performance. Our experiments and qualitative evaluations demonstrate\nthat the proposed methods outperform existing approaches and efficiently detect\nnews stories with misleading headlines in the real world.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 00:21:10 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 01:56:40 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Park", "Kunwoo", ""], ["Shin", "Joongbo", ""], ["Lim", "Hongjun", ""], ["Won", "Seungpil", ""], ["Cha", "Meeyoung", ""], ["Jung", "Kyomin", ""]]}, {"id": "1811.07078", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Di Wang, Chunyan Miao", "title": "An Affect-Rich Neural Conversational Model with Biased Attention and\n  Weighted Cross-Entropy Loss", "comments": "AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect conveys important implicit information in human communication. Having\nthe capability to correctly express affect during human-machine conversations\nis one of the major milestones in artificial intelligence. In recent years,\nextensive research on open-domain neural conversational models has been\nconducted. However, embedding affect into such models is still under explored.\nIn this paper, we propose an end-to-end affect-rich open-domain neural\nconversational model that produces responses not only appropriate in syntax and\nsemantics, but also with rich affect. Our model extends the Seq2Seq model and\nadopts VAD (Valence, Arousal and Dominance) affective notations to embed each\nword with affects. In addition, our model considers the effect of negators and\nintensifiers via a novel affective attention mechanism, which biases attention\ntowards affect-rich words in input sentences. Lastly, we train our model with\nan affect-incorporated objective function to encourage the generation of\naffect-rich words in the output responses. Evaluations based on both perplexity\nand human evaluations show that our model outperforms the state-of-the-art\nbaseline model of comparable size in producing natural and affect-rich\nresponses.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 02:29:18 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhong", "Peixiang", ""], ["Wang", "Di", ""], ["Miao", "Chunyan", ""]]}, {"id": "1811.07080", "submitter": "Ndapa Nakashole", "authors": "Ndapa Nakashole", "title": "Bilingual Dictionary Induction for Bantu Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning bilingual translation dictionaries between\nEnglish and Bantu languages. We show that exploiting the grammatical structure\ncommon to Bantu languages enables bilingual dictionary induction for languages\nwhere training data is unavailable.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 02:36:07 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 02:49:55 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Nakashole", "Ndapa", ""]]}, {"id": "1811.07092", "submitter": "Ndapa Nakashole", "authors": "Ndapa Nakashole", "title": "Unnamed Entity Recognition of Sense Mentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recognizing mentions of human senses in text. Our\ncontribution is a method for acquiring labeled data, and a learning method that\nis trained on this data. Experiments show the effectiveness of our proposed\ndata labeling approach and our learning model on the task of sense recognition\nin text.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 03:58:26 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 02:48:04 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Nakashole", "Ndapa", ""]]}, {"id": "1811.07098", "submitter": "Ndapa Nakashole", "authors": "Ndapa Nakashole", "title": "Sense Perception Common Sense Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often missing in existing knowledge bases of facts, are relationships that\nencode common sense knowledge about unnamed entities. In this paper, we propose\nto extract novel, common sense relationships pertaining to sense perception\nconcepts such as sound and smell.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 04:24:14 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 02:45:41 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Nakashole", "Ndapa", ""]]}, {"id": "1811.07208", "submitter": "Adam Lipowski", "authors": "Dorota Lipowska and Adam Lipowski", "title": "Emergence of linguistic conventions in multi-agent reinforcement\n  learning", "comments": "18 pages, PLOS ONE (accepted)", "journal-ref": "PLoS ONE 13(11): e0208095 (2018)", "doi": "10.1371/journal.pone.0208095", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, emergence of signaling conventions, among which language is a prime\nexample, draws a considerable interdisciplinary interest ranging from game\ntheory, to robotics to evolutionary linguistics. Such a wide spectrum of\nresearch is based on much different assumptions and methodologies, but\ncomplexity of the problem precludes formulation of a unifying and commonly\naccepted explanation. We examine formation of signaling conventions in a\nframework of a multi-agent reinforcement learning model. When the network of\ninteractions between agents is a complete graph or a sufficiently dense random\ngraph, a global consensus is typically reached with the emerging language being\na nearly unique object-word mapping or containing some synonyms and homonyms.\nOn finite-dimensional lattices, the model gets trapped in disordered\nconfigurations with a local consensus only. Such a trapping can be avoided by\nintroducing a population renewal, which in the presence of superlinear\nreinforcement restores an ordinary surface-tension driven coarsening and\nconsiderably enhances formation of efficient signaling.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 18:38:09 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Lipowska", "Dorota", ""], ["Lipowski", "Adam", ""]]}, {"id": "1811.07223", "submitter": "Binny Mathew", "authors": "Binny Mathew, Ritam Dutt, Suman Kalyan Maity, Pawan Goyal, and Animesh\n  Mukherjee", "title": "Deep Dive into Anonymity: A Large Scale Analysis of Quora Questions", "comments": "12 pages, 6 figures, and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymity forms an integral and important part of our digital life. It\nenables us to express our true selves without the fear of judgment. In this\npaper, we investigate the different aspects of anonymity in the social Q&A site\nQuora. The choice of Quora is motivated by the fact that this is one of the\nrare social Q&A sites that allow users to explicitly post anonymous questions\nand such activity in this forum has become normative rather than a taboo.\nThrough an analysis of 5.1 million questions, we observe that at a global scale\nalmost no difference manifests between the linguistic structure of the\nanonymous and the non-anonymous questions. We find that topical mixing at the\nglobal scale to be the primary reason for the absence. However, the differences\nstart to feature once we \"deep dive\" and (topically) cluster the questions and\ncompare the clusters that have high volumes of anonymous questions with those\nthat have low volumes of anonymous questions. In particular, we observe that\nthe choice to post the question as anonymous is dependent on the user's\nperception of anonymity and they often choose to speak about depression,\nanxiety, social ties and personal issues under the guise of anonymity. We\nfurther perform personality trait analysis and observe that the anonymous group\nof users has positive correlation with extraversion, agreeableness, and\nnegative correlation with openness. Subsequently, to gain further insights, we\nbuild an anonymity grid to identify the differences in the perception on\nanonymity of the user posting the question and the community of users answering\nit. We also look into the first response time of the questions and observe that\nit is lowest for topics which talk about personal and sensitive issues, which\nhints toward a higher degree of community support and user engagement.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 21:26:20 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Mathew", "Binny", ""], ["Dutt", "Ritam", ""], ["Maity", "Suman Kalyan", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1811.07234", "submitter": "Yao Wan", "authors": "Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu,\n  Philip S. Yu", "title": "Improving Automatic Source Code Summarization via Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code summarization provides a high level natural language description of the\nfunction performed by code, as it can benefit the software maintenance, code\ncategorization and retrieval. To the best of our knowledge, most\nstate-of-the-art approaches follow an encoder-decoder framework which encodes\nthe code into a hidden space and then decode it into natural language space,\nsuffering from two major drawbacks: a) Their encoders only consider the\nsequential content of code, ignoring the tree structure which is also critical\nfor the task of code summarization, b) Their decoders are typically trained to\npredict the next word by maximizing the likelihood of next ground-truth word\nwith previous ground-truth word given. However, it is expected to generate the\nentire sequence from scratch at test time. This discrepancy can cause an\n\\textit{exposure bias} issue, making the learnt decoder suboptimal. In this\npaper, we incorporate an abstract syntax tree structure as well as sequential\ncontent of code snippets into a deep reinforcement learning framework (i.e.,\nactor-critic network). The actor network provides the confidence of predicting\nthe next word according to current state. On the other hand, the critic network\nevaluates the reward value of all possible extensions of the current state and\ncan provide global guidance for explorations. We employ an advantage reward\ncomposed of BLEU metric to train both networks. Comprehensive experiments on a\nreal-world dataset show the effectiveness of our proposed model when compared\nwith some state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 22:21:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wan", "Yao", ""], ["Zhao", "Zhou", ""], ["Yang", "Min", ""], ["Xu", "Guandong", ""], ["Ying", "Haochao", ""], ["Wu", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.07236", "submitter": "Vicky Zayats", "authors": "Vicky Zayats and Mari Ostendorf", "title": "Robust cross-domain disfluency detection with pattern match networks", "comments": "This paper was submitted to EMNLP 2018 and was rejected. Our EMNLP\n  submission is posted here to establish concurrency with \"Disfluency Detection\n  using Auto-Correlational Neural Networks\" by P. Lou, P. Anderson, M. Johnson\n  which was submitted to EMNLP at the same time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel pattern match neural network architecture\nthat uses neighbor similarity scores as features, eliminating the need for\nfeature engineering in a disfluency detection task. We evaluate the approach in\ndisfluency detection for four different speech genres, showing that the\napproach is as effective as hand-engineered pattern match features when used on\nin-domain data and achieves superior performance in cross-domain scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 22:34:20 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zayats", "Vicky", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1811.07240", "submitter": "Kyle Kastner", "authors": "Kyle Kastner, Jo\\~ao Felipe Santos, Yoshua Bengio, Aaron Courville", "title": "Representation Mixing for TTS Synthesis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent character and phoneme-based parametric TTS systems using deep learning\nhave shown strong performance in natural speech generation. However, the choice\nbetween character or phoneme input can create serious limitations for practical\ndeployment, as direct control of pronunciation is crucial in certain cases. We\ndemonstrate a simple method for combining multiple types of linguistic\ninformation in a single encoder, named representation mixing, enabling flexible\nchoice between character, phoneme, or mixed representations during inference.\nExperiments and user studies on a public audiobook corpus show the efficacy of\nour approach.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 22:45:15 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 23:16:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kastner", "Kyle", ""], ["Santos", "Jo\u00e3o Felipe", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.07253", "submitter": "Yijun Xiao", "authors": "Yijun Xiao, William Yang Wang", "title": "Quantifying Uncertainties in Natural Language Processing Tasks", "comments": "To appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 01:36:05 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Xiao", "Yijun", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.07351", "submitter": "Xuan Su", "authors": "Xuan Su, Animesh Prasad, Min-Yen Kan, Kazunari Sugiyama", "title": "Neural Multi-Task Learning for Citation Function and Provenance", "comments": null, "journal-ref": "JCDL 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation function and provenance are two cornerstone tasks in citation\nanalysis. Given a citation, the former task determines its rhetorical role,\nwhile the latter locates the text in the cited paper that contains the relevant\ncited information. We hypothesize that these two tasks are synergistically\nrelated, and build a model that validates this claim. For both tasks, we show\nthat a single-layer convolutional neural network (CNN) outperforms existing\nstate-of-the-art baselines. More importantly, we show that the two tasks are\nindeed synergistic: by jointly training both of the tasks in a multi-task\nlearning setup, we demonstrate additional performance gains. Altogether, our\nmodels improve the current state-of-the-arts up to 2\\%, with statistical\nsignificance for both citation function and provenance prediction tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 16:55:25 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 03:02:17 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Su", "Xuan", ""], ["Prasad", "Animesh", ""], ["Kan", "Min-Yen", ""], ["Sugiyama", "Kazunari", ""]]}, {"id": "1811.07430", "submitter": "Sharath Chandra Guntuku", "authors": "Sharath Chandra Guntuku, Anneke Buffone, Kokil Jaidka, Johannes\n  Eichstaedt, Lyle Ungar", "title": "Understanding and Measuring Psychological Stress using Social Media", "comments": "Accepted for publication in the proceedings of ICWSM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A body of literature has demonstrated that users' mental health conditions,\nsuch as depression and anxiety, can be predicted from their social media\nlanguage. There is still a gap in the scientific understanding of how\npsychological stress is expressed on social media. Stress is one of the primary\nunderlying causes and correlates of chronic physical illnesses and mental\nhealth conditions. In this paper, we explore the language of psychological\nstress with a dataset of 601 social media users, who answered the Perceived\nStress Scale questionnaire and also consented to share their Facebook and\nTwitter data. Firstly, we find that stressed users post about exhaustion,\nlosing control, increased self-focus and physical pain as compared to posts\nabout breakfast, family-time, and travel by users who are not stressed.\nSecondly, we find that Facebook language is more predictive of stress than\nTwitter language. Thirdly, we demonstrate how the language based models thus\ndeveloped can be adapted and be scaled to measure county-level trends. Since\ncounty-level language is easily available on Twitter using the Streaming API,\nwe explore multiple domain adaptation algorithms to adapt user-level Facebook\nmodels to Twitter language. We find that domain-adapted and scaled social\nmedia-based measurements of stress outperform sociodemographic variables (age,\ngender, race, education, and income), against ground-truth survey-based stress\nmeasurements, both at the user- and the county-level in the U.S. Twitter\nlanguage that scores higher in stress is also predictive of poorer health, less\naccess to facilities and lower socioeconomic status in counties. We conclude\nwith a discussion of the implications of using social media as a new tool for\nmonitoring stress levels of both individuals and counties.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:18:48 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 17:00:56 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Guntuku", "Sharath Chandra", ""], ["Buffone", "Anneke", ""], ["Jaidka", "Kokil", ""], ["Eichstaedt", "Johannes", ""], ["Ungar", "Lyle", ""]]}, {"id": "1811.07435", "submitter": "Debasish Mohapatra", "authors": "Debasish Ray Mohapatra and Sidney Fels", "title": "Limitations of Source-Filter Coupling In Phonation", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": "10.1121/1.5068357", "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coupling of vocal fold (source) and vocal tract (filter) is one of the\nmost critical factors in source-filter articulation theory. The traditional\nlinear source-filter theory has been challenged by current research which\nclearly shows the impact of acoustic loading on the dynamic behavior of the\nvocal fold vibration as well as the variations in the glottal flow pulses\nshape. This paper outlines the underlying mechanism of source-filter\ninteractions; demonstrates the design and working principles of coupling for\nthe various existing vocal cord and vocal tract biomechanical models. For our\nstudy, we have considered self-oscillating lumped-element models of the\nacoustic source and computational models of the vocal tract as articulators. To\nunderstand the limitations of source-filter interactions which are associated\nwith each of those models, we compare them concerning their mechanical design,\nacoustic and physiological characteristics and aerodynamic simulation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:32:50 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Mohapatra", "Debasish Ray", ""], ["Fels", "Sidney", ""]]}, {"id": "1811.07453", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli and Titouan Parcollet and Yoshua Bengio", "title": "The PyTorch-Kaldi Speech Recognition Toolkit", "comments": "Accepted at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of open-source software is playing a remarkable role in the\npopularization of speech recognition and deep learning. Kaldi, for instance, is\nnowadays an established framework used to develop state-of-the-art speech\nrecognizers. PyTorch is used to build neural networks with the Python language\nand has recently spawn tremendous interest within the machine learning\ncommunity thanks to its simplicity and flexibility.\n  The PyTorch-Kaldi project aims to bridge the gap between these popular\ntoolkits, trying to inherit the efficiency of Kaldi and the flexibility of\nPyTorch. PyTorch-Kaldi is not only a simple interface between these software,\nbut it embeds several useful features for developing modern speech recognizers.\nFor instance, the code is specifically designed to naturally plug-in\nuser-defined acoustic models. As an alternative, users can exploit several\npre-implemented neural networks that can be customized using intuitive\nconfiguration files. PyTorch-Kaldi supports multiple feature and label streams\nas well as combinations of neural networks, enabling the use of complex neural\narchitectures. The toolkit is publicly-released along with a rich documentation\nand is designed to properly work locally or on HPC clusters.\n  Experiments, that are conducted on several datasets and tasks, show that\nPyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech\nrecognizers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 01:57:05 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 19:13:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Parcollet", "Titouan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1811.07485", "submitter": "Chenchen Li", "authors": "Chenchen Li, Jialin Wang, Hongwei Wang, Miao Zhao, Wenjie Li, Xiaotie\n  Deng", "title": "Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural\n  Networks", "comments": "Draft, 25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User emotion analysis toward videos is to automatically recognize the general\nemotional status of viewers from the multimedia content embedded in the online\nvideo stream. Existing works fall in two categories: 1) visual-based methods,\nwhich focus on visual content and extract a specific set of features of videos.\nHowever, it is generally hard to learn a mapping function from low-level video\npixels to high-level emotion space due to great intra-class variance. 2)\ntextual-based methods, which focus on the investigation of user-generated\ncomments associated with videos. The learned word representations by\ntraditional linguistic approaches typically lack emotion information and the\nglobal comments usually reflect viewers' high-level understandings rather than\ninstantaneous emotions. To address these limitations, in this paper, we propose\nto jointly utilize video content and user-generated texts simultaneously for\nemotion analysis. In particular, we introduce exploiting a new type of\nuser-generated texts, i.e., \"danmu\", which are real-time comments floating on\nthe video and contain rich information to convey viewers' emotional opinions.\nTo enhance the emotion discriminativeness of words in textual feature\nextraction, we propose Emotional Word Embedding (EWE) to learn text\nrepresentations by jointly considering their semantics and emotions.\nAfterwards, we propose a novel visual-textual emotion analysis model with Deep\nCoupled Video and Danmu Neural networks (DCVDN), in which visual and textual\nfeatures are synchronously extracted and fused to form a comprehensive\nrepresentation by deep-canonically-correlated-autoencoder-based multi-view\nlearning. Through extensive experiments on a self-crawled real-world\nvideo-danmu dataset, we prove that DCVDN significantly outperforms the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:51:19 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Chenchen", ""], ["Wang", "Jialin", ""], ["Wang", "Hongwei", ""], ["Zhao", "Miao", ""], ["Li", "Wenjie", ""], ["Deng", "Xiaotie", ""]]}, {"id": "1811.07497", "submitter": "Konstantinos Pappas", "authors": "Konstantinos Pappas, Mahmoud Azab, Rada Mihalcea", "title": "A Comparative Analysis of Content-based Geolocation in Blogs and Tweets", "comments": "31 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geolocation of online information is an essential component in any\ngeospatial application. While most of the previous work on geolocation has\nfocused on Twitter, in this paper we quantify and compare the performance of\ntext-based geolocation methods on social media data drawn from both Blogger and\nTwitter. We introduce a novel set of location specific features that are both\nhighly informative and easily interpretable, and show that we can achieve error\nrate reductions of up to 12.5% with respect to the best previously proposed\ngeolocation features. We also show that despite posting longer text, Blogger\nusers are significantly harder to geolocate than Twitter users. Additionally,\nwe investigate the effect of training and testing on different media\n(cross-media predictions), or combining multiple social media sources\n(multi-media predictions). Finally, we explore the geolocability of social\nmedia in relation to three user dimensions: state, gender, and industry.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:42:54 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pappas", "Konstantinos", ""], ["Azab", "Mahmoud", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1811.07514", "submitter": "Shobeir Fakhraei", "authors": "Shobeir Fakhraei, Joel Mathew, Jose Luis Ambite", "title": "NSEEN: Neural Semantic Embedding for Entity Normalization", "comments": "Accepted for publication at ECML-PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human knowledge is encoded in text, available in scientific\npublications, books, and the web. Given the rapid growth of these resources, we\nneed automated methods to extract such knowledge into machine-processable\nstructures, such as knowledge graphs. An important task in this process is\nentity normalization, which consists of mapping noisy entity mentions in text\nto canonical entities in well-known reference sets. However, entity\nnormalization is a challenging problem; there often are many textual forms for\na canonical entity that may not be captured in the reference set, and entities\nmentioned in text may include many syntactic variations, or errors. The problem\nis particularly acute in scientific domains, such as biology. To address this\nproblem, we have developed a general, scalable solution based on a deep Siamese\nneural network model to embed the semantic information about the entities, as\nwell as their syntactic variations. We use these embeddings for fast mapping of\nnew entities to large reference sets, and empirically show the effectiveness of\nour framework in challenging bio-entity normalization datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:04:13 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 07:19:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Fakhraei", "Shobeir", ""], ["Mathew", "Joel", ""], ["Ambite", "Jose Luis", ""]]}, {"id": "1811.07550", "submitter": "Xiujun Li", "authors": "Yuexin Wu and Xiujun Li and Jingjing Liu and Jianfeng Gao and Yiming\n  Yang", "title": "Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for\n  Task-Completion Dialogue Policy Learning", "comments": "8 pages, 9 figures, AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training task-completion dialogue agents with reinforcement learning usually\nrequires a large number of real user experiences. The Dyna-Q algorithm extends\nQ-learning by integrating a world model, and thus can effectively boost\ntraining efficiency using simulated experiences generated by the world model.\nThe effectiveness of Dyna-Q, however, depends on the quality of the world model\n- or implicitly, the pre-specified ratio of real vs. simulated experiences used\nfor Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)\nframework by integrating a switcher that automatically determines whether to\nuse a real or simulated experience for Q-learning. Furthermore, we explore the\nuse of active learning for improving sample efficiency, by encouraging the\nworld model to generate simulated experiences in the state-action space where\nthe agent has not (fully) explored. Our results show that by combining switcher\nand active learning, the new framework named as Switch-based Active Deep Dyna-Q\n(Switch-DDQ), leads to significant improvement over DDQ and Q-learning\nbaselines in both simulation and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:23:34 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wu", "Yuexin", ""], ["Li", "Xiujun", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""], ["Yang", "Yiming", ""]]}, {"id": "1811.07600", "submitter": "Anshuman Suri", "authors": "Parag Agrawal, Anshuman Suri, Tulasi Menon", "title": "A Trustworthy, Responsible and Interpretable System to Handle Chit Chat\n  in Conversational Bots", "comments": "7 pages, 5 figures, The Second AAAI Workshop on Reasoning and\n  Learning for Human-Machine Dialogues (DEEP-DIAL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most often, chat-bots are built to solve the purpose of a search engine or a\nhuman assistant: Their primary goal is to provide information to the user or\nhelp them complete a task. However, these chat-bots are incapable of responding\nto unscripted queries like \"Hi, what's up\", \"What's your favourite food\". Human\nevaluation judgments show that 4 humans come to a consensus on the intent of a\ngiven query which is from chat domain only 77% of the time, thus making it\nevident how non-trivial this task is. In our work, we show why it is difficult\nto break the chitchat space into clearly defined intents. We propose a system\nto handle this task in chat-bots, keeping in mind scalability,\ninterpretability, appropriateness, trustworthiness, relevance and coverage. Our\nwork introduces a pipeline for query understanding in chitchat using\nhierarchical intents as well as a way to use seq-seq auto-generation models in\nprofessional bots. We explore an interpretable model for chat domain detection\nand also show how various components such as adult/offensive classification,\ngrammars/regex patterns, curated personality based responses, generic guided\nevasive responses and response generation models can be combined in a scalable\nway to solve this problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:45:13 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 12:10:30 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Agrawal", "Parag", ""], ["Suri", "Anshuman", ""], ["Menon", "Tulasi", ""]]}, {"id": "1811.07631", "submitter": "Lili Yao", "authors": "Lili Yao, Ruijian Xu, Chao Li, Dongyan Zhao and Rui Yan", "title": "Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To build an open-domain multi-turn conversation system is one of the most\ninteresting and challenging tasks in Artificial Intelligence. Many research\nefforts have been dedicated to building such dialogue systems, yet few shed\nlight on modeling the conversation flow in an ongoing dialogue. Besides, it is\ncommon for people to talk about highly relevant aspects during a conversation.\nAnd the topics are coherent and drift naturally, which demonstrates the\nnecessity of dialogue flow modeling. To this end, we present the multi-turn\ncue-words driven conversation system with reinforcement learning method (RLCw),\nwhich strives to select an adaptive cue word with the greatest future credit,\nand therefore improve the quality of generated responses. We introduce a new\nreward to measure the quality of cue words in terms of effectiveness and\nrelevance. To further optimize the model for long-term conversations, a\nreinforcement approach is adopted in this paper. Experiments on real-life\ndataset demonstrate that our model consistently outperforms a set of\ncompetitive baselines in terms of simulated turns, diversity and human\nevaluation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:54:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yao", "Lili", ""], ["Xu", "Ruijian", ""], ["Li", "Chao", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1811.07655", "submitter": "Abbas Ehsanfar", "authors": "Abbas Ehsanfar and Mo Mansouri", "title": "An Influence-based Clustering Model on Twitter", "comments": "INFORMS 13th Data Mining and Decision Analytics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a temporal framework for detecting and clustering\nemergent and viral topics on social networks. Endogenous and exogenous\ninfluence on developing viral content is explored using a clustering method\nbased on the a user's behavior on social network and a dataset from Twitter\nAPI. Results are discussed by introducing metrics such as popularity,\nburstiness, and relevance score. The results show clear distinction in\ncharacteristics of developed content by the two classes of users.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 12:51:07 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ehsanfar", "Abbas", ""], ["Mansouri", "Mo", ""]]}, {"id": "1811.07684", "submitter": "Alice Coucke", "authors": "Alice Coucke, Mohammed Chlieh, Thibault Gisselbrecht, David Leroy,\n  Mathieu Poumeyrol, Thibaut Lavril", "title": "Efficient keyword spotting using dilated convolutions and gating", "comments": "Accepted for publication to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the application of end-to-end stateless temporal modeling to\nsmall-footprint keyword spotting as opposed to recurrent networks that model\nlong-term temporal dependencies using internal states. We propose a model\ninspired by the recent success of dilated convolutions in sequence modeling\napplications, allowing to train deeper architectures in resource-constrained\nconfigurations. Gated activations and residual connections are also added,\nfollowing a similar configuration to WaveNet. In addition, we apply a custom\ntarget labeling that back-propagates loss from specific frames of interest,\ntherefore yielding higher accuracy and only requiring to detect the end of the\nkeyword. Our experimental results show that our model outperforms a max-pooling\nloss trained recurrent neural network using LSTM cells, with a significant\ndecrease in false rejection rate. The underlying dataset - \"Hey Snips\"\nutterances recorded by over 2.2K different speakers - has been made publicly\navailable to establish an open reference for wake-word detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:51:10 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 16:21:04 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Coucke", "Alice", ""], ["Chlieh", "Mohammed", ""], ["Gisselbrecht", "Thibault", ""], ["Leroy", "David", ""], ["Poumeyrol", "Mathieu", ""], ["Lavril", "Thibaut", ""]]}, {"id": "1811.07720", "submitter": "Pradeep Rangan", "authors": "Pradeep Rangan and Sreenivasa Rao K", "title": "Beam Search Decoding using Manner of Articulation Detection Knowledge\n  Derived from Connectionist Temporal Classification", "comments": "5 pages, Submitted to ICASSP 2019. arXiv admin note: substantial text\n  overlap with arXiv:1811.01644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manner of articulation detection using deep neural networks require a priori\nknowledge of the attribute discriminative features or the decent phoneme\nalignments. However generating an appropriate phoneme alignment is complex and\nits performance depends on the choice of optimal number of senones, Gaussians,\netc. In the first part of our work, we exploit the manner of articulation\ndetection using connectionist temporal classification (CTC) which doesn't need\nany phoneme alignment. Later we modify the state-of-the-art character based\nposteriors generated by CTC using the manner of articulation CTC detector. Beam\nsearch decoding is performed on the modified posteriors and it's impact on open\nsource datasets such as AN4 and LibriSpeech is observed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 04:40:03 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Rangan", "Pradeep", ""], ["K", "Sreenivasa Rao", ""]]}, {"id": "1811.07851", "submitter": "Bob de Ruiter", "authors": "Bob de Ruiter, George Kachergis", "title": "The Mafiascum Dataset: A Large Text Corpus for Deception Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting deception in natural language has a wide variety of applications,\nbut because of its hidden nature there are currently no public, large-scale\nsources of labeled deceptive text. This work introduces the Mafiascum dataset\n[1], a collection of over 700 games of Mafia, in which players are randomly\nassigned either deceptive or non-deceptive roles and then interact via forum\npostings. Over 9000 documents were compiled from the dataset, which each\ncontained all messages written by a single player in a single game. This corpus\nwas used to construct a set of hand-picked linguistic features based on prior\ndeception research, as well as a set of average word vectors enriched with\nsubword information. A logistic regression classifier fit on a combination of\nthese feature sets achieved an average precision of 0.39 (chance = 0.26) and an\nAUROC of 0.68 on 5000+ word documents. On 50+ word documents, an average\nprecision of 0.29 (chance = 0.23) and an AUROC of 0.59 was achieved.\n  [1] https://bitbucket.org/bopjesvla/thesis/src\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:11:09 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:53:31 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 08:12:40 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["de Ruiter", "Bob", ""], ["Kachergis", "George", ""]]}, {"id": "1811.07882", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, Jacob\n  Andreas, John DeNero, Pieter Abbeel, Sergey Levine", "title": "Guiding Policies with Language via Meta-Learning", "comments": "Accepted at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral skills or policies for autonomous agents are conventionally\nlearned from reward functions, via reinforcement learning, or from\ndemonstrations, via imitation learning. However, both modes of task\nspecification have their disadvantages: reward functions require manual\nengineering, while demonstrations require a human expert to be able to actually\nperform the task in order to generate the demonstration. Instruction following\nfrom natural language instructions provides an appealing alternative: in the\nsame way that we can specify goals to other humans simply by speaking or\nwriting, we would like to be able to specify tasks for our machines. However, a\nsingle instruction may be insufficient to fully communicate our intent or, even\nif it is, may be insufficient for an autonomous agent to actually understand\nhow to perform the desired task. In this work, we propose an interactive\nformulation of the task specification problem, where iterative language\ncorrections are provided to an autonomous agent, guiding it in acquiring the\ndesired skill. Our proposed language-guided policy learning algorithm can\nintegrate an instruction and a sequence of corrections to acquire new skills\nvery quickly. In our experiments, we show that this method can enable a policy\nto follow instructions and corrections for simulated navigation and\nmanipulation tasks, substantially outperforming direct, non-interactive\ninstruction following.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:58:42 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 18:54:15 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Gupta", "Abhishek", ""], ["Sanjeev", "Suvansh", ""], ["Altieri", "Nick", ""], ["Andreas", "Jacob", ""], ["DeNero", "John", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1811.07901", "submitter": "Vivian Lai", "authors": "Vivian Lai, Chenhao Tan", "title": "On Human Predictions with Explanations and Predictions of Machine\n  Learning Models: A Case Study on Deception Detection", "comments": "17 pages, 19 figures, in Proceedings of ACM FAT* 2019, dataset & demo\n  available at https://deception.machineintheloop.com", "journal-ref": null, "doi": "10.1145/3287560.3287590", "report-no": null, "categories": "cs.AI cs.CL cs.CY physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are the final decision makers in critical tasks that involve ethical\nand legal concerns, ranging from recidivism prediction, to medical diagnosis,\nto fighting against fake news. Although machine learning models can sometimes\nachieve impressive performance in these tasks, these tasks are not amenable to\nfull automation. To realize the potential of machine learning for improving\nhuman decisions, it is important to understand how assistance from machine\nlearning models affects human performance and human agency.\n  In this paper, we use deception detection as a testbed and investigate how we\ncan harness explanations and predictions of machine learning models to improve\nhuman performance while retaining human agency. We propose a spectrum between\nfull human agency and full automation, and develop varying levels of machine\nassistance along the spectrum that gradually increase the influence of machine\npredictions. We find that without showing predicted labels, explanations alone\nslightly improve human performance in the end task. In comparison, human\nperformance is greatly improved by showing predicted labels (>20% relative\nimprovement) and can be further improved by explicitly suggesting strong\nmachine performance. Interestingly, when predicted labels are shown,\nexplanations of machine predictions induce a similar level of accuracy as an\nexplicit statement of strong machine performance. Our results demonstrate a\ntradeoff between human performance and human agency and show that explanations\nof machine predictions can moderate this tradeoff.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 19:00:01 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 20:47:12 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 03:52:47 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 21:15:07 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Lai", "Vivian", ""], ["Tan", "Chenhao", ""]]}, {"id": "1811.08008", "submitter": "Daniel Gillick", "authors": "Daniel Gillick, Alessandro Presta, Gaurav Singh Tomar", "title": "End-to-End Retrieval in Continuous Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:23:59 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Gillick", "Daniel", ""], ["Presta", "Alessandro", ""], ["Tomar", "Gaurav Singh", ""]]}, {"id": "1811.08040", "submitter": "Xiangan Liu", "authors": "Xiangan Liu, Keyang Xu, Pengtao Xie, Eric Xing", "title": "Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic\n  Health Records", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive summarization is very useful for physicians to better manage and\ndigest Electronic Health Records (EHRs). However, the training of a supervised\nmodel requires disease-specific medical background and is thus very expensive.\nWe studied how to utilize the intrinsic correlation between multiple EHRs to\ngenerate pseudo-labels and train a supervised model with no external\nannotation. Experiments on real-patient data validate that our model is\neffective in summarizing crucial disease-specific information for patients.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 01:08:09 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 02:43:37 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 22:01:21 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liu", "Xiangan", ""], ["Xu", "Keyang", ""], ["Xie", "Pengtao", ""], ["Xing", "Eric", ""]]}, {"id": "1811.08048", "submitter": "Oyvind Tafjord", "authors": "Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, Ashish\n  Sabharwal", "title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative\n  Relationships", "comments": "9 pages, AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language questions require recognizing and reasoning with\nqualitative relationships (e.g., in science, economics, and medicine), but are\nchallenging to answer with corpus-based methods. Qualitative modeling provides\ntools that support such reasoning, but the semantic parsing task of mapping\nquestions into those models has formidable challenges. We present QuaRel, a\ndataset of diverse story questions involving qualitative relationships that\ncharacterize these challenges, and techniques that begin to address them. The\ndataset has 2771 questions relating 19 different types of quantities. For\nexample, \"Jenny observes that the robot vacuum cleaner moves slower on the\nliving room carpet than on the bedroom carpet. Which carpet has more friction?\"\nWe contribute (1) a simple and flexible conceptual framework for representing\nthese kinds of questions; (2) the QuaRel dataset, including logical forms,\nexemplifying the parsing challenges; and (3) two novel models for this task,\nbuilt as extensions of type-constrained semantic parsing. The first of these\nmodels (called QuaSP+) significantly outperforms off-the-shelf tools on QuaRel.\nThe second (QuaSP+Zero) demonstrates zero-shot capability, i.e., the ability to\nhandle new qualitative relationships without requiring additional training\ndata, something not possible with previous models. This work thus makes inroads\ninto answering complex, qualitative questions that require reasoning, and\nscaling to new relationships at low cost. The dataset and models are available\nat http://data.allenai.org/quarel.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 02:59:30 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Tafjord", "Oyvind", ""], ["Clark", "Peter", ""], ["Gardner", "Matt", ""], ["Yih", "Wen-tau", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "1811.08065", "submitter": "Feiyang Chen", "authors": "Feiyang Chen, Ziqian Luo", "title": "Learning Robust Heterogeneous Signal Features from Parallel Neural\n  Network for Audio Sentiment Analysis", "comments": "21 pages, PR JOURNAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Sentiment Analysis is a popular research area which extends the\nconventional text-based sentiment analysis to depend on the effectiveness of\nacoustic features extracted from speech. However, current progress on audio\nsentiment analysis mainly focuses on extracting homogeneous acoustic features\nor doesn't fuse heterogeneous features effectively. In this paper, we propose\nan utterance-based deep neural network model, which has a parallel combination\nof Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based\nnetwork, to obtain representative features termed Audio Sentiment Vector (ASV),\nthat can maximally reflect sentiment information in an audio. Specifically, our\nmodel is trained by utterance-level labels and ASV can be extracted and fused\ncreatively from two branches. In the CNN model branch, spectrum graphs produced\nby signals are fed as inputs while in the LSTM model branch, inputs include\nspectral features and cepstrum coefficient extracted from dependent utterances\nin audio. Besides, Bidirectional Long Short-Term Memory (BiLSTM) with attention\nmechanism is used for feature fusion. Extensive experiments have been conducted\nto show our model can recognize audio sentiment precisely and quickly, and\ndemonstrate our ASV is better than traditional acoustic features or vectors\nextracted from other deep learning models. Furthermore, experimental results\nindicate that the proposed model outperforms the state-of-the-art approach by\n9.33\\% on Multimodal Opinion-level Sentiment Intensity dataset (MOSI) dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:10:23 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 06:00:25 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chen", "Feiyang", ""], ["Luo", "Ziqian", ""]]}, {"id": "1811.08078", "submitter": "Sinh Vu Trong", "authors": "Sinh Vu Trong, Minh Nguyen Le", "title": "An empirical evaluation of AMR parsing for legal documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches have been proposed to tackle the problem of Abstract Meaning\nRepresentation (AMR) parsing, helps solving various natural language processing\nissues recently. In our paper, we provide an overview of different methods in\nAMR parsing and their performances when analyzing legal documents. We conduct\nexperiments of different AMR parsers on our annotated dataset extracted from\nthe English version of Japanese Civil Code. Our results show the limitations as\nwell as open a room for improvements of current parsing techniques when\napplying in this complicated domain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:12:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Trong", "Sinh Vu", ""], ["Le", "Minh Nguyen", ""]]}, {"id": "1811.08100", "submitter": "Ryo Nakamura", "authors": "Ryo Nakamura, Katsuhito Sudoh, Koichiro Yoshino, Satoshi Nakamura", "title": "Another Diversity-Promoting Objective Function for Neural Dialogue\n  Generation", "comments": "AAAI 2019 Workshop on Reasoning and Learning for Human-Machine\n  Dialogues (DEEP-DIAL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although generation-based dialogue systems have been widely researched, the\nresponse generations by most existing systems have very low diversities. The\nmost likely reason for this problem is Maximum Likelihood Estimation (MLE) with\nSoftmax Cross-Entropy (SCE) loss. MLE trains models to generate the most\nfrequent responses from enormous generation candidates, although in actual\ndialogues there are various responses based on the context. In this paper, we\npropose a new objective function called Inverse Token Frequency (ITF) loss,\nwhich individually scales smaller loss for frequent token classes and larger\nloss for rare token classes. This function encourages the model to generate\nrare tokens rather than frequent tokens. It does not complicate the model and\nits training is stable because we only replace the objective function. On the\nOpenSubtitles dialogue dataset, our loss model establishes a state-of-the-art\nDIST-1 of 7.56, which is the unigram diversity score, while maintaining a good\nBLEU-1 score. On a Japanese Twitter replies dataset, our loss model achieves a\nDIST-1 score comparable to the ground truth.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:15:31 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 04:06:11 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Nakamura", "Ryo", ""], ["Sudoh", "Katsuhito", ""], ["Yoshino", "Koichiro", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1811.08129", "submitter": "Pranav A", "authors": "Pranav A", "title": "Alignment Analysis of Sequential Segmentation of Lexicons to Improve\n  Automatic Cognate Detection", "comments": "Published at ACL-SRW 2018", "journal-ref": "Proceedings of ACL 2018, Student Research Workshop. 2018", "doi": "10.18653/v1/P18-3019", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking functions in information retrieval are often used in search engines\nto recommend the relevant answers to the query. This paper makes use of this\nnotion of information retrieval and applies onto the problem domain of cognate\ndetection. The main contributions of this paper are: (1) positional\nsegmentation, which incorporates the sequential notion; (2) graphical error\nmodelling, which deduces the transformations. The current research work focuses\non classification problem; which is distinguishing whether a pair of words are\ncognates. This paper focuses on a harder problem, whether we could predict a\npossible cognate from the given input. Our study shows that when language\nmodelling smoothing methods are applied as the retrieval functions and used in\nconjunction with positional segmentation and error modelling gives better\nresults than competing baselines, in both classification and prediction of\ncognates.\n  Source code is at: https://github.com/pranav-ust/cognates\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:59:53 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["A", "Pranav", ""]]}, {"id": "1811.08162", "submitter": "Kedar Tatwawadi", "authors": "Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, Idoia Ochoa", "title": "DeepZip: Lossless Data Compression using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.SP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential data is being generated at an unprecedented pace in various forms,\nincluding text and genomic data. This creates the need for efficient\ncompression mechanisms to enable better storage, transmission and processing of\nsuch data. To solve this problem, many of the existing compressors attempt to\nlearn models for the data and perform prediction-based compression. Since\nneural networks are known as universal function approximators with the\ncapability to learn arbitrarily complex mappings, and in practice show\nexcellent performance in prediction tasks, we explore and devise methods to\ncompress sequential data using neural network predictors. We combine recurrent\nneural network predictors with an arithmetic coder and losslessly compress a\nvariety of synthetic, text and genomic datasets. The proposed compressor\noutperforms Gzip on the real datasets and achieves near-optimal compression for\nthe synthetic datasets. The results also help understand why and where neural\nnetworks are good alternatives for traditional finite context models\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:12:55 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Goyal", "Mohit", ""], ["Tatwawadi", "Kedar", ""], ["Chandak", "Shubham", ""], ["Ochoa", "Idoia", ""]]}, {"id": "1811.08417", "submitter": "Ananda Theertha Suresh", "authors": "Ehsan Variani, Ananda Theertha Suresh, Mitchel Weintraub", "title": "WEST: Word Encoded Sequence Transducers", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the parameters in large vocabulary models are used in embedding layer\nto map categorical features to vectors and in softmax layer for classification\nweights. This is a bottle-neck in memory constraint on-device training\napplications like federated learning and on-device inference applications like\nautomatic speech recognition (ASR). One way of compressing the embedding and\nsoftmax layers is to substitute larger units such as words with smaller\nsub-units such as characters. However, often the sub-unit models perform poorly\ncompared to the larger unit models. We propose WEST, an algorithm for encoding\ncategorical features and output classes with a sequence of random or domain\ndependent sub-units and demonstrate that this transduction can lead to\nsignificant compression without compromising performance. WEST bridges the gap\nbetween larger unit and sub-unit models and can be interpreted as a MaxEnt\nmodel over sub-unit features, which can be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:47:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Variani", "Ehsan", ""], ["Suresh", "Ananda Theertha", ""], ["Weintraub", "Mitchel", ""]]}, {"id": "1811.08465", "submitter": "Marcos Trevisan Dr.", "authors": "Diego E Shalom and Mariano Sigman and Gabriel Mindlin and Marcos A\n  Trevisan", "title": "Fading of collective attention shapes the evolution of linguistic\n  variants", "comments": "8 pages, 2 figures, 3 supplementary figures", "journal-ref": "Phys. Rev. E 100, 020102 (2019)", "doi": "10.1103/PhysRevE.100.020102", "report-no": null, "categories": "cs.CL physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language change involves the competition between alternative linguistic forms\n(1). The spontaneous evolution of these forms typically results in monotonic\ngrowths or decays (2, 3) like in winner-take-all attractor behaviors. In the\ncase of the Spanish past subjunctive, the spontaneous evolution of its two\ncompeting forms (ended in -ra and -se) was perturbed by the appearance of the\nRoyal Spanish Academy in 1713, which enforced the spelling of both forms as\nperfectly interchangeable variants (4), at a moment in which the -ra form was\ndominant (5). Time series extracted from a massive corpus of books (6) reveal\nthat this regulation in fact produced a transient renewed interest for the old\nform -se which, once faded, left the -ra again as the dominant form up to the\npresent day. We show that time series are successfully explained by a\ntwo-dimensional linear model that integrates an imitative and a novelty\ncomponent. The model reveals that the temporal scale over which collective\nattention fades is in inverse proportion to the verb frequency. The integration\nof the two basic mechanisms of imitation and attention to novelty allows to\nunderstand diverse competing objects, with lifetimes that range from hours for\nmemes and news (7, 8) to decades for verbs, suggesting the existence of a\ngeneral mechanism underlying cultural evolution.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 19:54:41 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 14:38:13 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 16:37:49 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 01:47:32 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Shalom", "Diego E", ""], ["Sigman", "Mariano", ""], ["Mindlin", "Gabriel", ""], ["Trevisan", "Marcos A", ""]]}, {"id": "1811.08541", "submitter": "Zhaopeng Tu", "authors": "Xiang Kong, Zhaopeng Tu, Shuming Shi, Eduard Hovy, Tong Zhang", "title": "Neural Machine Translation with Adequacy-Oriented Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Neural Machine Translation (NMT) models have advanced\nstate-of-the-art performance in machine translation, they face problems like\nthe inadequate translation. We attribute this to that the standard Maximum\nLikelihood Estimation (MLE) cannot judge the real translation quality due to\nits several limitations. In this work, we propose an adequacy-oriented learning\nmechanism for NMT by casting translation as a stochastic policy in\nReinforcement Learning (RL), where the reward is estimated by explicitly\nmeasuring translation adequacy. Benefiting from the sequence-level training of\nRL strategy and a more accurate reward designed specifically for translation,\nour model outperforms multiple strong baselines, including (1) standard and\ncoverage-augmented attention models with MLE-based training, and (2) advanced\nreinforcement and adversarial training strategies with rewards based on both\nword-level BLEU and character-level chrF3. Quantitative and qualitative\nanalyses on different language pairs and NMT architectures demonstrate the\neffectiveness and universality of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 01:48:22 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Kong", "Xiang", ""], ["Tu", "Zhaopeng", ""], ["Shi", "Shuming", ""], ["Hovy", "Eduard", ""], ["Zhang", "Tong", ""]]}, {"id": "1811.08600", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Shuaichen Chang, Xuanjing Huang, Jian Tang, Jackie Chi\n  Kit Cheung", "title": "Contextualized Non-local Neural Networks for Sequence Learning", "comments": "Accepted by AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a large number of neural mechanisms and models have been proposed\nfor sequence learning, of which self-attention, as exemplified by the\nTransformer model, and graph neural networks (GNNs) have attracted much\nattention. In this paper, we propose an approach that combines and draws on the\ncomplementary strengths of these two methods. Specifically, we propose\ncontextualized non-local neural networks (CN$^{\\textbf{3}}$), which can both\ndynamically construct a task-specific structure of a sentence and leverage rich\nlocal dependencies within a particular neighborhood.\n  Experimental results on ten NLP tasks in text classification, semantic\nmatching, and sequence labeling show that our proposed model outperforms\ncompetitive baselines and discovers task-specific dependency structures, thus\nproviding better interpretability to users.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 05:14:37 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Liu", "Pengfei", ""], ["Chang", "Shuaichen", ""], ["Huang", "Xuanjing", ""], ["Tang", "Jian", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "1811.08603", "submitter": "Yixin Cao", "authors": "Yixin Cao and Lei Hou and Juanzi Li and Zhiyuan Liu", "title": "Neural Collective Entity Linking", "comments": "12 pages, 3 figures, COLING2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Linking aims to link entity mentions in texts to knowledge bases, and\nneural models have achieved recent success in this task. However, most existing\nmethods rely on local contexts to resolve entities independently, which may\nusually fail due to the data sparsity of local information. To address this\nissue, we propose a novel neural model for collective entity linking, named as\nNCEL. NCEL applies Graph Convolutional Network to integrate both local\ncontextual features and global coherence information for entity linking. To\nimprove the computation efficiency, we approximately perform graph convolution\non a subgraph of adjacent entity mentions instead of those in the entire text.\nWe further introduce an attention scheme to improve the robustness of NCEL to\ndata noise and train the model on Wikipedia hyperlinks to avoid overfitting and\ndomain bias. In experiments, we evaluate NCEL on five publicly available\ndatasets to verify the linking performance as well as generalization ability.\nWe also conduct an extensive analysis of time complexity, the impact of key\nmodules, and qualitative results, which demonstrate the effectiveness and\nefficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 06:00:23 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Cao", "Yixin", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1811.08610", "submitter": "Yiming Cui", "authors": "Zhipeng Chen, Yiming Cui, Wentao Ma, Shijin Wang, Guoping Hu", "title": "Convolutional Spatial Attention Model for Reading Comprehension with\n  Multiple-Choice Questions", "comments": "8 pages. Accepted as a conference paper at AAAI-19 Technical Track", "journal-ref": "AAAI 2019 6276-6283", "doi": "10.1609/aaai.v33i01.33016276", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine Reading Comprehension (MRC) with multiple-choice questions requires\nthe machine to read given passage and select the correct answer among several\ncandidates. In this paper, we propose a novel approach called Convolutional\nSpatial Attention (CSA) model which can better handle the MRC with\nmultiple-choice questions. The proposed model could fully extract the mutual\ninformation among the passage, question, and the candidates, to form the\nenriched representations. Furthermore, to merge various attention results, we\npropose to use convolutional operation to dynamically summarize the attention\nvalues within the different size of regions. Experimental results show that the\nproposed model could give substantial improvements over various\nstate-of-the-art systems on both RACE and SemEval-2018 Task11 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 06:42:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chen", "Zhipeng", ""], ["Cui", "Yiming", ""], ["Ma", "Wentao", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1811.08615", "submitter": "Tzu-Ming Harry Hsu", "authors": "Tzu-Ming Harry Hsu, Wei-Hung Weng, Willie Boag, Matthew McDermott,\n  Peter Szolovits", "title": "Unsupervised Multimodal Representation Learning across Medical Images\n  and Reports", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/215", "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint embeddings between medical imaging modalities and associated radiology\nreports have the potential to offer significant benefits to the clinical\ncommunity, ranging from cross-domain retrieval to conditional generation of\nreports to the broader goals of multimodal representation learning. In this\nwork, we establish baseline joint embedding results measured via both local and\nglobal retrieval methods on the soon to be released MIMIC-CXR dataset\nconsisting of both chest X-ray images and the associated radiology reports. We\nexamine both supervised and unsupervised methods on this task and show that for\ndocument retrieval tasks with the learned representations, only a limited\namount of supervision is needed to yield results comparable to those of\nfully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:24:31 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Hsu", "Tzu-Ming Harry", ""], ["Weng", "Wei-Hung", ""], ["Boag", "Willie", ""], ["McDermott", "Matthew", ""], ["Szolovits", "Peter", ""]]}, {"id": "1811.08619", "submitter": "Saurav Jha", "authors": "Saurav Jha, Akhilesh Sudhakar and Anil Kumar Singh", "title": "Multi Task Deep Morphological Analyzer: Context Aware Joint\n  Morphological Tagging and Lemma Prediction", "comments": "28 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ambiguities introduced by the recombination of morphemes constructing\nseveral possible inflections for a word makes the prediction of syntactic\ntraits in Morphologically Rich Languages (MRLs) a notoriously complicated task.\nWe propose the Multi Task Deep Morphological analyzer (MT-DMA), a\ncharacter-level neural morphological analyzer based on multitask learning of\nword-level tag markers for Hindi and Urdu. MT-DMA predicts a set of six\nmorphological tags for words of Indo-Aryan languages: Parts-of-speech (POS),\nGender (G), Number (N), Person (P), Case (C), Tense-Aspect-Modality (TAM)\nmarker as well as the Lemma (L) by jointly learning all these in one trainable\nframework. We show the effectiveness of training of such deep neural networks\nby the simultaneous optimization of multiple loss functions and sharing of\ninitial parameters for context-aware morphological analysis. Exploiting\ncharacter-level features in phonological space optimized for each tag using\nmulti-objective genetic algorithm, our model establishes a new state-of-the-art\naccuracy score upon all seven of the tasks for both the languages. MT-DMA is\npublicly accessible: code, models and data are available at\nhttps://github.com/Saurav0074/morph_analyzer.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:55:16 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 20:19:42 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Jha", "Saurav", ""], ["Sudhakar", "Akhilesh", ""], ["Singh", "Anil Kumar", ""]]}, {"id": "1811.08705", "submitter": "Joewie Koh", "authors": "Joewie J. Koh and Barton Rhodes", "title": "Inline Detection of Domain Generation Algorithms with Context-Sensitive\n  Word Embeddings", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": "Proceedings of the 2018 IEEE International Conference on Big Data,\n  2018, pp. 2966-2971", "doi": "10.1109/BigData.2018.8622066", "report-no": null, "categories": "cs.CR cs.CL cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generation algorithms (DGAs) are frequently employed by malware to\ngenerate domains used for connecting to command-and-control (C2) servers.\nRecent work in DGA detection leveraged deep learning architectures like\nconvolutional neural networks (CNNs) and character-level long short-term memory\nnetworks (LSTMs) to classify domains. However, these classifiers perform poorly\nwith wordlist-based DGA families, which generate domains by pseudorandomly\nconcatenating dictionary words. We propose a novel approach that combines\ncontext-sensitive word embeddings with a simple fully-connected classifier to\nperform classification of domains based on word-level information. The word\nembeddings were pre-trained on a large unrelated corpus and left frozen during\nthe training on domain data. The resulting small number of trainable parameters\nenabled extremely short training durations, while the transfer of language\nknowledge stored in the representations allowed for high-performing models with\nsmall training datasets. We show that this architecture reliably outperformed\nexisting techniques on wordlist-based DGA families with just 30 DGA training\nexamples and achieved state-of-the-art performance with around 100 DGA training\nexamples, all while requiring an order of magnitude less time to train compared\nto current techniques. Of special note is the technique's performance on the\nmatsnu DGA: the classifier attained a 89.5% detection rate with a 1:1,000 false\npositive rate (FPR) after training on only 30 examples of the DGA domains, and\na 91.2% detection rate with a 1:10,000 FPR after 90 examples. Considering that\nsome of these DGAs have wordlists of several hundred words, our results\ndemonstrate that this technique does not rely on the classifier learning the\nDGA wordlists. Instead, the classifier is able to learn the semantic signatures\nof the wordlist-based DGA families.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 12:14:12 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Koh", "Joewie J.", ""], ["Rhodes", "Barton", ""]]}, {"id": "1811.08757", "submitter": "Zeljko Agic", "authors": "Barbara Plank, Sigrid Klerke, Zeljko Agic", "title": "The Best of Both Worlds: Lexical Resources To Improve Low-Resource\n  Part-of-Speech Tagging", "comments": "Under review for Natural Language Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language processing, the deep learning revolution has shifted the\nfocus from conventional hand-crafted symbolic representations to dense inputs,\nwhich are adequate representations learned automatically from corpora. However,\nparticularly when working with low-resource languages, small amounts of\nsymbolic lexical resources such as user-generated lexicons are often available\neven when gold-standard corpora are not. Such additional linguistic information\nis though often neglected, and recent neural approaches to cross-lingual\ntagging typically rely only on word and subword embeddings. While these\nrepresentations are effective, our recent work has shown clear benefits of\ncombining the best of both worlds: integrating conventional lexical information\nimproves neural cross-lingual part-of-speech (PoS) tagging. However, little is\nknown on how complementary such additional information is, and to what extent\nimprovements depend on the coverage and quality of these external resources.\nThis paper seeks to fill this gap by providing the first thorough analysis on\nthe contributions of lexical resources for cross-lingual PoS tagging in neural\ntimes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:36:30 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Plank", "Barbara", ""], ["Klerke", "Sigrid", ""], ["Agic", "Zeljko", ""]]}, {"id": "1811.08816", "submitter": "Saurav Jha", "authors": "Saurav Jha, Akhilesh Sudhakar and Anil Kumar Singh", "title": "Learning cross-lingual phonological and orthagraphic adaptations: a case\n  study in improving neural machine translation between low-resource languages", "comments": "47 pages, 4 figures, 21 tables (including Appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Out-of-vocabulary (OOV) words can pose serious challenges for machine\ntranslation (MT) tasks, and in particular, for low-resource language (LRL)\npairs, i.e., language pairs for which few or no parallel corpora exist. Our\nwork adapts variants of seq2seq models to perform transduction of such words\nfrom Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs\nbuilt from a bilingual dictionary of Hindi--Bhojpuri words. We demonstrate that\nour models can be effectively used for language pairs that have limited\nparallel corpora; our models work at the character level to grasp phonetic and\northographic similarities across multiple types of word adaptations, whether\nsynchronic or diachronic, loan words or cognates. We describe the training\naspects of several character level NMT systems that we adapted to this task and\ncharacterize their typical errors. Our method improves BLEU score by 6.3 on the\nHindi-to-Bhojpuri translation task. Further, we show that such transductions\ncan generalize well to other languages by applying it successfully to Hindi --\nBangla cognate pairs. Our work can be seen as an important step in the process\nof: (i) resolving the OOV words problem arising in MT tasks, (ii) creating\neffective parallel corpora for resource-constrained languages, and (iii)\nleveraging the enhanced semantic knowledge captured by word-level embeddings to\nperform character-level tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:36:08 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 11:02:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jha", "Saurav", ""], ["Sudhakar", "Akhilesh", ""], ["Singh", "Anil Kumar", ""]]}, {"id": "1811.08853", "submitter": "Ya-Hui An", "authors": "Ya-Hui An, Liangming Pan, Min-Yen Kan, Qiang Dong, Yan Fu", "title": "Resource Mention Extraction for MOOC Discussion Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In discussions hosted on discussion forums for MOOCs, references to online\nlearning resources are often of central importance. They contextualize the\ndiscussion, anchoring the discussion participants' presentation of the issues\nand their understanding. However they are usually mentioned in free text,\nwithout appropriate hyperlinking to their associated resource. Automated\nlearning resource mention hyperlinking and categorization will facilitate\ndiscussion and searching within MOOC forums, and also benefit the\ncontextualization of such resources across disparate views. We propose the\nnovel problem of learning resource mention identification in MOOC forums. As\nthis is a novel task with no publicly available data, we first contribute a\nlarge-scale labeled dataset, dubbed the Forum Resource Mention (FoRM) dataset,\nto facilitate our current research and future research on this task. We then\nformulate this task as a sequence tagging problem and investigate solution\narchitectures to address the problem. Importantly, we identify two major\nchallenges that hinder the application of sequence tagging models to the task:\n(1) the diversity of resource mention expression, and (2) long-range contextual\ndependencies. We address these challenges by incorporating character-level and\nthread context information into a LSTM-CRF model. First, we incorporate a\ncharacter encoder to address the out-of-vocabulary problem caused by the\ndiversity of mention expressions. Second, to address the context dependency\nchallenge, we encode thread contexts using an RNN-based context encoder, and\napply the attention mechanism to selectively leverage useful context\ninformation during sequence tagging. Experiments on FoRM show that the proposed\nmethod improves the baseline deep sequence tagging models notably,\nsignificantly bettering performance on instances that exemplify the two\nchallenges.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:59:56 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["An", "Ya-Hui", ""], ["Pan", "Liangming", ""], ["Kan", "Min-Yen", ""], ["Dong", "Qiang", ""], ["Fu", "Yan", ""]]}, {"id": "1811.08935", "submitter": "Gholamreza Anbarjafari", "authors": "Fatemeh Noroozi, Marina Marjanovic, Angelina Njegus, Sergio Escalera,\n  Gholamreza Anbarjafari", "title": "A Study of Language and Classifier-independent Feature Analysis for\n  Vocal Emotion Recognition", "comments": "24 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every speech signal carries implicit information about the emotions, which\ncan be extracted by speech processing methods. In this paper, we propose an\nalgorithm for extracting features that are independent from the spoken language\nand the classification method to have comparatively good recognition\nperformance on different languages independent from the employed classification\nmethods. The proposed algorithm is composed of three stages. In the first\nstage, we propose a feature ranking method analyzing the state-of-the-art voice\nquality features. In the second stage, we propose a method for finding the\nsubset of the common features for each language and classifier. In the third\nstage, we compare our approach with the recognition rate of the\nstate-of-the-art filter methods. We use three databases with different\nlanguages, namely, Polish, Serbian and English. Also three different\nclassifiers, namely, nearest neighbour, support vector machine and gradient\ndescent neural network, are employed. It is shown that our method for selecting\nthe most significant language-independent and method-independent features in\nmany cases outperforms state-of-the-art filter methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:54:24 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Noroozi", "Fatemeh", ""], ["Marjanovic", "Marina", ""], ["Njegus", "Angelina", ""], ["Escalera", "Sergio", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1811.09010", "submitter": "Zhong-Qiu Wang", "authors": "Zhong-Qiu Wang, Ke Tan, DeLiang Wang", "title": "Deep Learning Based Phase Reconstruction for Speaker Separation: A\n  Trigonometric Perspective", "comments": "5 pages, in submission to ICASSP-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates phase reconstruction for deep learning based monaural\ntalker-independent speaker separation in the short-time Fourier transform\n(STFT) domain. The key observation is that, for a mixture of two sources, with\ntheir magnitudes accurately estimated and under a geometric constraint, the\nabsolute phase difference between each source and the mixture can be uniquely\ndetermined; in addition, the source phases at each time-frequency (T-F) unit\ncan be narrowed down to only two candidates. To pick the right candidate, we\npropose three algorithms based on iterative phase reconstruction, group delay\nestimation, and phase-difference sign prediction. State-of-the-art results are\nobtained on the publicly available wsj0-2mix and 3mix corpus.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 03:46:44 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wang", "Zhong-Qiu", ""], ["Tan", "Ke", ""], ["Wang", "DeLiang", ""]]}, {"id": "1811.09021", "submitter": "Bo Li", "authors": "Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, William Chan", "title": "Bytes are All You Need: End-to-End Multilingual Speech Recognition and\n  Synthesis with Bytes", "comments": "submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two end-to-end models: Audio-to-Byte (A2B) and Byte-to-Audio\n(B2A), for multilingual speech recognition and synthesis. Prior work has\npredominantly used characters, sub-words or words as the unit of choice to\nmodel text. These units are difficult to scale to languages with large\nvocabularies, particularly in the case of multilingual processing. In this\nwork, we model text via a sequence of Unicode bytes, specifically, the UTF-8\nvariable length byte sequence for each character. Bytes allow us to avoid large\nsoftmaxes in languages with large vocabularies, and share representations in\nmultilingual models. We show that bytes are superior to grapheme characters\nover a wide variety of languages in monolingual end-to-end speech recognition.\nAdditionally, our multilingual byte model outperform each respective single\nlanguage baseline on average by 4.4% relatively. In Japanese-English\ncode-switching speech, our multilingual byte model outperform our monolingual\nbaseline by 38.6% relatively. Finally, we present an end-to-end multilingual\nspeech synthesis model using byte representations which matches the performance\nof our monolingual baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:37:55 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Bo", ""], ["Zhang", "Yu", ""], ["Sainath", "Tara", ""], ["Wu", "Yonghui", ""], ["Chan", "William", ""]]}, {"id": "1811.09242", "submitter": "Reinald Kim Amplayo", "authors": "Reinald Kim Amplayo and Seung-won Hwang and Min Song", "title": "AutoSense Model for Word Sense Induction", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense induction (WSI), or the task of automatically discovering multiple\nsenses or meanings of a word, has three main challenges: domain adaptability,\nnovel sense detection, and sense granularity flexibility. While current latent\nvariable models are known to solve the first two challenges, they are not\nflexible to different word sense granularities, which differ very much among\nwords, from aardvark with one sense, to play with over 50 senses. Current\nmodels either require hyperparameter tuning or nonparametric induction of the\nnumber of senses, which we find both to be ineffective. Thus, we aim to\neliminate these requirements and solve the sense granularity problem by\nproposing AutoSense, a latent variable model based on two observations: (1)\nsenses are represented as a distribution over topics, and (2) senses generate\npairings between the target word and its neighboring word. These observations\nalleviate the problem by (a) throwing garbage senses and (b) additionally\ninducing fine-grained word senses. Results show great improvements over the\nstate-of-the-art models on popular WSI datasets. We also show that AutoSense is\nable to learn the appropriate sense granularity of a word. Finally, we apply\nAutoSense to the unsupervised author name disambiguation task where the sense\ngranularity problem is more evident and show that AutoSense is evidently better\nthan competing models. We share our data and code here:\nhttps://github.com/rktamplayo/AutoSense.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:19:31 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Amplayo", "Reinald Kim", ""], ["Hwang", "Seung-won", ""], ["Song", "Min", ""]]}, {"id": "1811.09353", "submitter": "Kazuya Kawakami", "authors": "Kazuya Kawakami, Chris Dyer, Phil Blunsom", "title": "Learning to Discover, Ground and Use Words with Segmental Neural\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a segmental neural language model that combines the generalization\npower of neural networks with the ability to discover word-like units that are\nlatent in unsegmented character sequences. In contrast to previous segmentation\nmodels that treat word segmentation as an isolated task, our model unifies word\ndiscovery, learning how words fit together to form sentences, and, by\nconditioning the model on visual context, how words' meanings ground in\nrepresentations of non-linguistic modalities. Experiments show that the\nunconditional model learns predictive distributions better than character LSTM\nmodels, discovers words competitively with nonparametric Bayesian word\nsegmentation models, and that modeling language conditional on visual context\nimproves performance on both.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 03:39:27 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 09:21:34 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Kawakami", "Kazuya", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1811.09362", "submitter": "Yansen Wang", "authors": "Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh,\n  Louis-Philippe Morency", "title": "Words Can Shift: Dynamically Adjusting Word Representations Using\n  Nonverbal Behaviors", "comments": "Accepted by AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans convey their intentions through the usage of both verbal and nonverbal\nbehaviors during face-to-face communication. Speaker intentions often vary\ndynamically depending on different nonverbal contexts, such as vocal patterns\nand facial expressions. As a result, when modeling human language, it is\nessential to not only consider the literal meaning of the words but also the\nnonverbal contexts in which these words appear. To better model human language,\nwe first model expressive nonverbal representations by analyzing the\nfine-grained visual and acoustic patterns that occur during word segments. In\naddition, we seek to capture the dynamic nature of nonverbal intents by\nshifting word representations based on the accompanying nonverbal behaviors. To\nthis end, we propose the Recurrent Attended Variation Embedding Network (RAVEN)\nthat models the fine-grained structure of nonverbal subword sequences and\ndynamically shifts word representations based on nonverbal cues. Our proposed\nmodel achieves competitive performance on two publicly available datasets for\nmultimodal sentiment analysis and emotion recognition. We also visualize the\nshifted word representations in different nonverbal contexts and summarize\ncommon patterns regarding multimodal variations of word representations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 05:13:38 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 03:28:36 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Yansen", ""], ["Shen", "Ying", ""], ["Liu", "Zhun", ""], ["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1811.09364", "submitter": "Younggun Lee", "authors": "Younggun Lee and Suwon Shon and Taesu Kim", "title": "Learning pronunciation from a foreign language in speech synthesis\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are more than 6,500 languages in the world, the pronunciations\nof many phonemes sound similar across the languages. When people learn a\nforeign language, their pronunciation often reflects their native language's\ncharacteristics. This motivates us to investigate how the speech synthesis\nnetwork learns the pronunciation from datasets from different languages. In\nthis study, we are interested in analyzing and taking advantage of multilingual\nspeech synthesis network. First, we train the speech synthesis network\nbilingually in English and Korean and analyze how the network learns the\nrelations of phoneme pronunciation between the languages. Our experimental\nresult shows that the learned phoneme embedding vectors are located closer if\ntheir pronunciations are similar across the languages. Consequently, the\ntrained networks can synthesize the English speakers' Korean speech and vice\nversa. Using this result, we propose a training framework to utilize\ninformation from a different language. To be specific, we pre-train a speech\nsynthesis network using datasets from both high-resource language and\nlow-resource language, then we fine-tune the network using the low-resource\nlanguage dataset. Finally, we conducted more simulations on 10 different\nlanguages to show it is generally extendable to other languages.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 05:24:15 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 07:50:01 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 10:47:30 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 00:57:25 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Lee", "Younggun", ""], ["Shon", "Suwon", ""], ["Kim", "Taesu", ""]]}, {"id": "1811.09368", "submitter": "Riddhiman Dasgupta", "authors": "Riddhiman Dasgupta, Balaji Ganesan, Aswin Kannan, Berthold Reinwald,\n  Arun Kumar", "title": "Fine Grained Classification of Personal Data Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Type Classification can be defined as the task of assigning category\nlabels to entity mentions in documents. While neural networks have recently\nimproved the classification of general entity mentions, pattern matching and\nother systems continue to be used for classifying personal data entities (e.g.\nclassifying an organization as a media company or a government institution for\nGDPR, and HIPAA compliance). We propose a neural model to expand the class of\npersonal data entities that can be classified at a fine grained level, using\nthe output of existing pattern matching systems as additional contextual\nfeatures. We introduce new resources, a personal data entities hierarchy with\n134 types, and two datasets from the Wikipedia pages of elected representatives\nand Enron emails. We hope these resource will aid research in the area of\npersonal data discovery, and to that effect, we provide baseline results on\nthese datasets, and compare our method with state of the art models on\nOntoNotes dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 06:28:41 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Dasgupta", "Riddhiman", ""], ["Ganesan", "Balaji", ""], ["Kannan", "Aswin", ""], ["Reinwald", "Berthold", ""], ["Kumar", "Arun", ""]]}, {"id": "1811.09386", "submitter": "Cunxiao Du", "authors": "Cunxiao Du, Zhaozheng Chin, Fuli Feng, Lei Zhu, Tian Gan, Liqiang Nie", "title": "Explicit Interaction Model towards Text Classification", "comments": "8 pages", "journal-ref": "AAAI 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is one of the fundamental tasks in natural language\nprocessing. Recently, deep neural networks have achieved promising performance\nin the text classification task compared to shallow models. Despite of the\nsignificance of deep models, they ignore the fine-grained (matching signals\nbetween words and classes) classification clues since their classifications\nmainly rely on the text-level representations. To address this problem, we\nintroduce the interaction mechanism to incorporate word-level matching signals\ninto the text classification task. In particular, we design a novel framework,\nEXplicit interAction Model (dubbed as EXAM), equipped with the interaction\nmechanism. We justified the proposed approach on several benchmark datasets\nincluding both multi-label and multi-class text classification tasks. Extensive\nexperimental results demonstrate the superiority of the proposed method. As a\nbyproduct, we have released the codes and parameter settings to facilitate\nother researches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 08:30:19 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Du", "Cunxiao", ""], ["Chin", "Zhaozheng", ""], ["Feng", "Fuli", ""], ["Zhu", "Lei", ""], ["Gan", "Tian", ""], ["Nie", "Liqiang", ""]]}, {"id": "1811.09417", "submitter": "Antoine Neuraz", "authors": "Antoine Neuraz, Leonardo Campillos Llanos, Anita Burgun, Sophie Rosset", "title": "Natural language understanding for task oriented dialog in the\n  biomedical domain in a low resources context", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the biomedical domain, the lack of sharable datasets often limit the\npossibility of developing natural language processing systems, especially\ndialogue applications and natural language understanding models. To overcome\nthis issue, we explore data generation using templates and terminologies and\ndata augmentation approaches. Namely, we report our experiments using\nparaphrasing and word representations learned on a large EHR corpus with\nFasttext and ELMo, to learn a NLU model without any available dataset. We\nevaluate on a NLU task of natural language queries in EHRs divided in\nslot-filling and intent classification sub-tasks. On the slot-filling task, we\nobtain a F-score of 0.76 with the ELMo representation; and on the\nclassification task, a mean F-score of 0.71. Our results show that this method\ncould be used to develop a baseline system.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 10:20:02 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 08:59:42 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Neuraz", "Antoine", ""], ["Llanos", "Leonardo Campillos", ""], ["Burgun", "Anita", ""], ["Rosset", "Sophie", ""]]}, {"id": "1811.09529", "submitter": "Agnieszka Lawrynowicz", "authors": "Dawid Wisniewski, Jedrzej Potoniec, Agnieszka Lawrynowicz, C. Maria\n  Keet", "title": "Competency Questions and SPARQL-OWL Queries Dataset and Analysis", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competency Questions (CQs) are natural language questions outlining and\nconstraining the scope of knowledge represented by an ontology. Despite that\nCQs are a part of several ontology engineering methodologies, we have observed\nthat the actual publication of CQs for the available ontologies is very limited\nand even scarcer is the publication of their respective formalisations in terms\nof, e.g., SPARQL queries. This paper aims to contribute to addressing the\nengineering shortcomings of using CQs in ontology development, to facilitate\nwider use of CQs. In order to understand the relation between CQs and the\nqueries over the ontology to test the CQs on an ontology, we gather, analyse,\nand publicly release a set of 234 CQs and their translations to SPARQL-OWL for\nseveral ontologies in different domains developed by different groups. We\nanalysed the CQs in two principal ways. The first stage focused on a linguistic\nanalysis of the natural language text itself, i.e., a lexico-syntactic analysis\nwithout any presuppositions of ontology elements, and a subsequent step of\nsemantic analysis in order to find patterns. This increased diversity of CQ\nsources resulted in a 5-fold increase of hitherto published patterns, to 106\ndistinct CQ patterns, which have a limited subset of few patterns shared across\nthe CQ sets from the different ontologies. Next, we analysed the relation\nbetween the found CQ patterns and the 46 SPARQL-OWL query signatures, which\nrevealed that one CQ pattern may be realised by more than one SPARQL-OWL query\nsignature, and vice versa. We hope that our work will contribute to\nestablishing common practices, templates, automation, and user tools that will\nsupport CQ formulation, formalisation, execution, and general management.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:00:51 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wisniewski", "Dawid", ""], ["Potoniec", "Jedrzej", ""], ["Lawrynowicz", "Agnieszka", ""], ["Keet", "C. Maria", ""]]}, {"id": "1811.09575", "submitter": "Si Zuo", "authors": "Si Zuo, Zhimin Xu", "title": "A Hierarchical Neural Network for Sequence-to-Sequences Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the sequence-to-sequence learning neural networks with\nattention mechanism have achieved great progress. However, there are still\nchallenges, especially for Neural Machine Translation (NMT), such as lower\ntranslation quality on long sentences. In this paper, we present a hierarchical\ndeep neural network architecture to improve the quality of long sentences\ntranslation. The proposed network embeds sequence-to-sequence neural networks\ninto a two-level category hierarchy by following the coarse-to-fine paradigm.\nLong sentences are input by splitting them into shorter sequences, which can be\nwell processed by the coarse category network as the long distance dependencies\nfor short sentences is able to be handled by network based on\nsequence-to-sequence neural network. Then they are concatenated and corrected\nby the fine category network. The experiments shows that our method can achieve\nsuperior results with higher BLEU(Bilingual Evaluation Understudy) scores,\nlower perplexity and better performance in imitating expression style and words\nusage than the traditional networks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:40:30 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Zuo", "Si", ""], ["Xu", "Zhimin", ""]]}, {"id": "1811.09607", "submitter": "Eduardo Paluzo-Hidalgo", "authors": "R. Gonzalez-Diaz, E. Paluzo-Hidalgo, J.F. Quesada", "title": "Towards Emotion Recognition: A Persistent Entropy Application", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-10828-1_8", "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition and classification is a very active area of research. In\nthis paper, we present a first approach to emotion classification using\npersistent entropy and support vector machines. A topology-based model is\napplied to obtain a single real number from each raw signal. These data are\nused as input of a support vector machine to classify signals into 8 different\nemotions (calm, happy, sad, angry, fearful, disgust and surprised).\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:20:43 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Gonzalez-Diaz", "R.", ""], ["Paluzo-Hidalgo", "E.", ""], ["Quesada", "J. F.", ""]]}, {"id": "1811.09688", "submitter": "Haruna Isah", "authors": "Mandeep Singh Kandhari, Farhana Zulkernine, Haruna Isah", "title": "A Voice Controlled E-Commerce Web Application", "comments": "7 pages", "journal-ref": null, "doi": "10.1109/IEMCON.2018.8614771", "report-no": null, "categories": "cs.CY cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic voice-controlled systems have changed the way humans interact with\na computer. Voice or speech recognition systems allow a user to make a\nhands-free request to the computer, which in turn processes the request and\nserves the user with appropriate responses. After years of research and\ndevelopments in machine learning and artificial intelligence, today\nvoice-controlled technologies have become more efficient and are widely applied\nin many domains to enable and improve human-to-human and human-to-computer\ninteractions. The state-of-the-art e-commerce applications with the help of web\ntechnologies offer interactive and user-friendly interfaces. However, there are\nsome instances where people, especially with visual disabilities, are not able\nto fully experience the serviceability of such applications. A voice-controlled\nsystem embedded in a web application can enhance user experience and can\nprovide voice as a means to control the functionality of e-commerce websites.\nIn this paper, we propose a taxonomy of speech recognition systems (SRS) and\npresent a voice-controlled commodity purchase e-commerce application using IBM\nWatson speech-to-text to demonstrate its usability. The prototype can be\nextended to other application scenarios such as government service kiosks and\nenable analytics of the converted text data for scenarios such as medical\ndiagnosis at the clinics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 01:35:09 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Kandhari", "Mandeep Singh", ""], ["Zulkernine", "Farhana", ""], ["Isah", "Haruna", ""]]}, {"id": "1811.09725", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Yoshua Bengio", "title": "Interpretable Convolutional Filters with SincNet", "comments": "In Proceedings of NIPS@IRASL 2018. arXiv admin note: substantial text\n  overlap with arXiv:1808.00158", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal \"black-box\" representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 23:13:09 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 16:09:38 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1811.09740", "submitter": "Zhiting Hu", "authors": "Bowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Eric Xing", "title": "Connecting the Dots Between MLE and RL for Sequence Prediction", "comments": "Major revision. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence prediction models can be learned from example sequences with a\nvariety of training algorithms. Maximum likelihood learning is simple and\nefficient, yet can suffer from compounding error at test time. Reinforcement\nlearning such as policy gradient addresses the issue but can have prohibitively\npoor exploration efficiency. A rich set of other algorithms such as RAML, SPG,\nand data noising, have also been developed from different perspectives. This\npaper establishes a formal connection between these algorithms. We present a\ngeneralized entropy regularized policy optimization formulation, and show that\nthe apparently distinct algorithms can all be reformulated as special instances\nof the framework, with the only difference being the configurations of a reward\nfunction and a couple of hyperparameters. The unified interpretation offers a\nsystematic view of the varying properties of exploration and learning\nefficiency. Besides, inspired from the framework, we present a new algorithm\nthat dynamically interpolates among the family of algorithms for scheduled\nsequence model learning. Experiments on machine translation, text\nsummarization, and game imitation learning demonstrate the superiority of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:33:39 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 19:44:06 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Tan", "Bowen", ""], ["Hu", "Zhiting", ""], ["Yang", "Zichao", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric", ""]]}, {"id": "1811.09755", "submitter": "Xinzhi Wang", "authors": "Xinzhi Wang, Shengcheng Yuan, Hui Zhang, Yi Liu", "title": "Estimation of Inter-Sentiment Correlations Employing Deep Neural Network\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on sentiment mining and sentiment correlation analysis of\nweb events. Although neural network models have contributed a lot to mining\ntext information, little attention is paid to analysis of the inter-sentiment\ncorrelations. This paper fills the gap between sentiment calculation and\ninter-sentiment correlations. In this paper, the social emotion is divided into\nsix categories: love, joy, anger, sadness, fear, and surprise. Two deep neural\nnetwork models are presented for sentiment calculation. Three datasets - the\ntitles, the bodies, the comments of news articles - are collected, covering\nboth objective and subjective texts in varying lengths (long and short). From\neach dataset, three kinds of features are extracted: explicit expression,\nimplicit expression, and alphabet characters. The performance of the two models\nare analyzed, with respect to each of the three kinds of the features. There is\ncontroversial phenomenon on the interpretation of anger (fn) and love (gd). In\nsubjective text, other emotions are easily to be considered as anger. By\ncontrast, in objective news bodies and titles, it is easy to regard text as\ncaused love (gd). It means, journalist may want to arouse emotion love by\nwriting news, but cause anger after the news is published. This result reflects\nthe sentiment complexity and unpredictability.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 03:47:04 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Xinzhi", ""], ["Yuan", "Shengcheng", ""], ["Zhang", "Hui", ""], ["Liu", "Yi", ""]]}, {"id": "1811.09785", "submitter": "Aigul Nugmanova", "authors": "Aigul Nugmanova, Andrei Smirnov, Galina Lavrentyeva, Irina Chernykh", "title": "Strategy of the Negative Sampling for Training Retrieval-Based Dialogue\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes the new approach for quality improvement of automated\ndialogue systems for customer support service. Analysis produced in the paper\ndemonstrates the dependency of the quality of the retrieval-based dialogue\nsystem quality on the choice of negative responses. The proposed approach\nimplies choosing the negative samples according to the distribution of\nresponses in the train set. In this implementation the negative samples are\nrandomly chosen from the original response distribution and from the\n\"artificial\" distribution of negative responses, such as uniform distribution\nor the distribution obtained by transformation of the original one. The results\nobtained for the implemented systems and reported in this paper confirm the\nsignificant improvement of automated dialogue systems quality in case of using\nthe negative responses from transformed distribution.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 08:04:42 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Nugmanova", "Aigul", ""], ["Smirnov", "Andrei", ""], ["Lavrentyeva", "Galina", ""], ["Chernykh", "Irina", ""]]}, {"id": "1811.09786", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Recurrently Controlled Recurrent Networks", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) such as long short-term memory and gated\nrecurrent units are pivotal building blocks across a broad spectrum of sequence\nmodeling problems. This paper proposes a recurrently controlled recurrent\nnetwork (RCRN) for expressive and powerful sequence encoding. More concretely,\nthe key idea behind our approach is to learn the recurrent gating functions\nusing recurrent networks. Our architecture is split into two components - a\ncontroller cell and a listener cell whereby the recurrent controller actively\ninfluences the compositionality of the listener cell. We conduct extensive\nexperiments on a myriad of tasks in the NLP domain such as sentiment analysis\n(SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment\nclassification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading\ncomprehension (NarrativeQA). Across all 26 datasets, our results demonstrate\nthat RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs,\nsuggesting that our controller architecture might be a suitable replacement for\nthe widely adopted stacked architecture.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 08:15:50 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1811.10092", "submitter": "Xin Wang", "authors": "Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen,\n  Yuan-Fang Wang, William Yang Wang, Lei Zhang", "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning\n  for Vision-Language Navigation", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-language navigation (VLN) is the task of navigating an embodied agent\nto carry out natural language instructions inside real 3D environments. In this\npaper, we study how to address three critical challenges for this task: the\ncross-modal grounding, the ill-posed feedback, and the generalization problems.\nFirst, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that\nenforces cross-modal grounding both locally and globally via reinforcement\nlearning (RL). Particularly, a matching critic is used to provide an intrinsic\nreward to encourage global matching between instructions and trajectories, and\na reasoning navigator is employed to perform cross-modal grounding in the local\nvisual scene. Evaluation on a VLN benchmark dataset shows that our RCM model\nsignificantly outperforms previous methods by 10% on SPL and achieves the new\nstate-of-the-art performance. To improve the generalizability of the learned\npolicy, we further introduce a Self-Supervised Imitation Learning (SIL) method\nto explore unseen environments by imitating its own past, good decisions. We\ndemonstrate that SIL can approximate a better and more efficient policy, which\ntremendously minimizes the success rate performance gap between seen and unseen\nenvironments (from 30.7% to 11.7%).\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 20:49:58 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 05:43:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Xin", ""], ["Huang", "Qiuyuan", ""], ["Celikyilmaz", "Asli", ""], ["Gao", "Jianfeng", ""], ["Shen", "Dinghan", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""], ["Zhang", "Lei", ""]]}, {"id": "1811.10167", "submitter": "Zhuoran Ji", "authors": "Jianyu Zhao, Zhuoran Ji", "title": "LSICC: A Large Scale Informal Chinese Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based natural language processing model is proven powerful, but\nneed large-scale dataset. Due to the significant gap between the real-world\ntasks and existing Chinese corpus, in this paper, we introduce a large-scale\ncorpus of informal Chinese. This corpus contains around 37 million book reviews\nand 50 thousand netizen's comments to the news. We explore the informal words\nfrequencies of the corpus and show the difference between our corpus and the\nexisting ones. The corpus can be further used to train deep learning based\nnatural language processing tasks such as Chinese word segmentation, sentiment\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:58:09 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhao", "Jianyu", ""], ["Ji", "Zhuoran", ""]]}, {"id": "1811.10169", "submitter": "Jie Li", "authors": "Jie Li, Yahui Shan, Xiaorui Wang, Yan Li", "title": "Improving Gated Recurrent Unit Based Acoustic Modeling with Batch\n  Normalization and Enlarged Context", "comments": "ISCSLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of future contextual information is typically shown to be helpful for\nacoustic modeling. Recently, we proposed a RNN model called minimal gated\nrecurrent unit with input projection (mGRUIP), in which a context module namely\ntemporal convolution, is specifically designed to model the future context.\nThis model, mGRUIP with context module (mGRUIP-Ctx), has been shown to be able\nof utilizing the future context effectively, meanwhile with quite low model\nlatency and computation cost. In this paper, we continue to improve mGRUIP-Ctx\nwith two revisions: applying BN methods and enlarging model context.\nExperimental results on two Mandarin ASR tasks (8400 hours and 60K hours) show\nthat, the revised mGRUIP-Ctx outperform LSTM with a large margin (11% to 38%).\nIt even performs slightly better than a superior BLSTM on the 8400h task, with\n33M less parameters and just 290ms model latency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 04:00:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Jie", ""], ["Shan", "Yahui", ""], ["Wang", "Xiaorui", ""], ["Li", "Yan", ""]]}, {"id": "1811.10188", "submitter": "Zi Lin", "authors": "Zi Lin and Yang Liu", "title": "Implanting Rational Knowledge into Distributed Representation at\n  Morpheme Level", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previously, researchers paid no attention to the creation of unambiguous\nmorpheme embeddings independent from the corpus, while such information plays\nan important role in expressing the exact meanings of words for parataxis\nlanguages like Chinese. In this paper, after constructing the Chinese lexical\nand semantic ontology based on word-formation, we propose a novel approach to\nimplanting the structured rational knowledge into distributed representation at\nmorpheme level, naturally avoiding heavy disambiguation in the corpus. We\ndesign a template to create the instances as pseudo-sentences merely from the\npieces of knowledge of morphemes built in the lexicon. To exploit hierarchical\ninformation and tackle the data sparseness problem, the instance proliferation\ntechnique is applied based on similarity to expand the collection of\npseudo-sentences. The distributed representation for morphemes can then be\ntrained on these pseudo-sentences using word2vec. For evaluation, we validate\nthe paradigmatic and syntagmatic relations of morpheme embeddings, and apply\nthe obtained embeddings to word similarity measurement, achieving significant\nimprovements over the classical models by more than 5 Spearman scores or 8\npercentage points, which shows very promising prospects for adoption of the new\nsource of knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:01:35 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Lin", "Zi", ""], ["Liu", "Yang", ""]]}, {"id": "1811.10211", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Jie Fu, Yue Dong, Xipeng Qiu, Jackie Chi Kit Cheung", "title": "Multi-task Learning over Graph Structures", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two architectures for multi-task learning with neural sequence\nmodels. Our approach allows the relationships between different tasks to be\nlearned dynamically, rather than using an ad-hoc pre-defined structure as in\nprevious work. We adopt the idea from message-passing graph neural networks and\npropose a general \\textbf{graph multi-task learning} framework in which\ndifferent tasks can communicate with each other in an effective and\ninterpretable way. We conduct extensive experiments in text classification and\nsequence labeling to evaluate our approach on multi-task learning and transfer\nlearning. The empirical results show that our models not only outperform\ncompetitive baselines but also learn interpretable and transferable patterns\nacross tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:54:51 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Liu", "Pengfei", ""], ["Fu", "Jie", ""], ["Dong", "Yue", ""], ["Qiu", "Xipeng", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "1811.10229", "submitter": "Tom Williams", "authors": "Tom Williams and Ravenna Thielstrom and Evan Krause and Bradley\n  Oosterveld and Matthias Scheutz", "title": "Augmenting Robot Knowledge Consultants with Distributed Short Term\n  Memory", "comments": "International Conference on Social Robotics (ICSR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot communication in situated environments involves a complex\ninterplay between knowledge representations across a wide variety of\nmodalities. Crucially, linguistic information must be associated with\nrepresentations of objects, locations, people, and goals, which may be\nrepresented in very different ways. In previous work, we developed a Consultant\nFramework that facilitates modality-agnostic access to information distributed\nacross a set of heterogeneously represented knowledge sources. In this work, we\ndraw inspiration from cognitive science to augment these distributed knowledge\nsources with Short Term Memory Buffers to create an STM-augmented algorithm for\nreferring expression generation. We then discuss the potential performance\nbenefits of this approach and insights from cognitive science that may inform\nfuture refinements in the design of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 08:28:21 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Williams", "Tom", ""], ["Thielstrom", "Ravenna", ""], ["Krause", "Evan", ""], ["Oosterveld", "Bradley", ""], ["Scheutz", "Matthias", ""]]}, {"id": "1811.10238", "submitter": "Amit Sangroya", "authors": "Aishwarya Chhabra, Pratik Saini, Amit Sangroya, C. Anantaram", "title": "Learning Latent Beliefs and Performing Epistemic Reasoning for Efficient\n  and Meaningful Dialog Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dialogue management frameworks allow the system designer to directly\ndefine belief rules to implement an efficient dialog policy. Because these\nrules are directly defined, the components are said to be hand-crafted. As\ndialogues become more complex, the number of states, transitions, and policy\ndecisions becomes very large. To facilitate the dialog policy design process,\nwe propose an approach to automatically learn belief rules using a supervised\nmachine learning approach. We validate our ideas in Student-Advisor\nconversation domain, where we extract latent beliefs like student is curious,\nconfused and neutral, etc. Further, we also perform epistemic reasoning that\nhelps to tailor the dialog according to student's emotional state and hence\nimprove the overall effectiveness of the dialog system. Our latent belief\nidentification approach shows an accuracy of 87% and this results in efficient\nand meaningful dialog management.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:12:12 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 09:36:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chhabra", "Aishwarya", ""], ["Saini", "Pratik", ""], ["Sangroya", "Amit", ""], ["Anantaram", "C.", ""]]}, {"id": "1811.10278", "submitter": "Sina Ahmadi", "authors": "Sina Ahmadi", "title": "A Rule-based Kurdish Text Transliteration System", "comments": "To appear in the ACM Transactions on Asian and Low-Resource Language\n  Information Processing (TALLIP), 18:2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a rule-based approach for transliterating two\nmostly used orthographies in Sorani Kurdish. Our work consists of detecting a\ncharacter in a word by removing the possible ambiguities and mapping it into\nthe target orthography. We describe different challenges in Kurdish text mining\nand propose novel ideas concerning the transliteration task for Sorani Kurdish.\nOur transliteration system, named Wergor, achieves 82.79% overall precision and\nmore than 99% in detecting the double-usage characters. We also present a\nmanually transliterated corpus for Kurdish.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:37:05 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ahmadi", "Sina", ""]]}, {"id": "1811.10369", "submitter": "Joeran Beel", "authors": "Dominika Tkaczyk, Rohit Gupta, Riccardo Cinti, Joeran Beel", "title": "ParsRec: A Novel Meta-Learning Approach to Recommending Bibliographic\n  Reference Parsers", "comments": "Accepted at the 26th Irish Conference on Artificial Intelligence and\n  Cognitive Science. This paper is an extended version of a poster published at\n  the 12th ACM Conference on Recommender Systems, Proceedings of the 26th Irish\n  Conference on Artificial Intelligence and Cognitive Science (AICS). Dublin,\n  Ireland 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic reference parsers extract machine-readable metadata such as\nauthor names, title, journal, and year from bibliographic reference strings. To\nextract the metadata, the parsers apply heuristics or machine learning.\nHowever, no reference parser, and no algorithm, consistently gives the best\nresults in every scenario. For instance, one tool may be best in extracting\ntitles in ACM citation style, but only third best when APA is used. Another\ntool may be best in extracting English author names, while another one is best\nfor noisy data (i.e. inconsistent citation styles). In this paper, which is an\nextended version of our recent RecSys poster, we address the problem of\nreference parsing from a recommender-systems and meta-learning perspective. We\npropose ParsRec, a meta-learning based recommender-system that recommends the\npotentially most effective parser for a given reference string. ParsRec\nrecommends one out of 10 open-source parsers: Anystyle-Parser, Biblio, CERMINE,\nCitation, Citation-Parser, GROBID, ParsCit, PDFSSA4MET, Reference Tagger, and\nScience Parse. We evaluate ParsRec on 105k references from chemistry. We\npropose two approaches to meta-learning recommendations. The first approach\nlearns the best parser for an entire reference string. The second approach\nlearns the best parser for each metadata type in a reference string. The second\napproach achieved a 2.6% increase in F1 (0.909 vs. 0.886) over the best single\nparser (GROBID), reducing the false positive rate by 20.2% (0.075 vs. 0.094),\nand the false negative rate by 18.9% (0.107 vs. 0.132).\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:56:57 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Gupta", "Rohit", ""], ["Cinti", "Riccardo", ""], ["Beel", "Joeran", ""]]}, {"id": "1811.10403", "submitter": "Pablo Gordillo", "authors": "Elvira Albert and Pablo Gordillo and Albert Rubio and Ilya Sergey", "title": "Running on Fumes--Preventing Out-of-Gas Vulnerabilities in Ethereum\n  Smart Contracts using Static Resource Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gas is a measurement unit of the computational effort that it will take to\nexecute every single operation that takes part in the Ethereum blockchain\nplatform. Each instruction executed by the Ethereum Virtual Machine (EVM) has\nan associated gas consumption specified by Ethereum. If a transaction exceeds\nthe amount of gas allotted by the user (known as gas limit), an out-of-gas\nexception is raised. There is a wide family of contract vulnerabilities due to\nout-of-gas behaviours. We report on the design and implementation of GASTAP, a\nGas-Aware Smart contracT Analysis Platform, which takes as input a smart\ncontract (either in EVM, disassembled EVM, or in Solidity source code) and\nautomatically infers sound gas upper bounds for all its public functions. Our\nbounds ensure that if the gas limit paid by the user is higher than our\ninferred gas bounds, the contract is free of out-of-gas vulnerabilities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 13:19:07 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 09:42:09 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Albert", "Elvira", ""], ["Gordillo", "Pablo", ""], ["Rubio", "Albert", ""], ["Sergey", "Ilya", ""]]}, {"id": "1811.10418", "submitter": "S{\\l}awomir Dadas", "authors": "S{\\l}awomir Dadas", "title": "Combining neural and knowledge-based approaches to Named Entity\n  Recognition in Polish", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-20912-4_4", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is one of the tasks in natural language\nprocessing that can greatly benefit from the use of external knowledge sources.\nWe propose a named entity recognition framework composed of knowledge-based\nfeature extractors and a deep learning model including contextual word\nembeddings, long short-term memory (LSTM) layers and conditional random fields\n(CRF) inference layer. We use an entity linking module to integrate our system\nwith Wikipedia. The combination of effective neural architecture and external\nresources allows us to obtain state-of-the-art results on recognition of Polish\nproper names. We evaluate our model on data from PolEval 2018 NER challenge on\nwhich it outperforms other methods, reducing the error rate by 22.4% compared\nto the winning solution. Our work shows that combining neural NER model and\nentity linking model with a knowledge base is more effective in recognizing\nnamed entities than using NER model alone.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 14:52:06 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Dadas", "S\u0142awomir", ""]]}, {"id": "1811.10422", "submitter": "Nikola Milo\\v{s}evi\\'c MSc", "authors": "Nikola Milosevic, Goran Nenadic", "title": "Creating a contemporary corpus of similes in Serbian by using natural\n  language processing", "comments": "15 pages, submitted to journal Slovo, however, later withdrawn to\n  correct. Additional work was not done on it, so it is still waiting to be\n  extended. Output of the system can be seen here:\n  http://ezbirka.starisloveni.com/. arXiv admin note: text overlap with\n  arXiv:1605.06319", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simile is a figure of speech that compares two things through the use of\nconnection words, but where comparison is not intended to be taken literally.\nThey are often used in everyday communication, but they are also a part of\nlinguistic cultural heritage. In this paper we present a methodology for\nsemi-automated collection of similes from the World Wide Web using text mining\nand machine learning techniques. We expanded an existing corpus by collecting\n442 similes from the internet and adding them to the existing corpus collected\nby Vuk Stefanovic Karadzic that contained 333 similes. We, also, introduce\ncrowdsourcing to the collection of figures of speech, which helped us to build\ncorpus containing 787 unique similes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:55:40 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Milosevic", "Nikola", ""], ["Nenadic", "Goran", ""]]}, {"id": "1811.10475", "submitter": "Lei Yu", "authors": "Lei Yu, Cyprien de Masson d'Autume, Chris Dyer, Phil Blunsom, Lingpeng\n  Kong, Wang Ling", "title": "Sentence Encoding with Tree-constrained Relation Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The meaning of a sentence is a function of the relations that hold between\nits words. We instantiate this relational view of semantics in a series of\nneural models based on variants of relation networks (RNs) which represent a\nset of objects (for us, words forming a sentence) in terms of representations\nof pairs of objects. We propose two extensions to the basic RN model for\nnatural language. First, building on the intuition that not all word pairs are\nequally informative about the meaning of a sentence, we use constraints based\non both supervised and unsupervised dependency syntax to control which\nrelations influence the representation. Second, since higher-order relations\nare poorly captured by a sum of pairwise relations, we use a recurrent\nextension of RNs to propagate information so as to form representations of\nhigher order relations. Experiments on sentence classification, sentence pair\nclassification, and machine translation reveal that, while basic RNs are only\nmodestly effective for sentence representation, recurrent RNs with latent\nsyntax are a reliably powerful representational device.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:07:36 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Yu", "Lei", ""], ["d'Autume", "Cyprien de Masson", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""], ["Kong", "Lingpeng", ""], ["Ling", "Wang", ""]]}, {"id": "1811.10550", "submitter": "Claudia Schulz", "authors": "Claudia Schulz, Christian M. Meyer, Michael Sailer, Jan Kiesewetter,\n  Elisabeth Bauer, Frank Fischer, Martin R. Fischer, Iryna Gurevych", "title": "Challenges in the Automatic Analysis of Students' Diagnostic Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic reasoning is a key component of many professions. To improve\nstudents' diagnostic reasoning skills, educational psychologists analyse and\ngive feedback on epistemic activities used by these students while diagnosing,\nin particular, hypothesis generation, evidence generation, evidence evaluation,\nand drawing conclusions. However, this manual analysis is highly\ntime-consuming. We aim to enable the large-scale adoption of diagnostic\nreasoning analysis and feedback by automating the epistemic activity\nidentification. We create the first corpus for this task, comprising diagnostic\nreasoning self-explanations of students from two domains annotated with\nepistemic activities. Based on insights from the corpus creation and the task's\ncharacteristics, we discuss three challenges for the automatic identification\nof epistemic activities using AI methods: the correct identification of\nepistemic activity spans, the reliable distinction of similar epistemic\nactivities, and the detection of overlapping epistemic activities. We propose a\nseparate performance metric for each challenge and thus provide an evaluation\nframework for future research. Indeed, our evaluation of various\nstate-of-the-art recurrent neural network architectures reveals that current\ntechniques fail to address some of these challenges.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:53:17 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Schulz", "Claudia", ""], ["Meyer", "Christian M.", ""], ["Sailer", "Michael", ""], ["Kiesewetter", "Jan", ""], ["Bauer", "Elisabeth", ""], ["Fischer", "Frank", ""], ["Fischer", "Martin R.", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1811.10561", "submitter": "Jerome Abdelnour", "authors": "Jerome Abdelnour, Giampiero Salvi, Jean Rouat", "title": "CLEAR: A Dataset for Compositional Language and Elementary Acoustic\n  Reasoning", "comments": "NeurIPS 2018 Visually Grounded Interaction and Language (ViGIL)\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of acoustic question answering (AQA) in the area of\nacoustic reasoning. In this task an agent learns to answer questions on the\nbasis of acoustic context. In order to promote research in this area, we\npropose a data generation paradigm adapted from CLEVR (Johnson et al. 2017). We\ngenerate acoustic scenes by leveraging a bank elementary sounds. We also\nprovide a number of functional programs that can be used to compose questions\nand answers that exploit the relationships between the attributes of the\nelementary sounds in each scene. We provide AQA datasets of various sizes as\nwell as the data generation code. As a preliminary experiment to validate our\ndata, we report the accuracy of current state of the art visual question\nanswering models when they are applied to the AQA task without modifications.\nAlthough there is a plethora of question answering tasks based on text, image\nor video data, to our knowledge, we are the first to propose answering\nquestions directly on audio streams. We hope this contribution will facilitate\nthe development of research in the area.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:06:36 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Abdelnour", "Jerome", ""], ["Salvi", "Giampiero", ""], ["Rouat", "Jean", ""]]}, {"id": "1811.10652", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "Show, Control and Tell: A Framework for Generating Controllable and\n  Grounded Captions", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current captioning approaches can describe images using black-box\narchitectures whose behavior is hardly controllable and explainable from the\nexterior. As an image can be described in infinite ways depending on the goal\nand the context at hand, a higher degree of controllability is needed to apply\ncaptioning algorithms in complex scenarios. In this paper, we introduce a novel\nframework for image captioning which can generate diverse descriptions by\nallowing both grounding and controllability. Given a control signal in the form\nof a sequence or set of image regions, we generate the corresponding caption\nthrough a recurrent architecture which predicts textual chunks explicitly\ngrounded on regions, following the constraints of the given control.\nExperiments are conducted on Flickr30k Entities and on COCO Entities, an\nextended version of COCO in which we add grounding annotations collected in a\nsemi-automatic manner. Results demonstrate that our method achieves state of\nthe art performances on controllable image captioning, in terms of caption\nquality and diversity. Code and annotations are publicly available at:\nhttps://github.com/aimagelab/show-control-and-tell.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:23:33 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 14:18:54 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 07:58:20 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1811.10667", "submitter": "Xuelu Chen", "authors": "Xuelu Chen, Muhao Chen, Weijia Shi, Yizhou Sun, Carlo Zaniolo", "title": "Embedding Uncertain Knowledge Graphs", "comments": null, "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence.\n  Vol. 33. 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding models for deterministic Knowledge Graphs (KG) have been\nextensively studied, with the purpose of capturing latent semantic relations\nbetween entities and incorporating the structured knowledge into machine\nlearning. However, there are many KGs that model uncertain knowledge, which\ntypically model the inherent uncertainty of relations facts with a confidence\nscore, and embedding such uncertain knowledge represents an unresolved\nchallenge. The capturing of uncertain knowledge will benefit many\nknowledge-driven applications such as question answering and semantic search by\nproviding more natural characterization of the knowledge. In this paper, we\npropose a novel uncertain KG embedding model UKGE, which aims to preserve both\nstructural and uncertainty information of relation facts in the embedding\nspace. Unlike previous models that characterize relation facts with binary\nclassification techniques, UKGE learns embeddings according to the confidence\nscores of uncertain relation facts. To further enhance the precision of UKGE,\nwe also introduce probabilistic soft logic to infer confidence scores for\nunseen relation facts during training. We propose and evaluate two variants of\nUKGE based on different learning objectives. Experiments are conducted on three\nreal-world uncertain KGs via three tasks, i.e. confidence prediction, relation\nfact ranking, and relation fact classification. UKGE shows effectiveness in\ncapturing uncertain knowledge by achieving promising results on these tasks,\nand consistently outperforms baselines on these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:57:14 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 00:59:42 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chen", "Xuelu", ""], ["Chen", "Muhao", ""], ["Shi", "Weijia", ""], ["Sun", "Yizhou", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1811.10686", "submitter": "Mengting Wan", "authors": "Mengting Wan, Xin Chen", "title": "Beyond \"How may I help you?\": Assisting Customer Service Agents with\n  Proactive Responses", "comments": "7 pages, DEEP-DIAL 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of providing recommended responses to customer service\nagents in live-chat dialogue systems. Smart-reply systems have been widely\napplied in real-world applications (e.g. Gmail, LinkedIn Messaging), where most\nof them can successfully recommend reactive responses. However, we observe a\nmajor limitation of current methods is that they generally have difficulties in\nsuggesting proactive investigation act (e.g. \"Do you perhaps have another\naccount with us?\") due to the lack of long-term context information, which\nindeed act as critical steps for customer service agents to collect information\nand resolve customers' issues. Thus in this work, we propose an end-to-end\nmethod with special focus on suggesting proactive investigative questions to\ncustomer agents in Airbnb's customer service live-chat system. Effectiveness of\nour proposed method can be validated through qualitative and quantitative\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 20:56:38 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Wan", "Mengting", ""], ["Chen", "Xin", ""]]}, {"id": "1811.10761", "submitter": "Panayiotis Georgiou", "authors": "Tae Jin Park, Kyu Han, Ian Lane and Panayiotis Georgiou", "title": "Speaker Diarization With Lexical Information", "comments": "This version removed by arXiv administrators because the author did\n  not have the right to agree to our license at the time of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel approach to leverage lexical information for\nspeaker diarization. We introduce a speaker diarization system that can\ndirectly integrate lexical as well as acoustic information into a speaker\nclustering process. Thus, we propose an adjacency matrix integration technique\nto integrate word level speaker turn probabilities with speaker embeddings in a\ncomprehensive way. Our proposed method works without any reference transcript.\nWords, and word boundary information are provided by an ASR system. We show\nthat our proposed method improves a baseline speaker diarization system solely\nbased on speaker embeddings, achieving a meaningful improvement on the CALLHOME\nAmerican English Speech dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 00:58:39 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 22:22:06 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Park", "Tae Jin", ""], ["Han", "Kyu", ""], ["Lane", "Ian", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "1811.10773", "submitter": "Katharina Kann", "authors": "Katharina Kann, Alex Warstadt, Adina Williams and Samuel R. Bowman", "title": "Verb Argument Structure Alternations in Word and Sentence Embeddings", "comments": "Accepted to SCiL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verbs occur in different syntactic environments, or frames. We investigate\nwhether artificial neural networks encode grammatical distinctions necessary\nfor inferring the idiosyncratic frame-selectional properties of verbs. We\nintroduce five datasets, collectively called FAVA, containing in aggregate\nnearly 10k sentences labeled for grammatical acceptability, illustrating\ndifferent verbal argument structure alternations. We then test whether models\ncan distinguish acceptable English verb-frame combinations from unacceptable\nones using a sentence embedding alone. For converging evidence, we further\nconstruct LaVA, a corresponding word-level dataset, and investigate whether the\nsame syntactic features can be extracted from word embeddings. Our models\nperform reliable classifications for some verbal alternations but not others,\nsuggesting that while these representations do encode fine-grained lexical\ninformation, it is incomplete or can be hard to extract. Further, differences\nbetween the word- and sentence-level models show that some information present\nin word embeddings is not passed on to the down-stream sentence embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 02:07:36 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kann", "Katharina", ""], ["Warstadt", "Alex", ""], ["Williams", "Adina", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1811.10776", "submitter": "Yixin Cao", "authors": "Yixin Cao and Lei Hou and Juanzi Li and Zhiyuan Liu and Chengjiang Li\n  and Xu Chen and Tiansi Dong", "title": "Joint Representation Learning of Cross-lingual Words and Entities via\n  Attentive Distant Supervision", "comments": "11 pages, EMNLP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint representation learning of words and entities benefits many NLP tasks,\nbut has not been well explored in cross-lingual settings. In this paper, we\npropose a novel method for joint representation learning of cross-lingual words\nand entities. It captures mutually complementary knowledge, and enables\ncross-lingual inferences among knowledge bases and texts. Our method does not\nrequire parallel corpora, and automatically generates comparable data via\ndistant supervision using multi-lingual knowledge bases. We utilize two types\nof regularizers to align cross-lingual words and entities, and design knowledge\nattention and cross-lingual attention to further reduce noises. We conducted a\nseries of experiments on three tasks: word translation, entity relatedness, and\ncross-lingual entity linking. The results, both qualitatively and\nquantitatively, demonstrate the significance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 02:24:37 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Cao", "Yixin", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""], ["Li", "Chengjiang", ""], ["Chen", "Xu", ""], ["Dong", "Tiansi", ""]]}, {"id": "1811.10830", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi", "title": "From Recognition to Cognition: Visual Commonsense Reasoning", "comments": "CVPR 2019 oral. Project page at https://visualcommonsense.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual understanding goes well beyond object recognition. With one glance at\nan image, we can effortlessly imagine the world beyond the pixels: for\ninstance, we can infer people's actions, goals, and mental states. While this\ntask is easy for humans, it is tremendously difficult for today's vision\nsystems, requiring higher-order cognition and commonsense reasoning about the\nworld. We formalize this task as Visual Commonsense Reasoning. Given a\nchallenging question about an image, a machine must answer correctly and then\nprovide a rationale justifying its answer.\n  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA\nproblems derived from 110k movie scenes. The key recipe for generating\nnon-trivial and high-quality problems at scale is Adversarial Matching, a new\napproach to transform rich annotations into multiple choice questions with\nminimal bias. Experimental results show that while humans find VCR easy (over\n90% accuracy), state-of-the-art vision models struggle (~45%).\n  To move towards cognition-level understanding, we present a new reasoning\nengine, Recognition to Cognition Networks (R2C), that models the necessary\nlayered inferences for grounding, contextualization, and reasoning. R2C helps\nnarrow the gap between humans and machines (~65%); still, the challenge is far\nfrom solved, and we provide analysis that suggests avenues for future work.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:22:26 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 17:50:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Zellers", "Rowan", ""], ["Bisk", "Yonatan", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "1811.10971", "submitter": "James Thorne", "authors": "James Thorne, Andreas Vlachos, Oana Cocarascu, Christos\n  Christodoulopoulos, Arpit Mittal", "title": "The Fact Extraction and VERification (FEVER) Shared Task", "comments": "Revised from published version in the proceedings of the FEVER\n  workshop at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of the first Fact Extraction and VERification (FEVER)\nShared Task. The task challenged participants to classify whether human-written\nfactoid claims could be Supported or Refuted using evidence retrieved from\nWikipedia. We received entries from 23 competing teams, 19 of which scored\nhigher than the previously published baseline. The best performing system\nachieved a FEVER score of 64.21%. In this paper, we present the results of the\nshared task and a summary of the systems, highlighting commonalities and\ninnovations among participating systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:32:26 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 09:26:31 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Thorne", "James", ""], ["Vlachos", "Andreas", ""], ["Cocarascu", "Oana", ""], ["Christodoulopoulos", "Christos", ""], ["Mittal", "Arpit", ""]]}, {"id": "1811.10986", "submitter": "Somayeh Asadifar", "authors": "Somayeh Asadifar, Mohsen Kahani and Saeedeh Shekarpour", "title": "HCqa: Hybrid and Complex Question Answering on Textual Corpus and\n  Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) systems provide easy access to the vast amount of\nknowledge without having to know the underlying complex structure of the\nknowledge. The research community has provided ad hoc solutions to the key QA\ntasks, including named entity recognition and disambiguation, relation\nextraction and query building. Furthermore, some have integrated and composed\nthese components to implement many tasks automatically and efficiently.\nHowever, in general, the existing solutions are limited to simple and short\nquestions and still do not address complex questions composed of several\nsub-questions. Exploiting the answer to complex questions is further challenged\nif it requires integrating knowledge from unstructured data sources, i.e.,\ntextual corpus, as well as structured data sources, i.e., knowledge graphs. In\nthis paper, an approach (HCqa) is introduced for dealing with complex questions\nrequiring federating knowledge from a hybrid of heterogeneous data sources\n(structured and unstructured). We contribute in developing (i) a decomposition\nmechanism which extracts sub-questions from potentially long and complex input\nquestions, (ii) a novel comprehensive schema, first of its kind, for extracting\nand annotating relations, and (iii) an approach for executing and aggregating\nthe answers of sub-questions. The evaluation of HCqa showed a superior accuracy\nin the fundamental tasks, such as relation extraction, as well as the\nfederation task.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 07:03:53 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 08:19:45 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 09:42:23 GMT"}, {"version": "v4", "created": "Thu, 31 Jan 2019 06:39:48 GMT"}, {"version": "v5", "created": "Sun, 9 Jun 2019 04:56:51 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Asadifar", "Somayeh", ""], ["Kahani", "Mohsen", ""], ["Shekarpour", "Saeedeh", ""]]}, {"id": "1811.10990", "submitter": "Chenyang Huang", "authors": "Chenyang Huang and Osmar R. Za\\\"iane", "title": "Generating Responses Expressing Emotion in an Open-domain Dialogue\n  System", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-17705-8", "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based Open-ended conversational agents automatically generate\nresponses based on predictive models learned from a large number of pairs of\nutterances. The generated responses are typically acceptable as a sentence but\nare often dull, generic, and certainly devoid of any emotion. In this paper, we\npresent neural models that learn to express a given emotion in the generated\nresponse. We propose four models and evaluate them against 3 baselines. An\nencoder-decoder framework-based model with multiple attention layers provides\nthe best overall performance in terms of expressing the required emotion. While\nit does not outperform other models on all emotions, it presents promising\nresults in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 22:59:25 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Huang", "Chenyang", ""], ["Za\u00efane", "Osmar R.", ""]]}, {"id": "1811.10996", "submitter": "Ning Miao", "authors": "Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li", "title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling", "comments": "AAAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications of natural language generation, there are often\nconstraints on the target sentences in addition to fluency and naturalness\nrequirements. Existing language generation techniques are usually based on\nrecurrent neural networks (RNNs). However, it is non-trivial to impose\nconstraints on RNNs while maintaining generation quality, since RNNs generate\nsentences sequentially (or with beam search) from the first word to the last.\nIn this paper, we propose CGMH, a novel approach using Metropolis-Hastings\nsampling for constrained sentence generation. CGMH allows complicated\nconstraints such as the occurrence of multiple keywords in the target\nsentences, which cannot be handled in traditional RNN-based approaches.\nMoreover, CGMH works in the inference stage, and does not require parallel\ncorpora for training. We evaluate our method on a variety of tasks, including\nkeywords-to-sentence generation, unsupervised sentence paraphrasing, and\nunsupervised sentence error correction. CGMH achieves high performance compared\nwith previous supervised methods for sentence generation. Our code is released\nat https://github.com/NingMiao/CGMH\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:46:57 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Miao", "Ning", ""], ["Zhou", "Hao", ""], ["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Li", "Lei", ""]]}, {"id": "1811.10999", "submitter": "Zheng Li", "authors": "Zheng Li, Ying Wei, Yu Zhang, Xiang Zhang, Xin Li, Qiang Yang", "title": "Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-level sentiment classification (ASC) aims at identifying sentiment\npolarities towards aspects in a sentence, where the aspect can behave as a\ngeneral Aspect Category (AC) or a specific Aspect Term (AT). However, due to\nthe especially expensive and labor-intensive labeling, existing public corpora\nin AT-level are all relatively small. Meanwhile, most of the previous methods\nrely on complicated structures with given scarce data, which largely limits the\nefficacy of the neural models. In this paper, we exploit a new direction named\ncoarse-to-fine task transfer, which aims to leverage knowledge learned from a\nrich-resource source domain of the coarse-grained AC task, which is more easily\naccessible, to improve the learning in a low-resource target domain of the\nfine-grained AT task. To resolve both the aspect granularity inconsistency and\nfeature mismatch between domains, we propose a Multi-Granularity Alignment\nNetwork (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary\ntask can help the AC task modeling at the same fine-grained level with the AT\ntask. To alleviate the feature false alignment, a contrastive feature alignment\nmethod is adopted to align aspect-specific feature representations\nsemantically. In addition, a large-scale multi-domain dataset for the AC task\nis provided. Empirically, extensive experiments demonstrate the effectiveness\nof the MGAN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 07:09:30 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Li", "Zheng", ""], ["Wei", "Ying", ""], ["Zhang", "Yu", ""], ["Zhang", "Xiang", ""], ["Li", "Xin", ""], ["Yang", "Qiang", ""]]}, {"id": "1811.11001", "submitter": "Tianlin Liu", "authors": "Tianlin Liu, Lyle Ungar, Jo\\~ao Sedoc", "title": "Unsupervised Post-processing of Word Vectors via Conceptor Negation", "comments": "Accepted by AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word vectors are at the core of many natural language processing tasks.\nRecently, there has been interest in post-processing word vectors to enrich\ntheir semantic information. In this paper, we introduce a novel word vector\npost-processing technique based on matrix conceptors (Jaeger2014), a family of\nregularized identity maps. More concretely, we propose to use conceptors to\nsuppress those latent features of word vectors having high variances. The\nproposed method is purely unsupervised: it does not rely on any corpus or\nexternal linguistic database. We evaluate the post-processed word vectors on a\nbattery of intrinsic lexical evaluation tasks, showing that the proposed method\nconsistently outperforms existing state-of-the-art alternatives. We also show\nthat post-processed word vectors can be used for the downstream natural\nlanguage processing task of dialogue state tracking, yielding improved results\nin different dialogue domains.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 20:12:40 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 10:27:57 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Tianlin", ""], ["Ungar", "Lyle", ""], ["Sedoc", "Jo\u00e3o", ""]]}, {"id": "1811.11002", "submitter": "Tianlin Liu", "authors": "Tianlin Liu, Jo\\~ao Sedoc, Lyle Ungar", "title": "Correcting the Common Discourse Bias in Linear Representation of\n  Sentences using Conceptors", "comments": "Accepted by the BioCreative/OHNLP workshop of ACM-BCB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of words, better known as word embeddings, have\nbecome important building blocks for natural language processing tasks.\nNumerous studies are devoted to transferring the success of unsupervised word\nembeddings to sentence embeddings. In this paper, we introduce a simple\nrepresentation of sentences in which a sentence embedding is represented as a\nweighted average of word vectors followed by a soft projection. We demonstrate\nthe effectiveness of this proposed method on the clinical semantic textual\nsimilarity task of the BioCreative/OHNLP Challenge 2018.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 20:20:20 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liu", "Tianlin", ""], ["Sedoc", "Jo\u00e3o", ""], ["Ungar", "Lyle", ""]]}, {"id": "1811.11005", "submitter": "Maria Pikoula", "authors": "Spiros Denaxas, Pontus Stenetorp, Sebastian Riedel, Maria Pikoula,\n  Richard Dobson, Harry Hemingway", "title": "Application of Clinical Concept Embeddings for Heart Failure Prediction\n  in UK EHR data", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/37", "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) are increasingly being used for constructing\ndisease risk prediction models. Feature engineering in EHR data however is\nchallenging due to their highly dimensional and heterogeneous nature.\nLow-dimensional representations of EHR data can potentially mitigate these\nchallenges. In this paper, we use global vectors (GloVe) to learn word\nembeddings for diagnoses and procedures recorded using 13 million ontology\nterms across 2.7 million hospitalisations in national UK EHR. We demonstrate\nthe utility of these embeddings by evaluating their performance in identifying\npatients which are at higher risk of being hospitalised for congestive heart\nfailure. Our findings indicate that embeddings can enable the creation of\nrobust EHR-derived disease risk prediction models and address some the\nlimitations associated with manual clinical feature engineering.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:04:12 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 15:01:56 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Denaxas", "Spiros", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""], ["Pikoula", "Maria", ""], ["Dobson", "Richard", ""], ["Hemingway", "Harry", ""]]}, {"id": "1811.11008", "submitter": "Srikumar Krishnamoorthy", "authors": "Srikumar Krishnamoorthy", "title": "Sentiment Analysis of Financial News Articles using Performance\n  Indicators", "comments": "Knowledge and Information Systems Nov 2017", "journal-ref": null, "doi": "10.1007/s10115-017-1134-1", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining financial text documents and understanding the sentiments of\nindividual investors, institutions and markets is an important and challenging\nproblem in the literature. Current approaches to mine sentiments from financial\ntexts largely rely on domain specific dictionaries. However, dictionary based\nmethods often fail to accurately predict the polarity of financial texts. This\npaper aims to improve the state-of-the-art and introduces a novel sentiment\nanalysis approach that employs the concept of financial and non-financial\nperformance indicators. It presents an association rule mining based\nhierarchical sentiment classifier model to predict the polarity of financial\ntexts as positive, neutral or negative. The performance of the proposed model\nis evaluated on a benchmark financial dataset. The model is also compared\nagainst other state-of-the-art dictionary and machine learning based approaches\nand the results are found to be quite promising. The novel use of performance\nindicators for financial sentiment analysis offers interesting and useful\ninsights.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 01:36:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Krishnamoorthy", "Srikumar", ""]]}, {"id": "1811.11017", "submitter": "Mohan Zhang", "authors": "Mohan Zhang, Zhichao Luo, Hai Lu", "title": "Latent Dirichlet Allocation with Residual Convolutional Neural Network\n  Applied in Evaluating Credibility of Chinese Listed Companies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project demonstrated a methodology to estimating cooperate credibility\nwith a Natural Language Processing approach. As cooperate transparency impacts\nboth the credibility and possible future earnings of the firm, it is an\nimportant factor to be considered by banks and investors on risk assessments of\nlisted firms. This approach of estimating cooperate credibility can bypass\nhuman bias and inconsistency in the risk assessment, the use of large\nquantitative data and neural network models provides more accurate estimation\nin a more efficient manner compare to manual assessment. At the beginning, the\nmodel will employs Latent Dirichlet Allocation and THU Open Chinese Lexicon\nfrom Tsinghua University to classify topics in articles which are potentially\nrelated to corporate credibility. Then with the keywords related to each\ntopics, we trained a residual convolutional neural network with data labeled\naccording to surveys of fund manager and accountant's opinion on corporate\ncredibility. After the training, we run the model with preprocessed news\nreports regarding to all of the 3065 listed companies, the model is supposed to\ngive back companies ranking based on the level of their transparency.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:50:41 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Mohan", ""], ["Luo", "Zhichao", ""], ["Lu", "Hai", ""]]}, {"id": "1811.11041", "submitter": "EPTCS", "authors": "Tai-Danae Bradley (Graduate Center, CUNY), Martha Lewis (ILLC,\n  University of Amsterdam), Jade Master (Dept. Mathematics, UC Riverside), Brad\n  Theilman (Gentner Lab, UC San Diego)", "title": "Translating and Evolving: Towards a Model of Language Change in DisCoCat", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 50-61", "doi": "10.4204/EPTCS.283.4", "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional (DisCoCat) model of meaning\ndeveloped by Coecke et al. (2010) has been successful in modeling various\naspects of meaning. However, it fails to model the fact that language can\nchange. We give an approach to DisCoCat that allows us to represent language\nmodels and translations between them, enabling us to describe translations from\none language to another, or changes within the same language. We unify the\nproduct space representation given in (Coecke et al., 2010) and the functorial\ndescription in (Kartsaklis et al., 2013), in a way that allows us to view a\nlanguage as a catalogue of meanings. We formalize the notion of a lexicon in\nDisCoCat, and define a dictionary of meanings between two lexicons. All this is\ndone within the framework of monoidal categories. We give examples of how to\napply our methods, and give a concrete suggestion for compositional translation\nin corpora.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:11:18 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bradley", "Tai-Danae", "", "Graduate Center, CUNY"], ["Lewis", "Martha", "", "ILLC,\n  University of Amsterdam"], ["Master", "Jade", "", "Dept. Mathematics, UC Riverside"], ["Theilman", "Brad", "", "Gentner Lab, UC San Diego"]]}, {"id": "1811.11078", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Yi-Chiao Wu, Hsin-Te Hwang, Patrick Lumban Tobing,\n  Tomoki Hayashi, Kazuhiro Kobayashi, Tomoki Toda, Yu Tsao, Hsin-Min Wang", "title": "Refined WaveNet Vocoder for Variational Autoencoder Based Voice\n  Conversion", "comments": "5 pages, 7 figures, 1 table. Accepted to EUSIPCO 2019", "journal-ref": null, "doi": "10.23919/EUSIPCO.2019.8902651", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a refinement framework of WaveNet vocoders for\nvariational autoencoder (VAE) based voice conversion (VC), which reduces the\nquality distortion caused by the mismatch between the training data and testing\ndata. Conventional WaveNet vocoders are trained with natural acoustic features\nbut conditioned on the converted features in the conversion stage for VC, and\nsuch a mismatch often causes significant quality and similarity degradation. In\nthis work, we take advantage of the particular structure of VAEs to refine\nWaveNet vocoders with the self-reconstructed features generated by VAE, which\nare of similar characteristics with the converted features while having the\nsame temporal structure with the target natural features. We analyze these\nfeatures and show that the self-reconstructed features are similar to the\nconverted features. Objective and subjective experimental results demonstrate\nthe effectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:26:17 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 08:30:07 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Wu", "Yi-Chiao", ""], ["Hwang", "Hsin-Te", ""], ["Tobing", "Patrick Lumban", ""], ["Hayashi", "Tomoki", ""], ["Kobayashi", "Kazuhiro", ""], ["Toda", "Tomoki", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1811.11101", "submitter": "Juliette Millet", "authors": "Juliette Millet and Neil Zeghidour", "title": "Learning to detect dysarthria from raw speech", "comments": "5 pages, 3 figures, submitted to ICASSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech classifiers of paralinguistic traits traditionally learn from diverse\nhand-crafted low-level features, by selecting the relevant information for the\ntask at hand. We explore an alternative to this selection, by learning jointly\nthe classifier, and the feature extraction. Recent work on speech recognition\nhas shown improved performance over speech features by learning from the\nwaveform. We extend this approach to paralinguistic classification and propose\na neural network that can learn a filterbank, a normalization factor and a\ncompression power from the raw speech, jointly with the rest of the\narchitecture. We apply this model to dysarthria detection from sentence-level\naudio recordings. Starting from a strong attention-based baseline on which\nmel-filterbanks outperform standard low-level descriptors, we show that\nlearning the filters or the normalization and compression improves over fixed\nfeatures by 10% absolute accuracy. We also observe a gain over OpenSmile\nfeatures by learning jointly the feature extraction, the normalization, and the\ncompression factor with the architecture. This constitutes a first attempt at\nlearning jointly all these operations from raw audio for a speech\nclassification task.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:51:39 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 14:49:50 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Millet", "Juliette", ""], ["Zeghidour", "Neil", ""]]}, {"id": "1811.11136", "submitter": "TonTon Huang", "authors": "TonTon Hsien-De Huang, Po-Wei Hong, Ying-Tse Lee, Yi-Lun Wang,\n  Chi-Leong Lok, and Hung-Yu Kao", "title": "SOC: hunting the underground inside story of the ethereum Social-network\n  Opinion and Comment", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cryptocurrency is attracting more and more attention because of the\nblockchain technology. Ethereum is gaining a significant popularity in\nblockchain community, mainly due to the fact that it is designed in a way that\nenables developers to write smart contracts and decentralized applications\n(Dapps). There are many kinds of cryptocurrency information on the social\nnetwork. The risks and fraud problems behind it have pushed many countries\nincluding the United States, South Korea, and China to make warnings and set up\ncorresponding regulations. However, the security of Ethereum smart contracts\nhas not gained much attention. Through the Deep Learning approach, we propose a\nmethod of sentiment analysis for Ethereum's community comments. In this\nresearch, we first collected the users' cryptocurrency comments from the social\nnetwork and then fed to our LSTM + CNN model for training. Then we made\nprediction through sentiment analysis. With our research result, we have\ndemonstrated that both the precision and the recall of sentiment analysis can\nachieve 0.80+. More importantly, we deploy our sentiment analysis1 on\nRatingToken and Coin Master (mobile application of Cheetah Mobile Blockchain\nSecurity Center23). We can effectively provide detail information to resolve\nthe risks of being fake and fraud problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 17:54:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Huang", "TonTon Hsien-De", ""], ["Hong", "Po-Wei", ""], ["Lee", "Ying-Tse", ""], ["Wang", "Yi-Lun", ""], ["Lok", "Chi-Leong", ""], ["Kao", "Hung-Yu", ""]]}, {"id": "1811.11161", "submitter": "Lambert Mathias", "authors": "Amr Sharaf, Arpit Gupta, Hancheng Ge, Chetan Naik, Lambert Mathias", "title": "Cross-Lingual Approaches to Reference Resolution in Dialogue Systems", "comments": "Accepted at NIPS 2018 Conversational AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the slot-filling paradigm, where a user can refer back to slots in the\ncontext during the conversation, the goal of the contextual understanding\nsystem is to resolve the referring expressions to the appropriate slots in the\ncontext. In this paper, we build on the context carryover\nsystem~\\citep{Naik2018ContextualSC}, which provides a scalable multi-domain\nframework for resolving references. However, scaling this approach across\nlanguages is not a trivial task, due to the large demand on acquisition of\nannotated data in the target language. Our main focus is on cross-lingual\nmethods for reference resolution as a way to alleviate the need for annotated\ndata in the target language. In the cross-lingual setup, we assume there is\naccess to annotated resources as well as a well trained model in the source\nlanguage and little to no annotated data in the target language. In this paper,\nwe explore three different approaches for cross-lingual transfer \\textemdash~\\\ndelexicalization as data augmentation, multilingual embeddings and machine\ntranslation. We compare these approaches both on a low resource setting as well\nas a large resource setting. Our experiments show that multilingual embeddings\nand delexicalization via data augmentation have a significant impact in the low\nresource setting, but the gains diminish as the amount of available data in the\ntarget language increases. Furthermore, when combined with machine translation\nwe can get performance very close to actual live data in the target language,\nwith only 25\\% of the data projected into the target language.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:52:58 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Sharaf", "Amr", ""], ["Gupta", "Arpit", ""], ["Ge", "Hancheng", ""], ["Naik", "Chetan", ""], ["Mathias", "Lambert", ""]]}, {"id": "1811.11365", "submitter": "Kai Fan Dr", "authors": "Yuanhang Su, Kai Fan, Nguyen Bach, C.-C. Jay Kuo, Fei Huang", "title": "Unsupervised Multi-modal Neural Machine Translation", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results with only large monolingual corpora in each language.\nHowever, the uncertainty of associating target with source sentences makes UNMT\ntheoretically an ill-posed problem. This work investigates the possibility of\nutilizing images for disambiguation to improve the performance of UNMT. Our\nassumption is intuitively based on the invariant property of image, i.e., the\ndescription of the same visual content by different languages should be\napproximately similar. We propose an unsupervised multi-modal machine\ntranslation (UMNMT) framework based on the language translation cycle\nconsistency loss conditional on the image, targeting to learn the bidirectional\nmulti-modal translation simultaneously. Through an alternate training between\nmulti-modal and uni-modal, our inference model can translate with or without\nthe image. On the widely used Multi30K dataset, the experimental results of our\napproach are significantly better than those of the text-only UNMT on the 2016\ntest dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:48:25 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 02:00:51 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Su", "Yuanhang", ""], ["Fan", "Kai", ""], ["Bach", "Nguyen", ""], ["Kuo", "C. -C. Jay", ""], ["Huang", "Fei", ""]]}, {"id": "1811.11374", "submitter": "Bin Bi", "authors": "Ming Yan, Jiangnan Xia, Chen Wu, Bin Bi, Zhongzhou Zhao, Ji Zhang, Luo\n  Si, Rui Wang, Wei Wang, Haiqing Chen", "title": "A Deep Cascade Model for Multi-Document Reading Comprehension", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": "10.1609/aaai.v33i01.33017354", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental trade-off between effectiveness and efficiency needs to be\nbalanced when designing an online question answering system. Effectiveness\ncomes from sophisticated functions such as extractive machine reading\ncomprehension (MRC), while efficiency is obtained from improvements in\npreliminary retrieval components such as candidate document selection and\nparagraph ranking. Given the complexity of the real-world multi-document MRC\nscenario, it is difficult to jointly optimize both in an end-to-end system. To\naddress this problem, we develop a novel deep cascade learning model, which\nprogressively evolves from the document-level and paragraph-level ranking of\ncandidate texts to more precise answer extraction with machine reading\ncomprehension. Specifically, irrelevant documents and paragraphs are first\nfiltered out with simple functions for efficiency consideration. Then we\njointly train three modules on the remaining texts for better tracking the\nanswer: the document extraction, the paragraph extraction and the answer\nextraction. Experiment results show that the proposed method outperforms the\nprevious state-of-the-art methods on two large-scale multi-document benchmark\ndatasets, i.e., TriviaQA and DuReader. In addition, our online system can\nstably serve typical scenarios with millions of daily requests in less than\n50ms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 03:48:34 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Yan", "Ming", ""], ["Xia", "Jiangnan", ""], ["Wu", "Chen", ""], ["Bi", "Bin", ""], ["Zhao", "Zhongzhou", ""], ["Zhang", "Ji", ""], ["Si", "Luo", ""], ["Wang", "Rui", ""], ["Wang", "Wei", ""], ["Chen", "Haiqing", ""]]}, {"id": "1811.11430", "submitter": "Junki Ohmura", "authors": "Junki Ohmura and Maxine Eskenazi", "title": "Context-Aware Dialog Re-Ranking for Task-Oriented Dialog Systems", "comments": "Accepted in IEEE SLT 2018. 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog response ranking is used to rank response candidates by considering\ntheir relation to the dialog history. Although researchers have addressed this\nconcept for open-domain dialogs, little attention has been focused on\ntask-oriented dialogs. Furthermore, no previous studies have analyzed whether\nresponse ranking can improve the performance of existing dialog systems in real\nhuman-computer dialogs with speech recognition errors. In this paper, we\npropose a context-aware dialog response re-ranking system. Our system reranks\nresponses in two steps: (1) it calculates matching scores for each candidate\nresponse and the current dialog context; (2) it combines the matching scores\nand a probability distribution of the candidates from an existing dialog system\nfor response re-ranking. By using neural word embedding-based models and\nhandcrafted or logistic regression-based ensemble models, we have improved the\nperformance of a recently proposed end-to-end task-oriented dialog system on\nreal dialogs with speech recognition errors.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:58:16 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ohmura", "Junki", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1811.11456", "submitter": "Soumen Chakrabarti", "authors": "Divam Gupta, Tanmoy Chakraborty, Soumen Chakrabarti", "title": "GIRNet: Interleaved Multi-Task Recurrent State Sequence Models", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several natural language tasks, labeled sequences are available in\nseparate domains (say, languages), but the goal is to label sequences with\nmixed domain (such as code-switched text). Or, we may have available models for\nlabeling whole passages (say, with sentiments), which we would like to exploit\ntoward better position-specific label inference (say, target-dependent\nsentiment annotation). A key characteristic shared across such tasks is that\ndifferent positions in a primary instance can benefit from different `experts'\ntrained from auxiliary data, but labeled primary instances are scarce, and\nlabeling the best expert for each position entails unacceptable cognitive\nburden. We propose GITNet, a unified position-sensitive multi-task recurrent\nneural network (RNN) architecture for such applications. Auxiliary and primary\ntasks need not share training instances. Auxiliary RNNs are trained over\nauxiliary instances. A primary instance is also submitted to each auxiliary\nRNN, but their state sequences are gated and merged into a novel composite\nstate sequence tailored to the primary inference task. Our approach is in sharp\ncontrast to recent multi-task networks like the cross-stitch and sluice\nnetwork, which do not control state transfer at such fine granularity. We\ndemonstrate the superiority of GIRNet using three applications: sentiment\nclassification of code-switched passages, part-of-speech tagging of\ncode-switched text, and target position-sensitive annotation of sentiment in\nmonolingual passages. In all cases, we establish new state-of-the-art\nperformance beyond recent competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:20:13 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 07:04:27 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Gupta", "Divam", ""], ["Chakraborty", "Tanmoy", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1811.11523", "submitter": "Zulfat Miftahutdinov", "authors": "Elena Tutubalina, Zulfat Miftahutdinov, Sergey Nikolenko, Valentin\n  Malykh", "title": "Sequence Learning with RNNs for Medical Concept Normalization in\n  User-Generated Texts", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": "Journal of Biomedical Informatics. - 2018. - Vol.84, Is.. -\n  P.93-102", "doi": "10.1016/j.jbi.2018.06.006", "report-no": "ML4H/2018/117", "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the medical concept normalization problem, i.e.,\nthe problem of mapping a disease mention in free-form text to a concept in a\ncontrolled vocabulary, usually to the standard thesaurus in the Unified Medical\nLanguage System (UMLS). This task is challenging since medical terminology is\nvery different when coming from health care professionals or from the general\npublic in the form of social media texts. We approach it as a sequence learning\nproblem, with recurrent neural networks trained to obtain semantic\nrepresentations of one- and multi-word expressions. We develop end-to-end\nneural architectures tailored specifically to medical concept normalization,\nincluding bidirectional LSTM and GRU with an attention mechanism and additional\nsemantic similarity features based on UMLS. Our evaluation over a standard\nbenchmark shows that our model improves over a state of the art baseline for\nclassification based on CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 12:42:57 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 07:49:44 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tutubalina", "Elena", ""], ["Miftahutdinov", "Zulfat", ""], ["Nikolenko", "Sergey", ""], ["Malykh", "Valentin", ""]]}, {"id": "1811.11683", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl\n  Vondrick, and Shih-Fu Chang", "title": "Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of phrase grounding by lear ing a multi-level common\nsemantic space shared by the textual and visual modalities. We exploit multiple\nlevels of feature maps of a Deep Convolutional Neural Network, as well as\ncontextualized word and sentence embeddings extracted from a character-based\nlanguage model. Following dedicated non-linear mappings for visual features at\neach level, word, and sentence embeddings, we obtain multiple instantiations of\nour common semantic space in which comparisons between any target text and the\nvisual content is performed with cosine similarity. We guide the model by a\nmulti-level multimodal attention mechanism which outputs attended visual\nfeatures at each level. The best level is chosen to be compared with text\ncontent for maximizing the pertinence scores of image-sentence pairs of the\nground truth. Experiments conducted on three publicly available datasets show\nsignificant performance gains (20%-60% relative) over the state-of-the-art in\nphrase localization and set a new performance record on those datasets. We\nprovide a detailed ablation study to show the contribution of each element of\nour approach and release our code on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:05:27 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 20:49:53 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Akbari", "Hassan", ""], ["Karaman", "Svebor", ""], ["Bhargava", "Surabhi", ""], ["Chen", "Brian", ""], ["Vondrick", "Carl", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1811.11707", "submitter": "Vladimir Vlasov", "authors": "Vladimir Vlasov, Akela Drissner-Schmid, Alan Nichol", "title": "Few-Shot Generalization Across Dialogue Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning based dialogue managers are able to learn complex behaviors\nin order to complete a task, but it is not straightforward to extend their\ncapabilities to new domains. We investigate different policies' ability to\nhandle uncooperative user behavior, and how well expertise in completing one\ntask (such as restaurant reservations) can be reapplied when learning a new one\n(e.g. booking a hotel). We introduce the Recurrent Embedding Dialogue Policy\n(REDP), which embeds system actions and dialogue states in the same vector\nspace. REDP contains a memory component and attention mechanism based on a\nmodified Neural Turing Machine, and significantly outperforms a baseline LSTM\nclassifier on this task. We also show that both our architecture and baseline\nsolve the bAbI dialogue task, achieving 100% test accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:51:39 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Vlasov", "Vladimir", ""], ["Drissner-Schmid", "Akela", ""], ["Nichol", "Alan", ""]]}, {"id": "1811.11934", "submitter": "Chen Wu", "authors": "Wei Wang, Ming Yan, Chen Wu", "title": "Multi-granularity hierarchical attention fusion networks for reading\n  comprehension and question answering", "comments": "Proceedings of the 56th Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers) 1705-1714", "journal-ref": null, "doi": "10.18653/v1/P18-1158", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel hierarchical attention network for reading\ncomprehension style question answering, which aims to answer questions for a\ngiven narrative paragraph. In the proposed method, attention and fusion are\nconducted horizontally and vertically across layers at different levels of\ngranularity between question and paragraph. Specifically, it first encode the\nquestion and paragraph with fine-grained language embeddings, to better capture\nthe respective representations at semantic level. Then it proposes a\nmulti-granularity fusion approach to fully fuse information from both global\nand attended representations. Finally, it introduces a hierarchical attention\nnetwork to focuses on the answer span progressively with multi-level\nsoftalignment. Extensive experiments on the large-scale SQuAD and TriviaQA\ndatasets validate the effectiveness of the proposed method. At the time of\nwriting the paper (Jan. 12th 2018), our model achieves the first position on\nthe SQuAD leaderboard for both single and ensemble models. We also achieves\nstate-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:44:07 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Wang", "Wei", ""], ["Yan", "Ming", ""], ["Wu", "Chen", ""]]}, {"id": "1811.11945", "submitter": "Yonghao Jin", "authors": "Yonghao Jin, Fei Li and Hong Yu", "title": "HYPE: A High Performing NLP System for Automatically Detecting\n  Hypoglycemia Events from Electronic Health Record Notes", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/104", "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypoglycemia is common and potentially dangerous among those treated for\ndiabetes. Electronic health records (EHRs) are important resources for\nhypoglycemia surveillance. In this study, we report the development and\nevaluation of deep learning-based natural language processing systems to\nautomatically detect hypoglycemia events from the EHR narratives. Experts in\nPublic Health annotated 500 EHR notes from patients with diabetes. We used this\nannotated dataset to train and evaluate HYPE, supervised NLP systems for\nhypoglycemia detection. In our experiment, the convolutional neural network\nmodel yielded promising performance $Precision=0.96 \\pm 0.03, Recall=0.86 \\pm\n0.03, F1=0.91 \\pm 0.03$ in a 10-fold cross-validation setting. Despite the\nannotated data is highly imbalanced, our CNN-based HYPE system still achieved a\nhigh performance for hypoglycemia detection. HYPE could be used for EHR-based\nhypoglycemia surveillance and to facilitate clinicians for timely treatment of\nhigh-risk patients.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 03:40:55 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Jin", "Yonghao", ""], ["Li", "Fei", ""], ["Yu", "Hong", ""]]}, {"id": "1811.11958", "submitter": "Yuhui Zhang", "authors": "Yuhui Zhang, Allen Nie, James Zou", "title": "Large-scale Generative Modeling to Improve Automated Veterinary Disease\n  Coding", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/83", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning is limited both by the quantity and quality of the\nlabeled data. In the field of medical record tagging, writing styles between\nhospitals vary drastically. The knowledge learned from one hospital might not\ntransfer well to another. This problem is amplified in veterinary medicine\ndomain because veterinary clinics rarely apply medical codes to their records.\nWe proposed and trained the first large-scale generative modeling algorithm in\nautomated disease coding. We demonstrate that generative modeling can learn\ndiscriminative features when additionally trained with supervised fine-tuning.\nWe systematically ablate and evaluate the effect of generative modeling on the\nfinal system's performance. We compare the performance of our model with\nseveral baselines in a challenging cross-hospital setting with substantial\ndomain shift. We outperform competitive baselines by a large margin. In\naddition, we provide interpretation for what is learned by our model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:26:07 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Zhang", "Yuhui", ""], ["Nie", "Allen", ""], ["Zou", "James", ""]]}, {"id": "1811.12112", "submitter": "Tom McCoy", "authors": "R. Thomas McCoy and Tal Linzen", "title": "Non-entailed subsequences as a challenge for natural language inference", "comments": "Accepted as an abstract for SCiL 2019; added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models have shown great success at natural language inference\n(NLI), the task of determining whether a premise entails a hypothesis. However,\nrecent studies suggest that these models may rely on fallible heuristics rather\nthan deep language understanding. We introduce a challenge set to test whether\nNLI systems adopt one such heuristic: assuming that a sentence entails all of\nits subsequences, such as assuming that \"Alice believes Mary is lying\" entails\n\"Alice believes Mary.\" We evaluate several competitive NLI models on this\nchallenge set and find strong evidence that they do rely on the subsequence\nheuristic.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:05:18 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:51:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["McCoy", "R. Thomas", ""], ["Linzen", "Tal", ""]]}, {"id": "1811.12148", "submitter": "Igor Shalyminov", "authors": "Igor Shalyminov and Sungjin Lee", "title": "Improving Robustness of Neural Dialog Systems in a Data-Efficient Way\n  with Turn Dropout", "comments": "NeurIPS 2018 workshop on Conversational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based dialog models often lack robustness to anomalous,\nout-of-domain (OOD) user input which leads to unexpected dialog behavior and\nthus considerably limits such models' usage in mission-critical production\nenvironments. The problem is especially relevant in the setting of dialog\nsystem bootstrapping with limited training data and no access to OOD examples.\nIn this paper, we explore the problem of robustness of such systems to\nanomalous input and the associated to it trade-off in accuracies on seen and\nunseen data. We present a new dataset for studying the robustness of dialog\nsystems to OOD input, which is bAbI Dialog Task 6 augmented with OOD content in\na controlled way. We then present turn dropout, a simple yet efficient negative\nsampling-based technique for improving robustness of neural dialog models. We\ndemonstrate its effectiveness applied to Hybrid Code Network-family models\n(HCNs) which reach state-of-the-art results on our OOD-augmented dataset as\nwell as the original one. Specifically, an HCN trained with turn dropout\nachieves state-of-the-art performance of more than 75% per-utterance accuracy\non the augmented dataset's OOD turns and 74% F1-score as an OOD detector.\nFurthermore, we introduce a Variational HCN enhanced with turn dropout which\nachieves more than 56.5% accuracy on the original bAbI Task 6 dataset, thus\noutperforming the initially reported HCN's result.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:03:00 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Shalyminov", "Igor", ""], ["Lee", "Sungjin", ""]]}, {"id": "1811.12157", "submitter": "Ruiqi Hu", "authors": "Ruiqi Hu, Celina Ping Yu, Sai-Fu Fung, Shirui Pan, Haishuai Wang,\n  Guodong Long", "title": "Universal Network Representation for Heterogeneous Information Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network representation aims to represent the nodes in a network as continuous\nand compact vectors, and has attracted much attention in recent years due to\nits ability to capture complex structure relationships inside networks.\nHowever, existing network representation methods are commonly designed for\nhomogeneous information networks where all the nodes (entities) of a network\nare of the same type, e.g., papers in a citation network. In this paper, we\npropose a universal network representation approach (UNRA), that represents\ndifferent types of nodes in heterogeneous information networks in a continuous\nand common vector space. The UNRA is built on our latest mutually updated\nneural language module, which simultaneously captures inter-relationship among\nhomogeneous nodes and node-content correlation. Relationships between different\ntypes of nodes are also assembled and learned in a unified framework.\nExperiments validate that the UNRA achieves outstanding performance, compared\nto six other state-of-the-art algorithms, in node representation, node\nclassification, and network visualization. In node classification, the UNRA\nachieves a 3\\% to 132\\% performance improvement in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 13:43:01 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hu", "Ruiqi", ""], ["Yu", "Celina Ping", ""], ["Fung", "Sai-Fu", ""], ["Pan", "Shirui", ""], ["Wang", "Haishuai", ""], ["Long", "Guodong", ""]]}, {"id": "1811.12181", "submitter": "Irene Li", "authors": "Irene Li, Alexander R. Fabbri, Robert R. Tung and Dragomir R. Radev", "title": "What Should I Learn First: Introducing LectureBank for NLP Education and\n  Prerequisite Chain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rising popularity of Natural Language\nProcessing (NLP) and related fields such as Artificial Intelligence (AI) and\nMachine Learning (ML). Many online courses and resources are available even for\nthose without a strong background in the field. Often the student is curious\nabout a specific topic but does not quite know where to begin studying. To\nanswer the question of \"what should one learn first,\" we apply an\nembedding-based method to learn prerequisite relations for course concepts in\nthe domain of NLP. We introduce LectureBank, a dataset containing 1,352 English\nlecture files collected from university courses which are each classified\naccording to an existing taxonomy as well as 208 manually-labeled prerequisite\nrelation topics, which is publicly available. The dataset will be useful for\neducational purposes such as lecture preparation and organization as well as\napplications such as reading list generation. Additionally, we experiment with\nneural graph-based networks and non-neural classifiers to learn these\nprerequisite relations from our dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:09:20 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Irene", ""], ["Fabbri", "Alexander R.", ""], ["Tung", "Robert R.", ""], ["Radev", "Dragomir R.", ""]]}, {"id": "1811.12239", "submitter": "Carolin Lawrence", "authors": "Carolin Lawrence and Stefan Riezler", "title": "Counterfactual Learning from Human Proofreading Feedback for Semantic\n  Parsing", "comments": "\"Learning by Instruction\" Workshop at the 32nd Conference on Neural\n  Information Processing Systems (NIPS 2018), Montr\\'eal, Canada. arXiv admin\n  note: substantial text overlap with arXiv:1805.01252", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic parsing for question-answering, it is often too expensive to\ncollect gold parses or even gold answers as supervision signals. We propose to\nconvert model outputs into a set of human-understandable statements which allow\nnon-expert users to act as proofreaders, providing error markings as learning\nsignals to the parser. Because model outputs were suggested by a historic\nsystem, we operate in a counterfactual, or off-policy, learning setup. We\nintroduce new estimators which can effectively leverage the given feedback and\nwhich avoid known degeneracies in counterfactual learning, while still being\napplicable to stochastic gradient optimization for neural semantic parsing.\nFurthermore, we discuss how our feedback collection method can be seamlessly\nintegrated into deployed virtual personal assistants that embed a semantic\nparser. Our work is the first to show that semantic parsers can be improved\nsignificantly by counterfactual learning from logged human feedback data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:20:30 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Lawrence", "Carolin", ""], ["Riezler", "Stefan", ""]]}, {"id": "1811.12254", "submitter": "Aparna Balagopalan", "authors": "Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz and Marzyeh\n  Ghassemi", "title": "The Effect of Heterogeneous Data for Alzheimer's Disease Detection from\n  Speech", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/147", "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech datasets for identifying Alzheimer's disease (AD) are generally\nrestricted to participants performing a single task, e.g. describing an image\nshown to them. As a result, models trained on linguistic features derived from\nsuch datasets may not be generalizable across tasks. Building on prior work\ndemonstrating that same-task data of healthy participants helps improve AD\ndetection on a single-task dataset of pathological speech, we augment an\nAD-specific dataset consisting of subjects describing a picture with multi-task\nhealthy data. We demonstrate that normative data from multiple speech-based\ntasks helps improve AD detection by up to 9%. Visualization of decision\nboundaries reveals that models trained on a combination of structured picture\ndescriptions and unstructured conversational speech have the least out-of-task\nerror and show the most potential to generalize to multiple tasks. We analyze\nthe impact of age of the added samples and if they affect fairness in\nclassification. We also provide explanations for a possible inductive bias\neffect across tasks using model-agnostic feature anchors. This work highlights\nthe need for heterogeneous datasets for encoding changes in multiple facets of\ncognition and for developing a task-independent AD detection model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:37:45 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Balagopalan", "Aparna", ""], ["Novikova", "Jekaterina", ""], ["Rudzicz", "Frank", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1811.12276", "submitter": "Parminder Bhatia", "authors": "Mengqi Jin, Mohammad Taha Bahadori, Aaron Colak, Parminder Bhatia,\n  Busra Celikkaya, Ram Bhakta, Selvan Senthivel, Mohammed Khalilia, Daniel\n  Navarro, Borui Zhang, Tiberiu Doman, Arun Ravi, Matthieu Liger, Taha\n  Kass-hout", "title": "Improving Hospital Mortality Prediction with Medical Named Entities and\n  Multimodal Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical text provides essential information to estimate the acuity of a\npatient during hospital stays in addition to structured clinical data. In this\nstudy, we explore how clinical text can complement a clinical predictive\nlearning task. We leverage an internal medical natural language processing\nservice to perform named entity extraction and negation detection on clinical\nnotes and compose selected entities into a new text corpus to train document\nrepresentations. We then propose a multimodal neural network to jointly train\ntime series signals and unstructured clinical text representations to predict\nthe in-hospital mortality risk for ICU patients. Our model outperforms the\nbenchmark by 2% AUC.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:10:41 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 01:37:07 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Jin", "Mengqi", ""], ["Bahadori", "Mohammad Taha", ""], ["Colak", "Aaron", ""], ["Bhatia", "Parminder", ""], ["Celikkaya", "Busra", ""], ["Bhakta", "Ram", ""], ["Senthivel", "Selvan", ""], ["Khalilia", "Mohammed", ""], ["Navarro", "Daniel", ""], ["Zhang", "Borui", ""], ["Doman", "Tiberiu", ""], ["Ravi", "Arun", ""], ["Liger", "Matthieu", ""], ["Kass-hout", "Taha", ""]]}, {"id": "1811.12354", "submitter": "Yoav Artzi", "authors": "Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi", "title": "Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments", "comments": "arXiv admin note: text overlap with arXiv:1809.00786", "journal-ref": "Published in CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:06:22 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 02:17:42 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 17:27:38 GMT"}, {"version": "v4", "created": "Sat, 6 Apr 2019 17:35:24 GMT"}, {"version": "v5", "created": "Wed, 10 Apr 2019 18:43:40 GMT"}, {"version": "v6", "created": "Mon, 3 Jun 2019 16:24:49 GMT"}, {"version": "v7", "created": "Sat, 16 May 2020 23:36:36 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chen", "Howard", ""], ["Suhr", "Alane", ""], ["Misra", "Dipendra", ""], ["Snavely", "Noah", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.12640", "submitter": "Sudeshna Roy", "authors": "Sudeshna Roy, Meghana Madhyastha, Sheril Lawrence, Vaibhav Rajan", "title": "Inferring Concept Prerequisite Relations from Online Educational\n  Resources", "comments": "Accepted at the AAAI Conference on Innovative Applications of\n  Artificial Intelligence (IAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has rich and rapidly increasing sources of high quality\neducational content. Inferring prerequisite relations between educational\nconcepts is required for modern large-scale online educational technology\napplications such as personalized recommendations and automatic curriculum\ncreation. We present PREREQ, a new supervised learning method for inferring\nconcept prerequisite relations. PREREQ is designed using latent representations\nof concepts obtained from the Pairwise Latent Dirichlet Allocation model, and a\nneural network based on the Siamese network architecture. PREREQ can learn\nunknown concept prerequisites from course prerequisites and labeled concept\nprerequisite data. It outperforms state-of-the-art approaches on benchmark\ndatasets and can effectively learn from very less training data. PREREQ can\nalso use unlabeled video playlists, a steadily growing source of training data,\nto learn concept prerequisites, thus obviating the need for manual annotation\nof course prerequisites.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:55:20 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 00:39:00 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Roy", "Sudeshna", ""], ["Madhyastha", "Meghana", ""], ["Lawrence", "Sheril", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "1811.12728", "submitter": "Balaji Ganesan", "authors": "Aswin Kannan, Shanmukha C Guttula, Balaji Ganesan, Hima P Karanam,\n  Arun Kumar", "title": "Document Structure Measure for Hypernym discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypernym discovery is the problem of finding terms that have is-a\nrelationship with a given term. We introduce a new context type, and a\nrelatedness measure to differentiate hypernyms from other types of semantic\nrelationships. Our Document Structure measure is based on hierarchical position\nof terms in a document, and their presence or otherwise in definition text.\nThis measure quantifies the document structure using multiple attributes, and\nclasses of weighted distance functions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 11:10:59 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kannan", "Aswin", ""], ["Guttula", "Shanmukha C", ""], ["Ganesan", "Balaji", ""], ["Karanam", "Hima P", ""], ["Kumar", "Arun", ""]]}, {"id": "1811.12739", "submitter": "Yedid Hoshen", "authors": "Tavi Halperin and Ariel Ephrat and Yedid Hoshen", "title": "Neural separation of observed and unobserved distributions", "comments": "ICML'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating mixed distributions is a long standing challenge for machine\nlearning and signal processing. Most current methods either rely on making\nstrong assumptions on the source distributions or rely on having training\nsamples of each source in the mixture. In this work, we introduce a new\nmethod---Neural Egg Separation---to tackle the scenario of extracting a signal\nfrom an unobserved distribution additively mixed with a signal from an observed\ndistribution. Our method iteratively learns to separate the known distribution\nfrom progressively finer estimates of the unknown distribution. In some\nsettings, Neural Egg Separation is initialization sensitive, we therefore\nintroduce Latent Mixture Masking which ensures a good initialization. Extensive\nexperiments on audio and image separation tasks show that our method\noutperforms current methods that use the same level of supervision, and often\nachieves similar performance to full supervision.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 11:38:54 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 12:17:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Halperin", "Tavi", ""], ["Ephrat", "Ariel", ""], ["Hoshen", "Yedid", ""]]}, {"id": "1811.12793", "submitter": "Monica Agrawal", "authors": "Monica Agrawal, Griffin Adams, Nathan Nussbaum, Benjamin Birnbaum", "title": "TIFTI: A Framework for Extracting Drug Intervals from Longitudinal\n  Clinic Notes", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/69", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oral drugs are becoming increasingly common in oncology care. In contrast to\nintravenous chemotherapy, which is administered in the clinic and carefully\ntracked via structure electronic health records (EHRs), oral drug treatment is\nself-administered and therefore not tracked as well. Often, the details of oral\ncancer treatment occur only in unstructured clinic notes. Extracting this\ninformation is critical to understanding a patient's treatment history. Yet,\nthis a challenging task because treatment intervals must be inferred\nlongitudinally from both explicit mentions in the text as well as from document\ntimestamps. In this work, we present TIFTI (Temporally Integrated Framework for\nTreatment Intervals), a robust framework for extracting oral drug treatment\nintervals from a patient's unstructured notes. TIFTI leverages distinct sources\nof temporal information by breaking the problem down into two separate\nsubtasks: document-level sequence labeling and date extraction. On a labeled\ndataset of metastatic renal-cell carcinoma (RCC) patients, it exactly matched\nthe labeled start date in 46% of the examples (86% of the examples within 30\ndays), and it exactly matched the labeled end date in 52% of the examples (78%\nof the examples within 30 days). Without retraining, the model achieved a\nsimilar level of performance on a labeled dataset of advanced non-small-cell\nlung cancer (NSCLC) patients.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:50:15 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:38:29 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Agrawal", "Monica", ""], ["Adams", "Griffin", ""], ["Nussbaum", "Nathan", ""], ["Birnbaum", "Benjamin", ""]]}, {"id": "1811.12889", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu\n  Nguyen, Harm de Vries, Aaron Courville", "title": "Systematic Generalization: What Is Required and Can It Be Learned?", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous models for grounded language understanding have been recently\nproposed, including (i) generic models that can be easily adapted to any given\ntask and (ii) intuitively appealing modular models that require background\nknowledge to be instantiated. We compare both types of models in how much they\nlend themselves to a particular form of systematic generalization. Using a\nsynthetic VQA test, we evaluate which models are capable of reasoning about all\npossible object pairs after training on only a small subset of them. Our\nfindings show that the generalization of modular models is much more systematic\nand that it is highly sensitive to the module layout, i.e. to how exactly the\nmodules are connected. We furthermore investigate if modular models that\ngeneralize well could be made more end-to-end by learning their layout and\nparametrization. We find that end-to-end methods from prior work often learn\ninappropriate layouts or parametrizations that do not facilitate systematic\ngeneralization. Our results suggest that, in addition to modularity, systematic\ngeneralization in language understanding may require explicit regularizers or\npriors.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:01:28 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 14:58:38 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 15:37:46 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Murty", "Shikhar", ""], ["Noukhovitch", "Michael", ""], ["Nguyen", "Thien Huu", ""], ["de Vries", "Harm", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.12891", "submitter": "Rahul Goel", "authors": "Rahul Goel, Shachi Paul, Tagyoung Chung, Jeremie Lecomte, Arindam\n  Mandal, Dilek Hakkani-Tur", "title": "Flexible and Scalable State Tracking Framework for Goal-Oriented\n  Dialogue Systems", "comments": "NIPS CONVAI Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal-oriented dialogue systems typically rely on components specifically\ndeveloped for a single task or domain. This limits such systems in two\ndifferent ways: If there is an update in the task domain, the dialogue system\nusually needs to be updated or completely re-trained. It is also harder to\nextend such dialogue systems to different and multiple domains. The dialogue\nstate tracker in conventional dialogue systems is one such component - it is\nusually designed to fit a well-defined application domain. For example, it is\ncommon for a state variable to be a categorical distribution over a\nmanually-predefined set of entities (Henderson et al., 2013), resulting in an\ninflexible and hard-to-extend dialogue system. In this paper, we propose a new\napproach for dialogue state tracking that can generalize well over multiple\ndomains without incorporating any domain-specific knowledge. Under this\nframework, discrete dialogue state variables are learned independently and the\ninformation of a predefined set of possible values for dialogue state variables\nis not required. Furthermore, it enables adding arbitrary dialogue context as\nfeatures and allows for multiple values to be associated with a single state\nvariable. These characteristics make it much easier to expand the dialogue\nstate space. We evaluate our framework using the widely used dialogue state\ntracking challenge data set (DSTC2) and show that our framework yields\ncompetitive results with other state-of-the-art results despite incorporating\nlittle domain knowledge. We also show that this framework can benefit from\nwidely available external resources such as pre-trained word embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:01:48 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Goel", "Rahul", ""], ["Paul", "Shachi", ""], ["Chung", "Tagyoung", ""], ["Lecomte", "Jeremie", ""], ["Mandal", "Arindam", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1811.12900", "submitter": "Rahul Goel", "authors": "Chandra Khatri, Behnam Hedayatnia, Rahul Goel, Anushree Venkatesh,\n  Raefer Gabriel, Arindam Mandal", "title": "Detecting Offensive Content in Open-domain Conversations using Two Stage\n  Semi-supervision", "comments": "NIPS CONVAI Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As open-ended human-chatbot interaction becomes commonplace, sensitive\ncontent detection gains importance. In this work, we propose a two stage\nsemi-supervised approach to bootstrap large-scale data for automatic sensitive\nlanguage detection from publicly available web resources. We explore various\ndata selection methods including 1) using a blacklist to rank online discussion\nforums by the level of their sensitiveness followed by randomly sampling\nutterances and 2) training a weakly supervised model in conjunction with the\nblacklist for scoring sentences from online discussion forums to curate a\ndataset. Our data collection strategy is flexible and allows the models to\ndetect implicit sensitive content for which manual annotations may be\ndifficult. We train models using publicly available annotated datasets as well\nas using the proposed large-scale semi-supervised datasets. We evaluate the\nperformance of all the models on Twitter and Toxic Wikipedia comments testsets\nas well as on a manually annotated spoken language dataset collected during a\nlarge scale chatbot competition. Results show that a model trained on this\ncollected data outperforms the baseline models by a large margin on both\nin-domain and out-of-domain testsets, achieving an F1 score of 95.5% on an\nout-of-domain testset compared to a score of 75% for models trained on public\ndatasets. We also showcase that large scale two stage semi-supervision\ngeneralizes well across multiple classes of sensitivities such as hate speech,\nracism, sexual and pornographic content, etc. without even providing explicit\nlabels for these classes, leading to an average recall of 95.5% versus the\nmodels trained using annotated public datasets which achieve an average recall\nof 73.2% across seven sensitive classes on out-of-domain testsets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:14:30 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Khatri", "Chandra", ""], ["Hedayatnia", "Behnam", ""], ["Goel", "Rahul", ""], ["Venkatesh", "Anushree", ""], ["Gabriel", "Raefer", ""], ["Mandal", "Arindam", ""]]}]