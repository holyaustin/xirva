[{"id": "1702.00167", "submitter": "Shubham Tripathi", "authors": "Deepak Gupta, Shubham Tripathi, Asif Ekbal, Pushpak Bhattacharyya", "title": "SMPOST: Parts of Speech Tagger for Code-Mixed Indic Social Media Text", "comments": "5 pages, ICON 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of social media has grown dramatically during the last few years. Users\nfollow informal languages in communicating through social media. The language\nof communication is often mixed in nature, where people transcribe their\nregional language with English and this technique is found to be extremely\npopular. Natural language processing (NLP) aims to infer the information from\nthese text where Part-of-Speech (PoS) tagging plays an important role in\ngetting the prosody of the written text. For the task of PoS tagging on\nCode-Mixed Indian Social Media Text, we develop a supervised system based on\nConditional Random Field classifier. In order to tackle the problem\neffectively, we have focused on extracting rich linguistic features. We\nparticipate in three different language pairs, ie. English-Hindi,\nEnglish-Bengali and English-Telugu on three different social media platforms,\nTwitter, Facebook & WhatsApp. The proposed system is able to successfully\nassign coarse as well as fine-grained PoS tag labels for a given a code-mixed\nsentence. Experiments show that our system is quite generic that shows\nencouraging performance levels on all the three language pairs in all the\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 09:04:54 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 08:12:13 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Gupta", "Deepak", ""], ["Tripathi", "Shubham", ""], ["Ekbal", "Asif", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1702.00210", "submitter": "Scott A. Hale", "authors": "Scott A. Hale and Irene Eleta", "title": "Foreign-language Reviews: Help or Hindrance?", "comments": null, "journal-ref": "Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems, CHI 2017", "doi": "10.1145/3025453.3025575", "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number and quality of user reviews greatly affects consumer purchasing\ndecisions. While reviews in all languages are increasing, it is still often the\ncase (especially for non-English speakers) that there are only a few reviews in\na person's first language. Using an online experiment, we examine the value\nthat potential purchasers receive from interfaces showing additional reviews in\na second language. The results paint a complicated picture with both positive\nand negative reactions to the inclusion of foreign-language reviews. Roughly\n26-28% of subjects clicked to see translations of the foreign-language content\nwhen given the opportunity, and those who did so were more likely to select the\nproduct with foreign-language reviews than those who did not.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 11:18:47 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Hale", "Scott A.", ""], ["Eleta", "Irene", ""]]}, {"id": "1702.00500", "submitter": "Linfeng Song", "authors": "Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea", "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar", "comments": "camera-ready version of ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of AMR-to-text generation by leveraging\nsynchronous node replacement grammar. During training, graph-to-string rules\nare learned using a heuristic extraction algorithm. At test time, a graph\ntransducer is applied to collapse input AMRs and generate output sentences.\nEvaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which\nis the best reported so far.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 23:36:33 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 18:22:24 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 02:39:14 GMT"}, {"version": "v4", "created": "Fri, 28 Apr 2017 13:37:00 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Song", "Linfeng", ""], ["Peng", "Xiaochang", ""], ["Zhang", "Yue", ""], ["Wang", "Zhiguo", ""], ["Gildea", "Daniel", ""]]}, {"id": "1702.00523", "submitter": "Satish Palaniappan", "authors": "Satish Palaniappan and Ronojoy Adhikari", "title": "Deep Learning the Indus Script", "comments": "17 pages, 10 figures, 7 supporting figures (2 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized corpora of undeciphered scripts, a necessary starting point for\ncomputational epigraphy, requires laborious human effort for their preparation\nfrom raw archaeological records. Automating this process through machine\nlearning algorithms can be of significant aid to epigraphical research. Here,\nwe take the first steps in this direction and present a deep learning pipeline\nthat takes as input images of the undeciphered Indus script, as found in\narchaeological artifacts, and returns as output a string of graphemes, suitable\nfor inclusion in a standard corpus. The image is first decomposed into regions\nusing Selective Search and these regions are classified as containing textual\nand/or graphical information using a convolutional neural network. Regions\nclassified as potentially containing text are hierarchically merged and trimmed\nto remove non-textual information. The remaining textual part of the image is\nsegmented using standard image processing techniques to isolate individual\ngraphemes. This set is finally passed to a second convolutional neural network\nto classify the graphemes, based on a standard corpus. The classifier can\nidentify the presence or absence of the most frequent Indus grapheme, the \"jar\"\nsign, with an accuracy of 92%. Our results demonstrate the great potential of\ndeep learning approaches in computational epigraphy and, more generally, in the\ndigital humanities.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 01:56:22 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Palaniappan", "Satish", ""], ["Adhikari", "Ronojoy", ""]]}, {"id": "1702.00564", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth, Nicolas Chopin, Robin Ryder, Bruno Nicenboim", "title": "Modelling dependency completion in sentence comprehension as a Bayesian\n  hierarchical mixture process: A case study involving Chinese relative clauses", "comments": "6 pages, 2 figures. To appear in the Proceedings of the Cognitive\n  Science Conference 2017, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case-study demonstrating the usefulness of Bayesian hierarchical\nmixture modelling for investigating cognitive processes. In sentence\ncomprehension, it is widely assumed that the distance between linguistic\nco-dependents affects the latency of dependency resolution: the longer the\ndistance, the longer the retrieval time (the distance-based account). An\nalternative theory, direct-access, assumes that retrieval times are a mixture\nof two distributions: one distribution represents successful retrievals (these\nare independent of dependency distance) and the other represents an initial\nfailure to retrieve the correct dependent, followed by a reanalysis that leads\nto successful retrieval. We implement both models as Bayesian hierarchical\nmodels and show that the direct-access model explains Chinese relative clause\nreading time data better than the distance account.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:48:58 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 05:44:34 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Vasishth", "Shravan", ""], ["Chopin", "Nicolas", ""], ["Ryder", "Robin", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1702.00700", "submitter": "Rodrigo Agerri", "authors": "Egoitz Laparra and Rodrigo Agerri and Itziar Aldabe and German Rigau", "title": "Multilingual and Cross-lingual Timeline Extraction", "comments": "20 pages, 7 tables, 7 figures; submitted to Knowledge Based Systems\n  (Elsevier), January, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present an approach to extract ordered timelines of events,\ntheir participants, locations and times from a set of multilingual and\ncross-lingual data sources. Based on the assumption that event-related\ninformation can be recovered from different documents written in different\nlanguages, we extend the Cross-document Event Ordering task presented at\nSemEval 2015 by specifying two new tasks for, respectively, Multilingual and\nCross-lingual Timeline Extraction. We then develop three deterministic\nalgorithms for timeline extraction based on two main ideas. First, we address\nimplicit temporal relations at document level since explicit time-anchors are\ntoo scarce to build a wide coverage timeline extraction system. Second, we\nleverage several multilingual resources to obtain a single, inter-operable,\nsemantic representation of events across documents and across languages. The\nresult is a highly competitive system that strongly outperforms the current\nstate-of-the-art. Nonetheless, further analysis of the results reveals that\nlinking the event mentions with their target entities and time-anchors remains\na difficult challenge. The systems, resources and scorers are freely available\nto facilitate its use and guarantee the reproducibility of results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 14:44:17 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Laparra", "Egoitz", ""], ["Agerri", "Rodrigo", ""], ["Aldabe", "Itziar", ""], ["Rigau", "German", ""]]}, {"id": "1702.00716", "submitter": "Simon Gottschalk", "authors": "Simon Gottschalk, Elena Demidova", "title": "Analysing Temporal Evolution of Interlingual Wikipedia Article Pairs", "comments": "Published in the SIGIR '16 Proceedings of the 39th International ACM\n  SIGIR conference on Research and Development in Information Retrieval", "journal-ref": null, "doi": "10.1145/2911451.2911472", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia articles representing an entity or a topic in different language\neditions evolve independently within the scope of the language-specific user\ncommunities. This can lead to different points of views reflected in the\narticles, as well as complementary and inconsistent information. An analysis of\nhow the information is propagated across the Wikipedia language editions can\nprovide important insights in the article evolution along the temporal and\ncultural dimensions and support quality control. To facilitate such analysis,\nwe present MultiWiki - a novel web-based user interface that provides an\noverview of the similarities and differences across the article pairs\noriginating from different language editions on a timeline. MultiWiki enables\nusers to observe the changes in the interlingual article similarity over time\nand to perform a detailed visual comparison of the article snapshots at a\nparticular time point.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 15:41:46 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Gottschalk", "Simon", ""], ["Demidova", "Elena", ""]]}, {"id": "1702.00764", "submitter": "Fabio Massimo Zanzotto", "authors": "Lorenzo Ferrone and Fabio Massimo Zanzotto", "title": "Symbolic, Distributed and Distributional Representations for Natural\n  Language Processing in the Era of Deep Learning: a Survey", "comments": "25 pages", "journal-ref": "Frontiers in Robotics and AI, 2020", "doi": "10.3389/frobt.2019.00153", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language is inherently a discrete symbolic representation of human\nknowledge. Recent advances in machine learning (ML) and in natural language\nprocessing (NLP) seem to contradict the above intuition: discrete symbols are\nfading away, erased by vectors or tensors called distributed and distributional\nrepresentations. However, there is a strict link between\ndistributed/distributional representations and discrete symbols, being the\nfirst an approximation of the second. A clearer understanding of the strict\nlink between distributed/distributional representations and symbols may\ncertainly lead to radically new deep learning networks. In this paper we make a\nsurvey that aims to renew the link between symbolic representations and\ndistributed/distributional representations. This is the right time to\nrevitalize the area of interpreting how discrete symbols are represented inside\nneural networks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 17:53:29 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 13:32:42 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ferrone", "Lorenzo", ""], ["Zanzotto", "Fabio Massimo", ""]]}, {"id": "1702.00860", "submitter": "Jaimie Murdock", "authors": "Colin Allen and Hongliang Luo and Jaimie Murdock and Jianghuai Pu and\n  Xiaohong Wang and Yanjie Zhai and Kun Zhao", "title": "Topic Modeling the H\\`an di\\u{a}n Ancient Classics", "comments": "24 pages; 14 pages supplemental", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ancient Chinese texts present an area of enormous challenge and opportunity\nfor humanities scholars interested in exploiting computational methods to\nassist in the development of new insights and interpretations of culturally\nsignificant materials. In this paper we describe a collaborative effort between\nIndiana University and Xi'an Jiaotong University to support exploration and\ninterpretation of a digital corpus of over 18,000 ancient Chinese documents,\nwhich we refer to as the \"Handian\" ancient classics corpus (H\\`an di\\u{a}n\ng\\u{u} j\\'i, i.e, the \"Han canon\" or \"Chinese classics\"). It contains classics\nof ancient Chinese philosophy, documents of historical and biographical\nsignificance, and literary works. We begin by describing the Digital Humanities\ncontext of this joint project, and the advances in humanities computing that\nmade this project feasible. We describe the corpus and introduce our\napplication of probabilistic topic modeling to this corpus, with attention to\nthe particular challenges posed by modeling ancient Chinese documents. We give\na specific example of how the software we have developed can be used to aid\ndiscovery and interpretation of themes in the corpus. We outline more advanced\nforms of computer-aided interpretation that are also made possible by the\nprogramming interface provided by our system, and the general implications of\nthese methods for understanding the nature of meaning in these texts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 22:51:04 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Allen", "Colin", ""], ["Luo", "Hongliang", ""], ["Murdock", "Jaimie", ""], ["Pu", "Jianghuai", ""], ["Wang", "Xiaohong", ""], ["Zhai", "Yanjie", ""], ["Zhao", "Kun", ""]]}, {"id": "1702.00887", "submitter": "Yoon Kim", "authors": "Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush", "title": "Structured Attention Networks", "comments": "ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 01:40:45 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 16:37:44 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 17:52:03 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Kim", "Yoon", ""], ["Denton", "Carl", ""], ["Hoang", "Luong", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1702.00956", "submitter": "Suwon Shon", "authors": "Suwon Shon, Hanseok Ko", "title": "KU-ISPL Speaker Recognition Systems under Language mismatch condition\n  for NIST 2016 Speaker Recognition Evaluation", "comments": "SRE16, NIST SRE 2016 system description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed\nspeaker recognition system for SRE16 fixed training condition. Data for\nevaluation trials are collected from outside North America, spoken in Tagalog\nand Cantonese while training data only is spoken English. Thus, main issue for\nSRE16 is compensating the discrepancy between different languages. As\ndevelopment dataset which is spoken in Cebuano and Mandarin, we could prepare\nthe evaluation trials through preliminary experiments to compensate the\nlanguage mismatched condition. Our team developed 4 different approaches to\nextract i-vectors and applied state-of-the-art techniques as backend. To\ncompensate language mismatch, we investigated and endeavored unique method such\nas unsupervised language clustering, inter language variability compensation\nand gender/language dependent score normalization.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 10:15:29 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 03:37:28 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Shon", "Suwon", ""], ["Ko", "Hanseok", ""]]}, {"id": "1702.00992", "submitter": "Eric Malmi", "authors": "Eric Malmi, Daniele Pighin, Sebastian Krause, Mikhail Kozhevnikov", "title": "Automatic Prediction of Discourse Connectives", "comments": "This is a pre-print of an article appearing at LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of suitable discourse connectives (however, furthermore,\netc.) is a key component of any system aimed at building coherent and fluent\ndiscourses from shorter sentences and passages. As an example, a dialog system\nmight assemble a long and informative answer by sampling passages extracted\nfrom different documents retrieved from the Web. We formulate the task of\ndiscourse connective prediction and release a dataset of 2.9M sentence pairs\nseparated by discourse connectives for this task. Then, we evaluate the\nhardness of the task for human raters, apply a recently proposed decomposable\nattention (DA) model to this task and observe that the automatic predictor has\na higher F1 than human raters (32 vs. 30). Nevertheless, under specific\nconditions the raters still outperform the DA model, suggesting that there is\nheadroom for future improvements.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 13:06:25 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 15:43:28 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Malmi", "Eric", ""], ["Pighin", "Daniele", ""], ["Krause", "Sebastian", ""], ["Kozhevnikov", "Mikhail", ""]]}, {"id": "1702.01090", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock and Colin Allen and Katy B\\\"orner and Robert Light and\n  Simon McAlister and Andrew Ravenscroft and Robert Rose and Doori Rose and Jun\n  Otsuka and David Bourget and John Lawrence and Chris Reed", "title": "Multi-level computational methods for interdisciplinary research in the\n  HathiTrust Digital Library", "comments": "revised, 29 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0184188", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show how faceted search using a combination of traditional classification\nsystems and mixed-membership topic models can go beyond keyword search to\ninform resource discovery, hypothesis formulation, and argument extraction for\ninterdisciplinary research. Our test domain is the history and philosophy of\nscientific work on animal mind and cognition. The methods can be generalized to\nother research areas and ultimately support a system for semi-automatic\nidentification of argument structures. We provide a case study for the\napplication of the methods to the problem of identifying and extracting\narguments about anthropomorphism during a critical period in the development of\ncomparative psychology. We show how a combination of classification systems and\nmixed-membership models trained over large digital libraries can inform\nresource discovery in this domain. Through a novel approach of \"drill-down\"\ntopic modeling---simultaneously reducing both the size of the corpus and the\nunit of analysis---we are able to reduce a large collection of fulltext volumes\nto a much smaller set of pages within six focal volumes containing arguments of\ninterest to historians and philosophers of comparative psychology. The volumes\nidentified in this way did not appear among the first ten results of the\nkeyword search in the HathiTrust digital library and the pages bear the kind of\n\"close reading\" needed to generate original interpretations that is the heart\nof scholarly work in the humanities. Zooming back out, we provide a way to\nplace the books onto a map of science originally constructed from very\ndifferent data and for different purposes. The multilevel approach advances\nunderstanding of the intellectual and societal contexts in which writings are\ninterpreted.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 17:36:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:22:59 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Murdock", "Jaimie", ""], ["Allen", "Colin", ""], ["B\u00f6rner", "Katy", ""], ["Light", "Robert", ""], ["McAlister", "Simon", ""], ["Ravenscroft", "Andrew", ""], ["Rose", "Robert", ""], ["Rose", "Doori", ""], ["Otsuka", "Jun", ""], ["Bourget", "David", ""], ["Lawrence", "John", ""], ["Reed", "Chris", ""]]}, {"id": "1702.01101", "submitter": "Iacer Calixto", "authors": "Iacer Calixto and Qun Liu and Nick Campbell", "title": "Multilingual Multi-modal Embeddings for Natural Language Processing", "comments": "4 pages (5 including references), no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel discriminative model that learns embeddings from\nmultilingual and multi-modal data, meaning that our model can take advantage of\nimages and descriptions in multiple languages to improve embedding quality. To\nthat end, we introduce a modification of a pairwise contrastive estimation\noptimisation function as our training objective. We evaluate our embeddings on\nan image-sentence ranking (ISR), a semantic textual similarity (STS), and a\nneural machine translation (NMT) task. We find that the additional multilingual\nsignals lead to improvements on both the ISR and STS tasks, and the\ndiscriminative cost can also be used in re-ranking $n$-best lists produced by\nNMT models, yielding strong improvements.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 18:19:47 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Calixto", "Iacer", ""], ["Liu", "Qun", ""], ["Campbell", "Nick", ""]]}, {"id": "1702.01147", "submitter": "Maria N\\u{a}dejde", "authors": "Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin\n  Junczys-Dowmunt, Philipp Koehn, Alexandra Birch", "title": "Predicting Target Language CCG Supertags Improves Neural Machine\n  Translation", "comments": "Accepted at the Second Conference on Machine Translation (WMT17).\n  This version includes more results regarding target syntax for\n  Romanian->English and reports fewer results regarding source syntax", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) models are able to partially learn syntactic\ninformation from sequential lexical information. Still, some complex syntactic\nphenomena such as prepositional phrase attachment are poorly modeled. This work\naims to answer two questions: 1) Does explicitly modeling target language\nsyntax help NMT? 2) Is tight integration of words and syntax better than\nmultitask training? We introduce syntactic information in the form of CCG\nsupertags in the decoder, by interleaving the target supertags with the word\nsequence. Our results on WMT data show that explicitly modeling target-syntax\nimproves machine translation quality for German->English, a high-resource pair,\nand for Romanian->English, a low-resource pair and also several syntactic\nphenomena including prepositional phrase attachment. Furthermore, a tight\ncoupling of words and syntax improves translation quality more than multitask\ntraining. By combining target-syntax with adding source-side dependency labels\nin the embedding layer, we obtain a total improvement of 0.9 BLEU for\nGerman->English and 1.2 BLEU for Romanian->English.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 20:31:34 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 12:07:45 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Nadejde", "Maria", ""], ["Reddy", "Siva", ""], ["Sennrich", "Rico", ""], ["Dwojak", "Tomasz", ""], ["Junczys-Dowmunt", "Marcin", ""], ["Koehn", "Philipp", ""], ["Birch", "Alexandra", ""]]}, {"id": "1702.01172", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Thomas Risse", "title": "Insights into Entity Name Evolution on Wikipedia", "comments": "WISE 2014, Thessaloniki, Greece", "journal-ref": null, "doi": "10.1007/978-3-319-11746-1_4", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with Web archives raises a number of issues caused by their temporal\ncharacteristics. Depending on the age of the content, additional knowledge\nmight be needed to find and understand older texts. Especially facts about\nentities are subject to change. Most severe in terms of information retrieval\nare name changes. In order to find entities that have changed their name over\ntime, search engines need to be aware of this evolution. We tackle this problem\nby analyzing Wikipedia in terms of entity evolutions mentioned in articles\nregardless the structural elements. We gathered statistics and automatically\nextracted minimum excerpts covering name changes by incorporating lists\ndedicated to that subject. In future work, these excerpts are going to be used\nto discover patterns and detect changes in other sources. In this work we\ninvestigate whether or not Wikipedia is a suitable source for extracting the\nrequired knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:36:46 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01176", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Thomas Risse", "title": "Named Entity Evolution Analysis on Wikipedia", "comments": "WebSci 2014, Bloomington, IN, USA. arXiv admin note: substantial text\n  overlap with arXiv:1702.01172", "journal-ref": null, "doi": "10.1145/2615569.2615639", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessing Web archives raises a number of issues caused by their temporal\ncharacteristics. Additional knowledge is needed to find and understand older\ntexts. Especially entities mentioned in texts are subject to change. Most\nsevere in terms of information retrieval are name changes. In order to find\nentities that have changed their name over time, search engines need to be\naware of this evolution. We tackle this problem by analyzing Wikipedia in terms\nof entity evolutions mentioned in articles. We present statistical data on\nexcerpts covering name changes, which will be used to discover similar text\npassages and extract evolution knowledge in future work.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:44:26 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Holzmann", "Helge", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01179", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Thomas Risse", "title": "Extraction of Evolution Descriptions from the Web", "comments": "Digital Libraries (JCDL) 2014, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of named entities affects exploration and retrieval tasks in\ndigital libraries. An information retrieval system that is aware of name\nchanges can actively support users in finding former occurrences of evolved\nentities. However, current structured knowledge bases, such as DBpedia or\nFreebase, do not provide enough information about evolutions, even though the\ndata is available on their resources, like Wikipedia. Our \\emph{Evolution Base}\nprototype will demonstrate how excerpts describing name evolutions can be\nidentified on these websites with a promising precision. The descriptions are\nclassified by means of models that we trained based on a recent analysis of\nnamed entity evolutions on Wikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:48:18 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01187", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Nina Tahmasebi, Thomas Risse", "title": "Named Entity Evolution Recognition on the Blogosphere", "comments": "IJDL 2015", "journal-ref": "International Journal on Digital Libraries 2015, Volume 15, Issue\n  2, pp 209-235", "doi": "10.1007/s00799-014-0135-x", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in technology and culture lead to changes in our language. These\nchanges create a gap between the language known by users and the language\nstored in digital archives. It affects user's possibility to firstly find\ncontent and secondly interpret that content. In previous work we introduced our\napproach for Named Entity Evolution Recognition~(NEER) in newspaper\ncollections. Lately, increasing efforts in Web preservation lead to increased\navailability of Web archives covering longer time spans. However, language on\nthe Web is more dynamic than in traditional media and many of the basic\nassumptions from the newspaper domain do not hold for Web data. In this paper\nwe discuss the limitations of existing methodology for NEER. We approach these\nby adapting an existing NEER method to work on noisy data like the Web and the\nBlogosphere in particular. We develop novel filters that reduce the noise and\nmake use of Semantic Web resources to obtain more information about terms. Our\nevaluation shows the potentials of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 22:07:57 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Tahmasebi", "Nina", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01287", "submitter": "Iacer Calixto", "authors": "Iacer Calixto and Qun Liu and Nick Campbell", "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "comments": "8 pages (11 including references), 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Multi-modal Neural Machine Translation model in which a\ndoubly-attentive decoder naturally incorporates spatial visual features\nobtained using pre-trained convolutional neural networks, bridging the gap\nbetween image description and translation. Our decoder learns to attend to\nsource-language words and parts of an image independently by means of two\nseparate attention mechanisms as it generates words in the target language. We\nfind that our model can efficiently exploit not just back-translated in-domain\nmulti-modal data but also large general-domain text-only MT corpora. We also\nreport state-of-the-art results on the Multi30k data set.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 13:46:53 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Calixto", "Iacer", ""], ["Liu", "Qun", ""], ["Campbell", "Nick", ""]]}, {"id": "1702.01360", "submitter": "Chunxi Liu", "authors": "Chunxi Liu, Jinyi Yang, Ming Sun, Santosh Kesiraju, Alena Rott, Lucas\n  Ondel, Pegah Ghahremani, Najim Dehak, Lukas Burget, Sanjeev Khudanpur", "title": "An Empirical Evaluation of Zero Resource Acoustic Unit Discovery", "comments": "5 pages, 1 figure; Accepted for publication at ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic unit discovery (AUD) is a process of automatically identifying a\ncategorical acoustic unit inventory from speech and producing corresponding\nacoustic unit tokenizations. AUD provides an important avenue for unsupervised\nacoustic model training in a zero resource setting where expert-provided\nlinguistic knowledge and transcribed speech are unavailable. Therefore, to\nfurther facilitate zero-resource AUD process, in this paper, we demonstrate\nacoustic feature representations can be significantly improved by (i)\nperforming linear discriminant analysis (LDA) in an unsupervised self-trained\nfashion, and (ii) leveraging resources of other languages through building a\nmultilingual bottleneck (BN) feature extractor to give effective cross-lingual\ngeneralization. Moreover, we perform comprehensive evaluations of AUD efficacy\non multiple downstream speech applications, and their correlated performance\nsuggests that AUD evaluations are feasible using different alternative language\nresources when only a subset of these evaluation resources can be available in\ntypical zero resource applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 02:22:31 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Liu", "Chunxi", ""], ["Yang", "Jinyi", ""], ["Sun", "Ming", ""], ["Kesiraju", "Santosh", ""], ["Rott", "Alena", ""], ["Ondel", "Lucas", ""], ["Ghahremani", "Pegah", ""], ["Dehak", "Najim", ""], ["Burget", "Lukas", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1702.01417", "submitter": "Jiaqi Mu", "authors": "Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "All-but-the-Top: Simple and Effective Postprocessing for Word\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-valued word representations have transformed NLP applications; popular\nexamples are word2vec and GloVe, recognized for their ability to capture\nlinguistic regularities. In this paper, we demonstrate a {\\em very simple}, and\nyet counter-intuitive, postprocessing technique -- eliminate the common mean\nvector and a few top dominating directions from the word vectors -- that\nrenders off-the-shelf representations {\\em even stronger}. The postprocessing\nis empirically validated on a variety of lexical-level intrinsic tasks (word\nsimilarity, concept categorization, word analogy) and sentence-level tasks\n(semantic textural similarity and { text classification}) on multiple datasets\nand with a variety of representation methods and hyperparameter choices in\nmultiple languages; in each case, the processed representations are\nconsistently better than the original ones.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 15:43:07 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:28:27 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1702.01466", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "Prepositions in Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prepositions are highly polysemous, and their variegated senses encode\nsignificant semantic information. In this paper we match each preposition's\ncomplement and attachment and their interplay crucially to the geometry of the\nword vectors to the left and right of the preposition. Extracting such features\nfrom the vast number of instances of each preposition and clustering them makes\nfor an efficient preposition sense disambigution (PSD) algorithm, which is\ncomparable to and better than state-of-the-art on two benchmark datasets. Our\nreliance on no external linguistic resource allows us to scale the PSD\nalgorithm to a large WikiCorpus and learn sense-specific preposition\nrepresentations -- which we show to encode semantic relations and paraphrasing\nof verb particle compounds, via simple vector operations.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 23:16:01 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Gong", "Hongyu", ""], ["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1702.01517", "submitter": "Zhongqing Wang", "authors": "Zhongqing Wang, Yue Zhang", "title": "Opinion Recommendation using Neural Memory Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present opinion recommendation, a novel task of jointly predicting a\ncustom review with a rating score that a certain user would give to a certain\nproduct or service, given existing reviews and rating scores to the product or\nservice by other users, and the reviews that the user has given to other\nproducts and services. A characteristic of opinion recommendation is the\nreliance of multiple data sources for multi-task joint learning, which is the\nstrength of neural models. We use a single neural network to model users and\nproducts, capturing their correlation and generating customised product\nrepresentations using a deep memory network, from which customised ratings and\nreviews are constructed jointly. Results show that our opinion recommendation\nsystem gives ratings that are closer to real user ratings on Yelp.com data\ncompared with Yelp's own ratings, and our methods give better results compared\nto several pipelines baselines using state-of-the-art sentiment rating and\nsummarization systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 07:29:01 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Wang", "Zhongqing", ""], ["Zhang", "Yue", ""]]}, {"id": "1702.01569", "submitter": "Jonathan Herzig", "authors": "Jonathan Herzig, Jonathan Berant", "title": "Neural Semantic Parsing over Multiple Knowledge-bases", "comments": "Accepted to ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-2098", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in developing semantic parsers is the paucity of\nstrong supervision in the form of language utterances annotated with logical\nform. In this paper, we propose to exploit structural regularities in language\nin different domains, and train semantic parsers over multiple knowledge-bases\n(KBs), while sharing information across datasets. We find that we can\nsubstantially improve parsing accuracy by training a single\nsequence-to-sequence model over multiple KBs, when providing an encoding of the\ndomain at decoding time. Our model achieves state-of-the-art performance on the\nOvernight dataset (containing eight domains), improves performance over a\nsingle KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the\nnumber of model parameters.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 11:22:15 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 08:34:47 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Herzig", "Jonathan", ""], ["Berant", "Jonathan", ""]]}, {"id": "1702.01587", "submitter": "Shrikant Malviya", "authors": "Omkar Dhariya, Shrikant Malviya and Uma Shanker Tiwary", "title": "A Hybrid Approach For Hindi-English Machine Translation", "comments": "31st International Conference on Information Networking (ICOIN-2017)", "journal-ref": null, "doi": "10.1109/ICOIN.2017.7899465", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an extended combined approach of phrase based statistical\nmachine translation (SMT), example based MT (EBMT) and rule based MT (RBMT) is\nproposed to develop a novel hybrid data driven MT system capable of\noutperforming the baseline SMT, EBMT and RBMT systems from which it is derived.\nIn short, the proposed hybrid MT process is guided by the rule based MT after\ngetting a set of partial candidate translations provided by EBMT and SMT\nsubsystems. Previous works have shown that EBMT systems are capable of\noutperforming the phrase-based SMT systems and RBMT approach has the strength\nof generating structurally and morphologically more accurate results. This\nhybrid approach increases the fluency, accuracy and grammatical precision which\nimprove the quality of a machine translation system. A comparison of the\nproposed hybrid machine translation (HTM) model with renowned translators i.e.\nGoogle, BING and Babylonian is also presented which shows that the proposed\nmodel works better on sentences with ambiguity as well as comprised of idioms\nthan others.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 12:19:14 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Dhariya", "Omkar", ""], ["Malviya", "Shrikant", ""], ["Tiwary", "Uma Shanker", ""]]}, {"id": "1702.01711", "submitter": "Rodrigo Agerri", "authors": "I\\~naki San Vicente, Rodrigo Agerri, German Rigau", "title": "Q-WordNet PPV: Simple, Robust and (almost) Unsupervised Generation of\n  Polarity Lexicons for Multiple Languages", "comments": "8 pages plus 2 pages of references", "journal-ref": "Proceedings of the 14th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2014), pages 88-97,\n  Gothenburg, Sweden, April 26-30 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a simple, robust and (almost) unsupervised\ndictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector)\nto automatically generate polarity lexicons. We show that qwn-ppv outperforms\nother automatically generated lexicons for the four extrinsic evaluations\npresented here. It also shows very competitive and robust results with respect\nto manually annotated ones. Results suggest that no single lexicon is best for\nevery task and dataset and that the intrinsic evaluation of polarity lexicons\nis not a good performance indicator on a Sentiment Analysis task. The qwn-ppv\nmethod allows to easily create quality polarity lexicons whenever no\ndomain-based annotated corpora are available for a given language.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:14:29 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Vicente", "I\u00f1aki San", ""], ["Agerri", "Rodrigo", ""], ["Rigau", "German", ""]]}, {"id": "1702.01714", "submitter": "Daniele Falavigna", "authors": "Daniele Falavigna, Marco Matassoni, Shahab Jalalvand, Matteo Negri,\n  Marco Turchi", "title": "DNN adaptation by automatic quality estimation of ASR hypotheses", "comments": "Computer Speech & Language December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to exploit the automatic Quality Estimation (QE) of\nASR hypotheses to perform the unsupervised adaptation of a deep neural network\nmodeling acoustic probabilities. Our hypothesis is that significant\nimprovements can be achieved by: i)automatically transcribing the evaluation\ndata we are currently trying to recognise, and ii) selecting from it a subset\nof \"good quality\" instances based on the word error rate (WER) scores predicted\nby a QE component. To validate this hypothesis, we run several experiments on\nthe evaluation data sets released for the CHiME-3 challenge. First, we operate\nin oracle conditions in which manual transcriptions of the evaluation data are\navailable, thus allowing us to compute the \"true\" sentence WER. In this\nscenario, we perform the adaptation with variable amounts of data, which are\ncharacterised by different levels of quality. Then, we move to realistic\nconditions in which the manual transcriptions of the evaluation data are not\navailable. In this case, the adaptation is performed on data selected according\nto the WER scores \"predicted\" by a QE component. Our results indicate that: i)\nQE predictions allow us to closely approximate the adaptation results obtained\nin oracle conditions, and ii) the overall ASR performance based on the proposed\nQE-driven adaptation method is significantly better than the strong, most\nrecent, CHiME-3 baseline.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:21:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Falavigna", "Daniele", ""], ["Matassoni", "Marco", ""], ["Jalalvand", "Shahab", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "1702.01776", "submitter": "Wenya Wang", "authors": "Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier", "title": "Multi-task memory networks for category-specific aspect and opinion\n  terms co-extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 19:55:51 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 06:39:37 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Wang", "Wenya", ""], ["Pan", "Sinno Jialin", ""], ["Dahlmeier", "Daniel", ""]]}, {"id": "1702.01802", "submitter": "Markus Freitag", "authors": "Markus Freitag, Yaser Al-Onaizan, Baskaran Sankaran", "title": "Ensemble Distillation for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation describes a method for training a student network to\nperform better by learning from a stronger teacher network. Translating a\nsentence with an Neural Machine Translation (NMT) engine is time expensive and\nhaving a smaller model speeds up this process. We demonstrate how to transfer\nthe translation quality of an ensemble and an oracle BLEU teacher network into\na single NMT system. Further, we present translation improvements from a\nteacher network that has the same architecture and dimensions of the student\nnetwork. As the training of the student model is still expensive, we introduce\na data filtering method based on the knowledge of the teacher model that not\nonly speeds up the training, but also leads to better translation quality. Our\ntechniques need no code change and can be easily reproduced with any NMT\narchitecture to speed up the decoding process.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 21:49:12 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 01:41:25 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Freitag", "Markus", ""], ["Al-Onaizan", "Yaser", ""], ["Sankaran", "Baskaran", ""]]}, {"id": "1702.01806", "submitter": "Markus Freitag", "authors": "Markus Freitag and Yaser Al-Onaizan", "title": "Beam Search Strategies for Neural Machine Translation", "comments": "First Workshop on Neural Machine Translation, 2017", "journal-ref": "Proceedings of the First Workshop on Neural Machine Translation,\n  2017", "doi": "10.18653/v1/W17-3207", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic concept in Neural Machine Translation (NMT) is to train a large\nNeural Network that maximizes the translation performance on a given parallel\ncorpus. NMT is then using a simple left-to-right beam-search decoder to\ngenerate new translations that approximately maximize the trained conditional\nprobability. The current beam search strategy generates the target sentence\nword by word from left-to- right while keeping a fixed amount of active\ncandidates at each time step. First, this simple search is less adaptive as it\nalso expands candidates whose scores are much worse than the current best.\nSecondly, it does not expand hypotheses if they are not within the best scoring\ncandidates, even if their scores are close to the best one. The latter one can\nbe avoided by increasing the beam size until no performance improvement can be\nobserved. While you can reach better performance, this has the draw- back of a\nslower decoding speed. In this paper, we concentrate on speeding up the decoder\nby applying a more flexible beam search strategy whose candidate size may vary\nat each time step depending on the candidate scores. We speed up the original\ndecoder by up to 43% for the two language pairs German-English and\nChinese-English without losing any translation quality.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:08:46 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 01:00:18 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Freitag", "Markus", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "1702.01815", "submitter": "Gemma Boleda", "authors": "Gemma Boleda, Sebastian Pad\\'o, Nghia The Pham, Marco Baroni", "title": "Living a discrete life in a continuous world: Reference with distributed\n  representations", "comments": "Accepted at IWCS 2017. Final version, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference is a crucial property of language that allows us to connect\nlinguistic expressions to the world. Modeling it requires handling both\ncontinuous and discrete aspects of meaning. Data-driven models excel at the\nformer, but struggle with the latter, and the reverse is true for symbolic\nmodels.\n  This paper (a) introduces a concrete referential task to test both aspects,\ncalled cross-modal entity tracking; (b) proposes a neural network architecture\nthat uses external memory to build an entity library inspired in the DRSs of\nDRT, with a mechanism to dynamically introduce new referents or add information\nto referents that are already in the library.\n  Our model shows promise: it beats traditional neural network architectures on\nthe task. However, it is still outperformed by Memory Networks, another model\nwith external memory.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:50:49 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 08:44:28 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Boleda", "Gemma", ""], ["Pad\u00f3", "Sebastian", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1702.01829", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji, Noah Smith", "title": "Neural Discourse Structure for Text Categorization", "comments": "ACL 2017 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that discourse structure, as defined by Rhetorical Structure Theory\nand provided by an existing discourse parser, benefits text categorization. Our\napproach uses a recursive neural network and a newly proposed attention\nmechanism to compute a representation of the text that focuses on salient\ncontent, from the perspective of both RST and the task. Experiments consider\nvariants of the approach and illustrate its strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 00:26:56 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 02:08:57 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Ji", "Yangfeng", ""], ["Smith", "Noah", ""]]}, {"id": "1702.01841", "submitter": "Roy Schwartz", "authors": "Roy Schwartz, Maarten Sap, Ioannis Konstas, Li Zilles, Yejin Choi and\n  Noah A. Smith", "title": "The Effect of Different Writing Tasks on Linguistic Style: A Case Study\n  of the ROC Story Cloze Task", "comments": "11 pages, CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A writer's style depends not just on personal traits but also on her intent\nand mental state. In this paper, we show how variants of the same writing task\ncan lead to measurable differences in writing style. We present a case study\nbased on the story cloze task (Mostafazadeh et al., 2016a), where annotators\nwere assigned similar writing tasks with different constraints: (1) writing an\nentire story, (2) adding a story ending for a given story context, and (3)\nadding an incoherent ending to a story. We show that a simple linear classifier\ninformed by stylistic features is able to successfully distinguish among the\nthree cases, without even looking at the story context. In addition, combining\nour stylistic features with language model predictions reaches state of the art\nperformance on the story cloze challenge. Our results demonstrate that\ndifferent task framings can dramatically affect the way people write.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 01:39:02 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 23:10:25 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 20:26:33 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Schwartz", "Roy", ""], ["Sap", "Maarten", ""], ["Konstas", "Ioannis", ""], ["Zilles", "Li", ""], ["Choi", "Yejin", ""], ["Smith", "Noah A.", ""]]}, {"id": "1702.01923", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Katharina Kann, Mo Yu, Hinrich Sch\\\"utze", "title": "Comparative Study of CNN and RNN for Natural Language Processing", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have revolutionized the field of natural language\nprocessing (NLP). Convolutional neural network (CNN) and recurrent neural\nnetwork (RNN), the two main types of DNN architectures, are widely explored to\nhandle various NLP tasks. CNN is supposed to be good at extracting\nposition-invariant features and RNN at modeling units in sequence. The state of\nthe art on many NLP tasks often switches due to the battle between CNNs and\nRNNs. This work is the first systematic comparison of CNN and RNN on a wide\nrange of representative NLP tasks, aiming to give basic guidance for DNN\nselection.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 08:33:35 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Yin", "Wenpeng", ""], ["Kann", "Katharina", ""], ["Yu", "Mo", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1702.01925", "submitter": "Ibrahim Abu El-khair Dr. <", "authors": "Ibrahim Abu El-Khair", "title": "Effects of Stop Words Elimination for Arabic Information Retrieval: A\n  Comparative Study", "comments": null, "journal-ref": "Abu El-Khair, Ibrahim. (2006). Effects of stop words elimination\n  for Arabic information retrieval: a comparative study. International Journal\n  of Computing & Information Sciences 4 (3), 119-133", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of three stop words lists for Arabic Information\nRetrieval---General Stoplist, Corpus-Based Stoplist, Combined Stoplist ---were\ninvestigated in this study. Three popular weighting schemes were examined: the\ninverse document frequency weight, probabilistic weighting, and statistical\nlanguage modelling. The Idea is to combine the statistical approaches with\nlinguistic approaches to reach an optimal performance, and compare their effect\non retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was\nused with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi\nretrieval system had the best overall performance of the three weighting\nalgorithms used in the study, stoplists improved retrieval effectiveness\nespecially when used with the BM25 weight. The overall performance of a general\nstoplist was better than the other two lists.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 08:49:58 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["El-Khair", "Ibrahim Abu", ""]]}, {"id": "1702.01932", "submitter": "Michel Galley", "authors": "Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan,\n  Jianfeng Gao, Wen-tau Yih, Michel Galley", "title": "A Knowledge-Grounded Neural Conversation Model", "comments": "AAAI 2018 (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models are capable of generating extremely natural sounding\nconversational interactions. Nevertheless, these models have yet to demonstrate\nthat they can incorporate content in the form of factual information or\nentity-grounded opinion that would enable them to serve in more task-oriented\nconversational applications. This paper presents a novel, fully data-driven,\nand knowledge-grounded neural conversation model aimed at producing more\ncontentful responses without slot filling. We generalize the widely-used\nSeq2Seq approach by conditioning responses on both conversation history and\nexternal \"facts\", allowing the model to be versatile and applicable in an\nopen-domain setting. Our approach yields significant improvements over a\ncompetitive Seq2Seq baseline. Human judges found that our outputs are\nsignificantly more informative.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 09:16:46 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 19:04:48 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Ghazvininejad", "Marjan", ""], ["Brockett", "Chris", ""], ["Chang", "Ming-Wei", ""], ["Dolan", "Bill", ""], ["Gao", "Jianfeng", ""], ["Yih", "Wen-tau", ""], ["Galley", "Michel", ""]]}, {"id": "1702.01944", "submitter": "I\\~naki San Vicente Roncal", "authors": "I\\~naki San Vicente, Xabier Saralegi, Rodrigo Agerri", "title": "EliXa: A Modular and Flexible ABSA Platform", "comments": "5 pages, conference", "journal-ref": "Proceedings of the 9th International Workshop on Semantic\n  Evaluation (SemEval 2015). Association for Computational Linguistics, June\n  2015, Denver, Colorado, pp.748-752", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a supervised Aspect Based Sentiment Analysis (ABSA)\nsystem. Our aim is to develop a modular platform which allows to easily conduct\nexperiments by replacing the modules or adding new features. We obtain the best\nresult in the Opinion Target Extraction (OTE) task (slot 2) using an\noff-the-shelf sequence labeler. The target polarity classification (slot 3) is\naddressed by means of a multiclass SVM algorithm which includes lexical based\nfeatures such as the polarity values obtained from domain and open polarity\nlexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and\nlaptop domain respectively, and performs second best in the out-of-domain\nhotel, achieving an accuracy of 0.80.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 10:18:07 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Vicente", "I\u00f1aki San", ""], ["Saralegi", "Xabier", ""], ["Agerri", "Rodrigo", ""]]}, {"id": "1702.01991", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Grzegorz Chrupa{\\l}a, Lieke Gelderloos, Afra Alishahi", "title": "Representations of language in a model of visually grounded speech\n  signal", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1057", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a visually grounded model of speech perception which projects\nspoken utterances and images to a joint semantic space. We use a multi-layer\nrecurrent highway network to model the temporal nature of spoken speech, and\nshow that it learns to extract both form and meaning-based linguistic knowledge\nfrom the input signal. We carry out an in-depth analysis of the representations\nused by different components of the trained model and show that encoding of\nsemantic aspects tends to become richer as we go up the hierarchy of layers,\nwhereas encoding of form-related aspects of the language input tends to\ninitially increase and then plateau or decrease.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:02:09 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 12:57:36 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 07:34:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""], ["Gelderloos", "Lieke", ""], ["Alishahi", "Afra", ""]]}, {"id": "1702.02052", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "Knowledge Adaptation: Teaching to Adapt", "comments": "11 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is crucial in many real-world applications where the\ndistribution of the training data differs from the distribution of the test\ndata. Previous Deep Learning-based approaches to domain adaptation need to be\ntrained jointly on source and target domain data and are therefore unappealing\nin scenarios where models need to be adapted to a large number of domains or\nwhere a domain is evolving, e.g. spam detection where attackers continuously\nchange their tactics.\n  To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge\nDistillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain\nadaptation scenario. We show how a student model achieves state-of-the-art\nresults on unsupervised domain adaptation from multiple sources on a standard\nsentiment analysis benchmark by taking into account the domain-specific\nexpertise of multiple teachers and the similarities between their domains.\n  When learning from a single teacher, using domain similarity to gauge\ntrustworthiness is inadequate. To this end, we propose a simple metric that\ncorrelates well with the teacher's accuracy in the target domain. We\ndemonstrate that incorporating high-confidence examples selected by this metric\nenables the student model to achieve state-of-the-art performance in the\nsingle-source scenario.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 14:59:45 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1702.02092", "submitter": "Tom Anderson", "authors": "Tom A. F. Anderson, David M. W. Powers", "title": "Characterisation of speech diversity using self-organising maps", "comments": "16th Speech Science and Technology Conference (SST2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report investigations into speaker classification of larger quantities of\nunlabelled speech data using small sets of manually phonemically annotated\nspeech. The Kohonen speech typewriter is a semi-supervised method comprised of\nself-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a\n2D array of cells that learn vector representations of the data based on\nneighbourhoods. In this paper, we report a method to evaluate pronunciation\nusing multilevel SOMs with /hVd/ single syllable utterances for the study of\nvowels, for Australian pronunciation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 11:18:06 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Anderson", "Tom A. F.", ""], ["Powers", "David M. W.", ""]]}, {"id": "1702.02098", "submitter": "Emma Strubell", "authors": "Emma Strubell, Patrick Verga, David Belanger, Andrew McCallum", "title": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions", "comments": "In Conference on Empirical Methods in Natural Language Processing\n  (EMNLP). Copenhagen, Denmark. September 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today when many practitioners run basic NLP on the entire web and\nlarge-volume traffic, faster methods are paramount to saving time and energy\ncosts. Recent advances in GPU hardware have led to the emergence of\nbi-directional LSTMs as a standard method for obtaining per-token vector\nrepresentations serving as input to labeling tasks such as NER (often followed\nby prediction in a linear-chain CRF). Though expressive and accurate, these\nmodels fail to fully exploit GPU parallelism, limiting their computational\nefficiency. This paper proposes a faster alternative to Bi-LSTMs for NER:\nIterated Dilated Convolutional Neural Networks (ID-CNNs), which have better\ncapacity than traditional CNNs for large context and structured prediction.\nUnlike LSTMs whose sequential processing on sentences of length N requires O(N)\ntime even in the face of parallelism, ID-CNNs permit fixed-depth convolutions\nto run in parallel across entire documents. We describe a distinct combination\nof network structure, parameter sharing and training procedures that enable\ndramatic 14-20x test-time speedups while retaining accuracy comparable to the\nBi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire\ndocument are even more accurate while maintaining 8x faster test time speeds.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 16:58:18 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 14:21:59 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 04:04:30 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Strubell", "Emma", ""], ["Verga", "Patrick", ""], ["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1702.02170", "submitter": "Stanislaw Jastrzebski", "authors": "Stanis{\\l}aw Jastrzebski, Damian Le\\'sniak, Wojciech Marian Czarnecki", "title": "How to evaluate word embeddings? On importance of data efficiency and\n  simple supervised tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maybe the single most important goal of representation learning is making\nsubsequent learning faster. Surprisingly, this fact is not well reflected in\nthe way embeddings are evaluated. In addition, recent practice in word\nembeddings points towards importance of learning specialized representations.\nWe argue that focus of word representation evaluation should reflect those\ntrends and shift towards evaluating what useful information is easily\naccessible. Specifically, we propose that evaluation should focus on data\nefficiency and simple supervised tasks, where the amount of available data is\nvaried and scores of a supervised model are reported for each subset (as\ncommonly done in transfer learning).\n  In order to illustrate significance of such analysis, a comprehensive\nevaluation of selected word embeddings is presented. Proposed approach yields a\nmore complete picture and brings new insight into performance characteristics,\nfor instance information about word similarity or analogy tends to be\nnon--linearly encoded in the embedding space, which questions the cosine-based,\nunsupervised, evaluation methods. All results and analysis scripts are\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:21:50 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Jastrzebski", "Stanis\u0142aw", ""], ["Le\u015bniak", "Damian", ""], ["Czarnecki", "Wojciech Marian", ""]]}, {"id": "1702.02171", "submitter": "Sewon Min", "authors": "Sewon Min, Minjoon Seo, Hannaneh Hajishirzi", "title": "Question Answering through Transfer Learning from Large Fine-grained\n  Supervision Data", "comments": "Published as a conference paper at ACL 2017 (short paper). Code\n  available at https://github.com/shmsw25/qa-transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the task of question answering (QA) can significantly benefit\nfrom the transfer learning of models trained on a different large, fine-grained\nQA dataset. We achieve the state of the art in two well-studied QA datasets,\nWikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique\nfrom SQuAD. For WikiQA, our model outperforms the previous best model by more\nthan 8%. We demonstrate that finer supervision provides better guidance for\nlearning lexical and syntactic information than coarser supervision, through\nquantitative results and visual analysis. We also show that a similar transfer\nlearning procedure achieves the state of the art on an entailment task.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:22:06 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 17:15:18 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 19:38:38 GMT"}, {"version": "v4", "created": "Fri, 21 Apr 2017 14:47:24 GMT"}, {"version": "v5", "created": "Thu, 31 May 2018 22:23:13 GMT"}, {"version": "v6", "created": "Thu, 21 Jun 2018 16:54:12 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Min", "Sewon", ""], ["Seo", "Minjoon", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1702.02206", "submitter": "Zhilin Yang", "authors": "Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William W. Cohen", "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets", "comments": "Accepted as a long paper at ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of semi-supervised question answering----utilizing\nunlabeled text to boost the performance of question answering models. We\npropose a novel training framework, the Generative Domain-Adaptive Nets. In\nthis framework, we train a generative model to generate questions based on the\nunlabeled text, and combine model-generated questions with human-generated\nquestions for training question answering models. We develop novel domain\nadaptation algorithms, based on reinforcement learning, to alleviate the\ndiscrepancy between the model-generated data distribution and the\nhuman-generated data distribution. Experiments show that our proposed framework\nobtains substantial improvement from unlabeled text.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 21:23:01 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 20:31:01 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Yang", "Zhilin", ""], ["Hu", "Junjie", ""], ["Salakhutdinov", "Ruslan", ""], ["Cohen", "William W.", ""]]}, {"id": "1702.02211", "submitter": "Tarek Sakakini", "authors": "Tarek Sakakini, Suma Bhat, Pramod Viswanath", "title": "Fixing the Infix: Unsupervised Discovery of Root-and-Pattern Morphology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised and language-agnostic method for learning\nroot-and-pattern morphology in Semitic languages. This form of morphology,\nabundant in Semitic languages, has not been handled in prior unsupervised\napproaches. We harness the syntactico-semantic information in distributed word\nrepresentations to solve the long standing problem of root-and-pattern\ndiscovery in Semitic languages. Moreover, we construct an unsupervised root\nextractor based on the learned rules. We prove the validity of learned rules\nacross Arabic, Hebrew, and Amharic, alongside showing that our root extractor\ncompares favorably with a widely used, carefully engineered root extractor:\nISRI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 21:43:21 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 00:16:35 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Sakakini", "Tarek", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1702.02212", "submitter": "Tarek Sakakini", "authors": "Tarek Sakakini, Suma Bhat, Pramod Viswanath", "title": "MORSE: Semantic-ally Drive-n MORpheme SEgment-er", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a novel framework for morpheme segmentation which\nuses the morpho-syntactic regularities preserved by word representations, in\naddition to orthographic features, to segment words into morphemes. This\nframework is the first to consider vocabulary-wide syntactico-semantic\ninformation for this task. We also analyze the deficiencies of available\nbenchmarking datasets and introduce our own dataset that was created on the\nbasis of compositionality. We validate our algorithm across datasets and\npresent state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 21:49:13 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 00:13:28 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 12:36:34 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sakakini", "Tarek", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1702.02261", "submitter": "Pramod Bharadwaj Chandrashekar", "authors": "Pramod Bharadwaj Chandrashekar (1), Arjun Magge (1), Abeed Sarker (2),\n  Graciela Gonzalez (2) ((1) Arizona State University, (2) University of\n  Pennsylvania)", "title": "Social media mining for identification and exploration of health-related\n  information from pregnant women", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widespread use of social media has led to the generation of substantial\namounts of information about individuals, including health-related information.\nSocial media provides the opportunity to study health-related information about\nselected population groups who may be of interest for a particular study. In\nthis paper, we explore the possibility of utilizing social media to perform\ntargeted data collection and analysis from a particular population group --\npregnant women. We hypothesize that we can use social media to identify cohorts\nof pregnant women and follow them over time to analyze crucial health-related\ninformation. To identify potentially pregnant women, we employ simple\nrule-based searches that attempt to detect pregnancy announcements with\nmoderate precision. To further filter out false positives and noise, we employ\na supervised classifier using a small number of hand-annotated data. We then\ncollect their posts over time to create longitudinal health timelines and\nattempt to divide the timelines into different pregnancy trimesters. Finally,\nwe assess the usefulness of the timelines by performing a preliminary analysis\nto estimate drug intake patterns of our cohort at different trimesters. Our\nrule-based cohort identification technique collected 53,820 users over thirty\nmonths from Twitter. Our pregnancy announcement classification technique\nachieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user\ntimelines. Analysis of the timelines revealed that pertinent health-related\ninformation, such as drug-intake and adverse reactions can be mined from the\ndata. Our approach to using user timelines in this fashion has produced very\nencouraging results and can be employed for other important tasks where\ncohorts, for which health-related information may not be available from other\nsources, are required to be followed over time to derive population-based\nestimates.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:19:57 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Chandrashekar", "Pramod Bharadwaj", ""], ["Magge", "Arjun", ""], ["Sarker", "Abeed", ""], ["Gonzalez", "Graciela", ""]]}, {"id": "1702.02265", "submitter": "Kazuma Hashimoto", "authors": "Kazuma Hashimoto, Yoshimasa Tsuruoka", "title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "comments": "Accepted as a full paper at the 2017 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel neural machine translation model which jointly\nlearns translation and source-side latent graph representations of sentences.\nUnlike existing pipelined approaches using syntactic parsers, our end-to-end\nmodel learns a latent graph parser as part of the encoder of an attention-based\nneural machine translation model, and thus the parser is optimized according to\nthe translation objective. In experiments, we first show that our model\ncompares favorably with state-of-the-art sequential and pipelined syntax-based\nNMT models. We also show that the performance of our model can be further\nimproved by pre-training it with a small amount of treebank annotations. Our\nfinal ensemble model significantly outperforms the previous best models on the\nstandard English-to-Japanese translation dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:32:23 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 01:47:16 GMT"}, {"version": "v3", "created": "Sun, 16 Apr 2017 22:46:08 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 14:52:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hashimoto", "Kazuma", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1702.02287", "submitter": "Baichuan Zhang", "authors": "Baichuan Zhang, Mohammad Al Hasan", "title": "Name Disambiguation in Anonymized Graphs using Network Embedding", "comments": "The 26th ACM International Conference on Information and Knowledge\n  Management (CIKM 2017) research track full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world, our DNA is unique but many people share names. This phenomenon\noften causes erroneous aggregation of documents of multiple persons who are\nnamesake of one another. Such mistakes deteriorate the performance of document\nretrieval, web search, and more seriously, cause improper attribution of credit\nor blame in digital forensic. To resolve this issue, the name disambiguation\ntask is designed which aims to partition the documents associated with a name\nreference such that each partition contains documents pertaining to a unique\nreal-life person. Existing solutions to this task substantially rely on feature\nengineering, such as biographical feature extraction, or construction of\nauxiliary features from Wikipedia. However, for many scenarios, such features\nmay be costly to obtain or unavailable due to the risk of privacy violation. In\nthis work, we propose a novel name disambiguation method. Our proposed method\nis non-intrusive of privacy because instead of using attributes pertaining to a\nreal-life person, our method leverages only relational data in the form of\nanonymized graphs. In the methodological aspect, the proposed method uses a\nnovel representation learning model to embed each document in a low dimensional\nvector space where name disambiguation can be solved by a hierarchical\nagglomerative clustering algorithm. Our experimental results demonstrate that\nthe proposed method is significantly better than the existing name\ndisambiguation methods working in a similar setting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:54:09 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 00:40:44 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 14:29:03 GMT"}, {"version": "v4", "created": "Sat, 9 Sep 2017 23:05:04 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Baichuan", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1702.02363", "submitter": "Bahadir Sahin", "authors": "H. Bahadir Sahin, Caglar Tirkaz, Eray Yildiz, Mustafa Tolga Eren, Ozan\n  Sonmez", "title": "Automatically Annotated Turkish Corpus for Named Entity Recognition and\n  Text Categorization using Large-Scale Gazetteers", "comments": "10 page, 1 figure, white paper, update: added correct download link\n  for dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC)\ndataset is a collection of automatically categorized and annotated sentences\nobtained from Wikipedia. We constructed large-scale gazetteers by using a graph\ncrawler algorithm to extract relevant entity and domain information from a\nsemantic knowledge base, Freebase. The constructed gazetteers contains\napproximately 300K entities with thousands of fine-grained entity types under\n77 different domains. Since automated processes are prone to ambiguity, we also\nintroduce two new content specific noise reduction methodologies. Moreover, we\nmap fine-grained entity types to the equivalent four coarse-grained types:\nperson, loc, org, misc. Eventually, we construct six different dataset versions\nand evaluate the quality of annotations by comparing ground truths from human\nannotators. We make these datasets publicly available to support studies on\nTurkish named-entity recognition (NER) and text categorization (TC).\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 10:45:23 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 08:35:12 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Sahin", "H. Bahadir", ""], ["Tirkaz", "Caglar", ""], ["Yildiz", "Eray", ""], ["Eren", "Mustafa Tolga", ""], ["Sonmez", "Ozan", ""]]}, {"id": "1702.02367", "submitter": "Claudio Greco", "authors": "Claudio Greco, Alessandro Suglia, Pierpaolo Basile, Gaetano Rossiello,\n  Giovanni Semeraro", "title": "Iterative Multi-document Neural Attention for Multiple Answer Prediction", "comments": "Paper accepted and presented at the Deep Understanding and Reasoning:\n  A challenge for Next-generation Intelligent Agents (URANIA) workshop, held in\n  the context of the AI*IA 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People have information needs of varying complexity, which can be solved by\nan intelligent agent able to answer questions formulated in a proper way,\neventually considering user context and preferences. In a scenario in which the\nuser profile can be considered as a question, intelligent agents able to answer\nquestions can be used to find the most relevant answers for a given user. In\nthis work we propose a novel model based on Artificial Neural Networks to\nanswer questions with multiple answers by exploiting multiple facts retrieved\nfrom a knowledge base. The model is evaluated on the factoid Question Answering\nand top-n recommendation tasks of the bAbI Movie Dialog dataset. After\nassessing the performance of the model on both tasks, we try to define the\nlong-term goal of a conversational recommender system able to interact using\nnatural language and to support users in their information seeking processes in\na personalized way.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 10:58:02 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Greco", "Claudio", ""], ["Suglia", "Alessandro", ""], ["Basile", "Pierpaolo", ""], ["Rossiello", "Gaetano", ""], ["Semeraro", "Giovanni", ""]]}, {"id": "1702.02390", "submitter": "Stanislau Semeniuta", "authors": "Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth", "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the effect of architectural choices on learning a\nVariational Autoencoder (VAE) for text generation. In contrast to the\npreviously introduced VAE model for text where both the encoder and decoder are\nRNNs, we propose a novel hybrid architecture that blends fully feed-forward\nconvolutional and deconvolutional components with a recurrent language model.\nOur architecture exhibits several attractive properties such as faster run time\nand convergence, ability to better handle long sequences and, more importantly,\nit helps to avoid some of the major difficulties posed by training VAE models\non textual data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 12:11:41 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Semeniuta", "Stanislau", ""], ["Severyn", "Aliaksei", ""], ["Barth", "Erhardt", ""]]}, {"id": "1702.02426", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "Data Selection Strategies for Multi-Domain Sentiment Analysis", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is important in sentiment analysis as sentiment-indicating\nwords vary between domains. Recently, multi-domain adaptation has become more\npervasive, but existing approaches train on all available source domains\nincluding dissimilar ones. However, the selection of appropriate training data\nis as important as the choice of algorithm. We undertake -- to our knowledge\nfor the first time -- an extensive study of domain similarity metrics in the\ncontext of sentiment analysis and propose novel representations, metrics, and a\nnew scope for data selection. We evaluate the proposed methods on two\nlarge-scale multi-domain adaptation settings on tweets and reviews and\ndemonstrate that they consistently outperform strong random and balanced\nbaselines, while our proposed selection strategy outperforms instance-level\nselection and yields the best score on a large reviews corpus.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 13:49:59 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1702.02429", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Kyunghyun Cho and Victor O.K. Li", "title": "Trainable Greedy Decoding for Neural Machine Translation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in neural machine translation has largely focused on two\naspects; neural network architectures and end-to-end learning algorithms. The\nproblem of decoding, however, has received relatively little attention from the\nresearch community. In this paper, we solely focus on the problem of decoding\ngiven a trained neural machine translation model. Instead of trying to build a\nnew decoding algorithm for any specific decoding objective, we propose the idea\nof trainable decoding algorithm in which we train a decoding algorithm to find\na translation that maximizes an arbitrary decoding objective. More\nspecifically, we design an actor that observes and manipulates the hidden state\nof the neural machine translation decoder and propose to train it using a\nvariant of deterministic policy gradient. We extensively evaluate the proposed\nalgorithm using four language pairs and two decoding objectives and show that\nwe can indeed train a trainable greedy decoder that generates a better\ntranslation (in terms of a target decoding objective) with minimal\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 13:56:16 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1702.02535", "submitter": "Ye Zhang", "authors": "Ye Zhang, Matthew Lease, Byron C. Wallace", "title": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application\n  to Text Categorization", "comments": "This paper is accepted by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental advantage of neural models for NLP is their ability to learn\nrepresentations from scratch. However, in practice this often means ignoring\nexisting external linguistic resources, e.g., WordNet or domain specific\nontologies such as the Unified Medical Language System (UMLS). We propose a\ngeneral, novel method for exploiting such resources via weight sharing. Prior\nwork on weight sharing in neural networks has considered it largely as a means\nof model compression. In contrast, we treat weight sharing as a flexible\nmechanism for incorporating prior knowledge into neural models. We show that\nthis approach consistently yields improved performance on classification tasks\ncompared to baseline strategies that do not exploit weight sharing.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:30:51 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 20:40:43 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 16:33:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Zhang", "Ye", ""], ["Lease", "Matthew", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1702.02540", "submitter": "William Murdoch", "authors": "W. James Murdoch and Arthur Szlam", "title": "Automatic Rule Extraction from Long Short Term Memory Networks", "comments": "ICLR 2017 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:46:37 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 22:20:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Murdoch", "W. James", ""], ["Szlam", "Arthur", ""]]}, {"id": "1702.02584", "submitter": "Lei Chen", "authors": "Lei Chen and Chong MIn Lee", "title": "Predicting Audience's Laughter Using Convolutional Neural Network", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the purpose of automatically evaluating speakers' humor usage, we build a\npresentation corpus containing humorous utterances based on TED talks. Compared\nto previous data resources supporting humor recognition research, ours has\nseveral advantages, including (a) both positive and negative instances coming\nfrom a homogeneous data set, (b) containing a large number of speakers, and (c)\nbeing open. Focusing on using lexical cues for humor recognition, we\nsystematically compare a newly emerging text classification method based on\nConvolutional Neural Networks (CNNs) with a well-established conventional\nmethod using linguistic knowledge. The advantages of the CNN method are both\ngetting higher detection accuracies and being able to learn essential features\nautomatically.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 19:10:53 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:42:31 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Chen", "Lei", ""], ["Lee", "Chong MIn", ""]]}, {"id": "1702.02640", "submitter": "Zhe Gan", "authors": "Zhe Gan, P. D. Singh, Ameet Joshi, Xiaodong He, Jianshu Chen, Jianfeng\n  Gao, Li Deng", "title": "Character-level Deep Conflation for Business Data Analytics", "comments": "Accepted for publication, at ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connecting different text attributes associated with the same entity\n(conflation) is important in business data analytics since it could help merge\ntwo different tables in a database to provide a more comprehensive profile of\nan entity. However, the conflation task is challenging because two text strings\nthat describe the same entity could be quite different from each other for\nreasons such as misspelling. It is therefore critical to develop a conflation\nmodel that is able to truly understand the semantic meaning of the strings and\nmatch them at the semantic level. To this end, we develop a character-level\ndeep conflation model that encodes the input text strings from character level\ninto finite dimension feature vectors, which are then used to compute the\ncosine similarity between the text strings. The model is trained in an\nend-to-end manner using back propagation and stochastic gradient descent to\nmaximize the likelihood of the correct association. Specifically, we propose\ntwo variants of the deep conflation model, based on long-short-term memory\n(LSTM) recurrent neural network (RNN) and convolutional neural network (CNN),\nrespectively. Both models perform well on a real-world business analytics\ndataset and significantly outperform the baseline bag-of-character (BoC) model.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 22:24:14 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Gan", "Zhe", ""], ["Singh", "P. D.", ""], ["Joshi", "Ameet", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1702.02736", "submitter": "Chieh-Yang Huang", "authors": "Chieh-Yang Huang, Ting-Hao (Kenneth) Huang and Lun-Wei Ku", "title": "Challenges in Providing Automatic Affective Feedback in Instant\n  Messaging Applications", "comments": "7 pages, 2017 AAAI Spring Symposia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instant messaging is one of the major channels of computer mediated\ncommunication. However, humans are known to be very limited in understanding\nothers' emotions via text-based communication. Aiming on introducing emotion\nsensing technologies to instant messaging, we developed EmotionPush, a system\nthat automatically detects the emotions of the messages end-users received on\nFacebook Messenger and provides colored cues on their smartphones accordingly.\nWe conducted a deployment study with 20 participants during a time span of two\nweeks. In this paper, we revealed five challenges, along with examples, that we\nobserved in our study based on both user's feedback and chat logs, including\n(i)the continuum of emotions, (ii)multi-user conversations, (iii)different\ndynamics between different users, (iv)misclassification of emotions and\n(v)unconventional content. We believe this discussion will benefit the future\nexploration of affective computing for instant messaging, and also shed light\non research of conversational emotion sensing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:00:09 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Huang", "Chieh-Yang", "", "Kenneth"], ["Ting-Hao", "", "", "Kenneth"], ["Huang", "", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1702.02737", "submitter": "Xuan-Son Vu", "authors": "Xuan-Son Vu, Seong-Bae Park", "title": "Mining User/Movie Preferred Features Based on Reviews for Video\n  Recommendation System", "comments": "The 2nd Workshop on Future Researches of Computer Science and\n  Engineering, Kyungpook National University, pp. 21-24, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an approach for mining user preferences and\nrecommendation based on reviews. There have been various studies worked on\nrecommendation problem. However, most of the studies beyond one aspect user\ngenerated- content such as user ratings, user feedback and so on to state user\npreferences. There is a prob- lem in one aspect mining is lacking for stating\nuser preferences. As a demonstration, in collaborative filter recommendation,\nwe try to figure out the preference trend of crowded users, then use that trend\nto predict current user preference. Therefore, there is a gap between real user\npreferences and the trend of the crowded people. Additionally, user preferences\ncan be addressed from mining user reviews since user often comment about\nvarious aspects of products. To solve this problem, we mainly focus on mining\nproduct aspects and user aspects inside user reviews to directly state user\npreferences. We also take into account Social Network Analysis for cold-start\nitem problem. With cold-start user problem, collaborative filter algorithm is\nemployed in our work. The framework is general enough to be applied to\ndifferent recommendation domains. Theoretically, our method would achieve a\nsignificant enhancement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:03:16 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Vu", "Xuan-Son", ""], ["Park", "Seong-Bae", ""]]}, {"id": "1702.03033", "submitter": "Markus Freitag", "authors": "Markus Freitag, Jan-Thorsten Peter, Stephan Peitz, Minwei Feng and\n  Hermann Ney", "title": "Local System Voting Feature for Machine Translation System Combination", "comments": "published WMT 2015", "journal-ref": "Proceedings of the Tenth Workshop on Statistical Machine\n  Translation (WMT), 2015", "doi": "10.18653/v1/W15-3060", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we enhance the traditional confusion network system\ncombination approach with an additional model trained by a neural network. This\nwork is motivated by the fact that the commonly used binary system voting\nmodels only assign each input system a global weight which is responsible for\nthe global impact of each input system on all translations. This prevents\nindividual systems with low system weights from having influence on the system\ncombination output, although in some situations this could be helpful. Further,\nwords which have only been seen by one or few systems rarely have a chance of\nbeing present in the combined output. We train a local system voting model by a\nneural network which is based on the words themselves and the combinatorial\noccurrences of the different system outputs. This gives system combination the\noption to prefer other systems at different word positions even for the same\nsentence.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 01:27:00 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Freitag", "Markus", ""], ["Peter", "Jan-Thorsten", ""], ["Peitz", "Stephan", ""], ["Feng", "Minwei", ""], ["Ney", "Hermann", ""]]}, {"id": "1702.03082", "submitter": "Laurent Besacier", "authors": "J. Ferrero, F. Agnes, L. Besacier, D. Schwab", "title": "UsingWord Embedding for Cross-Language Plagiarism Detection", "comments": "Accepted to EACL 2017 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes to use distributed representation of words (word\nembeddings) in cross-language textual similarity detection. The main\ncontributions of this paper are the following: (a) we introduce new\ncross-language similarity detection methods based on distributed representation\nof words; (b) we combine the different methods proposed to verify their\ncomplementarity and finally obtain an overall F1 score of 89.15% for\nEnglish-French similarity detection at chunk level (88.5% at sentence level) on\na very challenging corpus.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 07:22:08 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Ferrero", "J.", ""], ["Agnes", "F.", ""], ["Besacier", "L.", ""], ["Schwab", "D.", ""]]}, {"id": "1702.03121", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Sayeed and Manfred\n  Pinkal", "title": "Modeling Semantic Expectation: Using Script Knowledge for Referent\n  Prediction", "comments": "14 pages, published at TACL, 2017, Volume-5, Pg 31-44, 2017", "journal-ref": "Transactions of ACL, Volume-5, Pg 31-44 (2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in psycholinguistics has provided increasing evidence that\nhumans predict upcoming content. Prediction also affects perception and might\nbe a key to robustness in human language processing. In this paper, we\ninvestigate the factors that affect human prediction by building a\ncomputational model that can predict upcoming discourse referents based on\nlinguistic knowledge alone vs. linguistic knowledge jointly with common-sense\nknowledge in the form of scripts. We find that script knowledge significantly\nimproves model estimates of human predictions. In a second study, we test the\nhighly controversial hypothesis that predictability influences referring\nexpression type but do not find evidence for such an effect.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 10:31:57 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""], ["Demberg", "Vera", ""], ["Sayeed", "Asad", ""], ["Pinkal", "Manfred", ""]]}, {"id": "1702.03196", "submitter": "Siva Reddy", "authors": "Siva Reddy, Oscar T\\\"ackstr\\\"om, Slav Petrov, Mark Steedman, Mirella\n  Lapata", "title": "Universal Semantic Parsing", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Universal Dependencies (UD) offer a uniform cross-lingual syntactic\nrepresentation, with the aim of advancing multilingual applications. Recent\nwork shows that semantic parsing can be accomplished by transforming syntactic\ndependencies to logical forms. However, this work is limited to English, and\ncannot process dependency graphs, which allow handling complex phenomena such\nas control. In this work, we introduce UDepLambda, a semantic interface for UD,\nwhich maps natural language to logical forms in an almost language-independent\nfashion and can process dependency graphs. We perform experiments on question\nanswering against Freebase and provide German and Spanish translations of the\nWebQuestions and GraphQuestions datasets to facilitate multilingual evaluation.\nResults show that UDepLambda outperforms strong baselines across languages and\ndatasets. For English, it achieves a 4.9 F1 point improvement over the\nstate-of-the-art on GraphQuestions. Our code and data can be downloaded at\nhttps://github.com/sivareddyg/udeplambda.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 14:57:26 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 16:31:42 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 20:43:17 GMT"}, {"version": "v4", "created": "Mon, 28 Aug 2017 23:59:14 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Reddy", "Siva", ""], ["T\u00e4ckstr\u00f6m", "Oscar", ""], ["Petrov", "Slav", ""], ["Steedman", "Mark", ""], ["Lapata", "Mirella", ""]]}, {"id": "1702.03197", "submitter": "Abdulaziz Alayba", "authors": "Abdulaziz M. Alayba, Vasile Palade, Matthew England and Rahat Iqbal", "title": "Arabic Language Sentiment Analysis on Health Services", "comments": "Authors accepted version of submission for ASAR 2017", "journal-ref": "Proc. 1st International Workshop on Arabic Script Analysis and\n  Recognition (ASAR '17), pp. 114-118. IEEE, 2017", "doi": "10.1109/ASAR.2017.8067771", "report-no": null, "categories": "cs.CL cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media network phenomenon leads to a massive amount of valuable\ndata that is available online and easy to access. Many users share images,\nvideos, comments, reviews, news and opinions on different social networks\nsites, with Twitter being one of the most popular ones. Data collected from\nTwitter is highly unstructured, and extracting useful information from tweets\nis a challenging task. Twitter has a huge number of Arabic users who mostly\npost and write their tweets using the Arabic language. While there has been a\nlot of research on sentiment analysis in English, the amount of researches and\ndatasets in Arabic language is limited. This paper introduces an Arabic\nlanguage dataset which is about opinions on health services and has been\ncollected from Twitter. The paper will first detail the process of collecting\nthe data from Twitter and also the process of filtering, pre-processing and\nannotating the Arabic text in order to build a big sentiment analysis dataset\nin Arabic. Several Machine Learning algorithms (Naive Bayes, Support Vector\nMachine and Logistic Regression) alongside Deep and Convolutional Neural\nNetworks were utilized in our experiments of sentiment analysis on our health\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 14:59:14 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Alayba", "Abdulaziz M.", ""], ["Palade", "Vasile", ""], ["England", "Matthew", ""], ["Iqbal", "Rahat", ""]]}, {"id": "1702.03274", "submitter": "Jason Williams", "authors": "Jason D. Williams, Kavosh Asadi, Geoffrey Zweig", "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control\n  with supervised and reinforcement learning", "comments": "Accepted as a long paper for the 55th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning of recurrent neural networks (RNNs) is an attractive\nsolution for dialog systems; however, current techniques are data-intensive and\nrequire thousands of dialogs to learn simple behaviors. We introduce Hybrid\nCode Networks (HCNs), which combine an RNN with domain-specific knowledge\nencoded as software and system action templates. Compared to existing\nend-to-end approaches, HCNs considerably reduce the amount of training data\nrequired, while retaining the key benefit of inferring a latent representation\nof dialog state. In addition, HCNs can be optimized with supervised learning,\nreinforcement learning, or a mixture of both. HCNs attain state-of-the-art\nperformance on the bAbI dialog dataset, and outperform two commercially\ndeployed customer-facing dialog systems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 18:24:13 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 14:39:27 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Williams", "Jason D.", ""], ["Asadi", "Kavosh", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "1702.03305", "submitter": "Federico Fancellu", "authors": "Federico Fancellu, Siva Reddy, Adam Lopez, Bonnie Webber", "title": "Universal Dependencies to Logical Forms with Negation Scope", "comments": "This a draft version of the paper. We welcome any comments you may\n  have regarding the content and presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many language technology applications would benefit from the ability to\nrepresent negation and its scope on top of widely-used linguistic resources. In\nthis paper, we investigate the possibility of obtaining a first-order logic\nrepresentation with negation scope marked using Universal Dependencies. To do\nso, we enhance UDepLambda, a framework that converts dependency graphs to\nlogical forms. The resulting UDepLambda$\\lnot$ is able to handle phenomena\nrelated to scope by means of an higher-order type theory, relevant not only to\nnegation but also to universal quantification and other complex semantic\nphenomena. The initial conversion we did for English is promising, in that one\ncan represent the scope of negation also in the presence of more complex\nphenomena such as universal quantifiers.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 19:16:40 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Fancellu", "Federico", ""], ["Reddy", "Siva", ""], ["Lopez", "Adam", ""], ["Webber", "Bonnie", ""]]}, {"id": "1702.03342", "submitter": "Walid Shalaby PhD", "authors": "Walid Shalaby, Wlodek Zadrozny", "title": "Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-018-1321-8", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit concept space models have proven efficacy for text representation in\nmany natural language and text mining applications. The idea is to embed\ntextual structures into a semantic space of concepts which captures the main\nideas, objects, and the characteristics of these structures. The so called Bag\nof Concepts (BoC) representation suffers from data sparsity causing low\nsimilarity scores between similar texts due to low concept overlap. To address\nthis problem, we propose two neural embedding models to learn continuous\nconcept vectors. Once they are learned, we propose an efficient vector\naggregation method to generate fully continuous BoC representations. We\nevaluate our concept embedding models on three tasks: 1) measuring entity\nsemantic relatedness and ranking where we achieve 1.6% improvement in\ncorrelation scores, 2) dataless concept categorization where we achieve\nstate-of-the-art performance and reduce the categorization error rate by more\nthan 5% compared to five prior word and entity embedding models, and 3)\ndataless document classification where our models outperform the sparse BoC\nrepresentations. In addition, by exploiting our efficient linear time vector\naggregation method, we achieve better accuracy scores with much less concept\ndimensions compared to previous BoC densification methods which operate in\npolynomial time and require hundreds of dimensions in the BoC representation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 22:44:59 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 02:25:15 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Shalaby", "Walid", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1702.03402", "submitter": "Mohamed Bouaziz", "authors": "Mohamed Bouaziz, Mohamed Morchid, Richard Dufour, Georges Linar\\`es,\n  Renato De Mori", "title": "Parallel Long Short-Term Memory for Multi-stream Classification", "comments": "2016 IEEE Workshop on Spoken Language Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning methods have provided a broad spectrum of original\nand efficient algorithms based on Deep Neural Networks (DNN) to automatically\npredict an outcome with respect to a sequence of inputs. Recurrent hidden cells\nallow these DNN-based models to manage long-term dependencies such as Recurrent\nNeural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these\nRNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM)\ndirections. But most of the information available nowadays is from multistreams\nor multimedia documents, and require RNNs to process these information\nsynchronously during the training. This paper presents an original LSTM-based\narchitecture, named Parallel LSTM (PLSTM), that carries out multiple parallel\nsynchronized input sequences in order to predict a common output. The proposed\nPLSTM method could be used for parallel sequence classification purposes. The\nPLSTM approach is evaluated on an automatic telecast genre sequences\nclassification task and compared with different state-of-the-art architectures.\nResults show that the proposed PLSTM method outperforms the baseline n-gram\nmodels as well as the state-of-the-art LSTM approach.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 09:50:40 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Bouaziz", "Mohamed", ""], ["Morchid", "Mohamed", ""], ["Dufour", "Richard", ""], ["Linar\u00e8s", "Georges", ""], ["De Mori", "Renato", ""]]}, {"id": "1702.03470", "submitter": "Ehsan Sherkat", "authors": "Ehsan Sherkat, Evangelos Milios", "title": "Vector Embedding of Wikipedia Concepts and Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep learning for different machine learning tasks such as image\nclassification and word embedding has recently gained many attentions. Its\nappealing performance reported across specific Natural Language Processing\n(NLP) tasks in comparison with other approaches is the reason for its\npopularity. Word embedding is the task of mapping words or phrases to a low\ndimensional numerical vector. In this paper, we use deep learning to embed\nWikipedia Concepts and Entities. The English version of Wikipedia contains more\nthan five million pages, which suggest its capability to cover many English\nEntities, Phrases, and Concepts. Each Wikipedia page is considered as a\nconcept. Some concepts correspond to entities, such as a person's name, an\norganization or a place. Contrary to word embedding, Wikipedia Concepts\nEmbedding is not ambiguous, so there are different vectors for concepts with\nsimilar surface form but different mentions. We proposed several approaches and\nevaluated their performance based on Concept Analogy and Concept Similarity\ntasks. The results show that proposed approaches have the performance\ncomparable and in some cases even higher than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 00:23:04 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Sherkat", "Ehsan", ""], ["Milios", "Evangelos", ""]]}, {"id": "1702.03525", "submitter": "Akiko Eriguchi", "authors": "Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho", "title": "Learning to Parse and Translate Improves Neural Machine Translation", "comments": "Accepted as a short paper at the 55th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been relatively little attention to incorporating linguistic prior\nto neural machine translation. Much of the previous work was further\nconstrained to considering linguistic prior on the source side. In this paper,\nwe propose a hybrid model, called NMT+RNNG, that learns to parse and translate\nby combining the recurrent neural network grammar into the attention-based\nneural machine translation. Our approach encourages the neural machine\ntranslation model to incorporate linguistic prior during training, and lets it\ntranslate on its own afterward. Extensive experiments with four language pairs\nshow the effectiveness of the proposed NMT+RNNG.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 13:19:03 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 16:52:03 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Eriguchi", "Akiko", ""], ["Tsuruoka", "Yoshimasa", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1702.03654", "submitter": "Bahadir Sahin", "authors": "Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin, Mustafa Tolga Eren, Ozan\n  Sonmez", "title": "A Morphology-aware Network for Morphological Disambiguation", "comments": "6 pages, 1 figure, Thirtieth AAAI Conference on Artificial\n  Intelligence. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agglutinative languages such as Turkish, Finnish and Hungarian require\nmorphological disambiguation before further processing due to the complex\nmorphology of words. A morphological disambiguator is used to select the\ncorrect morphological analysis of a word. Morphological disambiguation is\nimportant because it generally is one of the first steps of natural language\nprocessing and its performance affects subsequent analyses. In this paper, we\npropose a system that uses deep learning techniques for morphological\ndisambiguation. Many of the state-of-the-art results in computer vision, speech\nrecognition and natural language processing have been obtained through deep\nlearning models. However, applying deep learning techniques to morphologically\nrich languages is not well studied. In this work, while we focus on Turkish\nmorphological disambiguation we also present results for French and German in\norder to show that the proposed architecture achieves high accuracy with no\nlanguage-specific feature engineering or additional resource. In the\nexperiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation\naccuracy among the ambiguous words for Turkish, German and French respectively.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 07:08:28 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Yildiz", "Eray", ""], ["Tirkaz", "Caglar", ""], ["Sahin", "H. Bahadir", ""], ["Eren", "Mustafa Tolga", ""], ["Sonmez", "Ozan", ""]]}, {"id": "1702.03706", "submitter": "Daniele Bonadiman", "authors": "Daniele Bonadiman, Antonio Uva, Alessandro Moschitti", "title": "Multitask Learning with Deep Neural Networks for Community Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we developed a deep neural network (DNN) that learns to solve\nsimultaneously the three tasks of the cQA challenge proposed by the\nSemEval-2016 Task 3, i.e., question-comment similarity, question-question\nsimilarity and new question-comment similarity. The latter is the main task,\nwhich can exploit the previous two for achieving better results. Our DNN is\ntrained jointly on all the three cQA tasks and learns to encode questions and\ncomments into a single vector representation shared across the multiple tasks.\nThe results on the official challenge test set show that our approach produces\nhigher accuracy and faster convergence rates than the individual neural\nnetworks. Additionally, our method, which does not use any manual feature\nengineering, approaches the state of the art established with methods that make\nheavy use of it.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 10:24:55 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Bonadiman", "Daniele", ""], ["Uva", "Antonio", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "1702.03814", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Wael Hamza, Radu Florian", "title": "Bilateral Multi-Perspective Matching for Natural Language Sentences", "comments": "To appear in Proceedings of IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language sentence matching is a fundamental technology for a variety\nof tasks. Previous approaches either match sentences from a single direction or\nonly apply single granular (word-by-word or sentence-by-sentence) matching. In\nthis work, we propose a bilateral multi-perspective matching (BiMPM) model\nunder the \"matching-aggregation\" framework. Given two sentences $P$ and $Q$,\nour model first encodes them with a BiLSTM encoder. Next, we match the two\nencoded sentences in two directions $P \\rightarrow Q$ and $P \\leftarrow Q$. In\neach matching direction, each time step of one sentence is matched against all\ntime-steps of the other sentence from multiple perspectives. Then, another\nBiLSTM layer is utilized to aggregate the matching results into a fix-length\nmatching vector. Finally, based on the matching vector, the decision is made\nthrough a fully connected layer. We evaluate our model on three tasks:\nparaphrase identification, natural language inference and answer sentence\nselection. Experimental results on standard benchmark datasets show that our\nmodel achieves the state-of-the-art performance on all tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 15:26:27 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 14:03:53 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 19:45:07 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wang", "Zhiguo", ""], ["Hamza", "Wael", ""], ["Florian", "Radu", ""]]}, {"id": "1702.03856", "submitter": "Sameer Bansal", "authors": "Sameer Bansal, Herman Kamper, Adam Lopez and Sharon Goldwater", "title": "Towards speech-to-text translation without speech recognition", "comments": "To appear in EACL 2017 (short papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of translating speech to text in low-resource\nscenarios where neither automatic speech recognition (ASR) nor machine\ntranslation (MT) are available, but we have training data in the form of audio\npaired with text translations. We present the first system for this problem\napplied to a realistic multi-speaker dataset, the CALLHOME Spanish-English\nspeech translation corpus. Our approach uses unsupervised term discovery (UTD)\nto cluster repeated patterns in the audio, creating a pseudotext, which we pair\nwith translations to create a parallel text and train a simple bag-of-words MT\nmodel. We identify the challenges faced by the system, finding that the\ndifficulty of cross-speaker UTD results in low recall, but that our system is\nstill able to correctly translate some content words in test data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 16:30:23 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Bansal", "Sameer", ""], ["Kamper", "Herman", ""], ["Lopez", "Adam", ""], ["Goldwater", "Sharon", ""]]}, {"id": "1702.03859", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, David H. P. Turban, Steven Hamblin and Nils Y.\n  Hammerla", "title": "Offline bilingual word vectors, orthogonal transformations and the\n  inverted softmax", "comments": "Accepted to conference track at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually bilingual word vectors are trained \"online\". Mikolov et al. showed\nthey can also be found \"offline\", whereby two pre-trained embeddings are\naligned with a linear transformation, using dictionaries compiled from expert\nknowledge. In this work, we prove that the linear transformation between two\nspaces should be orthogonal. This transformation can be obtained using the\nsingular value decomposition. We introduce a novel \"inverted softmax\" for\nidentifying translation pairs, with which we improve the precision @1 of\nMikolov's original mapping from 34% to 43%, when translating a test set\ncomposed of both common and rare English words into Italian. Orthogonal\ntransformations are more robust to noise, enabling us to learn the\ntransformation without expert bilingual signal by constructing a\n\"pseudo-dictionary\" from the identical character strings which appear in both\nlanguages, achieving 40% precision on the same test set. Finally, we extend our\nmethod to retrieve the true translations of English sentences from a corpus of\n200k Italian sentences with a precision @1 of 68%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 16:31:06 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Smith", "Samuel L.", ""], ["Turban", "David H. P.", ""], ["Hamblin", "Steven", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "1702.03964", "submitter": "Johannes Bjerva", "authors": "Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik\n  van Noord, Pierre Ludmann, Duc-Duy Nguyen, Johan Bos", "title": "The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations\n  Annotated with Compositional Meaning Representations", "comments": "To appear at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Parallel Meaning Bank is a corpus of translations annotated with shared,\nformal meaning representations comprising over 11 million words divided over\nfour languages (English, German, Italian, and Dutch). Our approach is based on\ncross-lingual projection: automatically produced (and manually corrected)\nsemantic annotations for English sentences are mapped onto their word-aligned\ntranslations, assuming that the translations are meaning-preserving. The\nsemantic annotation consists of five main steps: (i) segmentation of the text\nin sentences and lexical items; (ii) syntactic parsing with Combinatory\nCategorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and\n(v) compositional semantic analysis based on Discourse Representation Theory.\nThese steps are performed using statistical models trained in a semi-supervised\nmanner. The employed annotation models are all language-neutral. Our first\nresults are promising.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 19:52:02 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Abzianidze", "Lasha", ""], ["Bjerva", "Johannes", ""], ["Evang", "Kilian", ""], ["Haagsma", "Hessel", ""], ["van Noord", "Rik", ""], ["Ludmann", "Pierre", ""], ["Nguyen", "Duc-Duy", ""], ["Bos", "Johan", ""]]}, {"id": "1702.04066", "submitter": "Courtney Napoles", "authors": "Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault", "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "comments": "To appear in EACL 2017 (short papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for\ndeveloping and evaluating grammatical error correction (GEC). Unlike other\ncorpora, it represents a broad range of language proficiency levels and uses\nholistic fluency edits to not only correct grammatical errors but also make the\noriginal text more native sounding. We describe the types of corrections made\nand benchmark four leading GEC systems on this corpus, identifying specific\nareas in which they do well and how they can improve. JFLEG fulfills the need\nfor a new gold standard to properly assess the current state of GEC.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 03:47:34 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Napoles", "Courtney", ""], ["Sakaguchi", "Keisuke", ""], ["Tetreault", "Joel", ""]]}, {"id": "1702.04241", "submitter": "Alok Pal", "authors": "Alok Ranjan Pal and Diganta Saha", "title": "Detection of Slang Words in e-Data using semi-Supervised Learning", "comments": "13 pages in International Journal of Artificial Intelligence &\n  Applications (IJAIA), Vol. 4, No. 5, September 2013", "journal-ref": null, "doi": "10.5121/ijaia.2013.4504", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed algorithmic approach deals with finding the sense of a word in\nan electronic data. Now a day,in different communication mediums like internet,\nmobile services etc. people use few words, which are slang in nature. This\napproach detects those abusive words using supervised learning procedure. But\nin the real life scenario, the slang words are not used in complete word forms\nalways. Most of the times, those words are used in different abbreviated forms\nlike sounds alike forms, taboo morphemes etc. This proposed approach can detect\nthose abbreviated forms also using semi supervised learning procedure. Using\nthe synset and concept analysis of the text, the probability of a suspicious\nword to be a slang word is also evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 11:38:42 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Pal", "Alok Ranjan", ""], ["Saha", "Diganta", ""]]}, {"id": "1702.04333", "submitter": "Angel Castro Martinez", "authors": "Angel Mario Castro Martinez, Sri Harish Mallidi, Bernd T. Meyer", "title": "On the Relevance of Auditory-Based Gabor Features for Deep Learning in\n  Automatic Speech Recognition", "comments": "accepted to Computer Speech & Language", "journal-ref": null, "doi": "10.1016/j.csl.2017.02.006", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies support the idea of merging auditory-based Gabor features\nwith deep learning architectures to achieve robust automatic speech\nrecognition, however, the cause behind the gain of such combination is still\nunknown. We believe these representations provide the deep learning decoder\nwith more discriminable cues. Our aim with this paper is to validate this\nhypothesis by performing experiments with three different recognition tasks\n(Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the\ninformation encoded by Gabor filterbank features. Additionally, to identify the\ncontribution of low, medium and high temporal modulation frequencies subsets of\nthe Gabor filterbank were used as features (dubbed LTM, MTM and HTM\nrespectively). With temporal modulation frequencies between 16 and 25 Hz, HTM\nconsistently outperformed the remaining ones in every condition, highlighting\nthe robustness of these representations against channel distortions, low\nsignal-to-noise ratios and acoustically challenging real-life scenarios with\nrelative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To\nexplain the results, a measure of similarity between phoneme classes from DNN\nactivations is proposed and linked to their acoustic properties. We find this\nmeasure to be consistent with the observed error rates and highlight specific\ndifferences on phoneme level to pinpoint the benefit of the proposed features.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 18:46:47 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Martinez", "Angel Mario Castro", ""], ["Mallidi", "Sri Harish", ""], ["Meyer", "Bernd T.", ""]]}, {"id": "1702.04372", "submitter": "Antonios Anastasopoulos", "authors": "Antonios Anastasopoulos and David Chiang", "title": "A case study on using speech-to-translation alignments for language\n  documentation", "comments": "to be presented at ComputEL-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many low-resource or endangered languages, spoken language resources are\nmore likely to be annotated with translations than with transcriptions. Recent\nwork exploits such annotations to produce speech-to-translation alignments,\nwithout access to any text transcriptions. We investigate whether providing\nsuch information can aid in producing better (mismatched) crowdsourced\ntranscriptions, which in turn could be valuable for training speech recognition\nsystems, and show that they can indeed be beneficial through a small-scale case\nstudy as a proof-of-concept. We also present a simple phonetically aware string\naveraging technique that produces transcriptions of higher quality.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 20:06:15 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Anastasopoulos", "Antonios", ""], ["Chiang", "David", ""]]}, {"id": "1702.04457", "submitter": "Jingbo Shang", "authors": "Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei\n  Han", "title": "Automated Phrase Mining from Massive Text Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the fundamental tasks in text analysis, phrase mining aims at\nextracting quality phrases from a text corpus. Phrase mining is important in\nvarious tasks such as information extraction/retrieval, taxonomy construction,\nand topic modeling. Most existing methods rely on complex, trained linguistic\nanalyzers, and thus likely have unsatisfactory performance on text corpora of\nnew domains and genres without extra but expensive adaption. Recently, a few\ndata-driven methods have been developed successfully for extraction of phrases\nfrom massive domain-specific text. However, none of the state-of-the-art models\nis fully automated because they require human experts for designing rules or\nlabeling phrases.\n  Since one can easily obtain many quality phrases from public knowledge bases\nto a scale that is much larger than that produced by human experts, in this\npaper, we propose a novel framework for automated phrase mining, AutoPhrase,\nwhich leverages this large amount of high-quality phrases in an effective way\nand achieves better performance compared to limited human labeled phrases. In\naddition, we develop a POS-guided phrasal segmentation model, which\nincorporates the shallow syntactic information in part-of-speech (POS) tags to\nfurther enhance the performance, when a POS tagger is available. Note that,\nAutoPhrase can support any language as long as a general knowledge base (e.g.,\nWikipedia) in that language is available, while benefiting from, but not\nrequiring, a POS tagger. Compared to the state-of-the-art methods, the new\nmethod has shown significant improvements in effectiveness on five real-world\ndatasets across different domains and languages.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 03:35:03 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 19:33:41 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Shang", "Jingbo", ""], ["Liu", "Jialu", ""], ["Jiang", "Meng", ""], ["Ren", "Xiang", ""], ["Voss", "Clare R", ""], ["Han", "Jiawei", ""]]}, {"id": "1702.04488", "submitter": "Jingjing Xu", "authors": "Jingjing Xu and Xu Sun", "title": "Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a\n  Novel Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown effectiveness in using neural networks for Chinese\nword segmentation. However, these models rely on large-scale data and are less\neffective for low-resource datasets because of insufficient training data. We\npropose a transfer learning method to improve low-resource word segmentation by\nleveraging high-resource corpora. First, we train a teacher model on\nhigh-resource corpora and then use the learned knowledge to initialize a\nstudent model. Second, a weighted data similarity method is proposed to train\nthe student model on low-resource data. Experiment results show that our work\nsignificantly improves the performance on low-resource datasets: 2.3% and 1.5%\nF-score on PKU and CTB datasets. Furthermore, this paper achieves\nstate-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 07:37:55 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 06:16:09 GMT"}, {"version": "v3", "created": "Sun, 7 May 2017 12:53:13 GMT"}, {"version": "v4", "created": "Wed, 17 May 2017 01:52:45 GMT"}, {"version": "v5", "created": "Thu, 14 Sep 2017 11:10:13 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Xu", "Jingjing", ""], ["Sun", "Xu", ""]]}, {"id": "1702.04510", "submitter": "Christian Hadiwinoto", "authors": "Christian Hadiwinoto, Hwee Tou Ng", "title": "A Dependency-Based Neural Reordering Model for Statistical Machine\n  Translation", "comments": "7 pages, 3 figures, Proceedings of AAAI-17", "journal-ref": "Proceedings of AAAI-17 (2017)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine translation (MT) that involves translating between two languages\nwith significant differences in word order, determining the correct word order\nof translated words is a major challenge. The dependency parse tree of a source\nsentence can help to determine the correct word order of the translated words.\nIn this paper, we present a novel reordering approach utilizing a neural\nnetwork and dependency-based embeddings to predict whether the translations of\ntwo source words linked by a dependency relation should remain in the same\norder or should be swapped in the translated sentence. Experiments on\nChinese-to-English translation show that our approach yields a statistically\nsignificant improvement of 0.57 BLEU point on benchmark NIST test sets,\ncompared to our prior state-of-the-art statistical MT system that uses sparse\ndependency-based reordering features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 09:08:21 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Hadiwinoto", "Christian", ""], ["Ng", "Hwee Tou", ""]]}, {"id": "1702.04521", "submitter": "Tim Rockt\\\"aschel", "authors": "Micha{\\l} Daniluk, Tim Rockt\\\"aschel, Johannes Welbl, Sebastian Riedel", "title": "Frustratingly Short Attention Spans in Neural Language Modeling", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 09:45:23 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Daniluk", "Micha\u0142", ""], ["Rockt\u00e4schel", "Tim", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1702.04615", "submitter": "Daniel Miller", "authors": "Daniel Miller", "title": "Automated Identification of Drug-Drug Interactions in Pediatric\n  Congestive Heart Failure Patients", "comments": "Final project report for CS 221: Artificial Intelligence, Fall 2016\n  at Stanford University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congestive Heart Failure, or CHF, is a serious medical condition that can\nresult in fluid buildup in the body as a result of a weak heart. When the heart\ncan't pump enough blood to efficiently deliver nutrients and oxygen to the\nbody, kidney function may be impaired, resulting in fluid retention. CHF\npatients require a broad drug regimen to maintain the delicate system balance,\nparticularly between their heart and kidneys. These drugs include ACE\ninhibitors and Beta Blockers to control blood pressure, anticoagulants to\nprevent blood clots, and diuretics to reduce fluid overload. Many of these\ndrugs may interact, and potential effects of these interactions must be weighed\nagainst their benefits. For this project, we consider a set of 44 drugs\nidentified as specifically relevant for treating CHF by pediatric cardiologists\nat Lucile Packard Children's Hospital. This list was generated as part of our\ncurrent work at the LPCH Heart Center. The goal of this project is to identify\nand evaluate potentially harmful drug-drug interactions (DDIs) within pediatric\npatients with Congestive Heart Failure. This identification will be done\nautonomously, so that it may continuously update by evaluating newly published\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 02:21:42 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Miller", "Daniel", ""]]}, {"id": "1702.04770", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Sumit Chopra, Marc'Aurelio Ranzato, Arthur Szlam, Ruoyu\n  Sun, Soumith Chintala, Nicolas Vasilache", "title": "Training Language Models Using Target-Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Truncated Back-Propagation through Time (BPTT) is the most popular\napproach to training Recurrent Neural Networks (RNNs), it suffers from being\ninherently sequential (making parallelization difficult) and from truncating\ngradient flow between distant time-steps. We investigate whether Target\nPropagation (TPROP) style approaches can address these shortcomings.\nUnfortunately, extensive experiments suggest that TPROP generally underperforms\nBPTT, and we end with an analysis of this phenomenon, and suggestions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 20:56:30 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Wiseman", "Sam", ""], ["Chopra", "Sumit", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""], ["Sun", "Ruoyu", ""], ["Chintala", "Soumith", ""], ["Vasilache", "Nicolas", ""]]}, {"id": "1702.04811", "submitter": "John Lalor", "authors": "John P. Lalor, Hao Wu, Tsendsuren Munkhdalai, Hong Yu", "title": "Understanding Deep Learning Performance through an Examination of Test\n  Set Difficulty: A Psychometric Case Study", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting the performance of deep learning models beyond test set accuracy\nis challenging. Characteristics of individual data points are often not\nconsidered during evaluation, and each data point is treated equally. We\nexamine the impact of a test set question's difficulty to determine if there is\na relationship between difficulty and performance. We model difficulty using\nwell-studied psychometric methods on human response patterns. Experiments on\nNatural Language Inference (NLI) and Sentiment Analysis (SA) show that the\nlikelihood of answering a question correctly is impacted by the question's\ndifficulty. As DNNs are trained with more data, easy examples are learned more\nquickly than hard examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 23:04:09 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 00:23:16 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 18:20:46 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lalor", "John P.", ""], ["Wu", "Hao", ""], ["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1702.04938", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Johannes Wahle, Pavel Sofroniev, and Gerhard J\\\"ager", "title": "Fast and unsupervised methods for multilingual cognate clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we explore the use of unsupervised methods for detecting\ncognates in multilingual word lists. We use online EM to train sound segment\nsimilarity weights for computing similarity between two words. We tested our\nonline systems on geographically spread sixteen different language groups of\nthe world and show that the Online PMI system (Pointwise Mutual Information)\noutperforms a HMM based system and two linguistically motivated systems:\nLexStat and ALINE. Our results suggest that a PMI system trained in an online\nfashion can be used by historical linguists for fast and accurate\nidentification of cognates in not so well-studied language families.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 12:10:18 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Rama", "Taraka", ""], ["Wahle", "Johannes", ""], ["Sofroniev", "Pavel", ""], ["J\u00e4ger", "Gerhard", ""]]}, {"id": "1702.05053", "submitter": "Xiaochang Peng", "authors": "Xiaochang Peng, Chuan Wang, Daniel Gildea and Nianwen Xue", "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "comments": "Accepted by EACL-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural attention models have achieved great success in different NLP tasks.\nHow- ever, they have not fulfilled their promise on the AMR parsing task due to\nthe data sparsity issue. In this paper, we de- scribe a sequence-to-sequence\nmodel for AMR parsing and present different ways to tackle the data sparsity\nproblem. We show that our methods achieve significant improvement over a\nbaseline neural atten- tion model and our results are also compet- itive\nagainst state-of-the-art systems that do not use extra linguistic resources.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 17:09:12 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Peng", "Xiaochang", ""], ["Wang", "Chuan", ""], ["Gildea", "Daniel", ""], ["Xue", "Nianwen", ""]]}, {"id": "1702.05270", "submitter": "Sandro Pezzelle", "authors": "Sandro Pezzelle, Marco Marelli, Raffaella Bernardi", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers\n  from Vision", "comments": "Accepted at EACL2017. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can refer to quantities in a visual scene by using either exact\ncardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,\nmost, all). In humans, these two processes underlie fairly different cognitive\nand neural mechanisms. Inspired by this evidence, the present study proposes\ntwo models for learning the objective meaning of cardinals and quantifiers from\nvisual scenes containing multiple objects. We show that a model capitalizing on\na 'fuzzy' measure of similarity is effective for learning quantifiers, whereas\nthe learning of exact cardinals is better accomplished when information about\nnumber is provided.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 09:26:10 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Pezzelle", "Sandro", ""], ["Marelli", "Marco", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1702.05398", "submitter": "Pradeep Dasigi", "authors": "Pradeep Dasigi, Gully A.P.C. Burns, Eduard Hovy, and Anita de Waard", "title": "Experiment Segmentation in Scientific Discourse as Clause-level\n  Structured Prediction using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a deep learning model for identifying structure within experiment\nnarratives in scientific literature. We take a sequence labeling approach to\nthis problem, and label clauses within experiment narratives to identify the\ndifferent parts of the experiment. Our dataset consists of paragraphs taken\nfrom open access PubMed papers labeled with rhetorical information as a result\nof our pilot annotation. Our model is a Recurrent Neural Network (RNN) with\nLong Short-Term Memory (LSTM) cells that labels clauses. The clause\nrepresentations are computed by combining word representations using a novel\nattention mechanism that involves a separate RNN. We compare this model against\nLSTMs where the input layer has simple or no attention and a feature rich CRF\nmodel. Furthermore, we describe how our work could be useful for information\nextraction from scientific literature.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:39:21 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Dasigi", "Pradeep", ""], ["Burns", "Gully A. P. C.", ""], ["Hovy", "Eduard", ""], ["de Waard", "Anita", ""]]}, {"id": "1702.05512", "submitter": "Parminder Bhatia", "authors": "Parminder Bhatia, Marsal Gavalda and Arash Einolghozati", "title": "soc2seq: Social Embedding meets Conversation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While liking or upvoting a post on a mobile app is easy to do, replying with\na written note is much more difficult, due to both the cognitive load of coming\nup with a meaningful response as well as the mechanics of entering the text.\nHere we present a novel textual reply generation model that goes beyond the\ncurrent auto-reply and predictive text entry models by taking into account the\ncontent preferences of the user, the idiosyncrasies of their conversational\nstyle, and even the structure of their social graph. Specifically, we have\ndeveloped two types of models for personalized user interactions: a\ncontent-based conversation model, which makes use of location together with\nuser information, and a social-graph-based conversation model, which combines\ncontent-based conversation models with social graphs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 20:26:50 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:14:22 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 22:21:52 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Bhatia", "Parminder", ""], ["Gavalda", "Marsal", ""], ["Einolghozati", "Arash", ""]]}, {"id": "1702.05531", "submitter": "Vladimir Zolotov", "authors": "Vladimir Zolotov and David Kung", "title": "Analysis and Optimization of fastText Linear Text Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper [1] shows that simple linear classifier can compete with complex\ndeep learning algorithms in text classification applications. Combining bag of\nwords (BoW) and linear classification techniques, fastText [1] attains same or\nonly slightly lower accuracy than deep learning algorithms [2-9] that are\norders of magnitude slower. We proved formally that fastText can be transformed\ninto a simpler equivalent classifier, which unlike fastText does not have any\nhidden layer. We also proved that the necessary and sufficient dimensionality\nof the word vector embedding space is exactly the number of document classes.\nThese results help constructing more optimal linear text classifiers with\nguaranteed maximum classification capabilities. The results are proven exactly\nby pure formal algebraic methods without attracting any empirical data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 22:10:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Zolotov", "Vladimir", ""], ["Kung", "David", ""]]}, {"id": "1702.05624", "submitter": "Roberto Santana", "authors": "Roberto Santana", "title": "Reproducing and learning new algebraic operations on word embeddings\n  using genetic programming", "comments": "17 pages, 7 tables, 8 figures. Python code available from\n  https://github.com/rsantana-isg/GP_word2vec", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word-vector representations associate a high dimensional real-vector to every\nword from a corpus. Recently, neural-network based methods have been proposed\nfor learning this representation from large corpora. This type of\nword-to-vector embedding is able to keep, in the learned vector space, some of\nthe syntactic and semantic relationships present in the original word corpus.\nThis, in turn, serves to address different types of language classification\ntasks by doing algebraic operations defined on the vectors. The general\npractice is to assume that the semantic relationships between the words can be\ninferred by the application of a-priori specified algebraic operations. Our\ngeneral goal in this paper is to show that it is possible to learn methods for\nword composition in semantic spaces. Instead of expressing the compositional\nmethod as an algebraic operation, we will encode it as a program, which can be\nlinear, nonlinear, or involve more intricate expressions. More remarkably, this\nprogram will be evolved from a set of initial random programs by means of\ngenetic programming (GP). We show that our method is able to reproduce the same\nbehavior as human-designed algebraic operators. Using a word analogy task as\nbenchmark, we also show that GP-generated programs are able to obtain accuracy\nvalues above those produced by the commonly used human-designed rule for\nalgebraic manipulation of word vectors. Finally, we show the robustness of our\napproach by executing the evolved programs on the word2vec GoogleNews vectors,\nlearned over 3 billion running words, and assessing their accuracy in the same\nword analogy task.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 15:29:01 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Santana", "Roberto", ""]]}, {"id": "1702.05638", "submitter": "Martin Potthast", "authors": "Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff,\n  Benno Stein", "title": "A Stylometric Inquiry into Hyperpartisan and Fake News", "comments": "10 pages, 3 figures, 6 tables, submitted to ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a writing style analysis of hyperpartisan (i.e.,\nextremely one-sided) news in connection to fake news. It presents a large\ncorpus of 1,627 articles that were manually fact-checked by professional\njournalists from BuzzFeed. The articles originated from 9 well-known political\npublishers, 3 each from the mainstream, the hyperpartisan left-wing, and the\nhyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% of\nwhich originated from hyperpartisan publishers.\n  We propose and demonstrate a new way of assessing style similarity between\ntext categories via Unmasking---a meta-learning approach originally devised for\nauthorship verification---, revealing that the style of left-wing and\nright-wing news have a lot more in common than any of the two have with the\nmainstream. Furthermore, we show that hyperpartisan news can be discriminated\nwell by its style from the mainstream (F1=0.78), as can be satire from both\n(F1=0.81). Unsurprisingly, style-based fake news detection does not live up to\nscratch (F1=0.46). Nevertheless, the former results are important to implement\npre-screening for fake news detectors.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 18:10:04 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Potthast", "Martin", ""], ["Kiesel", "Johannes", ""], ["Reinartz", "Kevin", ""], ["Bevendorff", "Janek", ""], ["Stein", "Benno", ""]]}, {"id": "1702.05793", "submitter": "Mark Dredze", "authors": "Ann Irvine and Mark Dredze", "title": "Harmonic Grammar, Optimality Theory, and Syntax Learnability: An\n  Empirical Exploration of Czech Word Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a systematic theoretical and empirical comparison of the\nmajor algorithms that have been proposed for learning Harmonic and Optimality\nTheory grammars (HG and OT, respectively). By comparing learning algorithms, we\nare also able to compare the closely related OT and HG frameworks themselves.\nExperimental results show that the additional expressivity of the HG framework\nover OT affords performance gains in the task of predicting the surface word\norder of Czech sentences. We compare the perceptron with the classic Gradual\nLearning Algorithm (GLA), which learns OT grammars, as well as the popular\nMaximum Entropy model. In addition to showing that the perceptron is\ntheoretically appealing, our work shows that the performance of the HG model it\nlearns approaches that of the upper bound in prediction accuracy on a held out\ntest set and that it is capable of accurately modeling observed variation.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 20:37:40 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Irvine", "Ann", ""], ["Dredze", "Mark", ""]]}, {"id": "1702.05821", "submitter": "Will Radford", "authors": "Bo Han, Will Radford, Ana\\\"is Cadilhac, Art Harol, Andrew Chisholm,\n  Ben Hachey", "title": "Post-edit Analysis of Collective Biography Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text generation is increasingly common but often requires manual post-editing\nwhere high precision is critical to end users. However, manual editing is\nexpensive so we want to ensure this effort is focused on high-value tasks. And\nwe want to maintain stylistic consistency, a particular challenge in crowd\nsettings. We present a case study, analysing human post-editing in the context\nof a template-based biography generation system. An edit flow visualisation\ncombined with manual characterisation of edits helps identify and prioritise\nwork for improving end-to-end efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 00:23:06 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Han", "Bo", ""], ["Radford", "Will", ""], ["Cadilhac", "Ana\u00efs", ""], ["Harol", "Art", ""], ["Chisholm", "Andrew", ""], ["Hachey", "Ben", ""]]}, {"id": "1702.05962", "submitter": "Kris Cao", "authors": "Kris Cao and Stephen Clark", "title": "Latent Variable Dialogue Models and their Diversity", "comments": "Accepted at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dialogue generation model that directly captures the variability\nin possible responses to a given input, which reduces the `boring output' issue\nof deterministic dialogue models. Experiments show that our model generates\nmore diverse outputs than baseline models, and also generates more consistently\nacceptable output than sampling from a deterministic encoder-decoder model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:36:23 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Cao", "Kris", ""], ["Clark", "Stephen", ""]]}, {"id": "1702.06027", "submitter": "Haluk O. Bingol", "authors": "Ibrahim Cimentepe and Haluk O. Bingol", "title": "Parent Oriented Teacher Selection Causes Language Diversity", "comments": null, "journal-ref": "Journal of Theoretical Biology 429, 142-148, 2017", "doi": "10.1016/j.jtbi.2017.06.032", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An evolutionary model for emergence of diversity in language is developed. We\ninvestigated the effects of two real life observations, namely, people prefer\npeople that they communicate with well, and people interact with people that\nare physically close to each other. Clearly these groups are relatively small\ncompared to the entire population. We restrict selection of the teachers from\nsuch small groups, called imitation sets, around parents. Then the child learns\nlanguage from a teacher selected within the imitation set of her parent. As a\nresult, there are subcommunities with their own languages developed. Within\nsubcommunity comprehension is found to be high. The number of languages is\nrelated to the relative size of imitation set by a power law.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 15:53:56 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 17:24:23 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Cimentepe", "Ibrahim", ""], ["Bingol", "Haluk O.", ""]]}, {"id": "1702.06135", "submitter": "Prasanna Raj Noel Dabre", "authors": "Raj Dabre, Fabien Cromieres, Sadao Kurohashi", "title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source\n  Sentences In Multiple Languages", "comments": "Official version of manuscript which was accepted in MT Summit 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore a simple solution to \"Multi-Source Neural Machine\nTranslation\" (MSNMT) which only relies on preprocessing a N-way multilingual\ncorpus without modifying the Neural Machine Translation (NMT) architecture or\ntraining procedure. We simply concatenate the source sentences to form a single\nlong multi-source input sentence while keeping the target side sentence as it\nis and train an NMT system using this preprocessed corpus. We evaluate our\nmethod in resource poor as well as resource rich settings and show its\neffectiveness (up to 4 BLEU using 2 source languages and up to 6 BLEU using 5\nsource languages). We also compare against existing methods for MSNMT and show\nthat our solution gives competitive results despite its simplicity. We also\nprovide some insights on how the NMT system leverages multilingual information\nin such a scenario by visualizing attention.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 19:00:06 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 08:25:29 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 11:37:41 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 04:10:10 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Dabre", "Raj", ""], ["Cromieres", "Fabien", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "1702.06216", "submitter": "Michael Bloodgood", "authors": "Alan Mishler, Kevin Wonus, Wendy Chambers and Michael Bloodgood", "title": "Filtering Tweets for Social Unrest", "comments": "7 pages, 8 figures, 3 tables; published in Proceedings of the 2017\n  IEEE 11th International Conference on Semantic Computing (ICSC), San Diego,\n  CA, USA, pages 17-23, January 2017", "journal-ref": "In Proceedings of the 2017 IEEE 11th International Conference on\n  Semantic Computing (ICSC), pages 17-23, San Diego, CA, USA, January 2017.\n  IEEE", "doi": "10.1109/ICSC.2017.75", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the events of the Arab Spring, there has been increased interest in\nusing social media to anticipate social unrest. While efforts have been made\ntoward automated unrest prediction, we focus on filtering the vast volume of\ntweets to identify tweets relevant to unrest, which can be provided to\ndownstream users for further analysis. We train a supervised classifier that is\nable to label Arabic language tweets as relevant to unrest with high\nreliability. We examine the relationship between training data size and\nperformance and investigate ways to optimize the model building process while\nminimizing cost. We also explore how confidence thresholds can be set to\nachieve desired levels of performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 23:48:39 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 22:37:35 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Mishler", "Alan", ""], ["Wonus", "Kevin", ""], ["Chambers", "Wendy", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1702.06235", "submitter": "Will Radford", "authors": "Andrew Chisholm, Will Radford, Ben Hachey", "title": "Learning to generate one-sentence biographies from Wikidata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the generation of one-sentence Wikipedia biographies from\nfacts derived from Wikidata slot-value pairs. We train a recurrent neural\nnetwork sequence-to-sequence model with attention to select facts and generate\ntextual summaries. Our model incorporates a novel secondary objective that\nhelps ensure it generates sentences that contain the input facts. The model\nachieves a BLEU score of 41, improving significantly upon the vanilla\nsequence-to-sequence model and scoring roughly twice that of a simple template\nbaseline. Human preference evaluation suggests the model is nearly as good as\nthe Wikipedia reference. Manual analysis explores content selection, suggesting\nthe model can trade the ability to infer knowledge against the risk of\nhallucinating incorrect information.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:30:59 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Chisholm", "Andrew", ""], ["Radford", "Will", ""], ["Hachey", "Ben", ""]]}, {"id": "1702.06239", "submitter": "Yang Gao", "authors": "Yang Gao, Hao Wang, Chen Zhang, Wei Wang", "title": "Reinforcement Learning Based Argument Component Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argument component detection (ACD) is an important sub-task in argumentation\nmining. ACD aims at detecting and classifying different argument components in\nnatural language texts. Historical annotations (HAs) are important features the\nhuman annotators consider when they manually perform the ACD task. However, HAs\nare largely ignored by existing automatic ACD techniques. Reinforcement\nlearning (RL) has proven to be an effective method for using HAs in some\nnatural language processing tasks. In this work, we propose a RL-based ACD\ntechnique, and evaluate its performance on two well-annotated corpora. Results\nsuggest that, in terms of classification accuracy, HAs-augmented RL outperforms\nplain RL by at most 17.85%, and outperforms the state-of-the-art supervised\nlearning algorithm by at most 11.94%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 02:18:38 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Gao", "Yang", ""], ["Wang", "Hao", ""], ["Zhang", "Chen", ""], ["Wang", "Wei", ""]]}, {"id": "1702.06336", "submitter": "Miroslav Vodol\\'an", "authors": "Miroslav Vodol\\'an, Rudolf Kadlec, Jan Kleindienst", "title": "Hybrid Dialog State Tracker with ASR Features", "comments": "Accepted to EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hybrid dialog state tracker enhanced by trainable\nSpoken Language Understanding (SLU) for slot-filling dialog systems. Our\narchitecture is inspired by previously proposed neural-network-based\nbelief-tracking systems. In addition, we extended some parts of our modular\narchitecture with differentiable rules to allow end-to-end training. We\nhypothesize that these rules allow our tracker to generalize better than pure\nmachine-learning based systems. For evaluation, we used the Dialog State\nTracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with\ndialogs from restaurant information system. To our knowledge, our hybrid\ntracker sets a new state-of-the-art result in three out of four categories\nwithin the DSTC2.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 11:34:14 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Vodol\u00e1n", "Miroslav", ""], ["Kadlec", "Rudolf", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1702.06378", "submitter": "Liang Lu", "authors": "Liang Lu, Lingpeng Kong, Chris Dyer and Noah A. Smith", "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition", "comments": "5 pages, 2 figures, camera ready version at Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmental conditional random fields (SCRFs) and connectionist temporal\nclassification (CTC) are two sequence labeling methods used for end-to-end\ntraining of speech recognition models. Both models define a transcription\nprobability by marginalizing decisions about latent segmentation alternatives\nto derive a sequence probability: the former uses a globally normalized joint\nmodel of segment labels and durations, and the latter classifies each frame as\neither an output symbol or a \"continuation\" of the previous label. In this\npaper, we train a recognition model by optimizing an interpolation between the\nSCRF and CTC losses, where the same recurrent neural network (RNN) encoder is\nused for feature extraction for both outputs. We find that this multitask\nobjective improves recognition accuracy when decoding with either the SCRF or\nCTC models. Additionally, we show that CTC can also be used to pretrain the RNN\nencoder, which improves the convergence rate when learning the joint model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:39:35 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 02:40:45 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 20:42:54 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 18:19:34 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Lu", "Liang", ""], ["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1702.06467", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Carlos-Emiliano Gonz\\'alez-Gallardo, Juan-Manuel Torres-Moreno,\n  Azucena Montes Rend\\'on and Gerardo Sierra", "title": "Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization", "comments": "8 pages, 6 figures, Conference paper", "journal-ref": "Proceedings of the 8th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management, Vol 1: KDIR,\n  307-314, 2016, Porto, Portugal", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:26:54 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Gonz\u00e1lez-Gallardo", "Carlos-Emiliano", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Rend\u00f3n", "Azucena Montes", ""], ["Sierra", "Gerardo", ""]]}, {"id": "1702.06478", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Xavier Bost, Ilaria Brunetti, Luis Adri\\'an Cabrera-Diego,\n  Jean-Val\\`ere Cossu, Andr\\'ea Linhares, Mohamed Morchid, Juan-Manuel\n  Torres-Moreno, Marc El-B\\`eze, Richard Dufour", "title": "Syst\\`emes du LIA \\`a DEFT'13", "comments": "12 pages, 3 tables, (Paper in French)", "journal-ref": "Proceedings of the Ninth DEFT Workshop, DEFT2013, Les\n  Sables-d'Olonne, France, 21st June 2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2013 D\\'efi de Fouille de Textes (DEFT) campaign is interested in two\ntypes of language analysis tasks, the document classification and the\ninformation extraction in the specialized domain of cuisine recipes. We present\nthe systems that the LIA has used in DEFT 2013. Our systems show interesting\nresults, even though the complexity of the proposed tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 17:14:56 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Bost", "Xavier", ""], ["Brunetti", "Ilaria", ""], ["Cabrera-Diego", "Luis Adri\u00e1n", ""], ["Cossu", "Jean-Val\u00e8re", ""], ["Linhares", "Andr\u00e9a", ""], ["Morchid", "Mohamed", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["El-B\u00e8ze", "Marc", ""], ["Dufour", "Richard", ""]]}, {"id": "1702.06510", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Luis Adri\\'an Cabrera-Diego, St\\'ephane Huet, Bassam Jabaian,\n  Alejandro Molina, Juan-Manuel Torres-Moreno, Marc El-B\\`eze, Barth\\'el\\'emy\n  Durette", "title": "Algorithmes de classification et d'optimisation: participation du\n  LIA/ADOC \\'a DEFT'14", "comments": "8 pages, 3 tables, Conference paper (in French)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This year, the DEFT campaign (D\\'efi Fouilles de Textes) incorporates a task\nwhich aims at identifying the session in which articles of previous TALN\nconferences were presented. We describe the three statistical systems developed\nat LIA/ADOC for this task. A fusion of these systems enables us to obtain\ninteresting results (micro-precision score of 0.76 measured on the test corpus)\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:24:52 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Cabrera-Diego", "Luis Adri\u00e1n", ""], ["Huet", "St\u00e9phane", ""], ["Jabaian", "Bassam", ""], ["Molina", "Alejandro", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["El-B\u00e8ze", "Marc", ""], ["Durette", "Barth\u00e9l\u00e9my", ""]]}, {"id": "1702.06589", "submitter": "Octavian-Eugen Ganea", "authors": "Till Haug and Octavian-Eugen Ganea and Paulina Grnarova", "title": "Neural Multi-Step Reasoning for Question Answering on Semi-Structured\n  Tables", "comments": null, "journal-ref": "European Conference on Information Retrieval (ECIR) 2018", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in natural language processing tasks have gained momentum in recent\nyears due to the increasingly popular neural network methods. In this paper, we\nexplore deep learning techniques for answering multi-step reasoning questions\nthat operate on semi-structured tables. Challenges here arise from the level of\nlogical compositionality expressed by questions, as well as the domain\nopenness. Our approach is weakly supervised, trained on question-answer-table\ntriples without requiring intermediate strong supervision. It performs two\nphases: first, machine understandable logical forms (programs) are generated\nfrom natural language questions following the work of [Pasupat and Liang,\n2015]. Second, paraphrases of logical forms and questions are embedded in a\njointly learned vector space using word and character convolutional neural\nnetworks. A neural scoring function is further used to rank and retrieve the\nmost probable logical form (interpretation) of a question. Our best single\nmodel achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best\nensemble of our models pushes the state-of-the-art score on this task to 38.7%,\nthus slightly surpassing both the engineered feature scoring baseline, as well\nas the Neural Programmer model of [Neelakantan et al., 2016].\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 21:24:26 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 15:29:26 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Haug", "Till", ""], ["Ganea", "Octavian-Eugen", ""], ["Grnarova", "Paulina", ""]]}, {"id": "1702.06594", "submitter": "Marco Kuhlmann", "authors": "Marco Kuhlmann and Giorgio Satta and Peter Jonsson", "title": "On the Complexity of CCG Parsing", "comments": "39 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the parsing complexity of Combinatory Categorial Grammar (CCG) in\nthe formalism of Vijay-Shanker and Weir (1994). As our main result, we prove\nthat any parsing algorithm for this formalism will take in the worst case\nexponential time when the size of the grammar, and not only the length of the\ninput sentence, is included in the analysis. This sets the formalism of\nVijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as\nTree-Adjoining Grammar (TAG), for which parsing can be performed in time\npolynomial in the combined size of grammar and input sentence. Our results\ncontribute to a refined understanding of the class of mildly context-sensitive\ngrammars, and inform the search for new, mildly context-sensitive versions of\nCCG.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 21:36:51 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 10:16:46 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Kuhlmann", "Marco", ""], ["Satta", "Giorgio", ""], ["Jonsson", "Peter", ""]]}, {"id": "1702.06663", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Bryan L. Lewis, Maimuna S.\n  Majumder, Emily Cohn, John S. Brownstein, Madhav V. Marathe, Naren\n  Ramakrishnan", "title": "Guided Deep List: Automating the Generation of Epidemiological Line\n  Lists from Open Sources", "comments": "This paper has been submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time monitoring and responses to emerging public health threats rely on\nthe availability of timely surveillance data. During the early stages of an\nepidemic, the ready availability of line lists with detailed tabular\ninformation about laboratory-confirmed cases can assist epidemiologists in\nmaking reliable inferences and forecasts. Such inferences are crucial to\nunderstand the epidemiology of a specific disease early enough to stop or\ncontrol the outbreak. However, construction of such line lists requires\nconsiderable human supervision and therefore, difficult to generate in\nreal-time. In this paper, we motivate Guided Deep List, the first tool for\nbuilding automated line lists (in near real-time) from open source reports of\nemerging disease outbreaks. Specifically, we focus on deriving epidemiological\ncharacteristics of an emerging disease and the affected population from reports\nof illness. Guided Deep List uses distributed vector representations (ala\nword2vec) to discover a set of indicators for each line list feature. This\ndiscovery of indicators is followed by the use of dependency parsing based\ntechniques for final extraction in tabular form. We evaluate the performance of\nGuided Deep List against a human annotated line list provided by HealthMap\ncorresponding to MERS outbreaks in Saudi Arabia. We demonstrate that Guided\nDeep List extracts line list features with increased accuracy compared to a\nbaseline method. We further show how these automatically extracted line list\nfeatures can be used for making epidemiological inferences, such as inferring\ndemographics and symptoms-to-hospitalization period of affected individuals.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 03:14:36 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Lewis", "Bryan L.", ""], ["Majumder", "Maimuna S.", ""], ["Cohn", "Emily", ""], ["Brownstein", "John S.", ""], ["Marathe", "Madhav V.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1702.06672", "submitter": "Aida Nematzadeh", "authors": "Aida Nematzadeh and Barend Beekhuizen and Shanshan Huang and Suzanne\n  Stevenson", "title": "Calculating Probabilities Simplifies Word Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children can use the statistical regularities of their environment to learn\nword meanings, a mechanism known as cross-situational learning. We take a\ncomputational approach to investigate how the information present during each\nobservation in a cross-situational framework can affect the overall acquisition\nof word meanings. We do so by formulating various in-the-moment learning\nmechanisms that are sensitive to different statistics of the environment, such\nas counts and conditional probabilities. Each mechanism introduces a unique\nsource of competition or mutual exclusivity bias to the model; the mechanism\nthat maximally uses the model's knowledge of word meanings performs the best.\nMoreover, the gap between this mechanism and others is amplified in more\nchallenging learning scenarios, such as learning from few examples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 04:30:09 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Nematzadeh", "Aida", ""], ["Beekhuizen", "Barend", ""], ["Huang", "Shanshan", ""], ["Stevenson", "Suzanne", ""]]}, {"id": "1702.06675", "submitter": "Ekaterina Vylomova", "authors": "Ekaterina Vylomova, Ryan Cotterell, Timothy Baldwin and Trevor Cohn", "title": "Context-Aware Prediction of Derivational Word-forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivational morphology is a fundamental and complex characteristic of\nlanguage. In this paper we propose the new task of predicting the derivational\nform of a given base-form lemma that is appropriate for a given context. We\npresent an encoder--decoder style neural network to produce a derived form\ncharacter-by-character, based on its corresponding character-level\nrepresentation of the base form and the context. We demonstrate that our model\nis able to generate valid context-sensitive derivations from known base forms,\nbut is less accurate under a lexicon agnostic setting.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 04:50:23 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Vylomova", "Ekaterina", ""], ["Cotterell", "Ryan", ""], ["Baldwin", "Timothy", ""], ["Cohn", "Trevor", ""]]}, {"id": "1702.06677", "submitter": "George Berry", "authors": "George Berry and Sean J. Taylor", "title": "Discussion quality diffuses in the digital public square", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of online social influence have demonstrated that friends have\nimportant effects on many types of behavior in a wide variety of settings.\nHowever, we know much less about how influence works among relative strangers\nin digital public squares, despite important conversations happening in such\nspaces. We present the results of a study on large public Facebook pages where\nwe randomly used two different methods--most recent and social feedback--to\norder comments on posts. We find that the social feedback condition results in\nhigher quality viewed comments and response comments. After measuring the\naverage quality of comments written by users before the study, we find that\nsocial feedback has a positive effect on response quality for both low and high\nquality commenters. We draw on a theoretical framework of social norms to\nexplain this empirical result. In order to examine the influence mechanism\nfurther, we measure the similarity between comments viewed and written during\nthe study, finding that similarity increases for the highest quality\ncontributors under the social feedback condition. This suggests that, in\naddition to norms, some individuals may respond with increased relevance to\nhigh-quality comments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 04:54:43 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Berry", "George", ""], ["Taylor", "Sean J.", ""]]}, {"id": "1702.06696", "submitter": "Thomas Kober", "authors": "Thomas Kober and Julie Weeds and John Wilkie and Jeremy Reffin and\n  David Weir", "title": "One Representation per Word - Does it make Sense for Composition?", "comments": "to appear at the EACL 2017 workshop on Sense, Concept and Entity\n  Representations and their Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate whether an a priori disambiguation of word\nsenses is strictly necessary or whether the meaning of a word in context can be\ndisambiguated through composition alone. We evaluate the performance of\noff-the-shelf single-vector and multi-sense vector models on a benchmark phrase\nsimilarity task and a novel task for word-sense discrimination. We find that\nsingle-sense vector models perform as well or better than multi-sense vector\nmodels despite arguably less clean elementary representations. Our findings\nfurthermore show that simple composition functions such as pointwise addition\nare able to recover sense specific information from a single-sense vector model\nremarkably well.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 07:41:08 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Kober", "Thomas", ""], ["Weeds", "Julie", ""], ["Wilkie", "John", ""], ["Reffin", "Jeremy", ""], ["Weir", "David", ""]]}, {"id": "1702.06700", "submitter": "Yuetan Lin", "authors": "Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang", "title": "Task-driven Visual Saliency and Attention-based Visual Question\n  Answering", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) has witnessed great progress since May, 2015\nas a classic problem unifying visual and textual data into a system. Many\nenlightening VQA works explore deep into the image and question encodings and\nfusing methods, of which attention is the most effective and infusive\nmechanism. Current attention based methods focus on adequate fusion of visual\nand textual features, but lack the attention to where people focus to ask\nquestions about the image. Traditional attention based methods attach a single\nvalue to the feature at each spatial location, which losses many useful\ninformation. To remedy these problems, we propose a general method to perform\nsaliency-like pre-selection on overlapped region features by the interrelation\nof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication\nbased attention method to capture more competent correlation information\nbetween visual and textual features. We conduct experiments on the large-scale\nCOCO-VQA dataset and analyze the effectiveness of our model demonstrated by\nstrong empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 08:19:38 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lin", "Yuetan", ""], ["Pang", "Zhangyang", ""], ["Wang", "Donghui", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1702.06703", "submitter": "Jiwei Li", "authors": "Jiwei Li, Will Monroe and Dan Jurafsky", "title": "Data Distillation for Controlling Specificity in Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People speak at different levels of specificity in different situations.\nDepending on their knowledge, interlocutors, mood, etc.} A conversational agent\nshould have this ability and know when to be specific and when to be general.\nWe propose an approach that gives a neural network--based conversational agent\nthis ability. Our approach involves alternating between \\emph{data\ndistillation} and model training : removing training examples that are closest\nto the responses most commonly produced by the model trained from the last\nround and then retrain the model on the remaining dataset. Dialogue generation\nmodels trained with different degrees of data distillation manifest different\nlevels of specificity.\n  We then train a reinforcement learning system for selecting among this pool\nof generation models, to choose the best level of specificity for a given\ninput. Compared to the original generative model trained without distillation,\nthe proposed system is capable of generating more interesting and\nhigher-quality responses, in addition to appropriately adjusting specificity\ndepending on the context.\n  Our research constitutes a specific case of a broader approach involving\ntraining multiple subsystems from a single dataset distinguished by differences\nin a specific property one wishes to model. We show that from such a set of\nsubsystems, one can use reinforcement learning to build a system that tailors\nits output to different input contexts at test time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 08:32:47 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Li", "Jiwei", ""], ["Monroe", "Will", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1702.06709", "submitter": "Abhishek .", "authors": "Abhishek, Ashish Anand and Amit Awekar", "title": "Fine-Grained Entity Type Classification by Jointly Learning\n  Representations and Label Embeddings", "comments": "11 pages, 5 figures, accepted at EACL 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained entity type classification (FETC) is the task of classifying an\nentity mention to a broad set of types. Distant supervision paradigm is\nextensively used to generate training data for this task. However, generated\ntraining data assigns same set of labels to every mention of an entity without\nconsidering its local context. Existing FETC systems have two major drawbacks:\nassuming training data to be noise free and use of hand crafted features. Our\nwork overcomes both drawbacks. We propose a neural network model that jointly\nlearns entity mentions and their context representation to eliminate use of\nhand crafted features. Our model treats training data as noisy and uses\nnon-parametric variant of hinge loss function. Experiments show that the\nproposed model outperforms previous state-of-the-art methods on two publicly\navailable datasets, namely FIGER (GOLD) and BBN with an average relative\nimprovement of 2.69% in micro-F1 score. Knowledge learnt by our model on one\ndataset can be transferred to other datasets while using same model or other\nFETC systems. These approaches of transferring knowledge further improve the\nperformance of respective models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 08:59:37 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Abhishek", "", ""], ["Anand", "Ashish", ""], ["Awekar", "Amit", ""]]}, {"id": "1702.06733", "submitter": "Jessica Ficler", "authors": "Jessica Ficler and Yoav Goldberg", "title": "Improving a Strong Neural Parser with Conjunction-Specific Features", "comments": null, "journal-ref": "EACL 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While dependency parsers reach very high overall accuracy, some dependency\nrelations are much harder than others. In particular, dependency parsers\nperform poorly in coordination construction (i.e., correctly attaching the\n\"conj\" relation). We extend a state-of-the-art dependency parser with\nconjunction-specific features, focusing on the similarity between the conjuncts\nhead words. Training the extended parser yields an improvement in \"conj\"\nattachment as well as in overall dependency parsing accuracy on the Stanford\ndependency conversion of the Penn TreeBank.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 10:10:44 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ficler", "Jessica", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1702.06740", "submitter": "Qiaolin Xia", "authors": "Qiaolin Xia, Baobao Chang, Zhifang Sui", "title": "Improving Chinese SRL with Heterogeneous Annotations", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  equation 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on Chinese semantic role labeling (SRL) have concentrated on\nsingle semantically annotated corpus. But the training data of single corpus is\noften limited. Meanwhile, there usually exists other semantically annotated\ncorpora for Chinese SRL scattered across different annotation frameworks. Data\nsparsity remains a bottleneck. This situation calls for larger training\ndatasets, or effective approaches which can take advantage of highly\nheterogeneous data. In these papers, we focus mainly on the latter, that is, to\nimprove Chinese SRL by using heterogeneous corpora together. We propose a novel\nprogressive learning model which augments the Progressive Neural Network with\nGated Recurrent Adapters. The model can accommodate heterogeneous inputs and\neffectively transfer knowledge between them. We also release a new corpus,\nChinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 10:34:47 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 06:46:12 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 13:05:23 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Xia", "Qiaolin", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "1702.06777", "submitter": "David Sanchez", "authors": "Gonzalo Donoso, David Sanchez", "title": "Dialectometric analysis of language variation in Twitter", "comments": "10 pages, 7 figures, 1 table. Accepted to VarDial 2017", "journal-ref": "Proceedings of the Fourth Workshop on NLP for Similar Languages,\n  Varieties and Dialects (VarDial), pp. 16-25, 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, microblogging platforms such as Twitter have given\nrise to a deluge of textual data that can be used for the analysis of informal\ncommunication between millions of individuals. In this work, we propose an\ninformation-theoretic approach to geographic language variation using a corpus\nbased on Twitter. We test our models with tens of concepts and their associated\nkeywords detected in Spanish tweets geolocated in Spain. We employ\ndialectometric measures (cosine similarity and Jensen-Shannon divergence) to\nquantify the linguistic distance on the lexical level between cells created in\na uniform grid over the map. This can be done for a single concept or in the\ngeneral case taking into account an average of the considered variants. The\nlatter permits an analysis of the dialects that naturally emerge from the data.\nInterestingly, our results reveal the existence of two dialect macrovarieties.\nThe first group includes a region-specific speech spoken in small towns and\nrural areas whereas the second cluster encompasses cities that tend to use a\nmore uniform variety. Since the results obtained with the two different metrics\nqualitatively agree, our work suggests that social media corpora can be\nefficiently used for dialectometric analyses.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 12:42:06 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Donoso", "Gonzalo", ""], ["Sanchez", "David", ""]]}, {"id": "1702.06794", "submitter": "Minh Le", "authors": "Minh Le, Antske Fokkens", "title": "Tackling Error Propagation through Reinforcement Learning: A Case of\n  Greedy Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error propagation is a common problem in NLP. Reinforcement learning explores\nerroneous states during training and can therefore be more robust when mistakes\nare made early in a process. In this paper, we apply reinforcement learning to\ngreedy dependency parsing which is known to suffer from error propagation.\nReinforcement learning improves accuracy of both labeled and unlabeled\ndependencies of the Stanford Neural Dependency Parser, a high performance\ngreedy parser, while maintaining its efficiency. We investigate the portion of\nerrors which are the result of error propagation and confirm that reinforcement\nlearning reduces the occurrence of error propagation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 13:49:18 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Le", "Minh", ""], ["Fokkens", "Antske", ""]]}, {"id": "1702.06875", "submitter": "Arman Cohan", "authors": "Arman Cohan, Sydney Young, Andrew Yates, Nazli Goharian", "title": "Triaging Content Severity in Online Mental Health Forums", "comments": "Accepted for publication in Journal of the Association for\n  Information Science and Technology (2017)", "journal-ref": null, "doi": "10.1002/asi.23865", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health forums are online communities where people express their issues\nand seek help from moderators and other users. In such forums, there are often\nposts with severe content indicating that the user is in acute distress and\nthere is a risk of attempted self-harm. Moderators need to respond to these\nsevere posts in a timely manner to prevent potential self-harm. However, the\nlarge volume of daily posted content makes it difficult for the moderators to\nlocate and respond to these critical posts. We present a framework for triaging\nuser content into four severity categories which are defined based on\nindications of self-harm ideation. Our models are based on a feature-rich\nclassification framework which includes lexical, psycholinguistic, contextual\nand topic modeling features. Our approaches improve the state of the art in\ntriaging the content severity in mental health forums by large margins (up to\n17% improvement over the F-1 scores). Using the proposed model, we analyze the\nmental state of users and we show that overall, long-term users of the forum\ndemonstrate a decreased severity of risk over time. Our analysis on the\ninteraction of the moderators with the users further indicates that without an\nautomatic way to identify critical content, it is indeed challenging for the\nmoderators to provide timely response to the users in need.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:14:12 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Cohan", "Arman", ""], ["Young", "Sydney", ""], ["Yates", "Andrew", ""], ["Goharian", "Nazli", ""]]}, {"id": "1702.06891", "submitter": "Derek Greene", "authors": "M. Atif Qureshi and Derek Greene", "title": "EVE: Explainable Vector Based Embedding Technique Using Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised explainable word embedding technique, called EVE,\nwhich is built upon the structure of Wikipedia. The proposed model defines the\ndimensions of a semantic vector representing a word using human-readable\nlabels, thereby it readily interpretable. Specifically, each vector is\nconstructed using the Wikipedia category graph structure together with the\nWikipedia article link structure. To test the effectiveness of the proposed\nword embedding model, we consider its usefulness in three fundamental tasks: 1)\nintruder detection - to evaluate its ability to identify a non-coherent vector\nfrom a list of coherent vectors, 2) ability to cluster - to evaluate its\ntendency to group related vectors together while keeping unrelated vectors in\nseparate clusters, and 3) sorting relevant items first - to evaluate its\nability to rank vectors (items) relevant to the query in the top order of the\nresult. For each task, we also propose a strategy to generate a task-specific\nhuman-interpretable explanation from the model. These demonstrate the overall\neffectiveness of the explainable embeddings generated by EVE. Finally, we\ncompare EVE with the Word2Vec, FastText, and GloVe embedding techniques across\nthe three tasks, and report improvements over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:50:25 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Qureshi", "M. Atif", ""], ["Greene", "Derek", ""]]}, {"id": "1702.07015", "submitter": "Jiaming Luo", "authors": "Jiaming Luo, Karthik Narasimhan, Regina Barzilay", "title": "Unsupervised Learning of Morphological Forests", "comments": "12 pages, 5 figures, accepted by TACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on unsupervised modeling of morphological families,\ncollectively comprising a forest over the language vocabulary. This formulation\nenables us to capture edgewise properties reflecting single-step morphological\nderivations, along with global distributional properties of the entire forest.\nThese global properties constrain the size of the affix set and encourage\nformation of tight morphological families. The resulting objective is solved\nusing Integer Linear Programming (ILP) paired with contrastive estimation. We\ntrain the model by alternating between optimizing the local log-linear model\nand the global ILP objective. We evaluate our system on three tasks: root\ndetection, clustering of morphological families and segmentation. Our\nexperiments demonstrate that our model yields consistent gains in all three\ntasks compared with the best published results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:44:02 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Luo", "Jiaming", ""], ["Narasimhan", "Karthik", ""], ["Barzilay", "Regina", ""]]}, {"id": "1702.07046", "submitter": "Travis Wolfe", "authors": "Travis Wolfe, Mark Dredze, Benjamin Van Durme", "title": "Feature Generation for Robust Semantic Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand-engineered feature sets are a well understood method for creating robust\nNLP models, but they require a lot of expertise and effort to create. In this\nwork we describe how to automatically generate rich feature sets from simple\nunits called featlets, requiring less engineering. Using information gain to\nguide the generation process, we train models which rival the state of the art\non two standard Semantic Role Labeling datasets with almost no task or\nlinguistic insight.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 23:39:03 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Wolfe", "Travis", ""], ["Dredze", "Mark", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1702.07071", "submitter": "Vladimir Vargas-Calder\\'on", "authors": "Keith Y. Patarroyo and Vladimir Vargas-Calder\\'on", "title": "Pronunciation recognition of English phonemes /\\textipa{@}/, /{\\ae}/,\n  /\\textipa{A}:/ and /\\textipa{2}/ using Formants and Mel Frequency Cepstral\n  Coefficients", "comments": "11 pages, pre-print version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Vocal Joystick Vowel Corpus, by Washington University, was used to study\nmonophthongs pronounced by native English speakers. The objective of this study\nwas to quantitatively measure the extent at which speech recognition methods\ncan distinguish between similar sounding vowels. In particular, the phonemes\n/\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ were analysed. 748\nsound files from the corpus were used and subjected to Linear Predictive Coding\n(LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients\n(MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree\nClassifier was used to build a predictive model that learnt the patterns of the\ntwo first formants measured in the data set, as well as the patterns of the 13\ncepstral coefficients. An accuracy of 70\\% was achieved using formants for the\nmentioned phonemes. For the MFCC analysis an accuracy of 52 \\% was achieved and\nan accuracy of 71\\% when /\\textipa{@}/ was ignored. The results obtained show\nthat the studied algorithms are far from mimicking the ability of\ndistinguishing subtle differences in sounds like human hearing does.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 02:31:03 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Patarroyo", "Keith Y.", ""], ["Vargas-Calder\u00f3n", "Vladimir", ""]]}, {"id": "1702.07092", "submitter": "Arman Cohan", "authors": "Arman Cohan, Allan Fong, Nazli Goharian, and Raj Ratwani", "title": "A Neural Attention Model for Categorizing Patient Safety Events", "comments": "ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical errors are leading causes of death in the US and as such, prevention\nof these errors is paramount to promoting health care. Patient Safety Event\nreports are narratives describing potential adverse events to the patients and\nare important in identifying and preventing medical errors. We present a neural\nnetwork architecture for identifying the type of safety events which is the\nfirst step in understanding these narratives. Our proposed model is based on a\nsoft neural attention model to improve the effectiveness of encoding long\nsequences. Empirical results on two large-scale real-world datasets of patient\nsafety reports demonstrate the effectiveness of our method with significant\nimprovements over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 04:27:49 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Cohan", "Arman", ""], ["Fong", "Allan", ""], ["Goharian", "Nazli", ""], ["Ratwani", "Raj", ""]]}, {"id": "1702.07117", "submitter": "Jarvan Law", "authors": "Jarvan Law, Hankz Hankui Zhuo, Junhua He and Erhu Rong (Dept. of\n  Computer Science, Sun Yat-Sen University, GuangZhou, China.)", "title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and\n  Vector Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been widely used in discovering latent topics which are\nshared across documents in text mining. Vector representations, word embeddings\nand topic embeddings, map words and topics into a low-dimensional and dense\nreal-value vector space, which have obtained high performance in NLP tasks.\nHowever, most of the existing models assume the result trained by one of them\nare perfect correct and used as prior knowledge for improving the other model.\nSome other models use the information trained from external large corpus to\nhelp improving smaller corpus. In this paper, we aim to build such an algorithm\nframework that makes topic models and vector representations mutually improve\neach other within the same corpus. An EM-style algorithm framework is employed\nto iteratively optimize both topic model and vector representations.\nExperimental results show that our model outperforms state-of-art methods on\nvarious NLP tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 07:16:03 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Law", "Jarvan", "", "Dept. of\n  Computer Science, Sun Yat-Sen University, GuangZhou, China."], ["Zhuo", "Hankz Hankui", "", "Dept. of\n  Computer Science, Sun Yat-Sen University, GuangZhou, China."], ["He", "Junhua", "", "Dept. of\n  Computer Science, Sun Yat-Sen University, GuangZhou, China."], ["Rong", "Erhu", "", "Dept. of\n  Computer Science, Sun Yat-Sen University, GuangZhou, China."]]}, {"id": "1702.07186", "submitter": "Derek Greene", "authors": "Mark Belford and Brian Mac Namee and Derek Greene", "title": "Stability of Topic Modeling via Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models can provide us with an insight into the underlying latent\nstructure of a large corpus of documents. A range of methods have been proposed\nin the literature, including probabilistic topic models and techniques based on\nmatrix factorization. However, in both cases, standard implementations rely on\nstochastic elements in their initialization phase, which can potentially lead\nto different results being generated on the same corpus when using the same\nparameter values. This corresponds to the concept of \"instability\" which has\npreviously been studied in the context of $k$-means clustering. In many\napplications of topic modeling, this problem of instability is not considered\nand topic models are treated as being definitive, even though the results may\nchange considerably if the initialization process is altered. In this paper we\ndemonstrate the inherent instability of popular topic modeling approaches,\nusing a number of new measures to assess stability. To address this issue in\nthe context of matrix factorization for topic modeling, we propose the use of\nensemble learning strategies. Based on experiments performed on annotated text\ncorpora, we show that a K-Fold ensemble strategy, combining both ensembles and\nstructured initialization, can significantly reduce instability, while\nsimultaneously yielding more accurate topic models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:00:10 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 17:06:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Belford", "Mark", ""], ["Mac Namee", "Brian", ""], ["Greene", "Derek", ""]]}, {"id": "1702.07203", "submitter": "Anoop Kunchukuttan", "authors": "Anoop Kunchukuttan, Maulik Shah, Pradyot Prakash, Pushpak\n  Bhattacharyya", "title": "Utilizing Lexical Similarity between Related, Low-resource Languages for\n  Pivot-based SMT", "comments": "Accepted at IJCNLP 2017, 7 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate pivot-based translation between related languages in a low\nresource, phrase-based SMT setting. We show that a subword-level pivot-based\nSMT model using a related pivot language is substantially better than word and\nmorpheme-level pivot models. It is also highly competitive with the best direct\ntranslation model, which is encouraging as no direct source-target training\ncorpus is used. We also show that combining multiple related language pivot\nmodels can rival a direct translation model. Thus, the use of subwords as\ntranslation units coupled with multiple related pivot languages can compensate\nfor the lack of a direct parallel corpus.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 13:13:53 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 20:55:03 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kunchukuttan", "Anoop", ""], ["Shah", "Maulik", ""], ["Prakash", "Pradyot", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1702.07285", "submitter": "Miguel Ballesteros", "authors": "Francesco Barbieri, Miguel Ballesteros, Horacio Saggion", "title": "Are Emojis Predictable?", "comments": "To appear at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emojis are ideograms which are naturally combined with plain text to visually\ncomplement or condense the meaning of a message. Despite being widely used in\nsocial media, their underlying semantics have received little attention from a\nNatural Language Processing standpoint. In this paper, we investigate the\nrelation between words and emojis, studying the novel task of predicting which\nemojis are evoked by text-based tweet messages. We train several models based\non Long Short-Term Memory networks (LSTMs) in this task. Our experimental\nresults show that our neural model outperforms two baselines as well as humans\nsolving the same task, suggesting that computational models are able to better\ncapture the underlying semantics of emojis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 16:47:01 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 12:59:19 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Barbieri", "Francesco", ""], ["Ballesteros", "Miguel", ""], ["Saggion", "Horacio", ""]]}, {"id": "1702.07324", "submitter": "Amanda Doucette", "authors": "Amanda Doucette", "title": "Inherent Biases of Recurrent Neural Networks for Phonological\n  Assimilation and Dissimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurrent neural network model of phonological pattern learning is\nproposed. The model is a relatively simple neural network with one recurrent\nlayer, and displays biases in learning that mimic observed biases in human\nlearning. Single-feature patterns are learned faster than two-feature patterns,\nand vowel or consonant-only patterns are learned faster than patterns involving\nvowels and consonants, mimicking the results of laboratory learning\nexperiments. In non-recurrent models, capturing these biases requires the use\nof alpha features or some other representation of repeated features, but with a\nrecurrent neural network, these elaborations are not necessary.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 18:19:35 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Doucette", "Amanda", ""]]}, {"id": "1702.07495", "submitter": "Shaohua Li", "authors": "Shaohua Li", "title": "Dirichlet-vMF Mixture Model", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is about the multi-document Von-Mises-Fisher mixture model with\na Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent\nDirichlet Allocation (LDA) in that they can capture the co-occurrence patterns\nacorss multiple documents. The difference is that in VMFMix, the topic-word\ndistribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix\nis used to derive topic embeddings, i.e., representative vectors, from multiple\nsets of embedding vectors. An efficient Variational Expectation-Maximization\ninference algorithm is derived. The performance of VMFMix on two document\nclassification tasks is reported, with some preliminary analysis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 08:35:10 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Li", "Shaohua", ""]]}, {"id": "1702.07507", "submitter": "Nafise Sadat Moosavi", "authors": "Nafise Sadat Moosavi and Michael Strube", "title": "Use Generalized Representations, But Do Not Forget Surface Features", "comments": "CORBON workshop@EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only a year ago, all state-of-the-art coreference resolvers were using an\nextensive amount of surface features. Recently, there was a paradigm shift\ntowards using word embeddings and deep neural networks, where the use of\nsurface features is very limited. In this paper, we show that a simple SVM\nmodel with surface features outperforms more complex neural models for\ndetecting anaphoric mentions. Our analysis suggests that using generalized\nrepresentations and surface features have different strength that should be\nboth taken into account for improving coreference resolution.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 09:26:10 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Moosavi", "Nafise Sadat", ""], ["Strube", "Michael", ""]]}, {"id": "1702.07680", "submitter": "Cem Sahin", "authors": "Cem Safak Sahin, Rajmonda S. Caceres, Brandon Oselio, William M.\n  Campbell", "title": "Consistent Alignment of Word Embedding Models", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models offer continuous vector representations that can\ncapture rich contextual semantics based on their word co-occurrence patterns.\nWhile these word vectors can provide very effective features used in many NLP\ntasks such as clustering similar words and inferring learning relationships,\nmany challenges and open research questions remain. In this paper, we propose a\nsolution that aligns variations of the same model (or different models) in a\njoint low-dimensional latent space leveraging carefully generated synthetic\ndata points. This generative process is inspired by the observation that a\nvariety of linguistic relationships is captured by simple linear operations in\nembedded space. We demonstrate that our approach can lead to substantial\nimprovements in recovering embeddings of local neighborhoods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:40:28 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Sahin", "Cem Safak", ""], ["Caceres", "Rajmonda S.", ""], ["Oselio", "Brandon", ""], ["Campbell", "William M.", ""]]}, {"id": "1702.07717", "submitter": "Liye Fu", "authors": "Liye Fu and Lillian Lee and Cristian Danescu-Niculescu-Mizil", "title": "When confidence and competence collide: Effects on online\n  decision-making discussions", "comments": "To appear in Proceedings of WWW 2017. Online multiplayer game\n  available at http://streetcrowd.us/start", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group discussions are a way for individuals to exchange ideas and arguments\nin order to reach better decisions than they could on their own. One of the\npremises of productive discussions is that better solutions will prevail, and\nthat the idea selection process is mediated by the (relative) competence of the\nindividuals involved. However, since people may not know their actual\ncompetence on a new task, their behavior is influenced by their self-estimated\ncompetence --- that is, their confidence --- which can be misaligned with their\nactual competence.\n  Our goal in this work is to understand the effects of confidence-competence\nmisalignment on the dynamics and outcomes of discussions. To this end, we\ndesign a large-scale natural setting, in the form of an online team-based\ngeography game, that allows us to disentangle confidence from competence and\nthus separate their effects.\n  We find that in task-oriented discussions, the more-confident individuals\nhave a larger impact on the group's decisions even when these individuals are\nat the same level of competence as their teammates. Furthermore, this\nunjustified role of confidence in the decision-making process often leads teams\nto under-perform. We explore this phenomenon by investigating the effects of\nconfidence on conversational dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 19:00:01 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 05:55:17 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Fu", "Liye", ""], ["Lee", "Lillian", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1702.07793", "submitter": "Yisen Wang", "authors": "Yisen Wang, Xuejiao Deng, Songbai Pu, Zhiheng Huang", "title": "Residual Convolutional CTC Networks for Automatic Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have been widely used in Automatic Speech\nRecognition (ASR) and they have achieved a significant accuracy improvement.\nEspecially, Convolutional Neural Networks (CNNs) have been revisited in ASR\nrecently. However, most CNNs used in existing work have less than 10 layers\nwhich may not be deep enough to capture all human speech signal information. In\nthis paper, we propose a novel deep and wide CNN architecture denoted as\nRCNN-CTC, which has residual connections and Connectionist Temporal\nClassification (CTC) loss function. RCNN-CTC is an end-to-end system which can\nexploit temporal and spectral structures of speech signals simultaneously.\nFurthermore, we introduce a CTC-based system combination, which is different\nfrom the conventional frame-wise senone-based one. The basic subsystems adopted\nin the combination are different types and thus mutually complementary to each\nother. Experimental results show that our proposed single system RCNN-CTC can\nachieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets,\ncompared to several widely used neural network systems in ASR. In addition, the\nproposed system combination can offer a further error reduction on these two\ndata sets, resulting in relative WER reductions of $14.91\\%$ and $6.52\\%$ on\nWSJ dev93 and Tencent Chat data sets respectively.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 22:49:13 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wang", "Yisen", ""], ["Deng", "Xuejiao", ""], ["Pu", "Songbai", ""], ["Huang", "Zhiheng", ""]]}, {"id": "1702.07825", "submitter": "Andrew Gibiansky", "authors": "Sercan O. Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew\n  Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman,\n  Shubho Sengupta, Mohammad Shoeybi", "title": "Deep Voice: Real-time Neural Text-to-Speech", "comments": "Submitted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 03:11:04 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 23:09:23 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Arik", "Sercan O.", ""], ["Chrzanowski", "Mike", ""], ["Coates", "Adam", ""], ["Diamos", "Gregory", ""], ["Gibiansky", "Andrew", ""], ["Kang", "Yongguo", ""], ["Li", "Xian", ""], ["Miller", "John", ""], ["Ng", "Andrew", ""], ["Raiman", "Jonathan", ""], ["Sengupta", "Shubho", ""], ["Shoeybi", "Mohammad", ""]]}, {"id": "1702.07826", "submitter": "Upol Ehsan", "authors": "Upol Ehsan, Brent Harrison, Larry Chan, Mark O. Riedl", "title": "Rationalization: A Neural Machine Translation Approach to Generating\n  Natural Language Explanations", "comments": "9 pages, 4 figures; added human evaluation section; added author;\n  changed author order-Upol Ehsan and Brent Harrison both contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AI rationalization, an approach for generating explanations of\nautonomous system behavior as if a human had performed the behavior. We\ndescribe a rationalization technique that uses neural machine translation to\ntranslate internal state-action representations of an autonomous agent into\nnatural language. We evaluate our technique in the Frogger game environment,\ntraining an autonomous game playing agent to rationalize its action choices\nusing natural language. A natural language training corpus is collected from\nhuman players thinking out loud as they play the game. We motivate the use of\nrationalization as an approach to explanation generation and show the results\nof two experiments evaluating the effectiveness of rationalization. Results of\nthese evaluations show that neural machine translation is able to accurately\ngenerate rationalizations that describe agent behavior, and that\nrationalizations are more satisfying to humans than other alternative methods\nof explanation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 03:20:49 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 05:20:09 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Ehsan", "Upol", ""], ["Harrison", "Brent", ""], ["Chan", "Larry", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1702.07835", "submitter": "Wajdi Zaghouani", "authors": "Wajdi Zaghouani", "title": "Critical Survey of the Freely Available Arabic Corpora", "comments": "Published in the Proceedings of the International Conference on\n  Language Resources and Evaluation (LREC'2014), OSACT Workshop. Reykjavik,\n  Iceland, 26-31 May 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of corpora is a major factor in building natural language\nprocessing applications. However, the costs of acquiring corpora can prevent\nsome researchers from going further in their endeavours. The ease of access to\nfreely available corpora is urgent needed in the NLP research community\nespecially for language such as Arabic. Currently, there is not easy was to\naccess to a comprehensive and updated list of freely available Arabic corpora.\nWe present in this paper, the results of a recent survey conducted to identify\nthe list of the freely available Arabic corpora and language resources. Our\npreliminary results showed an initial list of 66 sources. We presents our\nfindings in the various categories studied and we provided the direct links to\nget the data when possible.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 05:16:15 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Zaghouani", "Wajdi", ""]]}, {"id": "1702.07983", "submitter": "Yanran Li", "authors": "Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu\n  Song, Yoshua Bengio", "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the successes in capturing continuous distributions, the application\nof generative adversarial networks (GANs) to discrete settings, like natural\nlanguage tasks, is rather restricted. The fundamental reason is the difficulty\nof back-propagation through discrete random variables combined with the\ninherent instability of the GAN training objective. To address these problems,\nwe propose Maximum-Likelihood Augmented Discrete Generative Adversarial\nNetworks. Instead of directly optimizing the GAN objective, we derive a novel\nand low-variance objective using the discriminator's output that follows\ncorresponds to the log-likelihood. Compared with the original, the new\nobjective is proved to be consistent in theory and beneficial in practice. The\nexperimental results on various discrete datasets demonstrate the effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 03:19:13 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Che", "Tong", ""], ["Li", "Yanran", ""], ["Zhang", "Ruixiang", ""], ["Hjelm", "R Devon", ""], ["Li", "Wenjie", ""], ["Song", "Yangqiu", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1702.07998", "submitter": "Yinfei Yang Redfin Corportation", "authors": "Yinfei Yang, Forrest Sheng Bao, Ani Nenkova", "title": "Detecting (Un)Important Content for Single-Document News Summarization", "comments": "Accepted By EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust approach for detecting intrinsic sentence importance in\nnews, by training on two corpora of document-summary pairs. When used for\nsingle-document summarization, our approach, combined with the \"beginning of\ndocument\" heuristic, outperforms a state-of-the-art summarizer and the\nbeginning-of-article baseline in both automatic and manual evaluations. These\nresults represent an important advance because in the absence of cross-document\nrepetition, single document summarizers for news have not been able to\nconsistently outperform the strong beginning-of-article baseline.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 08:07:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Yang", "Yinfei", ""], ["Bao", "Forrest Sheng", ""], ["Nenkova", "Ani", ""]]}, {"id": "1702.08021", "submitter": "Viviana Patti", "authors": "Mirko Lai, Delia Iraz\\'u Hern\\'andez Far\\'ias, Viviana Patti, Paolo\n  Rosso", "title": "Friends and Enemies of Clinton and Trump: Using Context for Detecting\n  Stance in Political Tweets", "comments": "To appear in MICAI 2016 LNAI Proceedings", "journal-ref": null, "doi": "10.1007/978-3-319-62434-1_13", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance detection, the task of identifying the speaker's opinion towards a\nparticular target, has attracted the attention of researchers. This paper\ndescribes a novel approach for detecting stance in Twitter. We define a set of\nfeatures in order to consider the context surrounding a target of interest with\nthe final aim of training a model for predicting the stance towards the\nmentioned targets. In particular, we are interested in investigating political\ndebates in social media. For this reason we evaluated our approach focusing on\ntwo targets of the SemEval-2016 Task6 on Detecting stance in tweets, which are\nrelated to the political campaign for the 2016 U.S. presidential elections:\nHillary Clinton vs. Donald Trump. For the sake of comparison with the state of\nthe art, we evaluated our model against the dataset released in the\nSemEval-2016 Task 6 shared task competition. Our results outperform the best\nones obtained by participating teams, and show that information about enemies\nand friends of politicians help in detecting stance towards them.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 11:22:41 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Lai", "Mirko", ""], ["Far\u00edas", "Delia Iraz\u00fa Hern\u00e1ndez", ""], ["Patti", "Viviana", ""], ["Rosso", "Paolo", ""]]}, {"id": "1702.08139", "submitter": "Zichao Yang", "authors": "Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, Taylor Berg-Kirkpatrick", "title": "Improved Variational Autoencoders for Text Modeling using Dilated\n  Convolutions", "comments": "camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:16:01 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:31:34 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Yang", "Zichao", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1702.08217", "submitter": "Sreelekha S", "authors": "Sreelekha S, Pushpak Bhattacharyya", "title": "A case study on English-Malayalam Machine Translation", "comments": "This paper contains 10 pages with 4 figures and 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our work on a case study on Statistical Machine\nTranslation (SMT) and Rule based machine translation (RBMT) for translation\nfrom English to Malayalam and Malayalam to English. One of the motivations of\nour study is to make a three way performance comparison, such as, a) SMT and\nRBMT b) English to Malayalam SMT and Malayalam to English SMT c) English to\nMalayalam RBMT and Malayalam to English RBMT. We describe the development of\nEnglish to Malayalam and Malayalam to English baseline phrase based SMT system\nand the evaluation of its performance compared against the RBMT system. Based\non our study the observations are: a) SMT systems outperform RBMT systems, b)\nIn the case of SMT, English - Malayalam systems perform better than that of\nMalayalam - English systems, c) In the case RBMT, Malayalam to English systems\nare performing better than English to Malayalam systems. Based on our\nevaluations and detailed error analysis, we describe the requirements of\nincorporating morphological processing into the SMT to improve the accuracy of\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 10:09:46 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["S", "Sreelekha", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1702.08303", "submitter": "Joachim Bingel", "authors": "Joachim Bingel and Anders S{\\o}gaard", "title": "Identifying beneficial task relations for multi-task learning in deep\n  neural networks", "comments": "Accepted for publication at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-task learning (MTL) in deep neural networks for NLP has recently\nreceived increasing interest due to some compelling benefits, including its\npotential to efficiently regularize models and to reduce the need for labeled\ndata. While it has brought significant improvements in a number of NLP tasks,\nmixed results have been reported, and little is known about the conditions\nunder which MTL leads to gains in NLP. This paper sheds light on the specific\ntask relations that can lead to gains from MTL models over single-task setups.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 14:37:21 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bingel", "Joachim", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1702.08388", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Bo Wang, Maria Liakata, Rob Procter", "title": "Political Homophily in Independence Movements: Analysing and Classifying\n  Social Media Users by National Identity", "comments": "Accepted for publication in IEEE Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media and data mining are increasingly being used to analyse political\nand societal issues. Here we undertake the classification of social media users\nas supporting or opposing ongoing independence movements in their territories.\nIndependence movements occur in territories whose citizens have conflicting\nnational identities; users with opposing national identities will then support\nor oppose the sense of being part of an independent nation that differs from\nthe officially recognised country. We describe a methodology that relies on\nusers' self-reported location to build large-scale datasets for three\nterritories -- Catalonia, the Basque Country and Scotland. An analysis of these\ndatasets shows that homophily plays an important role in determining who people\nconnect with, as users predominantly choose to follow and interact with others\nfrom the same national identity. We show that a classifier relying on users'\nfollow networks can achieve accurate, language-independent classification\nperformances ranging from 85% to 97% for the three territories.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:19:03 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 18:01:10 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 11:48:11 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Wang", "Bo", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1702.08450", "submitter": "Mokhtar Boumedyen Billami", "authors": "Mokhtar Billami (LIF)", "title": "A Knowledge-Based Approach to Word Sense Disambiguation by\n  distributional selection and semantic features", "comments": "in French", "journal-ref": "22\\`eme Conf\\'erence sur le Traitement Automatique des Langues\n  Naturelles et 17\\`eme Rencontre des \\'Etudiants Chercheurs en Informatique\n  pour le Traitement Automatique des Langues, Jun 2015, CAEN, France. 22\\`eme\n  Conf\\'erence sur le Traitement Automatique des Langues Naturelles et 17\\`eme\n  Rencontre des \\'Etudiants Chercheurs en Informatique pour le Traitement\n  Automatique des Langues, pp.13--24, 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense disambiguation improves many Natural Language Processing (NLP)\napplications such as Information Retrieval, Information Extraction, Machine\nTranslation, or Lexical Simplification. Roughly speaking, the aim is to choose\nfor each word in a text its best sense. One of the most popular method\nestimates local semantic similarity relatedness between two word senses and\nthen extends it to all words from text. The most direct method computes a rough\nscore for every pair of word senses and chooses the lexical chain that has the\nbest score (we can imagine the exponential complexity that returns this\ncomprehensive approach). In this paper, we propose to use a combinatorial\noptimization metaheuristic for choosing the nearest neighbors obtained by\ndistributional selection around the word to disambiguate. The test and the\nevaluation of our method concern a corpus written in French by means of the\nsemantic network BabelNet. The obtained accuracy rate is 78 % on all names and\nverbs chosen for the evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 13:37:15 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Billami", "Mokhtar", "", "LIF"]]}, {"id": "1702.08451", "submitter": "Mokhtar Boumedyen Billami", "authors": "Mokhtar Billami (LIF), N\\'uria Gala (LIF)", "title": "Approches d'analyse distributionnelle pour am\\'eliorer la\n  d\\'esambigu\\\"isation s\\'emantique", "comments": "in French, Journ\\'ees internationales d'Analyse statistique des\n  Donn\\'ees Textuelles (JADT), Jun 2016, Nice, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense disambiguation (WSD) improves many Natural Language Processing\n(NLP) applications such as Information Retrieval, Machine Translation or\nLexical Simplification. WSD is the ability of determining a word sense among\ndifferent ones within a polysemic lexical unit taking into account the context.\nThe most straightforward approach uses a semantic proximity measure between the\nword sense candidates of the target word and those of its context. Such a\nmethod very easily entails a combinatorial explosion. In this paper, we propose\ntwo methods based on distributional analysis which enable to reduce the\nexponential complexity without losing the coherence. We present a comparison\nbetween the selection of distributional neighbors and the linearly nearest\nneighbors. The figures obtained show that selecting distributional neighbors\nleads to better results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 13:38:08 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Billami", "Mokhtar", "", "LIF"], ["Gala", "N\u00faria", "", "LIF"]]}, {"id": "1702.08563", "submitter": "John Lalor", "authors": "John P. Lalor, Hao Wu, Hong Yu", "title": "Soft Label Memorization-Generalization for Natural Language Inference", "comments": "Extended version of work presented at UAI UDL 2018 workshop. 8 pages\n  plus references, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often when multiple labels are obtained for a training example it is assumed\nthat there is an element of noise that must be accounted for. It has been shown\nthat this disagreement can be considered signal instead of noise. In this work\nwe investigate using soft labels for training data to improve generalization in\nmachine learning models. However, using soft labels for training Deep Neural\nNetworks (DNNs) is not practical due to the costs involved in obtaining\nmultiple labels for large data sets. We propose soft label\nmemorization-generalization (SLMG), a fine-tuning approach to using soft labels\nfor training DNNs. We assume that differences in labels provided by human\nannotators represent ambiguity about the true label instead of noise.\nExperiments with SLMG demonstrate improved generalization performance on the\nNatural Language Inference (NLI) task. Our experiments show that by injecting a\nsmall percentage of soft label training data (0.03% of training set size) we\ncan improve generalization performance over several baselines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:25:45 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 23:59:15 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 15:17:48 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Lalor", "John P.", ""], ["Wu", "Hao", ""], ["Yu", "Hong", ""]]}, {"id": "1702.08653", "submitter": "Asli Celikyilmaz", "authors": "Asli Celikyilmaz and Li Deng and Lihong Li and Chong Wang", "title": "Scaffolding Networks: Incremental Learning and Teaching Through\n  Questioning", "comments": "11 pages + Abstract + 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm of learning for reasoning, understanding, and\nprediction, as well as the scaffolding network to implement this paradigm. The\nscaffolding network embodies an incremental learning approach that is\nformulated as a teacher-student network architecture to teach machines how to\nunderstand text and do reasoning. The key to our computational scaffolding\napproach is the interactions between the teacher and the student through\nsequential questioning. The student observes each sentence in the text\nincrementally, and it uses an attention-based neural net to discover and\nregister the key information in relation to its current memory. Meanwhile, the\nteacher asks questions about the observed text, and the student network gets\nrewarded by correctly answering these questions. The entire network is updated\ncontinually using reinforcement learning. Our experimental results on synthetic\nand real datasets show that the scaffolding network not only outperforms\nstate-of-the-art methods but also learns to do reasoning in a scalable way even\nwith little human generated input.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 05:43:10 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 19:45:43 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Celikyilmaz", "Asli", ""], ["Deng", "Li", ""], ["Li", "Lihong", ""], ["Wang", "Chong", ""]]}, {"id": "1702.08866", "submitter": "Marina Sokolova", "authors": "Marina Sokolova, Vera Sazonova, Kanyi Huang, Rudraneel Chakraboty,\n  Stan Matwin", "title": "Studying Positive Speech on Twitter", "comments": "13 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results of empirical studies on positive speech on Twitter. By\npositive speech we understand speech that works for the betterment of a given\nsituation, in this case relations between different communities in a\nconflict-prone country. We worked with four Twitter data sets. Through\nsemi-manual opinion mining, we found that positive speech accounted for < 1% of\nthe data . In fully automated studies, we tested two approaches: unsupervised\nstatistical analysis, and supervised text classification based on distributed\nword representation. We discuss benefits and challenges of those approaches and\nreport empirical evidence obtained in the study.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 21:49:45 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Sokolova", "Marina", ""], ["Sazonova", "Vera", ""], ["Huang", "Kanyi", ""], ["Chakraboty", "Rudraneel", ""], ["Matwin", "Stan", ""]]}]