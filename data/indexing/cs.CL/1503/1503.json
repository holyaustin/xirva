[{"id": "1503.00030", "submitter": "Daniel Fern\\'andez-Gonz\\'alez", "authors": "Daniel Fern\\'andez-Gonz\\'alez and Andr\\'e F. T. Martins", "title": "Parsing as Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce phrase-representation parsing to dependency parsing. Our reduction\nis grounded on a new intermediate representation, \"head-ordered dependency\ntrees\", shown to be isomorphic to constituent trees. By encoding order\ninformation in the dependency labels, we show that any off-the-shelf, trainable\ndependency parser can be used to produce constituents. When this parser is\nnon-projective, we can perform discontinuous parsing in a very natural manner.\nDespite the simplicity of our approach, experiments show that the resulting\nparsers are on par with strong baselines, such as the Berkeley parser for\nEnglish and the best single system in the SPMRL-2014 shared task. Results are\nparticularly striking for discontinuous parsing of German, where we surpass the\ncurrent state of the art by a wide margin.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 22:52:37 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Fern\u00e1ndez-Gonz\u00e1lez", "Daniel", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "1503.00064", "submitter": "Sanja Fidler", "authors": "Dahua Lin, Chen Kong, Sanja Fidler, Raquel Urtasun", "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for generating lingual descriptions of\nindoor scenes. Whereas substantial efforts have been made to tackle this\nproblem, previous approaches focusing primarily on generating a single sentence\nfor each image, which is not sufficient for describing complex scenes. We\nattempt to go beyond this, by generating coherent descriptions with multiple\nsentences. Our approach is distinguished from conventional ones in several\naspects: (1) a 3D visual parsing system that jointly infers objects,\nattributes, and relations; (2) a generative grammar learned automatically from\ntraining text; and (3) a text generation algorithm that takes into account the\ncoherence among sentences. Experiments on the augmented NYU-v2 dataset show\nthat our framework can generate natural descriptions with substantially higher\nROGUE scores compared to those produced by the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 04:26:21 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Dahua", ""], ["Kong", "Chen", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1503.00075", "submitter": "Kai Sheng Tai", "authors": "Kai Sheng Tai, Richard Socher, Christopher D. Manning", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:31:50 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 20:13:25 GMT"}, {"version": "v3", "created": "Sat, 30 May 2015 06:51:20 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Tai", "Kai Sheng", ""], ["Socher", "Richard", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1503.00095", "submitter": "Kazuma Hashimoto", "authors": "Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, Yoshimasa Tsuruoka", "title": "Task-Oriented Learning of Word Embeddings for Semantic Relation\n  Classification", "comments": "The Nineteenth Conference on Computational Natural Language Learning\n  (CoNLL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning method for word embeddings designed for relation\nclassification. Our word embeddings are trained by predicting words between\nnoun pairs using lexical relation-specific features on a large unlabeled\ncorpus. This allows us to explicitly incorporate relation-specific information\ninto the word embeddings. The learned word embeddings are then used to\nconstruct feature vectors for a relation classification model. On a\nwell-established semantic relation classification task, our method\nsignificantly outperforms a baseline based on a previously introduced word\nembedding method, and compares favorably to previous state-of-the-art models\nthat use syntactic information or manually constructed external resources.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:59:59 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 00:04:42 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 13:48:30 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Hashimoto", "Kazuma", ""], ["Stenetorp", "Pontus", ""], ["Miwa", "Makoto", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1503.00107", "submitter": "Shujian Huang", "authors": "Shujian Huang and Huadong Chen and Xinyu Dai and Jiajun Chen", "title": "Non-linear Learning for Statistical Machine Translation", "comments": "submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical machine translation (SMT) systems usually use a linear\ncombination of features to model the quality of each translation hypothesis.\nThe linear combination assumes that all the features are in a linear\nrelationship and constrains that each feature interacts with the rest features\nin an linear manner, which might limit the expressive power of the model and\nlead to a under-fit model on the current data. In this paper, we propose a\nnon-linear modeling for the quality of translation hypotheses based on neural\nnetworks, which allows more complex interaction between features. A learning\nframework is presented for training the non-linear models. We also discuss\npossible heuristics in designing the network structure which may improve the\nnon-linear learning performance. Experimental results show that with the basic\nfeatures of a hierarchical phrase-based machine translation system, our method\nproduce translations that are better than a linear model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 09:53:32 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Huang", "Shujian", ""], ["Chen", "Huadong", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1503.00168", "submitter": "Jiwei Li", "authors": "Jiwei Li and Eduard Hovy", "title": "The NLP Engine: A Universal Turing Machine for NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly accepted that machine translation is a more complex task than\npart of speech tagging. But how much more complex? In this paper we make an\nattempt to develop a general framework and methodology for computing the\ninformational and/or processing complexity of NLP applications and tasks. We\ndefine a universal framework akin to a Turning Machine that attempts to fit\n(most) NLP tasks into one paradigm. We calculate the complexities of various\nNLP tasks using measures of Shannon Entropy, and compare `simple' ones such as\npart of speech tagging to `complex' ones such as machine translation. This\npaper provides a first, though far from perfect, attempt to quantify NLP tasks\nunder a uniform paradigm. We point out current deficiencies and suggest some\navenues for fruitful research.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 19:46:50 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Li", "Jiwei", ""], ["Hovy", "Eduard", ""]]}, {"id": "1503.00185", "submitter": "Jiwei Li", "authors": "Jiwei Li, Minh-Thang Luong, Dan Jurafsky and Eudard Hovy", "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural models, which use syntactic parse trees to recursively\ngenerate representations bottom-up, are a popular architecture. But there have\nnot been rigorous evaluations showing for exactly which tasks this syntax-based\nmethod is appropriate. In this paper we benchmark {\\bf recursive} neural models\nagainst sequential {\\bf recurrent} neural models (simple recurrent and LSTM\nmodels), enforcing apples-to-apples comparison as much as possible. We\ninvestigate 4 tasks: (1) sentiment classification at the sentence level and\nphrase level; (2) matching questions to answer-phrases; (3) discourse parsing;\n(4) semantic relation extraction (e.g., {\\em component-whole} between nouns).\n  Our goal is to understand better when, and why, recursive models can\noutperform simpler models. We find that recursive models help mainly on tasks\n(like semantic relation extraction) that require associating headwords across a\nlong distance, particularly on very long sequences. We then introduce a method\nfor allowing recurrent models to achieve similar performance: breaking long\nsentences into clause-like units at punctuation and processing them separately\nbefore combining. Our results thus help understand the limitations of both\nclasses of models, and suggest directions for improving recurrent models.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 21:39:31 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 18:16:50 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 17:14:49 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2015 22:07:45 GMT"}, {"version": "v5", "created": "Tue, 18 Aug 2015 05:59:18 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Li", "Jiwei", ""], ["Luong", "Minh-Thang", ""], ["Jurafsky", "Dan", ""], ["Hovy", "Eudard", ""]]}, {"id": "1503.00339", "submitter": "Vladislav Kargin", "authors": "Vladislav Kargin", "title": "Variation of word frequencies in Russian literary texts", "comments": "17 pages", "journal-ref": null, "doi": "10.1016/j.physa.2015.11.014", "report-no": null, "categories": "cs.CL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variation of word frequencies in Russian literary texts. Our\nfindings indicate that the standard deviation of a word's frequency across\ntexts depends on its average frequency according to a power law with exponent\n$0.62,$ showing that the rarer words have a relatively larger degree of\nfrequency volatility (i.e., \"burstiness\").\n  Several latent factors models have been estimated to investigate the\nstructure of the word frequency distribution. The dependence of a word's\nfrequency volatility on its average frequency can be explained by the asymmetry\nin the distribution of latent factors.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 19:37:27 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 17:51:21 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Kargin", "Vladislav", ""]]}, {"id": "1503.00693", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Noah A. Smith", "title": "Bayesian Optimization of Text Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying machine learning to problems in NLP, there are many choices to\nmake about how to represent input texts. These choices can have a big effect on\nperformance, but they are often uninteresting to researchers or practitioners\nwho simply need a module that performs well. We propose an approach to\noptimizing over this space of choices, formulating the problem as global\noptimization. We apply a sequential model-based optimization technique and show\nthat our method makes standard linear models competitive with more\nsophisticated, expensive state-of-the-art methods based on latent variable\nmodels or neural networks on various topic classification and sentiment\nanalysis problems. Our approach is a first step towards black-box NLP systems\nthat work with raw text and do not require manual tuning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:23:18 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Yogatama", "Dani", ""], ["Smith", "Noah A.", ""]]}, {"id": "1503.00841", "submitter": "Biao Liu", "authors": "Biao Liu, Minlie Huang", "title": "Robustly Leveraging Prior Knowledge in Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Prior knowledge has been shown very useful to address many natural language\nprocessing tasks. Many approaches have been proposed to formalise a variety of\nknowledge, however, whether the proposed approach is robust or sensitive to the\nknowledge supplied to the model has rarely been discussed. In this paper, we\npropose three regularization terms on top of generalized expectation criteria,\nand conduct extensive experiments to justify the robustness of the proposed\nmethods. Experimental results demonstrate that our proposed methods obtain\nremarkable improvements and are much more robust than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 06:59:28 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Liu", "Biao", ""], ["Huang", "Minlie", ""]]}, {"id": "1503.01129", "submitter": "Marcelo A. Montemurro", "authors": "Marcelo A Montemurro and Dami\\'an H Zanette", "title": "Complexity and universality in the long-range order of words", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As is the case of many signals produced by complex systems, language presents\na statistical structure that is balanced between order and disorder. Here we\nreview and extend recent results from quantitative characterisations of the\ndegree of order in linguistic sequences that give insights into two relevant\naspects of language: the presence of statistical universals in word ordering,\nand the link between semantic information and the statistical linguistic\nstructure. We first analyse a measure of relative entropy that assesses how\nmuch the ordering of words contributes to the overall statistical structure of\nlanguage. This measure presents an almost constant value close to 3.5 bits/word\nacross several linguistic families. Then, we show that a direct application of\ninformation theory leads to an entropy measure that can quantify and extract\nsemantic structures from linguistic samples, even without prior knowledge of\nthe underlying language.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 21:18:05 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Montemurro", "Marcelo A", ""], ["Zanette", "Dami\u00e1n H", ""]]}, {"id": "1503.01180", "submitter": "Chenhao Tan", "authors": "Chenhao Tan and Lillian Lee", "title": "All Who Wander: On the Prevalence and Characteristics of Multi-community\n  Engagement", "comments": "11 pages, data available at\n  https://chenhaot.com/pages/multi-community.html, Proceedings of WWW 2015\n  (updated references)", "journal-ref": null, "doi": "10.1145/2736277.2741661", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although analyzing user behavior within individual communities is an active\nand rich research domain, people usually interact with multiple communities\nboth on- and off-line. How do users act in such multi-community environments?\nAlthough there are a host of intriguing aspects to this question, it has\nreceived much less attention in the research community in comparison to the\nintra-community case. In this paper, we examine three aspects of\nmulti-community engagement: the sequence of communities that users post to, the\nlanguage that users employ in those communities, and the feedback that users\nreceive, using longitudinal posting behavior on Reddit as our main data source,\nand DBLP for auxiliary experiments. We also demonstrate the effectiveness of\nfeatures drawn from these aspects in predicting users' future level of\nactivity.\n  One might expect that a user's trajectory mimics the \"settling-down\" process\nin real life: an initial exploration of sub-communities before settling down\ninto a few niches. However, we find that the users in our data continually post\nin new communities; moreover, as time goes on, they post increasingly evenly\namong a more diverse set of smaller communities. Interestingly, it seems that\nusers that eventually leave the community are \"destined\" to do so from the very\nbeginning, in the sense of showing significantly different \"wandering\" patterns\nvery early on in their trajectories; this finding has potentially important\ndesign implications for community maintainers. Our multi-community perspective\nalso allows us to investigate the \"situation vs. personality\" debate from\nlanguage usage across different communities.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:05:41 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 20:00:27 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Tan", "Chenhao", ""], ["Lee", "Lillian", ""]]}, {"id": "1503.01190", "submitter": "Michael Bloodgood", "authors": "Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr,\n  Lori Levin, Christine D. Piatko, Owen Rambow and Benjamin Van Durme", "title": "Statistical modality tagging from rule-based annotations and\n  crowdsourcing", "comments": "8 pages, 6 tables; appeared in Proceedings of the Workshop on\n  Extra-Propositional Aspects of Meaning in Computational Linguistics, July\n  2012; In Proceedings of the Workshop on Extra-Propositional Aspects of\n  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,\n  July 2012. Association for Computational Linguistics", "journal-ref": "In Proceedings of the Workshop on Extra-Propositional Aspects of\n  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,\n  July 2012. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore training an automatic modality tagger. Modality is the attitude\nthat a speaker might have toward an event or state. One of the main hurdles for\ntraining a linguistic tagger is gathering training data. This is particularly\nproblematic for training a tagger for modality because modality triggers are\nsparse for the overwhelming majority of sentences. We investigate an approach\nto automatically training a modality tagger where we first gathered sentences\nbased on a high-recall simple rule-based modality tagger and then provided\nthese sentences to Mechanical Turk annotators for further annotation. We used\nthe resulting set of training data to train a precise modality tagger using a\nmulti-class SVM that delivers good performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:34:36 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Prabhakaran", "Vinodkumar", ""], ["Bloodgood", "Michael", ""], ["Diab", "Mona", ""], ["Dorr", "Bonnie", ""], ["Levin", "Lori", ""], ["Piatko", "Christine D.", ""], ["Rambow", "Owen", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1503.01258", "submitter": "Mariia Rubtcova V", "authors": "Oleg V. Pavenkov and Vladimir G. Pavenkov and Mariia V. Rubtcova", "title": "The concept \"altruism\" for sociological research: from conceptualization\n  to operationalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the question of the relevant conceptualization of\n{\\guillemotleft}altruism{\\guillemotright} in Russian from the perspective\nsociological research operationalization. It investigates the spheres of social\napplication of the word {\\guillemotleft}altruism{\\guillemotright}, include\nRussian equivalent {\\guillemotleft}vzaimopomoshh`{\\guillemotright} (mutual\nhelp). The data for the study comes from Russian National Corpus (Russian). The\ntheoretical framework consists of Paul F. Lazarsfeld`s Theory of Sociological\nResearch Methodology and the Natural Semantic Metalanguage (NSM). Quantitative\nanalysis shows features in the representation of altruism in Russian that\nsociologists need to know in the preparation of questionnaires, interview\nguides and analysis of transcripts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 09:27:13 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Pavenkov", "Oleg V.", ""], ["Pavenkov", "Vladimir G.", ""], ["Rubtcova", "Mariia V.", ""]]}, {"id": "1503.01397", "submitter": "David Belanger", "authors": "Luke Vilnis and David Belanger and Daniel Sheldon and Andrew McCallum", "title": "Bethe Projections for Non-Local Inference", "comments": "minor bug fix to appendix. appeared in UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems in structured prediction are naturally solved by\naugmenting a tractable dependency structure with complex, non-local auxiliary\nobjectives. This includes the mean field family of variational inference\nalgorithms, soft- or hard-constrained inference using Lagrangian relaxation or\nlinear programming, collective graphical models, and forms of semi-supervised\nlearning such as posterior regularization. We present a method to\ndiscriminatively learn broad families of inference objectives, capturing\npowerful non-local statistics of the latent variables, while maintaining\ntractable and provably fast inference using non-Euclidean projected gradient\ndescent with a distance-generating function given by the Bethe entropy. We\ndemonstrate the performance and flexibility of our method by (1) extracting\nstructured citations from research papers by learning soft global constraints,\n(2) achieving state-of-the-art results on a widely-used handwriting recognition\ntask using a novel learned non-convex inference procedure, and (3) providing a\nfast and highly scalable algorithm for the challenging problem of inference in\na collective graphical model applied to bird migration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 17:36:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 07:32:25 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 18:44:53 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Vilnis", "Luke", ""], ["Belanger", "David", ""], ["Sheldon", "Daniel", ""], ["McCallum", "Andrew", ""]]}, {"id": "1503.01549", "submitter": "Wesam Elshamy", "authors": "William Hsu, Mohammed Abduljabbar, Ryuichi Osuga, Max Lu, Wesam\n  Elshamy", "title": "Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping\n  and Data Mining Research Directions", "comments": "In Proceedings of The 2nd European Workshop on Human-Computer\n  Interaction and Information Retrieval EuroHCIR2012, pages 43--46, Nijmegen,\n  the Netherlands, 24th/25th August 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of spatiotemporal event visualization based on reports entails\nsubtasks ranging from named entity recognition to relationship extraction and\nmapping of events. We present an approach to event extraction that is driven by\ndata mining and visualization goals, particularly thematic mapping and trend\nanalysis. This paper focuses on bridging the information extraction and\nvisualization tasks and investigates topic modeling approaches. We develop a\nstatic, finite topic model and examine the potential benefits and feasibility\nof extending this to dynamic topic modeling with a large number of topics and\ncontinuous time. We describe an experimental test bed for event mapping that\nuses this end-to-end information retrieval system, and report preliminary\nresults on a geoinformatics problem: tracking of methamphetamine lab seizure\nevents across time and space.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 06:22:15 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Hsu", "William", ""], ["Abduljabbar", "Mohammed", ""], ["Osuga", "Ryuichi", ""], ["Lu", "Max", ""], ["Elshamy", "Wesam", ""]]}, {"id": "1503.01558", "submitter": "Jonathan Malmaud", "authors": "Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew\n  Rabinovich, and Kevin Murphy", "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and\n  Vision", "comments": "To appear in NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for aligning a sequence of instructions to a video\nof someone carrying out a task. In particular, we focus on the cooking domain,\nwhere the instructions correspond to the recipe. Our technique relies on an HMM\nto align the recipe steps to the (automatically generated) speech transcript.\nWe then refine this alignment using a state-of-the-art visual food detector,\nbased on a deep convolutional neural network. We show that our technique\noutperforms simpler techniques based on keyword spotting. It also enables\ninteresting applications, such as automatically illustrating recipes with\nkeyframes, and searching within a video for events of interest.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 07:07:48 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 04:11:49 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2015 18:55:22 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Malmaud", "Jonathan", ""], ["Huang", "Jonathan", ""], ["Rathod", "Vivek", ""], ["Johnston", "Nick", ""], ["Rabinovich", "Andrew", ""], ["Murphy", "Kevin", ""]]}, {"id": "1503.01655", "submitter": "Eneko Agirre", "authors": "Eneko Agirre, Ander Barrena and Aitor Soroa", "title": "Studying the Wikipedia Hyperlink Graph for Relatedness and\n  Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperlinks and other relations in Wikipedia are a extraordinary resource\nwhich is still not fully understood. In this paper we study the different types\nof links in Wikipedia, and contrast the use of the full graph with respect to\njust direct links. We apply a well-known random walk algorithm on two tasks,\nword relatedness and named-entity disambiguation. We show that using the full\ngraph is more effective than just direct links by a large margin, that\nnon-reciprocal links harm performance, and that there is no benefit from\ncategories and infoboxes, with coherent results on both tasks. We set new\nstate-of-the-art figures for systems based on Wikipedia links, comparable to\nsystems exploiting several information sources and/or supervised machine\nlearning. Our approach is open source, with instruction to reproduce results,\nand amenable to be integrated with complementary text-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 15:08:21 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 23:10:41 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Agirre", "Eneko", ""], ["Barrena", "Ander", ""], ["Soroa", "Aitor", ""]]}, {"id": "1503.01838", "submitter": "Fandong Meng", "authors": "Fandong Meng and Zhengdong Lu and Mingxuan Wang and Hang Li and Wenbin\n  Jiang and Qun Liu", "title": "Encoding Source Language with Convolutional Neural Network for Machine\n  Translation", "comments": "Accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed neural network joint model (NNJM) (Devlin et al., 2014)\naugments the n-gram target language model with a heuristically chosen source\ncontext window, achieving state-of-the-art performance in SMT. In this paper,\nwe give a more systematic treatment by summarizing the relevant source\ninformation through a convolutional architecture guided by the target\ninformation. With different guiding signals during decoding, our specifically\ndesigned convolution+gating architectures can pinpoint the parts of a source\nsentence that are relevant to predicting a target word, and fuse them with the\ncontext of entire source sentence to form a unified representation. This\nrepresentation, together with target language words, are fed to a deep neural\nnetwork (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English\ntranslation tasks show that the proposed model can achieve significant\nimprovements over the previous NNJM by up to +1.08 BLEU points on average\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 03:04:54 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 08:28:32 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 01:34:58 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 10:07:40 GMT"}, {"version": "v5", "created": "Mon, 8 Jun 2015 09:04:14 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Meng", "Fandong", ""], ["Lu", "Zhengdong", ""], ["Wang", "Mingxuan", ""], ["Li", "Hang", ""], ["Jiang", "Wenbin", ""], ["Liu", "Qun", ""]]}, {"id": "1503.02108", "submitter": "Zhen Huang", "authors": "Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jiadong Wu, and\n  Chin-Hui Lee", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian approach to adapting parameters of a well-trained\ncontext-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to\nimprove automatic speech recognition performance. Given an abundance of DNN\nparameters but with only a limited amount of data, the effectiveness of the\nadapted DNN model can often be compromised. We formulate maximum a posteriori\n(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an\naugmented linear hidden networks connected to the output tied states, or\nsenones, and compare it to feature space MAP linear regression previously\nproposed. Experimental evidences on the 20,000-word open vocabulary Wall Street\nJournal task demonstrate the feasibility of the proposed framework. In\nsupervised adaptation, the proposed MAP adaptation approach provides more than\n10% relative error reduction and consistently outperforms the conventional\ntransformation based methods. Furthermore, we present an initial attempt to\ngenerate hierarchical priors to improve adaptation efficiency and effectiveness\nwith limited adaptation data by exploiting similarities among senones.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 22:48:29 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 04:53:53 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Huang", "Zhen", ""], ["Siniscalchi", "Sabato Marco", ""], ["Chen", "I-Fan", ""], ["Wu", "Jiadong", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "1503.02120", "submitter": "Jake Williams", "authors": "Jake Ryland Williams, Eric M. Clark, James P. Bagrow, Christopher M.\n  Danforth, and Peter Sheridan Dodds", "title": "Identifying missing dictionary entries with frequency-conserving context\n  models", "comments": "16 pages, 6 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to better understand meaning from natural language texts, we\nexplore methods aimed at organizing lexical objects into contexts. A number of\nthese methods for organization fall into a family defined by word ordering.\nUnlike demographic or spatial partitions of data, these collocation models are\nof special importance for their universal applicability. While we are\ninterested here in text and have framed our treatment appropriately, our work\nis potentially applicable to other areas of research (e.g., speech, genomics,\nand mobility patterns) where one has ordered categorical data, (e.g., sounds,\ngenes, and locations). Our approach focuses on the phrase (whether word or\nlarger) as the primary meaning-bearing lexical unit and object of study. To do\nso, we employ our previously developed framework for generating word-conserving\nphrase-frequency data. Upon training our model with the Wiktionary---an\nextensive, online, collaborative, and open-source dictionary that contains over\n100,000 phrasal-definitions---we develop highly effective filters for the\nidentification of meaningful, missing phrase-entries. With our predictions we\nthen engage the editorial community of the Wiktionary and propose short lists\nof potential missing entries for definition, developing a breakthrough, lexical\nextraction technique, and expanding our knowledge of the defined English\nlexicon of phrases.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 02:45:06 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 18:59:35 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 01:36:24 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Williams", "Jake Ryland", ""], ["Clark", "Eric M.", ""], ["Bagrow", "James P.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1503.02335", "submitter": "Karthik Narasimhan", "authors": "Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola", "title": "An Unsupervised Method for Uncovering Morphological Chains", "comments": "11 pages, Appearing in the Transactions of the Association for\n  Computational Linguistics (TACL), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art systems today produce morphological analysis based only\non orthographic patterns. In contrast, we propose a model for unsupervised\nmorphological analysis that integrates orthographic and semantic views of\nwords. We model word formation in terms of morphological chains, from base\nwords to the observed words, breaking the chains into parent-child relations.\nWe use log-linear models with morpheme and word-level features to predict\npossible parents, including their modifications, for each word. The limited set\nof candidate parents for each word render contrastive estimation feasible. Our\nmodel consistently matches or outperforms five state-of-the-art systems on\nArabic, English and Turkish.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 22:18:30 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Narasimhan", "Karthik", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1503.02357", "submitter": "Zhaopeng Tu", "authors": "Zhaopeng Tu, Baotian Hu, Zhengdong Lu, and Hang Li", "title": "Context-Dependent Translation Selection Using Convolutional Neural\n  Network", "comments": "Short version is accepted by ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for translation selection in statistical machine\ntranslation, in which a convolutional neural network is employed to judge the\nsimilarity between a phrase pair in two languages. The specifically designed\nconvolutional architecture encodes not only the semantic similarity of the\ntranslation pair, but also the context containing the phrase in the source\nlanguage. Therefore, our approach is able to capture context-dependent semantic\nsimilarities of translation pairs. We adopt a curriculum learning strategy to\ntrain the model: we classify the training examples into easy, medium, and\ndifficult categories, and gradually build the ability of representing phrase\nand sentence level context by using training examples from easy to difficult.\nExperimental results show that our approach significantly outperforms the\nbaseline system by up to 1.4 BLEU points.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 02:16:19 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 01:07:40 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Tu", "Zhaopeng", ""], ["Hu", "Baotian", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1503.02364", "submitter": "Lifeng Shang", "authors": "Lifeng Shang, Zhengdong Lu, Hang Li", "title": "Neural Responding Machine for Short-Text Conversation", "comments": "accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Responding Machine (NRM), a neural network-based response\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\nframework: it formalizes the generation of response as a decoding process based\non the latent representation of the input text, while both encoding and\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\nwith a large amount of one-round conversation data collected from a\nmicroblogging service. Empirical study shows that NRM can generate\ngrammatically correct and content-wise appropriate responses to over 75% of the\ninput text, outperforming state-of-the-arts in the same setting, including\nretrieval-based and SMT-based models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 02:54:29 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 02:28:58 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Shang", "Lifeng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1503.02417", "submitter": "Ehsan Shareghi", "authors": "Ehsan Shareghi, Gholamreza Haffari, Trevor Cohn, Ann Nicholson", "title": "Structured Prediction of Sequences and Trees using Infinite Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic structures exhibit a rich array of global phenomena, however\ncommonly used Markov models are unable to adequately describe these phenomena\ndue to their strong locality assumptions. We propose a novel hierarchical model\nfor structured prediction over sequences and trees which exploits global\ncontext by conditioning each generation decision on an unbounded context of\nprior decisions. This builds on the success of Markov models but without\nimposing a fixed bound in order to better represent global phenomena. To\nfacilitate learning of this large and unbounded model, we use a hierarchical\nPitman-Yor process prior which provides a recursive form of smoothing. We\npropose prediction algorithms based on A* and Markov Chain Monte Carlo\nsampling. Empirical results demonstrate the potential of our model compared to\nbaseline finite-context Markov models on part-of-speech tagging and syntactic\nparsing.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 10:35:10 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Shareghi", "Ehsan", ""], ["Haffari", "Gholamreza", ""], ["Cohn", "Trevor", ""], ["Nicholson", "Ann", ""]]}, {"id": "1503.02427", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang and Zhengdong Lu and Hang Li and Qun Liu", "title": "Syntax-based Deep Matching of Short Texts", "comments": "Accepted by IJCAI-2015 as full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in natural language processing, ranging from machine translation\nto question answering, can be reduced to the problem of matching two sentences\nor more generally two short texts. We propose a new approach to the problem,\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\napproach consists of two components, 1) a mining algorithm to discover patterns\nfor matching two short-texts, defined in the product space of dependency trees,\nand 2) a deep neural network for matching short texts using the mined patterns,\nas well as a learning algorithm to build the network having a sparse structure.\nWe test our algorithm on the problem of matching a tweet and a response in\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\none without using dependency trees and one based on word-embedding, all with\nlarge margins\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 11:11:15 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 03:24:58 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 08:31:01 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 04:48:25 GMT"}, {"version": "v5", "created": "Mon, 18 May 2015 13:26:28 GMT"}, {"version": "v6", "created": "Fri, 12 Jun 2015 08:26:01 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Liu", "Qun", ""]]}, {"id": "1503.02510", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Compositional Distributional Semantics with Long Short Term Memory", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proposing an extension of the recursive neural network that makes use\nof a variant of the long short-term memory architecture. The extension allows\ninformation low in parse trees to be stored in a memory register (the `memory\ncell') and used much later higher up in the parse tree. This provides a\nsolution to the vanishing gradient problem and allows the network to capture\nlong range dependencies. Experimental results show that our composition\noutperformed the traditional neural-network composition on the Stanford\nSentiment Treebank.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:13:38 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 23:54:37 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1503.02801", "submitter": "Jiaming Xu", "authors": "Jiaming Xu, Bo Xu, Guanhua Tian, Jun Zhao, Fangyuan Wang, Hongwei Hao", "title": "Short Text Hashing Improved by Integrating Multi-Granularity Topics and\n  Tags", "comments": "12 pages, accepted at CICLing 2015", "journal-ref": null, "doi": "10.1007/978-3-319-18111-0_33", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to computational and storage efficiencies of compact binary codes,\nhashing has been widely used for large-scale similarity search. Unfortunately,\nmany existing hashing methods based on observed keyword features are not\neffective for short texts due to the sparseness and shortness. Recently, some\nresearchers try to utilize latent topics of certain granularity to preserve\nsemantic similarity in hash codes beyond keyword matching. However, topics of\ncertain granularity are not adequate to represent the intrinsic semantic\ninformation. In this paper, we present a novel unified approach for short text\nHashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, we\npropose a selection method to choose the optimal multi-granularity topics\ndepending on the type of dataset, and design two distinct hashing strategies to\nincorporate multi-granularity topics. We also propose a simple and effective\nmethod to exploit tags to enhance the similarity of related texts. We carry out\nextensive experiments on one short text dataset as well as on one normal text\ndataset. The results demonstrate that our approach is effective and\nsignificantly outperforms baselines on several evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 07:51:59 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Xu", "Jiaming", ""], ["Xu", "Bo", ""], ["Tian", "Guanhua", ""], ["Zhao", "Jun", ""], ["Wang", "Fangyuan", ""], ["Hao", "Hongwei", ""]]}, {"id": "1503.03244", "submitter": "Baotian Hu", "authors": "Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen", "title": "Convolutional Neural Network Architectures for Matching Natural Language\n  Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic matching is of central importance to many natural language tasks\n\\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to\nadequately model the internal structures of language objects and the\ninteraction between them. As a step toward this goal, we propose convolutional\nneural network models for matching two sentences, by adapting the convolutional\nstrategy in vision and speech. The proposed models not only nicely represent\nthe hierarchical structures of sentences with their layer-by-layer composition\nand pooling, but also capture the rich matching patterns at different levels.\nOur models are rather generic, requiring no prior knowledge on language, and\ncan hence be applied to matching tasks of different nature and in different\nlanguages. The empirical study on a variety of matching tasks demonstrates the\nefficacy of the proposed model on a variety of matching tasks and its\nsuperiority to competitor models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 09:46:36 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Hu", "Baotian", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Chen", "Qingcai", ""]]}, {"id": "1503.03512", "submitter": "Peter Sheridan Dodds", "authors": "Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds", "title": "Is language evolution grinding to a halt? The scaling of lexical\n  turbulence in English fiction suggests it is not", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of basic interest is the quantification of the long term growth of a\nlanguage's lexicon as it develops to more completely cover both a culture's\ncommunication requirements and knowledge space. Here, we explore the usage\ndynamics of words in the English language as reflected by the Google Books 2012\nEnglish Fiction corpus. We critique an earlier method that found decreasing\nbirth and increasing death rates of words over the second half of the 20th\nCentury, showing death rates to be strongly affected by the imposed time cutoff\nof the arbitrary present and not increasing dramatically. We provide a robust,\nprincipled approach to examining lexical evolution by tracking the volume of\nword flux across various relative frequency thresholds. We show that while the\noverall statistical structure of the English language remains stable over time\nin terms of its raw Zipf distribution, we find evidence of an enduring `lexical\nturbulence': The flux of words across frequency thresholds from decade to\ndecade scales superlinearly with word rank and exhibits a scaling break we\nconnect to that of Zipf's law. To better understand the changing lexicon, we\nexamine the contributions to the Jensen-Shannon divergence of individual words\ncrossing frequency thresholds. We also find indications that scholarly works\nabout fiction are strongly represented in the 2012 English Fiction corpus, and\nsuggest that a future revision of the corpus should attempt to separate\ncritical works from fiction itself.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:32:01 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 16:38:59 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 04:12:30 GMT"}, {"version": "v4", "created": "Fri, 24 Mar 2017 10:42:31 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Pechenick", "Eitan Adam", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1503.03535", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault,\n  Huei-Chi Lin, Fethi Bougares, Holger Schwenk, Yoshua Bengio", "title": "On Using Monolingual Corpora in Neural Machine Translation", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent work on end-to-end neural network-based architectures for machine\ntranslation has shown promising results for En-Fr and En-De translation.\nArguably, one of the major factors behind this success has been the\navailability of high quality parallel corpora. In this work, we investigate how\nto leverage abundant monolingual corpora for neural machine translation.\nCompared to a phrase-based and hierarchical baseline, we obtain up to $1.96$\nBLEU improvement on the low-resource language pair Turkish-English, and $1.59$\nBLEU on the focused domain task of Chinese-English chat messages. While our\nmethod was initially targeted toward such tasks with less parallel data, we\nshow that it also extends to high resource languages such as Cs-En and De-En\nwhere we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural\nmachine translation baselines, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 23:50:04 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 14:05:31 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Firat", "Orhan", ""], ["Xu", "Kelvin", ""], ["Cho", "Kyunghyun", ""], ["Barrault", "Loic", ""], ["Lin", "Huei-Chi", ""], ["Bougares", "Fethi", ""], ["Schwenk", "Holger", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1503.03989", "submitter": "Mirzanur Rahman", "authors": "Mirzanur Rahman and Shikhar Kumar Sarma", "title": "An implementation of Apertium based Assamese morphological analyzer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological Analysis is an important branch of linguistics for any Natural\nLanguage Processing Technology. Morphology studies the word structure and\nformation of word of a language. In current scenario of NLP research,\nmorphological analysis techniques have become more popular day by day. For\nprocessing any language, morphology of the word should be first analyzed.\nAssamese language contains very complex morphological structure. In our work we\nhave used Apertium based Finite-State-Transducers for developing morphological\nanalyzer for Assamese Language with some limited domain and we get 72.7%\naccuracy\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 09:03:21 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Rahman", "Mirzanur", ""], ["Sarma", "Shikhar Kumar", ""]]}, {"id": "1503.04250", "submitter": "Julia Bernd", "authors": "Julia Bernd, Damian Borth, Benjamin Elizalde, Gerald Friedland,\n  Heather Gallagher, Luke Gottlieb, Adam Janin, Sara Karabashlieva, Jocelyn\n  Takahashi, Jennifer Won", "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans", "comments": "47 pages; 3 figures; 25 tables. Also published as ICSI Technical\n  Report TR-15-001", "journal-ref": null, "doi": null, "report-no": "TR-15-001", "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The YLI Multimedia Event Detection corpus is a public-domain index of videos\nwith annotations and computed features, specialized for research in multimedia\nevent detection (MED), i.e., automatically identifying what's happening in a\nvideo by analyzing the audio and visual content. The videos indexed in the\nYLI-MED corpus are a subset of the larger YLI feature corpus, which is being\ndeveloped by the International Computer Science Institute and Lawrence\nLivermore National Laboratory based on the Yahoo Flickr Creative Commons 100\nMillion (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting\none of ten target events, or no target event, and are annotated for additional\nattributes like language spoken and whether the video has a musical score. The\nannotations also include degree of annotator agreement and average annotator\nconfidence scores for the event categorization of each video. Version 1.0 of\nYLI-MED includes 1823 \"positive\" videos that depict the target events and\n48,138 \"negative\" videos, as well as 177 supplementary videos that are similar\nto event videos but are not positive examples. Our goal in producing YLI-MED is\nto be as open about our data and procedures as possible. This report describes\nthe procedures used to collect the corpus; gives detailed descriptive\nstatistics about the corpus makeup (and how video attributes affected\nannotators' judgments); discusses possible biases in the corpus introduced by\nour procedural choices and compares it with the most similar existing dataset,\nTRECVID MED's HAVIC corpus; and gives an overview of our future plans for\nexpanding the annotation effort.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 23:36:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bernd", "Julia", ""], ["Borth", "Damian", ""], ["Elizalde", "Benjamin", ""], ["Friedland", "Gerald", ""], ["Gallagher", "Heather", ""], ["Gottlieb", "Luke", ""], ["Janin", "Adam", ""], ["Karabashlieva", "Sara", ""], ["Takahashi", "Jocelyn", ""], ["Won", "Jennifer", ""]]}, {"id": "1503.04723", "submitter": "Marco Guerini", "authors": "Marco Guerini and Jacopo Staiano", "title": "Deep Feelings: A Massive Cross-Lingual Study on the Relation between\n  Emotions and Virality", "comments": "preprint version of WWW 2015 'Web Science Track' paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a comprehensive investigation on the relations between\nvirality of news articles and the emotions they are found to evoke. Virality,\nin our view, is a phenomenon with many facets, i.e. under this generic term\nseveral different effects of persuasive communication are comprised. By\nexploiting a high-coverage and bilingual corpus of documents containing metrics\nof their spread on social networks as well as a massive affective annotation\nprovided by readers, we present a thorough analysis of the interplay between\nevoked emotions and viral facets. We highlight and discuss our findings in\nlight of a cross-lingual approach: while we discover differences in evoked\nemotions and corresponding viral effects, we provide preliminary evidence of a\ngeneralized explanatory model rooted in the deep structure of emotions: the\nValence-Arousal-Dominance (VAD) circumplex. We find that viral facets appear to\nbe consistently affected by particular VAD configurations, and these\nconfigurations indicate a clear connection with distinct phenomena underlying\npersuasive communication.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 16:43:58 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Guerini", "Marco", ""], ["Staiano", "Jacopo", ""]]}, {"id": "1503.04881", "submitter": "Xiaodan Zhu", "authors": "Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo", "title": "Long Short-Term Memory Over Tree Structures", "comments": "On February 6th, 2015, this work was submitted to the International\n  Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chain-structured long short-term memory (LSTM) has showed to be effective\nin a wide range of problems such as speech recognition and machine translation.\nIn this paper, we propose to extend it to tree structures, in which a memory\ncell can reflect the history memories of multiple child cells or multiple\ndescendant cells in a recursive process. We call the model S-LSTM, which\nprovides a principled way of considering long-distance interaction over\nhierarchies, e.g., language or image parse structures. We leverage the models\nfor semantic composition to understand the meaning of text, a fundamental\nproblem in natural language understanding, and show that it outperforms a\nstate-of-the-art recursive model by replacing its composition layers with the\nS-LSTM memory blocks. We also show that utilizing the given structures is\nhelpful in achieving a performance better than that without considering the\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 23:59:02 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Zhu", "Xiaodan", ""], ["Sobhani", "Parinaz", ""], ["Guo", "Hongyu", ""]]}, {"id": "1503.05034", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu", "title": "$gen$CNN: A Convolutional Architecture for Word Sequence Prediction", "comments": "Accepted by ACL as full paper(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel convolutional architecture, named $gen$CNN, for word\nsequence prediction. Different from previous work on neural network-based\nlanguage modeling and generation (e.g., RNN or LSTM), we choose not to greedily\nsummarize the history of words as a fixed length vector. Instead, we use a\nconvolutional neural network to predict the next word with the history of words\nof variable length. Also different from the existing feedforward networks for\nlanguage modeling, our model can effectively fuse the local correlation and\nglobal correlation in the word sequence, with a convolution-gating strategy\nspecifically designed for the task. We argue that our model can give adequate\nrepresentation of the history, and therefore can naturally exploit both the\nshort and long range dependencies. Our model is fast, easy to train, and\nreadily parallelized. Our extensive experiments on text generation and $n$-best\nre-ranking in machine translation show that $gen$CNN outperforms the\nstate-of-the-arts with big margins.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 13:26:08 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 08:44:05 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Jiang", "Wenbin", ""], ["Liu", "Qun", ""]]}, {"id": "1503.05123", "submitter": "Manuel Amunategui", "authors": "Manuel Amunategui, Tristan Markwell, Yelena Rozenfeld", "title": "Prediction Using Note Text: Synthetic Feature Creation with word2vec", "comments": "13 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  word2vec affords a simple yet powerful approach of extracting quantitative\nvariables from unstructured textual data. Over half of healthcare data is\nunstructured and therefore hard to model without involved expertise in data\nengineering and natural language processing. word2vec can serve as a bridge to\nquickly gather intelligence from such data sources.\n  In this study, we ran 650 megabytes of unstructured, medical chart notes from\nthe Providence Health & Services electronic medical record through word2vec. We\nused two different approaches in creating predictive variables and tested them\non the risk of readmission for patients with COPD (Chronic Obstructive Lung\nDisease). As a comparative benchmark, we ran the same test using the LACE risk\nmodel (a single score based on length of stay, acuity, comorbid conditions, and\nemergency department visits).\n  Using only free text and mathematical might, we found word2vec comparable to\nLACE in predicting the risk of readmission of COPD patients.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 17:04:27 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Amunategui", "Manuel", ""], ["Markwell", "Tristan", ""], ["Rozenfeld", "Yelena", ""]]}, {"id": "1503.05543", "submitter": "Alexander Alemi", "authors": "Alexander A Alemi, Paul Ginsparg", "title": "Text Segmentation based on Semantic Word Embeddings", "comments": "10 pages, 4 figures. KDD2015 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of semantic word embeddings in text segmentation\nalgorithms, including the C99 segmentation algorithm and new algorithms\ninspired by the distributed word vector representation. By developing a general\nframework for discussing a class of segmentation objectives, we study the\neffectiveness of greedy versus exact optimization approaches and suggest a new\niterative refinement technique for improving the performance of greedy\nstrategies. We compare our results to known benchmarks, using known metrics. We\ndemonstrate state-of-the-art performance for an untrained method with our\nContent Vector Segmentation (CVS) on the Choi test set. Finally, we apply the\nsegmentation procedure to an in-the-wild dataset consisting of text extracted\nfrom scholarly articles in the arXiv.org database.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 19:44:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Alemi", "Alexander A", ""], ["Ginsparg", "Paul", ""]]}, {"id": "1503.05615", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang, He He, Hal Daum\\'e III, John Langford", "title": "Learning to Search for Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a dependency parser can be built using a credit\nassignment compiler which removes the burden of worrying about low-level\nmachine learning details from the parser implementation. The result is a simple\nparser which robustly applies to many languages that provides similar\nstatistical and computational performance with best-to-date transition-based\nparsing approaches, while avoiding various downsides including randomization,\nextra feature requirements, and custom learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 23:33:17 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 22:12:11 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Chang", "Kai-Wei", ""], ["He", "He", ""], ["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""]]}, {"id": "1503.05626", "submitter": "Myongchol Pak K", "authors": "Myong-Chol Pak", "title": "Phrase database Approach to structural and semantic disambiguation in\n  English-Korean Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In machine translation it is common phenomenon that machine-readable\ndictionaries and standard parsing rules are not enough to ensure accuracy in\nparsing and translating English phrases into Korean language, which is revealed\nin misleading translation results due to consequent structural and semantic\nambiguities. This paper aims to suggest a solution to structural and semantic\nambiguities due to the idiomaticity and non-grammaticalness of phrases commonly\nused in English language by applying bilingual phrase database in\nEnglish-Korean Machine Translation (EKMT). This paper firstly clarifies what\nthe phrase unit in EKMT is based on the definition of the English phrase,\nsecondly clarifies what kind of language unit can be the target of the phrase\ndatabase for EKMT, thirdly suggests a way to build the phrase database by\npresenting the format of the phrase database with examples, and finally\ndiscusses briefly the method to apply this bilingual phrase database to the\nEKMT for structural and semantic disambiguation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 01:37:40 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Pak", "Myong-Chol", ""]]}, {"id": "1503.05907", "submitter": "Daniel Christen Mr.", "authors": "Daniel Christen", "title": "Syntagma Lexical Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the structure of Syntagma's Lexical Database (focused on\nItalian). The basic database consists in four tables. Table Forms contains word\ninflections, used by the POS-tagger for the identification of input-words.\nForms is related to Lemma. Table Lemma stores all kinds of grammatical features\nof words, word-level semantic data and restrictions. In the table Meanings\nmeaning-related data are stored: definition, examples, domain, and semantic\ninformation. Table Valency contains the argument structure of each meaning,\nwith syntactic and semantic features for each argument. The extended version of\nSLD contains the links to Syntagma's Semantic Net and to the WordNet synsets of\nother languages.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 19:45:24 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Christen", "Daniel", ""]]}, {"id": "1503.06151", "submitter": "Maxim Litvak", "authors": "Maxim Litvak", "title": "On measuring linguistic intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of measuring how many languages a person\n\"effectively\" speaks given that some of the languages are close to each other.\nIn other words, to assign a meaningful number to her language portfolio.\nIntuition says that someone who speaks fluently Spanish and Portuguese is\nlinguistically less proficient compared to someone who speaks fluently Spanish\nand Chinese since it takes more effort for a native Spanish speaker to learn\nChinese than Portuguese. As the number of languages grows and their proficiency\nlevels vary, it gets even more complicated to assign a score to a language\nportfolio. In this article we propose such a measure (\"linguistic quotient\" -\nLQ) that can account for these effects.\n  We define the properties that such a measure should have. They are based on\nthe idea of coherent risk measures from the mathematical finance. Having laid\ndown the foundation, we propose one such a measure together with the algorithm\nthat works on languages classification tree as input.\n  The algorithm together with the input is available online at lingvometer.com\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 16:41:05 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Litvak", "Maxim", ""]]}, {"id": "1503.06410", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "KIT-14-001", "categories": "cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure or F-score is one of the most commonly used single number\nmeasures in Information Retrieval, Natural Language Processing and Machine\nLearning, but it is based on a mistake, and the flawed assumptions render it\nunsuitable for use in most contexts! Fortunately, there are better\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:42:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1503.06450", "submitter": "Manaal Faruqui", "authors": "Manaal Faruqui and Shankar Kumar", "title": "Multilingual Open Relation Extraction Using Cross-lingual Projection", "comments": "Proceedings of NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open domain relation extraction systems identify relation and argument\nphrases in a sentence without relying on any underlying schema. However,\ncurrent state-of-the-art relation extraction systems are available only for\nEnglish because of their heavy reliance on linguistic tools such as\npart-of-speech taggers and dependency parsers. We present a cross-lingual\nannotation projection method for language independent relation extraction. We\nevaluate our method on a manually annotated test set and present results on\nthree typologically different languages. We release these manual annotations\nand extracted relations in 61 languages from Wikipedia.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 18:05:08 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 19:05:14 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 13:28:55 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Faruqui", "Manaal", ""], ["Kumar", "Shankar", ""]]}, {"id": "1503.06733", "submitter": "Mohammad Sadegh Rasooli", "authors": "Mohammad Sadegh Rasooli, Joel Tetreault", "title": "Yara Parser: A Fast and Accurate Dependency Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency parsers are among the most crucial tools in natural language\nprocessing as they have many important applications in downstream tasks such as\ninformation retrieval, machine translation and knowledge acquisition. We\nintroduce the Yara Parser, a fast and accurate open-source dependency parser\nbased on the arc-eager algorithm and beam search. It achieves an unlabeled\naccuracy of 93.32 on the standard WSJ test set which ranks it among the top\ndependency parsers. At its fastest, Yara can parse about 4000 sentences per\nsecond when in greedy mode (1 beam). When optimizing for accuracy (using 64\nbeams and Brown cluster features), Yara can parse 45 sentences per second. The\nparser can be trained on any syntactic dependency treebank and different\noptions are provided in order to make it more flexible and tunable for specific\ntasks. It is released with the Apache version 2.0 license and can be used for\nboth commercial and academic purposes. The parser can be found at\nhttps://github.com/yahoo/YaraParser.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 17:20:54 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 18:45:13 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Rasooli", "Mohammad Sadegh", ""], ["Tetreault", "Joel", ""]]}, {"id": "1503.06760", "submitter": "Chu-Cheng Lin", "authors": "Chu-Cheng Lin and Waleed Ammar and Chris Dyer and Lori Levin", "title": "Unsupervised POS Induction with Word Embeddings", "comments": "NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised word embeddings have been shown to be valuable as features in\nsupervised learning problems; however, their role in unsupervised problems has\nbeen less thoroughly explored. In this paper, we show that embeddings can\nlikewise add value to the problem of unsupervised POS induction. In two\nrepresentative models of POS induction, we replace multinomial distributions\nover the vocabulary with multivariate Gaussian distributions over word\nembeddings and observe consistent improvements in eight languages. We also\nanalyze the effect of various choices while inducing word embeddings on\n\"downstream\" POS induction results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 18:32:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Lin", "Chu-Cheng", ""], ["Ammar", "Waleed", ""], ["Dyer", "Chris", ""], ["Levin", "Lori", ""]]}, {"id": "1503.06934", "submitter": "Issa Atoum", "authors": "Issa Atoum and Chih How Bong", "title": "Measuring Software Quality in Use: State-of-the-Art and Research\n  Challenges", "comments": "4 Figures", "journal-ref": "ASQ.Software Quality Professional, 17(2), 2015", "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software quality in use comprises quality from the user's perspective. It has\ngained its importance in e-government applications, mobile-based applications,\nembedded systems, and even business process development. User's decisions on\nsoftware acquisitions are often ad hoc or based on preference due to difficulty\nin quantitatively measuring software quality in use. But, why is quality-in-use\nmeasurement difficult? Although there are many software quality models, to the\nauthors' knowledge no works survey the challenges related to software\nquality-in-use measurement. This article has two main contributions: 1) it\nidentifies and explains major issues and challenges in measuring software\nquality in use in the context of the ISO SQuaRE series and related software\nquality models and highlights open research areas; and 2) it sheds light on a\nresearch direction that can be used to predict software quality in use. In\nshort, the quality-in-use measurement issues are related to the complexity of\nthe current standard models and the limitations and incompleteness of the\ncustomized software quality models. A sentiment analysis of software reviews is\nproposed to deal with these issues.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 07:35:16 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Atoum", "Issa", ""], ["Bong", "Chih How", ""]]}, {"id": "1503.07283", "submitter": "Mikhail Korobov", "authors": "Mikhail Korobov", "title": "Morphological Analyzer and Generator for Russian and Ukrainian Languages", "comments": "AIST 2015 (http://aistconf.org/2015); 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian\nlanguages. It uses large efficiently encoded lexi- cons built from OpenCorpora\nand LanguageTool data. A set of linguistically motivated rules is developed to\nenable morphological analysis and generation of out-of-vocabulary words\nobserved in real-world documents. For Russian pymorphy2 provides\nstate-of-the-arts morphological analysis quality. The analyzer is implemented\nin Python programming language with optional C++ extensions. Emphasis is put on\nease of use, documentation and extensibility. The package is distributed under\na permissive open-source license, encouraging its use in both academic and\ncommercial setting.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 05:28:50 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Korobov", "Mikhail", ""]]}, {"id": "1503.07294", "submitter": "Issa Atoum", "authors": "Wendy Tan Wei Syn, Bong Chih How, Issa Atoum", "title": "Using Latent Semantic Analysis to Identify Quality in Use (QU)\n  Indicators from User Reviews", "comments": "4 Figures in The International Conference on Artificial Intelligence\n  and Pattern Recognition (AIPR2014),2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a novel approach to categorize users' reviews according\nto the three Quality in Use (QU) indicators defined in ISO: effectiveness,\nefficiency and freedom from risk. With the tremendous amount of reviews\npublished each day, there is a need to automatically summarize user reviews to\ninform us if any of the software able to meet requirement of a company\naccording to the quality requirements. We implemented the method of Latent\nSemantic Analysis (LSA) and its subspace to predict QU indicators. We build a\nreduced dimensionality universal semantic space from Information System\njournals and Amazon reviews. Next, we projected set of indicators' measurement\nscales into the universal semantic space and represent them as subspace. In the\nsubspace, we can map similar measurement scales to the unseen reviews and\npredict the QU indicators. Our preliminary study able to obtain the average of\nF-measure, 0.3627.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 06:42:05 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Syn", "Wendy Tan Wei", ""], ["How", "Bong Chih", ""], ["Atoum", "Issa", ""]]}, {"id": "1503.07613", "submitter": "David Fifield", "authors": "David Fifield, Torbj{\\o}rn Follan, Emil Lunde", "title": "Unsupervised authorship attribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We describe a technique for attributing parts of a written text to a set of\nunknown authors. Nothing is assumed to be known a priori about the writing\nstyles of potential authors. We use multiple independent clusterings of an\ninput text to identify parts that are similar and dissimilar to one another. We\ndescribe algorithms necessary to combine the multiple clusterings into a\nmeaningful output. We show results of the application of the technique on texts\nhaving multiple writing styles.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 04:02:26 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Fifield", "David", ""], ["Follan", "Torbj\u00f8rn", ""], ["Lunde", "Emil", ""]]}, {"id": "1503.07921", "submitter": "Julio Reis", "authors": "Julio Reis, Fabr{\\i}cio Benevenuto, Pedro O.S. Vaz de Melo, Raquel\n  Prates, Haewoon Kwak, Jisun An", "title": "Breaking the News: First Impressions Matter on Online News", "comments": "The paper appears in ICWSM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of people are changing the way they consume news, replacing\nthe traditional physical newspapers and magazines by their virtual online\nversions or/and weblogs. The interactivity and immediacy present in online news\nare changing the way news are being produced and exposed by media corporations.\nNews websites have to create effective strategies to catch people's attention\nand attract their clicks. In this paper we investigate possible strategies used\nby online news corporations in the design of their news headlines. We analyze\nthe content of 69,907 headlines produced by four major global media\ncorporations during a minimum of eight consecutive months in 2014. In order to\ndiscover strategies that could be used to attract clicks, we extracted features\nfrom the text of the news headlines related to the sentiment polarity of the\nheadline. We discovered that the sentiment of the headline is strongly related\nto the popularity of the news and also with the dynamics of the posted comments\non that particular news.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 22:20:40 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 19:58:04 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Reis", "Julio", ""], ["Benevenuto", "Fabr\u0131cio", ""], ["de Melo", "Pedro O. S. Vaz", ""], ["Prates", "Raquel", ""], ["Kwak", "Haewoon", ""], ["An", "Jisun", ""]]}, {"id": "1503.08155", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou and Thomas Fang Zheng", "title": "Learning Embedding Representations for Knowledge Inference on Imperfect\n  and Incomplete Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of knowledge inference on large-scale\nimperfect repositories with incomplete coverage by means of embedding entities\nand relations at the first attempt. We propose IIKE (Imperfect and Incomplete\nKnowledge Embedding), a probabilistic model which measures the probability of\neach belief, i.e. $\\langle h,r,t\\rangle$, in large-scale knowledge bases such\nas NELL and Freebase, and our objective is to learn a better low-dimensional\nvector representation for each entity ($h$ and $t$) and relation ($r$) in the\nprocess of minimizing the loss of fitting the corresponding confidence given by\nmachine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\\bf\nh} + {\\bf r} - {\\bf t}||$ to assess the plausibility of a belief when\nconducting inference. We use subsets of those inexact knowledge bases to train\nour model and test the performances of link prediction and triplet\nclassification on ground truth beliefs, respectively. The results of extensive\nexperiments show that IIKE achieves significant improvement compared with the\nbaseline and state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 17:13:03 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1503.08167", "submitter": "Slobodan Beliga", "authors": "Slobodan Beliga, Miran Pobar and Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Normalization of Non-Standard Words in Croatian Texts", "comments": "8 pages, 3 figures in Text, Speech and Dialogue extension to Lecture\n  Notes in Artificial Intelligence LNAI6836. Hebernal, Ivan; Matou\\v{s}ek,\n  V\\'aclav (ed). - Plzen: University of West Bohemia, 2011. 1-8 (ISBN:\n  987-80-261-0069-0)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents text normalization which is an integral part of any\ntext-to-speech synthesis system. Text normalization is a set of methods with a\ntask to write non-standard words, like numbers, dates, times, abbreviations,\nacronyms and the most common symbols, in their full expanded form are\npresented. The whole taxonomy for classification of non-standard words in\nCroatian language together with rule-based normalization methods combined with\na lookup dictionary are proposed. Achieved token rate for normalization of\nCroatian texts is 95%, where 80% of expanded words are in correct morphological\nform.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 17:57:00 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 07:45:18 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Beliga", "Slobodan", ""], ["Pobar", "Miran", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1503.08542", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Nonparametric Relational Topic Models through Dependent Gamma Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:40:41 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08581", "submitter": "Ioannis Partalas", "authors": "Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry\n  Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza\n  Amini, Patrick Galinari", "title": "LSHTC: A Benchmark for Large-Scale Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSHTC is a series of challenges which aims to assess the performance of\nclassification systems in large-scale classification in a a large number of\nclasses (up to hundreds of thousands). This paper describes the dataset that\nhave been released along the LSHTC series. The paper details the construction\nof the datsets and the design of the tracks as well as the evaluation measures\nthat we implemented and a quick overview of the results. All of these datasets\nare available online and runs may still be submitted on the online server of\nthe challenges.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 08:03:47 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Partalas", "Ioannis", ""], ["Kosmopoulos", "Aris", ""], ["Baskiotis", "Nicolas", ""], ["Artieres", "Thierry", ""], ["Paliouras", "George", ""], ["Gaussier", "Eric", ""], ["Androutsopoulos", "Ion", ""], ["Amini", "Massih-Reza", ""], ["Galinari", "Patrick", ""]]}, {"id": "1503.08895", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston and Rob Fergus", "title": "End-To-End Memory Networks", "comments": "Accepted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural network with a recurrent attention model over a\npossibly large external memory. The architecture is a form of Memory Network\n(Weston et al., 2015) but unlike the model in that work, it is trained\nend-to-end, and hence requires significantly less supervision during training,\nmaking it more generally applicable in realistic settings. It can also be seen\nas an extension of RNNsearch to the case where multiple computational steps\n(hops) are performed per output symbol. The flexibility of the model allows us\nto apply it to tasks as diverse as (synthetic) question answering and to\nlanguage modeling. For the former our approach is competitive with Memory\nNetworks, but with less supervision. For the latter, on the Penn TreeBank and\nText8 datasets our approach demonstrates comparable performance to RNNs and\nLSTMs. In both cases we show that the key concept of multiple computational\nhops yields improved results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 03:05:37 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 02:23:20 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 04:19:33 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2015 21:42:20 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2015 19:41:57 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Weston", "Jason", ""], ["Fergus", "Rob", ""]]}, {"id": "1503.09144", "submitter": "David Martins de Matos", "authors": "Ant\\'onio Lopes and David Martins de Matos and Vera Cabarr\\~ao and\n  Ricardo Ribeiro and Helena Moniz and Isabel Trancoso and Ana Isabel Mata", "title": "Towards Using Machine Translation Techniques to Induce Multilingual\n  Lexica of Discourse Markers", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse markers are universal linguistic events subject to language\nvariation. Although an extensive literature has already reported language\nspecific traits of these events, little has been said on their cross-language\nbehavior and on building an inventory of multilingual lexica of discourse\nmarkers. This work describes new methods and approaches for the description,\nclassification, and annotation of discourse markers in the specific domain of\nthe Europarl corpus. The study of discourse markers in the context of\ntranslation is crucial due to the idiomatic nature of these structures.\nMultilingual lexica together with the functional analysis of such structures\nare useful tools for the hard task of translating discourse markers into\npossible equivalents from one language to another. Using Daniel Marcu's\nvalidated discourse markers for English, extracted from the Brown Corpus, our\npurpose is to build multilingual lexica of discourse markers for other\nlanguages, based on machine translation techniques. The major assumption in\nthis study is that the usage of a discourse marker is independent of the\nlanguage, i.e., the rhetorical function of a discourse marker in a sentence in\none language is equivalent to the rhetorical function of the same discourse\nmarker in another language.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 17:56:07 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Lopes", "Ant\u00f3nio", ""], ["de Matos", "David Martins", ""], ["Cabarr\u00e3o", "Vera", ""], ["Ribeiro", "Ricardo", ""], ["Moniz", "Helena", ""], ["Trancoso", "Isabel", ""], ["Mata", "Ana Isabel", ""]]}]