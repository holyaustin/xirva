[{"id": "2105.00030", "submitter": "Sara Lafia", "authors": "Sara Lafia, Andrea Thomer, David Bleckley, Dharma Akmon, Libby\n  Hemphill", "title": "Leveraging Machine Learning to Detect Data Curation Activities", "comments": "10 pages, 4 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a machine learning approach for annotating and analyzing\ndata curation work logs at ICPSR, a large social sciences data archive. The\nsystems we studied track curation work and coordinate team decision-making at\nICPSR. Repository staff use these systems to organize, prioritize, and document\ncuration work done on datasets, making them promising resources for studying\ncuration work and its impact on data reuse, especially in combination with data\nusage analytics. A key challenge, however, is classifying similar activities so\nthat they can be measured and associated with impact metrics. This paper\ncontributes: 1) a schema of data curation activities; 2) a computational model\nfor identifying curation actions in work log descriptions; and 3) an analysis\nof frequent data curation activities at ICPSR over time. We first propose a\nschema of data curation actions to help us analyze the impact of curation work.\nWe then use this schema to annotate a set of data curation logs, which contain\nrecords of data transformations and project management decisions completed by\nrepository staff. Finally, we train a text classifier to detect the frequency\nof curation actions in a large set of work logs. Our approach supports the\nanalysis of curation work documented in work log systems as an important step\ntoward studying the relationship between research data curation and data reuse.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:17:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lafia", "Sara", ""], ["Thomer", "Andrea", ""], ["Bleckley", "David", ""], ["Akmon", "Dharma", ""], ["Hemphill", "Libby", ""]]}, {"id": "2105.00059", "submitter": "Alexandr Sboev", "authors": "Alexander Sboev, Sanna Sboeva, Ivan Moloshnikov, Artem Gryaznov, Roman\n  Rybka, Alexander Naumov, Anton Selivanov, Gleb Rylkov, Viacheslav Ilyin", "title": "An analysis of full-size Russian complexly NER labelled corpus of\n  Internet user reviews on the drugs based on deep learning and language neural\n  nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the full-size Russian complexly NER-labeled corpus of Internet\nuser reviews, along with an evaluation of accuracy levels reached on this\ncorpus by a set of advanced deep learning neural networks to extract the\npharmacologically meaningful entities from Russian texts. The corpus annotation\nincludes mentions of the following entities: Medication (33005 mentions),\nAdverse Drug Reaction (1778), Disease (17403), and Note (4490). Two of them -\nMedication and Disease - comprise a set of attributes. A part of the corpus has\nthe coreference annotation with 1560 coreference chains in 300 documents.\nSpecial multi-label model based on a language model and the set of features is\ndeveloped, appropriate for presented corpus labeling. The influence of the\nchoice of different modifications of the models: word vector representations,\ntypes of language models pre-trained for Russian, text normalization styles,\nand other preliminary processing are analyzed. The sufficient size of our\ncorpus allows to study the effects of particularities of corpus labeling and\nbalancing entities in the corpus. As a result, the state of the art for the\npharmacological entity extraction problem for Russian is established on a\nfull-size labeled corpus. In case of the adverse drug reaction (ADR)\nrecognition, it is 61.1 by the F1-exact metric that, as our analysis shows, is\non par with the accuracy level for other language corpora with similar\ncharacteristics and the ADR representativnes. The evaluated baseline precision\nof coreference relation extraction on the corpus is 71, that is higher the\nresults reached on other Russian corpora.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:46:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sboev", "Alexander", ""], ["Sboeva", "Sanna", ""], ["Moloshnikov", "Ivan", ""], ["Gryaznov", "Artem", ""], ["Rybka", "Roman", ""], ["Naumov", "Alexander", ""], ["Selivanov", "Anton", ""], ["Rylkov", "Gleb", ""], ["Ilyin", "Viacheslav", ""]]}, {"id": "2105.00071", "submitter": "Nouha Dziri", "authors": "Nouha Dziri, Hannah Rashkin, Tal Linzen, David Reitter", "title": "Evaluating Groundedness in Dialogue Systems: The BEGIN Benchmark", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge-grounded dialogue agents are systems designed to conduct a\nconversation based on externally provided background information, such as a\nWikipedia page. Such dialogue agents, especially those based on neural network\nlanguage models, often produce responses that sound fluent but are not\njustified by the background information. Progress towards addressing this\nproblem requires developing automatic evaluation metrics that can quantify the\nextent to which responses are grounded in background information. To facilitate\nevaluation of such metrics, we introduce the Benchmark for Evaluation of\nGrounded INteraction (BEGIN). BEGIN consists of 8113 dialogue turns generated\nby language-model-based dialogue systems, accompanied by humans annotations\nspecifying the relationship between the system's response and the background\ninformation. These annotations are based on an extension of the natural\nlanguage inference paradigm. We use the benchmark to demonstrate the\neffectiveness of adversarially generated data for improving an evaluation\nmetric based on existing natural language inference datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:17:52 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dziri", "Nouha", ""], ["Rashkin", "Hannah", ""], ["Linzen", "Tal", ""], ["Reitter", "David", ""]]}, {"id": "2105.00079", "submitter": "Ziming Li", "authors": "Ziming Li, Julia Kiseleva, Maarten de Rijke", "title": "Improving Response Quality with Backward Reasoning in Open-domain\n  Dialogue Systems", "comments": "5 pages, 2 figures, Sigir 2021 short", "journal-ref": null, "doi": "10.1145/3404835.3463004", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to generate informative and coherent dialogue responses is crucial\nwhen designing human-like open-domain dialogue systems. Encoder-decoder-based\ndialogue models tend to produce generic and dull responses during the decoding\nstep because the most predictable response is likely to be a non-informative\nresponse instead of the most suitable one. To alleviate this problem, we\npropose to train the generation model in a bidirectional manner by adding a\nbackward reasoning step to the vanilla encoder-decoder training. The proposed\nbackward reasoning step pushes the model to produce more informative and\ncoherent content because the forward generation step's output is used to infer\nthe dialogue context in the backward direction. The advantage of our method is\nthat the forward generation and backward reasoning steps are trained\nsimultaneously through the use of a latent variable to facilitate bidirectional\noptimization. Our method can improve response quality without introducing side\ninformation (e.g., a pre-trained topic model). The proposed bidirectional\nresponse generation method achieves state-of-the-art performance for response\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:38:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Ziming", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2105.00150", "submitter": "Yuta Koreeda", "authors": "Yuta Koreeda, Christopher D. Manning", "title": "Capturing Logical Structure of Visually Structured Documents with\n  Multimodal Transition Parser", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many NLP papers, tasks and pipelines assume raw, clean texts, many\ntexts we encounter in the wild are not so clean, with many of them being\nvisually structured documents (VSDs) such as PDFs. Conventional preprocessing\ntools for VSDs mainly focused on word segmentation and coarse layout analysis,\nwhile fine-grained logical structure analysis (such as identifying paragraph\nboundaries and their hierarchies) of VSDs is underexplored. To that end, we\nproposed to formulate the task as prediction of transition labels between text\nfragments that maps the fragments to a tree, and developed a feature-based\nmachine learning system that fuses visual, textual and semantic cues. Our\nsystem significantly outperformed baselines in identifying different structures\nin VSDs. For example, our system obtained a paragraph boundary detection F1\nscore of 0.951 which is significantly better than a popular PDF-to-text tool\nwith a F1 score of 0.739.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 02:33:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Koreeda", "Yuta", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2105.00164", "submitter": "Shaofeng Li", "authors": "Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue,\n  Haojin Zhu, Jialiang Lu", "title": "Hidden Backdoors in Human-Centric Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language processing (NLP) systems have been proven to be vulnerable\nto backdoor attacks, whereby hidden features (backdoors) are trained into a\nlanguage model and may only be activated by specific inputs (called triggers),\nto trick the model into producing unexpected behaviors. In this paper, we\ncreate covert and natural triggers for textual backdoor attacks, \\textit{hidden\nbackdoors}, where triggers can fool both modern language models and human\ninspection. We deploy our hidden backdoors through two state-of-the-art trigger\nembedding methods. The first approach via homograph replacement, embeds the\ntrigger into deep neural networks through the visual spoofing of lookalike\ncharacter replacement. The second approach uses subtle differences between text\ngenerated by language models and real natural text to produce trigger sentences\nwith correct grammar and high fluency. We demonstrate that the proposed hidden\nbackdoors can be effective across three downstream security-critical NLP tasks,\nrepresentative of modern human-centric NLP systems, including toxic comment\ndetection, neural machine translation (NMT), and question answering (QA). Our\ntwo hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at\nleast $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection,\n$95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$\nASR against QA updated with only 27 poisoning data samples on a model\npreviously trained with 92,024 samples (0.029\\%). We are able to demonstrate\nthe adversary's high success rate of attacks, while maintaining functionality\nfor regular users, with triggers inconspicuous by the human administrators.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 04:41:00 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 13:46:40 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Shaofeng", ""], ["Liu", "Hui", ""], ["Dong", "Tian", ""], ["Zhao", "Benjamin Zi Hao", ""], ["Xue", "Minhui", ""], ["Zhu", "Haojin", ""], ["Lu", "Jialiang", ""]]}, {"id": "2105.00171", "submitter": "Yao-Fei Cheng", "authors": "Yao-Fei Cheng, Hung-Shin Lee, and Hsin-Min Wang", "title": "AlloST: Low-resource Speech Translation without Source Transcription", "comments": "Accepted by Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource conditions.\nMost ST models have shown unsatisfactory results, especially in the absence of\nword information from the source speech utterance. In this study, we survey\nmethods to improve ST performance without using source transcription, and\npropose a learning framework that utilizes a language-independent universal\nphone recognizer. The framework is based on an attention-based\nsequence-to-sequence model, where the encoder generates the phonetic embeddings\nand phone-aware acoustic representations, and the decoder controls the fusion\nof the two embedding streams to produce the target token sequence. In addition\nto investigating different fusion strategies, we explore the specific usage of\nbyte pair encoding (BPE), which compresses a phone sequence into a\nsyllable-like segmented sequence. Due to the conversion of symbols, a segmented\nsequence represents not only pronunciation but also language-dependent\ninformation lacking in phones. Experiments conducted on the Fisher\nSpanish-English and Taigi-Mandarin drama corpora show that our method\noutperforms the conformer-based baseline, and the performance is close to that\nof the existing best method using source transcription.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 05:30:18 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:37:47 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 04:58:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Cheng", "Yao-Fei", ""], ["Lee", "Hung-Shin", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "2105.00239", "submitter": "Guokai Tang", "authors": "Saurabh Jain, Guokai Tang, Lim Sze Chi", "title": "MRCBert: A Machine Reading ComprehensionApproach for Unsupervised\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When making an online purchase, it becomes important for the customer to read\nthe product reviews carefully and make a decision based on that. However,\nreviews can be lengthy, may contain repeated, or sometimes irrelevant\ninformation that does not help in decision making. In this paper, we introduce\nMRCBert, a novel unsupervised method to generate summaries from product\nreviews. We leverage Machine Reading Comprehension, i.e. MRC, approach to\nextract relevant opinions and generate both rating-wise and aspect-wise\nsummaries from reviews. Through MRCBert we show that we can obtain reasonable\nperformance using existing models and transfer learning, which can be useful\nfor learning under limited or low resource scenarios. We demonstrated our\nresults on reviews of a product from the Electronics category in the Amazon\nReviews dataset. Our approach is unsupervised as it does not require any\ndomain-specific dataset, such as the product review dataset, for training or\nfine-tuning. Instead, we have used SQuAD v1.1 dataset only to fine-tune BERT\nfor the MRC task. Since MRCBert does not require a task-specific dataset, it\ncan be easily adapted and used in other domains.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 12:57:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jain", "Saurabh", ""], ["Tang", "Guokai", ""], ["Chi", "Lim Sze", ""]]}, {"id": "2105.00260", "submitter": "Sarenne Wallbridge Miss", "authors": "Sarenne Wallbridge, Peter Bell, Catherine Lai", "title": "It's not what you said, it's how you said it: discriminative perception\n  of speech as a multichannel communication system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People convey information extremely effectively through spoken interaction\nusing multiple channels of information transmission: the lexical channel of\nwhat is said, and the non-lexical channel of how it is said. We propose\nstudying human perception of spoken communication as a means to better\nunderstand how information is encoded across these channels, focusing on the\nquestion 'What characteristics of communicative context affect listener's\nexpectations of speech?'. To investigate this, we present a novel behavioural\ntask testing whether listeners can discriminate between the true utterance in a\ndialogue and utterances sampled from other contexts with the same lexical\ncontent. We characterize how perception - and subsequent discriminative\ncapability - is affected by different degrees of additional contextual\ninformation across both the lexical and non-lexical channel of speech. Results\ndemonstrate that people can effectively discriminate between different prosodic\nrealisations, that non-lexical context is informative, and that this channel\nprovides more salient information than the lexical channel, highlighting the\nimportance of the non-lexical channel in spoken interaction.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 14:30:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wallbridge", "Sarenne", ""], ["Bell", "Peter", ""], ["Lai", "Catherine", ""]]}, {"id": "2105.00309", "submitter": "Arman Malekzadeh Lashkaryani", "authors": "Arman Malekzadeh and Amin Gheibi and Ali Mohades", "title": "PREDICT: Persian Reverse Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding the appropriate words to convey concepts (i.e., lexical access) is\nessential for effective communication. Reverse dictionaries fulfill this need\nby helping individuals to find the word(s) which could relate to a specific\nconcept or idea. To the best of our knowledge, this resource has not been\navailable for the Persian language. In this paper, we compare four different\narchitectures for implementing a Persian reverse dictionary (PREDICT).\n  We evaluate our models using (phrase,word) tuples extracted from the only\nPersian dictionaries available online, namely Amid, Moein, and Dehkhoda where\nthe phrase describes the word. Given the phrase, a model suggests the most\nrelevant word(s) in terms of the ability to convey the concept. The model is\nconsidered to perform well if the correct word is one of its top suggestions.\n  Our experiments show that a model consisting of Long Short-Term Memory (LSTM)\nunits enhanced by an additive attention mechanism is enough to produce\nsuggestions comparable to (or in some cases better than) the word in the\noriginal dictionary. The study also reveals that the model sometimes produces\nthe synonyms of the word as its output which led us to introduce a new metric\nfor the evaluation of reverse dictionaries called Synonym Accuracy accounting\nfor the percentage of times the event of producing the word or a synonym of it\noccurs. The assessment of the best model using this new metric also indicates\nthat at least 62% of the times, it produces an accurate result within the top\n100 suggestions.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 17:37:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Malekzadeh", "Arman", ""], ["Gheibi", "Amin", ""], ["Mohades", "Ali", ""]]}, {"id": "2105.00328", "submitter": "Marshall Ho", "authors": "Marshall Ho, Zhipeng Zhou, Judith He", "title": "When to Fold'em: How to answer Unanswerable questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present 3 different question-answering models trained on the SQuAD2.0\ndataset -- BIDAF, DocumentQA and ALBERT Retro-Reader -- demonstrating the\nimprovement of language models in the past three years. Through our research in\nfine-tuning pre-trained models for question-answering, we developed a novel\napproach capable of achieving a 2% point improvement in SQuAD2.0 F1 in reduced\ntraining time. Our method of re-initializing select layers of a\nparameter-shared language model is simple yet empirically powerful.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:08:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ho", "Marshall", ""], ["Zhou", "Zhipeng", ""], ["He", "Judith", ""]]}, {"id": "2105.00377", "submitter": "Shuai Peng", "authors": "Shuai Peng, Ke Yuan, Liangcai Gao, Zhi Tang", "title": "MathBERT: A Pre-Trained Model for Mathematical Formula Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained models like BERT, have obtained a great success in\nvarious Natural Language Processing (NLP) tasks, while it is still a challenge\nto adapt them to the math-related tasks. Current pre-trained models neglect the\nstructural features and the semantic correspondence between formula and its\ncontext. To address these issues, we propose a novel pre-trained model, namely\n\\textbf{MathBERT}, which is jointly trained with mathematical formulas and\ntheir corresponding contexts. In addition, in order to further capture the\nsemantic-level structural features of formulas, a new pre-training task is\ndesigned to predict the masked formula substructures extracted from the\nOperator Tree (OPT), which is the semantic structural representation of\nformulas. We conduct various experiments on three downstream tasks to evaluate\nthe performance of MathBERT, including mathematical information retrieval,\nformula topic classification and formula headline generation. Experimental\nresults demonstrate that MathBERT significantly outperforms existing methods on\nall those three tasks. Moreover, we qualitatively show that this pre-trained\nmodel effectively captures the semantic-level structural information of\nformulas. To the best of our knowledge, MathBERT is the first pre-trained model\nfor mathematical formula understanding.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 02:10:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Peng", "Shuai", ""], ["Yuan", "Ke", ""], ["Gao", "Liangcai", ""], ["Tang", "Zhi", ""]]}, {"id": "2105.00403", "submitter": "Koji Inoue", "authors": "Tatsuya Kawahara, Koji Inoue, Divesh Lala", "title": "Intelligent Conversational Android ERICA Applied to Attentive Listening\n  and Job Interview", "comments": "7 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Following the success of spoken dialogue systems (SDS) in smartphone\nassistants and smart speakers, a number of communicative robots are developed\nand commercialized. Compared with the conventional SDSs designed as a\nhuman-machine interface, interaction with robots is expected to be in a closer\nmanner to talking to a human because of the anthropomorphism and physical\npresence. The goal or task of dialogue may not be information retrieval, but\nthe conversation itself. In order to realize human-level \"long and deep\"\nconversation, we have developed an intelligent conversational android ERICA. We\nset up several social interaction tasks for ERICA, including attentive\nlistening, job interview, and speed dating. To allow for spontaneous,\nincremental multiple utterances, a robust turn-taking model is implemented\nbased on TRP (transition-relevance place) prediction, and a variety of\nbackchannels are generated based on time frame-wise prediction instead of\nIPU-based prediction. We have realized an open-domain attentive listening\nsystem with partial repeats and elaborating questions on focus words as well as\nassessment responses. It has been evaluated with 40 senior people, engaged in\nconversation of 5-7 minutes without a conversation breakdown. It was also\ncompared against the WOZ setting. We have also realized a job interview system\nwith a set of base questions followed by dynamic generation of elaborating\nquestions. It has also been evaluated with student subjects, showing promising\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 06:37:23 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kawahara", "Tatsuya", ""], ["Inoue", "Koji", ""], ["Lala", "Divesh", ""]]}, {"id": "2105.00477", "submitter": "Debanjana Kar", "authors": "Debanjana Kar, Sudeshna Sarkar, Pawan Goyal", "title": "Event Argument Extraction using Causal Knowledge Structures", "comments": "10 pages, 6 figures, Accepted in 17th International Conference on\n  Natural Language Processing (ICON 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event Argument extraction refers to the task of extracting structured\ninformation from unstructured text for a particular event of interest. The\nexisting works exhibit poor capabilities to extract causal event arguments like\nReason and After Effects. Furthermore, most of the existing works model this\ntask at a sentence level, restricting the context to a local scope. While it\nmay be effective for short spans of text, for longer bodies of text such as\nnews articles, it has often been observed that the arguments for an event do\nnot necessarily occur in the same sentence as that containing an event trigger.\nTo tackle the issue of argument scattering across sentences, the use of global\ncontext becomes imperative in this task. In our work, we propose an external\nknowledge aided approach to infuse document-level event information to aid the\nextraction of complex event arguments. We develop a causal network for our\nevent-annotated dataset by extracting relevant event causal structures from\nConceptNet and phrases from Wikipedia. We use the extracted event causal\nfeatures in a bi-directional transformer encoder to effectively capture\nlong-range inter-sentence dependencies. We report the effectiveness of our\nproposed approach through both qualitative and quantitative analysis. In this\ntask, we establish our findings on an event annotated dataset in 5 Indian\nlanguages. This dataset adds further complexity to the task by labelling\narguments of entity type (like Time, Place) as well as more complex argument\ntypes (like Reason, After-Effect). Our approach achieves state-of-the-art\nperformance across all the five languages. Since our work does not rely on any\nlanguage-specific features, it can be easily extended to other languages.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 13:59:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kar", "Debanjana", ""], ["Sarkar", "Sudeshna", ""], ["Goyal", "Pawan", ""]]}, {"id": "2105.00572", "submitter": "Alexis Conneau", "authors": "Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau", "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has demonstrated the effectiveness of cross-lingual language\nmodel pretraining for cross-lingual understanding. In this study, we present\nthe results of two larger multilingual masked language models, with 3.5B and\n10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform\nXLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the\nRoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on\naverage while handling 99 more languages. This suggests pretrained models with\nlarger capacity may obtain both strong performance on high-resource languages\nwhile greatly improving low-resource languages. We make our code and models\npublicly available.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:15:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Goyal", "Naman", ""], ["Du", "Jingfei", ""], ["Ott", "Myle", ""], ["Anantharaman", "Giri", ""], ["Conneau", "Alexis", ""]]}, {"id": "2105.00573", "submitter": "Siddharth Dalmia", "authors": "Siddharth Dalmia, Brian Yan, Vikas Raunak, Florian Metze and Shinji\n  Watanabe", "title": "Searchable Hidden Intermediates for End-to-End Models of Decomposable\n  Sequence Tasks", "comments": "NAACL 2021. All code and models are released as part of the ESPnet\n  toolkit: https://github.com/espnet/espnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches for sequence tasks are becoming increasingly popular.\nYet for complex sequence tasks, like speech translation, systems that cascade\nseveral models trained on sub-tasks have shown to be superior, suggesting that\nthe compositionality of cascaded systems simplifies learning and enables\nsophisticated search capabilities. In this work, we present an end-to-end\nframework that exploits compositionality to learn searchable hidden\nrepresentations at intermediate stages of a sequence model using decomposed\nsub-tasks. These hidden intermediates can be improved using beam search to\nenhance the overall performance and can also incorporate external models at\nintermediate stages of the network to re-score or adapt towards out-of-domain\ndata. One instance of the proposed framework is a Multi-Decoder model for\nspeech translation that extracts the searchable hidden intermediates from a\nspeech recognition sub-task. The model demonstrates the aforementioned benefits\nand outperforms the previous state-of-the-art by around +6 and +3 BLEU on the\ntwo test sets of Fisher-CallHome and by around +3 and +4 BLEU on the\nEnglish-German and English-French test sets of MuST-C.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:22:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dalmia", "Siddharth", ""], ["Yan", "Brian", ""], ["Raunak", "Vikas", ""], ["Metze", "Florian", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2105.00574", "submitter": "Workneh Y. Ayele", "authors": "W. Y. Ayele", "title": "Adapting CRISP-DM for Idea Mining: A Data Mining Process for Generating\n  Ideas Using a Textual Dataset", "comments": "13 pages, 14 figures. International Journal of Advanced Computer\n  Science and Applications, 2020", "journal-ref": null, "doi": "10.14569/issn.2156-5570", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data mining project managers can benefit from using standard data mining\nprocess models. The benefits of using standard process models for data mining,\nsuch as the de facto and the most popular, Cross-Industry-Standard-Process\nmodel for Data Mining (CRISP-DM) are reduced cost and time. Also, standard\nmodels facilitate knowledge transfer, reuse of best practices, and minimize\nknowledge requirements. On the other hand, to unlock the potential of\never-growing textual data such as publications, patents, social media data, and\ndocuments of various forms, digital innovation is increasingly needed.\nFurthermore, the introduction of cutting-edge machine learning tools and\ntechniques enable the elicitation of ideas. The processing of unstructured\ntextual data to generate new and useful ideas is referred to as idea mining.\nExisting literature about idea mining merely overlooks the utilization of\nstandard data mining process models. Therefore, the purpose of this paper is to\npropose a reusable model to generate ideas, CRISP-DM, for Idea Mining\n(CRISP-IM). The design and development of the CRISP-IM are done following the\ndesign science approach. The CRISP-IM facilitates idea generation, through the\nuse of Dynamic Topic Modeling (DTM), unsupervised machine learning, and\nsubsequent statistical analysis on a dataset of scholarly articles. The adapted\nCRISP-IM can be used to guide the process of identifying trends using scholarly\nliterature datasets or temporally organized patent or any other textual dataset\nof any domain to elicit ideas. The ex-post evaluation of the CRISP-IM is left\nfor future study.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:24:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ayele", "W. Y.", ""]]}, {"id": "2105.00809", "submitter": "Dessalew Yohannes", "authors": "Dessalew Yohannes and Yeregal Assabie", "title": "Amharic Text Clustering Using Encyclopedic Knowledge with Neural Word\n  Embedding", "comments": "9 PAGES, AfricaNLP 2021 submission (#33) for the workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this digital era, almost in every discipline people are using automated\nsystems that generate information represented in document format in different\nnatural languages. As a result, there is a growing interest towards better\nsolutions for finding, organizing and analyzing these documents. In this paper,\nwe propose a system that clusters Amharic text documents using Encyclopedic\nKnowledge (EK) with neural word embedding. EK enables the representation of\nrelated concepts and neural word embedding allows us to handle the contexts of\nthe relatedness. During the clustering process, all the text documents pass\nthrough preprocessing stages. Enriched text document features are extracted\nfrom each document by mapping with EK and word embedding model. TF-IDF weighted\nvector of enriched feature was generated. Finally, text documents are clustered\nusing popular spherical K-means algorithm. The proposed system is tested with\nAmharic text corpus and Amharic Wikipedia data. Test results show that the use\nof EK with word embedding for document clustering improves the average accuracy\nover the use of only EK. Furthermore, changing the size of the class has a\nsignificant effect on accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:37:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yohannes", "Dessalew", ""], ["Assabie", "Yeregal", ""]]}, {"id": "2105.00810", "submitter": "Wuraola   Fisayo Oyewusi", "authors": "Wuraola Fisayo Oyewusi, Olubayo Adekanmbi, Ifeoma Okoh, Vitus Onuigwe,\n  Mary Idera Salami, Opeyemi Osakuade, Sharon Ibejih, Usman Abdullahi Musa", "title": "NaijaNER : Comprehensive Named Entity Recognition for 5 Nigerian\n  Languages", "comments": "Accepted at the AfricaNLP Workshop, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the common applications of Named Entity Recognition (NER) is on\nEnglish and other highly available languages. In this work, we present our\nfindings on Named Entity Recognition for 5 Nigerian Languages (Nigerian\nEnglish, Nigerian Pidgin English, Igbo, Yoruba and Hausa). These languages are\nconsidered low-resourced, and very little openly available Natural Language\nProcessing work has been done in most of them. In this work, individual NER\nmodels were trained and metrics recorded for each of the languages. We also\nworked on a combined model that can handle Named Entity Recognition (NER) for\nany of the five languages. The combined model works well for Named Entity\nRecognition(NER) on each of the languages and with better performance compared\nto individual NER models trained specifically on annotated data for the\nspecific language. The aim of this work is to share our learning on how\ninformation extraction using Named Entity Recognition can be optimized for the\nlisted Nigerian Languages for inclusion, ease of deployment in production and\nreusability of models. Models developed during this project are available on\nGitHub https://git.io/JY0kk and an interactive web app\nhttps://nigner.herokuapp.com/.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:10:54 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Oyewusi", "Wuraola Fisayo", ""], ["Adekanmbi", "Olubayo", ""], ["Okoh", "Ifeoma", ""], ["Onuigwe", "Vitus", ""], ["Salami", "Mary Idera", ""], ["Osakuade", "Opeyemi", ""], ["Ibejih", "Sharon", ""], ["Musa", "Usman Abdullahi", ""]]}, {"id": "2105.00811", "submitter": "Abdelghny Orogat", "authors": "Abdelghny Orogat, Isabelle Liu, Ahmed El-Roby", "title": "CBench: Towards Better Evaluation of Question Answering Over Knowledge\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been an increase in the number of knowledge graphs that\ncan be only queried by experts. However, describing questions using structured\nqueries is not straightforward for non-expert users who need to have sufficient\nknowledge about both the vocabulary and the structure of the queried knowledge\ngraph, as well as the syntax of the structured query language used to describe\nthe user's information needs. The most popular approach introduced to overcome\nthe aforementioned challenges is to use natural language to query these\nknowledge graphs. Although several question answering benchmarks can be used to\nevaluate question-answering systems over a number of popular knowledge graphs,\nchoosing a benchmark to accurately assess the quality of a question answering\nsystem is a challenging task.\n  In this paper, we introduce CBench, an extensible, and more informative\nbenchmarking suite for analyzing benchmarks and evaluating question answering\nsystems. CBench can be used to analyze existing benchmarks with respect to\nseveral fine-grained linguistic, syntactic, and structural properties of the\nquestions and queries in the benchmark. We show that existing benchmarks vary\nsignificantly with respect to these properties deeming choosing a small subset\nof them unreliable in evaluating QA systems. Until further research improves\nthe quality and comprehensiveness of benchmarks, CBench can be used to\nfacilitate this evaluation using a set of popular benchmarks that can be\naugmented with other user-provided benchmarks. CBench not only evaluates a\nquestion answering system based on popular single-number metrics but also gives\na detailed analysis of the linguistic, syntactic, and structural properties of\nanswered and unanswered questions to better help the developers of question\nanswering systems to better understand where their system excels and where it\nstruggles.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:41:14 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Orogat", "Abdelghny", ""], ["Liu", "Isabelle", ""], ["El-Roby", "Ahmed", ""]]}, {"id": "2105.00812", "submitter": "Jinchuan Tian", "authors": "Jinchuan Tian, Rongzhi Gu, Helin Wang, Yuexian Zou", "title": "Layer Reduction: Accelerating Conformer-Based Self-Supervised Model via\n  Layer Consistency", "comments": "5 pages, 3 figures, submit to Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transformer-based self-supervised models are trained as feature extractors\nand have empowered many downstream speech tasks to achieve state-of-the-art\nperformance. However, both the training and inference process of these models\nmay encounter prohibitively high computational cost and large parameter budget.\nAlthough Parameter Sharing Strategy (PSS) proposed in ALBERT paves the way for\nparameter reduction, the computation required remains the same. Interestingly,\nwe found in experiments that distributions of feature embeddings from different\nTransformer layers are similar when PSS is integrated: a property termed as\nLayer Consistency (LC) in this paper. Given this similarity of feature\ndistributions, we assume that feature embeddings from different layers would\nhave similar representing power. In this work, Layer Consistency enables us to\nadopt Transformer-based models in a more efficient manner: the number of\nConformer layers in each training iteration could be uniformly sampled and\nShallow Layer Inference (SLI) could be applied to reduce the number of layers\nin inference stage. In experiments, our models are trained with LibriSpeech\ndataset and then evaluated on both phone classification and Speech Recognition\ntasks. We experimentally achieve 7.8X parameter reduction, 41.9% training\nspeedup and 37.7% inference speedup while maintaining comparable performance\nwith conventional BERT-like self-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:21:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tian", "Jinchuan", ""], ["Gu", "Rongzhi", ""], ["Wang", "Helin", ""], ["Zou", "Yuexian", ""]]}, {"id": "2105.00813", "submitter": "Preslav Nakov", "authors": "Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov", "title": "Transformers: \"The End of History\" for NLP?", "comments": "Transformers, NLP, BERT, RoBERTa, XLNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural architectures, such as the Transformer, coupled\nwith the emergence of large-scale pre-trained models such as BERT, have\nrevolutionized the field of Natural Language Processing (NLP), pushing the\nstate-of-the-art for a number of NLP tasks. A rich family of variations of\nthese models has been proposed, such as RoBERTa, ALBERT, and XLNet, but\nfundamentally, they all remain limited in their ability to model certain kinds\nof information, and they cannot cope with certain information sources, which\nwas easy for pre-existing models. Thus, here we aim to shed some light on some\nimportant theoretical limitations of pre-trained BERT-style models that are\ninherent in the general Transformer architecture. First, we demonstrate in\npractice on two general types of tasks -- segmentation and segment labeling --\nand four datasets that these limitations are indeed harmful and that addressing\nthem, even in some very simple and naive ways, can yield sizable improvements\nover vanilla RoBERTa and XLNet. Then, we offer a more general discussion on\ndesiderata for future additions to the Transformer architecture that would\nincrease its expressiveness, which we hope could help in the design of the next\ngeneration of deep NLP architectures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:29:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chernyavskiy", "Anton", ""], ["Ilvovsky", "Dmitry", ""], ["Nakov", "Preslav", ""]]}, {"id": "2105.00815", "submitter": "Zhuang Li", "authors": "Zhuang Li", "title": "Representation Learning for Weakly Supervised Relation Extraction", "comments": "Master Research Thesis of the Australian National University, 60\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen rapid development in Information Extraction, as well\nas its subtask, Relation Extraction. Relation Extraction is able to detect\nsemantic relations between entities in sentences. Currently, many efficient\napproaches have been applied to relation extraction tasks. Supervised learning\napproaches especially have good performance. However, there are still many\ndifficult challenges. One of the most serious problems is that manually labeled\ndata is difficult to acquire. In most cases, limited data for supervised\napproaches equals lousy performance. Thus here, under the situation with only\nlimited training data, we focus on how to improve the performance of our\nsupervised baseline system with unsupervised pre-training. Feature is one of\nthe key components in improving the supervised approaches. Traditional\napproaches usually apply hand-crafted features, which require expert knowledge\nand expensive human labor. However, this type of feature might suffer from data\nsparsity: when the training set size is small, the model parameters might be\npoorly estimated. In this thesis, we present several novel unsupervised\npre-training models to learn the distributed text representation features,\nwhich are encoded with rich syntactic-semantic patterns of relation\nexpressions. The experiments have demonstrated that this type of feature,\ncombine with the traditional hand-crafted features, could improve the\nperformance of the logistic classification model for relation extraction,\nespecially on the classification of relations with only minor training\ninstances.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 12:22:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Zhuang", ""]]}, {"id": "2105.00816", "submitter": "Griffin Adams", "authors": "Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, No\\'emie\n  Elhadad", "title": "What's in a Summary? Laying the Groundwork for Advances in\n  Hospital-Course Summarization", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization of clinical narratives is a long-standing research problem.\nHere, we introduce the task of hospital-course summarization. Given the\ndocumentation authored throughout a patient's hospitalization, generate a\nparagraph that tells the story of the patient admission. We construct an\nEnglish, text-to-text dataset of 109,000 hospitalizations (2M source notes) and\ntheir corresponding summary proxy: the clinician-authored \"Brief Hospital\nCourse\" paragraph written as part of a discharge note. Exploratory analyses\nreveal that the BHC paragraphs are highly abstractive with some long extracted\nfragments; are concise yet comprehensive; differ in style and content\norganization from the source notes; exhibit minimal lexical cohesion; and\nrepresent silver-standard references. Our analysis identifies multiple\nimplications for modeling this complex, multi-document summarization task.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:31:48 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Adams", "Griffin", ""], ["Alsentzer", "Emily", ""], ["Ketenci", "Mert", ""], ["Zucker", "Jason", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "2105.00817", "submitter": "Andr\\'e Bodmer Dr.", "authors": "Michael Freunek and Andr\\'e Bodmer", "title": "BERT based freedom to operate patent analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method to apply BERT to freedom to operate patent\nanalysis and patent searches. According to the method, BERT is fine-tuned by\ntraining patent descriptions to the independent claims. Each description\nrepresents an invention which is protected by the corresponding claims. Such a\ntrained BERT could be able to identify or order freedom to operate relevant\npatents based on a short description of an invention or product. We tested the\nmethod by training BERT on the patent class G06T1/00 and applied the trained\nBERT on five inventions classified in G06T1/60, described via DOCDB abstracts.\nThe DOCDB abstract are available on ESPACENET of the European Patent Office.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:30:46 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 07:14:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Freunek", "Michael", ""], ["Bodmer", "Andr\u00e9", ""]]}, {"id": "2105.00819", "submitter": "Schyan Zafar", "authors": "Schyan Zafar and Geoff Nicholls", "title": "Measuring diachronic sense change: new models and Monte Carlo methods\n  for Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a bag-of-words model, the senses of a word with multiple meanings, e.g.\n\"bank\" (used either in a river-bank or an institution sense), are represented\nas probability distributions over context words, and sense prevalence is\nrepresented as a probability distribution over senses. Both of these may change\nwith time. Modelling and measuring this kind of sense change is challenging due\nto the typically high-dimensional parameter space and sparse datasets. A\nrecently published corpus of ancient Greek texts contains expert-annotated\nsense labels for selected target words. Automatic sense-annotation for the word\n\"kosmos\" (meaning decoration, order or world) has been used as a test case in\nrecent work with related generative models and Monte Carlo methods. We adapt an\nexisting generative sense change model to develop a simpler model for the main\neffects of sense and time, and give MCMC methods for Bayesian inference on all\nthese models that are more efficient than existing methods. We carry out\nautomatic sense-annotation of snippets containing \"kosmos\" using our model, and\nmeasure the time-evolution of its three senses and their prevalence. As far as\nwe are aware, ours is the first analysis of this data, within the class of\ngenerative models we consider, that quantifies uncertainty and returns credible\nsets for evolving sense prevalence in good agreement with those given by expert\nannotation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:40:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zafar", "Schyan", ""], ["Nicholls", "Geoff", ""]]}, {"id": "2105.00823", "submitter": "Guy Marshall", "authors": "Guy Marshall and Mokanarangan Thayaparan and Philip Osborne and Andre\n  Freitas", "title": "Switching Contexts: Transportability Measures for NLP", "comments": "10 pages, 4 figures. To appear in IWCS 2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the topic of transportability, as a sub-area of\ngeneralisability. By proposing the utilisation of metrics based on\nwell-established statistics, we are able to estimate the change in performance\nof NLP models in new contexts. Defining a new measure for transportability may\nallow for better estimation of NLP system performance in new domains, and is\ncrucial when assessing the performance of NLP systems in new tasks and domains.\nThrough several instances of increasing complexity, we demonstrate how\nlightweight domain similarity measures can be used as estimators for the\ntransportability in NLP applications. The proposed transportability measures\nare evaluated in the context of Named Entity Recognition and Natural Language\nInference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:15:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Marshall", "Guy", ""], ["Thayaparan", "Mokanarangan", ""], ["Osborne", "Philip", ""], ["Freitas", "Andre", ""]]}, {"id": "2105.00824", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum", "title": "A Survey of Recent Abstract Summarization Techniques", "comments": "6 tables, 1 figure, additionals (data):\n  https://drive.google.com/drive/folders/152XMSCU3ctshB2BvzEQHY0vo6xkZ9gOl?usp=sharing\n  ,\n  https://drive.google.com/drive/folders/1z09D2-4arE6aOQZxgxcwMVF41xMH4EEH?usp=sharing.\n  Awaiting at https://www.springer.com/gp/book/9789811621017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper surveys several recent abstract summarization methods: T5,\nPegasus, and ProphetNet. We implement the systems in two languages: English and\nIndonesian languages. We investigate the impact of pre-training models (one T5,\nthree Pegasuses, three ProphetNets) on several Wikipedia datasets in English\nand Indonesian language and compare the results to the Wikipedia systems'\nsummaries. The T5-Large, the Pegasus-XSum, and the ProphetNet-CNNDM provide the\nbest summarization. The most significant factors that influence ROUGE\nperformance are coverage, density, and compression. The higher the scores, the\nbetter the summary. Other factors that influence the ROUGE scores are the\npre-training goal, the dataset's characteristics, the dataset used for testing\nthe pre-trained model, and the cross-lingual function. Several suggestions to\nimprove this paper's limitation are: 1) assure that the dataset used for the\npre-training model must sufficiently large, contains adequate instances for\nhandling cross-lingual purpose; 2) Advanced process (finetuning) shall be\nreasonable. We recommend using the large dataset consists of comprehensive\ncoverage of topics from many languages before implementing advanced processes\nsuch as the train-infer-train procedure to the zero-shot translation in the\ntraining stage of the pre-training model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:01:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Puspitaningrum", "Diyah", ""]]}, {"id": "2105.00825", "submitter": "Yu Li", "authors": "Yu Li, Shirley Anugrah Hayati, Weiyan Shi and Zhou Yu", "title": "DEUX: An Attribute-Guided Framework for Sociable Recommendation Dialog\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important for sociable recommendation dialog systems to perform as both\non-task content and social content to engage users and gain their favor. In\naddition to understand the user preferences and provide a satisfying\nrecommendation, such systems must be able to generate coherent and natural\nsocial conversations to the user. Traditional dialog state tracking cannot be\napplied to such systems because it does not track the attributes in the social\ncontent. To address this challenge, we propose DEUX, a novel attribute-guided\nframework to create better user experiences while accomplishing a movie\nrecommendation task. DEUX has a module that keeps track of the movie attributes\n(e.g., favorite genres, actors,etc.) in both user utterances and system\nresponses. This allows the system to introduce new movie attributes in its\nsocial content. Then, DEUX has multiple values for the same attribute type\nwhich suits the recommendation task since a user may like multiple genres, for\ninstance. Experiments suggest that DEUX outperforms all the baselines on being\nmore consistent, fitting the user preferences better, and providing a more\nengaging chat experience. Our approach can be used for any similar problems of\nsociable task-oriented dialog system.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:12:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Yu", ""], ["Hayati", "Shirley Anugrah", ""], ["Shi", "Weiyan", ""], ["Yu", "Zhou", ""]]}, {"id": "2105.00826", "submitter": "Preslav Nakov", "authors": "Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov", "title": "WhatTheWikiFact: Fact-Checking Claims Against Wikipedia", "comments": "fact-checking, veracity, factuality, stance detection, evidence\n  retrieval, fake news, FEVER, Wikipedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of Internet has made it a major source of information.\nUnfortunately, not all information online is true, and thus a number of\nfact-checking initiatives have been launched, both manual and automatic. Here,\nwe present our contribution in this regard: WhatTheWikiFact, a system for\nautomatic claim verification using Wikipedia. The system predicts the veracity\nof an input claim, and it further shows the evidence it has retrieved as part\nof the verification process. It shows confidence scores and a list of relevant\nWikipedia articles, together with detailed information about each article,\nincluding the phrase used to retrieve it, the most relevant sentences it\ncontains, and their stances with respect to the input claim, with associated\nprobabilities.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:23:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chernyavskiy", "Anton", ""], ["Ilvovsky", "Dmitry", ""], ["Nakov", "Preslav", ""]]}, {"id": "2105.00827", "submitter": "Katikapalli Subramanyam Kalyan", "authors": "Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha", "title": "AMMU -- A Survey of Transformer-based Biomedical Pretrained Language\n  Models", "comments": "Preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioMegatron and CoderBERT models. We strongly believe there is a need\nfor a survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:09:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kalyan", "Katikapalli Subramanyam", ""], ["Rajasekharan", "Ajit", ""], ["Sangeetha", "Sivanesan", ""]]}, {"id": "2105.00828", "submitter": "Michael T\\\"anzer Mr", "authors": "Michael T\\\"anzer, Sebastian Ruder, Marek Rei", "title": "BERT memorisation and pitfalls in low-resource scenarios", "comments": "14 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art pre-trained models have been shown to memorise facts and\nperform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal performances even on extremely noisy datasets.\nConversely, we also find that they completely fail when tested on low-resource\ntasks such as few-shot learning and rare entity recognition. To mitigate such\nlimitations, we propose a novel architecture based on BERT and prototypical\nnetworks that improves performance in low-resource named entity recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:53:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["T\u00e4nzer", "Michael", ""], ["Ruder", "Sebastian", ""], ["Rei", "Marek", ""]]}, {"id": "2105.00830", "submitter": "Vignav Ramesh", "authors": "Vignav Ramesh, Anton Kolonin", "title": "Natural Language Generation Using Link Grammar for General\n  Conversational Intelligence", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current artificial general intelligence (AGI) and natural language\nprocessing (NLP) architectures do not possess general conversational\nintelligence--that is, they either do not deal with language or are unable to\nconvey knowledge in a form similar to the human language without manual,\nlabor-intensive methods such as template-based customization. In this paper, we\npropose a new technique to automatically generate grammatically valid sentences\nusing the Link Grammar database. This natural language generation method far\noutperforms current state-of-the-art baselines and may serve as the final\ncomponent in a proto-AGI question answering pipeline that understandably\nhandles natural language material.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:16:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ramesh", "Vignav", ""], ["Kolonin", "Anton", ""]]}, {"id": "2105.00831", "submitter": "Lodovico Giaretta", "authors": "Daniel Garcia Bernal, Lodovico Giaretta, Sarunas Girdzijauskas, Magnus\n  Sahlgren", "title": "Federated Word2Vec: Leveraging Federated Learning to Encourage\n  Collaborative Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large scale contextual representation models have significantly advanced NLP\nin recent years, understanding the semantics of text to a degree never seen\nbefore. However, they need to process large amounts of data to achieve\nhigh-quality results. Joining and accessing all these data from multiple\nsources can be extremely challenging due to privacy and regulatory reasons.\nFederated Learning can solve these limitations by training models in a\ndistributed fashion, taking advantage of the hardware of the devices that\ngenerate the data. We show the viability of training NLP models, specifically\nWord2Vec, with the Federated Learning protocol. In particular, we focus on a\nscenario in which a small number of organizations each hold a relatively large\ncorpus. The results show that neither the quality of the results nor the\nconvergence time in Federated Word2Vec deteriorates as compared to centralised\nWord2Vec.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:39:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bernal", "Daniel Garcia", ""], ["Giaretta", "Lodovico", ""], ["Girdzijauskas", "Sarunas", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "2105.00846", "submitter": "Alexander Robertson", "authors": "Alexander Robertson, Farhana Ferdousi Liza, Dong Nguyen, Barbara\n  McGillivray, Scott A. Hale", "title": "Semantic Journeys: Quantifying Change in Emoji Meaning from 2012-2018", "comments": null, "journal-ref": "4th International Workshop on Emoji Understanding and Applications\n  in Social Media 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The semantics of emoji has, to date, been considered from a static\nperspective. We offer the first longitudinal study of how emoji semantics\nchanges over time, applying techniques from computational linguistics to six\nyears of Twitter data. We identify five patterns in emoji semantic development\nand find evidence that the less abstract an emoji is, the more likely it is to\nundergo semantic change. In addition, we analyse select emoji in more detail,\nexamining the effect of seasonality and world events on emoji semantics. To aid\nfuture work on emoji and semantics, we make our data publicly available along\nwith a web-based interface that anyone can use to explore semantic change in\nemoji.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:35:10 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 08:28:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Robertson", "Alexander", ""], ["Liza", "Farhana Ferdousi", ""], ["Nguyen", "Dong", ""], ["McGillivray", "Barbara", ""], ["Hale", "Scott A.", ""]]}, {"id": "2105.00858", "submitter": "Rui Zhao", "authors": "Rui Zhao, Jian Xue, Jinyu Li, Wenning Wei, Lei He, Yifan Gong", "title": "On Addressing Practical Challenges for RNN-Transducer", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, several works are proposed to address practical challenges for\ndeploying RNN Transducer (RNN-T) based speech recognition system. These\nchallenges are adapting a well-trained RNN-T model to a new domain without\ncollecting the audio data, obtaining time stamps and confidence scores at word\nlevel. The first challenge is solved with a splicing data method which\nconcatenates the speech segments extracted from the source domain data. To get\nthe time stamp, a phone prediction branch is added to the RNN-T model by\nsharing the encoder for the purpose of force alignment. Finally, we obtain\nword-level confidence scores by utilizing several types of features calculated\nduring decoding and from confusion network. Evaluated with Microsoft production\ndata, the splicing data adaptation method improves the baseline and adaptation\nwith the text to speech method by 58.03% and 15.25% relative word error rate\nreduction, respectively. The proposed time stamping method can get less than\n50ms word timing difference from the ground truth alignment on average while\nmaintaining the recognition accuracy of the RNN-T model. We also obtain high\nconfidence annotation performance with limited computation cost.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 23:31:43 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 17:11:18 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 18:30:21 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhao", "Rui", ""], ["Xue", "Jian", ""], ["Li", "Jinyu", ""], ["Wei", "Wenning", ""], ["He", "Lei", ""], ["Gong", "Yifan", ""]]}, {"id": "2105.00895", "submitter": "Sowmya Vajjala", "authors": "Sowmya Vajjala", "title": "Teaching NLP outside Linguistics and Computer Science classrooms: Some\n  challenges and some opportunities", "comments": "To appear in the Teaching NLP workshop at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  NLP's sphere of influence went much beyond computer science research and the\ndevelopment of software applications in the past decade. We see people using\nNLP methods in a range of academic disciplines from Asian Studies to Clinical\nOncology. We also notice the presence of NLP as a module in most of the data\nscience curricula within and outside of regular university setups. These\ncourses are taken by students from very diverse backgrounds. This paper takes a\ncloser look at some issues related to teaching NLP to these diverse audiences\nbased on my classroom experiences, and identifies some challenges the\ninstructors face, particularly when there is no ecosystem of related courses\nfor the students. In this process, it also identifies a few challenge areas for\nboth NLP researchers and tool developers.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:30:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Vajjala", "Sowmya", ""]]}, {"id": "2105.00896", "submitter": "Congying Xia", "authors": "Congying Xia, Caiming Xiong, Philip Yu", "title": "Pseudo Siamese Network for Few-shot Intent Generation", "comments": "5 pages, accepted to SIGIR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot intent detection is a challenging task due to the scare annotation\nproblem. In this paper, we propose a Pseudo Siamese Network (PSN) to generate\nlabeled data for few-shot intents and alleviate this problem. PSN consists of\ntwo identical subnetworks with the same structure but different weights: an\naction network and an object network. Each subnetwork is a transformer-based\nvariational autoencoder that tries to model the latent distribution of\ndifferent components in the sentence. The action network is learned to\nunderstand action tokens and the object network focuses on object-related\nexpressions. It provides an interpretable framework for generating an utterance\nwith an action and an object existing in a given intent. Experiments on two\nreal-world datasets show that PSN achieves state-of-the-art performance for the\ngeneralized few shot intent detection task.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:30:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xia", "Congying", ""], ["Xiong", "Caiming", ""], ["Yu", "Philip", ""]]}, {"id": "2105.00908", "submitter": "Christine Basta", "authors": "Christine Basta and Marta R. Costa-juss\\`a", "title": "Impact of Gender Debiased Word Embeddings in Language Modeling", "comments": "9 pages, 2 figures, 6 tables, accepted in 20th International\n  Conference on Intelligent Text Processing and Computational Linguistics,\n  CICLing 2019. To be published in Springer LNCS volume", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gender, race and social biases have recently been detected as evident\nexamples of unfairness in applications of Natural Language Processing. A key\npath towards fairness is to understand, analyse and interpret our data and\nalgorithms. Recent studies have shown that the human-generated data used in\ntraining is an apparent factor of getting biases. In addition, current\nalgorithms have also been proven to amplify biases from data.\n  To further address these concerns, in this paper, we study how an\nstate-of-the-art recurrent neural language model behaves when trained on data,\nwhich under-represents females, using pre-trained standard and debiased word\nembeddings. Results show that language models inherit higher bias when trained\non unbalanced data when using pre-trained embeddings, in comparison with using\nembeddings trained within the task. Moreover, results show that, on the same\ndata, language models inherit lower bias when using debiased pre-trained\nemdeddings, compared to using standard pre-trained embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:45:10 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 10:28:07 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 09:43:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Basta", "Christine", ""], ["Costa-juss\u00e0", "Marta R.", ""]]}, {"id": "2105.00944", "submitter": "Antonio Bruto da Costa", "authors": "Priyanka Sinha, Pabitra Mitra, Antonio Anastasio Bruto da Costa,\n  Nikolaos Kekatos", "title": "Explaining Outcomes of Multi-Party Dialogues using Causal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-party dialogues are common in enterprise social media on technical as\nwell as non-technical topics. The outcome of a conversation may be positive or\nnegative. It is important to analyze why a dialogue ends with a particular\nsentiment from the point of view of conflict analysis as well as future\ncollaboration design. We propose an explainable time series mining algorithm\nfor such analysis. A dialogue is represented as an attributed time series of\noccurrences of keywords, EMPATH categories, and inferred sentiments at various\npoints in its progress. A special decision tree, with decision metrics that\ntake into account temporal relationships between dialogue events, is used for\npredicting the cause of the outcome sentiment. Interpretable rules mined from\nthe classifier are used to explain the prediction. Experimental results are\npresented for the enterprise social media posts in a large company.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:18:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Sinha", "Priyanka", ""], ["Mitra", "Pabitra", ""], ["da Costa", "Antonio Anastasio Bruto", ""], ["Kekatos", "Nikolaos", ""]]}, {"id": "2105.00973", "submitter": "Sowmya Vajjala", "authors": "Sowmya Vajjala", "title": "Trends, Limitations and Open Challenges in Automatic Readability\n  Assessment Research", "comments": "Working paper. Contact author for missing references or suggestions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Readability assessment is the task of evaluating the reading difficulty of a\ngiven piece of text. Although research on computational approaches to\nreadability assessment is now two decades old, there is not much work on\nsynthesizing this research. This article is a brief survey of contemporary\nresearch on developing computational models for readability assessment. We\nidentify the common approaches, discuss their shortcomings, and identify some\nchallenges for the future. Where possible, we also connect computational\nresearch with insights from related work in other disciplines such as education\nand psychology.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:18:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Vajjala", "Sowmya", ""]]}, {"id": "2105.00981", "submitter": "Ilya Gusev", "authors": "Ilya Gusev, Ivan Smurov", "title": "Russian News Clustering and Headline Selection Shared Task", "comments": "Accepted to Dialogue 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results of the Russian News Clustering and Headline\nSelection shared task. As a part of it, we propose the tasks of Russian news\nevent detection, headline selection, and headline generation. These tasks are\naccompanied by datasets and baselines. The presented datasets for event\ndetection and headline selection are the first public Russian datasets for\ntheir tasks. The headline generation dataset is based on clustering and\nprovides multiple reference headlines for every cluster, unlike the previous\ndatasets. Finally, the approaches proposed by the shared task participants are\nreported and analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:32:33 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 21:23:28 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 17:39:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gusev", "Ilya", ""], ["Smurov", "Ivan", ""]]}, {"id": "2105.00982", "submitter": "Zolt\\'an T\\\"uske", "authors": "Zolt\\'an T\\\"uske, George Saon, Brian Kingsbury", "title": "On the limit of English conversational speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous work we demonstrated that a single headed attention\nencoder-decoder model is able to reach state-of-the-art results in\nconversational speech recognition. In this paper, we further improve the\nresults for both Switchboard 300 and 2000. Through use of an improved\noptimizer, speaker vector embeddings, and alternative speech representations we\nreduce the recognition errors of our LSTM system on Switchboard-300 by 4%\nrelative. Compensation of the decoder model with the probability ratio approach\nallows more efficient integration of an external language model, and we report\n5.9% and 11.5% WER on the SWB and CHM parts of Hub5'00 with very simple LSTM\nmodels. Our study also considers the recently proposed conformer, and more\nadvanced self-attention based language models. Overall, the conformer shows\nsimilar performance to the LSTM; nevertheless, their combination and decoding\nwith an improved LM reaches a new record on Switchboard-300, 5.0% and 10.0% WER\non SWB and CHM. Our findings are also confirmed on Switchboard-2000, and a new\nstate of the art is reported, practically reaching the limit of the benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:32:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["T\u00fcske", "Zolt\u00e1n", ""], ["Saon", "George", ""], ["Kingsbury", "Brian", ""]]}, {"id": "2105.00991", "submitter": "Yingwei Xin", "authors": "Yunwen Chen, Zuotao Liu, Daqi Ji, Yingwei Xin, Wenguang Wang, Lu Yao,\n  Yi Zou", "title": "Context-aware Ensemble of Multifaceted Factorization Models for\n  Recommendation Prediction in Social Networks", "comments": "KDD 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the solution of Shanda Innovations team to Task 1 of\nKDD-Cup 2012. A novel approach called Multifaceted Factorization Models is\nproposed to incorporate a great variety of features in social networks. Social\nrelationships and actions between users are integrated as implicit feedbacks to\nimprove the recommendation accuracy. Keywords, tags, profiles, time and some\nother features are also utilized for modeling user interests. In addition, user\nbehaviors are modeled from the durations of recommendation records. A\ncontext-aware ensemble framework is then applied to combine multiple predictors\nand produce final recommendation results. The proposed approach obtained\n0.43959 (public score) / 0.41874 (private score) on the testing dataset, which\nachieved the 2nd place in the KDD-Cup competition.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:42:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Yunwen", ""], ["Liu", "Zuotao", ""], ["Ji", "Daqi", ""], ["Xin", "Yingwei", ""], ["Wang", "Wenguang", ""], ["Yao", "Lu", ""], ["Zou", "Yi", ""]]}, {"id": "2105.01009", "submitter": "Hyun Gi Lee", "authors": "Hyun Gi Lee, Evan Sholle, Ashley Beecy, Subhi Al'Aref and Yifan Peng", "title": "Leveraging Deep Representations of Radiology Reports in Survival\n  Analysis for Predicting Heart Failure Patient Mortality", "comments": "NAACL-HLT 2021, Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Utilizing clinical texts in survival analysis is difficult because they are\nlargely unstructured. Current automatic extraction models fail to capture\ntextual information comprehensively since their labels are limited in scope.\nFurthermore, they typically require a large amount of data and high-quality\nexpert annotations for training. In this work, we present a novel method of\nusing BERT-based hidden layer representations of clinical texts as covariates\nfor proportional hazards models to predict patient survival outcomes. We show\nthat hidden layers yield notably more accurate predictions than predefined\nfeatures, outperforming the previous baseline model by 5.7% on average across\nC-index and time-dependent AUC. We make our work publicly available at\nhttps://github.com/bionlplab/heart_failure_mortality.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:54:52 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Hyun Gi", ""], ["Sholle", "Evan", ""], ["Beecy", "Ashley", ""], ["Al'Aref", "Subhi", ""], ["Peng", "Yifan", ""]]}, {"id": "2105.01029", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak and Neil Tenenholtz and Lester Mackey and Nicol\\`o Fusi", "title": "Initialization and Regularization of Factorized Neural Layers", "comments": "ICLR 2021 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorized layers--operations parameterized by products of two or more\nmatrices--occur in a variety of deep learning contexts, including compressed\nmodel training, certain types of knowledge distillation, and multi-head\nself-attention architectures. We study how to initialize and regularize deep\nnets containing such layers, examining two simple, understudied schemes,\nspectral initialization and Frobenius decay, for improving their performance.\nThe guiding insight is to design optimization routines for these networks that\nare as close as possible to that of their well-tuned, non-decomposed\ncounterparts; we back this intuition with an analysis of how the initialization\nand regularization schemes impact training with gradient descent, drawing on\nmodern attempts to understand the interplay of weight-decay and\nbatch-normalization. Empirically, we highlight the benefits of spectral\ninitialization and Frobenius decay across a variety of settings. In model\ncompression, we show that they enable low-rank methods to significantly\noutperform both unstructured sparsity and tensor methods on the task of\ntraining low-memory residual networks; analogs of the schemes also improve the\nperformance of tensor decomposition techniques. For knowledge distillation,\nFrobenius decay enables a simple, overcomplete baseline that yields a compact\nmodel from over-parameterized training without requiring retraining with or\npruning a teacher network. Finally, we show how both schemes applied to\nmulti-head attention lead to improved performance on both translation and\nunsupervised pre-training.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:28:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khodak", "Mikhail", ""], ["Tenenholtz", "Neil", ""], ["Mackey", "Lester", ""], ["Fusi", "Nicol\u00f2", ""]]}, {"id": "2105.01044", "submitter": "Eugene Yang", "authors": "Eugene Yang, Sean MacAvaney, David D. Lewis, Ophir Frieder", "title": "Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Technology-assisted review (TAR) refers to iterative active learning\nworkflows for document review in high recall retrieval (HRR) tasks. TAR\nresearch and most commercial TAR software have applied linear models such as\nlogistic regression or support vector machines to lexical features.\nTransformer-based models with supervised tuning have been found to improve\neffectiveness on many text classification tasks, suggesting their use in TAR.\nWe indeed find that the pre-trained BERT model reduces review volume by 30% in\nTAR workflows simulated on the RCV1-v2 newswire collection. In contrast, we\nfind that linear models outperform BERT for simulated legal discovery topics on\nthe Jeb Bush e-mail collection. This suggests the match between transformer\npre-training corpora and the task domain is more important than generally\nappreciated. Additionally, we show that just-right language model fine-tuning\non the task collection before starting active learning is critical. Both too\nlittle or too much fine-tuning results in performance worse than that of linear\nmodels, even for RCV1-v2.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:41:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yang", "Eugene", ""], ["MacAvaney", "Sean", ""], ["Lewis", "David D.", ""], ["Frieder", "Ophir", ""]]}, {"id": "2105.01051", "submitter": "Shu-Wen Yang", "authors": "Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal\n  Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting\n  Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang,\n  Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, Hung-yi Lee", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of\nthe shared model, we especially focus on extracting the representation learned\nfrom SSL due to its preferable re-usability. We present a simple framework to\nsolve SUPERB tasks by learning task-specialized lightweight prediction heads on\ntop of the frozen shared model. Our results demonstrate that the framework is\npromising as SSL representations show competitive generalizability and\naccessibility across SUPERB tasks. We release SUPERB as a challenge with a\nleaderboard and a benchmark toolkit to fuel the research in representation\nlearning and general speech processing.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:51:09 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 01:35:31 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 09:22:20 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yang", "Shu-wen", ""], ["Chi", "Po-Han", ""], ["Chuang", "Yung-Sung", ""], ["Lai", "Cheng-I Jeff", ""], ["Lakhotia", "Kushal", ""], ["Lin", "Yist Y.", ""], ["Liu", "Andy T.", ""], ["Shi", "Jiatong", ""], ["Chang", "Xuankai", ""], ["Lin", "Guan-Ting", ""], ["Huang", "Tzu-Hsien", ""], ["Tseng", "Wei-Cheng", ""], ["Lee", "Ko-tik", ""], ["Liu", "Da-Rong", ""], ["Huang", "Zili", ""], ["Dong", "Shuyan", ""], ["Li", "Shang-Wen", ""], ["Watanabe", "Shinji", ""], ["Mohamed", "Abdelrahman", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2105.01052", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala", "title": "Applied Language Technology: NLP for the Humanities", "comments": "Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This contribution describes a two-course module that seeks to provide\nhumanities majors with a basic understanding of language technology and its\napplications using Python. The learning materials consist of interactive\nJupyter Notebooks and accompanying YouTube videos, which are openly available\nwith a Creative Commons licence.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:51:17 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hiippala", "Tuomo", ""]]}, {"id": "2105.01129", "submitter": "Gaurav Sahu", "authors": "Gaurav Sahu, Robin Cohen, Olga Vechtomova", "title": "Towards A Multi-agent System for Online Hate Speech Detection", "comments": "Accepted to the 2nd International Workshop on Autonomous Agents for\n  Social Good (AASG), AAMAS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper envisions a multi-agent system for detecting the presence of hate\nspeech in online social media platforms such as Twitter and Facebook. We\nintroduce a novel framework employing deep learning techniques to coordinate\nthe channels of textual and im-age processing. Our experimental results aim to\ndemonstrate the effectiveness of our methods for classifying online content,\ntraining the proposed neural network model to effectively detect hateful\ninstances in the input. We conclude with a discussion of how our system may be\nof use to provide recommendations to users who are managing online social\nnetworks, showcasing the immense potential of intelligent multi-agent systems\ntowards delivering social good.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 19:06:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sahu", "Gaurav", ""], ["Cohen", "Robin", ""], ["Vechtomova", "Olga", ""]]}, {"id": "2105.01150", "submitter": "Shadi Shahsavari", "authors": "Pavan Holur, Shadi Shahsavari, Ehsan Ebrahimzadeh, Timothy R.\n  Tangherlini, Vwani Roychowdhury", "title": "Modeling Social Readers: Novel Tools for Addressing Reception from\n  Online Book Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readers' responses to literature have received scant attention in\ncomputational literary studies. The rise of social media offers an opportunity\nto capture a segment of these responses while data-driven analysis of these\nresponses can provide new critical insight into how people \"read\". Posts\ndiscussing an individual book on Goodreads, a social media platform that hosts\nuser discussions of popular literature, are referred to as \"reviews\", and\nconsist of plot summaries, opinions, quotes, or some mixture of these. Since\nthese reviews are written by readers, computationally modeling them allows one\nto discover the overall non-professional discussion space about a work,\nincluding an aggregated summary of the work's plot, an implicit ranking of the\nimportance of events, and the readers' impressions of main characters. We\ndevelop a pipeline of interlocking computational tools to extract a\nrepresentation of this reader generated shared narrative model. Using a corpus\nof reviews of five popular novels, we discover the readers' distillation of the\nmain storylines in a novel, their understanding of the relative importance of\ncharacters, as well as the readers' varying impressions of these characters. In\nso doing, we make three important contributions to the study of infinite\nvocabulary networks: (i) an automatically derived narrative network that\nincludes meta-actants; (ii) a new sequencing algorithm, REV2SEQ, that generates\na consensus sequence of events based on partial trajectories aggregated from\nthe reviews; and (iii) a new \"impressions\" algorithm, SENT2IMP, that provides\nfiner, non-trivial and multi-modal insight into readers' opinions of\ncharacters.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:10:14 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 17:36:01 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Holur", "Pavan", ""], ["Shahsavari", "Shadi", ""], ["Ebrahimzadeh", "Ehsan", ""], ["Tangherlini", "Timothy R.", ""], ["Roychowdhury", "Vwani", ""]]}, {"id": "2105.01180", "submitter": "Aina Gar\\'i Soler", "authors": "Aina Gar\\'i Soler and Marianna Apidianaki", "title": "Scalar Adjective Identification and Multilingual Ranking", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensity relationship that holds between scalar adjectives (e.g., nice <\ngreat < wonderful) is highly relevant for natural language inference and\ncommon-sense reasoning. Previous research on scalar adjective ranking has\nfocused on English, mainly due to the availability of datasets for evaluation.\nWe introduce a new multilingual dataset in order to promote research on scalar\nadjectives in new languages. We perform a series of experiments and set\nperformance baselines on this dataset, using monolingual and multilingual\ncontextual language models. Additionally, we introduce a new binary\nclassification task for English scalar adjective identification which examines\nthe models' ability to distinguish scalar from relational adjectives. We probe\ncontextualised representations and report baseline results for future\ncomparison on this task.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:32:41 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Soler", "Aina Gar\u00ed", ""], ["Apidianaki", "Marianna", ""]]}, {"id": "2105.01189", "submitter": "Long Chen", "authors": "Yuhan Liu, Yuhan Gao, Zhifan Nan, Long Chen", "title": "Textual Analysis of Communications in COVID-19 Infected Community on\n  Social Media", "comments": "5 pages, 4 figures, coursework for DS-GA 1011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the COVID-19 pandemic, people started to discuss about\npandemic-related topics on social media. On subreddit\n\\textit{r/COVID19positive}, a number of topics are discussed or being shared,\nincluding experience of those who got a positive test result, stories of those\nwho presumably got infected, and questions asked regarding the pandemic and the\ndisease. In this study, we try to understand, from a linguistic perspective,\nthe nature of discussions on the subreddit. We found differences in linguistic\ncharacteristics (e.g. psychological, emotional and reasoning) across three\ndifferent categories of topics. We also classified posts into the different\ncategories using SOTA pre-trained language models. Such classification model\ncan be used for pandemic-related research on social media.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:09:35 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liu", "Yuhan", ""], ["Gao", "Yuhan", ""], ["Nan", "Zhifan", ""], ["Chen", "Long", ""]]}, {"id": "2105.01192", "submitter": "Andrey Kutuzov", "authors": "Tatyana Iazykova, Denis Kapelyushnik, Olga Bystrova, Andrey Kutuzov", "title": "Unreasonable Effectiveness of Rule-Based Heuristics in Solving Russian\n  SuperGLUE Tasks", "comments": "Accepted to Dialogue'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leader-boards like SuperGLUE are seen as important incentives for active\ndevelopment of NLP, since they provide standard benchmarks for fair comparison\nof modern language models. They have driven the world's best engineering teams\nas well as their resources to collaborate and solve a set of tasks for general\nlanguage understanding. Their performance scores are often claimed to be close\nto or even higher than the human performance. These results encouraged more\nthorough analysis of whether the benchmark datasets featured any statistical\ncues that machine learning based language models can exploit. For English\ndatasets, it was shown that they often contain annotation artifacts. This\nallows solving certain tasks with very simple rules and achieving competitive\nrankings.\n  In this paper, a similar analysis was done for the Russian SuperGLUE (RSG), a\nrecently published benchmark set and leader-board for Russian natural language\nunderstanding. We show that its test datasets are vulnerable to shallow\nheuristics. Often approaches based on simple rules outperform or come close to\nthe results of the notorious pre-trained language models like GPT-3 or BERT. It\nis likely (as the simplest explanation) that a significant part of the SOTA\nmodels performance in the RSG leader-board is due to exploiting these shallow\nheuristics and that has nothing in common with real language understanding. We\nprovide a set of recommendations on how to improve these datasets, making the\nRSG leader-board even more representative of the real progress in Russian NLU.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:19:22 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Iazykova", "Tatyana", ""], ["Kapelyushnik", "Denis", ""], ["Bystrova", "Olga", ""], ["Kutuzov", "Andrey", ""]]}, {"id": "2105.01279", "submitter": "Yan Song", "authors": "Yan Song, Tong Zhang, Yonggang Wang, Kai-Fu Lee", "title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text\n  Encoders", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained text encoders have drawn sustaining attention in natural language\nprocessing (NLP) and shown their capability in obtaining promising results in\ndifferent tasks. Recent studies illustrated that external self-supervised\nsignals (or knowledge extracted by unsupervised learning, such as n-grams) are\nbeneficial to provide useful semantic evidence for understanding languages such\nas Chinese, so as to improve the performance on various downstream tasks\naccordingly. To further enhance the encoders, in this paper, we propose to\npre-train n-gram-enhanced encoders with a large volume of data and advanced\ntechniques for training. Moreover, we try to extend the encoder to different\nlanguages as well as different domains, where it is confirmed that the same\narchitecture is applicable to these varying circumstances and new\nstate-of-the-art performance is observed from a long list of NLP tasks across\nlanguages and domains.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:08:58 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Song", "Yan", ""], ["Zhang", "Tong", ""], ["Wang", "Yonggang", ""], ["Lee", "Kai-Fu", ""]]}, {"id": "2105.01296", "submitter": "Anubhav Jangra", "authors": "Anubhav Jangra, Raghav Jain, Vaibhav Mavi, Sriparna Saha, Pushpak\n  Bhattacharyya", "title": "Semantic Extractor-Paraphraser based Abstractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The anthology of spoken languages today is inundated with textual\ninformation, necessitating the development of automatic summarization models.\nIn this manuscript, we propose an extractor-paraphraser based abstractive\nsummarization system that exploits semantic overlap as opposed to its\npredecessors that focus more on syntactic information overlap. Our model\noutperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word\nmover similarity (WMS), establishing the superiority of the proposed system via\nextensive ablation experiments. We have also challenged the summarization\ncapabilities of the state of the art Pointer Generator Network (PGN), and\nthrough thorough experimentation, shown that PGN is more of a paraphraser,\ncontrary to the prevailing notion of a summarizer; illustrating it's\nincapability to accumulate information across multiple sentences.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:24:28 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Jangra", "Anubhav", ""], ["Jain", "Raghav", ""], ["Mavi", "Vaibhav", ""], ["Saha", "Sriparna", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2105.01305", "submitter": "Heiko Paulheim", "authors": "Petar Ristoski, Stefano Faralli, Simone Paolo Ponzetto and Heiko\n  Paulheim", "title": "Large-scale Taxonomy Induction Using Entity and Word Embeddings", "comments": "Published at IEEE/WIC/ACM International Conference on Web\n  Intelligence 2017 (WI'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomies are an important ingredient of knowledge organization, and serve\nas a backbone for more sophisticated knowledge representations in intelligent\nsystems, such as formal ontologies. However, building taxonomies manually is a\ncostly endeavor, and hence, automatic methods for taxonomy induction are a good\nalternative to build large-scale taxonomies. In this paper, we propose TIEmb,\nan approach for automatic unsupervised class subsumption axiom extraction from\nknowledge bases using entity and text embeddings. We apply the approach on the\nWebIsA database, a database of subsumption relations extracted from the large\nportion of the World Wide Web, to extract class hierarchies in the Person and\nPlace domain.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:53:12 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ristoski", "Petar", ""], ["Faralli", "Stefano", ""], ["Ponzetto", "Simone Paolo", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2105.01306", "submitter": "Youngseo Son", "authors": "Youngseo Son, H Andrew Schwartz", "title": "Discourse Relation Embeddings: Representing the Relations between\n  Discourse Segments in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Discourse relations are typically modeled as a discrete class that\ncharacterizes the relation between segments of text (e.g. causal explanations,\nexpansions). However, such predefined discrete classes limits the universe of\npotential relationships and their nuanced differences. Analogous to contextual\nword embeddings, we propose representing discourse relations as points in high\ndimensional continuous space. However, unlike words, discourse relations often\nhave no surface form (relations are between two segments, often with no word or\nphrase in that gap) which presents a challenge for existing embedding\ntechniques. We present a novel method for automatically creating discourse\nrelation embeddings (DiscRE), addressing the embedding challenge through a\nweakly supervised, multitask approach to learn diverse and nuanced relations\nbetween discourse segments in social media. Results show DiscRE can: (1) obtain\nthe best performance on Twitter discourse relation classification task (macro\nF1=0.76) (2) improve the state of the art in social media causality prediction\n(from F1=.79 to .81), (3) perform beyond modern sentence and contextual word\nembeddings at traditional discourse relation classification, and (4) capture\nnovel nuanced relations (e.g. relations semantically at the intersection of\ncausal explanations and counterfactuals).\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 05:58:27 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Son", "Youngseo", ""], ["Schwartz", "H Andrew", ""]]}, {"id": "2105.01311", "submitter": "Siyan Li", "authors": "Xiangyu Peng, Siyan Li, Sarah Wiegreffe, Mark Riedl", "title": "Inferring the Reader: Guiding Automated Story Generation with\n  Commonsense Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based language model approaches to automated story generation\ncurrently provide state-of-the-art results. However, they still suffer from\nplot incoherence when generating narratives over time, and critically lack\nbasic commonsense reasoning. Furthermore, existing methods generally focus only\non single-character stories, or fail to track characters at all. To improve the\ncoherence of generated narratives and to expand the scope of character-centric\nnarrative generation, we introduce Commonsense-inference Augmented neural\nStoryTelling (CAST), a framework for introducing commonsense reasoning into the\ngeneration process while modeling the interaction between multiple characters.\nWe find that our CAST method produces significantly more coherent and on-topic\ntwo-character stories, outperforming baselines in dimensions including plot\nplausibility and staying on topic. We also show how the CAST method can be used\nto further train language models that generate more coherent stories and reduce\ncomputation cost.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 06:40:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Peng", "Xiangyu", ""], ["Li", "Siyan", ""], ["Wiegreffe", "Sarah", ""], ["Riedl", "Mark", ""]]}, {"id": "2105.01331", "submitter": "Hasan Kemik", "authors": "Hasan Kemik, Nusret \\\"Ozate\\c{s}, Meysam Asgari-Chenaghlu, Erik\n  Cambria", "title": "BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on\n  Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Protection of human rights is one of the most important problems of our\nworld. In this paper, our aim is to provide a dataset which covers one of the\nmost significant human rights contradiction in recent months affected the whole\nworld, George Floyd incident. We propose a labeled dataset for topic detection\nthat contains 17 million tweets. These Tweets are collected from 25 May 2020 to\n21 August 2020 that covers 89 days from start of this incident. We labeled the\ndataset by monitoring most trending news topics from global and local\nnewspapers. Apart from that, we present two baselines, TF-IDF and LDA. We\nevaluated the results of these two methods with three different k values for\nmetrics of precision, recall and f1-score. The collected dataset is available\nat https://github.com/MeysamAsgariC/BLMT.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:27:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kemik", "Hasan", ""], ["\u00d6zate\u015f", "Nusret", ""], ["Asgari-Chenaghlu", "Meysam", ""], ["Cambria", "Erik", ""]]}, {"id": "2105.01466", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Gerhard Hagerer, Bj\\\"orn W. Schuller, Georg Groh", "title": "Unsupervised Graph-based Topic Modeling from Video Transcriptions", "comments": "JT and LS contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To unfold the tremendous amount of audiovisual data uploaded daily to social\nmedia platforms, effective topic modelling techniques are needed. Existing work\ntends to apply variants of topic models on text data sets. In this paper, we\naim at developing a topic extractor on video transcriptions. The model improves\ncoherence by exploiting neural word embeddings through a graph-based clustering\nmethod. Unlike typical topic models, this approach works without knowing the\ntrue number of topics. Experimental results on the real-life multimodal data\nset MuSe-CaR demonstrates that our approach extracts coherent and meaningful\ntopics, outperforming baseline methods. Furthermore, we successfully\ndemonstrate the generalisability of our approach on a pure text review data\nset.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:48:17 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Stappen", "Lukas", ""], ["Hagerer", "Gerhard", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Groh", "Georg", ""]]}, {"id": "2105.01468", "submitter": "Muhammad Zain Ali", "authors": "Muhammad Zain Ali, Kashif Javed, Ehsan ul Haq, Anoshka Tariq", "title": "Sentiment and Emotion Classification of Epidemic Related Bilingual data\n  from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, sentiment analysis and emotion classification are two of the\nmost abundantly used techniques in the field of Natural Language Processing\n(NLP). Although sentiment analysis and emotion classification are used commonly\nin applications such as analyzing customer reviews, the popularity of\ncandidates contesting in elections, and comments about various sporting events;\nhowever, in this study, we have examined their application for epidemic\noutbreak detection. Early outbreak detection is the key to deal with epidemics\neffectively, however, the traditional ways of outbreak detection are\ntime-consuming which inhibits prompt response from the respective departments.\nSocial media platforms such as Twitter, Facebook, Instagram, etc. allow the\nusers to express their thoughts related to different aspects of life, and\ntherefore, serve as a substantial source of information in such situations. The\nproposed study exploits the bilingual (Urdu and English) data from Twitter and\nNEWS websites related to the dengue epidemic in Pakistan, and sentiment\nanalysis and emotion classification are performed to acquire deep insights from\nthe data set for gaining a fair idea related to an epidemic outbreak. Machine\nlearning and deep learning algorithms have been used to train and implement the\nmodels for the execution of both tasks. The comparative performance of each\nmodel has been evaluated using accuracy, precision, recall, and f1-measure.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:51:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ali", "Muhammad Zain", ""], ["Javed", "Kashif", ""], ["Haq", "Ehsan ul", ""], ["Tariq", "Anoshka", ""]]}, {"id": "2105.01542", "submitter": "Son T. Luu", "authors": "Son T. Luu, Mao Nguyen Bui, Loi Duc Nguyen, Khiem Vinh Tran, Kiet Van\n  Nguyen, Ngan Luu-Thuy Nguyen", "title": "Conversational Machine Reading Comprehension for Vietnamese Healthcare\n  Texts", "comments": "Camera-ready version. To be appeared at The 13th International\n  Conference on Computational Collective Intelligence (ICCCI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: http://nlp.uit.edu.vn/datasets/ for research purposes.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:50:39 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 01:48:26 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 10:43:18 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 09:47:05 GMT"}, {"version": "v5", "created": "Fri, 2 Jul 2021 01:59:01 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Luu", "Son T.", ""], ["Bui", "Mao Nguyen", ""], ["Nguyen", "Loi Duc", ""], ["Tran", "Khiem Vinh", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2105.01633", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Michelle Lienhart, Annalena B\\\"atz,\n  Bj\\\"orn Schuller", "title": "An Estimation of Online Video User Engagement from Features of\n  Continuous Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Portraying emotion and trustworthiness is known to increase the appeal of\nvideo content. However, the causal relationship between these signals and\nonline user engagement is not well understood. This limited understanding is\npartly due to a scarcity in emotionally annotated data and the varied\nmodalities which express user engagement online. In this contribution, we\nutilise a large dataset of YouTube review videos which includes ca. 600 hours\nof dimensional arousal, valence and trustworthiness annotations. We investigate\nfeatures extracted from these signals against various user engagement\nindicators including views, like/dislike ratio, as well as the sentiment of\ncomments. In doing so, we identify the positive and negative influences which\nsingle features have, as well as interpretable patterns in each dimension which\nrelate to user engagement. Our results demonstrate that smaller boundary ranges\nand fluctuations for arousal lead to an increase in user engagement.\nFurthermore, the extracted time-series features reveal significant (p<0.05)\ncorrelations for each dimension, such as, count below signal mean (arousal),\nnumber of peaks (valence), and absolute energy (trustworthiness). From this, an\neffective combination of features is outlined for approaches aiming to\nautomatically predict several user engagement indicators. In a user engagement\nprediction paradigm we compare all features against semi-automatic\n(cross-task), and automatic (task-specific) feature selection methods. These\nselected feature sets appear to outperform the usage of all features, e.g.,\nusing all features achieves 1.55 likes per day (Lp/d) mean absolute error from\nvalence; this improves through semi-automatic and automatic selection to 1.33\nand 1.23 Lp/d, respectively (data mean 9.72 Lp/d with a std. 28.75 Lp/d).\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:18:47 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Lienhart", "Michelle", ""], ["B\u00e4tz", "Annalena", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "2105.01691", "submitter": "Toan Q. Nguyen", "authors": "Toan Q. Nguyen, Kenton Murray, David Chiang", "title": "Data Augmentation by Concatenation for Low-Resource Translation: A\n  Mystery and a Solution", "comments": "Accepted at IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the driving factors behind concatenation, a\nsimple but effective data augmentation method for low-resource neural machine\ntranslation. Our experiments suggest that discourse context is unlikely the\ncause for the improvement of about +1 BLEU across four language pairs. Instead,\nwe demonstrate that the improvement comes from three other factors unrelated to\ndiscourse: context diversity, length diversity, and (to a lesser extent)\nposition shifting.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:18:07 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:44:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Nguyen", "Toan Q.", ""], ["Murray", "Kenton", ""], ["Chiang", "David", ""]]}, {"id": "2105.01735", "submitter": "Piotr Rybak", "authors": "Robert Mroczkowski, Piotr Rybak, Alina Wr\\'oblewska, Ireneusz Gawlik", "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for\n  Polish", "comments": "Published in Proceedings of the 8th Workshop on Balto-Slavic Natural\n  Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT-based models are currently used for solving nearly all Natural Language\nProcessing (NLP) tasks and most often achieve state-of-the-art results.\nTherefore, the NLP community conducts extensive research on understanding these\nmodels, but above all on designing effective and efficient training procedures.\nSeveral ablation studies investigating how to train BERT-like models have been\ncarried out, but the vast majority of them concerned only the English language.\nA training procedure designed for English does not have to be universal and\napplicable to other especially typologically different languages. Therefore,\nthis paper presents the first ablation study focused on Polish, which, unlike\nthe isolating English language, is a fusional language. We design and\nthoroughly evaluate a pretraining procedure of transferring knowledge from\nmultilingual to monolingual BERT-based models. In addition to multilingual\nmodel initialization, other factors that possibly influence pretraining are\nalso explored, i.e. training objective, corpus size, BPE-Dropout, and\npretraining length. Based on the proposed procedure, a Polish BERT-based\nlanguage model -- HerBERT -- is trained. This model achieves state-of-the-art\nresults on multiple downstream tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:16:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mroczkowski", "Robert", ""], ["Rybak", "Piotr", ""], ["Wr\u00f3blewska", "Alina", ""], ["Gawlik", "Ireneusz", ""]]}, {"id": "2105.01736", "submitter": "Fei Wang", "authors": "Fei Wang, Kexuan Sun, Muhao Chen, Jay Pujara, Pedro Szekely", "title": "Retrieving Complex Tables with Multi-Granular Graph Representation\n  Learning", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462909", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The task of natural language table retrieval (NLTR) seeks to retrieve\nsemantically relevant tables based on natural language queries. Existing\nlearning systems for this task often treat tables as plain text based on the\nassumption that tables are structured as dataframes. However, tables can have\ncomplex layouts which indicate diverse dependencies between subtable\nstructures, such as nested headers. As a result, queries may refer to different\nspans of relevant content that is distributed across these structures.\nMoreover, such systems fail to generalize to novel scenarios beyond those seen\nin the training set. Prior methods are still distant from a generalizable\nsolution to the NLTR problem, as they fall short in handling complex table\nlayouts or queries over multiple granularities. To address these issues, we\npropose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with\nmulti-granular graph representation learning. In our framework, a table is\nfirst converted into a tabular graph, with cell nodes, row nodes and column\nnodes to capture content at different granularities. Then the tabular graph is\ninput to a Graph Transformer model that can capture both table cell content and\nthe layout structures. To enhance the robustness and generalizability of the\nmodel, we further incorporate a self-supervised pre-training task based on\ngraph-context matching. Experimental results on two benchmarks show that our\nmethod leads to significant improvements over the current state-of-the-art\nsystems. Further experiments demonstrate promising performance of our method on\ncross-dataset generalization, and enhanced capability of handling complex\ntables and fulfilling diverse query intents. Code and data are available at\nhttps://github.com/FeiWang96/GTR.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:19:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wang", "Fei", ""], ["Sun", "Kexuan", ""], ["Chen", "Muhao", ""], ["Pujara", "Jay", ""], ["Szekely", "Pedro", ""]]}, {"id": "2105.01786", "submitter": "Thomas Glarner", "authors": "Thomas Glarner, Janek Ebbers, Reinhold H\\\"ab-Umbach", "title": "Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering speaker independent acoustic units purely from spoken input is\nknown to be a hard problem. In this work we propose an unsupervised speaker\nnormalization technique prior to unit discovery. It is based on separating\nspeaker related from content induced variations in a speech signal with an\nadversarial contrastive predictive coding approach. This technique does neither\nrequire transcribed speech nor speaker labels, and, furthermore, can be trained\nin a multilingual fashion, thus achieving speaker normalization even if only\nfew unlabeled data is available from the target language. The speaker\nnormalization is done by mapping all utterances to a medoid style which is\nrepresentative for the whole database. We demonstrate the effectiveness of the\napproach by conducting acoustic unit discovery with a hidden Markov model\nvariational autoencoder noting, however, that the proposed speaker\nnormalization can serve as a front end to any unit discovery system.\nExperiments on English, Yoruba and Mboshi show improvements compared to using\nnon-normalized input.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 22:40:41 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Glarner", "Thomas", ""], ["Ebbers", "Janek", ""], ["H\u00e4b-Umbach", "Reinhold", ""]]}, {"id": "2105.01819", "submitter": "Bonan Min", "authors": "Bonan Min, Benjamin Rozonoyer, Haoling Qiu, Alexander Zamanian,\n  Jessica MacBride", "title": "ExcavatorCovid: Extracting Events and Relations from Text Corpora for\n  Temporal and Causal Analysis for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Timely responses from policy makers to mitigate the impact of the COVID-19\npandemic rely on a comprehensive grasp of events, their causes, and their\nimpacts. These events are reported at such a speed and scale as to be\noverwhelming. In this paper, we present ExcavatorCovid, a machine reading\nsystem that ingests open-source text documents (e.g., news and scientific\npublications), extracts COVID19 related events and relations between them, and\nbuilds a Temporal and Causal Analysis Graph (TCAG). Excavator will help\ngovernment agencies alleviate the information overload, understand likely\ndownstream effects of political and economic decisions and events related to\nthe pandemic, and respond in a timely manner to mitigate the impact of\nCOVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic:\nanalysts and decision makers will be empowered by Excavator to better\nunderstand and solve complex problems in the future. An interactive TCAG\nvisualization is available at http://afrl402.bbn.com:5050/index.html. We also\nreleased a demonstration video at https://vimeo.com/528619007.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:18:46 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Min", "Bonan", ""], ["Rozonoyer", "Benjamin", ""], ["Qiu", "Haoling", ""], ["Zamanian", "Alexander", ""], ["MacBride", "Jessica", ""]]}, {"id": "2105.01893", "submitter": "Zhenxin Yang", "authors": "Zhengxin Yang", "title": "Full-Sentence Models Perform Better in Simultaneous Translation Using\n  the Information Enhanced Decoding Strategy", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous translation, which starts translating each sentence after\nreceiving only a few words in source sentence, has a vital role in many\nscenarios. Although the previous prefix-to-prefix framework is considered\nsuitable for simultaneous translation and achieves good performance, it still\nhas two inevitable drawbacks: the high computational resource costs caused by\nthe need to train a separate model for each latency $k$ and the insufficient\nability to encode information because each target token can only attend to a\nspecific source prefix. We propose a novel framework that adopts a simple but\neffective decoding strategy which is designed for full-sentence models. Within\nthis framework, training a single full-sentence model can achieve arbitrary\ngiven latency and save computational resources. Besides, with the competence of\nthe full-sentence model to encode the whole sentence, our decoding strategy can\nenhance the information maintained in the decoded states in real time.\nExperimental results show that our method achieves better translation quality\nthan baselines on 4 directions: Zh$\\rightarrow$En, En$\\rightarrow$Ro and\nEn$\\leftrightarrow$De.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:03:41 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Zhengxin", ""]]}, {"id": "2105.01925", "submitter": "Simon Razniewski", "authors": "Simon Razniewski", "title": "Commonsense Knowledge Base Construction in the Age of Big Data", "comments": "Manuscript for the cancelled BTW 2021 demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compiling commonsense knowledge is traditionally an AI topic approached by\nmanual labor. Recent advances in web data processing have enabled automated\napproaches. In this demonstration we will showcase three systems for automated\ncommonsense knowledge base construction, highlighting each time one aspect of\nspecific interest to the data management community. (i) We use Quasimodo to\nillustrate knowledge extraction systems engineering, (ii) Dice to illustrate\nthe role that schema constraints play in cleaning fuzzy commonsense knowledge,\nand (iii) Ascent to illustrate the relevance of conceptual modelling. The demos\nare available online at https://quasimodo.r2.enst.fr,\nhttps://dice.mpi-inf.mpg.de and ascent.mpi-inf.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:27:36 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Razniewski", "Simon", ""]]}, {"id": "2105.01949", "submitter": "Peter Wallis", "authors": "Peter Wallis", "title": "Mind Reading at Work: Cooperation without common ground", "comments": "Working Paper. Contact the author for missing references or\n  suggestions (6 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Stefan Kopp and Nicole Kramer say in their recent paper[Frontiers in\nPsychology 12 (2021) 597], despite some very impressive demonstrations over the\nlast decade or so, we still don't know how how to make a computer have a half\ndecent conversation with a human. They argue that the capabilities required to\ndo this include incremental joint co-construction and mentalizing. Although\nagreeing whole heartedly with their statement of the problem, this paper argues\nfor a different approach to the solution based on the \"new\" AI of situated\naction.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:37:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 11:35:05 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 08:41:09 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wallis", "Peter", ""]]}, {"id": "2105.01974", "submitter": "Marco Valentino", "authors": "Marco Valentino, Ian Pratt-Hartmann, Andr\\'e Freitas", "title": "Do Natural Language Explanations Represent Valid Logical Arguments?\n  Verifying Entailment in Explainable NLI Gold Standards", "comments": "To appear in IWCS 2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging line of research in Explainable NLP is the creation of datasets\nenriched with human-annotated explanations and rationales, used to build and\nevaluate models with step-wise inference and explanation generation\ncapabilities. While human-annotated explanations are used as ground-truth for\nthe inference, there is a lack of systematic assessment of their consistency\nand rigour. In an attempt to provide a critical quality assessment of\nExplanation Gold Standards (XGSs) for NLI, we propose a systematic annotation\nmethodology, named Explanation Entailment Verification (EEV), to quantify the\nlogical validity of human-annotated explanations. The application of EEV on\nthree mainstream datasets reveals the surprising conclusion that a majority of\nthe explanations, while appearing coherent on the surface, represent logically\ninvalid arguments, ranging from being incomplete to containing clearly\nidentifiable logical errors. This conclusion confirms that the inferential\nproperties of explanations are still poorly formalised and understood, and that\nadditional work on this line of research is necessary to improve the way\nExplanation Gold Standards are constructed.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 10:59:26 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 10:11:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Valentino", "Marco", ""], ["Pratt-Hartmann", "Ian", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2105.01990", "submitter": "Hadi Abdine", "authors": "Hadi Abdine (1), Christos Xypolopoulos (1), Moussa Kamal Eddine (1),\n  Michalis Vazirgiannis (1 and 2) ((1) Ecole Polytechnique, (2) AUEB)", "title": "Evaluation Of Word Embeddings From Large-Scale French Web Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word representations are popularly used in many tasks in natural\nlanguage processing, adding that pre-trained word vectors on huge text corpus\nachieved high performance in many different NLP tasks. This paper introduces\nmultiple high quality word vectors for the French language where two of them\nare trained on huge crawled French data and the others are trained on an\nalready existing French corpus. We also evaluate the quality of our proposed\nword vectors and the existing French word vectors on the French word analogy\ntask. In addition, we do the evaluation on multiple real NLP tasks that show\nthe important performance enhancement of the pre-trained word vectors compared\nto the existing and random ones. Finally, we created a demo web application to\ntest and visualize the obtained word embeddings. The produced French word\nembeddings are available to the public, along with the fine-tuning code on the\nNLU tasks and the demo code.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:34:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Abdine", "Hadi", "", "Ecole Polytechnique"], ["Xypolopoulos", "Christos", "", "Ecole Polytechnique"], ["Eddine", "Moussa Kamal", "", "Ecole Polytechnique"], ["Vazirgiannis", "Michalis", "", "1 and 2"]]}, {"id": "2105.01995", "submitter": "Hang Dong", "authors": "Hang Dong, V\\'ictor Su\\'arez-Paniagua, Huayu Zhang, Minhong Wang, Emma\n  Whitfield, Honghan Wu", "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak\n  Supervision", "comments": "5 pages, 3 figures, accepted for IEEE EMBC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The identification of rare diseases from clinical notes with Natural Language\nProcessing (NLP) is challenging due to the few cases available for machine\nlearning and the need of data annotation from clinical experts. We propose a\nmethod using ontologies and weak supervision. The approach includes two steps:\n(i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language\nSystem (UMLS), with a named entity linking tool (e.g. SemEHR) and weak\nsupervision based on customised rules and Bidirectional Encoder Representations\nfrom Transformers (BERT) based contextual representations, and (ii)\nUMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease\nOntology (ORDO). Using MIMIC-III US intensive care discharge summaries as a\ncase study, we show that the Text-to-UMLS process can be greatly improved with\nweak supervision, without any annotated data from domain experts. Our analysis\nshows that the overall pipeline processing discharge summaries can surface rare\ndisease cases, which are mostly uncaptured in manual ICD codes of the hospital\nadmissions.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:49:09 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 10:20:10 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 16:19:12 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Dong", "Hang", ""], ["Su\u00e1rez-Paniagua", "V\u00edctor", ""], ["Zhang", "Huayu", ""], ["Wang", "Minhong", ""], ["Whitfield", "Emma", ""], ["Wu", "Honghan", ""]]}, {"id": "2105.02033", "submitter": "Anna Jonsson", "authors": "Johanna Bj\\\"orklund, Frank Drewes, and Anna Jonsson", "title": "Polynomial Graph Parsing with Non-Structural Reentrancies", "comments": "23 pages with 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semantic representations are valuable in natural language\nprocessing, where it is often simple and effective to represent linguistic\nconcepts as nodes, and relations as edges between them. Several attempts has\nbeen made to find a generative device that is sufficiently powerful to\nrepresent languages of semantic graphs, while at the same allowing efficient\nparsing. We add to this line of work by introducing graph extension grammar,\nwhich consists of an algebra over graphs together with a regular tree grammar\nthat generates expressions over the operations of the algebra. Due to the\ndesign of the operations, these grammars can generate graphs with\nnon-structural reentrancies; a type of node-sharing that is excessively common\nin formalisms such as abstract meaning representation, but for which existing\ndevices offer little support. We provide a parsing algorithm for graph\nextension grammars, which is proved to be correct and run in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:05:01 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 11:20:22 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 08:11:22 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bj\u00f6rklund", "Johanna", ""], ["Drewes", "Frank", ""], ["Jonsson", "Anna", ""]]}, {"id": "2105.02263", "submitter": "Deniz Beser", "authors": "Ryan Gabbard, Deniz Beser, Jacob Lichtefeld, Joe Cecil, Mitch Marcus,\n  Sarah Payne, Charles Yang, and Marjorie Freedman", "title": "ADAM: A Sandbox for Implementing Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ADAM, a software system for designing and running child language\nlearning experiments in Python. The system uses a virtual world to simulate a\ngrounded language acquisition process in which the language learner utilizes\ncognitively plausible learning algorithms to form perceptual and linguistic\nrepresentations of the observed world. The modular nature of ADAM makes it easy\nto design and test different language learning curricula as well as learning\nalgorithms. In this report, we describe the architecture of the ADAM system in\ndetail, and illustrate its components with examples. We provide our code.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:19:29 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Gabbard", "Ryan", ""], ["Beser", "Deniz", ""], ["Lichtefeld", "Jacob", ""], ["Cecil", "Joe", ""], ["Marcus", "Mitch", ""], ["Payne", "Sarah", ""], ["Yang", "Charles", ""], ["Freedman", "Marjorie", ""]]}, {"id": "2105.02274", "submitter": "Donald Metzler", "authors": "Donald Metzler, Yi Tay, Dara Bahri, Marc Najork", "title": "Rethinking Search: Making Domain Experts out of Dilettantes", "comments": null, "journal-ref": "SIGIR Forum 55, 1, Article 13 (June 2021), 27 pages", "doi": "10.1145/3476415.3476428", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When experiencing an information need, users want to engage with a domain\nexpert, but often turn to an information retrieval system, such as a search\nengine, instead. Classical information retrieval systems do not answer\ninformation needs directly, but instead provide references to (hopefully\nauthoritative) answers. Successful question answering systems offer a limited\ncorpus created on-demand by human experts, which is neither timely nor\nscalable. Pre-trained language models, by contrast, are capable of directly\ngenerating prose that may be responsive to an information need, but at present\nthey are dilettantes rather than domain experts -- they do not have a true\nunderstanding of the world, they are prone to hallucinating, and crucially they\nare incapable of justifying their utterances by referring to supporting\ndocuments in the corpus they were trained over. This paper examines how ideas\nfrom classical information retrieval and pre-trained language models can be\nsynthesized and evolved into systems that truly deliver on the promise of\ndomain expert advice.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:40:00 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 18:36:56 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Metzler", "Donald", ""], ["Tay", "Yi", ""], ["Bahri", "Dara", ""], ["Najork", "Marc", ""]]}, {"id": "2105.02351", "submitter": "Necati Cihan Camgoz Dr.", "authors": "Necati Cihan Camgoz, Ben Saunders, Guillaume Rochette, Marco\n  Giovanelli, Giacomo Inches, Robin Nachtrab-Ribback, Richard Bowden", "title": "Content4All Open Research Sign Language Translation Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computational sign language research lacks the large-scale datasets that\nenables the creation of useful reallife applications. To date, most research\nhas been limited to prototype systems on small domains of discourse, e.g.\nweather forecasts. To address this issue and to push the field forward, we\nrelease six datasets comprised of 190 hours of footage on the larger domain of\nnews. From this, 20 hours of footage have been annotated by Deaf experts and\ninterpreters and is made publicly available for research purposes. In this\npaper, we share the dataset collection process and tools developed to enable\nthe alignment of sign language video and subtitles, as well as baseline\ntranslation results to underpin future research.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:14:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Camgoz", "Necati Cihan", ""], ["Saunders", "Ben", ""], ["Rochette", "Guillaume", ""], ["Giovanelli", "Marco", ""], ["Inches", "Giacomo", ""], ["Nachtrab-Ribback", "Robin", ""], ["Bowden", "Richard", ""]]}, {"id": "2105.02365", "submitter": "William Chen", "authors": "William Chen, Kensal Ramos, Kalyan Naidu Mullaguri", "title": "Genetic Algorithms For Extractive Summarization", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most current work in NLP utilizes deep learning, which requires a lot of\ntraining data and computational power. This paper investigates the strengths of\nGenetic Algorithms (GAs) for extractive summarization, as we hypothesized that\nGAs could construct more efficient solutions for the summarization task due to\ntheir relative customizability relative to deep learning models. This is done\nby building a vocabulary set, the words of which are represented as an array of\nweights, and optimizing those set of weights with the GA. These weights can be\nused to build an overall weighting of a sentence, which can then be passed to\nsome threshold for extraction. Our results showed that the GA was able to learn\na weight representation that could filter out excessive vocabulary and thus\ndictate sentence importance based on common English words.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 23:14:41 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chen", "William", ""], ["Ramos", "Kensal", ""], ["Mullaguri", "Kalyan Naidu", ""]]}, {"id": "2105.02472", "submitter": "Milan Gritta", "authors": "Milan Gritta, Ignacio Iacobacci", "title": "XeroAlign: Zero-Shot Cross-lingual Transformer Alignment", "comments": "Accepted as long paper at Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of pretrained cross-lingual language models brought decisive\nimprovements to multilingual NLP tasks. However, the lack of labelled task data\nnecessitates a variety of methods aiming to close the gap to high-resource\nlanguages. Zero-shot methods in particular, often use translated task data as a\ntraining signal to bridge the performance gap between the source and target\nlanguage(s). We introduce XeroAlign, a simple method for task-specific\nalignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign\nuses translated task data to encourage the model to generate similar sentence\nembeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows\nstrong improvements over the baseline models to achieve state-of-the-art\nzero-shot results on three multilingual natural language understanding tasks.\nXLM-RA's text classification accuracy exceeds that of XLM-R trained with\nlabelled data and performs on par with state-of-the-art models on a\ncross-lingual adversarial paraphrasing task.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:10:00 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Gritta", "Milan", ""], ["Iacobacci", "Ignacio", ""]]}, {"id": "2105.02477", "submitter": "Li-Hsin Chang", "authors": "Li-Hsin Chang, Sampo Pyysalo, Jenna Kanerva, Filip Ginter", "title": "Quantitative Evaluation of Alternative Translations in a Corpus of\n  Highly Dissimilar Finnish Paraphrases", "comments": "Accepted to Workshop on MOdelling TRAnslation: Translatology in the\n  Digital Age", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we present a quantitative evaluation of differences between\nalternative translations in a large recently released Finnish paraphrase corpus\nfocusing in particular on non-trivial variation in translation. We combine a\nseries of automatic steps detecting systematic variation with manual analysis\nto reveal regularities and identify categories of translation differences. We\nfind the paraphrase corpus to contain highly non-trivial translation variants\ndifficult to recognize through automatic approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:22:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chang", "Li-Hsin", ""], ["Pyysalo", "Sampo", ""], ["Kanerva", "Jenna", ""], ["Ginter", "Filip", ""]]}, {"id": "2105.02482", "submitter": "Siqi Bao", "authors": "Siqi Bao, Bingjin Chen, Huang He, Xin Tian, Han Zhou, Fan Wang, Hua\n  Wu, Haifeng Wang, Wenquan Wu, Yingzhan Lin", "title": "A Unified Pre-training Framework for Conversational AI", "comments": "Presented at AAAI-21 DSTC9 Workshop. First five authors contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the application of PLATO-2 on various dialogue\nsystems, including open-domain conversation, knowledge grounded dialogue, and\ntask-oriented conversation. PLATO-2 is initially designed as an open-domain\nchatbot, trained via two-stage curriculum learning. In the first stage, a\ncoarse-grained response generation model is learned to fit the simplified\none-to-one mapping relationship. This model is applied to the task-oriented\nconversation, given that the semantic mappings tend to be deterministic in task\ncompletion. In the second stage, another fine-grained generation model and an\nevaluation model are further learned for diverse response generation and\ncoherence estimation, respectively. With superior capability on capturing\none-to-many mapping, such models are suitable for the open-domain conversation\nand knowledge grounded dialogue. For the comprehensive evaluation of PLATO-2,\nwe have participated in multiple tasks of DSTC9, including interactive\nevaluation of open-domain conversation (Track3-task2), static evaluation of\nknowledge grounded dialogue (Track3-task1), and end-to-end task-oriented\nconversation (Track2-task1). PLATO-2 has obtained the 1st place in all three\ntasks, verifying its effectiveness as a unified framework for various dialogue\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:27:11 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 02:52:03 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bao", "Siqi", ""], ["Chen", "Bingjin", ""], ["He", "Huang", ""], ["Tian", "Xin", ""], ["Zhou", "Han", ""], ["Wang", "Fan", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""], ["Wu", "Wenquan", ""], ["Lin", "Yingzhan", ""]]}, {"id": "2105.02486", "submitter": "Abulhair Saparov", "authors": "Abulhair Saparov, Tom M. Mitchell", "title": "A Generative Symbolic Model for More General Natural Language\n  Understanding and Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new fully-symbolic Bayesian model of semantic parsing and\nreasoning which we hope to be the first step in a research program toward more\ndomain- and task-general NLU and AI. Humans create internal mental models of\ntheir observations which greatly aid in their ability to understand and reason\nabout a large variety of problems. We aim to capture this in our model, which\nis fully interpretable and Bayesian, designed specifically with generality in\nmind, and therefore provides a clearer path for future research to expand its\ncapabilities. We derive and implement an inference algorithm, and evaluate it\non an out-of-domain ProofWriter question-answering/reasoning task, achieving\nzero-shot accuracies of 100% and 93.43%, depending on the experimental setting,\nthereby demonstrating its value as a proof-of-concept.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:38:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Saparov", "Abulhair", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "2105.02520", "submitter": "Hao Fei", "authors": "Shengqiong Wu and Hao Fei and Yafeng Ren and Donghong Ji and Jingye Li", "title": "Learn from Syntax: Improving Pair-wise Aspect and Opinion Terms\n  Extractionwith Rich Syntactic Knowledge", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose to enhance the pair-wise aspect and opinion terms\nextraction (PAOTE) task by incorporating rich syntactic knowledge. We first\nbuild a syntax fusion encoder for encoding syntactic features, including a\nlabel-aware graph convolutional network (LAGCN) for modeling the dependency\nedges and labels, as well as the POS tags unifiedly, and a local-attention\nmodule encoding POS tags for better term boundary detection. During pairing, we\nthen adopt Biaffine and Triaffine scoring for high-order aspect-opinion term\npairing, in the meantime re-harnessing the syntax-enriched representations in\nLAGCN for syntactic-aware scoring. Experimental results on four benchmark\ndatasets demonstrate that our model outperforms current state-of-the-art\nbaselines, meanwhile yielding explainable predictions with syntactic knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:45:40 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wu", "Shengqiong", ""], ["Fei", "Hao", ""], ["Ren", "Yafeng", ""], ["Ji", "Donghong", ""], ["Li", "Jingye", ""]]}, {"id": "2105.02544", "submitter": "Junwei Bao Doctor", "authors": "Jing Zhao, Junwei Bao, Yifan Wang, Youzheng Wu, Xiaodong He, Bowen\n  Zhou", "title": "SGG: Learning to Select, Guide, and Generate for Keyphrase Generation", "comments": "10 pages, 4 figures, accepted by NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Keyphrases, that concisely summarize the high-level topics discussed in a\ndocument, can be categorized into present keyphrase which explicitly appears in\nthe source text, and absent keyphrase which does not match any contiguous\nsubsequence but is highly semantically related to the source. Most existing\nkeyphrase generation approaches synchronously generate present and absent\nkeyphrases without explicitly distinguishing these two categories. In this\npaper, a Select-Guide-Generate (SGG) approach is proposed to deal with present\nand absent keyphrase generation separately with different mechanisms.\nSpecifically, SGG is a hierarchical neural network which consists of a\npointing-based selector at low layer concentrated on present keyphrase\ngeneration, a selection-guided generator at high layer dedicated to absent\nkeyphrase generation, and a guider in the middle to transfer information from\nselector to generator. Experimental results on four keyphrase generation\nbenchmarks demonstrate the effectiveness of our model, which significantly\noutperforms the strong baselines for both present and absent keyphrases\ngeneration. Furthermore, we extend SGG to a title generation task which\nindicates its extensibility in natural language generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:43:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhao", "Jing", ""], ["Bao", "Junwei", ""], ["Wang", "Yifan", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "2105.02570", "submitter": "Thomas Louf", "authors": "Thomas Louf, David Sanchez and Jose J. Ramasco", "title": "Capturing the diversity of multilingual societies", "comments": "Main text: 11 pages, 6 figures, 47 references. Supplementary\n  Information: 26 pages, 15 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cultural diversity encoded within languages of the world is at risk, as many\nlanguages have become endangered in the last decades in a context of growing\nglobalization. To preserve this diversity, it is first necessary to understand\nwhat drives language extinction, and which mechanisms might enable coexistence.\nHere, we consider the processes at work in language shift through a conjunction\nof theoretical and data-driven perspectives. A large-scale empirical study of\nspatial patterns of languages in multilingual societies using Twitter and\ncensus data yields a wide diversity. It ranges from an almost complete mixing\nof language speakers, including multilinguals, to segregation with a neat\nseparation of the linguistic domains and with multilinguals mainly at their\nboundaries. To understand how these different states can emerge and,\nespecially, become stable, we propose a model in which coexistence of languages\nmay be reached when learning the other language is facilitated and when\nbilinguals favor the use of the endangered language. Simulations carried out in\na metapopulation framework highlight the importance of spatial interactions\narising from people mobility to explain the stability of a mixed state or the\npresence of a boundary between two linguistic regions. Changes in the\nparameters regulating the relation between the languages can destabilize a\nsystem, which undergoes global transitions. According to our model, the\nevolution of the system once it undergoes a transition is highly\nhistory-dependent. It is easy to change the status quo but going back to a\nprevious state may not be simple or even possible.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:27:43 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 09:16:10 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Louf", "Thomas", ""], ["Sanchez", "David", ""], ["Ramasco", "Jose J.", ""]]}, {"id": "2105.02573", "submitter": "Yahui Liu", "authors": "Jiannan Xiang, Yahui Liu, Deng Cai, Huayang Li, Defu Lian and Lemao\n  Liu", "title": "Assessing Dialogue Systems with Distribution Distances", "comments": "7 pages, 2 figures", "journal-ref": "Findings of ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of developing dialogue systems is how to evaluate and\ncompare the performance of different systems. Existing automatic evaluation\nmetrics are based on turn-level quality evaluation and use average scores for\nsystem-level comparison. In this paper, we propose to measure the performance\nof a dialogue system by computing the distribution-wise distance between its\ngenerated conversations and real-world conversations. Specifically, two\ndistribution-wise metrics, FBD and PRD, are developed and evaluated.\nExperiments on several dialogue corpora show that our proposed metrics\ncorrelate better with human judgments than existing metrics.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:30:13 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 05:24:16 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 12:15:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Xiang", "Jiannan", ""], ["Liu", "Yahui", ""], ["Cai", "Deng", ""], ["Li", "Huayang", ""], ["Lian", "Defu", ""], ["Liu", "Lemao", ""]]}, {"id": "2105.02584", "submitter": "Hiroshi Iida", "authors": "Hiroshi Iida, Dung Thai, Varun Manjunatha, Mohit Iyyer", "title": "TABBIE: Pretrained Representations of Tabular Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on tabular representation learning jointly models tables and\nassociated text using self-supervised objective functions derived from\npretrained language models such as BERT. While this joint pretraining improves\ntasks involving paired tables and text (e.g., answering questions about\ntables), we show that it underperforms on tasks that operate over tables\nwithout any associated text (e.g., populating missing cells). We devise a\nsimple pretraining objective (corrupt cell detection) that learns exclusively\nfrom tabular data and reaches the state-of-the-art on a suite of table based\nprediction tasks. Unlike competing approaches, our model (TABBIE) provides\nembeddings of all table substructures (cells, rows, and columns), and it also\nrequires far less compute to train. A qualitative analysis of our model's\nlearned cell, column, and row representations shows that it understands complex\ntable semantics and numerical trends.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:15:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Iida", "Hiroshi", ""], ["Thai", "Dung", ""], ["Manjunatha", "Varun", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2105.02590", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty, Kathy Baxter, Araz Taeihagh, Gregory A.\n  Bennett, Min-Yen Kan", "title": "Reliability Testing for Natural Language Processing Systems", "comments": "Accepted to ACL-IJCNLP 2021 (main conference). Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questions of fairness, robustness, and transparency are paramount to address\nbefore deploying NLP systems. Central to these concerns is the question of\nreliability: Can NLP systems reliably treat different demographics fairly and\nfunction correctly in diverse and noisy environments? To address this, we argue\nfor the need for reliability testing and contextualize it among existing work\non improving accountability. We show how adversarial attacks can be reframed\nfor this goal, via a framework for developing reliability tests. We argue that\nreliability testing -- with an emphasis on interdisciplinary collaboration --\nwill enable rigorous and targeted testing, and aid in the enactment and\nenforcement of industry standards.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:24:58 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 04:17:44 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 03:55:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""], ["Baxter", "Kathy", ""], ["Taeihagh", "Araz", ""], ["Bennett", "Gregory A.", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2105.02605", "submitter": "Junhan Yang", "authors": "Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Guangzhong Sun, and\n  Xing Xie", "title": "GraphFormers: GNN-nested Language Models for Linked Text Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linked text representation is critical for many intelligent web applications,\nsuch as online advertisement and recommender systems. Recent breakthroughs on\npretrained language models and graph neural networks facilitate the development\nof corresponding techniques. However, the existing works mainly rely on\ncascaded model structures: the texts are independently encoded by language\nmodels at first, and the textual embeddings are further aggregated by graph\nneural networks. We argue that the neighbourhood information is insufficiently\nutilized within the above process, which restricts the representation quality.\nIn this work, we propose GraphFormers, where graph neural networks are nested\nalongside each transformer layer of the language models. On top of the above\narchitecture, the linked texts will iteratively extract neighbourhood\ninformation for the enhancement of their own semantics. Such an iterative\nworkflow gives rise to more effective utilization of neighbourhood information,\nwhich contributes to the representation quality. We further introduce an\nadaptation called unidirectional GraphFormers, which is much more efficient and\ncomparably effective; and we leverage a pretraining strategy called the\nneighbourhood-aware masked language modeling to enhance the training effect. We\nperform extensive experiment studies with three large-scale linked text\ndatasets, whose results verify the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 12:20:41 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yang", "Junhan", ""], ["Liu", "Zheng", ""], ["Xiao", "Shitao", ""], ["Li", "Chaozhuo", ""], ["Sun", "Guangzhong", ""], ["Xie", "Xing", ""]]}, {"id": "2105.02626", "submitter": "Xingjian Zhen", "authors": "Varun Nagaraj Rao, Xingjian Zhen, Karen Hovsepian, Mingwei Shen", "title": "A First Look: Towards Explainable TextVQA Models via Visual and Textual\n  Explanations", "comments": "This paper is done when Xingjian was an intern in Amazon PARS group,\n  summer 2020. This paper is accepted by NAACL-MAI-Workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable deep learning models are advantageous in many situations. Prior\nwork mostly provide unimodal explanations through post-hoc approaches not part\nof the original system design. Explanation mechanisms also ignore useful\ntextual information present in images. In this paper, we propose MTXNet, an\nend-to-end trainable multimodal architecture to generate multimodal\nexplanations, which focuses on the text in the image. We curate a novel dataset\nTextVQA-X, containing ground truth visual and multi-reference textual\nexplanations that can be leveraged during both training and evaluation. We then\nquantitatively show that training with multimodal explanations complements\nmodel performance and surpasses unimodal baselines by up to 7% in CIDEr scores\nand 2% in IoU. More importantly, we demonstrate that the multimodal\nexplanations are consistent with human interpretations, help justify the\nmodels' decision, and provide useful insights to help diagnose an incorrect\nprediction. Finally, we describe a real-world e-commerce application for using\nthe generated multimodal explanations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:36:17 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rao", "Varun Nagaraj", ""], ["Zhen", "Xingjian", ""], ["Hovsepian", "Karen", ""], ["Shen", "Mingwei", ""]]}, {"id": "2105.02629", "submitter": "Yifan Hou", "authors": "Yifan Hou and Mrinmaya Sachan", "title": "Bird's Eye: Probing for Linguistic Graph Structures with a Simple\n  Information-Theoretic Approach", "comments": "Accepted for publication at ACL 2021. This is the camera ready\n  version. Our implementation is available in\n  https://github.com/yifan-h/Graph_Probe-Birds_Eye", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP has a rich history of representing our prior understanding of language in\nthe form of graphs. Recent work on analyzing contextualized text\nrepresentations has focused on hand-designed probe models to understand how and\nto what extent do these representations encode a particular linguistic\nphenomenon. However, due to the inter-dependence of various phenomena and\nrandomness of training probe models, detecting how these representations encode\nthe rich information in these linguistic graphs remains a challenging problem.\nIn this paper, we propose a new information-theoretic probe, Bird's Eye, which\nis a fairly simple probe method for detecting if and how these representations\nencode the information in these linguistic graphs. Instead of using classifier\nperformance, our probe takes an information-theoretic view of probing and\nestimates the mutual information between the linguistic graph embedded in a\ncontinuous space and the contextualized word representations. Furthermore, we\nalso propose an approach to use our probe to investigate localized linguistic\ninformation in the linguistic graphs using perturbation analysis. We call this\nprobing setup Worm's Eye. Using these probes, we analyze BERT models on their\nability to encode a syntactic and a semantic graph structure, and find that\nthese models encode to some degree both syntactic as well as semantic\ninformation; albeit syntactic information to a greater extent.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:01:57 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 13:25:16 GMT"}, {"version": "v3", "created": "Sat, 22 May 2021 08:13:05 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 21:44:25 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hou", "Yifan", ""], ["Sachan", "Mrinmaya", ""]]}, {"id": "2105.02657", "submitter": "George Chrysostomou", "authors": "George Chrysostomou and Nikolaos Aletras", "title": "Improving the Faithfulness of Attention-based Explanations with\n  Task-specific Information for Text Classification", "comments": "NLP Interpretability ; Accepted at ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network architectures in natural language processing often use\nattention mechanisms to produce probability distributions over input token\nrepresentations. Attention has empirically been demonstrated to improve\nperformance in various tasks, while its weights have been extensively used as\nexplanations for model predictions. Recent studies (Jain and Wallace, 2019;\nSerrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot\ngenerally be considered as a faithful explanation (Jacovi and Goldberg, 2020)\nacross encoders and tasks. In this paper, we seek to improve the faithfulness\nof attention-based explanations for text classification. We achieve this by\nproposing a new family of Task-Scaling (TaSc) mechanisms that learn\ntask-specific non-contextualised information to scale the original attention\nweights. Evaluation tests for explanation faithfulness, show that the three\nproposed variants of TaSc improve attention-based explanations across two\nattention mechanisms, five encoders and five text classification datasets\nwithout sacrificing predictive performance. Finally, we demonstrate that TaSc\nconsistently provides more faithful attention-based explanations compared to\nthree widely-used interpretability techniques.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:35:03 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 15:58:45 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Chrysostomou", "George", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2105.02692", "submitter": "Seanie Lee", "authors": "Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang", "title": "Learning to Perturb Word Embeddings for Out-of-distribution QA", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  QA models based on pretrained language mod-els have achieved remarkable\nperformance on various benchmark datasets.However, QA models do not generalize\nwell to unseen data that falls outside the training distribution, due to\ndistributional shifts.Data augmentation (DA) techniques which drop/replace\nwords have shown to be effective in regularizing the model from overfitting to\nthe training data.Yet, they may adversely affect the QA tasks since they incur\nsemantic changes that may lead to wrong answers for the QA task. To tackle this\nproblem, we propose a simple yet effective DA method based on a stochastic\nnoise generator, which learns to perturb the word embedding of the input\nquestions and context without changing their semantics. We validate the\nperformance of the QA models trained with our word embedding perturbation on a\nsingle source dataset, on five different target domains.The results show that\nour method significantly outperforms the baselineDA methods. Notably, the model\ntrained with ours outperforms the model trained with more than 240K\nartificially generated QA pairs.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:12:26 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 22:53:37 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 11:31:55 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lee", "Seanie", ""], ["Kang", "Minki", ""], ["Lee", "Juho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2105.02732", "submitter": "Joseph Viviano", "authors": "Alexandra Sasha Luccioni, Joseph D. Viviano", "title": "What's in the Box? A Preliminary Analysis of Undesirable Content in the\n  Common Crawl Corpus", "comments": "5 pages, 1 figure, 3 tables. Published as a main conference paper at\n  ACL-IJCNLP 2021, submission #87. Code available at\n  https://github.com/josephdviviano/whatsinthebox", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas much of the success of the current generation of neural language\nmodels has been driven by increasingly large training corpora, relatively\nlittle research has been dedicated to analyzing these massive sources of\ntextual data. In this exploratory analysis, we delve deeper into the Common\nCrawl, a colossal web corpus that is extensively used for training language\nmodels. We find that it contains a significant amount of undesirable content,\nincluding hate speech and sexually explicit content, even after filtering\nprocedures. We discuss the potential impacts of this content on language models\nand conclude with future research directions and a more mindful approach to\ncorpus collection and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:49:43 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 17:28:38 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 20:39:54 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Luccioni", "Alexandra Sasha", ""], ["Viviano", "Joseph D.", ""]]}, {"id": "2105.02746", "submitter": "Sanya Bathla Taneja", "authors": "Sanya B. Taneja, Richard D. Boyce, William T. Reynolds, Denis\n  Newman-Griffis", "title": "Introducing Information Retrieval for Biomedical Informatics Students", "comments": "To appear in the Proceedings of the Fifth Workshop on Teaching NLP @\n  NAACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introducing biomedical informatics (BMI) students to natural language\nprocessing (NLP) requires balancing technical depth with practical know-how to\naddress application-focused needs. We developed a set of three activities\nintroducing introductory BMI students to information retrieval with NLP,\ncovering document representation strategies and language models from TF-IDF to\nBERT. These activities provide students with hands-on experience targeted\ntowards common use cases, and introduce fundamental components of NLP workflows\nfor a wide variety of applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:15:54 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Taneja", "Sanya B.", ""], ["Boyce", "Richard D.", ""], ["Reynolds", "William T.", ""], ["Newman-Griffis", "Denis", ""]]}, {"id": "2105.02751", "submitter": "Nikolaos Aletras", "authors": "Dimitrios Tsarapatsanis, Nikolaos Aletras", "title": "On the Ethical Limits of Natural Language Processing on Legal Text", "comments": "Accepted at ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language processing (NLP) methods for analyzing legal text offer\nlegal scholars and practitioners a range of tools allowing to empirically\nanalyze law on a large scale. However, researchers seem to struggle when it\ncomes to identifying ethical limits to using NLP systems for acquiring genuine\ninsights both about the law and the systems' predictive capacity. In this paper\nwe set out a number of ways in which to think systematically about such issues.\nWe place emphasis on three crucial normative parameters which have, to the best\nof our knowledge, been underestimated by current debates: (a) the importance of\nacademic freedom, (b) the existence of a wide diversity of legal and ethical\nnorms domestically but even more so internationally and (c) the threat of\nmoralism in research related to computational law. For each of these three\nparameters we provide specific recommendations for the legal NLP community. Our\ndiscussion is structured around the study of a real-life scenario that has\nprompted recent debate in the legal NLP research community.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:22:24 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 10:05:17 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 09:21:15 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Tsarapatsanis", "Dimitrios", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2105.02778", "submitter": "Haochen Liu", "authors": "Haochen Liu, Wei Jin, Hamid Karimi, Zitao Liu and Jiliang Tang", "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep\n  Text Classification", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is evident that deep text classification models trained on human data\ncould be biased. In particular, they produce biased outcomes for texts that\nexplicitly include identity terms of certain demographic groups. We refer to\nthis type of bias as explicit bias, which has been extensively studied.\nHowever, deep text classification models can also produce biased outcomes for\ntexts written by authors of certain demographic groups. We refer to such bias\nas implicit bias of which we still have a rather limited understanding. In this\npaper, we first demonstrate that implicit bias exists in different text\nclassification tasks for different demographic groups. Then, we build a\nlearning-based interpretation method to deepen our knowledge of implicit bias.\nSpecifically, we verify that classifiers learn to make predictions based on\nlanguage features that are related to the demographic attributes of the\nauthors. Next, we propose a framework Debiased-TC to train deep text\nclassifiers to make predictions on the right features and consequently mitigate\nimplicit bias. We conduct extensive experiments on three real-world datasets.\nThe results show that the text classification models trained under our proposed\nframework outperform traditional models significantly in terms of fairness, and\nalso slightly in terms of classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:17:38 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Liu", "Haochen", ""], ["Jin", "Wei", ""], ["Karimi", "Hamid", ""], ["Liu", "Zitao", ""], ["Tang", "Jiliang", ""]]}, {"id": "2105.02844", "submitter": "Jacques Savoy", "authors": "Dominique Labb\\'e, Jacques Savoy", "title": "Stylistic Analysis of the French Presidential Speeches: Is Macron really\n  different?", "comments": "15 pages with the references, 5 tables", "journal-ref": null, "doi": "10.1093/llc/fqz090", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Presidential speeches indicate the government's intentions and justifications\nsupported by a dedicated style and rhetoric oscillating between explanation and\ncontroversy. Over a period of sixty years, can we observe stylistic variations\nby the different French presidents of the Fifth Republic (1958-2018)? Based on\nofficial transcripts of all their allocution, this paper illustrates the\nstylistic evolution and presents the underlying main trends. This study shows\nthat de Gaulle's rhetoric is not mainly dedicated to his own person, or that\nthe two terms of J. Chirac are not fully similar. According to several overall\nstylistic indicators, Macron's style does not appear as complex compared to his\npredecessors (F. Hollande or N. Sarkozy) but a more careful analysis clearly\ndemonstrates his noticeable new style. Compared to the recent US presidents,\nthe French ones present some similarities (e.g., similar mean sentence length)\nand dissimilarities (more I-words, less we-words). In this comparative\nanalysis, Macron's style is also clearly distinctive from both the US and\nformer French presidents. Opting for a more abstract discourse, less anchored\nin space, using less numbers, E. Macron tends to use long sentences. These\nvarious stylistic and rhetorical features could explain his being misunderstood\nby the French people and his recurrent low approval ratings.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:35:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Labb\u00e9", "Dominique", ""], ["Savoy", "Jacques", ""]]}, {"id": "2105.02855", "submitter": "Wietse de Vries", "authors": "Wietse de Vries, Martijn Bartelds, Malvina Nissim, Martijn Wieling", "title": "Adapting Monolingual Models: Data can be Scarce when Language Similarity\n  is High", "comments": "Findings of ACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many (minority) languages, the resources needed to train large models are\nnot available. We investigate the performance of zero-shot transfer learning\nwith as little data as possible, and the influence of language similarity in\nthis process. We retrain the lexical layers of four BERT-based models using\ndata from two low-resource target language varieties, while the Transformer\nlayers are independently fine-tuned on a POS-tagging task in the model's source\nlanguage. By combining the new lexical layers and fine-tuned Transformer\nlayers, we achieve high task performance for both target languages. With high\nlanguage similarity, 10MB of data appears sufficient to achieve substantial\nmonolingual transfer performance. Monolingual BERT-based models generally\nachieve higher downstream task performance after retraining the lexical layer\nthan multilingual BERT, even when the target language is included in the\nmultilingual model.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:43:40 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 09:28:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["de Vries", "Wietse", ""], ["Bartelds", "Martijn", ""], ["Nissim", "Malvina", ""], ["Wieling", "Martijn", ""]]}, {"id": "2105.02923", "submitter": "Tanner Bohn", "authors": "Tanner Bohn and Charles X. Ling", "title": "Hone as You Read: A Practical Type of Interactive Summarization", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HARE, a new task where reader feedback is used to optimize\ndocument summaries for personal interest during the normal flow of reading.\nThis task is related to interactive summarization, where personalized summaries\nare produced following a long feedback stage where users may read the same\nsentences many times. However, this process severely interrupts the flow of\nreading, making it impractical for leisurely reading. We propose to gather\nminimally-invasive feedback during the reading process to adapt to user\ninterests and augment the document in real-time. Building off of recent\nadvances in unsupervised summarization evaluation, we propose a suitable metric\nfor this task and use it to evaluate a variety of approaches. Our approaches\nrange from simple heuristics to preference-learning and their analysis provides\ninsight into this important task. Human evaluation additionally supports the\npracticality of HARE. The code to reproduce this work is available at\nhttps://github.com/tannerbohn/HoneAsYouRead.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 19:36:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bohn", "Tanner", ""], ["Ling", "Charles X.", ""]]}, {"id": "2105.02935", "submitter": "Vedant Bahel", "authors": "Vedant Bahel and Achamma Thomas", "title": "Text similarity analysis for evaluation of descriptive answers", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping in mind the necessity of intelligent system in educational sector,\nthis paper proposes a text analysis based automated approach for automatic\nevaluation of the descriptive answers in an examination. In particular, the\nresearch focuses on the use of intelligent concepts of Natural Language\nProcessing and Data Mining for computer aided examination evaluation system.\nThe paper present an architecture for fair evaluation of answer sheet. In this\narchitecture, the examiner creates a sample answer sheet for given sets of\nquestion. By using the concept of text summarization, text semantics and\nkeywords summarization, the final score for each answer is calculated. The text\nsimilarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of\nthis research were compared to manually graded assignments and other existing\nsystem. This approach was found to be very efficient in order to be implemented\nin an institution or in an university.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:19:58 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bahel", "Vedant", ""], ["Thomas", "Achamma", ""]]}, {"id": "2105.02947", "submitter": "Marvin M. Ag\\\"uero-Torales", "authors": "Marvin M. Ag\\\"uero-Torales, David Vilares, Antonio G. L\\'opez-Herrera", "title": "On the logistical difficulties and findings of Jopara Sentiment Analysis", "comments": "Accepted in the CALCS 2021 (co-located with NAACL 2021) - Fifth\n  Workshop on Computational Approaches to Linguistic Code Switching, to appear\n  (June 2021)", "journal-ref": "Proceedings on CALCS 2021 (co-located with NAACL 2021) - Fifth\n  Workshop on Computational Approaches to Linguistic Code Switching", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sentiment analysis for Jopara, a\ncode-switching language between Guarani and Spanish. We first collect a corpus\nof Guarani-dominant tweets and discuss on the difficulties of finding quality\ndata for even relatively easy-to-annotate tasks, such as sentiment analysis.\nThen, we train a set of neural models, including pre-trained language models,\nand explore whether they perform better than traditional machine learning ones\nin this low-resource setup. Transformer architectures obtain the best results,\ndespite not considering Guarani during pre-training, but traditional machine\nlearning models perform close due to the low-resource nature of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:52:29 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:45:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ag\u00fcero-Torales", "Marvin M.", ""], ["Vilares", "David", ""], ["L\u00f3pez-Herrera", "Antonio G.", ""]]}, {"id": "2105.02978", "submitter": "Hanqing Lu Mr", "authors": "Hanqing Lu, Youna Hu, Tong Zhao, Tony Wu, Yiwei Song, Bing Yin", "title": "Graph-based Multilingual Product Retrieval in E-commerce Search", "comments": "Accepted by 2021 Annual Conference of the North American Chapter of\n  the Association for Computational Linguistics (NAACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, with many e-commerce platforms conducting global business,\ne-commerce search systems are required to handle product retrieval under\nmultilingual scenarios. Moreover, comparing with maintaining per-country\nspecific e-commerce search systems, having a universal system across countries\ncan further reduce the operational and computational costs, and facilitate\nbusiness expansion to new countries. In this paper, we introduce a universal\nend-to-end multilingual retrieval system, and discuss our learnings and\ntechnical details when training and deploying the system to serve billion-scale\nproduct retrieval for e-commerce search. In particular, we propose a\nmultilingual graph attention based retrieval network by leveraging recent\nadvances in transformer-based multilingual language models and graph neural\nnetwork architectures to capture the interactions between search queries and\nitems in e-commerce search. Offline experiments on five countries data show\nthat our algorithm outperforms the state-of-the-art baselines by 35% recall and\n25% mAP on average. Moreover, the proposed model shows significant increase of\nconversion/revenue in online A/B experiments and has been deployed in\nproduction for multiple countries.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:49:10 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Lu", "Hanqing", ""], ["Hu", "Youna", ""], ["Zhao", "Tong", ""], ["Wu", "Tony", ""], ["Song", "Yiwei", ""], ["Yin", "Bing", ""]]}, {"id": "2105.02987", "submitter": "Kanishka Misra", "authors": "Kanishka Misra and Allyson Ettinger and Julia Taylor Rayz", "title": "Do language models learn typicality judgments from text?", "comments": "Accepted as a talk to CogSci 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building on research arguing for the possibility of conceptual and\ncategorical knowledge acquisition through statistics contained in language, we\nevaluate predictive language models (LMs) -- informed solely by textual input\n-- on a prevalent phenomenon in cognitive science: typicality. Inspired by\nexperiments that involve language processing and show robust typicality effects\nin humans, we propose two tests for LMs. Our first test targets whether\ntypicality modulates LM probabilities in assigning taxonomic category\nmemberships to items. The second test investigates sensitivities to typicality\nin LMs' probabilities when extending new information about items to their\ncategories. Both tests show modest -- but not completely absent --\ncorrespondence between LMs and humans, suggesting that text-based exposure\nalone is insufficient to acquire typicality knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:56:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Misra", "Kanishka", ""], ["Ettinger", "Allyson", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2105.03010", "submitter": "Ngoc Quan Pham", "authors": "Ngoc-Quan Pham, Tuan-Nam Nguyen, Sebastian Stueker, Alexander Waibel", "title": "Efficient Weight factorization for Multilingual Speech Recognition", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end multilingual speech recognition involves using a single model\ntraining on a compositional speech corpus including many languages, resulting\nin a single neural network to handle transcribing different languages. Due to\nthe fact that each language in the training data has different characteristics,\nthe shared network may struggle to optimize for all various languages\nsimultaneously. In this paper we propose a novel multilingual architecture that\ntargets the core operation in neural networks: linear transformation functions.\nThe key idea of the method is to assign fast weight matrices for each language\nby decomposing each weight matrix into a shared component and a language\ndependent component. The latter is then factorized into vectors using rank-1\nassumptions to reduce the number of parameters per language. This efficient\nfactorization scheme is proved to be effective in two multilingual settings\nwith $7$ and $27$ languages, reducing the word error rates by $26\\%$ and $27\\%$\nrel. for two popular architectures LSTM and Transformer, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:12:02 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Pham", "Ngoc-Quan", ""], ["Nguyen", "Tuan-Nam", ""], ["Stueker", "Sebastian", ""], ["Waibel", "Alexander", ""]]}, {"id": "2105.03011", "submitter": "Pradeep Dasigi", "authors": "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, Matt\n  Gardner", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in\n  Research Papers", "comments": "Accepted at NAACL 2021; Project page:\n  https://allenai.org/project/qasper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Readers of academic research papers often read with the goal of answering\nspecific questions. Question Answering systems that can answer those questions\ncan make consumption of the content much more efficient. However, building such\ntools requires data that reflect the difficulty of the task arising from\ncomplex reasoning about claims made in multiple parts of a paper. In contrast,\nexisting information-seeking question answering datasets usually contain\nquestions about generic factoid-type information. We therefore present QASPER,\na dataset of 5,049 questions over 1,585 Natural Language Processing papers.\nEach question is written by an NLP practitioner who read only the title and\nabstract of the corresponding paper, and the question seeks information present\nin the full text. The questions are then answered by a separate set of NLP\npractitioners who also provide supporting evidence to answers. We find that\nexisting models that do well on other QA tasks do not perform well on answering\nthese questions, underperforming humans by at least 27 F1 points when answering\nthem from entire papers, motivating further research in document-grounded,\ninformation-seeking QA, which our dataset is designed to facilitate.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:12:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Dasigi", "Pradeep", ""], ["Lo", "Kyle", ""], ["Beltagy", "Iz", ""], ["Cohan", "Arman", ""], ["Smith", "Noah A.", ""], ["Gardner", "Matt", ""]]}, {"id": "2105.03023", "submitter": "Alisa Liu", "authors": "Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra\n  Bhagavatula, Noah A. Smith, Yejin Choi", "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and\n  Anti-Experts", "comments": "ACL 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in natural language generation, it remains\nchallenging to control attributes of generated text. We propose DExperts:\nDecoding-time Experts, a decoding-time method for controlled text generation\nthat combines a pretrained language model with \"expert\" LMs and/or\n\"anti-expert\" LMs in a product of experts. Intuitively, under the ensemble,\ntokens only get high probability if they are considered likely by the experts,\nand unlikely by the anti-experts. We apply DExperts to language detoxification\nand sentiment-controlled generation, where we outperform existing controllable\ngeneration methods on both automatic and human evaluations. Moreover, because\nDExperts operates only on the output of the pretrained LM, it is effective with\n(anti-)experts of smaller size, including when operating on GPT-3. Our work\nhighlights the promise of tuning small LMs on text with (un)desirable\nattributes for efficient decoding-time steering.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 01:19:38 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 05:26:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Alisa", ""], ["Sap", "Maarten", ""], ["Lu", "Ximing", ""], ["Swayamdipta", "Swabha", ""], ["Bhagavatula", "Chandra", ""], ["Smith", "Noah A.", ""], ["Choi", "Yejin", ""]]}, {"id": "2105.03036", "submitter": "Zhao You", "authors": "Zhao You, Shulin Feng, Dan Su and Dong Yu", "title": "SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture\n  of Experts", "comments": "5 pages, 2 figures. Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently, Mixture of Experts (MoE) based Transformer has shown promising\nresults in many domains. This is largely due to the following advantages of\nthis architecture: firstly, MoE based Transformer can increase model capacity\nwithout computational cost increasing both at training and inference time.\nBesides, MoE based Transformer is a dynamic network which can adapt to the\nvarying complexity of input instances in realworld applications. In this work,\nwe explore the MoE based model for speech recognition, named SpeechMoE. To\nfurther control the sparsity of router activation and improve the diversity of\ngate values, we propose a sparsity L1 loss and a mean importance loss\nrespectively. In addition, a new router architecture is used in SpeechMoE which\ncan simultaneously utilize the information from a shared embedding network and\nthe hierarchical representation of different MoE layers. Experimental results\nshow that SpeechMoE can achieve lower character error rate (CER) with\ncomparable computation cost than traditional static networks, providing\n7.0%-23.0% relative CER improvements on four evaluation datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 02:38:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["You", "Zhao", ""], ["Feng", "Shulin", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "2105.03038", "submitter": "Dusko Pavlovic", "authors": "Dusko Pavlovic", "title": "Lambek pregroups are Frobenius spiders in preorders", "comments": "21 pages, 16 diagrams, submitted. v2: corrected typos and style", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.CL cs.FL cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Spider\" is a nickname of *special Frobenius algebras*, a fundamental\nstructure from mathematics, physics, and computer science. *Pregroups* are a\nfundamental structure from linguistics. Pregroups and spiders have been used\ntogether in natural language processing: one for syntax, the other for\nsemantics. It turns out that pregroups themselves can be characterized as\npointed spiders in the category of preordered relations, where they naturally\narise from grammars. The other way around preordered spider algebras in general\ncan be characterized as unions of pregroups. This extends the characterization\nof relational spider algebras as disjoint unions of groups. The compositional\nframework that emerged with the results suggests new ways to understand and\napply the basis structures in machine learning and data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 02:42:03 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:48:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Pavlovic", "Dusko", ""]]}, {"id": "2105.03048", "submitter": "Yuqing Xie", "authors": "Yuqing Xie, Yi-an Lai, Yuanjun Xiong, Yi Zhang, Stefano Soatto", "title": "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing\n  Regressions In NLP Model Updates", "comments": "13 pages, 3 figures, Accepted at ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior of deep neural networks can be inconsistent between different\nversions. Regressions during model update are a common cause of concern that\noften over-weigh the benefits in accuracy or efficiency gain. This work focuses\non quantifying, reducing and analyzing regression errors in the NLP model\nupdates. Using negative flip rate as regression measure, we show that\nregression has a prevalent presence across tasks in the GLUE benchmark. We\nformulate the regression-free model updates into a constrained optimization\nproblem, and further reduce it into a relaxed form which can be approximately\noptimized through knowledge distillation training method. We empirically\nanalyze how model ensemble reduces regression. Finally, we conduct CheckList\nbehavioral testing to understand the distribution of regressions across\nlinguistic phenomena, and the efficacy of ensemble and distillation methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 03:33:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Xie", "Yuqing", ""], ["Lai", "Yi-an", ""], ["Xiong", "Yuanjun", ""], ["Zhang", "Yi", ""], ["Soatto", "Stefano", ""]]}, {"id": "2105.03070", "submitter": "Yi-Chen Chen", "authors": "Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin,\n  Sung-Feng Huang, Da-Rong Liu, Chi-Liang Liu, Cheng-Kuang Lee, Hung-yi Lee", "title": "SpeechNet: A Universal Modularized Model for Speech Processing Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a wide variety of speech processing tasks ranging from extracting\ncontent information from speech signals to generating speech signals. For\ndifferent tasks, model networks are usually designed and tuned separately. If a\nuniversal model can perform multiple speech processing tasks, some tasks might\nbe improved with the related abilities learned from other tasks. The multi-task\nlearning of a wide variety of speech processing tasks with a universal model\nhas not been studied. This paper proposes a universal modularized model,\nSpeechNet, which treats all speech processing tasks into a speech/text input\nand speech/text output format. We select five essential speech processing tasks\nfor multi-task learning experiments with SpeechNet. We show that SpeechNet\nlearns all of the above tasks, and we further analyze which tasks can be\nimproved by other tasks. SpeechNet is modularized and flexible for\nincorporating more modules, tasks, or training approaches in the future. We\nrelease the code and experimental settings to facilitate the research of\nmodularized universal models and multi-task learning of speech processing\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 05:31:34 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:05:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Yi-Chen", ""], ["Chi", "Po-Han", ""], ["Yang", "Shu-wen", ""], ["Chang", "Kai-Wei", ""], ["Lin", "Jheng-hao", ""], ["Huang", "Sung-Feng", ""], ["Liu", "Da-Rong", ""], ["Liu", "Chi-Liang", ""], ["Lee", "Cheng-Kuang", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2105.03075", "submitter": "Steven Y. Feng", "authors": "Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush\n  Vosoughi, Teruko Mitamura, Eduard Hovy", "title": "A Survey of Data Augmentation Approaches for NLP", "comments": "Accepted to ACL 2021 Findings. GitHub repo with paper list at\n  https://github.com/styfeng/DataAug4NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 06:03:45 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 23:39:31 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 21:57:45 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 20:34:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Feng", "Steven Y.", ""], ["Gangal", "Varun", ""], ["Wei", "Jason", ""], ["Chandar", "Sarath", ""], ["Vosoughi", "Soroush", ""], ["Mitamura", "Teruko", ""], ["Hovy", "Eduard", ""]]}, {"id": "2105.03095", "submitter": "Chi Han", "authors": "Chi Han, Mingxuan Wang, Heng Ji, Lei Li", "title": "Learning Shared Semantic Space for Speech-to-Text Translation", "comments": "9 pages, 5 figures, Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having numerous potential applications and great impact, end-to-end speech\ntranslation (ST) has long been treated as an independent task, failing to fully\ndraw strength from the rapid advances of its sibling - text machine translation\n(MT). With text and audio inputs represented differently, the modality gap has\nrendered MT data and its end-to-end models incompatible with their ST\ncounterparts. In observation of this obstacle, we propose to bridge this\nrepresentation gap with Chimera. By projecting audio and text features to a\ncommon semantic representation, Chimera unifies MT and ST tasks and boosts the\nperformance on ST benchmarks, MuST-C and Augmented Librispeech, to a new\nstate-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,\nimproving the SOTA by a +1.9 BLEU margin. Further experimental analyses\ndemonstrate that the shared semantic space indeed conveys common knowledge\nbetween these two tasks and thus paves a new way for augmenting training\nresources across modalities. Code, data, and resources are available at\nhttps://github.com/Glaciohound/Chimera-ST.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:49:56 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 16:57:33 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Han", "Chi", ""], ["Wang", "Mingxuan", ""], ["Ji", "Heng", ""], ["Li", "Lei", ""]]}, {"id": "2105.03143", "submitter": "Mohamed Seghir Hadj Ameur", "authors": "Mohamed Seghir Hadj Ameur, Hassina Aliane", "title": "AraCOVID19-MFH: Arabic COVID-19 Multi-label Fake News and Hate Speech\n  Detection Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Along with the COVID-19 pandemic, an \"infodemic\" of false and misleading\ninformation has emerged and has complicated the COVID-19 response efforts.\nSocial networking sites such as Facebook and Twitter have contributed largely\nto the spread of rumors, conspiracy theories, hate, xenophobia, racism, and\nprejudice. To combat the spread of fake news, researchers around the world have\nand are still making considerable efforts to build and share COVID-19 related\nresearch articles, models, and datasets. This paper releases \"AraCOVID19-MFH\" a\nmanually annotated multi-label Arabic COVID-19 fake news and hate speech\ndetection dataset. Our dataset contains 10,828 Arabic tweets annotated with 10\ndifferent labels. The labels have been designed to consider some aspects\nrelevant to the fact-checking task, such as the tweet's check worthiness,\npositivity/negativity, and factuality. To confirm our annotated dataset's\npractical utility, we used it to train and evaluate several classification\nmodels and reported the obtained results. Though the dataset is mainly designed\nfor fake news detection, it can also be used for hate speech detection,\nopinion/news classification, dialect identification, and many other tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 09:52:44 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ameur", "Mohamed Seghir Hadj", ""], ["Aliane", "Hassina", ""]]}, {"id": "2105.03157", "submitter": "Maria Becker", "authors": "Maria Becker, Katharina Korfhage, Debjit Paul, Anette Frank", "title": "CO-NNECT: A Framework for Revealing Commonsense Knowledge Paths as\n  Explicitations of Implicit Knowledge in Texts", "comments": "Accepted at IWCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we leverage commonsense knowledge in form of knowledge paths to\nestablish connections between sentences, as a form of explicitation of implicit\nknowledge. Such connections can be direct (singlehop paths) or require\nintermediate concepts (multihop paths). To construct such paths we combine two\nmodel types in a joint framework we call Co-nnect: a relation classifier that\npredicts direct connections between concepts; and a target prediction model\nthat generates target or intermediate concepts given a source concept and a\nrelation, which we use to construct multihop paths. Unlike prior work that\nrelies exclusively on static knowledge sources, we leverage language models\nfinetuned on knowledge stored in ConceptNet, to dynamically generate knowledge\npaths, as explanations of implicit knowledge that connects sentences in texts.\nAs a central contribution we design manual and automatic evaluation settings\nfor assessing the quality of the generated paths. We conduct evaluations on two\nargumentative datasets and show that a combination of the two model types\ngenerates meaningful, high-quality knowledge paths between sentences that\nreveal implicit knowledge conveyed in text.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:43:43 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Becker", "Maria", ""], ["Korfhage", "Katharina", ""], ["Paul", "Debjit", ""], ["Frank", "Anette", ""]]}, {"id": "2105.03160", "submitter": "Alexander Robertson", "authors": "Alexander Robertson, Walid Magdy, Sharon Goldwater", "title": "Identity Signals in Emoji Do not Influence Perception of Factual Truth\n  on Twitter", "comments": null, "journal-ref": "International Workshop on Emoji Understanding and Applications in\n  Social Media 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work has shown that Twitter users use skin-toned emoji as an act of\nself-representation to express their racial/ethnic identity. We test whether\nthis signal of identity can influence readers' perceptions about the content of\na post containing that signal. In a large scale (n=944) pre-registered\ncontrolled experiment, we manipulate the presence of skin-toned emoji and\nprofile photos in a task where readers rate obscure trivia facts (presented as\ntweets) as true or false. Using a Bayesian statistical analysis, we find that\nneither emoji nor profile photo has an effect on how readers rate these facts.\nThis result will be of some comfort to anyone concerned about the manipulation\nof online users through the crafting of fake profiles.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:56:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Robertson", "Alexander", ""], ["Magdy", "Walid", ""], ["Goldwater", "Sharon", ""]]}, {"id": "2105.03168", "submitter": "Keenan Jones", "authors": "Keenan Jones, Jason R. C. Nurse, Shujun Li", "title": "The Shadowy Lives of Emojis: An Analysis of a Hacktivist Collective's\n  Use of Emojis on Twitter", "comments": "10 pages, 1 figure, 7 tables", "journal-ref": null, "doi": "10.36190/2021.04", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emojis have established themselves as a popular means of communication in\nonline messaging. Despite the apparent ubiquity in these image-based tokens,\nhowever, interpretation and ambiguity may allow for unique uses of emojis to\nappear. In this paper, we present the first examination of emoji usage by\nhacktivist groups via a study of the Anonymous collective on Twitter. This\nresearch aims to identify whether Anonymous affiliates have evolved their own\napproach to using emojis. To do this, we compare a large dataset of Anonymous\ntweets to a baseline tweet dataset from randomly sampled Twitter users using\ncomputational and qualitative analysis to compare their emoji usage. We utilise\nWord2Vec language models to examine the semantic relationships between emojis,\nidentifying clear distinctions in the emoji-emoji relationships of Anonymous\nusers. We then explore how emojis are used as a means of conveying emotions,\nfinding that despite little commonality in emoji-emoji semantic ties, Anonymous\nemoji usage displays similar patterns of emotional purpose to the emojis of\nbaseline Twitter users. Finally, we explore the textual context in which these\nemojis occur, finding that although similarities exist between the emoji usage\nof our Anonymous and baseline Twitter datasets, Anonymous users appear to have\nadopted more specific interpretations of certain emojis. This includes the use\nof emojis as a means of expressing adoration and infatuation towards notable\nAnonymous affiliates. These findings indicate that emojis appear to retain a\nconsiderable degree of similarity within Anonymous accounts as compared to more\ntypical Twitter users. However, their are signs that emoji usage in Anonymous\naccounts has evolved somewhat, gaining additional group-specific associations\nthat reveal new insights into the behaviours of this unusual collective.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:21:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jones", "Keenan", ""], ["Nurse", "Jason R. C.", ""], ["Li", "Shujun", ""]]}, {"id": "2105.03207", "submitter": "Deniz Beser", "authors": "Deniz Beser, Joe Cecil, Marjorie Freedman, Jacob Lichtefeld, Mitch\n  Marcus, Sarah Payne, and Charles Yang", "title": "A Grounded Approach to Modeling Generic Knowledge Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and implement a cognitively plausible model for learning from\ngeneric language, statements that express generalizations about members of a\ncategory and are an important aspect of concept development in language\nacquisition (Carlson & Pelletier, 1995; Gelman, 2009). We extend a\ncomputational framework designed to model grounded language acquisition by\nintroducing the concept network. This new layer of abstraction enables the\nsystem to encode knowledge learned from generic statements and represent the\nassociations between concepts learned by the system. Through three tasks that\nutilize the concept network, we demonstrate that our extensions to ADAM can\nacquire generic information and provide an example of how ADAM can be used to\nmodel language acquisition.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 12:27:55 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Beser", "Deniz", ""], ["Cecil", "Joe", ""], ["Freedman", "Marjorie", ""], ["Lichtefeld", "Jacob", ""], ["Marcus", "Mitch", ""], ["Payne", "Sarah", ""], ["Yang", "Charles", ""]]}, {"id": "2105.03229", "submitter": "Anthony Ferritto", "authors": "Haoyang Wen, Anthony Ferritto, Heng Ji, Radu Florian, Avirup Sil", "title": "VAULT: VAriable Unified Long Text Representation for Machine Reading\n  Comprehension", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models on Machine Reading Comprehension (MRC) require complex model\narchitecture for effectively modeling long texts with paragraph representation\nand classification, thereby making inference computationally inefficient for\nproduction use. In this work, we propose VAULT: a light-weight and\nparallel-efficient paragraph representation for MRC based on contextualized\nrepresentation from long document input, trained using a new Gaussian\ndistribution-based objective that pays close attention to the partially correct\ninstances that are close to the ground-truth. We validate our VAULT\narchitecture showing experimental results on two benchmark MRC datasets that\nrequire long context modeling; one Wikipedia-based (Natural Questions (NQ)) and\nthe other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ\nwith a state-of-the-art (SOTA) complex document modeling approach while being\n16 times faster, demonstrating the efficiency of our proposed model. We also\ndemonstrate that our model can also be effectively adapted to a completely\ndifferent domain -- TechQA -- with large improvement over a model fine-tuned on\na previously published large PLM.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:03:43 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:28:26 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wen", "Haoyang", ""], ["Ferritto", "Anthony", ""], ["Ji", "Heng", ""], ["Florian", "Radu", ""], ["Sil", "Avirup", ""]]}, {"id": "2105.03278", "submitter": "Jie Huang", "authors": "Jie Huang", "title": "A Multi-Size Neural Network with Attention Mechanism for Answer\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic matching is of central significance to the answer selection task\nwhich aims to select correct answers for a given question from a candidate\nanswer pool. A useful method is to employ neural networks with attention to\ngenerate sentences representations in a way that information from pair\nsentences can mutually influence the computation of representations. In this\nwork, an effective architecture,multi-size neural network with attention\nmechanism (AM-MSNN),is introduced into the answer selection task. This\narchitecture captures more levels of language granularities in parallel,\nbecause of the various sizes of filters comparing with single-layer CNN and\nmulti-layer CNNs. Meanwhile it extends the sentence representations by\nattention mechanism, thus containing more information for different types of\nquestions. The empirical study on three various benchmark tasks of answer\nselection demonstrates the efficacy of the proposed model in all the benchmarks\nand its superiority over competitors. The experimental results show that (1)\nmulti-size neural network (MSNN) is a more useful method to capture abstract\nfeatures on different levels of granularities than single/multi-layer CNNs; (2)\nthe attention mechanism (AM) is a better strategy to derive more informative\nrepresentations; (3) AM-MSNN is a better architecture for the answer selection\ntask for the moment.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 02:13:26 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Huang", "Jie", ""]]}, {"id": "2105.03279", "submitter": "Mantas Luko\\v{s}evi\\v{c}ius", "authors": "Lukas Stankevi\\v{c}ius and Mantas Luko\\v{s}evi\\v{c}ius", "title": "Generating abstractive summaries of Lithuanian news articles using a\n  transformer model", "comments": "Accepted in ICIST 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:10:42 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 13:36:37 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Stankevi\u010dius", "Lukas", ""], ["Luko\u0161evi\u010dius", "Mantas", ""]]}, {"id": "2105.03280", "submitter": "Tosin Adewumi", "authors": "Tosin P. Adewumi, Saleha Javed, Roshanak Vadoodi, Aparajita Tripathy,\n  Konstantina Nikolaidou, Foteini Liwicki and Marcus Liwicki", "title": "Potential Idiomatic Expression (PIE)-English: Corpus for Classes of\n  Idioms", "comments": "7 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a fairly large, Potential Idiomatic Expression (PIE) dataset for\nNatural Language Processing (NLP) in English. The challenges with NLP systems\nwith regards to tasks such as Machine Translation (MT), word sense\ndisambiguation (WSD) and information retrieval make it imperative to have a\nlabelled idioms dataset with classes such as it is in this work. To the best of\nthe authors' knowledge, this is the first idioms corpus with classes of idioms\nbeyond the literal and the general idioms classification. In particular, the\nfollowing classes are labelled in the dataset: metaphor, simile, euphemism,\nparallelism, personification, oxymoron, paradox, hyperbole, irony and literal.\nMany past efforts have been limited in the corpus size and classes of samples\nbut this dataset contains over 20,100 samples with almost 1,200 cases of idioms\n(with their meanings) from 10 classes (or senses). The corpus may also be\nextended by researchers to meet specific needs. The corpus has part of speech\n(PoS) tagging from the NLTK library. Classification experiments performed on\nthe corpus to obtain a baseline and comparison among three common models,\nincluding the BERT model, give good results. We also make publicly available\nthe corpus and the relevant codes for working with it for NLP tasks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 13:05:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Adewumi", "Tosin P.", ""], ["Javed", "Saleha", ""], ["Vadoodi", "Roshanak", ""], ["Tripathy", "Aparajita", ""], ["Nikolaidou", "Konstantina", ""], ["Liwicki", "Foteini", ""], ["Liwicki", "Marcus", ""]]}, {"id": "2105.03287", "submitter": "Stefan Schouten BSc", "authors": "Michael Neely, Stefan F. Schouten, Maurits J. R. Bleeker, and Ana\n  Lucic", "title": "Order in the Court: Explainable AI Methods Prone to Disagreement", "comments": "Accepted for presentation at the ICML Workshop on Theoretic\n  Foundation, Criticism, and Application Trend of Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:27:37 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:45:39 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 09:51:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Neely", "Michael", ""], ["Schouten", "Stefan F.", ""], ["Bleeker", "Maurits J. R.", ""], ["Lucic", "Ana", ""]]}, {"id": "2105.03311", "submitter": "Lifeng Han", "authors": "Lifeng Han, Gareth J. F. Jones and Alan F. Smeaton", "title": "Translation Quality Assessment: A Brief Survey on Manual and Automatic\n  Methods", "comments": "Accepted to 23rd Nordic Conference on Computational Linguistics\n  (NoDaLiDa 2021): Workshop on Modelling Translation: Translatology in the\n  Digital Age (MoTra21). arXiv admin note: substantial text overlap with\n  arXiv:1605.04515", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To facilitate effective translation modeling and translation studies, one of\nthe crucial questions to address is how to assess translation quality. From the\nperspectives of accuracy, reliability, repeatability and cost, translation\nquality assessment (TQA) itself is a rich and challenging task. In this work,\nwe present a high-level and concise survey of TQA methods, including both\nmanual judgement criteria and automated evaluation metrics, which we classify\ninto further detailed sub-categories. We hope that this work will be an asset\nfor both translation model researchers and quality assessment researchers. In\naddition, we hope that it will enable practitioners to quickly develop a better\nunderstanding of the conventional TQA field, and to find corresponding closely\nrelevant evaluation solutions for their own needs. This work may also serve\ninspire further development of quality assessment and evaluation methodologies\nfor other natural language processing (NLP) tasks in addition to machine\ntranslation (MT), such as automatic text summarization (ATS), natural language\nunderstanding (NLU) and natural language generation (NLG).\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:28:10 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Han", "Lifeng", ""], ["Jones", "Gareth J. F.", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2105.03313", "submitter": "Genoveva Vargas-Solar", "authors": "Raj Ratn Pranesh and Mehrdad Farokhnejad and Ambesh Shekhar and\n  Genoveva Vargas-Solar", "title": "Looking for COVID-19 misinformation in multilingual social media texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Multilingual COVID-19 Analysis Method (CMTA) for\ndetecting and observing the spread of misinformation about this disease within\ntexts. CMTA proposes a data science (DS) pipeline that applies machine learning\nmodels for processing, classifying (Dense-CNN) and analyzing (MBERT)\nmultilingual (micro)-texts. DS pipeline data preparation tasks extract features\nfrom multilingual textual data and categorize it into specific information\nclasses (i.e., 'false', 'partly false', 'misleading'). The CMTA pipeline has\nbeen experimented with multilingual micro-texts (tweets), showing\nmisinformation spread across different languages. To assess the performance of\nCMTA and put it in perspective, we performed a comparative analysis of CMTA\nwith eight monolingual models used for detecting misinformation. The comparison\nshows that CMTA has surpassed various monolingual models and suggests that it\ncan be used as a general method for detecting misinformation in multilingual\nmicro-texts. CMTA experimental results show misinformation trends about\nCOVID-19 in different languages during the first pandemic months.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:30:49 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Pranesh", "Raj Ratn", ""], ["Farokhnejad", "Mehrdad", ""], ["Shekhar", "Ambesh", ""], ["Vargas-Solar", "Genoveva", ""]]}, {"id": "2105.03314", "submitter": "YiPeng Deng", "authors": "YiPeng Deng, YinHui Luo", "title": "Recognition and Processing of NATOM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we show how to process the NOTAM (Notice to Airmen) data of the\nfield in civil aviation. The main research contents are as follows: 1.Data\npreprocessing: For the original data of the NOTAM, there is a mixture of\nChinese and English, and the structure is poor. The original data is cleaned,\nthe Chinese data and the English data are processed separately, word\nsegmentation is completed, and stopping-words are removed. Using Glove word\nvector methods to represent the data for using a custom mapping vocabulary.\n2.Decoupling features and classifiers: In order to improve the ability of the\ntext classification model to recognize minority samples, the overall model\ntraining process is decoupled from the perspective of the algorithm as a whole,\ndivided into two stages of feature learning and classifier learning. The\nweights of the feature learning stage and the classifier learning stage adopt\ndifferent strategies to overcome the influence of the head data and tail data\nof the imbalanced data set on the classification model. Experiments have proved\nthat the use of decoupling features and classifier methods based on the neural\nnetwork classification model can complete text multi-classification tasks in\nthe field of civil aviation, and at the same time can improve the recognition\naccuracy of the minority samples in the data set.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 10:12:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Deng", "YiPeng", ""], ["Luo", "YinHui", ""]]}, {"id": "2105.03315", "submitter": "Ning Wang", "authors": "Ning Wang, Fan Luo, Yuvraj Shivtare, Varsha D. Badal, K.P.\n  Subbalakshmi, R. Chandramouli, Ellen Lee", "title": "Learning Models for Suicide Prediction from Social Media Posts", "comments": "This work is accepted to CLPsych 2021, to be held in conjunction with\n  NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning architecture and test three other machine learning\nmodels to automatically detect individuals that will attempt suicide within (1)\n30 days and (2) six months, using their social media post data provided in the\nCLPsych 2021 shared task. Additionally, we create and extract three sets of\nhandcrafted features for suicide risk detection based on the three-stage theory\nof suicide and prior work on emotions and the use of pronouns among persons\nexhibiting suicidal ideations. Extensive experimentations show that some of the\ntraditional machine learning methods outperform the baseline with an F1 score\nof 0.741 and F2 score of 0.833 on subtask 1 (prediction of a suicide attempt 30\ndays prior). However, the proposed deep learning method outperforms the\nbaseline with F1 score of 0.737 and F2 score of 0.843 on subtask 2 (prediction\nof suicide 6 months prior).\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:52:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Ning", ""], ["Luo", "Fan", ""], ["Shivtare", "Yuvraj", ""], ["Badal", "Varsha D.", ""], ["Subbalakshmi", "K. P.", ""], ["Chandramouli", "R.", ""], ["Lee", "Ellen", ""]]}, {"id": "2105.03316", "submitter": "Shalin Shah", "authors": "Shalin Shah, Ryan Siskind", "title": "Multi-Task Learning of Query Intent and Named Entities using Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Named entity recognition (NER) has been studied extensively and the earlier\nalgorithms were based on sequence labeling like Hidden Markov Models (HMM) and\nconditional random fields (CRF). These were followed by neural network based\ndeep learning models. Recently, BERT has shown new state of the art accuracy in\nsequence labeling tasks like NER. In this short article, we study various\napproaches to task specific NER. Task specific NER has two components -\nidentifying the intent of a piece of text (like search queries), and then\nlabeling the query with task specific named entities. For example, we consider\nthe task of labeling Target store locations in a search query (which could be\nentered in a search box or spoken in a device like Alexa or Google Home). Store\nlocations are highly ambiguous and sometimes it is difficult to differentiate\nbetween say a location and a non-location. For example, \"pickup my order at\norange store\" has \"orange\" as the store location, while \"buy orange at target\"\nhas \"orange\" as a fruit. We explore this difficulty by doing multi-task\nlearning which we call global to local transfer of information. We jointly\nlearn the query intent (i.e. store lookup) and the named entities by using\nmultiple loss functions in our BERT based model and find interesting results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 23:59:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shah", "Shalin", ""], ["Siskind", "Ryan", ""]]}, {"id": "2105.03317", "submitter": "Celine Lee", "authors": "Celine Lee (1 and 2), Justin Gottschlich (1 and 2), Dan Roth (2) ((1)\n  Intel Labs, (2) University of Pennsylvania)", "title": "Toward Code Generation: A Survey and Lessons from Semantic Parsing", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of natural language processing techniques and demand for\nimproved software engineering efficiency, there is an emerging interest in\ntranslating intention from human languages to programming languages. In this\nsurvey paper, we attempt to provide an overview of the growing body of research\nin this space. We begin by reviewing natural language semantic parsing\ntechniques and draw parallels with program synthesis efforts. We then consider\nsemantic parsing works from an evolutionary perspective, with specific analyses\non neuro-symbolic methods, architecture, and supervision. We then analyze\nadvancements in frameworks for semantic parsing for code generation. In\nclosing, we present what we believe are some of the emerging open challenges in\nthis domain.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 22:05:22 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lee", "Celine", "", "1 and 2"], ["Gottschlich", "Justin", "", "1 and 2"], ["Roth", "Dan", ""]]}, {"id": "2105.03322", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen\n  Qin, Donald Metzler", "title": "Are Pre-trained Convolutions Better than Pre-trained Transformers?", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of pre-trained language models, Transformers are the de facto\nchoice of model architectures. While recent research has shown promise in\nentirely convolutional, or CNN, architectures, they have not been explored\nusing the pre-train-fine-tune paradigm. In the context of language models, are\nconvolutional models competitive to Transformers when pre-trained? This paper\ninvestigates this research question and presents several interesting findings.\nAcross an extensive set of experiments on 8 datasets/tasks, we find that\nCNN-based pre-trained models are competitive and outperform their Transformer\ncounterpart in certain scenarios, albeit with caveats. Overall, the findings\noutlined in this paper suggest that conflating pre-training and architectural\nadvances is misguided and that both advances should be considered\nindependently. We believe our research paves the way for a healthy amount of\noptimism in alternative architectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:13:30 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Gupta", "Jai", ""], ["Bahri", "Dara", ""], ["Aribandi", "Vamsi", ""], ["Qin", "Zhen", ""], ["Metzler", "Donald", ""]]}, {"id": "2105.03343", "submitter": "Yang Gao", "authors": "Yang Gao and Nicolo Colombo and Wei Wang", "title": "Adapting by Pruning: A Case Study on BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting pre-trained neural models to downstream tasks has become the\nstandard practice for obtaining high-quality models. In this work, we propose a\nnovel model adaptation paradigm, adapting by pruning, which prunes neural\nconnections in the pre-trained model to optimise the performance on the target\ntask; all remaining connections have their weights intact. We formulate\nadapting-by-pruning as an optimisation problem with a differentiable loss and\npropose an efficient algorithm to prune the model. We prove that the algorithm\nis near-optimal under standard assumptions and apply the algorithm to adapt\nBERT to some GLUE tasks. Results suggest that our method can prune up to 50%\nweights in BERT while yielding similar performance compared to the fine-tuned\nfull model. We also compare our method with other state-of-the-art pruning\nmethods and study the topological differences of their obtained sub-networks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:51:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Gao", "Yang", ""], ["Colombo", "Nicolo", ""], ["Wang", "Wei", ""]]}, {"id": "2105.03375", "submitter": "Stefan Kuhn", "authors": "Antonio Cau, Stefan Kuhn, James Hoey", "title": "Executable Interval Temporal Logic Specifications", "comments": "11 pages, 0 figures, 1 table, abridged version to published in\n  Reversible Computation 2021", "journal-ref": null, "doi": "10.1007/978-3-030-79837-6_13", "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the reversibility of executable Interval Temporal Logic (ITL)\nspecifications is investigated. ITL allows for the reasoning about systems in\nterms of behaviours which are represented as non-empty sequences of states. It\nallows for the specification of systems at different levels of abstraction. At\na high level this specification is in terms of properties, for instance safety\nand liveness properties. At concrete level one can specify a system in terms of\nprogramming constructs. One can execute these concrete specification, i.e.,\ntest and simulate the behaviour of the system. In this paper we will formalise\nthis notion of executability of ITL specifications. ITL also has a reflection\noperator which allows for the reasoning about reversed behaviours. We will\ninvestigate the reversibility of executable ITL specifications, i.e., how one\ncan use this reflection operator to reverse the concrete behaviour of a\nparticular system.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:36:24 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Cau", "Antonio", ""], ["Kuhn", "Stefan", ""], ["Hoey", "James", ""]]}, {"id": "2105.03409", "submitter": "Binbin Xu", "authors": "Binbin Xu and Chongyang Tao and Zidu Feng and Youssef Raqui and Sylvie\n  Ranwez", "title": "A Benchmarking on Cloud based Speech-To-Text Services for French Speech\n  and Background Noise Effect", "comments": "6th National Conference on Practical Applications of Artificial\n  Intelligence, 2021, Bordeaux, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a large scale benchmarking on cloud based Speech-To-Text\nsystems: {Google Cloud Speech-To-Text}, {Microsoft Azure Cognitive Services},\n{Amazon Transcribe}, {IBM Watson Speech to Text}. For each systems, 40158 clean\nand noisy speech files about 101 hours are tested. Effect of background noise\non STT quality is also evaluated with 5 different Signal-to-noise ratios from\n40dB to 0dB. Results showed that {Microsoft Azure} provided lowest\ntranscription error rate $9.09\\%$ on clean speech, with high robustness to\nnoisy environment. {Google Cloud} and {Amazon Transcribe} gave similar\nperformance, but the latter is very limited for time-constraint usage. Though\n{IBM Watson} could work correctly in quiet conditions, it is highly sensible to\nnoisy speech which could strongly limit its application in real life\nsituations.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:41:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Xu", "Binbin", ""], ["Tao", "Chongyang", ""], ["Feng", "Zidu", ""], ["Raqui", "Youssef", ""], ["Ranwez", "Sylvie", ""]]}, {"id": "2105.03417", "submitter": "Mokanarangan Thayaparan", "authors": "Mokanarangan Thayaparan, Marco Valentino, Deborah Ferreira, Julia\n  Rozanova, Andr\\'e Freitas", "title": "$\\partial$-Explainer: Abductive Natural Language Inference via\n  Differentiable Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained optimization solvers with Integer Linear programming (ILP) have\nbeen the cornerstone for explainable natural language inference during its\ninception. ILP based approaches provide a way to encode explicit and\ncontrollable assumptions casting natural language inference as an abductive\nreasoning problem, where the solver constructs a plausible explanation for a\ngiven hypothesis. While constrained based solvers provide explanations, they\nare often limited by the use of explicit constraints and cannot be integrated\nas part of broader deep neural architectures. In contrast, state-of-the-art\ntransformer-based models can learn from data and implicitly encode complex\nconstraints. However, these models are intrinsically black boxes. This paper\npresents a novel framework named $\\partial$-Explainer (Diff-Explainer) that\ncombines the best of both worlds by casting the constrained optimization as\npart of a deep neural network via differentiable convex optimization and\nfine-tuning pre-trained transformers for downstream explainable NLP tasks. To\ndemonstrate the efficacy of the framework, we transform the constraints\npresented by TupleILP and integrate them with sentence embedding transformers\nfor the task of explainable science QA. Our experiments show up to $\\approx\n10\\%$ improvement over non-differentiable solver while still providing\nexplanations for supporting its inference.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:49:19 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Ferreira", "Deborah", ""], ["Rozanova", "Julia", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2105.03432", "submitter": "Giulio Zhou", "authors": "Giulio Zhou and Gerasimos Lampouras", "title": "Generalising Multilingual Concept-to-Text NLG with Language Agnostic\n  Delexicalisation", "comments": "To be published in the proceedings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:48:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Giulio", ""], ["Lampouras", "Gerasimos", ""]]}, {"id": "2105.03458", "submitter": "Zaixiang Zheng", "authors": "Zaixiang Zheng, Hao Zhou, Shujian Huang, Jiajun Chen, Jingjing Xu and\n  Lei Li", "title": "Duplex Sequence-to-Sequence Learning for Reversible Machine Translation", "comments": "Under review, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sequence-to-sequence (seq2seq) problems such as machine translation are\nbidirectional, which naturally derive a pair of directional tasks and two\ndirectional learning signals. However, typical seq2seq neural networks are {\\em\nsimplex} that only model one unidirectional task, which cannot fully exploit\nthe potential of bidirectional learning signals from parallel data. To address\nthis issue, we propose a {\\em duplex} seq2seq neural network, REDER (Reversible\nDuplex Transformer), and apply it to machine translation. The architecture of\nREDER has two ends, each of which specializes in a language so as to read and\nyield sequences in that language. As a result, REDER can simultaneously learn\nfrom the bidirectional signals, and enables {\\em reversible machine\ntranslation} by simply flipping the input and output ends, Experiments on\nwidely-used machine translation benchmarks verify that REDER achieves the first\nsuccess of reversible machine translation, which helps obtain considerable\ngains over several strong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:21:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zheng", "Zaixiang", ""], ["Zhou", "Hao", ""], ["Huang", "Shujian", ""], ["Chen", "Jiajun", ""], ["Xu", "Jingjing", ""], ["Li", "Lei", ""]]}, {"id": "2105.03482", "submitter": "Patrick Fernandes", "authors": "Patrick Fernandes, Kayo Yin, Graham Neubig, Andr\\'e F. T. Martins", "title": "Measuring and Increasing Context Usage in Context-Aware Machine\n  Translation", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in neural machine translation has demonstrated both the necessity\nand feasibility of using inter-sentential context -- context from sentences\nother than those currently being translated. However, while many current\nmethods present model architectures that theoretically can use this extra\ncontext, it is often not clear how much they do actually utilize it at\ntranslation time. In this paper, we introduce a new metric, conditional\ncross-mutual information, to quantify the usage of context by these models.\nUsing this metric, we measure how much document-level machine translation\nsystems use particular varieties of context. We find that target context is\nreferenced more than source context, and that conditioning on a longer context\nhas a diminishing effect on results. We then introduce a new, simple training\nmethod, context-aware word dropout, to increase the usage of context by\ncontext-aware models. Experiments show that our method increases context usage\nand that this reflects on the translation quality according to metrics such as\nBLEU and COMET, as well as performance on anaphoric pronoun resolution and\nlexical cohesion contrastive datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:55:35 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 11:07:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fernandes", "Patrick", ""], ["Yin", "Kayo", ""], ["Neubig", "Graham", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2105.03484", "submitter": "Adithya V Ganesan", "authors": "Adithya V Ganesan, Matthew Matero, Aravind Reddy Ravula, Huy Vu and H.\n  Andrew Schwartz", "title": "Empirical Evaluation of Pre-trained Transformers for Human-Level NLP:\n  The Role of Sample Size and Dimensionality", "comments": "2021 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics (NAACL-HLT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-level NLP tasks, such as predicting mental health, personality, or\ndemographics, the number of observations is often smaller than the standard\n768+ hidden state sizes of each layer within modern transformer-based language\nmodels, limiting the ability to effectively leverage transformers. Here, we\nprovide a systematic study on the role of dimension reduction methods\n(principal components analysis, factorization techniques, or multi-layer\nauto-encoders) as well as the dimensionality of embedding vectors and sample\nsizes as a function of predictive performance. We first find that fine-tuning\nlarge models with a limited amount of data pose a significant difficulty which\ncan be overcome with a pre-trained dimension reduction regime. RoBERTa\nconsistently achieves top performance in human-level tasks, with PCA giving\nbenefit over other reduction methods in better handling users that write longer\ntexts. Finally, we observe that a majority of the tasks achieve results\ncomparable to the best performance with just $\\frac{1}{12}$ of the embedding\ndimensions.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:06:24 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ganesan", "Adithya V", ""], ["Matero", "Matthew", ""], ["Ravula", "Aravind Reddy", ""], ["Vu", "Huy", ""], ["Schwartz", "H. Andrew", ""]]}, {"id": "2105.03495", "submitter": "Anne Beyer", "authors": "Anne Beyer and Sharid Lo\\'aiciga and David Schlangen", "title": "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction\n  from Language Models", "comments": "Accepted as long paper at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coherent discourse is distinguished from a mere collection of utterances by\nthe satisfaction of a diverse set of constraints, for example choice of\nexpression, logical relation between denoted events, and implicit compatibility\nwith world-knowledge. Do neural language models encode such constraints? We\ndesign an extendable set of test suites addressing different aspects of\ndiscourse and dialogue coherence. Unlike most previous coherence evaluation\nstudies, we address specific linguistic devices beyond sentence order\nperturbations, allowing for a more fine-grained analysis of what constitutes\ncoherence and what neural models trained on a language modelling objective do\nencode. Extending the targeted evaluation paradigm for neural language models\n(Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this\nparadigm is equally suited to evaluate linguistic qualities that contribute to\nthe notion of coherence.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:28:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Beyer", "Anne", ""], ["Lo\u00e1iciga", "Sharid", ""], ["Schlangen", "David", ""]]}, {"id": "2105.03505", "submitter": "Irene Li", "authors": "Irene Li, Vanessa Yan, Tianxiao Li, Rihao Qu and Dragomir Radev", "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational\n  Graph Autoencoders", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning prerequisite chains is an essential task for efficiently acquiring\nknowledge in both known and unknown domains. For example, one may be an expert\nin the natural language processing (NLP) domain but want to determine the best\norder to learn new concepts in an unfamiliar Computer Vision domain (CV). Both\ndomains share some common concepts, such as machine learning basics and deep\nlearning models. In this paper, we propose unsupervised cross-domain concept\nprerequisite chain learning using an optimized variational graph autoencoder.\nOur model learns to transfer concept prerequisite relations from an\ninformation-rich domain (source domain) to an information-poor domain (target\ndomain), substantially surpassing other baseline models. Also, we expand an\nexisting dataset by introducing two new domains: CV and Bioinformatics (BIO).\nThe annotated data and resources, as well as the code, will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 21:02:41 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:40:04 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 18:20:42 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Li", "Irene", ""], ["Yan", "Vanessa", ""], ["Li", "Tianxiao", ""], ["Qu", "Rihao", ""], ["Radev", "Dragomir", ""]]}, {"id": "2105.03519", "submitter": "Arian Hosseini", "authors": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Devon Hjelm,\n  Alessandro Sordoni and Aaron Courville", "title": "Understanding by Understanding Not: Modeling Negation in Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negation is a core construction in natural language. Despite being very\nsuccessful on many tasks, state-of-the-art pre-trained language models often\nhandle negation incorrectly. To improve language models in this regard, we\npropose to augment the language modeling objective with an unlikelihood\nobjective that is based on negated generic sentences from a raw text corpus. By\ntraining BERT with the resulting combined objective we reduce the mean top~1\nerror rate to 4% on the negated LAMA dataset. We also see some improvements on\nthe negated NLI benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 21:58:35 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hosseini", "Arian", ""], ["Reddy", "Siva", ""], ["Bahdanau", "Dzmitry", ""], ["Hjelm", "R Devon", ""], ["Sordoni", "Alessandro", ""], ["Courville", "Aaron", ""]]}, {"id": "2105.03571", "submitter": "Puhai Yang", "authors": "Puhai Yang and Heyan Huang and Xian-Ling Mao", "title": "Comprehensive Study: How the Context Information of Different\n  Granularity Affects Dialogue State Tracking?", "comments": "Accepted as long paper at main conference of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:18:13 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:55:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Puhai", ""], ["Huang", "Heyan", ""], ["Mao", "Xian-Ling", ""]]}, {"id": "2105.03599", "submitter": "Hongyin Tang", "authors": "Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang,\n  Wei Wu", "title": "Improving Document Representations by Generating Pseudo Query Embeddings\n  for Dense Retrieval", "comments": "11 pages, 2 figures, Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:28:24 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tang", "Hongyin", ""], ["Sun", "Xingwu", ""], ["Jin", "Beihong", ""], ["Wang", "Jingang", ""], ["Zhang", "Fuzheng", ""], ["Wu", "Wei", ""]]}, {"id": "2105.03627", "submitter": "Wei-Cheng Huang", "authors": "Wei-Cheng Huang, Chien-yu Huang, Hung-yi Lee", "title": "Improving Cross-Lingual Reading Comprehension with Self-Training", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Substantial improvements have been made in machine reading comprehension,\nwhere the machine answers questions based on a given context. Current\nstate-of-the-art models even surpass human performance on several benchmarks.\nHowever, their abilities in the cross-lingual scenario are still to be\nexplored. Previous works have revealed the abilities of pre-trained\nmultilingual models for zero-shot cross-lingual reading comprehension. In this\npaper, we further utilized unlabeled data to improve the performance. The model\nis first supervised-trained on source language corpus, and then self-trained\nwith unlabeled target language data. The experiment results showed improvements\nfor all languages, and we also analyzed how self-training benefits\ncross-lingual reading comprehension in qualitative aspects.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 08:04:30 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Wei-Cheng", ""], ["Huang", "Chien-yu", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2105.03640", "submitter": "Emanuele La Malfa", "authors": "Emanuele La Malfa, Agnieszka Zbrzezny, Rhiannon Michelmore, Nicola\n  Paoletti and Marta Kwiatkowska", "title": "On Guaranteed Optimal Robust Explanations for NLP Models", "comments": "13 pages (8+5 Appendix). Accepted as long-paper at IJCAI 2021", "journal-ref": "IJCAI 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on abduction-based explanations for ma-chine learning and develop a\nmethod for computing local explanations for neural network models in natural\nlanguage processing (NLP). Our explanations comprise a subset of the words of\nthe in-put text that satisfies two key features: optimality w.r.t. a\nuser-defined cost function, such as the length of explanation, and robustness,\nin that they ensure prediction invariance for any bounded perturbation in the\nembedding space of the left out words. We present two solution algorithms,\nrespectively based on implicit hitting sets and maximum universal subsets,\nintroducing a number of algorithmic improvements to speed up convergence of\nhard instances. We show how our method can be con-figured with different\nperturbation sets in the em-bedded space and used to detect bias in predictions\nby enforcing include/exclude constraints on biased terms, as well as to enhance\nexisting heuristic-based NLP explanation frameworks such as Anchors. We\nevaluate our framework on three widely used sentiment analysis tasks and texts\nof up to100words from SST, Twitter and IMDB datasets,demonstrating the\neffectiveness of the derived explanations.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 08:44:48 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 08:41:57 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["La Malfa", "Emanuele", ""], ["Zbrzezny", "Agnieszka", ""], ["Michelmore", "Rhiannon", ""], ["Paoletti", "Nicola", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "2105.03641", "submitter": "Zhixian Yang", "authors": "Zhixian Yang, Xiaojun Wan", "title": "Neural Text Generation with Part-of-Speech Guided Softmax", "comments": "Main text: 8 pages, 2 figures, 8 tables. Supplementary Information: 2\n  pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation models are likely to suffer from the low-diversity\nproblem. Various decoding strategies and training-based methods have been\nproposed to promote diversity only by exploiting contextual features, but\nrarely do they consider incorporating syntactic structure clues. In this work,\nwe propose using linguistic annotation, i.e., part-of-speech (POS), to guide\nthe text generation. In detail, we introduce POS Guided Softmax (POSG-Softmax)\nto explicitly model two posterior probabilities: (i) next-POS, and (ii)\nnext-token from the vocabulary of the target POS. A POS guided sampling\nstrategy is further proposed to address the low-diversity problem by enriching\nthe diversity of POS. Extensive experiments and human evaluations demonstrate\nthat, compared with existing state-of-the-art methods, our proposed methods can\ngenerate more diverse text while maintaining comparable quality.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 08:53:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Zhixian", ""], ["Wan", "Xiaojun", ""]]}, {"id": "2105.03654", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei\n  Huang, Kewei Tu", "title": "Improving Named Entity Recognition by External Context Retrieving and\n  Cooperative Learning", "comments": "Accepted to ACL 2021, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in Named Entity Recognition (NER) show that document-level\ncontexts can significantly improve model performance. In many application\nscenarios, however, such contexts are not available. In this paper, we propose\nto find external contexts of a sentence by retrieving and selecting a set of\nsemantically relevant texts through a search engine, with the original sentence\nas the query. We find empirically that the contextual representations computed\non the retrieval-based input view, constructed through the concatenation of a\nsentence and its external contexts, can achieve significantly improved\nperformance compared to the original input view based only on the sentence.\nFurthermore, we can improve the model performance of both input views by\nCooperative Learning, a training method that encourages the two input views to\nproduce similar contextual representations or output label distributions.\nExperiments show that our approach can achieve new state-of-the-art performance\non 8 NER data sets across 5 domains.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:45:21 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 02:08:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Xinyu", ""], ["Jiang", "Yong", ""], ["Bach", "Nguyen", ""], ["Wang", "Tao", ""], ["Huang", "Zhongqiang", ""], ["Huang", "Fei", ""], ["Tu", "Kewei", ""]]}, {"id": "2105.03659", "submitter": "Siyuan Wang", "authors": "Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin\n  Jiang, Ming Zhou and Nan Duan", "title": "Logic-Driven Context Extension and Data Augmentation for Logical\n  Reasoning of Text", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logical reasoning of text requires understanding critical logical information\nin the text and performing inference over them. Large-scale pre-trained models\nfor logical reasoning mainly focus on word-level semantics of text while\nstruggling to capture symbolic logic. In this paper, we propose to understand\nlogical symbols and expressions in the text to arrive at the answer. Based on\nsuch logical information, we not only put forward a context extension framework\nbut also propose a data augmentation algorithm. The former extends the context\nto cover implicit logical expressions following logical equivalence laws. The\nlatter augments literally similar but logically different instances to better\ncapture logical information, especially logical negative and conditional\nrelationships. We conduct experiments on ReClor dataset. The results show that\nour method achieves the state-of-the-art performance, and both logic-driven\ncontext extension framework and data augmentation algorithm can help improve\nthe accuracy. And our multi-model ensemble system is the first to surpass human\nperformance on both EASY set and HARD set of ReClor.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:09:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Siyuan", ""], ["Zhong", "Wanjun", ""], ["Tang", "Duyu", ""], ["Wei", "Zhongyu", ""], ["Fan", "Zhihao", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""], ["Duan", "Nan", ""]]}, {"id": "2105.03664", "submitter": "Yufang Hou", "authors": "Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, Nancy X.R. Wang", "title": "D2S: Document-to-Slide Generation Via Query-Based Text Summarization", "comments": "accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presentations are critical for communication in all areas of our lives, yet\nthe creation of slide decks is often tedious and time-consuming. There has been\nlimited research aiming to automate the document-to-slides generation process\nand all face a critical challenge: no publicly available dataset for training\nand benchmarking. In this work, we first contribute a new dataset, SciDuet,\nconsisting of pairs of papers and their corresponding slides decks from recent\nyears' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel\nsystem that tackles the document-to-slides task with a two-step approach: 1)\nUse slide titles to retrieve relevant and engaging text, figures, and tables;\n2) Summarize the retrieved context into bullet points with long-form question\nanswering. Our evaluation suggests that long-form QA outperforms\nstate-of-the-art summarization baselines on both automated ROUGE metrics and\nqualitative human evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 10:29:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sun", "Edward", ""], ["Hou", "Yufang", ""], ["Wang", "Dakuo", ""], ["Zhang", "Yunfeng", ""], ["Wang", "Nancy X. R.", ""]]}, {"id": "2105.03710", "submitter": "Deniz Beser", "authors": "Deniz Beser", "title": "Falling Through the Gaps: Neural Architectures as Models of\n  Morphological Rule Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in neural architectures have revived the problem of\nmorphological rule learning. We evaluate the Transformer as a model of\nmorphological rule learning and compare it with Recurrent Neural Networks (RNN)\non English, German, and Russian. We bring to the fore a hitherto overlooked\nproblem, the morphological gaps, where the expected inflection of a word is\nmissing. For example, 63 Russian verbs lack a first-person-singular present\nform such that one cannot comfortably say \"*o\\v{s}\\v{c}u\\v{s}\\v{c}u\" (\"I\nfeel\"). Even English has gaps, such as the past participle of \"stride\": the\nfunction of morphological inflection can be partial. Both neural architectures\nproduce inflections that ought to be missing. Analyses reveal that Transformers\nrecapitulate the statistical distribution of inflections in the training data,\nsimilar to RNNs. Models' success on English and German is driven by the fact\nthat rules in these languages can be identified with the majority forms, which\nis not universal.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 14:48:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Beser", "Deniz", ""]]}, {"id": "2105.03716", "submitter": "Anton Ragni", "authors": "Sindre Andr\\'e Jacobsen and Anton Ragni", "title": "Continuous representations of intents for dialogue systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent modelling has become an important part of modern dialogue systems.\nWith the rapid expansion of practical dialogue systems and virtual assistants,\nsuch as Amazon Alexa, Apple Siri, and Google Assistant, the interest has only\nincreased. However, up until recently the focus has been on detecting a fixed,\ndiscrete, number of seen intents. Recent years have seen some work done on\nunseen intent detection in the context of zero-shot learning. This paper\ncontinues the prior work by proposing a novel model where intents are\ncontinuous points placed in a specialist Intent Space that yields several\nadvantages. First, the continuous representation enables to investigate\nrelationships between the seen intents. Second, it allows any unseen intent to\nbe reliably represented given limited quantities of data. Finally, this paper\nwill show how the proposed model can be augmented with unseen intents without\nretraining any of the seen ones. Experiments show that the model can reliably\nadd unseen intents with a high accuracy while retaining a high performance on\nthe seen intents.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 15:08:20 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jacobsen", "Sindre Andr\u00e9", ""], ["Ragni", "Anton", ""]]}, {"id": "2105.03743", "submitter": "Jiehang Zeng", "authors": "Jiehang Zeng, Xiaoqing Zheng, Jianhan Xu, Linyang Li, Liping Yuan and\n  Xuanjing Huang", "title": "Certified Robustness to Text Adversarial Attacks by Randomized [MASK]", "comments": "Under Review for TOPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, few certified defense methods have been developed to provably\nguarantee the robustness of a text classifier to adversarial synonym\nsubstitutions. However, all existing certified defense methods assume that the\ndefenders are informed of how the adversaries generate synonyms, which is not a\nrealistic scenario. In this paper, we propose a certifiably robust defense\nmethod by randomly masking a certain proportion of the words in an input text,\nin which the above unrealistic assumption is no longer necessary. The proposed\nmethod can defend against not only word substitution-based attacks, but also\ncharacter-level perturbations. We can certify the classifications of over 50%\ntexts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on\nSST2 dataset. The experimental results show that our randomized smoothing\nmethod significantly outperforms recently proposed defense methods across\nmultiple datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:59:10 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 08:30:40 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 17:28:12 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zeng", "Jiehang", ""], ["Zheng", "Xiaoqing", ""], ["Xu", "Jianhan", ""], ["Li", "Linyang", ""], ["Yuan", "Liping", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2105.03761", "submitter": "Maxime Kayser", "authors": "Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde,\n  Virginie Do, Zeynep Akata, Thomas Lukasiewicz", "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an increasing number of works have introduced models capable of\ngenerating natural language explanations (NLEs) for their predictions on\nvision-language (VL) tasks. Such models are appealing because they can provide\nhuman-friendly and comprehensive explanations. However, there is still a lack\nof unified evaluation approaches for the explanations generated by these\nmodels. Moreover, there are currently only few datasets of NLEs for VL tasks.\nIn this work, we introduce e-ViL, a benchmark for explainable vision-language\ntasks that establishes a unified evaluation framework and provides the first\ncomprehensive comparison of existing approaches that generate NLEs for VL\ntasks. e-ViL spans four models and three datasets. Both automatic metrics and\nhuman evaluation are used to assess model-generated explanations. We also\nintroduce e-SNLI-VE, the largest existing VL dataset with NLEs (over 430k\ninstances). Finally, we propose a new model that combines UNITER, which learns\njoint embeddings of images and text, and GPT-2, a pre-trained language model\nthat is well-suited for text generation. It surpasses the previous\nstate-of-the-art by a large margin across all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 18:46:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kayser", "Maxime", ""], ["Camburu", "Oana-Maria", ""], ["Salewski", "Leonard", ""], ["Emde", "Cornelius", ""], ["Do", "Virginie", ""], ["Akata", "Zeynep", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2105.03775", "submitter": "Hossein Basafa", "authors": "Hossein Basafa, Sajad Movahedi, Ali Ebrahimi, Azadeh Shakery and\n  Heshaam Faili", "title": "NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using\n  the Long Document Transformer", "comments": "6 pages, 1 figure. Accepted in SemEval2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technical report of our submission to the 4th task of\nSemEval-2021, titled: Reading Comprehension of Abstract Meaning. In this task,\nwe want to predict the correct answer based on a question given a context.\nUsually, contexts are very lengthy and require a large receptive field from the\nmodel. Thus, common contextualized language models like BERT miss fine\nrepresentation and performance due to the limited capacity of the input tokens.\nTo tackle this problem, we used the Longformer model to better process the\nsequences. Furthermore, we utilized the method proposed in the Longformer\nbenchmark on Wikihop dataset which improved the accuracy on our task data from\n23.01% and 22.95% achieved by the baselines for subtask 1 and 2, respectively,\nto 70.30% and 64.38%.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 20:48:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Basafa", "Hossein", ""], ["Movahedi", "Sajad", ""], ["Ebrahimi", "Ali", ""], ["Shakery", "Azadeh", ""], ["Faili", "Heshaam", ""]]}, {"id": "2105.03791", "submitter": "Benjamin Minixhofer", "authors": "Benjamin Minixhofer, Milan Gritta, Ignacio Iacobacci", "title": "Enhancing Transformers with Gradient Boosted Decision Trees for NLI\n  Fine-Tuning", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning has become the dominant paradigm for many natural language\nprocessing tasks. In addition to models being pretrained on large datasets,\nthey can be further trained on intermediate (supervised) tasks that are similar\nto the target task. For small Natural Language Inference (NLI) datasets,\nlanguage modelling is typically followed by pretraining on a large (labelled)\nNLI dataset before fine-tuning with each NLI subtask. In this work, we explore\nGradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used\nMulti-Layer Perceptron (MLP) classification head. GBDTs have desirable\nproperties such as good performance on dense, numerical features and are\neffective where the ratio of the number of samples w.r.t the number of features\nis low. We then introduce FreeGBDT, a method of fitting a GBDT head on the\nfeatures computed during fine-tuning to increase performance without additional\ncomputation by the neural network. We demonstrate the effectiveness of our\nmethod on several NLI datasets using a strong baseline model (RoBERTa-large\nwith MNLI pretraining). The FreeGBDT shows a consistent improvement over the\nMLP classification head.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 22:31:51 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:35:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Minixhofer", "Benjamin", ""], ["Gritta", "Milan", ""], ["Iacobacci", "Ignacio", ""]]}, {"id": "2105.03801", "submitter": "Potsawee Manakul", "authors": "Potsawee Manakul and Mark J. F. Gales", "title": "Long-Span Summarization via Local Attention and Content Selection", "comments": "ACL 2021 (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models have achieved state-of-the-art results in a wide\nrange of natural language processing (NLP) tasks including document\nsummarization. Typically these systems are trained by fine-tuning a large\npre-trained model to the target task. One issue with these transformer-based\nmodels is that they do not scale well in terms of memory and compute\nrequirements as the input length grows. Thus, for long document summarization,\nit can be challenging to train or fine-tune these models. In this work, we\nexploit large pre-trained transformer-based models and address long-span\ndependencies in abstractive summarization using two methods: local\nself-attention; and explicit content selection. These approaches are compared\non a range of network configurations. Experiments are carried out on standard\nlong-span summarization tasks, including Spotify Podcast, arXiv, and PubMed\ndatasets. We demonstrate that by combining these methods, we can achieve\nstate-of-the-art results on all three tasks in the ROUGE scores. Moreover,\nwithout a large-scale GPU card, our approach can achieve comparable or better\nresults than existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 23:53:03 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 11:23:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Manakul", "Potsawee", ""], ["Gales", "Mark J. F.", ""]]}, {"id": "2105.03815", "submitter": "Junyi Li", "authors": "Junyi Li, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan and Ji-Rong\n  Wen", "title": "Knowledge-based Review Generation by Coherence Enhanced Text Planning", "comments": "Accepted by SIGIR 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a natural language generation task, it is challenging to generate\ninformative and coherent review text. In order to enhance the informativeness\nof the generated text, existing solutions typically learn to copy entities or\ntriples from knowledge graphs (KGs). However, they lack overall consideration\nto select and arrange the incorporated knowledge, which tends to cause text\nincoherence.\n  To address the above issue, we focus on improving entity-centric coherence of\nthe generated reviews by leveraging the semantic structure of KGs. In this\npaper, we propose a novel Coherence Enhanced Text Planning model (CETP) based\non knowledge graphs (KGs) to improve both global and local coherence for review\ngeneration. The proposed model learns a two-level text plan for generating a\ndocument: (1) the document plan is modeled as a sequence of sentence plans in\norder, and (2) the sentence plan is modeled as an entity-based subgraph from\nKG. Local coherence can be naturally enforced by KG subgraphs through\nintra-sentence correlations between entities. For global coherence, we design a\nhierarchical self-attentive architecture with both subgraph- and node-level\nattention to enhance the correlations between subgraphs. To our knowledge, we\nare the first to utilize a KG-based text planning model to enhance text\ncoherence for review generation. Extensive experiments on three datasets\nconfirm the effectiveness of our model on improving the content coherence of\ngenerated texts.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 02:12:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Junyi", ""], ["Zhao", "Wayne Xin", ""], ["Wei", "Zhicheng", ""], ["Yuan", "Nicholas Jing", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2105.03824", "submitter": "James Lee-Thorp", "authors": "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon", "title": "FNet: Mixing Tokens with Fourier Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Transformer encoder architectures can be massively sped up, with\nlimited accuracy costs, by replacing the self-attention sublayers with simple\nlinear transformations that \"mix\" input tokens. These linear transformations,\nalong with standard nonlinearities in feed-forward layers, prove competent at\nmodeling semantic relationships in several text classification tasks. Most\nsurprisingly, we find that replacing the self-attention sublayer in a\nTransformer encoder with a standard, unparameterized Fourier Transform achieves\n92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains\nnearly seven times faster on GPUs and twice as fast on TPUs. The resulting\nmodel, FNet, also scales very efficiently to long inputs. Specifically, when\ncompared to the \"efficient\" Transformers on the Long Range Arena benchmark,\nFNet matches the accuracy of the most accurate models, but is faster than the\nfastest models across all sequence lengths on GPUs (and across relatively\nshorter lengths on TPUs). Finally, FNet has a light memory footprint and is\nparticularly efficient at smaller model sizes: for a fixed speed and accuracy\nbudget, small FNet models outperform Transformer counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:32:48 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 18:56:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lee-Thorp", "James", ""], ["Ainslie", "Joshua", ""], ["Eckstein", "Ilya", ""], ["Ontanon", "Santiago", ""]]}, {"id": "2105.03842", "submitter": "Yichong Leng", "authors": "Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu,\n  Tao Qin, Xiang-Yang Li, Ed Lin, Tie-Yan Liu", "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 05:35:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 13:20:13 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 02:11:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Leng", "Yichong", ""], ["Tan", "Xu", ""], ["Zhu", "Linchen", ""], ["Xu", "Jin", ""], ["Luo", "Renqian", ""], ["Liu", "Linquan", ""], ["Qin", "Tao", ""], ["Li", "Xiang-Yang", ""], ["Lin", "Ed", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.03887", "submitter": "Chaojun Xiao", "authors": "Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, Maosong Sun", "title": "Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal artificial intelligence (LegalAI) aims to benefit legal systems with\nthe technology of artificial intelligence, especially natural language\nprocessing (NLP). Recently, inspired by the success of pre-trained language\nmodels (PLMs) in the generic domain, many LegalAI researchers devote their\neffort to apply PLMs to legal tasks. However, utilizing PLMs to address legal\ntasks is still challenging, as the legal documents usually consist of thousands\nof tokens, which is far longer than the length that mainstream PLMs can\nprocess. In this paper, we release the Longformer-based pre-trained language\nmodel, named as Lawformer, for Chinese legal long documents understanding. We\nevaluate Lawformer on a variety of LegalAI tasks, including judgment\nprediction, similar case retrieval, legal reading comprehension, and legal\nquestion answering. The experimental results demonstrate that our model can\nachieve promising improvement on tasks with long documents as inputs.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 09:39:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xiao", "Chaojun", ""], ["Hu", "Xueyu", ""], ["Liu", "Zhiyuan", ""], ["Tu", "Cunchao", ""], ["Sun", "Maosong", ""]]}, {"id": "2105.03928", "submitter": "Noam Wies", "authors": "Noam Wies, Yoav Levine, Daniel Jannai, Amnon Shashua", "title": "Which transformer architecture fits my data? A vocabulary bottleneck in\n  self-attention", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  After their successful debut in natural language processing, Transformer\narchitectures are now becoming the de-facto standard in many domains. An\nobstacle for their deployment over new modalities is the architectural\nconfiguration: the optimal depth-to-width ratio has been shown to dramatically\nvary across data types (e.g., $10$x larger over images than over language). We\ntheoretically predict the existence of an embedding rank bottleneck that limits\nthe contribution of self-attention width to the Transformer expressivity. We\nthus directly tie the input vocabulary size and rank to the optimal\ndepth-to-width ratio, since a small vocabulary size or rank dictates an added\nadvantage of depth over width. We empirically demonstrate the existence of this\nbottleneck and its implications on the depth-to-width interplay of Transformer\narchitectures, linking the architecture variability across domains to the often\nglossed-over usage of different vocabulary sizes or embedding ranks in\ndifferent domains. As an additional benefit, our rank bottlenecking framework\nallows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models\nsuch as ALBERT and T5.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:08:26 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:18:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wies", "Noam", ""], ["Levine", "Yoav", ""], ["Jannai", "Daniel", ""], ["Shashua", "Amnon", ""]]}, {"id": "2105.03943", "submitter": "Rishi Hazra", "authors": "Rishi Hazra and Sonu Dixit", "title": "gComm: An environment for investigating generalization in Grounded\n  Language Acquisition", "comments": "Accepted in NAACL 2021 workshop: Visually Grounded Interaction and\n  Language (ViGIL). arXiv admin note: substantial text overlap with\n  arXiv:2012.05011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  gComm is a step towards developing a robust platform to foster research in\ngrounded language acquisition in a more challenging and realistic setting. It\ncomprises a 2-d grid environment with a set of agents (a stationary speaker and\na mobile listener connected via a communication channel) exposed to a\ncontinuous array of tasks in a partially observable setting. The key to solving\nthese tasks lies in agents developing linguistic abilities and utilizing them\nfor efficiently exploring the environment. The speaker and listener have access\nto information provided in different modalities, i.e. the speaker's input is a\nnatural language instruction that contains the target and task specifications\nand the listener's input is its grid-view. Each must rely on the other to\ncomplete the assigned task, however, the only way they can achieve the same, is\nto develop and use some form of communication. gComm provides several tools for\nstudying different forms of communication and assessing their generalization.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:44:55 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 01:20:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hazra", "Rishi", ""], ["Dixit", "Sonu", ""]]}, {"id": "2105.03949", "submitter": "Shashi Gowda", "authors": "Shashi Gowda, Yingbo Ma, Alessandro Cheli, Maja Gwozdz, Viral B. Shah,\n  Alan Edelman, Christopher Rackauckas", "title": "High-performance symbolic-numerics via multiple dispatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MS cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As mathematical computing becomes more democratized in high-level languages,\nhigh-performance symbolic-numeric systems are necessary for domain scientists\nand engineers to get the best performance out of their machine without deep\nknowledge of code optimization. Naturally, users need different term types\neither to have different algebraic properties for them, or to use efficient\ndata structures. To this end, we developed Symbolics.jl, an extendable symbolic\nsystem which uses dynamic multiple dispatch to change behavior depending on the\ndomain needs. In this work we detail an underlying abstract term interface\nwhich allows for speed without sacrificing generality. We show that by\nformalizing a generic API on actions independent of implementation, we can\nretroactively add optimized data structures to our system without changing the\npre-existing term rewriters. We showcase how this can be used to optimize term\nconstruction and give a 113x acceleration on general symbolic transformations.\nFurther, we show that such a generic API allows for complementary\nterm-rewriting implementations. We demonstrate the ability to swap between\nclassical term-rewriting simplifiers and e-graph-based term-rewriting\nsimplifiers. We showcase an e-graph ruleset which minimizes the number of CPU\ncycles during expression evaluation, and demonstrate how it simplifies a\nreal-world reaction-network simulation to halve the runtime. Additionally, we\nshow a reaction-diffusion partial differential equation solver which is able to\nbe automatically converted into symbolic expressions via multiple dispatch\ntracing, which is subsequently accelerated and parallelized to give a 157x\nsimulation speedup. Together, this presents Symbolics.jl as a next-generation\nsymbolic-numeric computing environment geared towards modeling and simulation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 14:22:43 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:02:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Gowda", "Shashi", ""], ["Ma", "Yingbo", ""], ["Cheli", "Alessandro", ""], ["Gwozdz", "Maja", ""], ["Shah", "Viral B.", ""], ["Edelman", "Alan", ""], ["Rackauckas", "Christopher", ""]]}, {"id": "2105.03953", "submitter": "Zihan Liu", "authors": "Zihan Liu, Genta Indra Winata, Pascale Fung", "title": "Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural\n  Machine Translation", "comments": "Accepted in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data scarcity in low-resource languages has become a bottleneck to\nbuilding robust neural machine translation systems. Fine-tuning a multilingual\npre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a\ngood approach for low-resource languages; however, its performance will be\ngreatly limited when there are unseen languages in the translation pairs. In\nthis paper, we present a continual pre-training (CPT) framework on mBART to\neffectively adapt it to unseen languages. We first construct noisy\nmixed-language text from the monolingual corpus of the target language in the\ntranslation pair to cover both the source and target languages, and then, we\ncontinue pre-training mBART to reconstruct the original monolingual text.\nResults show that our method can consistently improve the fine-tuning\nperformance upon the mBART baseline, as well as other strong baselines, across\nall tested low-resource translation pairs containing unseen languages.\nFurthermore, our approach also boosts the performance on translation pairs\nwhere both languages are seen in the original mBART's pre-training. The code is\navailable at https://github.com/zliucr/cpt-nmt.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 14:49:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Zihan", ""], ["Winata", "Genta Indra", ""], ["Fung", "Pascale", ""]]}, {"id": "2105.03979", "submitter": "Binbin Xu", "authors": "Th\\'eo Ding and Walter Vermeiren and Sylvie Ranwez and Binbin Xu", "title": "Improving Patent Mining and Relevance Classification using Transformers", "comments": "6th National Conference on Practical Applications of Artificial\n  Intelligence, 2021, Bordeaux, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patent analysis and mining are time-consuming and costly processes for\ncompanies, but nevertheless essential if they are willing to remain\ncompetitive. To face the overload induced by numerous patents, the idea is to\nautomatically filter them, bringing only few to read to experts. This paper\nreports a successful application of fine-tuning and retraining on pre-trained\ndeep Natural Language Processing models on patent classification. The solution\nthat we propose combines several state-of-the-art treatments to achieve our\ngoal - decrease the workload while preserving recall and precision metrics.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 17:57:55 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 13:09:29 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ding", "Th\u00e9o", ""], ["Vermeiren", "Walter", ""], ["Ranwez", "Sylvie", ""], ["Xu", "Binbin", ""]]}, {"id": "2105.03983", "submitter": "Rajdeep Mukherjee", "authors": "Rajdeep Mukherjee, Atharva Naik, Sriyash Poddar, Soham Dasgupta, Niloy\n  Ganguly", "title": "Understanding the Role of Affect Dimensions in Detecting Emotions from\n  Tweets: A Multi-task Approach", "comments": "5 pages, Short Paper accepted at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463080", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose VADEC, a multi-task framework that exploits the correlation\nbetween the categorical and dimensional models of emotion representation for\nbetter subjectivity analysis. Focusing primarily on the effective detection of\nemotions from tweets, we jointly train multi-label emotion classification and\nmulti-dimensional emotion regression, thereby utilizing the inter-relatedness\nbetween the tasks. Co-training especially helps in improving the performance of\nthe classification task as we outperform the strongest baselines with 3.4%,\n11%, and 3.9% gains in Jaccard Accuracy, Macro-F1, and Micro-F1 scores\nrespectively on the AIT dataset. We also achieve state-of-the-art results with\n11.3% gains averaged over six different metrics on the SenWave dataset. For the\nregression task, VADEC, when trained with SenWave, achieves 7.6% and 16.5%\ngains in Pearson Correlation scores over the current state-of-the-art on the\nEMOBANK dataset for the Valence (V) and Dominance (D) affect dimensions\nrespectively. We conclude our work with a case study on COVID-19 tweets posted\nby Indians that further helps in establishing the efficacy of our proposed\nsolution.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:07:04 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Mukherjee", "Rajdeep", ""], ["Naik", "Atharva", ""], ["Poddar", "Sriyash", ""], ["Dasgupta", "Soham", ""], ["Ganguly", "Niloy", ""]]}, {"id": "2105.03986", "submitter": "Sarit Kraus", "authors": "Aviram Aviv, Yaniv Oshrat, Samuel A. Assefa, Tobi Mustapha, Daniel\n  Borrajo, Manuela Veloso, Sarit Kraus", "title": "Advising Agent for Service-Providing Live-Chat Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Call centers, in which human operators attend clients using textual chat, are\nvery common in modern e-commerce. Training enough skilled operators who are\nable to provide good service is a challenge. We suggest an algorithm and a\nmethod to train and implement an assisting agent that provides on-line advice\nto operators while they attend clients. The agent is domain-independent and can\nbe introduced to new domains without major efforts in design, training and\norganizing structured knowledge of the professional discipline. We demonstrate\nthe applicability of the system in an experiment that realizes its full\nlife-cycle on a specific domain and analyze its capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:10:54 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 13:46:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Aviv", "Aviram", ""], ["Oshrat", "Yaniv", ""], ["Assefa", "Samuel A.", ""], ["Mustapha", "Tobi", ""], ["Borrajo", "Daniel", ""], ["Veloso", "Manuela", ""], ["Kraus", "Sarit", ""]]}, {"id": "2105.03994", "submitter": "Alberto Cetoli", "authors": "Alberto Cetoli", "title": "Dispatcher: A Message-Passing Approach To Language Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a message-passing mechanism to address language\nmodelling. A new layer type is introduced that aims to substitute\nself-attention. The system is shown to be competitive with existing methods:\nGiven N tokens, the computational complexity is O(N log N) and the memory\ncomplexity is O(N) under reasonable assumptions. In the end, the Dispatcher\nlayer is seen to achieve comparable perplexity to prior results while being\nmore efficient\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:57:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cetoli", "Alberto", ""]]}, {"id": "2105.04024", "submitter": "Dominik Stammbach", "authors": "Dominik Stammbach, Elliott Ash", "title": "DocSCAN: Unsupervised Text Classification via Learning from Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce DocSCAN, a completely unsupervised text classification approach\nusing Semantic Clustering by Adopting Nearest-Neighbors (SCAN). For each\ndocument, we obtain semantically informative vectors from a large pre-trained\nlanguage model. Similar documents have proximate vectors, so neighbors in the\nrepresentation space tend to share topic labels. Our learnable clustering\napproach uses pairs of neighboring datapoints as a weak learning signal. The\nproposed approach learns to assign classes to the whole dataset without\nprovided ground-truth labels. On five topic classification benchmarks, we\nimprove on various unsupervised baselines by a large margin. In datasets with\nrelatively few and balanced outcome classes, DocSCAN approaches the performance\nof supervised classification. The method fails for other types of\nclassification, such as sentiment analysis, pointing to important conceptual\nand practical differences between classifying images and texts.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 21:20:31 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:32:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Stammbach", "Dominik", ""], ["Ash", "Elliott", ""]]}, {"id": "2105.04047", "submitter": "Danae Sanchez Villegas", "authors": "Danae S\\'anchez Villegas, Saeid Mokaram, Nikolaos Aletras", "title": "Analyzing Online Political Advertisements", "comments": "Accepted at ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online political advertising is a central aspect of modern election\ncampaigning for influencing public opinion. Computational analysis of political\nads is of utmost importance in political science to understand the\ncharacteristics of digital campaigning. It is also important in computational\nlinguistics to study features of political discourse and communication on a\nlarge scale. In this work, we present the first computational study on online\npolitical ads with the aim to (1) infer the political ideology of an ad\nsponsor; and (2) identify whether the sponsor is an official political party or\na third-party organization. We develop two new large datasets for the two tasks\nconsisting of ads from the U.S.. Evaluation results show that our approach that\ncombines textual and visual information from pre-trained neural models\noutperforms a state-of-the-art method for generic commercial ad classification.\nFinally, we provide an in-depth analysis of the limitations of our\nbest-performing models and linguistic analysis to study the characteristics of\npolitical ads discourse.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 23:18:37 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 12:44:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Villegas", "Danae S\u00e1nchez", ""], ["Mokaram", "Saeid", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2105.04054", "submitter": "Emily Sheng", "authors": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng", "title": "Societal Biases in Language Generation: Progress and Challenges", "comments": "ACL 2021 camera-ready (v2), updated references (v3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 00:17:33 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 18:54:32 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 20:44:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sheng", "Emily", ""], ["Chang", "Kai-Wei", ""], ["Natarajan", "Premkumar", ""], ["Peng", "Nanyun", ""]]}, {"id": "2105.04098", "submitter": "Chun Yuan Yuan", "authors": "Chunyuan Yuan, Wanhui Qian, Qianwen Ma, Wei Zhou, Songlin Hu", "title": "SRLF: A Stance-aware Reinforcement Learning Framework for Content-based\n  Rumor Detection on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of social media changes the lifestyle of people and\nsimultaneously provides an ideal place for publishing and disseminating rumors,\nwhich severely exacerbates social panic and triggers a crisis of social trust.\nEarly content-based methods focused on finding clues from the text and user\nprofiles for rumor detection. Recent studies combine the stances of users'\ncomments with news content to capture the difference between true and false\nrumors. Although the user's stance is effective for rumor detection, the manual\nlabeling process is time-consuming and labor-intensive, which limits the\napplication of utilizing it to facilitate rumor detection.\n  In this paper, we first finetune a pre-trained BERT model on a small labeled\ndataset and leverage this model to annotate weak stance labels for users'\ncomment data to overcome the problem mentioned above. Then, we propose a novel\nStance-aware Reinforcement Learning Framework (SRLF) to select high-quality\nlabeled stance data for model training and rumor detection. Both the stance\nselection and rumor detection tasks are optimized simultaneously to promote\nboth tasks mutually. We conduct experiments on two commonly used real-world\ndatasets. The experimental results demonstrate that our framework outperforms\nthe state-of-the-art models significantly, which confirms the effectiveness of\nthe proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:58:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yuan", "Chunyuan", ""], ["Qian", "Wanhui", ""], ["Ma", "Qianwen", ""], ["Zhou", "Wei", ""], ["Hu", "Songlin", ""]]}, {"id": "2105.04117", "submitter": "KayYen Wong", "authors": "KayYen Wong, Miriam Redi, Diego Saez-Trumper", "title": "Wiki-Reliability: A Large Scale Dataset for Content Reliability on\n  Wikipedia", "comments": "Proceedings of the 44th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '21), 2021", "journal-ref": null, "doi": "10.1145/3404835.3463253", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wikipedia is the largest online encyclopedia, used by algorithms and web\nusers as a central hub of reliable information on the web. The quality and\nreliability of Wikipedia content is maintained by a community of volunteer\neditors. Machine learning and information retrieval algorithms could help scale\nup editors' manual efforts around Wikipedia content reliability. However, there\nis a lack of large-scale data to support the development of such research. To\nfill this gap, in this paper, we propose Wiki-Reliability, the first dataset of\nEnglish Wikipedia articles annotated with a wide set of content reliability\nissues. To build this dataset, we rely on Wikipedia \"templates\". Templates are\ntags used by expert Wikipedia editors to indicate content issues, such as the\npresence of \"non-neutral point of view\" or \"contradictory articles\", and serve\nas a strong signal for detecting reliability issues in a revision. We select\nthe 10 most popular reliability-related templates on Wikipedia, and propose an\neffective method to label almost 1M samples of Wikipedia article revisions as\npositive or negative with respect to each template. Each positive/negative\nexample in the dataset comes with the full article text and 20 features from\nthe revision's metadata. We provide an overview of the possible downstream\ntasks enabled by such data, and show that Wiki-Reliability can be used to train\nlarge-scale models for content reliability prediction. We release all data and\ncode for public use.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 05:07:03 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 11:57:14 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wong", "KayYen", ""], ["Redi", "Miriam", ""], ["Saez-Trumper", "Diego", ""]]}, {"id": "2105.04126", "submitter": "Yiming Cui", "authors": "Yiming Cui, Ting Liu, Wanxiang Che, Zhigang Chen, Shijin Wang", "title": "ExpMRC: Explainability Evaluation for Machine Reading Comprehension", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Achieving human-level performance on some of Machine Reading Comprehension\n(MRC) datasets is no longer challenging with the help of powerful Pre-trained\nLanguage Models (PLMs). However, it is necessary to provide both answer\nprediction and its explanation to further improve the MRC system's reliability,\nespecially for real-life applications. In this paper, we propose a new\nbenchmark called ExpMRC for evaluating the explainability of the MRC systems.\nExpMRC contains four subsets, including SQuAD, CMRC 2018, RACE$^+$, and C$^3$\nwith additional annotations of the answer's evidence. The MRC systems are\nrequired to give not only the correct answer but also its explanation. We use\nstate-of-the-art pre-trained language models to build baseline systems and\nadopt various unsupervised approaches to extract evidence without a\nhuman-annotated training set. The experimental results show that these models\nare still far from human performance, suggesting that the ExpMRC is\nchallenging. Resources will be available through\nhttps://github.com/ymcui/expmrc\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:00:20 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Che", "Wanxiang", ""], ["Chen", "Zhigang", ""], ["Wang", "Shijin", ""]]}, {"id": "2105.04165", "submitter": "Pan Lu", "authors": "Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan\n  Liang, Song-Chun Zhu", "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language\n  and Symbolic Reasoning", "comments": "Accepted to ACL 2021, 13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geometry problem solving has attracted much attention in the NLP community\nrecently. The task is challenging as it requires abstract problem understanding\nand symbolic reasoning with axiomatic knowledge. However, current datasets are\neither small in scale or not publicly available. Thus, we construct a new\nlarge-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with\ndense annotation in formal language. We further propose a novel geometry\nsolving approach with formal language and symbolic reasoning, called\nInterpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the\nproblem text and diagram into formal language automatically via rule-based text\nparsing and neural object detecting, respectively. Unlike implicit learning in\nexisting methods, Inter-GPS incorporates theorem knowledge as conditional rules\nand performs symbolic reasoning step by step. Also, a theorem predictor is\ndesigned to infer the theorem application sequence fed to the symbolic solver\nfor the more efficient and reasonable searching path. Extensive experiments on\nthe Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves\nsignificant improvements over existing methods. The project with code and data\nis available at https://lupantech.github.io/inter-gps.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:46:55 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:28:02 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 23:22:27 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lu", "Pan", ""], ["Gong", "Ran", ""], ["Jiang", "Shibiao", ""], ["Qiu", "Liang", ""], ["Huang", "Siyuan", ""], ["Liang", "Xiaodan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2105.04201", "submitter": "Fangkai Jiao", "authors": "Fangkai Jiao, Yangyang Guo, Yilin Niu, Feng Ji, Feng-Lin Li, Liqiang\n  Nie", "title": "REPT: Bridging Language Models and Machine Reading Comprehension via\n  Retrieval-Based Pre-training", "comments": "14 pages, 3 figures, Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained Language Models (PLMs) have achieved great success on Machine\nReading Comprehension (MRC) over the past few years. Although the general\nlanguage representation learned from large-scale corpora does benefit MRC, the\npoor support in evidence extraction which requires reasoning across multiple\nsentences hinders PLMs from further advancing MRC. To bridge the gap between\ngeneral PLMs and MRC, we present REPT, a REtrieval-based Pre-Training approach.\nIn particular, we introduce two self-supervised tasks to strengthen evidence\nextraction during pre-training, which is further inherited by downstream MRC\ntasks through the consistent retrieval operation and model architecture. To\nevaluate our proposed method, we conduct extensive experiments on five MRC\ndatasets that require collecting evidence from and reasoning across multiple\nsentences. Experimental results demonstrate the effectiveness of our\npre-training approach. Moreover, further analysis shows that our approach is\nable to enhance the capacity of evidence extraction without explicit\nsupervision.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:54:46 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 02:56:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jiao", "Fangkai", ""], ["Guo", "Yangyang", ""], ["Niu", "Yilin", ""], ["Ji", "Feng", ""], ["Li", "Feng-Lin", ""], ["Nie", "Liqiang", ""]]}, {"id": "2105.04221", "submitter": "Abdulkareem Alsudais", "authors": "Abdulkareem Alsudais, Wafa Alotaibi, Faye Alomary", "title": "Similarities between Arabic Dialects: Investigating Geographical\n  Proximity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic classification of Arabic dialects is an ongoing research\nchallenge, which has been explored in recent work that defines dialects based\non increasingly limited geographic areas like cities and provinces. This paper\nfocuses on a related yet relatively unexplored topic: the effects of the\ngeographical proximity of cities located in Arab countries on their dialectical\nsimilarity. Our work is twofold, reliant on: 1) comparing the textual\nsimilarities between dialects using cosine similarity and 2) measuring the\ngeographical distance between locations. We study MADAR and NADI, two\nestablished datasets with Arabic dialects from many cities and provinces. Our\nresults indicate that cities located in different countries may in fact have\nmore dialectical similarity than cities within the same country, depending on\ntheir geographical proximity. The correlation between dialectical similarity\nand city proximity suggests that cities that are closer together are more\nlikely to share dialectical attributes, regardless of country borders. This\nnuance provides the potential for important advancements in Arabic dialect\nresearch because it indicates that a more granular approach to dialect\nclassification is essential to understanding how to frame the problem of Arabic\ndialects identification.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:32:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Alsudais", "Abdulkareem", ""], ["Alotaibi", "Wafa", ""], ["Alomary", "Faye", ""]]}, {"id": "2105.04222", "submitter": "Zhaojiang Lin", "authors": "Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul Crook, Zhenpeng Zhou,\n  Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, Rajen Subba", "title": "Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State\n  Tracking", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot cross-domain dialogue state tracking (DST) enables us to handle\ntask-oriented dialogue in unseen domains without the expense of collecting\nin-domain data. In this paper, we propose a slot description enhanced\ngenerative approach for zero-shot cross-domain DST. Specifically, our model\nfirst encodes dialogue context and slots with a pre-trained self-attentive\nencoder, and generates slot values in an auto-regressive manner. In addition,\nwe incorporate Slot Type Informed Descriptions that capture the shared\ninformation across slots to facilitate cross-domain knowledge transfer.\nExperimental results on the MultiWOZ dataset show that our proposed method\nsignificantly improves existing state-of-the-art results in the zero-shot\ncross-domain setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:34:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lin", "Zhaojiang", ""], ["Liu", "Bing", ""], ["Moon", "Seungwhan", ""], ["Crook", "Paul", ""], ["Zhou", "Zhenpeng", ""], ["Wang", "Zhiguang", ""], ["Yu", "Zhou", ""], ["Madotto", "Andrea", ""], ["Cho", "Eunjoon", ""], ["Subba", "Rajen", ""]]}, {"id": "2105.04241", "submitter": "Yury Zemlyanskiy", "authors": "Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong, Philip Pham, Ilya\n  Eckstein, Fei Sha", "title": "ReadTwice: Reading Very Large Documents with Memories", "comments": "To appear in the proceedings of NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge-intensive tasks such as question answering often require\nassimilating information from different sections of large inputs such as books\nor article collections. We propose ReadTwice, a simple and effective technique\nthat combines several strengths of prior approaches to model long-range\ndependencies with Transformers. The main idea is to read text in small\nsegments, in parallel, summarizing each segment into a memory table to be used\nin a second read of the text. We show that the method outperforms models of\ncomparable size on several question answering (QA) datasets and sets a new\nstate of the art on the challenging NarrativeQA task, with questions about\nentire books. Source code and pre-trained checkpoints for ReadTwice can be\nfound at https://goo.gle/research-readtwice.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:13:09 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:07:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zemlyanskiy", "Yury", ""], ["Ainslie", "Joshua", ""], ["de Jong", "Michiel", ""], ["Pham", "Philip", ""], ["Eckstein", "Ilya", ""], ["Sha", "Fei", ""]]}, {"id": "2105.04271", "submitter": "Kuicai Dong", "authors": "Kuicai Dong, Yilin Zhao, Aixin Sun, Jung-Jae Kim, Xiaoli Li", "title": "DocOIE: A Document-level Context-Aware Dataset for OpenIE", "comments": "To appear in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Information Extraction (OpenIE) aims to extract structured relational\ntuples (subject, relation, object) from sentences and plays critical roles for\nmany downstream NLP applications. Existing solutions perform extraction at\nsentence level, without referring to any additional contextual information. In\nreality, however, a sentence typically exists as part of a document rather than\nstandalone; we often need to access relevant contextual information around the\nsentence before we can accurately interpret it. As there is no document-level\ncontext-aware OpenIE dataset available, we manually annotate 800 sentences from\n80 documents in two domains (Healthcare and Transportation) to form a DocOIE\ndataset for evaluation. In addition, we propose DocIE, a novel document-level\ncontext-aware OpenIE model. Our experimental results based on DocIE demonstrate\nthat incorporating document-level context is helpful in improving OpenIE\nperformance. Both DocOIE dataset and DocIE model are released for public.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:14:30 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 01:49:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dong", "Kuicai", ""], ["Zhao", "Yilin", ""], ["Sun", "Aixin", ""], ["Kim", "Jung-Jae", ""], ["Li", "Xiaoli", ""]]}, {"id": "2105.04295", "submitter": "Salvatore Vilella", "authors": "Alfonso Semeraro, Salvatore Vilella and Giancarlo Ruffo", "title": "PyPlutchik: visualising and comparing emotion-annotated corpora", "comments": "18 pages, 13 figures. Submitted to IEEE for possible publication;\n  copyright may change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing availability of textual corpora and data fetched from social\nnetworks is fuelling a huge production of works based on the model proposed by\npsychologist Robert Plutchik, often referred simply as the ``Plutchik Wheel''.\nRelated researches range from annotation tasks description to emotions\ndetection tools. Visualisation of such emotions is traditionally carried out\nusing the most popular layouts, as bar plots or tables, which are however\nsub-optimal. The classic representation of the Plutchik's wheel follows the\nprinciples of proximity and opposition between pairs of emotions: spatial\nproximity in this model is also a semantic proximity, as adjacent emotions\nelicit a complex emotion (a primary dyad) when triggered together; spatial\nopposition is a semantic opposition as well, as positive emotions are opposite\nto negative emotions. The most common layouts fail to preserve both features,\nnot to mention the need of visually allowing comparisons between different\ncorpora in a blink of an eye, that is hard with basic design solutions. We\nintroduce PyPlutchik, a Python library specifically designed for the\nvisualisation of Plutchik's emotions in texts or in corpora. PyPlutchik draws\nthe Plutchik's flower with each emotion petal sized after how much that emotion\nis detected or annotated in the corpus, also representing three degrees of\nintensity for each of them. Notably, PyPlutchik allows users to display also\nprimary, secondary, tertiary and opposite dyads in a compact, intuitive way. We\nsubstantiate our claim that PyPlutchik outperforms other classic visualisations\nwhen displaying Plutchik emotions and we showcase a few examples that display\nour library's most compelling features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:34:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Semeraro", "Alfonso", ""], ["Vilella", "Salvatore", ""], ["Ruffo", "Giancarlo", ""]]}, {"id": "2105.04339", "submitter": "Hayato Tsukagoshi", "authors": "Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda", "title": "DefSent: Sentence Embeddings using Definition Sentences", "comments": "Accepted at ACL-IJCNLP 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence embedding methods using natural language inference (NLI) datasets\nhave been successfully applied to various tasks. However, these methods are\nonly available for limited languages due to relying heavily on the large NLI\ndatasets. In this paper, we propose DefSent, a sentence embedding method that\nuses definition sentences from a word dictionary, which performs comparably on\nunsupervised semantics textual similarity (STS) tasks and slightly better on\nSentEval tasks than conventional methods. Since dictionaries are available for\nmany languages, DefSent is more broadly applicable than methods using NLI\ndatasets without constructing additional datasets. We demonstrate that DefSent\nperforms comparably on unsupervised semantics textual similarity (STS) tasks\nand slightly better on SentEval tasks to the methods using large NLI datasets.\nOur code is publicly available at https://github.com/hpprc/defsent .\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:13:39 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:45:57 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 14:32:12 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tsukagoshi", "Hayato", ""], ["Sasano", "Ryohei", ""], ["Takeda", "Koichi", ""]]}, {"id": "2105.04371", "submitter": "Hang Zhang", "authors": "Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan\n  Duan, Weizhu Chen", "title": "Poolingformer: Long Document Modeling with Pooling Attention", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a two-level attention schema, Poolingformer, for\nlong document modeling. Its first level uses a smaller sliding window pattern\nto aggregate information from neighbors. Its second level employs a larger\nwindow to increase receptive fields with pooling attention to reduce both\ncomputational cost and memory consumption. We first evaluate Poolingformer on\ntwo long sequence QA tasks: the monolingual NQ and the multilingual TyDi QA.\nExperimental results show that Poolingformer sits atop three official\nleaderboards measured by F1, outperforming previous state-of-the-art models by\n1.9 points (79.8 vs. 77.9) on NQ long answer, 1.9 points (79.5 vs. 77.6) on\nTyDi QA passage answer, and 1.6 points (67.6 vs. 66.0) on TyDi QA minimal\nanswer. We further evaluate Poolingformer on a long sequence summarization\ntask. Experimental results on the arXiv benchmark continue to demonstrate its\nsuperior performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:53:08 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Hang", ""], ["Gong", "Yeyun", ""], ["Shen", "Yelong", ""], ["Li", "Weisheng", ""], ["Lv", "Jiancheng", ""], ["Duan", "Nan", ""], ["Chen", "Weizhu", ""]]}, {"id": "2105.04387", "submitter": "Jinjie Ni", "authors": "Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, Vinay Adiga, Erik\n  Cambria", "title": "Recent Advances in Deep Learning Based Dialogue Systems: A Systematic\n  Survey", "comments": "76 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue systems are a popular Natural Language Processing (NLP) task as it\nis promising in real-life applications. It is also a complicated task since\nmany NLP tasks deserving study are involved. As a result, a multitude of novel\nworks on this task are carried out, and most of them are deep learning-based\ndue to the outstanding performance. In this survey, we mainly focus on the deep\nlearning-based dialogue systems. We comprehensively review state-of-the-art\nresearch outcomes in dialogue systems and analyze them from two angles: model\ntype and system type. Specifically, from the angle of model type, we discuss\nthe principles, characteristics, and applications of different models that are\nwidely used in dialogue systems. This will help researchers acquaint these\nmodels and see how they are applied in state-of-the-art frameworks, which is\nrather helpful when designing a new dialogue system. From the angle of system\ntype, we discuss task-oriented and open-domain dialogue systems as two streams\nof research, providing insight into the hot topics related. Furthermore, we\ncomprehensively review the evaluation methods and datasets for dialogue systems\nto pave the way for future research. Finally, some possible research trends are\nidentified based on the recent research outcomes. To the best of our knowledge,\nthis survey is the most comprehensive and up-to-date one at present in the area\nof dialogue systems and dialogue-related tasks, extensively covering the\npopular frameworks, topics, and datasets.\n  Keywords: Dialogue Systems, Chatbots, Conversational AI, Task-oriented, Open\nDomain, Chit-chat, Question Answering, Artificial Intelligence, Natural\nLanguage Processing, Information Retrieval, Deep Learning, Neural Networks,\nCNN, RNN, Hierarchical Recurrent Encoder-Decoder, Memory Networks, Attention,\nTransformer, Pointer Net, CopyNet, Reinforcement Learning, GANs, Knowledge\nGraph, Survey, Review\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:07:49 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 13:45:12 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 04:23:43 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 06:12:48 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ni", "Jinjie", ""], ["Young", "Tom", ""], ["Pandelea", "Vlad", ""], ["Xue", "Fuzhao", ""], ["Adiga", "Vinay", ""], ["Cambria", "Erik", ""]]}, {"id": "2105.04443", "submitter": "Zhenghao Liu PhD.", "authors": "Zhenghao Liu, Xiaoyuan Yi, Maosong Sun, Liner Yang and Tat-Seng Chua", "title": "Neural Quality Estimation with Multiple Hypotheses for Grammatical Error\n  Correction", "comments": "Accepted by NAACL2021, 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical Error Correction (GEC) aims to correct writing errors and help\nlanguage learners improve their writing skills. However, existing GEC models\ntend to produce spurious corrections or fail to detect lots of errors. The\nquality estimation model is necessary to ensure learners get accurate GEC\nresults and avoid misleading from poorly corrected sentences. Well-trained GEC\nmodels can generate several high-quality hypotheses through decoding, such as\nbeam search, which provide valuable GEC evidence and can be used to evaluate\nGEC quality. However, existing models neglect the possible GEC evidence from\ndifferent hypotheses. This paper presents the Neural Verification Network\n(VERNet) for GEC quality estimation with multiple hypotheses. VERNet\nestablishes interactions among hypotheses with a reasoning graph and conducts\ntwo kinds of attention mechanisms to propagate GEC evidence to verify the\nquality of generated hypotheses. Our experiments on four GEC datasets show that\nVERNet achieves state-of-the-art grammatical error detection performance,\nachieves the best quality estimation results, and significantly improves GEC\nperformance by reranking hypotheses. All data and source codes are available at\nhttps://github.com/thunlp/VERNet.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:04:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Zhenghao", ""], ["Yi", "Xiaoyuan", ""], ["Sun", "Maosong", ""], ["Yang", "Liner", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2105.04458", "submitter": "Shakti Kumar", "authors": "Shakti Kumar, Jithin Pradeep, Hussain Zaidi", "title": "Learning Robust Latent Representations for Controllable Speech Synthesis", "comments": "Accepted in ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  State-of-the-art Variational Auto-Encoders (VAEs) for learning disentangled\nlatent representations give impressive results in discovering features like\npitch, pause duration, and accent in speech data, leading to highly\ncontrollable text-to-speech (TTS) synthesis. However, these LSTM-based VAEs\nfail to learn latent clusters of speaker attributes when trained on either\nlimited or noisy datasets. Further, different latent variables start encoding\nthe same features, limiting the control and expressiveness during speech\nsynthesis. To resolve these issues, we propose RTI-VAE (Reordered Transformer\nwith Information reduction VAE) where we minimize the mutual information\nbetween different latent variables and devise a modified Transformer\narchitecture with layer reordering to learn controllable latent representations\nin speech data. We show that RTI-VAE reduces the cluster overlap of speaker\nattributes by at least 30\\% over LSTM-VAE and by at least 7\\% over vanilla\nTransformer-VAE.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:49:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kumar", "Shakti", ""], ["Pradeep", "Jithin", ""], ["Zaidi", "Hussain", ""]]}, {"id": "2105.04475", "submitter": "Liang Ding", "authors": "Lei Zhou, Liang Ding, Kevin Duh, Shinji Watanabe, Ryohei Sasano,\n  Koichi Takeda", "title": "Self-Guided Curriculum Learning for Neural Machine Translation", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the field of machine learning, the well-trained model is assumed to be\nable to recover the training labels, i.e. the synthetic labels predicted by the\nmodel should be as close to the ground-truth labels as possible. Inspired by\nthis, we propose a self-guided curriculum strategy to encourage the learning of\nneural machine translation (NMT) models to follow the above recovery criterion,\nwhere we cast the recovery degree of each training example as its learning\ndifficulty. Specifically, we adopt the sentence level BLEU score as the proxy\nof recovery degree. Different from existing curricula relying on linguistic\nprior knowledge or third-party language models, our chosen learning difficulty\nis more suitable to measure the degree of knowledge mastery of the NMT models.\nExperiments on translation benchmarks, including WMT14\nEnglish$\\Rightarrow$German and WMT17 Chinese$\\Rightarrow$English, demonstrate\nthat our approach can consistently improve translation performance against\nstrong baseline Transformer.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:12:14 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 10:20:12 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhou", "Lei", ""], ["Ding", "Liang", ""], ["Duh", "Kevin", ""], ["Watanabe", "Shinji", ""], ["Sasano", "Ryohei", ""], ["Takeda", "Koichi", ""]]}, {"id": "2105.04489", "submitter": "SouYoung Jin", "authors": "Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio\n  Feris, James Glass, Aude Oliva", "title": "Spoken Moments: Learning Joint Audio-Visual Representations from Video\n  Descriptions", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people observe events, they are able to abstract key information and\nbuild concise summaries of what is happening. These summaries include\ncontextual and semantic information describing the important high-level details\n(what, where, who and how) of the observed event and exclude background\ninformation that is deemed unimportant to the observer. With this in mind, the\ndescriptions people generate for videos of different dynamic events can greatly\nimprove our understanding of the key information of interest in each video.\nThese descriptions can be captured in captions that provide expanded attributes\nfor video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing\nus to gain new insight into what people find important or necessary to\nsummarize specific events. Existing caption datasets for video understanding\nare either small in scale or restricted to a specific domain. To address this,\nwe present the Spoken Moments (S-MiT) dataset of 500k spoken captions each\nattributed to a unique short video depicting a broad range of different events.\nWe collect our descriptions using audio recordings to ensure that they remain\nas natural and concise as possible while allowing us to scale the size of a\nlarge classification dataset. In order to utilize our proposed dataset, we\npresent a novel Adaptive Mean Margin (AMM) approach to contrastive learning and\nevaluate our models on video/caption retrieval on multiple datasets. We show\nthat our AMM approach consistently improves our results and that models trained\non our Spoken Moments dataset generalize better than those trained on other\nvideo-caption datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:30:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Monfort", "Mathew", ""], ["Jin", "SouYoung", ""], ["Liu", "Alexander", ""], ["Harwath", "David", ""], ["Feris", "Rogerio", ""], ["Glass", "James", ""], ["Oliva", "Aude", ""]]}, {"id": "2105.04512", "submitter": "Gerard I. G\\'allego", "authors": "Gerard I. G\\'allego, Ioannis Tsiamas, Carlos Escolano, Jos\\'e A. R.\n  Fonollosa, Marta R. Costa-juss\\`a", "title": "End-to-End Speech Translation with Pre-trained Models and Adapters: UPC\n  at IWSLT 2021", "comments": "Submitted to IWSLT 2021; changed the title and added submission\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the submission to the IWSLT 2021 offline speech\ntranslation task by the UPC Machine Translation group. The task consists of\nbuilding a system capable of translating English audio recordings extracted\nfrom TED talks into German text. Submitted systems can be either cascade or\nend-to-end and use a custom or given segmentation. Our submission is an\nend-to-end speech translation system, which combines pre-trained models\n(Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder,\nand uses an efficient fine-tuning technique, which trains only 20% of its total\nparameters. We show that adding an Adapter to the system and pre-training it,\ncan increase the convergence speed and the final result, with which we achieve\na BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble\nthat obtains 28.22 BLEU score on the same set. Our submission also uses a\ncustom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for\nidentifying periods of untranscribable text and can bring improvements of 2.5\nto 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the\ngiven segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:04:11 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:26:41 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["G\u00e1llego", "Gerard I.", ""], ["Tsiamas", "Ioannis", ""], ["Escolano", "Carlos", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""], ["Costa-juss\u00e0", "Marta R.", ""]]}, {"id": "2105.04616", "submitter": "Yingxue Fu", "authors": "Yingxue Fu, Mark-Jan Nederhof", "title": "Automatic Classification of Human Translation and Machine Translation: A\n  Study from the Perspective of Lexical Diversity", "comments": "accepted by MoTra21, Nodalida 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By using a trigram model and fine-tuning a pretrained BERT model for sequence\nclassification, we show that machine translation and human translation can be\nclassified with an accuracy above chance level, which suggests that machine\ntranslation and human translation are different in a systematic way. The\nclassification accuracy of machine translation is much higher than of human\ntranslation. We show that this may be explained by the difference in lexical\ndiversity between machine translation and human translation. If machine\ntranslation has independent patterns from human translation, automatic metrics\nwhich measure the deviation of machine translation from human translation may\nconflate difference with quality. Our experiment with two different types of\nautomatic metrics shows correlation with the result of the classification task.\nTherefore, we suggest the difference in lexical diversity between machine\ntranslation and human translation be given more attention in machine\ntranslation evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:55:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fu", "Yingxue", ""], ["Nederhof", "Mark-Jan", ""]]}, {"id": "2105.04623", "submitter": "Feng Nan", "authors": "Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng,\n  Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O.\n  Arnold, Bing Xiang", "title": "Improving Factual Consistency of Abstractive Summarization via Question\n  Answering", "comments": "ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A commonly observed problem with the state-of-the art abstractive\nsummarization models is that the generated summaries can be factually\ninconsistent with the input documents. The fact that automatic summarization\nmay produce plausible-sounding yet inaccurate summaries is a major concern that\nlimits its wide application. In this paper we present an approach to address\nfactual consistency in summarization. We first propose an efficient automatic\nevaluation metric to measure factual consistency; next, we propose a novel\nlearning algorithm that maximizes the proposed metric during model training.\nThrough extensive experiments, we confirm that our method is effective in\nimproving factual consistency and even overall quality of the summaries, as\njudged by both automatic metrics and human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:07:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Nan", "Feng", ""], ["Santos", "Cicero Nogueira dos", ""], ["Zhu", "Henghui", ""], ["Ng", "Patrick", ""], ["McKeown", "Kathleen", ""], ["Nallapati", "Ramesh", ""], ["Zhang", "Dejiao", ""], ["Wang", "Zhiguo", ""], ["Arnold", "Andrew O.", ""], ["Xiang", "Bing", ""]]}, {"id": "2105.04631", "submitter": "Fatemeh Kaveh-Yazdy", "authors": "Fatemeh Kaveh-Yazdy, Sajjad Zarifzadeh", "title": "Measuring Economic Policy Uncertainty Using an Unsupervised Word\n  Embedding-based Method", "comments": "21 Pages, 8 Figures, 2 Tables, This article aimed at computing\n  economic policy uncertainty of Iran for the first time using text mining\n  techniques", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Economic Policy Uncertainty (EPU) is a critical indicator in economic\nstudies, while it can be used to forecast a recession. Under higher levels of\nuncertainty, firms' owners cut their investment, which leads to a longer\npost-recession recovery. EPU index is computed by counting news articles\ncontaining pre-defined keywords related to policy-making and economy and convey\nuncertainty. Unfortunately, this method is sensitive to the original keyword\nset, its richness, and the news coverage. Thus, reproducing its results for\ndifferent countries is challenging. In this paper, we propose an unsupervised\ntext mining method that uses word-embedding representation space to select\nrelevant keywords. This method is not strictly sensitive to the semantic\nsimilarity threshold applied to the word embedding vectors and does not require\na pre-defined dictionary. Our experiments using a massive repository of Persian\nnews show that the EPU series computed by the proposed method precisely follows\nmajor events affecting Iran's economy and is compatible with the World\nUncertainty Index (WUI) of Iran.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:34:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kaveh-Yazdy", "Fatemeh", ""], ["Zarifzadeh", "Sajjad", ""]]}, {"id": "2105.04633", "submitter": "Casey Kennington", "authors": "Casey Kennington", "title": "Language Acquisition is Embodied, Interactive, Emotive: a Research\n  Proposal", "comments": "6 pages, ICLR 2021 Embodied Multimodal Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans' experience of the world is profoundly multimodal from the beginning,\nso why do existing state-of-the-art language models only use text as a modality\nto learn and represent semantic meaning? In this paper we review the literature\non the role of embodiment and emotion in the interactive setting of spoken\ndialogue as necessary prerequisites for language learning for human children,\nincluding how words in child vocabularies are largely concrete, then shift to\nbecome more abstract as the children get older. We sketch a model of semantics\nthat leverages current transformer-based models and a word-level grounded\nmodel, then explain the robot-dialogue system that will make use of our\nsemantic model, the setting for the system to learn language, and existing\nbenchmarks for evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:40:17 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kennington", "Casey", ""]]}, {"id": "2105.04645", "submitter": "Aryan Arbabi", "authors": "Aryan Arbabi, Mingqiu Wang, Laurent El Shafey, Nan Du, Izhak Shafran", "title": "R2D2: Relational Text Decoding with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for modeling the interaction between graphical\nstructures and the natural language text associated with their nodes and edges.\nExisting approaches typically fall into two categories. On group ignores the\nrelational structure by converting them into linear sequences and then utilize\nthe highly successful Seq2Seq models. The other side ignores the sequential\nnature of the text by representing them as fixed-dimensional vectors and apply\ngraph neural networks. Both simplifications lead to information loss.\n  Our proposed method utilizes both the graphical structure as well as the\nsequential nature of the texts. The input to our model is a set of text\nsegments associated with the nodes and edges of the graph, which are then\nprocessed with a transformer encoder-decoder model, equipped with a\nself-attention mechanism that is aware of the graphical relations between the\nnodes containing the segments. This also allows us to use BERT-like models that\nare already trained on large amounts of text.\n  While the proposed model has wide applications, we demonstrate its\ncapabilities on data-to-text generation tasks. Our approach compares favorably\nagainst state-of-the-art methods in four tasks without tailoring the model\narchitecture. We also provide an early demonstration in a novel practical\napplication -- generating clinical notes from the medical entities mentioned\nduring clinical visits.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:59:11 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Arbabi", "Aryan", ""], ["Wang", "Mingqiu", ""], ["Shafey", "Laurent El", ""], ["Du", "Nan", ""], ["Shafran", "Izhak", ""]]}, {"id": "2105.04650", "submitter": "Zilong Wang", "authors": "Zilong Wang, Mingjie Zhan, Houxing Ren, Zhaohui Hou, Yuwei Wu, Xingyan\n  Zhang, Ding Liang", "title": "GroupLink: An End-to-end Multitask Method for Word Grouping and Relation\n  Extraction in Form Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forms are a common type of document in real life and carry rich information\nthrough textual contents and the organizational structure. To realize automatic\nprocessing of forms, word grouping and relation extraction are two fundamental\nand crucial steps after preliminary processing of optical character reader\n(OCR). Word grouping is to aggregate words that belong to the same semantic\nentity, and relation extraction is to predict the links between semantic\nentities. Existing works treat them as two individual tasks, but these two\ntasks are correlated and can reinforce each other. The grouping process will\nrefine the integrated representation of the corresponding entity, and the\nlinking process will give feedback to the grouping performance. For this\npurpose, we acquire multimodal features from both textual data and layout\ninformation and build an end-to-end model through multitask training to combine\nword grouping and relation extraction to enhance performance on each task. We\nvalidate our proposed method on a real-world, fully-annotated, noisy-scanned\nbenchmark, FUNSD, and extensive experiments demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:15:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Zilong", ""], ["Zhan", "Mingjie", ""], ["Ren", "Houxing", ""], ["Hou", "Zhaohui", ""], ["Wu", "Yuwei", ""], ["Zhang", "Xingyan", ""], ["Liang", "Ding", ""]]}, {"id": "2105.04674", "submitter": "Francis Tyers", "authors": "Francis M. Tyers and Josh Meyer", "title": "What shall we do with an hour of data? Speech recognition for the un-\n  and under-served languages of Common Voice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This technical report describes the methods and results of a three-week\nsprint to produce deployable speech recognition models for 31 under-served\nlanguages of the Common Voice project. We outline the preprocessing steps,\nhyperparameter selection, and resulting accuracy on official testing sets. In\naddition to this we evaluate the models on multiple tasks: closed-vocabulary\nspeech recognition, pre-transcription, forced alignment, and key-word spotting.\nThe following experiments use Coqui STT, a toolkit for training and deployment\nof neural Speech-to-Text models.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:16:28 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Tyers", "Francis M.", ""], ["Meyer", "Josh", ""]]}, {"id": "2105.04688", "submitter": "Laura P\\'erez-Mayos", "authors": "Laura P\\'erez-Mayos, Alba T\\'aboas Garc\\'ia, Simon Mille, Leo Wanner", "title": "Assessing the Syntactic Capabilities of Transformer-based Multilingual\n  Language Models", "comments": "To be published in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual Transformer-based language models, usually pretrained on more\nthan 100 languages, have been shown to achieve outstanding results in a wide\nrange of cross-lingual transfer tasks. However, it remains unknown whether the\noptimization for different languages conditions the capacity of the models to\ngeneralize over syntactic structures, and how languages with syntactic\nphenomena of different complexity are affected. In this work, we explore the\nsyntactic generalization capabilities of the monolingual and multilingual\nversions of BERT and RoBERTa. More specifically, we evaluate the syntactic\ngeneralization potential of the models on English and Spanish tests, comparing\nthe syntactic abilities of monolingual and multilingual models on the same\nlanguage (English), and of multilingual models on two different languages\n(English and Spanish). For English, we use the available SyntaxGym test suite;\nfor Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic\ntests in Spanish, designed to evaluate the syntactic generalization\ncapabilities of language models through the SyntaxGym online platform.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 22:06:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["P\u00e9rez-Mayos", "Laura", ""], ["Garc\u00eda", "Alba T\u00e1boas", ""], ["Mille", "Simon", ""], ["Wanner", "Leo", ""]]}, {"id": "2105.04708", "submitter": "Bimal Bhattarai", "authors": "Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao", "title": "Word-level Human Interpretable Scoring Mechanism for Novel Text\n  Detection Using Tsetlin Machines", "comments": "18 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in novelty detection focuses mainly on document-level\nclassification, employing deep neural networks (DNN). However, the black-box\nnature of DNNs makes it difficult to extract an exact explanation of why a\ndocument is considered novel. In addition, dealing with novelty at the\nword-level is crucial to provide a more fine-grained analysis than what is\navailable at the document level. In this work, we propose a Tsetlin machine\n(TM)-based architecture for scoring individual words according to their\ncontribution to novelty. Our approach encodes a description of the novel\ndocuments using the linguistic patterns captured by TM clauses. We then adopt\nthis description to measure how much a word contributes to making documents\nnovel. Our experimental results demonstrate how our approach breaks down\nnovelty into interpretable phrases, successfully measuring novelty.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 23:41:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bhattarai", "Bimal", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2105.04719", "submitter": "Pengwei Wang", "authors": "Pengwei Wang, Xin Ye, Xiaohuan Zhou, Jinghui Xie, Hao Wang", "title": "Speech2Slot: An End-to-End Knowledge-based Slot Filling from Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to conventional pipeline Spoken Language Understanding (SLU)\nwhich consists of automatic speech recognition (ASR) and natural language\nunderstanding (NLU), end-to-end SLU infers the semantic meaning directly from\nspeech and overcomes the error propagation caused by ASR. End-to-end slot\nfilling (SF) from speech is an essential component of end-to-end SLU, and is\nusually regarded as a sequence-to-sequence generation problem, heavily relied\non the performance of language model of ASR. However, it is hard to generate a\ncorrect slot when the slot is out-of-vovabulary (OOV) in training data,\nespecially when a slot is an anti-linguistic entity without grammatical rule.\nInspired by object detection in computer vision that is to detect the object\nfrom an image, we consider SF as the task of slot detection from speech. In\nthis paper, we formulate the SF task as a matching task and propose an\nend-to-end knowledge-based SF model, named Speech-to-Slot (Speech2Slot), to\nleverage knowledge to detect the boundary of a slot from the speech. We also\nrelease a large-scale dataset of Chinese speech for slot filling, containing\nmore than 830,000 samples. The experiments show that our approach is markedly\nsuperior to the conventional pipeline SLU approach, and outperforms the\nstate-of-the-art end-to-end SF approach with 12.51% accuracy improvement.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:31:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Pengwei", ""], ["Ye", "Xin", ""], ["Zhou", "Xiaohuan", ""], ["Xie", "Jinghui", ""], ["Wang", "Hao", ""]]}, {"id": "2105.04727", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Jonah Casebeer, Zhepei Wang, Paris Smaragdis", "title": "Separate but Together: Unsupervised Federated Learning for Speech\n  Enhancement from Non-IID Data", "comments": "Accepted to WASPAA 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:47:18 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 23:11:12 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Casebeer", "Jonah", ""], ["Wang", "Zhepei", ""], ["Smaragdis", "Paris", ""]]}, {"id": "2105.04774", "submitter": "Xuhui Ren", "authors": "Xuhui Ren, Hongzhi Yin, Tong Chen, Hao Wang, Zi Huang, Kai Zheng", "title": "Learning to Ask Appropriate Questions in Conversational Recommendation", "comments": "to be published in SIGIR'2021", "journal-ref": null, "doi": "10.1145/3404835.3462839", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational recommender systems (CRSs) have revolutionized the\nconventional recommendation paradigm by embracing dialogue agents to\ndynamically capture the fine-grained user preference. In a typical\nconversational recommendation scenario, a CRS firstly generates questions to\nlet the user clarify her/his demands and then makes suitable recommendations.\nHence, the ability to generate suitable clarifying questions is the key to\ntimely tracing users' dynamic preferences and achieving successful\nrecommendations. However, existing CRSs fall short in asking high-quality\nquestions because: (1) system-generated responses heavily depends on the\nperformance of the dialogue policy agent, which has to be trained with huge\nconversation corpus to cover all circumstances; and (2) current CRSs cannot\nfully utilize the learned latent user profiles for generating appropriate and\npersonalized responses.\n  To mitigate these issues, we propose the Knowledge-Based Question Generation\nSystem (KBQG), a novel framework for conversational recommendation. Distinct\nfrom previous conversational recommender systems, KBQG models a user's\npreference in a finer granularity by identifying the most relevant relations\nfrom a structured knowledge graph (KG). Conditioned on the varied importance of\ndifferent relations, the generated clarifying questions could perform better in\nimpelling users to provide more details on their preferences. Finially,\naccurate recommendations can be generated in fewer conversational turns.\nFurthermore, the proposed KBQG outperforms all baselines in our experiments on\ntwo real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:58:10 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ren", "Xuhui", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Wang", "Hao", ""], ["Huang", "Zi", ""], ["Zheng", "Kai", ""]]}, {"id": "2105.04779", "submitter": "Yu Yan", "authors": "Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan\n  Duan and Ruofei Zhang", "title": "EL-Attention: Memory Efficient Lossless Attention for Generation", "comments": "ICML 2021. Version 2: add pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer model with multi-head attention requires caching intermediate\nresults for efficient inference in generation tasks. However, cache brings new\nmemory-related costs and prevents leveraging larger batch size for faster\nspeed. We propose memory-efficient lossless attention (called EL-attention) to\naddress this issue. It avoids heavy operations for building multi-head keys and\nvalues, cache for them is not needed. EL-attention constructs an ensemble of\nattention results by expanding query while keeping key and value shared. It\nproduces the same result as multi-head attention with less GPU memory and\nfaster inference speed. We conduct extensive experiments on Transformer, BART,\nand GPT-2 for summarization and question generation tasks. The results show\nEL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 04:37:52 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 21:18:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yan", "Yu", ""], ["Chen", "Jiusheng", ""], ["Qi", "Weizhen", ""], ["Bhendawade", "Nikhil", ""], ["Gong", "Yeyun", ""], ["Duan", "Nan", ""], ["Zhang", "Ruofei", ""]]}, {"id": "2105.04780", "submitter": "Zixu Wang", "authors": "Zixu Wang, Yishu Miao, Lucia Specia", "title": "Cross-Modal Generative Augmentation for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation is an approach that can effectively improve the performance\nof multimodal machine learning. This paper introduces a generative model for\ndata augmentation by leveraging the correlations among multiple modalities.\nDifferent from conventional data augmentation approaches that apply low level\noperations with deterministic heuristics, our method proposes to learn an\naugmentation sampler that generates samples of the target modality conditioned\non observed modalities in the variational auto-encoder framework. Additionally,\nthe proposed model is able to quantify the confidence of augmented data by its\ngenerative probability, and can be jointly updated with a downstream pipeline.\nExperiments on Visual Question Answering tasks demonstrate the effectiveness of\nthe proposed generative model, which is able to boost the strong UpDn-based\nmodels to the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 04:51:26 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Zixu", ""], ["Miao", "Yishu", ""], ["Specia", "Lucia", ""]]}, {"id": "2105.04817", "submitter": "Mayuko Kori", "authors": "Mayuko Kori, Ichiro Hasuo, Shin-ya Katsumata", "title": "Fibrational Initial Algebra-Final Coalgebra Coincidence over Initial\n  Algebras: Turning Verification Witnesses Upside Down", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coincidence between initial algebras (IAs) and final coalgebras (FCs) is\na phenomenon that underpins various important results in theoretical computer\nscience. In this paper, we identify a general fibrational condition for the\nIA-FC coincidence, namely in the fiber over an initial algebra in the base\ncategory. Identifying (co)algebras in a fiber as (co)inductive predicates, our\nfibrational IA-FC coincidence allows one to use coinductive witnesses (such as\ninvariants) for verifying inductive properties (such as liveness). Our general\nfibrational theory features the technical condition of stability of chain\ncolimits; we extend the framework to the presence of a monadic effect, too,\nrestricting to fibrations of complete lattice-valued predicates. Practical\nbenefits of our categorical theory are exemplified by new \"upside-down\" witness\nnotions for three verification problems: probabilistic liveness, and acceptance\nand model-checking with respect to bottom-up tree automata.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:10:13 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:11:05 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kori", "Mayuko", ""], ["Hasuo", "Ichiro", ""], ["Katsumata", "Shin-ya", ""]]}, {"id": "2105.04837", "submitter": "Diego Antognini", "authors": "Diego Antognini and Boi Faltings", "title": "Rationalization through Concepts", "comments": "Accepted at ACL 2021 (findings). 15 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated predictions require explanations to be interpretable by humans. One\ntype of explanation is a rationale, i.e., a selection of input features such as\nrelevant text snippets from which the model computes the outcome. However, a\nsingle overall selection does not provide a complete explanation, e.g.,\nweighing several aspects for decisions. To this end, we present a novel\nself-interpretable model called ConRAT. Inspired by how human explanations for\nhigh-level decisions are often based on key concepts, ConRAT extracts a set of\ntext snippets as concepts and infers which ones are described in the document.\nThen, it explains the outcome with a linear aggregation of concepts. Two\nregularizers drive ConRAT to build interpretable concepts. In addition, we\npropose two techniques to boost the rationale and predictive performance\nfurther. Experiments on both single- and multi-aspect sentiment classification\ntasks show that ConRAT is the first to generate concepts that align with human\nrationalization while using only the overall label. Further, it outperforms\nstate-of-the-art methods trained on each aspect label independently.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:46:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Antognini", "Diego", ""], ["Faltings", "Boi", ""]]}, {"id": "2105.04840", "submitter": "Yung-Sung Chuang", "authors": "Shun-Po Chuang, Yung-Sung Chuang, Chih-Chiang Chang, Hung-yi Lee", "title": "Investigating the Reordering Capability in CTC-based Non-Autoregressive\n  End-to-End Speech Translation", "comments": "Accepted in Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the possibilities of building a non-autoregressive speech-to-text\ntranslation model using connectionist temporal classification (CTC), and use\nCTC-based automatic speech recognition as an auxiliary task to improve the\nperformance. CTC's success on translation is counter-intuitive due to its\nmonotonicity assumption, so we analyze its reordering capability. Kendall's tau\ndistance is introduced as the quantitative metric, and gradient-based\nvisualization provides an intuitive way to take a closer look into the model.\nOur analysis shows that transformer encoders have the ability to change the\nword order and points out the future research direction that worth being\nexplored more on non-autoregressive speech translation.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:48:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chuang", "Shun-Po", ""], ["Chuang", "Yung-Sung", ""], ["Chang", "Chih-Chiang", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2105.04846", "submitter": "Limsi Publications", "authors": "Jitao Xu (TLP), Fran\\c{c}ois Yvon (TLP)", "title": "Can You Traducir This? Machine Translation for Code-Switched Input", "comments": null, "journal-ref": "Workshop on Computational Approaches to Linguistic Code Switching,\n  Jun 2021, Online, United States", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-Switching (CSW) is a common phenomenon that occurs in multilingual\ngeographic or social contexts, which raises challenging problems for natural\nlanguage processing tools. We focus here on Machine Translation (MT) of CSW\ntexts, where we aim to simultaneously disentangle and translate the two mixed\nlanguages. Due to the lack of actual translated CSW data, we generate\nartificial training data from regular parallel texts. Experiments show this\ntraining strategy yields MT systems that surpass multilingual systems for\ncode-switched texts. These results are confirmed in an alternative task aimed\nat providing contextual translations for a L2 writing assistant.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:06:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Jitao", "", "TLP"], ["Yvon", "Fran\u00e7ois", "", "TLP"]]}, {"id": "2105.04850", "submitter": "Magdalena Kaiser", "authors": "Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum", "title": "Reinforcement Learning from Reformulations in Conversational Question\n  Answering over Knowledge Graphs", "comments": "SIGIR 2021 Long Paper, 11 pages", "journal-ref": null, "doi": "10.1145/3404835.3462859", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of personal assistants has made conversational question answering\n(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art\nmethods for ConvQA over knowledge graphs (KGs) can only learn from crisp\nquestion-answer pairs found in popular benchmarks. In reality, however, such\ntraining data is hard to come by: users would rarely mark answers explicitly as\ncorrect or wrong. In this work, we take a step towards a more natural learning\nparadigm - from noisy and implicit feedback via question reformulations. A\nreformulation is likely to be triggered by an incorrect system response,\nwhereas a new follow-up question could be a positive signal on the previous\nturn's answer. We present a reinforcement learning model, termed CONQUER, that\ncan learn from a conversational stream of questions and reformulations. CONQUER\nmodels the answering process as multiple agents walking in parallel on the KG,\nwhere the walks are determined by actions sampled using a policy network. This\npolicy network takes the question along with the conversational context as\ninputs and is trained via noisy rewards obtained from the reformulation\nlikelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark\nwith about 11k natural conversations containing around 205k reformulations.\nExperiments show that CONQUER successfully learns to answer conversational\nquestions from noisy reward signals, significantly improving over a\nstate-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:08:35 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kaiser", "Magdalena", ""], ["Roy", "Rishiraj Saha", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2105.04876", "submitter": "Matthias A{\\ss}enmacher", "authors": "M. A{\\ss}enmacher, P. Schulze, C. Heumann", "title": "Benchmarking down-scaled (not so large) pre-trained language models", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Transformer-based language models are pre-trained on corpora of varying\nsizes, for a different number of steps and with different batch sizes. At the\nsame time, more fundamental components, such as the pre-training objective or\narchitectural hyperparameters, are modified. In total, it is therefore\ndifficult to ascribe changes in performance to specific factors. Since\nsearching the hyperparameter space over the full systems is too costly, we\npre-train down-scaled versions of several popular Transformer-based\narchitectures on a common pre-training corpus and benchmark them on a subset of\nthe GLUE tasks (Wang et al., 2018). Specifically, we systematically compare\nthree pre-training objectives for different shape parameters and model sizes,\nwhile also varying the number of pre-training steps and the batch size. In our\nexperiments MLM + NSP (BERT-style) consistently outperforms MLM (RoBERTa-style)\nas well as the standard LM objective. Furthermore, we find that additional\ncompute should be mainly allocated to an increased model size, while training\nfor more steps is inefficient. Based on these observations, as a final step we\nattempt to scale up several systems using compound scaling (Tan and Le, 2019)\nadapted to Transformer-based language models.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:01:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["A\u00dfenmacher", "M.", ""], ["Schulze", "P.", ""], ["Heumann", "C.", ""]]}, {"id": "2105.04900", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon, F. Grippa, B. Guardabascio, F. Ravazzolo", "title": "Exploring the Antecedents of Consumer Confidence through Semantic\n  Network Analysis of Online News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CL cs.SI q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the impact of online news on social and economic\nconsumer perceptions through the application of semantic network analysis.\nUsing almost 1.3 million online articles on Italian media covering a period of\nfour years, we assessed the incremental predictive power of economic-related\nkeywords on the Consumer Confidence Index. We transformed news into networks of\nco-occurring words and calculated the semantic importance of specific keywords,\nto see if words appearing in the articles could anticipate consumers'\njudgements about the economic situation. Results show that economic-related\nkeywords have a stronger predictive power if we consider the current households\nand national situation, while their predictive power is less significant with\nregards to expectations about the future. Our indicator of semantic importance\noffers a complementary approach to estimate consumer confidence, lessening the\nlimitations of traditional survey-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:41:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Guardabascio", "B.", ""], ["Ravazzolo", "F.", ""]]}, {"id": "2105.04903", "submitter": "Faegheh Hasibi", "authors": "Hideaki Joko, Faegheh Hasibi, Krisztian Balog, Arjen P. de Vries", "title": "Conversational Entity Linking: Problem Definition and Datasets", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3463258", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine understanding of user utterances in conversational systems is of\nutmost importance for enabling engaging and meaningful conversations with\nusers. Entity Linking (EL) is one of the means of text understanding, with\nproven efficacy for various downstream tasks in information retrieval. In this\npaper, we study entity linking for conversational systems. To develop a better\nunderstanding of what EL in a conversational setting entails, we analyze a\nlarge number of dialogues from existing conversational datasets and annotate\nreferences to concepts, named entities, and personal entities using\ncrowdsourcing. Based on the annotated dialogues, we identify the main\ncharacteristics of conversational entity linking. Further, we report on the\nperformance of traditional EL systems on our Conversational Entity Linking\ndataset, ConEL, and present an extension to these methods to better fit the\nconversational setting. The resources released with this paper include\nannotated datasets, detailed descriptions of crowdsourcing setups, as well as\nthe annotations produced by various EL systems. These new resources allow for\nan investigation of how the role of entities in conversations is different from\nthat in documents or isolated short text utterances like queries and tweets,\nand complement existing conversational datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:44:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Joko", "Hideaki", ""], ["Hasibi", "Faegheh", ""], ["Balog", "Krisztian", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "2105.04913", "submitter": "Ananya Srivastava", "authors": "Ananya Srivastava, Mohammed Hasan, Bhargav Yagnik, Rahee Walambe and\n  Ketan Kotecha", "title": "Role of Artificial Intelligence in Detection of Hateful Speech for\n  Hinglish Data on Social Media", "comments": "This work was presented at ICAAIML2020 and will be published in\n  Lecture Notes in Electrical Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social networking platforms provide a conduit to disseminate our ideas, views\nand thoughts and proliferate information. This has led to the amalgamation of\nEnglish with natively spoken languages. Prevalence of Hindi-English code-mixed\ndata (Hinglish) is on the rise with most of the urban population all over the\nworld. Hate speech detection algorithms deployed by most social networking\nplatforms are unable to filter out offensive and abusive content posted in\nthese code-mixed languages. Thus, the worldwide hate speech detection rate of\naround 44% drops even more considering the content in Indian colloquial\nlanguages and slangs. In this paper, we propose a methodology for efficient\ndetection of unstructured code-mix Hinglish language. Fine-tuning based\napproaches for Hindi-English code-mixed language are employed by utilizing\ncontextual based embeddings such as ELMo (Embeddings for Language Models),\nFLAIR, and transformer-based BERT (Bidirectional Encoder Representations from\nTransformers). Our proposed approach is compared against the pre-existing\nmethods and results are compared for various datasets. Our model outperforms\nthe other methods and frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:02:28 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Srivastava", "Ananya", ""], ["Hasan", "Mohammed", ""], ["Yagnik", "Bhargav", ""], ["Walambe", "Rahee", ""], ["Kotecha", "Ketan", ""]]}, {"id": "2105.04949", "submitter": "Asahi Ushio", "authors": "Asahi Ushio and Luis Espinosa-Anke and Steven Schockaert and Jose\n  Camacho-Collados", "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models\n  Identify Analogies?", "comments": "Accepted by ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analogies play a central role in human commonsense reasoning. The ability to\nrecognize analogies such as \"eye is to seeing what ear is to hearing\",\nsometimes referred to as analogical proportions, shape how we structure\nknowledge and understand language. Surprisingly, however, the task of\nidentifying such analogies has not yet received much attention in the language\nmodel era. In this paper, we analyze the capabilities of transformer-based\nlanguage models on this unsupervised task, using benchmarks obtained from\neducational settings, as well as more commonly used datasets. We find that\noff-the-shelf language models can identify analogies to a certain extent, but\nstruggle with abstract and complex relations, and results are highly sensitive\nto model architecture and hyperparameters. Overall the best results were\nobtained with GPT-2 and RoBERTa, while configurations using BERT were not able\nto outperform word embedding models. Our results raise important questions for\nfuture work about how, and to what extent, pre-trained language models capture\nknowledge about abstract semantic relations.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:38:49 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:10:38 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 16:39:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ushio", "Asahi", ""], ["Espinosa-Anke", "Luis", ""], ["Schockaert", "Steven", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2105.04971", "submitter": "Mikhail Fain", "authors": "Mikhail Fain, Niall Twomey and Danushka Bollegala", "title": "Backretrieval: An Image-Pivoted Evaluation Metric for Cross-Lingual Text\n  Representations Without Parallel Corpora", "comments": "SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463027", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual text representations have gained popularity lately and act as\nthe backbone of many tasks such as unsupervised machine translation and\ncross-lingual information retrieval, to name a few. However, evaluation of such\nrepresentations is difficult in the domains beyond standard benchmarks due to\nthe necessity of obtaining domain-specific parallel language data across\ndifferent pairs of languages. In this paper, we propose an automatic metric for\nevaluating the quality of cross-lingual textual representations using images as\na proxy in a paired image-text evaluation dataset. Experimentally,\nBackretrieval is shown to highly correlate with ground truth metrics on\nannotated datasets, and our analysis shows statistically significant\nimprovements over baselines. Our experiments conclude with a case study on a\nrecipe dataset without parallel cross-lingual data. We illustrate how to judge\ncross-lingual embedding quality with Backretrieval, and validate the outcome\nwith a small human study.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:14:24 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fain", "Mikhail", ""], ["Twomey", "Niall", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2105.04976", "submitter": "Maya Raifer", "authors": "Maya Raifer, Guy Rotman, Reut Apel, Moshe Tennenholtz, Roi Reichart", "title": "Designing an Automatic Agent for Repeated Language based Persuasion\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Persuasion games are fundamental in economics and AI research and serve as\nthe basis for important applications. However, work on this setup assumes\ncommunication with stylized messages that do not consist of rich human\nlanguage. In this paper we consider a repeated sender (expert) -- receiver\n(decision maker) game, where the sender is fully informed about the state of\nthe world and aims to persuade the receiver to accept a deal by sending one of\nseveral possible natural language reviews. We design an automatic expert that\nplays this repeated game, aiming to achieve the maximal payoff. Our expert is\nimplemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep\nlearning models that exploit behavioral and linguistic signals in order to\npredict the next action of the decision maker, and the future payoff of the\nexpert given the state of the game and a candidate review. We demonstrate the\nsuperiority of our expert over strong baselines, its adaptability to different\ndecision makers, and that its selected reviews are nicely adapted to the\nproposed deal.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:25:57 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Raifer", "Maya", ""], ["Rotman", "Guy", ""], ["Apel", "Reut", ""], ["Tennenholtz", "Moshe", ""], ["Reichart", "Roi", ""]]}, {"id": "2105.05005", "submitter": "Jaeyoung Kim", "authors": "Jaeyoung Kim, Han Lu, Anshuman Tripathi, Qian Zhang and Hasim Sak", "title": "Reducing Streaming ASR Model Delay with Self Alignment", "comments": "submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reducing prediction delay for streaming end-to-end ASR models with minimal\nperformance regression is a challenging problem. Constrained alignment is a\nwell-known existing approach that penalizes predicted word boundaries using\nexternal low-latency acoustic models. On the contrary, recently proposed\nFastEmit is a sequence-level delay regularization scheme encouraging vocabulary\ntokens over blanks without any reference alignments. Although all these schemes\nare successful in reducing delay, ASR word error rate (WER) often severely\ndegrades after applying these delay constraining schemes. In this paper, we\npropose a novel delay constraining method, named self alignment. Self alignment\ndoes not require external alignment models. Instead, it utilizes Viterbi\nforced-alignments from the trained model to find the lower latency alignment\ndirection. From LibriSpeech evaluation, self alignment outperformed existing\nschemes: 25% and 56% less delay compared to FastEmit and constrained alignment\nat the similar word error rate. For Voice Search evaluation,12% and 25% delay\nreductions were achieved compared to FastEmit and constrained alignment with\nmore than 2% WER improvements.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 18:00:11 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kim", "Jaeyoung", ""], ["Lu", "Han", ""], ["Tripathi", "Anshuman", ""], ["Zhang", "Qian", ""], ["Sak", "Hasim", ""]]}, {"id": "2105.05020", "submitter": "Teresa Lynn", "authors": "Carla Parra Escart\\'in and Teresa Lynn and Joss Moorkens and Jane\n  Dunne", "title": "Towards transparency in NLP shared tasks", "comments": "38 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article reports on a survey carried out across the Natural Language\nProcessing (NLP) community. The survey aimed to capture the opinions of the\nresearch community on issues surrounding shared tasks, with respect to both\nparticipation and organisation. Amongst the 175 responses received, both\npositive and negative observations were made. We carried out and report on an\nextensive analysis of these responses, which leads us to propose a Shared Task\nOrganisation Checklist that could support future participants and organisers.\nThe proposed Checklist is flexible enough to accommodate the wide diversity of\nshared tasks in our field and its goal is not to be prescriptive, but rather to\nserve as a tool that encourages shared task organisers to foreground ethical\nbehaviour, beginning with the common issues that the 175 respondents deemed\nimportant. Its usage would not only serve as an instrument to reflect on\nimportant aspects of shared tasks, but would also promote increased\ntransparency around them.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:29:35 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Escart\u00edn", "Carla Parra", ""], ["Lynn", "Teresa", ""], ["Moorkens", "Joss", ""], ["Dunne", "Jane", ""]]}, {"id": "2105.05041", "submitter": "Guillermo C\\'ambara", "authors": "Guillermo C\\'ambara, Alex Peir\\'o-Lilja, Mireia Farr\\'us, Jordi Luque", "title": "English Accent Accuracy Analysis in a State-of-the-Art Automatic Speech\n  Recognition System", "comments": "2 pages, 1 figure, 1 table. To be published in Phonetics and\n  Phonology in Europe 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, research in speech technologies has gotten a lot out thanks to\nrecently created public domain corpora that contain thousands of recording\nhours. These large amounts of data are very helpful for training the new\ncomplex models based on deep learning technologies. However, the lack of\ndialectal diversity in a corpus is known to cause performance biases in speech\nsystems, mainly for underrepresented dialects. In this work, we propose to\nevaluate a state-of-the-art automatic speech recognition (ASR) deep\nlearning-based model, using unseen data from a corpus with a wide variety of\nlabeled English accents from different countries around the world. The model\nhas been trained with 44.5K hours of English speech from an open access corpus\ncalled Multilingual LibriSpeech, showing remarkable results in popular\nbenchmarks. We test the accuracy of such ASR against samples extracted from\nanother public corpus that is continuously growing, the Common Voice dataset.\nThen, we present graphically the accuracy in terms of Word Error Rate of each\nof the different English included accents, showing that there is indeed an\naccuracy bias in terms of accentual variety, favoring the accents most\nprevalent in the training corpus.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:24:33 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["C\u00e1mbara", "Guillermo", ""], ["Peir\u00f3-Lilja", "Alex", ""], ["Farr\u00fas", "Mireia", ""], ["Luque", "Jordi", ""]]}, {"id": "2105.05052", "submitter": "Yang Li", "authors": "Yang Li, Ben Athiwaratkun, Cicero Nogueira dos Santos, Bing Xiang", "title": "Joint Text and Label Generation for Spoken Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is a central problem in machine learning, especially when data\nis limited. Using prior information to enforce constraints is the principled\nway of encouraging generalization. In this work, we propose to leverage the\nprior information embedded in pretrained language models (LM) to improve\ngeneralization for intent classification and slot labeling tasks with limited\ntraining data. Specifically, we extract prior knowledge from pretrained LM in\nthe form of synthetic data, which encode the prior implicitly. We fine-tune the\nLM to generate an augmented language, which contains not only text but also\nencodes both intent labels and slot labels. The generated synthetic data can be\nused to train a classifier later. Since the generated data may contain noise,\nwe rephrase the learning from generated data as learning with noisy labels. We\nthen utilize the mixout regularization for the classifier and prove its\neffectiveness to resist label noise in generated data. Empirically, our method\ndemonstrates superior performance and outperforms the baseline by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:02:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Yang", ""], ["Athiwaratkun", "Ben", ""], ["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""]]}, {"id": "2105.05069", "submitter": "Rishi Hazra", "authors": "Rishi Hazra, Sonu Dixit, Sayambhu Sen", "title": "Zero-Shot Generalization using Intrinsically Motivated Compositional\n  Emergent Protocols", "comments": "Accepted in NAACL 2021 workshop: Visually Grounded Interaction and\n  Language (ViGIL). arXiv admin note: substantial text overlap with\n  arXiv:2012.05011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human language has been described as a system that makes \\textit{use of\nfinite means to express an unlimited array of thoughts}. Of particular interest\nis the aspect of compositionality, whereby, the meaning of a compound language\nexpression can be deduced from the meaning of its constituent parts. If\nartificial agents can develop compositional communication protocols akin to\nhuman language, they can be made to seamlessly generalize to unseen\ncombinations. Studies have recognized the role of curiosity in enabling\nlinguistic development in children. In this paper, we seek to use this\nintrinsic feedback in inducing a systematic and unambiguous protolanguage. We\ndemonstrate how compositionality can enable agents to not only interact with\nunseen objects but also transfer skills from one task to another in a zero-shot\nsetting: \\textit{Can an agent, trained to `pull' and `push twice', `pull\ntwice'?}.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:20:26 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hazra", "Rishi", ""], ["Dixit", "Sonu", ""], ["Sen", "Sayambhu", ""]]}, {"id": "2105.05091", "submitter": "Arijit Gupta", "authors": "Arijit Gupta, Rajaswa Patil and Veeky Baths", "title": "Using Diachronic Distributed Word Representations as Models of Lexical\n  Development in Children", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown that distributed word representations can encode\nabstract semantic and syntactic information from child-directed speech. In this\npaper, we use diachronic distributed word representations to perform temporal\nmodeling and analysis of lexical development in children. Unlike all previous\nwork, we use temporally sliced speech corpus to learn distributed word\nrepresentations of child and child-directed speech. Through our modeling\nexperiments, we demonstrate the dynamics of growing lexical knowledge in\nchildren over time, as compared against a saturated level of lexical knowledge\nin child-directed adult speech. We also fit linear mixed-effects models with\nthe rate of semantic change in the diachronic representations and word\nfrequencies. This allows us to inspect the role of word frequencies towards\nlexical development in children. Further, we perform a qualitative analysis of\nthe diachronic representations from our model, which reveals the categorization\nand word associations in the mental lexicon of children.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:44:05 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Gupta", "Arijit", ""], ["Patil", "Rajaswa", ""], ["Baths", "Veeky", ""]]}, {"id": "2105.05112", "submitter": "Rida  Miraj", "authors": "Rida Miraj, Masaki Aono", "title": "Integrating extracted information from bert and multiple embedding\n  methods with the deep neural network for humour detection", "comments": null, "journal-ref": null, "doi": "10.5121/ijnlc.2021.10202", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humour detection from sentences has been an interesting and challenging task\nin the last few years. In attempts to highlight humour detection, most research\nwas conducted using traditional approaches of embedding, e.g., Word2Vec or\nGlove. Recently BERT sentence embedding has also been used for this task. In\nthis paper, we propose a framework for humour detection in short texts taken\nfrom news headlines. Our proposed framework (IBEN) attempts to extract\ninformation from written text via the use of different layers of BERT. After\nseveral trials, weights were assigned to different layers of the BERT model.\nThe extracted information was then sent to a Bi-GRU neural network as an\nembedding matrix. We utilized the properties of some external embedding models.\nA multi-kernel convolution in our neural network was also employed to extract\nhigher-level sentence representations. This framework performed very well on\nthe task of humour detection.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:09:19 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Miraj", "Rida", ""], ["Aono", "Masaki", ""]]}, {"id": "2105.05135", "submitter": "Rida  Miraj", "authors": "Rida Miraj, Masaki Aono", "title": "kdehumor at semeval-2020 task 7: a neural network model for detecting\n  funniness in dataset humicroedit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our contribution to SemEval-2020 Task 7: Assessing Humor\nin Edited News Headlines. Here we present a method based on a deep neural\nnetwork. In recent years, quite some attention has been devoted to humor\nproduction and perception. Our team KdeHumor employs recurrent neural network\nmodels including Bi-Directional LSTMs (BiLSTMs). Moreover, we utilize the\nstate-of-the-art pre-trained sentence embedding techniques. We analyze the\nperformance of our method and demonstrate the contribution of each component of\nour architecture.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:44:03 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Miraj", "Rida", ""], ["Aono", "Masaki", ""]]}, {"id": "2105.05209", "submitter": "Elazar Gershuni", "authors": "Elazar Gershuni, Yuval Pinter", "title": "Restoring Hebrew Diacritics Without a Dictionary", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate that it is feasible to diacritize Hebrew script without any\nhuman-curated resources other than plain diacritized text. We present NAKDIMON,\na two-layer character level LSTM, that performs on par with much more\ncomplicated curation-dependent systems, across a diverse array of modern Hebrew\nsources.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:23:29 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 21:26:51 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gershuni", "Elazar", ""], ["Pinter", "Yuval", ""]]}, {"id": "2105.05222", "submitter": "Kayo Yin", "authors": "Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe\n  Alikhani", "title": "Including Signed Languages in Natural Language Processing", "comments": "ACL 2021 Best Theme Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:37:55 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 19:12:46 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yin", "Kayo", ""], ["Moryossef", "Amit", ""], ["Hochgesang", "Julie", ""], ["Goldberg", "Yoav", ""], ["Alikhani", "Malihe", ""]]}, {"id": "2105.05227", "submitter": "Yu Guo", "authors": "Yu Guo", "title": "Doing Natural Language Processing in A Natural Way: An NLP toolkit based\n  on object-oriented knowledge base and multi-level grammar base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an NLP toolkit based on object-oriented knowledge base and\nmulti-level grammar base. This toolkit focuses on semantic parsing, it also has\nabilities to discover new knowledge and grammar automatically, new discovered\nknowledge and grammar will be identified by human, and will be used to update\nthe knowledge base and grammar base. This process can be iterated many times to\nimprove the toolkit continuously.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:43:06 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:15:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Guo", "Yu", ""]]}, {"id": "2105.05241", "submitter": "Jack Bandy", "authors": "Jack Bandy, Nicholas Vincent", "title": "Addressing \"Documentation Debt\" in Machine Learning Research: A\n  Retrospective Datasheet for BookCorpus", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature has underscored the importance of dataset documentation\nwork for machine learning, and part of this work involves addressing\n\"documentation debt\" for datasets that have been used widely but documented\nsparsely. This paper aims to help address documentation debt for BookCorpus, a\npopular text dataset for training large language models. Notably, researchers\nhave used BookCorpus to train OpenAI's GPT-N models and Google's BERT models,\neven though little to no documentation exists about the dataset's motivation,\ncomposition, collection process, etc. We offer a preliminary datasheet that\nprovides key context and information about BookCorpus, highlighting several\nnotable deficiencies. In particular, we find evidence that (1) BookCorpus\nlikely violates copyright restrictions for many books, (2) BookCorpus contains\nthousands of duplicated books, and (3) BookCorpus exhibits significant skews in\ngenre representation. We also find hints of other potential deficiencies that\ncall for future research, including problematic content, potential skews in\nreligious representation, and lopsided author contributions. While more work\nremains, this initial effort to provide a datasheet for BookCorpus adds to\ngrowing literature that urges more careful and systematic documentation for\nmachine learning datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:59:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bandy", "Jack", ""], ["Vincent", "Nicholas", ""]]}, {"id": "2105.05361", "submitter": "Philippe Laban", "authors": "Philippe Laban, Andrew Hsi, John Canny, Marti A. Hearst", "title": "The Summary Loop: Learning to Write Abstractive Summaries Without\n  Examples", "comments": "ACL2020, 16 pages, 9 figures", "journal-ref": "Association for Computational Linguistics (2020) 5135-5150", "doi": "10.18653/v1/2020.acl-main.460", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new approach to unsupervised abstractive summarization\nbased on maximizing a combination of coverage and fluency for a given length\nconstraint. It introduces a novel method that encourages the inclusion of key\nterms from the original document into the summary: key terms are masked out of\nthe original document and must be filled in by a coverage model using the\ncurrent generated summary. A novel unsupervised training procedure leverages\nthis coverage model along with a fluency model to generate and score summaries.\nWhen tested on popular news summarization datasets, the method outperforms\nprevious unsupervised methods by more than 2 R-1 points, and approaches results\nof competitive supervised methods. Our model attains higher levels of\nabstraction with copied passages roughly two times shorter than prior work, and\nlearns to compress and merge sentences without supervision.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 23:19:46 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Laban", "Philippe", ""], ["Hsi", "Andrew", ""], ["Canny", "John", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2105.05391", "submitter": "Philippe Laban", "authors": "Philippe Laban, Lucas Bandarkar, Marti A. Hearst", "title": "News Headline Grouping as a Challenging NLU Task", "comments": "NAACL2021, 13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in Natural Language Understanding (NLU) has seen the latest\nmodels outperform human performance on many standard tasks. These impressive\nresults have led the community to introspect on dataset limitations, and\niterate on more nuanced challenges. In this paper, we introduce the task of\nHeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056\npairs of news headlines, each labeled with a binary judgement as to whether the\npair belongs within the same group. On HLGD, human annotators achieve high\nperformance of around 0.9 F-1, while current state-of-the art Transformer\nmodels only reach 0.75 F-1, opening the path for further improvements. We\nfurther propose a novel unsupervised Headline Generator Swap model for the task\nof HeadLine Grouping that achieves within 3 F-1 of the best supervised model.\nFinally, we analyze high-performing models with consistency tests, and find\nthat models are not consistent in their predictions, revealing modeling limits\nof current architectures.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 01:40:49 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Laban", "Philippe", ""], ["Bandarkar", "Lucas", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2105.05392", "submitter": "Philippe Laban", "authors": "Philippe Laban, John Canny, Marti A. Hearst", "title": "What's The Latest? A Question-driven News Chatbot", "comments": "ACL2020 Demo Track, 8 pages, 5 figures", "journal-ref": "ACL Demos (2020) 380-387", "doi": "10.18653/v1/2020.acl-demos.43", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes an automatic news chatbot that draws content from a\ndiverse set of news articles and creates conversations with a user about the\nnews. Key components of the system include the automatic organization of news\narticles into topical chatrooms, integration of automatically generated\nquestions into the conversation, and a novel method for choosing which\nquestions to present which avoids repetitive suggestions. We describe the\nalgorithmic framework and present the results of a usability study that shows\nthat news readers using the system successfully engage in multi-turn\nconversations about specific news stories.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 01:41:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Laban", "Philippe", ""], ["Canny", "John", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2105.05418", "submitter": "Aman Madaan", "authors": "Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, Eduard Hovy", "title": "Could you give me a hint? Generating inference graphs for defeasible\n  reasoning", "comments": "Findings of the Association for Computational Linguistics: ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defeasible reasoning is the mode of reasoning where conclusions can be\noverturned by taking into account new evidence. A commonly used method in\ncognitive science and logic literature is to handcraft argumentation supporting\ninference graphs. While humans find inference graphs very useful for reasoning,\nconstructing them at scale is difficult. In this paper, we automatically\ngenerate such inference graphs through transfer learning from another NLP task\nthat shares the kind of reasoning that inference graphs support. Through\nautomated metrics and human evaluation, we find that our method generates\nmeaningful graphs for the defeasible inference task. Human accuracy on this\ntask improves by 20% by consulting the generated graphs. Our findings open up\nexciting new research avenues for cases where machine reasoning can help human\nreasoning. (A dataset of 230,000 influence graphs for each defeasible query is\nlocated at: https://tinyurl.com/defeasiblegraphs.)\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 04:04:10 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 03:46:46 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Madaan", "Aman", ""], ["Rajagopal", "Dheeraj", ""], ["Tandon", "Niket", ""], ["Yang", "Yiming", ""], ["Hovy", "Eduard", ""]]}, {"id": "2105.05435", "submitter": "Haoyang Liu", "authors": "Haoyang Liu, M. Janina Sarol and Halil Kilicoglu", "title": "UIUC_BioNLP at SemEval-2021 Task 11: A Cascade of Neural Models for\n  Structuring Scholarly NLP Contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cascade of neural models that performs sentence classification,\nphrase recognition, and triple extraction to automatically structure the\nscholarly contributions of NLP publications. To identify the most important\ncontribution sentences in a paper, we used a BERT-based classifier with\npositional features (Subtask 1). A BERT-CRF model was used to recognize and\ncharacterize relevant phrases in contribution sentences (Subtask 2). We\ncategorized the triples into several types based on whether and how their\nelements were expressed in text, and addressed each type using separate\nBERT-based classifiers as well as rules (Subtask 3). Our system was officially\nranked second in Phase 1 evaluation and first in both parts of Phase 2\nevaluation. After fixing a submission error in Pharse 1, our approach yields\nthe best results overall. In this paper, in addition to a system description,\nwe also provide further analysis of our results, highlighting its strengths and\nlimitations. We make our code publicly available at\nhttps://github.com/Liu-Hy/nlp-contrib-graph.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 05:24:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Liu", "Haoyang", ""], ["Sarol", "M. Janina", ""], ["Kilicoglu", "Halil", ""]]}, {"id": "2105.05457", "submitter": "Ting-Yun Chang", "authors": "Ting-Yun Chang, Yang Liu, Karthik Gopalakrishnan, Behnam Hedayatnia,\n  Pei Zhou, Dilek Hakkani-Tur", "title": "Incorporating Commonsense Knowledge Graph in Pretrained Models for\n  Social Commonsense Tasks", "comments": "EMNLP2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models have excelled at many NLP tasks recently; however,\ntheir social intelligence is still unsatisfactory. To enable this, machines\nneed to have a more general understanding of our complicated world and develop\nthe ability to perform commonsense reasoning besides fitting the specific\ndownstream tasks. External commonsense knowledge graphs (KGs), such as\nConceptNet, provide rich information about words and their relationships. Thus,\ntowards general commonsense learning, we propose two approaches to\n\\emph{implicitly} and \\emph{explicitly} infuse such KGs into pretrained\nlanguage models. We demonstrate our proposed methods perform well on SocialIQA,\na social commonsense reasoning task, in both limited and full training data\nregimes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:45:26 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Chang", "Ting-Yun", ""], ["Liu", "Yang", ""], ["Gopalakrishnan", "Karthik", ""], ["Hedayatnia", "Behnam", ""], ["Zhou", "Pei", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2105.05498", "submitter": "Gyubok Lee", "authors": "Gyubok Lee, Seongjun Yang, Edward Choi", "title": "Improving Lexically Constrained Neural Machine Translation with\n  Source-Conditioned Masked Span Prediction", "comments": "To appear in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate terminology translation is crucial for ensuring the practicality and\nreliability of neural machine translation (NMT) systems. To address this,\nlexically constrained NMT explores various methods to ensure pre-specified\nwords and phrases appear in the translation output. However, in many cases,\nthose methods are studied on general domain corpora, where the terms are mostly\nuni- and bi-grams (>98%). In this paper, we instead tackle a more challenging\nsetup consisting of domain-specific corpora with much longer n-gram and highly\nspecialized terms. Inspired by the recent success of masked span prediction\nmodels, we propose a simple and effective training strategy that achieves\nconsistent improvements on both terminology and sentence-level translation for\nthree domain-specific corpora in two language pairs.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:11:33 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 10:59:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Gyubok", ""], ["Yang", "Seongjun", ""], ["Choi", "Edward", ""]]}, {"id": "2105.05502", "submitter": "Britta Grusdt", "authors": "Britta Grusdt and Daniel Lassiter and Michael Franke", "title": "Probabilistic modeling of rational communication with conditionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While a large body of work has scrutinized the meaning of conditional\nsentences, considerably less attention has been paid to formal models of their\npragmatic use and interpretation. Here, we take a probabilistic approach to\npragmatic reasoning about indicative conditionals which flexibly integrates\ngradient beliefs about richly structured world states. We model listeners'\nupdate of their prior beliefs about the causal structure of the world and the\njoint probabilities of the consequent and antecedent based on assumptions about\nthe speaker's utterance production protocol. We show that, when supplied with\nnatural contextual assumptions, our model uniformly explains a number of\ninferences attested in the literature, including epistemic inferences,\nConditional Perfection and the dependency between antecedent and consequent of\na conditional. We argue that this approach also helps explain three puzzles\nintroduced by Douven (2012) about updating with conditionals: depending on the\nutterance context, the listener's belief in the antecedent may increase,\ndecrease or remain unchanged.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:21:25 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 18:20:56 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Grusdt", "Britta", ""], ["Lassiter", "Daniel", ""], ["Franke", "Michael", ""]]}, {"id": "2105.05535", "submitter": "Yuki Taya", "authors": "Yuki Taya, Lis Kanashiro Pereira, Fei Cheng, Ichiro Kobayashi", "title": "OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and\n  Robustness for Lexical Complexity Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an ensemble model for predicting the lexical complexity of words\nand multiword expressions (MWEs). The model receives as input a sentence with a\ntarget word or MWEand outputs its complexity score. Given that a key challenge\nwith this task is the limited size of annotated data, our model relies on\npretrained contextual representations from different state-of-the-art\ntransformer-based language models (i.e., BERT and RoBERTa), and on a variety of\ntraining methods for further enhancing model generalization and\nrobustness:multi-step fine-tuning and multi-task learning, and adversarial\ntraining. Additionally, we propose to enrich contextual representations by\nadding hand-crafted features during training. Our model achieved competitive\nresults and ranked among the top-10 systems in both sub-tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:27:46 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 01:20:17 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 06:39:14 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Taya", "Yuki", ""], ["Pereira", "Lis Kanashiro", ""], ["Cheng", "Fei", ""], ["Kobayashi", "Ichiro", ""]]}, {"id": "2105.05541", "submitter": "Manan Dey", "authors": "Shanya Sharma, Manan Dey and Koustuv Sinha", "title": "Evaluating Gender Bias in Natural Language Inference", "comments": "NeurIPS 2020 Workshop on Dataset Curation and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gender-bias stereotypes have recently raised significant ethical concerns in\nnatural language processing. However, progress in detection and evaluation of\ngender bias in natural language understanding through inference is limited and\nrequires further investigation. In this work, we propose an evaluation\nmethodology to measure these biases by constructing a challenge task that\ninvolves pairing gender-neutral premises against a gender-specific hypothesis.\nWe use our challenge task to investigate state-of-the-art NLI models on the\npresence of gender stereotypes using occupations. Our findings suggest that\nthree models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are\nsignificantly prone to gender-induced prediction errors. We also find that\ndebiasing techniques such as augmenting the training dataset to ensure a\ngender-balanced dataset can help reduce such bias in certain cases.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:41:51 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sharma", "Shanya", ""], ["Dey", "Manan", ""], ["Sinha", "Koustuv", ""]]}, {"id": "2105.05542", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Khalid Alnajjar and Mika H\\\"am\\\"al\\\"ainen", "title": "!Qu\\'e maravilla! Multimodal Sarcasm Detection in Spanish: a Dataset and\n  a Baseline", "comments": "Accepted to The Third Workshop on Multimodal Artificial Intelligence\n  (MAI-Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We construct the first ever multimodal sarcasm dataset for Spanish. The\naudiovisual dataset consists of sarcasm annotated text that is aligned with\nvideo and audio. The dataset represents two varieties of Spanish, a Latin\nAmerican variety and a Peninsular Spanish variety, which ensures a wider\ndialectal coverage for this global language. We present several models for\nsarcasm detection that will serve as baselines in the future research. Our\nresults show that results with text only (89%) are worse than when combining\ntext with audio (91.9%). Finally, the best results are obtained when combining\nall the modalities: text, audio and video (93.1%).\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:43:11 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Alnajjar", "Khalid", ""], ["H\u00e4m\u00e4l\u00e4inen", "Mika", ""]]}, {"id": "2105.05557", "submitter": "Daniel Wiegreffe", "authors": "Christopher Schr\\\"oder, Kim B\\\"urgl, Yves Annanias, Andreas Niekler,\n  Lydia M\\\"uller, Daniel Wiegreffe, Christian Bender, Christoph Mengs, Gerik\n  Scheuermann, Gerhard Heyer", "title": "Mining Legacy Issues in Open Pit Mining Sites: Innovation & Support of\n  Renaturalization and Land Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open pit mines left many regions worldwide inhospitable or uninhabitable. To\nput these regions back into use, entire stretches of land must be\nrenaturalized. For the sustainable subsequent use or transfer to a new primary\nuse, many contaminated sites and soil information have to be permanently\nmanaged. In most cases, this information is available in the form of expert\nreports in unstructured data collections or file folders, which in the best\ncase are digitized. Due to size and complexity of the data, it is difficult for\na single person to have an overview of this data in order to be able to make\nreliable statements. This is one of the most important obstacles to the rapid\ntransfer of these areas to after-use. An information-based approach to this\nissue supports fulfilling several Sustainable Development Goals regarding\nenvironment issues, health and climate action. We use a stack of Optical\nCharacter Recognition, Text Classification, Active Learning and Geographic\nInformation System Visualization to effectively mine and visualize this\ninformation. Subsequently, we link the extracted information to geographic\ncoordinates and visualize them using a Geographic Information System. Active\nLearning plays a vital role because our dataset provides no training data. In\ntotal, we process nine categories and actively learn their representation in\nour dataset. We evaluate the OCR, Active Learning and Text Classification\nseparately to report the performance of the system. Active Learning and text\nclassification results are twofold: Whereas our categories about restrictions\nwork sufficient ($>$.85 F1), the seven topic-oriented categories were\ncomplicated for human coders and hence the results achieved mediocre evaluation\nscores ($<$.70 F1).\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:18:14 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 10:47:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Schr\u00f6der", "Christopher", ""], ["B\u00fcrgl", "Kim", ""], ["Annanias", "Yves", ""], ["Niekler", "Andreas", ""], ["M\u00fcller", "Lydia", ""], ["Wiegreffe", "Daniel", ""], ["Bender", "Christian", ""], ["Mengs", "Christoph", ""], ["Scheuermann", "Gerik", ""], ["Heyer", "Gerhard", ""]]}, {"id": "2105.05571", "submitter": "Chen Shani", "authors": "Chen Shani, Alexander Libov, Sofia Tolmach, Liane Lewin-Eytan, Yoelle\n  Maarek, Dafna Shahaf", "title": "\"Alexa, what do you do for fun?\" Characterizing playful requests with\n  virtual assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual assistants such as Amazon's Alexa, Apple's Siri, Google Home, and\nMicrosoft's Cortana, are becoming ubiquitous in our daily lives and\nsuccessfully help users in various daily tasks, such as making phone calls or\nplaying music. Yet, they still struggle with playful utterances, which are not\nmeant to be interpreted literally. Examples include jokes or absurd requests or\nquestions such as, \"Are you afraid of the dark?\", \"Who let the dogs out?\", or\n\"Order a zillion gummy bears\". Today, virtual assistants often return\nirrelevant answers to such utterances, except for hard-coded ones addressed by\ncanned replies.\n  To address the challenge of automatically detecting playful utterances, we\nfirst characterize the different types of playful human-virtual assistant\ninteraction. We introduce a taxonomy of playful requests rooted in theories of\nhumor and refined by analyzing real-world traffic from Alexa. We then focus on\none node, personification, where users refer to the virtual assistant as a\nperson (\"What do you do for fun?\"). Our conjecture is that understanding such\nutterances will improve user experience with virtual assistants. We conducted a\nWizard-of-Oz user study and showed that endowing virtual assistant s with the\nability to identify humorous opportunities indeed has the potential to increase\nuser satisfaction. We hope this work will contribute to the understanding of\nthe landscape of the problem and inspire novel ideas and techniques towards the\nvision of giving virtual assistants a sense of humor.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:48:00 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Shani", "Chen", ""], ["Libov", "Alexander", ""], ["Tolmach", "Sofia", ""], ["Lewin-Eytan", "Liane", ""], ["Maarek", "Yoelle", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2105.05582", "submitter": "Bertrand Higy", "authors": "Bertrand Higy, Lieke Gelderloos, Afra Alishahi and Grzegorz\n  Chrupa{\\l}a", "title": "Discrete representations in neural models of spoken language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The distributed and continuous representations used by neural networks are at\nodds with representations employed in linguistics, which are typically\nsymbolic. Vector quantization has been proposed as a way to induce discrete\nneural representations that are closer in nature to their linguistic\ncounterparts. However, it is not clear which metrics are the best-suited to\nanalyze such discrete representations. We compare the merits of four commonly\nused metrics in the context of weakly supervised models of spoken language. We\nperform a systematic analysis of the impact of (i) architectural choices, (ii)\nthe learning objective and training dataset, and (iii) the evaluation metric.\nWe find that the different evaluation metrics can give inconsistent results. In\nparticular, we find that the use of minimal pairs of phoneme triples as stimuli\nduring evaluation disadvantages larger embeddings, unlike metrics applied to\ncomplete utterances.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:02:02 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Higy", "Bertrand", ""], ["Gelderloos", "Lieke", ""], ["Alishahi", "Afra", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "2105.05596", "submitter": "Ziheng Zhang", "authors": "Zhiyuan Qi, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Yuejia Xiang, Ningyu\n  Zhang, Yefeng Zheng", "title": "Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and\n  Semantic Embedding", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent\nentities, relations, and others) between two KGs. The existing methods can be\ndivided into the embedding-based models, and the conventional reasoning and\nlexical matching based systems. The former compute the similarity of entities\nvia their cross-KG embeddings, but they usually rely on an ideal supervised\nlearning setting for good performance and lack appropriate reasoning to avoid\nlogically wrong mappings; while the latter address the reasoning issue but are\npoor at utilizing the KG graph structures and the entity contexts. In this\nstudy, we aim at combining the above two solutions and thus propose an\niterative framework named PRASE which is based on probabilistic reasoning and\nsemantic embedding. It learns the KG embeddings via entity mappings from a\nprobabilistic reasoning system named PARIS, and feeds the resultant entity\nmappings and embeddings back into PARIS for augmentation. The PRASE framework\nis compatible with different embedding-based models, and our experiments on\nmultiple datasets have demonstrated its state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:27:46 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:25:23 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 07:51:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qi", "Zhiyuan", ""], ["Zhang", "Ziheng", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Xi", ""], ["Xiang", "Yuejia", ""], ["Zhang", "Ningyu", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2105.05601", "submitter": "DongHyun Choi", "authors": "DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, Dong Ryeol Shin", "title": "OutFlip: Generating Out-of-Domain Samples for Unknown Intent Detection\n  with Natural Language Attack", "comments": "9 pages, 3 figures; to be appear in ACL Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Out-of-domain (OOD) input detection is vital in a task-oriented dialogue\nsystem since the acceptance of unsupported inputs could lead to an incorrect\nresponse of the system. This paper proposes OutFlip, a method to generate\nout-of-domain samples using only in-domain training dataset automatically. A\nwhite-box natural language attack method HotFlip is revised to generate\nout-of-domain samples instead of adversarial examples. Our evaluation results\nshowed that integrating OutFlip-generated out-of-domain samples into the\ntraining dataset could significantly improve an intent classification model's\nout-of-domain detection performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:38:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Choi", "DongHyun", ""], ["Shin", "Myeong Cheol", ""], ["Kim", "EungGyun", ""], ["Shin", "Dong Ryeol", ""]]}, {"id": "2105.05605", "submitter": "Ruben Cardoso", "authors": "Ruben Cardoso, Afonso Mendes, Andre Lamurias", "title": "Priberam Labs at the NTCIR-15 SHINRA2020-ML: Classification Task", "comments": "Presented at NTCIR-15 conference (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wikipedia is an online encyclopedia available in 285 languages. It composes\nan extremely relevant Knowledge Base (KB), which could be leveraged by\nautomatic systems for several purposes. However, the structure and organisation\nof such information are not prone to automatic parsing and understanding and it\nis, therefore, necessary to structure this knowledge. The goal of the current\nSHINRA2020-ML task is to leverage Wikipedia pages in order to categorise their\ncorresponding entities across 268 hierarchical categories, belonging to the\nExtended Named Entity (ENE) ontology. In this work, we propose three distinct\nmodels based on the contextualised embeddings yielded by Multilingual BERT. We\nexplore the performances of a linear layer with and without explicit usage of\nthe ontology's hierarchy, and a Gated Recurrent Units (GRU) layer. We also test\nseveral pooling strategies to leverage BERT's embeddings and selection criteria\nbased on the labels' scores. We were able to achieve good performance across a\nlarge variety of languages, including those not seen during the fine-tuning\nprocess (zero-shot languages).\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:49:19 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cardoso", "Ruben", ""], ["Mendes", "Afonso", ""], ["Lamurias", "Andre", ""]]}, {"id": "2105.05614", "submitter": "Ruben Cardoso", "authors": "Ruben Cardoso, Zita Marinho, Afonso Mendes, Sebasti\\~ao Miranda", "title": "Priberam at MESINESP Multi-label Classification of Medical Texts Task", "comments": "Presented at CLEF2020 conference (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical articles provide current state of the art treatments and diagnostics\nto many medical practitioners and professionals. Existing public databases such\nas MEDLINE contain over 27 million articles, making it difficult to extract\nrelevant content without the use of efficient search engines. Information\nretrieval tools are crucial in order to navigate and provide meaningful\nrecommendations for articles and treatments. Classifying these articles into\nbroader medical topics can improve the retrieval of related articles. The set\nof medical labels considered for the MESINESP task is on the order of several\nthousands of labels (DeCS codes), which falls under the extreme multi-label\nclassification problem. The heterogeneous and highly hierarchical structure of\nmedical topics makes the task of manually classifying articles extremely\nlaborious and costly. It is, therefore, crucial to automate the process of\nclassification. Typical machine learning algorithms become computationally\ndemanding with such a large number of labels and achieving better recall on\nsuch datasets becomes an unsolved problem.\n  This work presents Priberam's participation at the BioASQ task Mesinesp. We\naddress the large multi-label classification problem through the use of four\ndifferent models: a Support Vector Machine (SVM), a customised search engine\n(Priberam Search), a BERT based classifier, and a SVM-rank ensemble of all the\nprevious models. Results demonstrate that all three individual models perform\nwell and the best performance is achieved by their ensemble, granting Priberam\nthe 6th place in the present challenge and making it the 2nd best team.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:14:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cardoso", "Ruben", ""], ["Marinho", "Zita", ""], ["Mendes", "Afonso", ""], ["Miranda", "Sebasti\u00e3o", ""]]}, {"id": "2105.05621", "submitter": "Ashutosh Modi", "authors": "Pradip Swarnakar and Ashutosh Modi", "title": "NLP for Climate Policy: Creating a Knowledge Platform for Holistic and\n  Effective Climate Action", "comments": "12 Pages (8 + 4 pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Climate change is a burning issue of our time, with the Sustainable\nDevelopment Goal (SDG) 13 of the United Nations demanding global climate\naction. Realizing the urgency, in 2015 in Paris, world leaders signed an\nagreement committing to taking voluntary action to reduce carbon emissions.\nHowever, the scale, magnitude, and climate action processes vary globally,\nespecially between developed and developing countries. Therefore, from\nparliament to social media, the debates and discussions on climate change\ngather data from wide-ranging sources essential to the policy design and\nimplementation. The downside is that we do not currently have the mechanisms to\npool the worldwide dispersed knowledge emerging from the structured and\nunstructured data sources.\n  The paper thematically discusses how NLP techniques could be employed in\nclimate policy research and contribute to society's good at large. In\nparticular, we exemplify symbiosis of NLP and Climate Policy Research via four\nmethodologies. The first one deals with the major topics related to climate\npolicy using automated content analysis. We investigate the opinions\n(sentiments) of major actors' narratives towards climate policy in the second\nmethodology. The third technique explores the climate actors' beliefs towards\npro or anti-climate orientation. Finally, we discuss developing a Climate\nKnowledge Graph.\n  The present theme paper further argues that creating a knowledge platform\nwould help in the formulation of a holistic climate policy and effective\nclimate action. Such a knowledge platform would integrate the policy actors'\nvaried opinions from different social sectors like government, business, civil\nsociety, and the scientific community. The research outcome will add value to\neffective climate action because policymakers can make informed decisions by\nlooking at the diverse public opinion on a comprehensive platform.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 12:30:02 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Swarnakar", "Pradip", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2105.05641", "submitter": "Vamsi Aribandi", "authors": "Vamsi Aribandi, Yi Tay, Donald Metzler", "title": "How Reliable are Model Diagnostics?", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the pursuit of a deeper understanding of a model's behaviour, there is\nrecent impetus for developing suites of probes aimed at diagnosing models\nbeyond simple metrics like accuracy or BLEU. This paper takes a step back and\nasks an important and timely question: how reliable are these diagnostics in\nproviding insight into models and training setups? We critically examine three\nrecent diagnostic tests for pre-trained language models, and find that\nlikelihood-based and representation-based model diagnostics are not yet as\nreliable as previously assumed. Based on our empirical findings, we also\nformulate recommendations for practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 13:20:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Aribandi", "Vamsi", ""], ["Tay", "Yi", ""], ["Metzler", "Donald", ""]]}, {"id": "2105.05686", "submitter": "Guilherme Rosa", "authors": "Guilherme Moraes Rosa, Ruan Chaves Rodrigues, Roberto Lotufo, Rodrigo\n  Nogueira", "title": "Yes, BM25 is a Strong Baseline for Legal Case Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our single submission to task 1 of COLIEE 2021. Our vanilla BM25\ngot second place, well above the median of submissions. Code is available at\nhttps://github.com/neuralmind-ai/coliee.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 18:33:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Rosa", "Guilherme Moraes", ""], ["Rodrigues", "Ruan Chaves", ""], ["Lotufo", "Roberto", ""], ["Nogueira", "Rodrigo", ""]]}, {"id": "2105.05727", "submitter": "Jiwei Li", "authors": "Yuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han, Kun Kuang, Jiwei\n  Li and Fei Wu", "title": "BertGCN: Transductive Text Classification by Combining GCN and BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we propose BertGCN, a model that combines large scale\npretraining and transductive learning for text classification. BertGCN\nconstructs a heterogeneous graph over the dataset and represents documents as\nnodes using BERT representations. By jointly training the BERT and GCN modules\nwithin BertGCN, the proposed model is able to leverage the advantages of both\nworlds: large-scale pretraining which takes the advantage of the massive amount\nof raw data and transductive learning which jointly learns representations for\nboth training data and unlabeled test data by propagating label influence\nthrough graph convolution. Experiments show that BertGCN achieves SOTA\nperformances on a wide range of text classification datasets. Code is available\nat https://github.com/ZeroRin/BertGCN.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:20:01 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 11:32:18 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 07:57:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lin", "Yuxiao", ""], ["Meng", "Yuxian", ""], ["Sun", "Xiaofei", ""], ["Han", "Qinghong", ""], ["Kuang", "Kun", ""], ["Li", "Jiwei", ""], ["Wu", "Fei", ""]]}, {"id": "2105.05737", "submitter": "Zili Zhou", "authors": "Zili Zhou, Marco Valentino, Donal Landers, Andre Freitas", "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:42:50 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 20:10:09 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhou", "Zili", ""], ["Valentino", "Marco", ""], ["Landers", "Donal", ""], ["Freitas", "Andre", ""]]}, {"id": "2105.05744", "submitter": "Sandipan Basu Mr", "authors": "Sandipan Basu, Aravind Gaddala, Pooja Chetan, Garima Tiwari, Narayana\n  Darapaneni, Sadwik Parvathaneni, Anwesh Reddy Paduri", "title": "Building a Question and Answer System for News Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This project attempts to build a Question- Answering system in the News\nDomain, where Passages will be News articles, and anyone can ask a Question\nagainst it. We have built a span-based model using an Attention mechanism,\nwhere the model predicts the answer to a question as to the position of the\nstart and end tokens in a paragraph. For training our model, we have used the\nStanford Question and Answer (SQuAD 2.0) dataset[1]. To do well on SQuAD 2.0,\nsystems must not only answer questions when possible but also determine when no\nanswer is supported by the paragraph and abstain from answering. Our model\narchitecture comprises three layers- Embedding Layer, RNN Layer, and the\nAttention Layer. For the Embedding layer, we used GloVe and the Universal\nSentence Encoder. For the RNN Layer, we built variations of the RNN Layer\nincluding bi-LSTM and Stacked LSTM and we built an Attention Layer using a\nContext to Question Attention and also improvised on the innovative\nBidirectional Attention Layer. Our best performing model which uses GloVe\nEmbedding combined with Bi-LSTM and Context to Question Attention achieved an\nF1 Score and EM of 33.095 and 33.094 respectively. We also leveraged transfer\nlearning and built a Transformer based model using BERT. The BERT-based model\nachieved an F1 Score and EM of 57.513 and 49.769 respectively. We concluded\nthat the BERT model is superior in all aspects of answering various types of\nquestions.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:56:21 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Basu", "Sandipan", ""], ["Gaddala", "Aravind", ""], ["Chetan", "Pooja", ""], ["Tiwari", "Garima", ""], ["Darapaneni", "Narayana", ""], ["Parvathaneni", "Sadwik", ""], ["Paduri", "Anwesh Reddy", ""]]}, {"id": "2105.05748", "submitter": "Lia Yeh", "authors": "Benjamin Rodatz, Razin A. Shaikh and Lia Yeh", "title": "Conversational Negation using Worldly Context in Compositional\n  Distributional Semantics", "comments": "13 pages, 5 figures, To be published in Proceedings of SEMSPACE 2021\n  and to appear in the ACL anthology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework to model an operational conversational negation by\napplying worldly context (prior knowledge) to logical negation in compositional\ndistributional semantics. Given a word, our framework can create its negation\nthat is similar to how humans perceive negation. The framework corrects logical\nnegation to weight meanings closer in the entailment hierarchy more than\nmeanings further apart. The proposed framework is flexible to accommodate\ndifferent choices of logical negations, compositions, and worldly context\ngeneration. In particular, we propose and motivate a new logical negation using\nmatrix inverse.\n  We validate the sensibility of our conversational negation framework by\nperforming experiments, leveraging density matrices to encode graded entailment\ninformation. We conclude that the combination of subtraction negation and\nphaser in the basis of the negated word yields the highest Pearson correlation\nof 0.635 with human ratings.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:04:36 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Rodatz", "Benjamin", ""], ["Shaikh", "Razin A.", ""], ["Yeh", "Lia", ""]]}, {"id": "2105.05752", "submitter": "Chen Xu", "authors": "Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, shen huang, Qi Ju, Tong\n  Xiao, Jingbo Zhu", "title": "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained\n  Models into Speech Translation Encoders", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder pre-training is promising in end-to-end Speech Translation (ST),\ngiven the fact that speech-to-translation data is scarce. But ST encoders are\nnot simple instances of Automatic Speech Recognition (ASR) or Machine\nTranslation (MT) encoders. For example, we find that ASR encoders lack the\nglobal context representation, which is necessary for translation, whereas MT\nencoders are not designed to deal with long but locally attentive acoustic\nsequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding\n(SATE) method for speech translation. Our encoder begins with processing the\nacoustic sequence as usual, but later behaves more like an MT encoder for a\nglobal representation of the input sequence. In this way, it is straightforward\nto incorporate the pre-trained models into the system. Also, we develop an\nadaptor module to alleviate the representation inconsistency between the\npre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge\ndistillation method to preserve the pre-training knowledge. Experimental\nresults on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method\nachieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we\nare the first to develop an end-to-end ST system that achieves comparable or\neven better BLEU performance than the cascaded ST counterpart when large-scale\nASR and MT data is available.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:09:53 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:29:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Xu", "Chen", ""], ["Hu", "Bojie", ""], ["Li", "Yanyang", ""], ["Zhang", "Yuhao", ""], ["huang", "shen", ""], ["Ju", "Qi", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""]]}, {"id": "2105.05762", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon", "title": "Forecasting election results by studying brand importance in online news", "comments": null, "journal-ref": "International Journal of Forecasting 36(2), 414-427 (2020)", "doi": "10.1016/j.ijforecast.2019.05.013", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study uses the semantic brand score, a novel measure of brand importance\nin big textual data, to forecast elections based on online news. About 35,000\nonline news articles were transformed into networks of co-occurring words and\nanalyzed by combining methods and tools from social network analysis and text\nmining. Forecasts made for four voting events in Italy provided consistent\nresults across different voting systems: a general election, a referendum, and\na municipal election in two rounds. This work contributes to the research on\nelectoral forecasting by focusing on predictions based on online big data; it\noffers new perspectives regarding the textual analysis of online news through a\nmethodology which is relatively fast and easy to apply. This study also\nsuggests the existence of a link between the brand importance of political\ncandidates and parties and electoral results.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:30:33 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Colladon", "A. Fronzetti", ""]]}, {"id": "2105.05781", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A Fronzetti Colladon", "title": "The Semantic Brand Score", "comments": null, "journal-ref": "Journal of Business Research 88, 150-160 (2018)", "doi": "10.1016/j.jbusres.2018.03.026", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Semantic Brand Score (SBS) is a new measure of brand importance\ncalculated on text data, combining methods of social network and semantic\nanalysis. This metric is flexible as it can be used in different contexts and\nacross products, markets and languages. It is applicable not only to brands,\nbut also to multiple sets of words. The SBS, described together with its three\ndimensions of brand prevalence, diversity and connectivity, represents a\ncontribution to the research on brand equity and on word co-occurrence\nnetworks. It can be used to support decision-making processes within companies;\nfor example, it can be applied to forecast a company's stock price or to assess\nbrand importance with respect to competitors. On the one side, the SBS relates\nto familiar constructs of brand equity, on the other, it offers new\nperspectives for effective strategic management of brands in the era of big\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:54:57 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Colladon", "A Fronzetti", ""]]}, {"id": "2105.05790", "submitter": "Caleb Belth", "authors": "Caleb Belth, Sarah Payne, Deniz Beser, Jordan Kodner, Charles Yang", "title": "The Greedy and Recursive Search for Morphological Productivity", "comments": "CogSci 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As children acquire the knowledge of their language's morphology, they\ninvariably discover the productive processes that can generalize to new words.\nMorphological learning is made challenging by the fact that even fully\nproductive rules have exceptions, as in the well-known case of English past\ntense verbs, which features the -ed rule against the irregular verbs. The\nTolerance Principle is a recent proposal that provides a precise threshold of\nexceptions that a productive rule can withstand. Its empirical application so\nfar, however, requires the researcher to fully specify rules defined over a set\nof words. We propose a greedy search model that automatically hypothesizes\nrules and evaluates their productivity over a vocabulary. When the search for\nbroader productivity fails, the model recursively subdivides the vocabulary and\ncontinues the search for productivity over narrower rules. Trained on\npsychologically realistic data from child-directed input, our model displays\ndevelopmental patterns observed in child morphology acquisition, including the\nnotoriously complex case of German noun pluralization. It also produces\nresponses to nonce words that, despite receiving only a fraction of the\ntraining data, are more similar to those of human subjects than current neural\nnetwork models' responses are.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:02:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Belth", "Caleb", ""], ["Payne", "Sarah", ""], ["Beser", "Deniz", ""], ["Kodner", "Jordan", ""], ["Yang", "Charles", ""]]}, {"id": "2105.05796", "submitter": "Tomasz Stanis{\\l}awek", "authors": "Tomasz Stanis{\\l}awek and Filip Grali\\'nski and Anna Wr\\'oblewska and\n  Dawid Lipi\\'nski and Agnieszka Kaliska and Paulina Rosalska and Bartosz\n  Topolski and Przemys{\\l}aw Biecek", "title": "Kleister: Key Information Extraction Datasets Involving Long Documents\n  with Complex Layouts", "comments": "accepted to ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relevance of the Key Information Extraction (KIE) task is increasingly\nimportant in natural language processing problems. But there are still only a\nfew well-defined problems that serve as benchmarks for solutions in this area.\nTo bridge this gap, we introduce two new datasets (Kleister NDA and Kleister\nCharity). They involve a mix of scanned and born-digital long formal\nEnglish-language documents. In these datasets, an NLP system is expected to\nfind or infer various types of entities by employing both textual and\nstructural layout features. The Kleister Charity dataset consists of 2,788\nannual financial reports of charity organizations, with 61,643 unique pages and\n21,612 entities to extract. The Kleister NDA dataset has 540 Non-disclosure\nAgreements, with 3,229 unique pages and 2,160 entities to extract. We provide\nseveral state-of-the-art baseline systems from the KIE domain (Flair, BERT,\nRoBERTa, LayoutLM, LAMBERT), which show that our datasets pose a strong\nchallenge to existing models. The best model achieved an 81.77% and an 83.57%\nF1-score on respectively the Kleister NDA and the Kleister Charity datasets. We\nshare the datasets to encourage progress on more in-depth and complex\ninformation extraction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:08:01 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Stanis\u0142awek", "Tomasz", ""], ["Grali\u0144ski", "Filip", ""], ["Wr\u00f3blewska", "Anna", ""], ["Lipi\u0144ski", "Dawid", ""], ["Kaliska", "Agnieszka", ""], ["Rosalska", "Paulina", ""], ["Topolski", "Bartosz", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2105.05885", "submitter": "Divya Koyyalagunta", "authors": "Divya Koyyalagunta, Anna Sun, Rachel Lea Draelos, Cynthia Rudin", "title": "Playing Codenames with Language Graphs and Word Embeddings", "comments": "Divya Koyyalagunta and Anna Sun contributed equally to this work.\n  This is an arXiv version of the paper that has been accepted for publication\n  in the Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although board games and video games have been studied for decades in\nartificial intelligence research, challenging word games remain relatively\nunexplored. Word games are not as constrained as games like chess or poker.\nInstead, word game strategy is defined by the players' understanding of the way\nwords relate to each other. The word game Codenames provides a unique\nopportunity to investigate common sense understanding of relationships between\nwords, an important open challenge. We propose an algorithm that can generate\nCodenames clues from the language graph BabelNet or from any of several\nembedding methods - word2vec, GloVe, fastText or BERT. We introduce a new\nscoring function that measures the quality of clues, and we propose a weighting\nterm called DETECT that incorporates dictionary-based word representations and\ndocument frequency to improve clue selection. We develop BabelNet-Word\nSelection Framework (BabelNet-WSF) to improve BabelNet clue quality and\novercome the computational barriers that previously prevented leveraging\nlanguage graphs for Codenames. Extensive experiments with human evaluators\ndemonstrate that our proposed innovations yield state-of-the-art performance,\nwith up to 102.8% improvement in precision@2 in some cases. Overall, this work\nadvances the formal study of word games and approaches for common sense\nlanguage understanding.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:23:03 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Koyyalagunta", "Divya", ""], ["Sun", "Anna", ""], ["Draelos", "Rachel Lea", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2105.05887", "submitter": "Alexander Robertson", "authors": "Alexander Robertson, Walid Magdy, Sharon Goldwater", "title": "Black or White but never neutral: How readers perceive identity from\n  yellow or skin-toned emoji", "comments": null, "journal-ref": "ACM Conference On Computer-supported Cooperative Work And Social\n  Computing 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in sociology and linguistics shows that people use language not only\nto express their own identity but to understand the identity of others. Recent\nwork established a connection between expression of identity and emoji usage on\nsocial media, through use of emoji skin tone modifiers. Motivated by that\nfinding, this work asks if, as with language, readers are sensitive to such\nacts of self-expression and use them to understand the identity of authors. In\nbehavioral experiments (n=488), where text and emoji content of social media\nposts were carefully controlled before being presented to participants, we find\nin the affirmative -- emoji are a salient signal of author identity. That\nsignal is distinct from, and complementary to, the one encoded in language.\nParticipant groups (based on self-identified ethnicity) showed no differences\nin how they perceive this signal, except in the case of the default yellow\nemoji. While both groups associate this with a White identity, the effect was\nstronger in White participants. Our finding that emoji can index social\nvariables will have experimental applications for researchers but also\nimplications for designers: supposedly ``neutral`` defaults may be more\nrepresentative of some users than others.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:23:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Robertson", "Alexander", ""], ["Magdy", "Walid", ""], ["Goldwater", "Sharon", ""]]}, {"id": "2105.05912", "submitter": "Ahmad Rashid", "authors": "Ahmad Rashid, Vasileios Lioutas and Mehdi Rezagholizadeh", "title": "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of large pre-trained language models has given rise to rapid\nprogress in the field of Natural Language Processing (NLP). While the\nperformance of these models on standard benchmarks has scaled with size,\ncompression techniques such as knowledge distillation have been key in making\nthem practical. We present, MATE-KD, a novel text-based adversarial training\nalgorithm which improves the performance of knowledge distillation. MATE-KD\nfirst trains a masked language model based generator to perturb text by\nmaximizing the divergence between teacher and student logits. Then using\nknowledge distillation a student is trained on both the original and the\nperturbed training samples. We evaluate our algorithm, using BERT-based models,\non the GLUE benchmark and demonstrate that MATE-KD outperforms competitive\nadversarial learning and data augmentation baselines. On the GLUE test set our\n6 layer RoBERTa based model outperforms BERT-Large.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:11:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rashid", "Ahmad", ""], ["Lioutas", "Vasileios", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "2105.05913", "submitter": "Ting-Yun Chang", "authors": "Ting-Yun Chang, Yang Liu, Karthik Gopalakrishnan, Behnam Hedayatnia,\n  Pei Zhou, Dilek Hakkani-Tur", "title": "Go Beyond Plain Fine-tuning: Improving Pretrained Models for Social\n  Commonsense", "comments": "SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models have demonstrated outstanding performance in many\nNLP tasks recently. However, their social intelligence, which requires\ncommonsense reasoning about the current situation and mental states of others,\nis still developing. Towards improving language models' social intelligence, we\nfocus on the Social IQA dataset, a task requiring social and emotional\ncommonsense reasoning. Building on top of the pretrained RoBERTa and GPT2\nmodels, we propose several architecture variations and extensions, as well as\nleveraging external commonsense corpora, to optimize the model for Social IQA.\nOur proposed system achieves competitive results as those top-ranking models on\nthe leaderboard. This work demonstrates the strengths of pretrained language\nmodels, and provides viable ways to improve their performance for a particular\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:18:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chang", "Ting-Yun", ""], ["Liu", "Yang", ""], ["Gopalakrishnan", "Karthik", ""], ["Hedayatnia", "Behnam", ""], ["Zhou", "Pei", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2105.05915", "submitter": "Boxiang Liu", "authors": "Boxiang Liu, Jiaji Huang, Xingyu Cai, Kenneth Church", "title": "Better than BERT but Worse than Baseline", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper compares BERT-SQuAD and Ab3P on the Abbreviation Definition\nIdentification (ADI) task. ADI inputs a text and outputs short forms\n(abbreviations/acronyms) and long forms (expansions). BERT with reranking\nimproves over BERT without reranking but fails to reach the Ab3P rule-based\nbaseline. What is BERT missing? Reranking introduces two new features:\ncharmatch and freq. The first feature identifies opportunities to take\nadvantage of character constraints in acronyms and the second feature\nidentifies opportunities to take advantage of frequency constraints across\ndocuments.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:18:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Liu", "Boxiang", ""], ["Huang", "Jiaji", ""], ["Cai", "Xingyu", ""], ["Church", "Kenneth", ""]]}, {"id": "2105.05975", "submitter": "B{\\l}a\\.zej Dolicki", "authors": "B{\\l}a\\.zej Dolicki and Gerasimos Spanakis", "title": "Analysing The Impact Of Linguistic Features On Cross-Lingual Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing amount of evidence that in cases with little or no\ndata in a target language, training on a different language can yield\nsurprisingly good results. However, currently there are no established\nguidelines for choosing the training (source) language. In attempt to solve\nthis issue we thoroughly analyze a state-of-the-art multilingual model and try\nto determine what impacts good transfer between languages. As opposed to the\nmajority of multilingual NLP literature, we don't only train on English, but on\na group of almost 30 languages. We show that looking at particular syntactic\nfeatures is 2-4 times more helpful in predicting the performance than an\naggregated syntactic similarity. We find out that the importance of syntactic\nfeatures strongly differs depending on the downstream task - no single feature\nis a good performance predictor for all NLP tasks. As a result, one should not\nexpect that for a target language $L_1$ there is a single language $L_2$ that\nis the best choice for any NLP task (for instance, for Bulgarian, the best\nsource language is French on POS tagging, Russian on NER and Thai on NLI). We\ndiscuss the most important linguistic features affecting the transfer quality\nusing statistical and machine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:22:58 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Dolicki", "B\u0142a\u017cej", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "2105.05977", "submitter": "Alex Kuznetsov", "authors": "Alex Kuznetsov, Hector Urdiales", "title": "Spelling Correction with Denoising Transformer", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method of performing spelling correction on short input\nstrings, such as search queries or individual words. At its core lies a\nprocedure for generating artificial typos which closely follow the error\npatterns manifested by humans. This procedure is used to train the production\nspelling correction model based on a transformer architecture. This model is\ncurrently served in the HubSpot product search. We show that our approach to\ntypo generation is superior to the widespread practice of adding noise, which\nignores human patterns. We also demonstrate how our approach may be extended to\nresource-scarce settings and train spelling correction models for Arabic,\nGreek, Russian, and Setswana languages, without using any labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:35:18 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Kuznetsov", "Alex", ""], ["Urdiales", "Hector", ""]]}, {"id": "2105.05996", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Marcos Zampieri", "title": "Multilingual Offensive Language Identification for Low-resource\n  Languages", "comments": "Accepted to ACM Transactions on Asian and Low-Resource Language\n  Information Processing (TALLIP). This is an extended version of a paper\n  accepted to EMNLP. arXiv admin note: substantial text overlap with\n  arXiv:2010.05324", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 22:50:16 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 15:39:26 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 11:20:49 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2105.05999", "submitter": "Jingxuan Tu", "authors": "James Pustejovsky, Eben Holderness, Jingxuan Tu, Parker Glenn,\n  Kyeongmin Rim, Kelley Lynch, Richard Brutti", "title": "Designing Multimodal Datasets for NLP Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that the design and development of multimodal\ndatasets for natural language processing (NLP) challenges should be enhanced in\ntwo significant respects: to more broadly represent commonsense semantic\ninferences; and to better reflect the dynamics of actions and events, through a\nsubstantive alignment of textual and visual information. We identify challenges\nand tasks that are reflective of linguistic and cognitive competencies that\nhumans have when speaking and reasoning, rather than merely the performance of\nsystems on isolated tasks. We introduce the distinction between challenge-based\ntasks and competence-based performance, and describe a diagnostic dataset,\nRecipe-to-Video Questions (R2VQ), designed for testing competence-based\ncomprehension over a multimodal recipe collection (http://r2vq.org/). The\ncorpus contains detailed annotation supporting such inferencing tasks and\nfacilitating a rich set of question families that we use to evaluate NLP\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 23:02:46 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pustejovsky", "James", ""], ["Holderness", "Eben", ""], ["Tu", "Jingxuan", ""], ["Glenn", "Parker", ""], ["Rim", "Kyeongmin", ""], ["Lynch", "Kelley", ""], ["Brutti", "Richard", ""]]}, {"id": "2105.06020", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Dhruba Ghosh, Dan Klein, Jacob Steinhardt", "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing\n  Performance at the Instance Level", "comments": "ACL 2021 Findings. Code and data:\n  https://github.com/ruiqi-zhong/acl2021-instance-level", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Larger language models have higher accuracy on average, but are they better\non every single instance (datapoint)? Some work suggests larger models have\nhigher out-of-distribution robustness, while other work suggests they have\nlower accuracy on rare subgroups. To understand these differences, we\ninvestigate these models at the level of individual instances. However, one\nmajor challenge is that individual predictions are highly sensitive to noise in\nthe randomness in training. We develop statistically rigorous methods to\naddress this, and after accounting for pretraining and finetuning noise, we\nfind that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances\nacross MNLI, SST-2, and QQP, compared to the overall accuracy improvement of\n2-10%. We also find that finetuning noise increases with model size and that\ninstance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium\ncorrelates with improvement from BERT-Medium to BERT-Large. Our findings\nsuggest that instance-level predictions provide a rich source of information;\nwe therefore, recommend that researchers supplement model weights with model\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:10:51 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Ghosh", "Dhruba", ""], ["Klein", "Dan", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2105.06027", "submitter": "Oleg Vasilyev", "authors": "Neslihan Iskender, Oleg Vasilyev, Tim Polzehl, John Bohannon,\n  Sebastian M\\\"oller", "title": "Towards Human-Free Automatic Quality Evaluation of German Summarization", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating large summarization corpora using humans has proven to be\nexpensive from both the organizational and the financial perspective.\nTherefore, many automatic evaluation metrics have been developed to measure the\nsummarization quality in a fast and reproducible way. However, most of the\nmetrics still rely on humans and need gold standard summaries generated by\nlinguistic experts. Since BLANC does not require golden summaries and\nsupposedly can use any underlying language model, we consider its application\nto the evaluation of summarization in German. This work demonstrates how to\nadjust the BLANC metric to a language other than English. We compare BLANC\nscores with the crowd and expert ratings, as well as with commonly used\nautomatic metrics on a German summarization data set. Our results show that\nBLANC in German is especially good in evaluating informativeness.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:29:33 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Iskender", "Neslihan", ""], ["Vasilyev", "Oleg", ""], ["Polzehl", "Tim", ""], ["Bohannon", "John", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2105.06041", "submitter": "Silin Gao", "authors": "Silin Gao, Ryuichi Takanobu, Wei Peng, Qun Liu, Minlie Huang", "title": "HyKnow: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge\n  Management", "comments": "Findings of ACL-IJCNLP 2021, long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialog (TOD) systems typically manage structured knowledge\n(e.g. ontologies and databases) to guide the goal-oriented conversations.\nHowever, they fall short of handling dialog turns grounded on unstructured\nknowledge (e.g. reviews and documents). In this paper, we formulate a task of\nmodeling TOD grounded on both structured and unstructured knowledge. To address\nthis task, we propose a TOD system with hybrid knowledge management, HyKnow. It\nextends the belief state to manage both structured and unstructured knowledge,\nand is the first end-to-end model that jointly optimizes dialog modeling\ngrounded on these two kinds of knowledge. We conduct experiments on the\nmodified version of MultiWOZ 2.1 dataset, where dialogs are grounded on hybrid\nknowledge. Experimental results show that HyKnow has strong end-to-end\nperformance compared to existing TOD systems. It also outperforms the pipeline\nknowledge management schemes, with higher unstructured knowledge retrieval\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:58:39 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:14:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Gao", "Silin", ""], ["Takanobu", "Ryuichi", ""], ["Peng", "Wei", ""], ["Liu", "Qun", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.06071", "submitter": "Dongdong Li", "authors": "Dongdong Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Miao Fan, Jun Ma,\n  Maarten de Rijke", "title": "Semi-Supervised Variational Reasoning for Medical Dialogue Generation", "comments": "Accepted by Sigir2021", "journal-ref": null, "doi": "10.1145/3404835.3462921", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical dialogue generation aims to provide automatic and accurate responses\nto assist physicians to obtain diagnosis and treatment suggestions in an\nefficient manner. In medical dialogues two key characteristics are relevant for\nresponse generation: patient states (such as symptoms, medication) and\nphysician actions (such as diagnosis, treatments). In medical scenarios\nlarge-scale human annotations are usually not available, due to the high costs\nand privacy requirements. Hence, current approaches to medical dialogue\ngeneration typically do not explicitly account for patient states and physician\nactions, and focus on implicit representation instead. We propose an end-to-end\nvariational reasoning approach to medical dialogue generation. To be able to\ndeal with a limited amount of labeled data, we introduce both patient state and\nphysician action as latent variables with categorical priors for explicit\npatient state tracking and physician policy learning, respectively. We propose\na variational Bayesian generative approach to approximate posterior\ndistributions over patient states and physician actions. We use an efficient\nstochastic gradient variational Bayes estimator to optimize the derived\nevidence lower bound, where a 2-stage collapsed inference method is proposed to\nreduce the bias during model training. A physician policy network composed of\nan action-classifier and two reasoning detectors is proposed for augmented\nreasoning ability. We conduct experiments on three datasets collected from\nmedical platforms. Our experimental results show that the proposed method\noutperforms state-of-the-art baselines in terms of objective and subjective\nevaluation metrics. Our experiments also indicate that our proposed\nsemi-supervised reasoning method achieves a comparable performance as\nstate-of-the-art fully supervised learning baselines for physician policy\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 04:14:35 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Li", "Dongdong", ""], ["Ren", "Zhaochun", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Fan", "Miao", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2105.06083", "submitter": "Zihan Wang", "authors": "Zihan Wang, Hongye Song, Zhaochun Ren, Pengjie Ren, Zhumin Chen,\n  Xiaozhong Liu, Hongsong Li, Maarten de Rijke", "title": "Cross-Domain Contract Element Extraction with a Bi-directional Feedback\n  Clause-Element Relation Network", "comments": "Accepted by SIGIR2021", "journal-ref": null, "doi": "10.1145/3404835.3462873", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contract element extraction (CEE) is the novel task of automatically\nidentifying and extracting legally relevant elements such as contract dates,\npayments, and legislation references from contracts. Automatic methods for this\ntask view it as a sequence labeling problem and dramatically reduce human\nlabor. However, as contract genres and element types may vary widely, a\nsignificant challenge for this sequence labeling task is how to transfer\nknowledge from one domain to another, i.e., cross-domain CEE. Cross-domain CEE\ndiffers from cross-domain named entity recognition (NER) in two important ways.\nFirst, contract elements are far more fine-grained than named entities, which\nhinders the transfer of extractors. Second, the extraction zones for\ncross-domain CEE are much larger than for cross-domain NER. As a result, the\ncontexts of elements from different domains can be more diverse. We propose a\nframework, the Bi-directional Feedback cLause-Element relaTion network\n(Bi-FLEET), for the cross-domain CEE task that addresses the above challenges.\nBi-FLEET has three main components: (1) a context encoder, (2) a clause-element\nrelation encoder, and (3) an inference layer. To incorporate invariant\nknowledge about element and clause types, a clause-element graph is constructed\nacross domains and a hierarchical graph neural network is adopted in the\nclause-element relation encoder. To reduce the influence of context variations,\na multi-task framework with a bi-directional feedback scheme is designed in the\ninference layer, conducting both clause classification and element extraction.\nThe experimental results over both cross-domain NER and CEE tasks show that\nBi-FLEET significantly outperforms state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 05:14:36 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Zihan", ""], ["Song", "Hongye", ""], ["Ren", "Zhaochun", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Liu", "Xiaozhong", ""], ["Li", "Hongsong", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2105.06097", "submitter": "Yuval Marton", "authors": "Yuval Marton, Asad Sayeed", "title": "Thematic fit bits: Annotation quality and quantity for event participant\n  representation", "comments": "8.5 pages before references, 11 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modeling thematic fit (a verb--argument compositional semantics task)\ncurrently requires a very large burden of data. We take a high-performing\nneural approach to modeling verb--argument fit, previously trained on a\nlinguistically machine-annotated large corpus, and replace corpus layers with\noutput from higher-quality taggers. Contrary to popular beliefs that, in the\ndeep learning era, more data is as effective as higher quality annotation, we\ndiscover that higher annotation quality dramatically reduces our data\nrequirement while demonstrating better supervised predicate-argument\nclassification. But in applying the model to a psycholinguistic task outside\nthe training objective, we saw only small gains in one of two thematic fit\nestimation tasks, and none in the other. We replicate previous studies while\nmodifying certain role representation details, and set a new state-of-the-art\nin event modeling, using a fraction of the data.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 06:13:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Marton", "Yuval", ""], ["Sayeed", "Asad", ""]]}, {"id": "2105.06216", "submitter": "Andrew Broekman", "authors": "Andrew Broekman and Linda Marshall", "title": "Linguistic Inspired Graph Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isomorphisms allow human cognition to transcribe a potentially unsolvable\nproblem from one domain to a different domain where the problem might be more\neasily addressed. Current approaches only focus on transcribing structural\ninformation from the source to target structure, ignoring semantic and\npragmatic information. Functional Language Theory presents five subconstructs\nfor the classification and understanding of languages. By deriving a mapping\nbetween the metamodels in linguistics and graph theory it will be shown that\ncurrently, no constructs exist in canonical graphs for the representation of\nsemantic and pragmatic information. It is found that further work needs to be\ndone to understand how graphs can be enriched to allow for isomorphisms to\ncapture semantic and pragmatic information. This capturing of additional\ninformation could lead to understandings of the source structure and enhanced\nmanipulations and interrogations of the contained relationships. Current\nmathematical graph structures in their general definition do not allow for the\nexpression of higher information levels of a source.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:16:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Broekman", "Andrew", ""], ["Marshall", "Linda", ""]]}, {"id": "2105.06232", "submitter": "Yan Xu", "authors": "Yan Xu, Etsuko Ishii, Zihan Liu, Genta Indra Winata, Dan Su, Andrea\n  Madotto, Pascale Fung", "title": "Retrieval-Free Knowledge-Grounded Dialogue Response Generation with\n  Adapters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To diversify and enrich generated dialogue responses, knowledge-grounded\ndialogue has been investigated in recent years. Despite the success of the\nexisting methods, they mainly follow the paradigm of retrieving the relevant\nsentences over a large corpus and augment the dialogues with explicit extra\ninformation, which is time- and resource-consuming. In this paper, we propose\nKnowExpert, an end-to-end framework to bypass the retrieval process by\ninjecting prior knowledge into the pre-trained language models with lightweight\nadapters. To the best of our knowledge, this is the first attempt to tackle\nthis task relying solely on a generation-based approach. Experimental results\nshow that KnowExpert performs comparably with the retrieval-based baselines,\ndemonstrating the potential of our proposed direction.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:33:23 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xu", "Yan", ""], ["Ishii", "Etsuko", ""], ["Liu", "Zihan", ""], ["Winata", "Genta Indra", ""], ["Su", "Dan", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2105.06247", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi\n  Zhou, Rick Siow Mong Goh", "title": "Video Corpus Moment Retrieval with Contrastive Learning", "comments": "11 pages, 7 figures and 6 tables. Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462874", "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of untrimmed and unsegmented videos, video corpus moment\nretrieval (VCMR) is to retrieve a temporal moment (i.e., a fraction of a video)\nthat semantically corresponds to a given text query. As video and text are from\ntwo distinct feature spaces, there are two general approaches to address VCMR:\n(i) to separately encode each modality representations, then align the two\nmodality representations for query processing, and (ii) to adopt fine-grained\ncross-modal interaction to learn multi-modal representations for query\nprocessing. While the second approach often leads to better retrieval accuracy,\nthe first approach is far more efficient. In this paper, we propose a Retrieval\nand Localization Network with Contrastive Learning (ReLoCLNet) for VCMR. We\nadopt the first approach and introduce two contrastive learning objectives to\nrefine video encoder and text encoder to learn video and text representations\nseparately but with better alignment for VCMR. The video contrastive learning\n(VideoCL) is to maximize mutual information between query and candidate video\nat video-level. The frame contrastive learning (FrameCL) aims to highlight the\nmoment region corresponds to the query at frame-level, within a video.\nExperimental results show that, although ReLoCLNet encodes text and video\nseparately for efficiency, its retrieval accuracy is comparable with baselines\nadopting cross-modal interaction learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:54:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Nan", "Guoshun", ""], ["Zhen", "Liangli", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2105.06337", "submitter": "Mikhail Kudinov", "authors": "Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail\n  Kudinov", "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, denoising diffusion probabilistic models and generative score\nmatching have shown high potential in modelling complex data distributions\nwhile stochastic calculus has provided a unified point of view on these\ntechniques allowing for flexible inference schemes. In this paper we introduce\nGrad-TTS, a novel text-to-speech model with score-based decoder producing\nmel-spectrograms by gradually transforming noise predicted by encoder and\naligned with text input by means of Monotonic Alignment Search. The framework\nof stochastic differential equations helps us to generalize conventional\ndiffusion probabilistic models to the case of reconstructing data from noise\nwith different parameters and allows to make this reconstruction flexible by\nexplicitly controlling trade-off between sound quality and inference speed.\nSubjective human evaluation shows that Grad-TTS is competitive with\nstate-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We\nwill make the code publicly available shortly.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:47:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Popov", "Vadim", ""], ["Vovk", "Ivan", ""], ["Gogoryan", "Vladimir", ""], ["Sadekova", "Tasnima", ""], ["Kudinov", "Mikhail", ""]]}, {"id": "2105.06354", "submitter": "Sian Gooding", "authors": "Sian Gooding, Yevgeni Berzak, Tony Mak, Matt Sharifi", "title": "Predicting Text Readability from Scrolling Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Judging the readability of text has many important applications, for instance\nwhen performing text simplification or when sourcing reading material for\nlanguage learners. In this paper, we present a 518 participant study which\ninvestigates how scrolling behaviour relates to the readability of a text. We\nmake our dataset publicly available and show that (1) there are statistically\nsignificant differences in the way readers interact with text depending on the\ntext level, (2) such measures can be used to predict the readability of text,\nand (3) the background of a reader impacts their reading interactions and the\nfactors contributing to text difficulty.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:27:00 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Gooding", "Sian", ""], ["Berzak", "Yevgeni", ""], ["Mak", "Tony", ""], ["Sharifi", "Matt", ""]]}, {"id": "2105.06456", "submitter": "Radu Tudor Ionescu", "authors": "Ana-Cristina Rogoz, Mihaela Gaman, Radu Tudor Ionescu", "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:54:37 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:24:26 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 16:35:45 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rogoz", "Ana-Cristina", ""], ["Gaman", "Mihaela", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2105.06457", "submitter": "Peng Qi", "authors": "Peng Qi, Jing Huang, Youzheng Wu, Xiaodong He, Bowen Zhou", "title": "Conversational AI Systems for Social Good: Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational artificial intelligence (ConvAI) systems have attracted much\nacademic and commercial attention recently, making significant progress on both\nfronts. However, little existing work discusses how these systems can be\ndeveloped and deployed for social good. In this paper, we briefly review the\nprogress the community has made towards better ConvAI systems and reflect on\nhow existing technologies can help advance social good initiatives from various\nangles that are unique for ConvAI, or not yet become common knowledge in the\ncommunity. We further discuss about the challenges ahead for ConvAI systems to\nbetter help us achieve these goals and highlight the risks involved in their\ndevelopment and deployment in the real world.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:56:04 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Qi", "Peng", ""], ["Huang", "Jing", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "2105.06511", "submitter": "Kaushik Roy", "authors": "Nathan Dolbir, Triyasha Dastidar, and Kaushik Roy", "title": "NLP is Not enough -- Contextualization of User Input in Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI chatbots have made vast strides in technology improvement in recent years\nand are already operational in many industries. Advanced Natural Language\nProcessing techniques, based on deep networks, efficiently process user\nrequests to carry out their functions. As chatbots gain traction, their\napplicability in healthcare is an attractive proposition due to the reduced\neconomic and people costs of an overburdened system. However, healthcare bots\nrequire safe and medically accurate information capture, which deep networks\naren't yet capable of due to user text and speech variations. Knowledge in\nsymbolic structures is more suited for accurate reasoning but cannot handle\nnatural language processing directly. Thus, in this paper, we study the effects\nof combining knowledge and neural representations on chatbot safety, accuracy,\nand understanding.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:57:32 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Dolbir", "Nathan", ""], ["Dastidar", "Triyasha", ""], ["Roy", "Kaushik", ""]]}, {"id": "2105.06514", "submitter": "Bansidhar Mangalwedhekar", "authors": "Bansidhar Mangalwedhekar", "title": "Distilling BERT for low complexity network training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the efficiency of transferring BERT learnings to low\ncomplexity models like BiLSTM, BiLSTM with attention and shallow CNNs using\nsentiment analysis on SST-2 dataset. It also compares the complexity of\ninference of the BERT model with these lower complexity models and underlines\nthe importance of these techniques in enabling high performance NLP models on\nedge devices like mobiles, tablets and MCU development boards like Raspberry Pi\netc. and enabling exciting new applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:09:22 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Mangalwedhekar", "Bansidhar", ""]]}, {"id": "2105.06546", "submitter": "Robert Hawkins", "authors": "Sonia K. Murthy and Robert D. Hawkins and Thomas L. Griffiths", "title": "Shades of confusion: Lexical uncertainty modulates ad hoc coordination\n  in an interactive communication task", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is substantial variability in the expectations that communication\npartners bring into interactions, creating the potential for misunderstandings.\nTo directly probe these gaps and our ability to overcome them, we propose a\ncommunication task based on color-concept associations. In Experiment 1, we\nestablish several key properties of the mental representations of these\nexpectations, or \\emph{lexical priors}, based on recent probabilistic theories.\nAssociations are more variable for abstract concepts, variability is\nrepresented as uncertainty within each individual, and uncertainty enables\naccurate predictions about whether others are likely to share the same\nassociation. In Experiment 2, we then examine the downstream consequences of\nthese representations for communication. Accuracy is initially low when\ncommunicating about concepts with more variable associations, but rapidly\nincreases as participants form ad hoc conventions. Together, our findings\nsuggest that people cope with variability by maintaining well-calibrated\nuncertainty about their partner and appropriately adaptable representations of\ntheir own.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:42:28 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Murthy", "Sonia K.", ""], ["Hawkins", "Robert D.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "2105.06597", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel\n  Galley, Jianfeng Gao, Bill Dolan", "title": "Joint Retrieval and Generation Training for Grounded Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large-scale pre-training such as GPT-3 allow seemingly\nhigh quality text to be generated from a given prompt. However, such generation\nsystems often suffer from problems of hallucinated facts, and are not\ninherently designed to incorporate useful external information. Grounded\ngeneration models appear to offer remedies, but their training typically relies\non rarely-available parallel data where corresponding information-relevant\ndocuments are provided for context. We propose a framework that alleviates this\ndata constraint by jointly training a grounded generator and document retriever\non the language model signal. The model learns to reward retrieval of the\ndocuments with the highest utility in generation, and attentively combines them\nusing a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We\ndemonstrate that both generator and retriever can take advantage of this joint\ntraining and work synergistically to produce more informative and relevant text\nin both prose and dialogue generation.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 00:11:38 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 02:24:28 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Yizhe", ""], ["Sun", "Siqi", ""], ["Gao", "Xiang", ""], ["Fang", "Yuwei", ""], ["Brockett", "Chris", ""], ["Galley", "Michel", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "2105.06603", "submitter": "Emily Allaway", "authors": "Emily Allaway, Malavika Srikanth, and Kathleen McKeown", "title": "Adversarial Learning for Zero-Shot Stance Detection on Social Media", "comments": "To appear in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stance detection on social media can help to identify and understand slanted\nnews or commentary in everyday life. In this work, we propose a new model for\nzero-shot stance detection on Twitter that uses adversarial learning to\ngeneralize across topics. Our model achieves state-of-the-art performance on a\nnumber of unseen test topics with minimal computational costs. In addition, we\nextend zero-shot stance detection to new topics, highlighting future directions\nfor zero-shot transfer.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 01:08:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Allaway", "Emily", ""], ["Srikanth", "Malavika", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2105.06679", "submitter": "Zhixing Tan", "authors": "Zhixing Tan, Maosong Sun, Yang Liu", "title": "Dynamic Multi-Branch Layers for On-Device Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of artificial intelligence (AI), there is a trend\nin moving AI applications such as neural machine translation (NMT) from cloud\nto mobile devices such as smartphones. Constrained by limited hardware\nresources and battery, the performance of on-device NMT systems is far from\nsatisfactory. Inspired by conditional computation, we propose to improve the\nperformance of on-device NMT systems with dynamic multi-branch layers.\nSpecifically, we design a layer-wise dynamic multi-branch network with only one\nbranch activated during training and inference. As not all branches are\nactivated during training, we propose shared-private reparameterization to\nensure sufficient training for each branch. At almost the same computational\ncost, our method achieves improvements of up to 1.7 BLEU points on the WMT14\nEnglish-German translation task and 1.8 BLEU points on the WMT20\nChinese-English translation task over the Transformer model, respectively.\nCompared with a strong baseline that also uses multiple branches, the proposed\nmethod is up to 1.6 times faster with the same number of parameters.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:32:53 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Tan", "Zhixing", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "2105.06681", "submitter": "Elena Volodina", "authors": "Elena Volodina, Yousuf Ali Mohammed, Julia Klezl", "title": "DaLAJ - a dataset for linguistic acceptability judgments for Swedish:\n  Format, baseline, sharing", "comments": "This is an extended version of an article accepted to the 10th\n  NLP4CALL workshop (2021), Link\\\"oping Electronic Conference Proceedings 177,\n  ISSN: 1650-3740 (online). In the extended version (available at arXiv) we\n  have added a description of an experiment and baseline results to the dataset\n  description accepted for NLP4CALL publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present DaLAJ 1.0, a Dataset for Linguistic Acceptability Judgments for\nSwedish, comprising 9 596 sentences in its first version; and the initial\nexperiment using it for the binary classification task. DaLAJ is based on the\nSweLL second language learner data, consisting of essays at different levels of\nproficiency. To make sure the dataset can be freely available despite the GDPR\nregulations, we have sentence-scrambled learner essays and removed part of the\nmetadata about learners, keeping for each sentence only information about the\nmother tongue and the level of the course where the essay has been written. We\nuse the normalized version of learner language as the basis for the DaLAJ\nsentences, and keep only one error per sentence. We repeat the same sentence\nfor each individual correction tag used in the sentence. For DaLAJ 1.0 we have\nused four error categories (out of 35 available in SweLL), all connected to\nlexical or word-building choices. Our baseline results for the binary\nclassification show an accuracy of 58% for DaLAJ 1.0 using BERT embeddings. The\ndataset is included in the SwedishGlue (Swe. SuperLim) benchmark. Below, we\ndescribe the format of the dataset, first experiments, our insights and the\nmotivation for the chosen approach to data sharing.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:37:38 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Volodina", "Elena", ""], ["Mohammed", "Yousuf Ali", ""], ["Klezl", "Julia", ""]]}, {"id": "2105.06717", "submitter": "Farhad Moghimifar", "authors": "Farhad Moghimifar, Lizhen Qu, Yue Zhuo, Gholamreza Haffari, Mahsa\n  Baktashmotlagh", "title": "Neural-Symbolic Commonsense Reasoner with Relation Predictors", "comments": "ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning aims to incorporate sets of commonsense facts,\nretrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about\nordinary situations. The dynamic nature of commonsense knowledge postulates\nmodels capable of performing multi-hop reasoning over new situations. This\nfeature also results in having large-scale sparse Knowledge Graphs, where such\nreasoning process is needed to predict relations between new events. However,\nexisting approaches in this area are limited by considering CKGs as a limited\nset of facts, thus rendering them unfit for reasoning over new unseen\nsituations and events. In this paper, we present a neural-symbolic reasoner,\nwhich is capable of reasoning over large-scale dynamic CKGs. The logic rules\nfor reasoning over CKGs are learned during training by our model. In addition\nto providing interpretable explanation, the learned logic rules help to\ngeneralise prediction to newly introduced events. Experimental results on the\ntask of link prediction on CKGs prove the effectiveness of our model by\noutperforming the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:54:25 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Moghimifar", "Farhad", ""], ["Qu", "Lizhen", ""], ["Zhuo", "Yue", ""], ["Haffari", "Gholamreza", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2105.06750", "submitter": "Seonghyeon Lee", "authors": "Seonghyeon Lee, Dongha Lee and Hwanjo Yu", "title": "Out-of-Manifold Regularization in Contextual Embedding Space for Text\n  Classification", "comments": "ACL2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies on neural networks with pre-trained weights (i.e., BERT) have\nmainly focused on a low-dimensional subspace, where the embedding vectors\ncomputed from input words (or their contexts) are located. In this work, we\npropose a new approach to finding and regularizing the remainder of the space,\nreferred to as out-of-manifold, which cannot be accessed through the words.\nSpecifically, we synthesize the out-of-manifold embeddings based on two\nembeddings obtained from actually-observed words, to utilize them for\nfine-tuning the network. A discriminator is trained to detect whether an input\nembedding is located inside the manifold or not, and simultaneously, a\ngenerator is optimized to produce new embeddings that can be easily identified\nas out-of-manifold by the discriminator. These two modules successfully\ncollaborate in a unified and end-to-end manner for regularizing the\nout-of-manifold. Our extensive evaluation on various text classification\nbenchmarks demonstrates the effectiveness of our approach, as well as its good\ncompatibility with existing data augmentation techniques which aim to enhance\nthe manifold.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:17:59 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lee", "Seonghyeon", ""], ["Lee", "Dongha", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2105.06752", "submitter": "Xin Su", "authors": "Xin Su, Timothy Miller, Xiyu Ding, Majid Afshar and Dmitriy Dligach", "title": "Classifying Long Clinical Documents with Pre-trained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic phenotyping is a task of identifying cohorts of patients that match\na predefined set of criteria. Phenotyping typically involves classifying long\nclinical documents that contain thousands of tokens. At the same time, recent\nstate-of-art transformer-based pre-trained language models limit the input to a\nfew hundred tokens (e.g. 512 tokens for BERT). We evaluate several strategies\nfor incorporating pre-trained sentence encoders into document-level\nrepresentations of clinical text, and find that hierarchical transformers\nwithout pre-training are competitive with task pre-trained models.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:24:58 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Su", "Xin", ""], ["Miller", "Timothy", ""], ["Ding", "Xiyu", ""], ["Afshar", "Majid", ""], ["Dligach", "Dmitriy", ""]]}, {"id": "2105.06762", "submitter": "Yulong Chen", "authors": "Yulong Chen, Yang Liu, Liang Chen and Yue Zhang", "title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset", "comments": "ACL findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proposal of large-scale datasets has facilitated research on deep neural\nmodels for news summarization. Deep learning can also be potentially useful for\nspoken dialogue summarization, which can benefit a range of real-life scenarios\nincluding customer service management and medication tracking. To this end, we\npropose DialogSum, a large-scale labeled dialogue summarization dataset. We\nconduct empirical analysis on DialogSum using state-of-the-art neural\nsummarizers. Experimental results show unique challenges in dialogue\nsummarization, such as spoken terms, special discourse structures, coreferences\nand ellipsis, pragmatics and social common sense, which require specific\nrepresentation learning technologies to better deal with.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 11:12:40 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 08:32:00 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 03:46:14 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 06:34:42 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chen", "Yulong", ""], ["Liu", "Yang", ""], ["Chen", "Liang", ""], ["Zhang", "Yue", ""]]}, {"id": "2105.06804", "submitter": "Yongliang Shen", "authors": "Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang, Wen Wang and Weiming\n  Lu", "title": "Locate and Label: A Two-stage Identifier for Nested Named Entity\n  Recognition", "comments": "Accepted to ACL 2021, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a well-studied task in natural language\nprocessing. Traditional NER research only deals with flat entities and ignores\nnested entities. The span-based methods treat entity recognition as a span\nclassification task. Although these methods have the innate ability to handle\nnested NER, they suffer from high computational cost, ignorance of boundary\ninformation, under-utilization of the spans that partially match with entities,\nand difficulties in long entity recognition. To tackle these issues, we propose\na two-stage entity identifier. First we generate span proposals by filtering\nand boundary regression on the seed spans to locate the entities, and then\nlabel the boundary-adjusted span proposals with the corresponding categories.\nOur method effectively utilizes the boundary information of entities and\npartially matched spans during training. Through boundary regression, entities\nof any length can be covered theoretically, which improves the ability to\nrecognize long entities. In addition, many low-quality seed spans are filtered\nout in the first stage, which reduces the time complexity of inference.\nExperiments on nested NER datasets demonstrate that our proposed method\noutperforms previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 12:52:34 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:01:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Shen", "Yongliang", ""], ["Ma", "Xinyin", ""], ["Tan", "Zeqi", ""], ["Zhang", "Shuai", ""], ["Wang", "Wen", ""], ["Lu", "Weiming", ""]]}, {"id": "2105.06813", "submitter": "Guilherme Moraes Rosa", "authors": "Guilherme Moraes Rosa, Luiz Henrique Bonifacio, Leandro Rodrigues de\n  Souza, Roberto Lotufo and Rodrigo Nogueira", "title": "A cost-benefit analysis of cross-lingual transfer methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective method for cross-lingual transfer is to fine-tune a bilingual or\nmultilingual model on a supervised dataset in one language and evaluating it on\nanother language in a zero-shot manner. Translating examples at training time\nor inference time are also viable alternatives. However, there are costs\nassociated with these methods that are rarely addressed in the literature. In\nthis work, we analyze cross-lingual methods in terms of their effectiveness\n(e.g., accuracy), development and deployment costs, as well as their latencies\nat inference time. Our experiments on three tasks indicate that the best\ncross-lingual method is highly task-dependent. Finally, by combining zero-shot\nand translation methods, we achieve the state-of-the-art in two of the three\ndatasets used in this work. Based on these results, we question the need for\nmanually labeled training data in a target language. Code, models and\ntranslated datasets are available at\nhttps://github.com/unicamp-dl/cross-lingual-analysis\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:21:12 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 14:09:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rosa", "Guilherme Moraes", ""], ["Bonifacio", "Luiz Henrique", ""], ["de Souza", "Leandro Rodrigues", ""], ["Lotufo", "Roberto", ""], ["Nogueira", "Rodrigo", ""]]}, {"id": "2105.06829", "submitter": "Yubo Xie", "authors": "Yubo Xie, Pearl Pu", "title": "Generating Empathetic Responses with a Large Scale Dialog Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of empathetic response generation aims at generating syntactically\ncorrect and, more importantly, emotionally appropriate responses following\nprevious dialog turns. Existing models either directly incorporate pre-defined\nemotion information to guide the response generation, or use deterministic\nrules to decide the response emotion, ignoring the subtle emotion interactions\ncaptured in human conversations. With the advent of advanced language models,\nit is possible to learn the nuanced emotional exchanges captured in natural\nlanguage dialogs. To fully explore the range of emotions and dialog intents, it\nis important to curate a dataset large enough to shed light on the general\nunderstanding of human emotional interactions in our conversations. In this\npaper, we describe in detail the curation process of a large-scale dialog\ndataset where each utterance is labeled with one of 32 emotions and 9 intent\ncategories. We then show how to build a multi-turn empathetic dialog model that\nperforms well compared to its baselines over 6,000 human evaluated instances.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:45:40 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Xie", "Yubo", ""], ["Pu", "Pearl", ""]]}, {"id": "2105.06839", "submitter": "Yue Zhang", "authors": "Yue Zhang, Quan Guo, Parisa Kordjamshidi", "title": "Towards Navigation by Reasoning over Spatial Configurations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the navigation problem where the agent follows natural language\ninstructions while observing the environment. Focusing on language\nunderstanding, we show the importance of spatial semantics in grounding\nnavigation instructions into visual perceptions. We propose a neural agent that\nuses the elements of spatial configurations and investigate their influence on\nthe navigation agent's reasoning ability. Moreover, we model the sequential\nexecution order and align visual objects with spatial configurations in the\ninstruction. Our neural agent improves strong baselines on the seen\nenvironments and shows competitive performance on the unseen environments.\nAdditionally, the experimental results demonstrate that explicit modeling of\nspatial semantic elements in the instructions can improve the grounding and\nspatial reasoning of the model.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:04:23 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Yue", ""], ["Guo", "Quan", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2105.06912", "submitter": "Chien-Sheng Wu", "authors": "Chien-Sheng Wu, Andrea Madotto, Wenhao Liu, Pascale Fung, Caiming\n  Xiong", "title": "QAConv: Question Answering on Informative Conversations", "comments": "Data and code are available at https://github.com/salesforce/QAConv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces QAConv, a new question answering (QA) dataset that uses\nconversations as a knowledge source. We focus on informative conversations\nincluding business emails, panel discussions, and work channels. Unlike\nopen-domain and task-oriented dialogues, these conversations are usually long,\ncomplex, asynchronous, and involve strong domain knowledge. In total, we\ncollect 34,204 QA pairs, including span-based, free-form, and unanswerable\nquestions, from 10,259 selected conversations with both human-written and\nmachine-generated questions. We segment long conversations into chunks, and use\na question generator and dialogue summarizer as auxiliary tools to collect\nmulti-hop questions. The dataset has two testing scenarios, chunk mode and full\nmode, depending on whether the grounded chunk is provided or retrieved from a\nlarge conversational pool. Experimental results show that state-of-the-art QA\nsystems trained on existing QA datasets have limited zero-shot ability and tend\nto predict our questions as unanswerable. Fine-tuning such systems on our\ncorpus can achieve significant improvement up to 23.6% and 13.6% in both chunk\nmode and full mode, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:53:05 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wu", "Chien-Sheng", ""], ["Madotto", "Andrea", ""], ["Liu", "Wenhao", ""], ["Fung", "Pascale", ""], ["Xiong", "Caiming", ""]]}, {"id": "2105.06947", "submitter": "Huiyuan Lai", "authors": "Huiyuan Lai, Antonio Toral, Malvina Nissim", "title": "Thank you BART! Rewarding Pre-Trained Models Improves Formality Style\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of parallel data causes formality style transfer models to have\nscarce success in preserving content. We show that fine-tuning pre-trained\nlanguage (GPT-2) and sequence-to-sequence (BART) models boosts content\npreservation, and that this is possible even with limited amounts of parallel\ndata. Augmenting these models with rewards that target style and content -- the\ntwo core aspects of the task -- we achieve a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:39:22 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 08:45:10 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Lai", "Huiyuan", ""], ["Toral", "Antonio", ""], ["Nissim", "Malvina", ""]]}, {"id": "2105.06950", "submitter": "Yun-Wei Chu", "authors": "Chi-Yang Hsu, Yun-Wei Chu, Ting-Hao 'Kenneth' Huang, Lun-Wei Ku", "title": "Plot and Rework: Modeling Storylines for Visual Storytelling", "comments": "9 pages, ACL-IJCNLP 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Writing a coherent and engaging story is not easy. Creative writers use their\nknowledge and worldview to put disjointed elements together to form a coherent\nstoryline, and work and rework iteratively toward perfection. Automated visual\nstorytelling (VIST) models, however, make poor use of external knowledge and\niterative generation when attempting to create stories. This paper introduces\nPR-VIST, a framework that represents the input image sequence as a story graph\nin which it finds the best path to form a storyline. PR-VIST then takes this\npath and learns to generate the final story via an iterative training process.\nThis framework produces stories that are superior in terms of diversity,\ncoherence, and humanness, per both automatic and human evaluations. An ablation\nstudy shows that both plotting and reworking contribute to the model's\nsuperiority.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:41:29 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 19:13:55 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 14:59:28 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hsu", "Chi-Yang", ""], ["Chu", "Yun-Wei", ""], ["Huang", "Ting-Hao 'Kenneth'", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2105.06965", "submitter": "Grusha Prasad", "authors": "Shauli Ravfogel, Grusha Prasad, Tal Linzen, Yoav Goldberg", "title": "Counterfactual Interventions Reveal the Causal Effect of Relative Clause\n  Representations on Agreement Prediction", "comments": "Equal contribution by SR and GP. Uploaded new version to fix typo in\n  the abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When language models process syntactically complex sentences, do they use\nabstract syntactic information present in these sentences in a manner that is\nconsistent with the grammar of English, or do they rely solely on a set of\nheuristics? We propose a method to tackle this question, AlterRep. For any\nlinguistic feature in the sentence, AlterRep allows us to generate\ncounterfactual representations by altering how this feature is encoded, while\nleaving all other aspects of the original representation intact. Then, by\nmeasuring the change in a models' word prediction with these counterfactual\nrepresentations in different sentences, we can draw causal conclusions about\nthe contexts in which the model uses the linguistic feature (if any). Applying\nthis method to study how BERT uses relative clause (RC) span information, we\nfound that BERT uses information about RC spans during agreement prediction\nusing the linguistically correct strategy. We also found that counterfactual\nrepresentations generated for a specific RC subtype influenced the number\nprediction in sentences with other RC subtypes, suggesting that information\nabout RC boundaries was encoded abstractly in BERT's representation.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:11:55 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 22:17:56 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ravfogel", "Shauli", ""], ["Prasad", "Grusha", ""], ["Linzen", "Tal", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2105.06977", "submitter": "Kayo Yin", "authors": "Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, Andr\\'e\n  F. T. Martins, Graham Neubig", "title": "Do Context-Aware Translation Models Pay the Right Attention?", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:32:24 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:26:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yin", "Kayo", ""], ["Fernandes", "Patrick", ""], ["Pruthi", "Danish", ""], ["Chaudhary", "Aditi", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Neubig", "Graham", ""]]}, {"id": "2105.06982", "submitter": "Haoran Li", "authors": "Haoran Li, Arash Einolghozati, Srinivasan Iyer, Bhargavi Paranjape,\n  Yashar Mehdad, Sonal Gupta, Marjan Ghazvininejad", "title": "EASE: Extractive-Abstractive Summarization with Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current abstractive summarization systems outperform their extractive\ncounterparts, but their widespread adoption is inhibited by the inherent lack\nof interpretability. To achieve the best of both worlds, we propose EASE, an\nextractive-abstractive framework for evidence-based text generation and apply\nit to document summarization. We present an explainable summarization system\nbased on the Information Bottleneck principle that is jointly trained for\nextraction and abstraction in an end-to-end fashion. Inspired by previous\nresearch that humans use a two-stage framework to summarize long documents\n(Jing and McKeown, 2000), our framework first extracts a pre-defined amount of\nevidence spans as explanations and then generates a summary using only the\nevidence. Using automatic and human evaluations, we show that explanations from\nour framework are more relevant than simple baselines, without substantially\nsacrificing the quality of the generated summary.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:45:06 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Li", "Haoran", ""], ["Einolghozati", "Arash", ""], ["Iyer", "Srinivasan", ""], ["Paranjape", "Bhargavi", ""], ["Mehdad", "Yashar", ""], ["Gupta", "Sonal", ""], ["Ghazvininejad", "Marjan", ""]]}, {"id": "2105.06990", "submitter": "Saurabh Kulshreshtha", "authors": "Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers and Anna Rumshisky", "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers", "comments": "Accepted as long paper at Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple studies have shown that Transformers are remarkably robust to\npruning. Contrary to this received wisdom, we demonstrate that pre-trained\nTransformer encoders are surprisingly fragile to the removal of a very small\nnumber of features in the layer outputs (<0.0001% of model weights). In case of\nBERT and other pre-trained encoder Transformers, the affected component is the\nscaling factors and biases in the LayerNorm. The outliers are high-magnitude\nnormalization parameters that emerge early in pre-training and show up\nconsistently in the same dimensional position throughout the model. We show\nthat disabling them significantly degrades both the MLM loss and the downstream\ntask performance. This effect is observed across several BERT-family models and\nother popular pre-trained Transformer architectures, including BART, XLNet and\nELECTRA; we also show a similar effect in GPT-2.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:54:28 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 18:09:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kovaleva", "Olga", ""], ["Kulshreshtha", "Saurabh", ""], ["Rogers", "Anna", ""], ["Rumshisky", "Anna", ""]]}, {"id": "2105.07071", "submitter": "Minhua Wu", "authors": "Swayambhu Nath Ray, Minhua Wu, Anirudh Raju, Pegah Ghahremani,\n  Raghavendra Bilgi, Milind Rao, Harish Arsikere, Ariya Rastrow, Andreas\n  Stolcke, Jasha Droppo", "title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent\n  Front-End", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehending the overall intent of an utterance helps a listener recognize\nthe individual words spoken. Inspired by this fact, we perform a novel study of\nthe impact of explicitly incorporating intent representations as additional\ninformation to improve a recurrent neural network-transducer (RNN-T) based\nautomatic speech recognition (ASR) system. An audio-to-intent (A2I) model\nencodes the intent of the utterance in the form of embeddings or posteriors,\nand these are used as auxiliary inputs for RNN-T training and inference.\nExperimenting with a 50k-hour far-field English speech corpus, this study shows\nthat when running the system in non-streaming mode, where intent representation\nis extracted from the entire utterance and then used to bias streaming RNN-T\nsearch from the start, it provides a 5.56% relative word error rate reduction\n(WERR). On the other hand, a streaming system using per-frame intent posteriors\nas extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A\nfurther detailed analysis of the streaming system indicates that our proposed\nmethod brings especially good gain on media-playing related intents (e.g. 9.12%\nrelative WERR on PlayMusicIntent).\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 21:19:30 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 19:19:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ray", "Swayambhu Nath", ""], ["Wu", "Minhua", ""], ["Raju", "Anirudh", ""], ["Ghahremani", "Pegah", ""], ["Bilgi", "Raghavendra", ""], ["Rao", "Milind", ""], ["Arsikere", "Harish", ""], ["Rastrow", "Ariya", ""], ["Stolcke", "Andreas", ""], ["Droppo", "Jasha", ""]]}, {"id": "2105.07109", "submitter": "Evan Hernandez", "authors": "Evan Hernandez and Jacob Andreas", "title": "The Low-Dimensional Linear Geometry of Contextualized Word\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box probing models can reliably extract linguistic features like tense,\nnumber, and syntactic role from pretrained word representations. However, the\nmanner in which these features are encoded in representations remains poorly\nunderstood. We present a systematic study of the linear geometry of\ncontextualized word representations in ELMO and BERT. We show that a variety of\nlinguistic features (including structured dependency relationships) are encoded\nin low-dimensional subspaces. We then refine this geometric picture, showing\nthat there are hierarchical relations between the subspaces encoding general\nlinguistic categories and more specific ones, and that low-dimensional feature\nencodings are distributed rather than aligned to individual neurons. Finally,\nwe demonstrate that these linear subspaces are causally related to model\nbehavior, and can be used to perform fine-grained manipulation of BERT's output\ndistribution.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 00:58:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hernandez", "Evan", ""], ["Andreas", "Jacob", ""]]}, {"id": "2105.07122", "submitter": "Qingxiu Dong", "authors": "Qingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng, Shoujie Tong, Haoran\n  Meng, Lin Xu, Tianyu Liu, Zuifang Sui, Weidong Zhan, Sujian Li and Zhongyu\n  Wei", "title": "Premise-based Multimodal Reasoning: A Human-like Cognitive Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning is one of the major challenges of Human-like AI and has recently\nattracted intensive attention from natural language processing (NLP)\nresearchers. However, cross-modal reasoning needs further research. For\ncross-modal reasoning, we observe that most methods fall into shallow feature\nmatching without in-depth human-like reasoning.The reason lies in that existing\ncross-modal tasks directly ask questions for a image. However, human reasoning\nin real scenes is often made under specific background information, a process\nthat is studied by the ABC theory in social psychology. We propose a shared\ntask named \"Premise-based Multimodal Reasoning\" (PMR), which requires\nparticipating models to reason after establishing a profound understanding of\nbackground information. We believe that the proposed PMR would contribute to\nand help shed a light on human-like in-depth reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 03:25:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dong", "Qingxiu", ""], ["Qin", "Ziwei", ""], ["Xia", "Heming", ""], ["Feng", "Tian", ""], ["Tong", "Shoujie", ""], ["Meng", "Haoran", ""], ["Xu", "Lin", ""], ["Liu", "Tianyu", ""], ["Sui", "Zuifang", ""], ["Zhan", "Weidong", ""], ["Li", "Sujian", ""], ["Wei", "Zhongyu", ""]]}, {"id": "2105.07144", "submitter": "Jason Wei", "authors": "Jason Wei, Clara Meister, and Ryan Cotterell", "title": "A Cognitive Regularizer for Language Modeling", "comments": "ACL 2021 Camera-ready (fixed ordering of affiliation emojis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The uniform information density (UID) hypothesis, which posits that speakers\nbehaving optimally tend to distribute information uniformly across a linguistic\nsignal, has gained traction in psycholinguistics as an explanation for certain\nsyntactic, morphological, and prosodic choices. In this work, we explore\nwhether the UID hypothesis can be operationalized as an inductive bias for\nstatistical language modeling. Specifically, we augment the canonical MLE\nobjective for training language models with a regularizer that encodes UID. In\nexperiments on ten languages spanning five language families, we find that\nusing UID regularization consistently improves perplexity in language models,\nhaving a larger effect when training data is limited. Moreover, via an analysis\nof generated sequences, we find that UID-regularized language models have other\ndesirable properties, e.g., they generate text that is more lexically diverse.\nOur results not only suggest that UID is a reasonable inductive bias for\nlanguage modeling, but also provide an alternative validation of the UID\nhypothesis using modern-day NLP tools.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 05:37:42 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 20:44:20 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 01:46:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wei", "Jason", ""], ["Meister", "Clara", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2105.07148", "submitter": "Wei Liu", "authors": "Wei Liu, Xiyan Fu, Yue Zhang and Wenming Xiao", "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter", "comments": "Accepted by ACL2021(Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 06:13:39 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 16:00:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Liu", "Wei", ""], ["Fu", "Xiyan", ""], ["Zhang", "Yue", ""], ["Xiao", "Wenming", ""]]}, {"id": "2105.07149", "submitter": "Qu Cui", "authors": "Qu Cui, Shujian Huang, Jiahuan Li, Xiang Geng, Zaixiang Zheng, Guoping\n  Huang, Jiajun Chen", "title": "DirectQE: Direct Pretraining for Machine Translation Quality Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation Quality Estimation (QE) is a task of predicting the\nquality of machine translations without relying on any reference. Recently, the\npredictor-estimator framework trains the predictor as a feature extractor,\nwhich leverages the extra parallel corpora without QE labels, achieving\npromising QE performance. However, we argue that there are gaps between the\npredictor and the estimator in both data quality and training objectives, which\npreclude QE models from benefiting from a large number of parallel corpora more\ndirectly. We propose a novel framework called DirectQE that provides a direct\npretraining for QE tasks. In DirectQE, a generator is trained to produce pseudo\ndata that is closer to the real QE data, and a detector is pretrained on these\ndata with novel objectives that are akin to the QE task. Experiments on widely\nused benchmarks show that DirectQE outperforms existing methods, without using\nany pretraining models such as BERT. We also give extensive analyses showing\nhow fixing the two gaps contributes to our improvements.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 06:18:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cui", "Qu", ""], ["Huang", "Shujian", ""], ["Li", "Jiahuan", ""], ["Geng", "Xiang", ""], ["Zheng", "Zaixiang", ""], ["Huang", "Guoping", ""], ["Chen", "Jiajun", ""]]}, {"id": "2105.07205", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, Yuexian Zou", "title": "Rethinking Skip Connection with Layer Normalization in Transformers and\n  ResNets", "comments": "Accepted by COLING2020 (The 28th International Conference on\n  Computational Linguistics (COLING 2020))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skip connection, is a widely-used technique to improve the performance and\nthe convergence of deep neural networks, which is believed to relieve the\ndifficulty in optimization due to non-linearity by propagating a linear\ncomponent through the neural network layers. However, from another point of\nview, it can also be seen as a modulating mechanism between the input and the\noutput, with the input scaled by a pre-defined value one. In this work, we\ninvestigate how the scale factors in the effectiveness of the skip connection\nand reveal that a trivial adjustment of the scale will lead to spurious\ngradient exploding or vanishing in line with the deepness of the models, which\ncould be addressed by normalization, in particular, layer normalization, which\ninduces consistent improvements over the plain skip connection. Inspired by the\nfindings, we further propose to adaptively adjust the scale of the input by\nrecursively applying skip connection with layer normalization, which promotes\nthe performance substantially and generalizes well across diverse tasks\nincluding both machine translation and image classification datasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 11:44:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Fenglin", ""], ["Ren", "Xuancheng", ""], ["Zhang", "Zhiyuan", ""], ["Sun", "Xu", ""], ["Zou", "Yuexian", ""]]}, {"id": "2105.07220", "submitter": "Mitja Kulczynski", "authors": "Murphy Berzish, Joel D. Day, Vijay Ganesh, Mitja Kulczynski, Florin\n  Manea, Federico Mora, Dirk Nowotka", "title": "String Theories involving Regular Membership Predicates: From Practice\n  to Theory and Back", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.07253", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widespread use of string solvers in formal analysis of string-heavy programs\nhas led to a growing demand for more efficient and reliable techniques which\ncan be applied in this context, especially for real-world cases. Designing an\nalgorithm for the (generally undecidable) satisfiability problem for systems of\nstring constraints requires a thorough understanding of the structure of\nconstraints present in the targeted cases. In this paper, we investigate\nbenchmarks presented in the literature containing regular expression membership\npredicates, extract different first order logic theories, and prove their\ndecidability, resp. undecidability. Notably, the most common theories in\nreal-world benchmarks are PSPACE-complete and directly lead to the\nimplementation of a more efficient algorithm to solving string constraints.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 13:13:50 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Berzish", "Murphy", ""], ["Day", "Joel D.", ""], ["Ganesh", "Vijay", ""], ["Kulczynski", "Mitja", ""], ["Manea", "Florin", ""], ["Mora", "Federico", ""], ["Nowotka", "Dirk", ""]]}, {"id": "2105.07263", "submitter": "Nicholas Andrews", "authors": "Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop, Nicholas\n  Andrews", "title": "A Deep Metric Learning Approach to Account Linking", "comments": "13 pages; to be published in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of linking social media accounts that belong to the same\nauthor in an automated fashion on the basis of the content and metadata of\ntheir corresponding document streams. We focus on learning an embedding that\nmaps variable-sized samples of user activity -- ranging from single posts to\nentire months of activity -- to a vector space, where samples by the same\nauthor map to nearby points. The approach does not require human-annotated data\nfor training purposes, which allows us to leverage large amounts of social\nmedia content. The proposed model outperforms several competitive baselines\nunder a novel evaluation framework modeled after established recognition\nbenchmarks in other domains. Our method achieves high linking accuracy, even\nwith small samples from accounts not seen at training time, a prerequisite for\npractical applications of the proposed linking framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 17:06:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Khan", "Aleem", ""], ["Fleming", "Elizabeth", ""], ["Schofield", "Noah", ""], ["Bishop", "Marcus", ""], ["Andrews", "Nicholas", ""]]}, {"id": "2105.07270", "submitter": "Marcel Wever", "authors": "Marie-Luis Merten, Marcel Wever, Michaela Geierhos, Doris Tophinke,\n  Eyke H\\\"ullermeier", "title": "Annotation Uncertainty in the Context of Grammatical Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper elaborates on the notion of uncertainty in the context of\nannotation in large text corpora, specifically focusing on (but not limited to)\nhistorical languages. Such uncertainty might be due to inherent properties of\nthe language, for example, linguistic ambiguity and overlapping categories of\nlinguistic description, but could also be caused by lacking annotation\nexpertise. By examining annotation uncertainty in more detail, we identify the\nsources and deepen our understanding of the nature and different types of\nuncertainty encountered in daily annotation practice. Moreover, some practical\nimplications of our theoretical findings are also discussed. Last but not\nleast, this article can be seen as an attempt to reconcile the perspectives of\nthe main scientific disciplines involved in corpus projects, linguistics and\ncomputer science, to develop a unified view and to highlight the potential\nsynergies between these disciplines.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 17:45:29 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 06:56:43 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Merten", "Marie-Luis", ""], ["Wever", "Marcel", ""], ["Geierhos", "Michaela", ""], ["Tophinke", "Doris", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2105.07314", "submitter": "Aakanksha Naik", "authors": "Luke Breitfeller, Aakanksha Naik, Carolyn Rose", "title": "STAGE: Tool for Automated Extraction of Semantic Time Cues to Enrich\n  Neural Temporal Ordering Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite achieving state-of-the-art accuracy on temporal ordering of events,\nneural models showcase significant gaps in performance. Our work seeks to fill\none of these gaps by leveraging an under-explored dimension of textual\nsemantics: rich semantic information provided by explicit textual time cues. We\ndevelop STAGE, a system that consists of a novel temporal framework and a\nparser that can automatically extract time cues and convert them into\nrepresentations suitable for integration with neural models. We demonstrate the\nutility of extracted cues by integrating them with an event ordering model\nusing a joint BiLSTM and ILP constraint architecture. We outline the\nfunctionality of the 3-part STAGE processing approach, and show two methods of\nintegrating its representations with the BiLSTM-ILP model: (i) incorporating\nsemantic cues as additional features, and (ii) generating new constraints from\nsemantic cues to be enforced in the ILP. We demonstrate promising results on\ntwo event ordering datasets, and highlight important issues in semantic cue\nrepresentation and integration for future research.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 23:34:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Breitfeller", "Luke", ""], ["Naik", "Aakanksha", ""], ["Rose", "Carolyn", ""]]}, {"id": "2105.07316", "submitter": "Ibrahim Sharaf", "authors": "Rob van der Goot, Ibrahim Sharaf, Aizhan Imankulova, Ahmet \\\"Ust\\\"un,\n  Marija Stepanovi\\'c, Alan Ramponi, Siti Oryza Khairunnisa, Mamoru Komachi,\n  Barbara Plank", "title": "From Masked Language Modeling to Translation: Non-English Auxiliary\n  Tasks Improve Zero-shot Spoken Language Understanding", "comments": "To appear in the proceedings of NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of publicly available evaluation data for low-resource languages\nlimits progress in Spoken Language Understanding (SLU). As key tasks like\nintent classification and slot filling require abundant training data, it is\ndesirable to reuse existing data in high-resource languages to develop models\nfor low-resource scenarios. We introduce xSID, a new benchmark for\ncross-lingual Slot and Intent Detection in 13 languages from 6 language\nfamilies, including a very low-resource dialect. To tackle the challenge, we\npropose a joint learning approach, with English SLU training data and\nnon-English auxiliary tasks from raw text, syntax and translation for transfer.\nWe study two setups which differ by type and language coverage of the\npre-trained embeddings. Our results show that jointly learning the main tasks\nwith masked language modeling is effective for slots, while machine translation\ntransfer works best for intent classification.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 23:51:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["van der Goot", "Rob", ""], ["Sharaf", "Ibrahim", ""], ["Imankulova", "Aizhan", ""], ["\u00dcst\u00fcn", "Ahmet", ""], ["Stepanovi\u0107", "Marija", ""], ["Ramponi", "Alan", ""], ["Khairunnisa", "Siti Oryza", ""], ["Komachi", "Mamoru", ""], ["Plank", "Barbara", ""]]}, {"id": "2105.07319", "submitter": "Chengqi Zhao", "authors": "Chengqi Zhao and Zhicheng Liu and Jian Tong and Tao Wang and Mingxuan\n  Wang and Rong Ye and Qianqian Dong and Jun Cao and Lei Li", "title": "The Volctrans Neural Speech Translation System for IWSLT 2021", "comments": "IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 8.1 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We will\npublish our code and model to facilitate both future research works and\nindustrial applications.\n  This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 7.9 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We release\nour code and model at\n\\url{https://github.com/bytedance/neurst/tree/master/examples/iwslt21} to\nfacilitate both future research works and industrial applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 00:11:59 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 10:39:45 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhao", "Chengqi", ""], ["Liu", "Zhicheng", ""], ["Tong", "Jian", ""], ["Wang", "Tao", ""], ["Wang", "Mingxuan", ""], ["Ye", "Rong", ""], ["Dong", "Qianqian", ""], ["Cao", "Jun", ""], ["Li", "Lei", ""]]}, {"id": "2105.07400", "submitter": "Samia Touileb", "authors": "Samia Touileb and Jeremy Barnes", "title": "The interplay between language similarity and script on a novel\n  multi-layer Algerian dialect corpus", "comments": "Accepted at Findings of ACL: ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen a rise in interest for cross-lingual transfer between\nlanguages with similar typology, and between languages of various scripts.\nHowever, the interplay between language similarity and difference in script on\ncross-lingual transfer is a less studied problem. We explore this interplay on\ncross-lingual transfer for two supervised tasks, namely part-of-speech tagging\nand sentiment analysis. We introduce a newly annotated corpus of Algerian\nuser-generated comments comprising parallel annotations of Algerian written in\nLatin, Arabic, and code-switched scripts, as well as annotations for sentiment\nand topic categories. We perform baseline experiments by fine-tuning\nmulti-lingual language models. We further explore the effect of script vs.\nlanguage similarity in cross-lingual transfer by fine-tuning multi-lingual\nmodels on languages which are a) typologically distinct, but use the same\nscript, b) typologically similar, but use a distinct script, or c) are\ntypologically similar and use the same script. We find there is a delicate\nrelationship between script and typology for part-of-speech, while sentiment\nanalysis is less sensitive.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:22:21 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 08:57:31 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 14:45:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Touileb", "Samia", ""], ["Barnes", "Jeremy", ""]]}, {"id": "2105.07452", "submitter": "Bai Li", "authors": "Bai Li, Zining Zhu, Guillaume Thomas, Yang Xu, Frank Rudzicz", "title": "How is BERT surprised? Layerwise detection of linguistic anomalies", "comments": "ACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer language models have shown remarkable ability in detecting when a\nword is anomalous in context, but likelihood scores offer no information about\nthe cause of the anomaly. In this work, we use Gaussian models for density\nestimation at intermediate layers of three language models (BERT, RoBERTa, and\nXLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark.\nIn lower layers, surprisal is highly correlated to low token frequency, but\nthis correlation diminishes in upper layers. Next, we gather datasets of\nmorphosyntactic, semantic, and commonsense anomalies from psycholinguistic\nstudies; we find that the best performing model RoBERTa exhibits surprisal in\nearlier layers when the anomaly is morphosyntactic than when it is semantic,\nwhile commonsense anomalies do not exhibit surprisal at any intermediate layer.\nThese results suggest that language models employ separate mechanisms to detect\ndifferent types of linguistic anomalies.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 15:20:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Bai", ""], ["Zhu", "Zining", ""], ["Thomas", "Guillaume", ""], ["Xu", "Yang", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2105.07464", "submitter": "Ning Ding", "authors": "Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie,\n  Hai-Tao Zheng, Zhiyuan Liu", "title": "Few-NERD: A Few-Shot Named Entity Recognition Dataset", "comments": "Accepted by ACL-IJCNLP 2021 (long paper), update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 15:53:17 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 08:50:14 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 06:56:03 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 07:23:06 GMT"}, {"version": "v5", "created": "Sun, 20 Jun 2021 14:55:18 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ding", "Ning", ""], ["Xu", "Guangwei", ""], ["Chen", "Yulin", ""], ["Wang", "Xiaobin", ""], ["Han", "Xu", ""], ["Xie", "Pengjun", ""], ["Zheng", "Hai-Tao", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2105.07476", "submitter": "Amit Moryossef", "authors": "Amit Moryossef, Kayo Yin, Graham Neubig, Yoav Goldberg", "title": "Data Augmentation for Sign Language Gloss Translation", "comments": "4 pages, 1 page abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language translation (SLT) is often decomposed into video-to-gloss\nrecognition and gloss-to-text translation, where a gloss is a sequence of\ntranscribed spoken-language words in the order in which they are signed. We\nfocus here on gloss-to-text translation, which we treat as a low-resource\nneural machine translation (NMT) problem. However, unlike traditional\nlow-resource NMT, gloss-to-text translation differs because gloss-text pairs\noften have a higher lexical overlap and lower syntactic overlap than pairs of\nspoken languages. We exploit this lexical overlap and handle syntactic\ndivergence by proposing two rule-based heuristics that generate pseudo-parallel\ngloss-text pairs from monolingual spoken language text. By pre-training on the\nthus obtained synthetic data, we improve translation from American Sign\nLanguage (ASL) to English and German Sign Language (DGS) to German by up to\n3.14 and 2.20 BLEU, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 16:37:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Moryossef", "Amit", ""], ["Yin", "Kayo", ""], ["Neubig", "Graham", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2105.07510", "submitter": "Benjamin Townsend", "authors": "Benjamin Townsend, Eamon Ito-Fisher, Lily Zhang and Madison May", "title": "Doc2Dict: Information Extraction as Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typically, information extraction (IE) requires a pipeline approach: first, a\nsequence labeling model is trained on manually annotated documents to extract\nrelevant spans; then, when a new document arrives, a model predicts spans which\nare then post-processed and standardized to convert the information into a\ndatabase entry. We replace this labor-intensive workflow with a transformer\nlanguage model trained on existing database records to directly generate\nstructured JSON. Our solution removes the workload associated with producing\ntoken-level annotations and takes advantage of a data source which is generally\nquite plentiful (e.g. database records). As long documents are common in\ninformation extraction tasks, we use gradient checkpointing and chunked\nencoding to apply our method to sequences of up to 32,000 tokens on a single\nGPU. Our Doc2Dict approach is competitive with more complex, hand-engineered\npipelines and offers a simple but effective baseline for document-level\ninformation extraction. We release our Doc2Dict model and code to reproduce our\nexperiments and facilitate future work.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 20:46:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Townsend", "Benjamin", ""], ["Ito-Fisher", "Eamon", ""], ["Zhang", "Lily", ""], ["May", "Madison", ""]]}, {"id": "2105.07571", "submitter": "Yohan Jo", "authors": "Yohan Jo, Seojin Bang, Chris Reed, Eduard Hovy", "title": "Classifying Argumentative Relations Using Logical Mechanisms and\n  Argumentation Schemes", "comments": "To Appear in TACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While argument mining has achieved significant success in classifying\nargumentative relations between statements (support, attack, and neutral), we\nhave a limited computational understanding of logical mechanisms that\nconstitute those relations. Most recent studies rely on black-box models, which\nare not as linguistically insightful as desired. On the other hand, earlier\nstudies use rather simple lexical features, missing logical relations between\nstatements. To overcome these limitations, our work classifies argumentative\nrelations based on four logical and theory-informed mechanisms between two\nstatements, namely (i) factual consistency, (ii) sentiment coherence, (iii)\ncausal relation, and (iv) normative relation. We demonstrate that our\noperationalization of these logical mechanisms classifies argumentative\nrelations without directly training on data labeled with the relations,\nsignificantly better than several unsupervised baselines. We further\ndemonstrate that these mechanisms also improve supervised classifiers through\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:41:39 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jo", "Yohan", ""], ["Bang", "Seojin", ""], ["Reed", "Chris", ""], ["Hovy", "Eduard", ""]]}, {"id": "2105.07622", "submitter": "Ting-Wei Wu", "authors": "Ting-Wei Wu, Yung-An Hsieh, Yi-Chieh Liu", "title": "Ensemble-based Transfer Learning for Low-resource Machine Translation\n  Quality Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality Estimation (QE) of Machine Translation (MT) is a task to estimate the\nquality scores for given translation outputs from an unknown MT system.\nHowever, QE scores for low-resource languages are usually intractable and hard\nto collect. In this paper, we focus on the Sentence-Level QE Shared Task of the\nFifth Conference on Machine Translation (WMT20), but in a more challenging\nsetting. We aim to predict QE scores of given translation outputs when barely\nnone of QE scores of that paired languages are given during training. We\npropose an ensemble-based predictor-estimator QE model with transfer learning\nto overcome such QE data scarcity challenge by leveraging QE scores from other\nmiscellaneous languages and translation results of targeted languages. Based on\nthe evaluation results, we provide a detailed analysis of how each of our\nextension affects QE models on the reliability and the generalization ability\nto perform transfer learning under multilingual tasks. Finally, we achieve the\nbest performance on the ensemble model combining the models pretrained by\nindividual languages as well as different levels of parallel trained corpus\nwith a Pearson's correlation of 0.298, which is 2.54 times higher than\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:02:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wu", "Ting-Wei", ""], ["Hsieh", "Yung-An", ""], ["Liu", "Yi-Chieh", ""]]}, {"id": "2105.07623", "submitter": "Jiwei Li", "authors": "Xiaofei Sun, Yuxian Meng, Xiang Ao, Fei Wu, Tianwei Zhang, Jiwei Li\n  and Chun Fan", "title": "Sentence Similarity Based on Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing methods to measure sentence similarity are faced with two\nchallenges: (1) labeled datasets are usually limited in size, making them\ninsufficient to train supervised neural models; (2) there is a training-test\ngap for unsupervised language modeling (LM) based models to compute semantic\nscores between sentences, since sentence-level semantics are not explicitly\nmodeled at training. This results in inferior performances in this task. In\nthis work, we propose a new framework to address these two issues. The proposed\nframework is based on the core idea that the meaning of a sentence should be\ndefined by its contexts, and that sentence similarity can be measured by\ncomparing the probabilities of generating two sentences given the same context.\nThe proposed framework is able to generate high-quality, large-scale dataset\nwith semantic similarity scores between two sentences in an unsupervised\nmanner, with which the train-test gap can be largely bridged. Extensive\nexperiments show that the proposed framework achieves significant performance\nboosts over existing baselines under both the supervised and unsupervised\nsettings across different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:03:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sun", "Xiaofei", ""], ["Meng", "Yuxian", ""], ["Ao", "Xiang", ""], ["Wu", "Fei", ""], ["Zhang", "Tianwei", ""], ["Li", "Jiwei", ""], ["Fan", "Chun", ""]]}, {"id": "2105.07624", "submitter": "Fengbin Zhu", "authors": "Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,\n  Jiancheng Lv, Fuli Feng and Tat-Seng Chua", "title": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and\n  Textual Content in Finance", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hybrid data combining both tabular and textual content (e.g., financial\nreports) are quite pervasive in the real world. However, Question Answering\n(QA) over such hybrid data is largely neglected in existing research. In this\nwork, we extract samples from real financial reports to build a new large-scale\nQA dataset containing both Tabular And Textual data, named TAT-QA, where\nnumerical reasoning is usually required to infer the answer, such as addition,\nsubtraction, multiplication, division, counting, comparison/sorting, and the\ncompositions. We further propose a novel QA model termed TAGOP, which is\ncapable of reasoning over both tables and text. It adopts sequence tagging to\nextract relevant cells from the table along with relevant spans from the text\nto infer their semantics, and then applies symbolic reasoning over them with a\nset of aggregation operators to arrive at the final answer. TAGOPachieves 58.0%\ninF1, which is an 11.1% absolute increase over the previous best baseline\nmodel, according to our experiments on TAT-QA. But this result still lags far\nbehind performance of expert human, i.e.90.8% in F1. It is demonstrated that\nour TAT-QA is very challenging and can serve as a benchmark for training and\ntesting powerful QA models that address hybrid form data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 06:12:06 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 05:38:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhu", "Fengbin", ""], ["Lei", "Wenqiang", ""], ["Huang", "Youcheng", ""], ["Wang", "Chao", ""], ["Zhang", "Shuo", ""], ["Lv", "Jiancheng", ""], ["Feng", "Fuli", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2105.07654", "submitter": "Jiwei Li", "authors": "Leilei Gan, Yuxian Meng, Kun Kuang, Xiaofei Sun, Chun Fan, Fei Wu and\n  Jiwei Li", "title": "Dependency Parsing as MRC-based Span-Span Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Higher-order methods for dependency parsing can partially but not fully\naddresses the issue that edges in dependency tree should be constructed at the\ntext span/subtree level rather than word level. % This shortcoming can cause an\nincorrect span covered the corresponding tree rooted at a certain word though\nthe word is correctly linked to its head. In this paper, we propose a new\nmethod for dependency parsing to address this issue. The proposed method\nconstructs dependency trees by directly modeling span-span (in other words,\nsubtree-subtree) relations. It consists of two modules: the {\\it text span\nproposal module} which proposes candidate text spans, each of which represents\na subtree in the dependency tree denoted by (root, start, end); and the {\\it\nspan linking module}, which constructs links between proposed spans. We use the\nmachine reading comprehension (MRC) framework as the backbone to formalize the\nspan linking module in an MRC setup, where one span is used as a query to\nextract the text span/subtree it should be linked to. The proposed method comes\nwith the following merits: (1) it addresses the fundamental problem that edges\nin a dependency tree should be constructed between subtrees; (2) the MRC\nframework allows the method to retrieve missing spans in the span proposal\nstage, which leads to higher recall for eligible spans. Extensive experiments\non the PTB, CTB and Universal Dependencies (UD) benchmarks demonstrate the\neffectiveness of the proposed method. We are able to achieve new SOTA\nperformances on PTB and UD benchmarks, and competitive performances to previous\nSOTA models on the CTB dataset. Code is available at\nhttps://github.com/ShannonAI/mrc-for-dependency-parsing.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:03:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gan", "Leilei", ""], ["Meng", "Yuxian", ""], ["Kuang", "Kun", ""], ["Sun", "Xiaofei", ""], ["Fan", "Chun", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""]]}, {"id": "2105.07688", "submitter": "Yuejia Xiang", "authors": "Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Zhenxi Lin, Yefeng\n  Zheng", "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph\n  Embedding", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic embedding has been widely investigated for aligning knowledge graph\n(KG) entities. Current methods have explored and utilized the graph structure,\nthe entity names and attributes, but ignore the ontology (or ontological\nschema) which contains critical meta information such as classes and their\nmembership relationships with entities. In this paper, we propose an\nontology-guided entity alignment method named OntoEA, where both KGs and their\nontologies are jointly embedded, and the class hierarchy and the class\ndisjointness are utilized to avoid false mappings. Extensive experiments on\nseven public and industrial benchmarks have demonstrated the state-of-the-art\nperformance of OntoEA and the effectiveness of the ontologies.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:18:56 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 08:45:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xiang", "Yuejia", ""], ["Zhang", "Ziheng", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Xi", ""], ["Lin", "Zhenxi", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2105.07698", "submitter": "Casper Hansen", "authors": "Casper Hansen and Christian Hansen and Lucas Chaves Lima", "title": "Automatic Fake News Detection: Are Models Learning to Reason?", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most fact checking models for automatic fake news detection are based on\nreasoning: given a claim with associated evidence, the models aim to estimate\nthe claim veracity based on the supporting or refuting content within the\nevidence. When these models perform well, it is generally assumed to be due to\nthe models having learned to reason over the evidence with regards to the\nclaim. In this paper, we investigate this assumption of reasoning, by exploring\nthe relationship and importance of both claim and evidence. Surprisingly, we\nfind on political fact checking datasets that most often the highest\neffectiveness is obtained by utilizing only the evidence, as the impact of\nincluding the claim is either negligible or harmful to the effectiveness. This\nhighlights an important problem in what constitutes evidence in existing\napproaches for automatic fake news detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:34:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Lima", "Lucas Chaves", ""]]}, {"id": "2105.07720", "submitter": "Dimitri Kartsaklis", "authors": "Richie Yeung, Dimitri Kartsaklis", "title": "A CCG-Based Version of the DisCoCat Framework", "comments": "SemSpace 2021: Semantic Spaces at the Intersection of NLP, Physics,\n  and Cognitive Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the DisCoCat model (Coecke et al., 2010) has been proved a valuable\ntool for studying compositional aspects of language at the level of semantics,\nits strong dependency on pregroup grammars poses important restrictions: first,\nit prevents large-scale experimentation due to the absence of a pregroup\nparser; and second, it limits the expressibility of the model to context-free\ngrammars. In this paper we solve these problems by reformulating DisCoCat as a\npassage from Combinatory Categorial Grammar (CCG) to a category of semantics.\nWe start by showing that standard categorial grammars can be expressed as a\nbiclosed category, where all rules emerge as currying/uncurrying the identity;\nwe then proceed to model permutation-inducing rules by exploiting the symmetry\nof the compact closed category encoding the word meaning. We provide a proof of\nconcept for our method, converting \"Alice in Wonderland\" into DisCoCat form, a\ncorpus that we make available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 10:32:18 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 11:05:47 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 15:25:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yeung", "Richie", ""], ["Kartsaklis", "Dimitri", ""]]}, {"id": "2105.07727", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A Fronzetti Colladon, B Guardabascio, R Innarella", "title": "Using social network and semantic analysis to analyze online travel\n  forums and forecast tourism demand", "comments": null, "journal-ref": "Decision Support Systems 123, 113075 (2019)", "doi": "10.1016/j.dss.2019.113075", "report-no": null, "categories": "econ.EM cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Forecasting tourism demand has important implications for both policy makers\nand companies operating in the tourism industry. In this research, we applied\nmethods and tools of social network and semantic analysis to study\nuser-generated content retrieved from online communities which interacted on\nthe TripAdvisor travel forum. We analyzed the forums of 7 major European\ncapital cities, over a period of 10 years, collecting more than 2,660,000\nposts, written by about 147,000 users. We present a new methodology of analysis\nof tourism-related big data and a set of variables which could be integrated\ninto traditional forecasting models. We implemented Factor Augmented\nAutoregressive and Bridge models with social network and semantic variables\nwhich often led to a better forecasting performance than univariate models and\nmodels based on Google Trend data. Forum language complexity and the\ncentralization of the communication network, i.e. the presence of eminent\ncontributors, were the variables that contributed more to the forecasting of\ninternational airport arrivals.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 10:54:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Colladon", "A Fronzetti", ""], ["Guardabascio", "B", ""], ["Innarella", "R", ""]]}, {"id": "2105.07749", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon, F. Grippa, R. Innarella", "title": "Studying the association of online brand importance with museum\n  visitors: An application of the semantic brand score", "comments": null, "journal-ref": "Tourism Management Perspectives 33, 100588 (2020)", "doi": "10.1016/j.tmp.2019.100588", "report-no": null, "categories": "cs.CL cs.SI econ.GN physics.soc-ph q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper explores the association between brand importance and growth in\nmuseum visitors. We analyzed 10 years of online forum discussions and applied\nthe Semantic Brand Score (SBS) to assess the brand importance of five European\nMuseums. Our Naive Bayes and regression models indicate that variations in the\ncombined dimensions of the SBS (prevalence, diversity and connectivity) are\naligned with changes in museum visitors. Results suggest that, in order to\nattract more visitors, museum brand managers should focus on increasing the\nvolume of online posting and the richness of information generated by users\naround the brand, rather than controlling for the posts' overall positivity or\nnegativity.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:49:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Innarella", "R.", ""]]}, {"id": "2105.07847", "submitter": "Beatrice Alex", "authors": "Beatrice Alex, Clare Llewellyn, Pawel Michal Orzechowski, Maria\n  Boutchkova", "title": "The Online Pivot: Lessons Learned from Teaching a Text and Data Mining\n  Course in Lockdown, Enhancing online Teaching with Pair Programming and\n  Digital Badges", "comments": "11 pages, 4 figures, to appear in the Proceedings of the Fifth\n  Workshop on Teaching NLP @ NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we provide an account of how we ported a text and data mining\ncourse online in summer 2020 as a result of the COVID-19 pandemic and how we\nimproved it in a second pilot run. We describe the course, how we adapted it\nover the two pilot runs and what teaching techniques we used to improve\nstudents' learning and community building online. We also provide information\non the relentless feedback collected during the course which helped us to adapt\nour teaching from one session to the next and one pilot to the next. We discuss\nthe lessons learned and promote the use of innovative teaching techniques\napplied to the digital such as digital badges and pair programming in break-out\nrooms for teaching Natural Language Processing courses to beginners and\nstudents with different backgrounds.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:38:26 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Alex", "Beatrice", ""], ["Llewellyn", "Clare", ""], ["Orzechowski", "Pawel Michal", ""], ["Boutchkova", "Maria", ""]]}, {"id": "2105.07850", "submitter": "Manex Agirrezabal", "authors": "Manex Agirrezabal", "title": "The Flipped Classroom model for teaching Conditional Random Fields in an\n  NLP course", "comments": "Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we show and discuss our experience in applying the flipped\nclassroom method for teaching Conditional Random Fields in a Natural Language\nProcessing course. We present the activities that we developed together with\ntheir relationship to a cognitive complexity model (Bloom's taxonomy). After\nthis, we provide our own reflections and expectations of the model itself.\nBased on the evaluation got from students, it seems that students learn about\nthe topic and also that the method is rewarding for some students.\nAdditionally, we discuss some shortcomings and we propose possible solutions to\nthem. We conclude the paper with some possible future work.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:21:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Agirrezabal", "Manex", ""]]}, {"id": "2105.07879", "submitter": "Reza Vaezi", "authors": "Hadi Esmaeilzadeh and Reza Vaezi", "title": "Conscious AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in artificial intelligence (AI) have achieved human-scale\nspeed and accuracy for classification tasks. In turn, these capabilities have\nmade AI a viable replacement for many human activities that at their core\ninvolve classification, such as basic mechanical and analytical tasks in\nlow-level service jobs. Current systems do not need to be conscious to\nrecognize patterns and classify them. However, for AI to progress to more\ncomplicated tasks requiring intuition and empathy, it must develop capabilities\nsuch as metathinking, creativity, and empathy akin to human self-awareness or\nconsciousness. We contend that such a paradigm shift is possible only through a\nfundamental shift in the state of artificial intelligence toward consciousness,\na shift similar to what took place for humans through the process of natural\nselection and evolution. As such, this paper aims to theoretically explore the\nrequirements for the emergence of consciousness in AI. It also provides a\nprincipled understanding of how conscious AI can be detected and how it might\nbe manifested in contrast to the dominant paradigm that seeks to ultimately\ncreate machines that are linguistically indistinguishable from humans.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:53:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Esmaeilzadeh", "Hadi", ""], ["Vaezi", "Reza", ""]]}, {"id": "2105.07903", "submitter": "Nils Holzenberger", "authors": "Nils Holzenberger and Benjamin Van Durme", "title": "Factoring Statutory Reasoning as Language Understanding Challenges", "comments": "18 pages, 3 figures. To appear in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statutory reasoning is the task of determining whether a legal statute,\nstated in natural language, applies to the text description of a case. Prior\nwork introduced a resource that approached statutory reasoning as a monolithic\ntextual entailment problem, with neural baselines performing nearly at-chance.\nTo address this challenge, we decompose statutory reasoning into four types of\nlanguage-understanding challenge problems, through the introduction of concepts\nand structure found in Prolog programs. Augmenting an existing benchmark, we\nprovide annotations for the four tasks, and baselines for three of them. Models\nfor statutory reasoning are shown to benefit from the additional structure,\nimproving on prior baselines. Further, the decomposition into subtasks\nfacilitates finer-grained model diagnostics and clearer incremental progress.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:33:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Holzenberger", "Nils", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2105.07911", "submitter": "Kuan Xu", "authors": "Kuan Xuan, Yongbo Wang, Yongliang Wang, Zujie Wen, Yang Dong", "title": "SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In text-to-SQL task, seq-to-seq models often lead to sub-optimal performance\ndue to limitations in their architecture. In this paper, we present a simple\nyet effective approach that adapts transformer-based seq-to-seq model to robust\ntext-to-SQL generation. Instead of inducing constraint to decoder or reformat\nthe task as slot-filling, we propose to train seq-to-seq model with Schema\naware Denoising (SeaD), which consists of two denoising objectives that train\nmodel to either recover input or predict output from two novel erosion and\nshuffle noises. These denoising objectives acts as the auxiliary tasks for\nbetter modeling the structural data in S2S generation. In addition, we improve\nand propose a clause-sensitive execution guided (EG) decoding strategy to\novercome the limitation of EG decoding for generative model. The experiments\nshow that the proposed method improves the performance of seq-to-seq model in\nboth schema linking and grammar correctness and establishes new\nstate-of-the-art on WikiSQL benchmark. The results indicate that the capacity\nof vanilla seq-to-seq architecture for text-to-SQL may have been\nunder-estimated.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:49:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Xuan", "Kuan", ""], ["Wang", "Yongbo", ""], ["Wang", "Yongliang", ""], ["Wen", "Zujie", ""], ["Dong", "Yang", ""]]}, {"id": "2105.07949", "submitter": "Abhijit Suresh", "authors": "Abhijit Suresh, Jennifer Jacobs, Vivian Lai, Chenhao Tan, Wayne Ward,\n  James H. Martin, Tamara Sumner", "title": "Using Transformers to Provide Teachers with Personalized Feedback on\n  their Classroom Discourse: The TalkMoves Application", "comments": "Presented at the AAAI 2021 Spring Symposium on Artificial\n  Intelligence for K-12 Education", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TalkMoves is an innovative application designed to support K-12 mathematics\nteachers to reflect on, and continuously improve their instructional practices.\nThis application combines state-of-the-art natural language processing\ncapabilities with automated speech recognition to automatically analyze\nclassroom recordings and provide teachers with personalized feedback on their\nuse of specific types of discourse aimed at broadening and deepening classroom\nconversations about mathematics. These specific discourse strategies are\nreferred to as \"talk moves\" within the mathematics education community and\nprior research has documented the ways in which systematic use of these\ndiscourse strategies can positively impact student engagement and learning. In\nthis article, we describe the TalkMoves application's cloud-based\ninfrastructure for managing and processing classroom recordings, and its\ninterface for providing teachers with feedback on their use of talk moves\nduring individual teaching episodes. We present the series of model\narchitectures we developed, and the studies we conducted, to develop our\nbest-performing, transformer-based model (F1 = 79.3%). We also discuss several\ntechnical challenges that need to be addressed when working with real-world\nspeech and language data from noisy K-12 classrooms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:45:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Suresh", "Abhijit", ""], ["Jacobs", "Jennifer", ""], ["Lai", "Vivian", ""], ["Tan", "Chenhao", ""], ["Ward", "Wayne", ""], ["Martin", "James H.", ""], ["Sumner", "Tamara", ""]]}, {"id": "2105.08008", "submitter": "Marco Valentino", "authors": "Julia Rozanova, Deborah Ferreira, Mokanarangan Thayaparan, Marco\n  Valentino, Andr\\'e Freitas", "title": "Supporting Context Monotonicity Abstractions in Neural NLI Models", "comments": "NALOMA'21 (NAtural LOgic Meets MAchine Learning) @IWCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language contexts display logical regularities with respect to\nsubstitutions of related concepts: these are captured in a functional\norder-theoretic property called monotonicity. For a certain class of NLI\nproblems where the resulting entailment label depends only on the context\nmonotonicity and the relation between the substituted concepts, we build on\nprevious techniques that aim to improve the performance of NLI models for these\nproblems, as consistent performance across both upward and downward monotone\ncontexts still seems difficult to attain even for state-of-the-art models. To\nthis end, we reframe the problem of context monotonicity classification to make\nit compatible with transformer-based pre-trained NLI models and add this task\nto the training pipeline. Furthermore, we introduce a sound and complete\nsimplified monotonicity logic formalism which describes our treatment of\ncontexts as abstract units. Using the notions in our formalism, we adapt\ntargeted challenge sets to investigate whether an intermediate context\nmonotonicity classification task can aid NLI models' performance on examples\nexhibiting monotonicity reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:43:43 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rozanova", "Julia", ""], ["Ferreira", "Deborah", ""], ["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2105.08021", "submitter": "Qingyun Wang", "authors": "Qingyun Wang, Semih Yavuz, Victoria Lin, Heng Ji, Nazneen Rajani", "title": "Stage-wise Fine-tuning for Graph-to-Text Generation", "comments": "10 pages, Accepted by Proceedings of ACL-IJCNLP 2021 Student Research\n  Workshop, Code and Resources at\n  https://github.com/EagleW/Stage-wise-Fine-tuning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-to-text generation has benefited from pre-trained language models\n(PLMs) in achieving better performance than structured graph encoders. However,\nthey fail to fully utilize the structure information of the input graph. In\nthis paper, we aim to further improve the performance of the pre-trained\nlanguage model by proposing a structured graph-to-text model with a two-step\nfine-tuning mechanism which first fine-tunes the model on Wikipedia before\nadapting to the graph-to-text generation. In addition to using the traditional\ntoken and position embeddings to encode the knowledge graph (KG), we propose a\nnovel tree-level embedding method to capture the inter-dependency structures of\nthe input graph. This new approach has significantly improved the performance\nof all text generation metrics for the English WebNLG 2017 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:15:29 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 22:47:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Qingyun", ""], ["Yavuz", "Semih", ""], ["Lin", "Victoria", ""], ["Ji", "Heng", ""], ["Rajani", "Nazneen", ""]]}, {"id": "2105.08031", "submitter": "MeiXing Dong", "authors": "MeiXing Dong, Xueming Xu, Yiwei Zhang, Ian Stewart, Rada Mihalcea", "title": "Room to Grow: Understanding Personal Characteristics Behind Self\n  Improvement Using Social Media", "comments": "10 pages, Accepted to be published at SocialNLP at NAACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people aim for change, but not everyone succeeds. While there are a\nnumber of social psychology theories that propose motivation-related\ncharacteristics of those who persist with change, few computational studies\nhave explored the motivational stage of personal change. In this paper, we\ninvestigate a new dataset consisting of the writings of people who manifest\nintention to change, some of whom persist while others do not. Using a variety\nof linguistic analysis techniques, we first examine the writing patterns that\ndistinguish the two groups of people. Persistent people tend to reference more\ntopics related to long-term self-improvement and use a more complicated writing\nstyle. Drawing on these consistent differences, we build a classifier that can\nreliably identify the people more likely to persist, based on their language.\nOur experiments provide new insights into the motivation-related behavior of\npeople who persist with their intention to change.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:30:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dong", "MeiXing", ""], ["Xu", "Xueming", ""], ["Zhang", "Yiwei", ""], ["Stewart", "Ian", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2105.08039", "submitter": "Nadir Durrani Dr", "authors": "Hassan Sajjad, Narine Kokhlikyan, Fahim Dalvi, Nadir Durrani", "title": "Fine-grained Interpretation and Causation Analysis in Deep NLP Models", "comments": "Accepted at NAACL Tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a write-up for the tutorial on \"Fine-grained Interpretation and\nCausation Analysis in Deep NLP Models\" that we are presenting at NAACL 2021. We\npresent and discuss the research work on interpreting fine-grained components\nof a model from two perspectives, i) fine-grained interpretation, ii) causation\nanalysis. The former introduces methods to analyze individual neurons and a\ngroup of neurons with respect to a language property or a task. The latter\nstudies the role of neurons and input features in explaining decisions made by\nthe model. We also discuss application of neuron analysis such as network\nmanipulation and domain adaptation. Moreover, we present two toolkits namely\nNeuroX and Captum, that support functionalities discussed in this tutorial.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:43:36 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 09:14:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sajjad", "Hassan", ""], ["Kokhlikyan", "Narine", ""], ["Dalvi", "Fahim", ""], ["Durrani", "Nadir", ""]]}, {"id": "2105.08049", "submitter": "Yang Zhang", "authors": "Yang Zhang, Vahid Noroozi, Evelina Bakhturina, Boris Ginsburg", "title": "SGD-QA: Fast Schema-Guided Dialogue State Tracking for Unseen Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue state tracking is an essential part of goal-oriented dialogue\nsystems, while most of these state tracking models often fail to handle unseen\nservices. In this paper, we propose SGD-QA, a simple and extensible model for\nschema-guided dialogue state tracking based on a question answering approach.\nThe proposed multi-pass model shares a single encoder between the domain\ninformation and dialogue utterance. The domain's description represents the\nquery and the dialogue utterance serves as the context. The model improves\nperformance on unseen services by at least 1.6x compared to single-pass\nbaseline models on the SGD dataset. SGD-QA shows competitive performance\ncompared to state-of-the-art multi-pass models while being significantly more\nefficient in terms of memory consumption and training performance. We provide a\nthorough discussion on the model with ablation study and error analysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:54:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Yang", ""], ["Noroozi", "Vahid", ""], ["Bakhturina", "Evelina", ""], ["Ginsburg", "Boris", ""]]}, {"id": "2105.08050", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le", "title": "Pay Attention to MLPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have become one of the most important architectural innovations\nin deep learning and have enabled many breakthroughs over the past few years.\nHere we propose a simple network architecture, gMLP, based on MLPs with gating,\nand show that it can perform as well as Transformers in key language and vision\napplications. Our comparisons show that self-attention is not critical for\nVision Transformers, as gMLP can achieve the same accuracy. For BERT, our model\nachieves parity with Transformers on pretraining perplexity and is better on\nsome downstream NLP tasks. On finetuning tasks where gMLP performs worse,\nmaking the gMLP model substantially larger can close the gap with Transformers.\nIn general, our experiments show that gMLP can scale as well as Transformers\nover increased data and compute.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:55:04 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 20:24:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Hanxiao", ""], ["Dai", "Zihang", ""], ["So", "David R.", ""], ["Le", "Quoc V.", ""]]}, {"id": "2105.08106", "submitter": "Nikita Bhalla", "authors": "Hiba Ahsan, Nikita Bhalla, Daivat Bhatt, Kaivankumar Shah", "title": "Multi-Modal Image Captioning for the Visually Impaired", "comments": "8 pages, 2 figures, 2 tables, accepted to NAACL-HLT SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the ways blind people understand their surroundings is by clicking\nimages and relying on descriptions generated by image captioning systems.\nCurrent work on captioning images for the visually impaired do not use the\ntextual data present in the image when generating captions. This problem is\ncritical as many visual scenes contain text. Moreover, up to 21% of the\nquestions asked by blind people about the images they click pertain to the text\npresent in them. In this work, we propose altering AoANet, a state-of-the-art\nimage captioning model, to leverage the text detected in the image as an input\nfeature. In addition, we use a pointer-generator mechanism to copy the detected\ntext to the caption when tokens need to be reproduced accurately. Our model\noutperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2%\nperformance improvement on CIDEr and SPICE scores, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:35:24 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ahsan", "Hiba", ""], ["Bhalla", "Nikita", ""], ["Bhatt", "Daivat", ""], ["Shah", "Kaivankumar", ""]]}, {"id": "2105.08146", "submitter": "Yiqun Yao", "authors": "Yiqun Yao, Michalis Papakostas, Mihai Burzo, Mohamed Abouelenien, Rada\n  Mihalcea", "title": "MUSER: MUltimodal Stress Detection using Emotion Recognition as an\n  Auxiliary Task", "comments": "NAACL 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The capability to automatically detect human stress can benefit artificial\nintelligent agents involved in affective computing and human-computer\ninteraction. Stress and emotion are both human affective states, and stress has\nproven to have important implications on the regulation and expression of\nemotion. Although a series of methods have been established for multimodal\nstress detection, limited steps have been taken to explore the underlying\ninter-dependence between stress and emotion. In this work, we investigate the\nvalue of emotion recognition as an auxiliary task to improve stress detection.\nWe propose MUSER -- a transformer-based model architecture and a novel\nmulti-task learning algorithm with speed-based dynamic sampling strategy.\nEvaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our\nmodel is effective for stress detection with both internal and external\nauxiliary tasks, and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:24:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yao", "Yiqun", ""], ["Papakostas", "Michalis", ""], ["Burzo", "Mihai", ""], ["Abouelenien", "Mohamed", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2105.08165", "submitter": "Liming Luke Chen", "authors": "Sahraoui Dhelim, Liming Luke Chen, Huansheng Ning, Sajal K Das, Chris\n  Nugent, Devin Burns, Gerard Leavey, Dirk Pesch and Eleanor Bantry-White", "title": "Social Behavior and Mental Health: A Snapshot Survey under COVID-19\n  Pandemic", "comments": "Submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social media provides a channel for monitoring people's social\nbehaviors and their mental distress. Due to the restrictions imposed by\nCOVID-19 people are increasingly using online social networks to express their\nfeelings. Consequently, there is a significant amount of diverse user-generated\nsocial media content. However, COVID-19 pandemic has changed the way we live,\nstudy, socialize and recreate and this has affected our well-being and mental\nhealth problems. There are growing researches that leverage online social media\nanalysis to detect and assess user's mental status. In this paper, we survey\nthe literature of social media analysis for mental disorders detection, with a\nspecial focus on the studies conducted in the context of COVID-19 during\n2020-2021. Firstly, we classify the surveyed studies in terms of feature\nextraction types, varying from language usage patterns to aesthetic preferences\nand online behaviors. Secondly, we explore detection methods used for mental\ndisorders detection including machine learning and deep learning detection\nmethods. Finally, we discuss the challenges of mental disorder detection using\nsocial media data, including the privacy and ethical concerns, as well as the\ntechnical challenges of scaling and deploying such systems at large scales, and\ndiscuss the learnt lessons over the last few years.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:08:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Chen", "Liming Luke", ""], ["Ning", "Huansheng", ""], ["Das", "Sajal K", ""], ["Nugent", "Chris", ""], ["Burns", "Devin", ""], ["Leavey", "Gerard", ""], ["Pesch", "Dirk", ""], ["Bantry-White", "Eleanor", ""]]}, {"id": "2105.08185", "submitter": "Shuyang Li", "authors": "Shuyang Li, Yufei Li, Jianmo Ni, Julian McAuley", "title": "SHARE: a System for Hierarchical Assistive Recipe Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce SHARE: a System for Hierarchical Assistive Recipe Editing to\nassist home cooks with dietary restrictions -- a population under-served by\nexisting cooking resources. Our hierarchical recipe editor makes necessary\nsubstitutions to a recipe's ingredients list and re-writes the directions to\nmake use of the new ingredients. We introduce the novel RecipePairs dataset of\n84K pairs of similar recipes in which one recipe satisfies one of seven dietary\nconstraints, allowing for supervised training of such recipe editing models.\nExperiments on this dataset demonstrate that our system produces convincing,\ncoherent recipes that are appropriate for a target dietary constraint (contain\nno prohibited ingredients). We show that this is a challenging task that cannot\nbe adequately solved with human-written ingredient substitution rules or\nstraightforward adaptation of state-of-the-art models for recipe generation. We\nfurther demonstrate through human evaluations and real-world cooking trials\nthat recipes edited by our system can be easily followed by home cooks to\ncreate delicious and satisfactory dishes.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:38:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Shuyang", ""], ["Li", "Yufei", ""], ["Ni", "Jianmo", ""], ["McAuley", "Julian", ""]]}, {"id": "2105.08206", "submitter": "Machel Reid", "authors": "Machel Reid and Victor Zhong", "title": "LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer", "comments": "ACL-IJCNLP 2021 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of text style transfer can be achieved with only small, precise\nedits (e.g. sentiment transfer from I had a terrible time... to I had a great\ntime...). We propose a coarse-to-fine editor for style transfer that transforms\ntext using Levenshtein edit operations (e.g. insert, replace, delete). Unlike\nprior single-span edit methods, our method concurrently edits multiple spans in\nthe source text. To train without parallel style text pairs (e.g. pairs of +/-\nsentiment statements), we propose an unsupervised data synthesis procedure. We\nfirst convert text to style-agnostic templates using style classifier attention\n(e.g. I had a SLOT time...), then fill in slots in these templates using\nfine-tuned pretrained language models. Our method outperforms existing\ngeneration and editing style transfer methods on sentiment (Yelp, Amazon) and\npoliteness (Polite) transfer. In particular, multi-span editing achieves higher\nperformance and more diverse output than single-span editing. Moreover,\ncompared to previous methods on unsupervised data synthesis, our method results\nin higher quality parallel style pairs and improves model performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 00:08:30 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Reid", "Machel", ""], ["Zhong", "Victor", ""]]}, {"id": "2105.08209", "submitter": "Wojciech Kry\\'sci\\'nski", "authors": "Wojciech Kry\\'sci\\'nski, Nazneen Rajani, Divyansh Agarwal, Caiming\n  Xiong, Dragomir Radev", "title": "BookSum: A Collection of Datasets for Long-form Narrative Summarization", "comments": "19 pages, 12 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of available text summarization datasets include short-form\nsource documents that lack long-range causal and temporal dependencies, and\noften contain strong layout and stylistic biases. While relevant, such datasets\nwill offer limited challenges for future generations of text summarization\nsystems. We address these issues by introducing BookSum, a collection of\ndatasets for long-form narrative summarization. Our dataset covers source\ndocuments from the literature domain, such as novels, plays and stories, and\nincludes highly abstractive, human written summaries on three levels of\ngranularity of increasing difficulty: paragraph-, chapter-, and book-level. The\ndomain and structure of our dataset poses a unique set of challenges for\nsummarization systems, which include: processing very long documents,\nnon-trivial causal and temporal dependencies, and rich discourse structures. To\nfacilitate future work, we trained and evaluated multiple extractive and\nabstractive summarization models as baselines for our dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 00:22:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Kry\u015bci\u0144ski", "Wojciech", ""], ["Rajani", "Nazneen", ""], ["Agarwal", "Divyansh", ""], ["Xiong", "Caiming", ""], ["Radev", "Dragomir", ""]]}, {"id": "2105.08213", "submitter": "Ridong Han", "authors": "Ridong Han, Tao Peng, Jiayu Han, Lin Yue, Hai Cui, Lu Liu", "title": "Distantly Supervised Relation Extraction via Recursive\n  Hierarchy-Interactive Attention and Entity-Order Perception", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distantly supervised relation extraction has drawn significant attention\nrecently. However, almost all prior works ignore the fact that, in a sentence,\nthe appearance order of two entities contributes to the understanding of its\nsemantics. Furthermore, they leverage relation hierarchies but don't fully\nexploit the heuristic effect between relation levels, i.e., higher-level\nrelations can give useful information to the lower ones. In this paper, we\ndesign a novel Recursive Hierarchy-Interactive Attention network (RHIA), which\nuses the hierarchical structure of the relation to model the interactive\ninformation between the relation levels to further handle long-tail relations.\nIt generates relation-augmented sentence representations along hierarchical\nrelation chains in a recursive structure. Besides, we introduce a newfangled\ntraining objective, called Entity-Order Perception (EOP), to make the sentence\nencoder retain more entity appearance information. Substantial experiments on\nthe popular New York Times (NYT) dataset are conducted. Compared to prior\nbaselines, our approach achieves state-of-the-art performance in terms of\nprecision-recall (P-R) curves, AUC, Top-N precision and other evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 00:45:25 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Han", "Ridong", ""], ["Peng", "Tao", ""], ["Han", "Jiayu", ""], ["Yue", "Lin", ""], ["Cui", "Hai", ""], ["Liu", "Lu", ""]]}, {"id": "2105.08214", "submitter": "Meisin Lee", "authors": "Meisin Lee, Lay-Ki Soon, Eu-Gene Siew, Ly Fie Sugianto", "title": "The Commodities News Corpus: A Resource forUnderstanding Commodity News\n  Better", "comments": "Submitted to journal, currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Commodity News contains a wealth of information such as sum-mary of the\nrecent commodity price movement and notable events that led tothe movement.\nThrough event extraction, useful information extracted fromcommodity news is\nextremely useful in mining for causal relation betweenevents and commodity\nprice movement, which can be used for commodity priceprediction. To facilitate\nthe future research, we introduce a new dataset withthe following information\nidentified and annotated: (i) entities (both nomi-nal and named), (ii) events\n(trigger words and argument roles), (iii) eventmetadata: modality, polarity and\nintensity and (iv) event-event relations.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 00:52:47 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 03:15:11 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lee", "Meisin", ""], ["Soon", "Lay-Ki", ""], ["Siew", "Eu-Gene", ""], ["Sugianto", "Ly Fie", ""]]}, {"id": "2105.08251", "submitter": "Yutao Zhu", "authors": "Hao Jiang, Yutao Zhu, Xinyu Zhang, Zhicheng Dou, Pan Du, Te Pi, Yantao\n  Jia", "title": "Emotion Eliciting Machine: Emotion Eliciting Conversation Generation\n  based on Dual Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed great progress on building emotional chatbots.\nTremendous methods have been proposed for chatbots to generate responses with\ngiven emotions. However, the emotion changes of the user during the\nconversation has not been fully explored. In this work, we study the problem of\npositive emotion elicitation, which aims to generate responses that can elicit\npositive emotion of the user, in human-machine conversation. We propose a\nweakly supervised Emotion Eliciting Machine (EEM) to address this problem.\nSpecifically, we first collect weak labels of user emotion status changes in a\nconversion based on a pre-trained emotion classifier. Then we propose a dual\nencoder-decoder structure to model the generation of responses in both positive\nand negative side based on the changes of the user's emotion status in the\nconversation. An emotion eliciting factor is introduced on top of the dual\nstructure to balance the positive and negative emotional impacts on the\ngenerated response during emotion elicitation. The factor also provides a\nfine-grained controlling manner for emotion elicitation. Experimental results\non a large real-world dataset show that EEM outperforms the existing models in\ngenerating responses with positive emotion elicitation.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 03:19:25 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jiang", "Hao", ""], ["Zhu", "Yutao", ""], ["Zhang", "Xinyu", ""], ["Dou", "Zhicheng", ""], ["Du", "Pan", ""], ["Pi", "Te", ""], ["Jia", "Yantao", ""]]}, {"id": "2105.08261", "submitter": "Tong Zhang", "authors": "Tong Zhang, Yong Liu, Peixiang Zhong, Chen Zhang, Hao Wang, Chunyan\n  Miao", "title": "KECRS: Towards Knowledge-Enriched Conversational Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chit-chat-based conversational recommendation systems (CRS) provide item\nrecommendations to users through natural language interactions. To better\nunderstand user's intentions, external knowledge graphs (KG) have been\nintroduced into chit-chat-based CRS. However, existing chit-chat-based CRS\nusually generate repetitive item recommendations, and they cannot properly\ninfuse knowledge from KG into CRS to generate informative responses. To remedy\nthese issues, we first reformulate the conversational recommendation task to\nhighlight that the recommended items should be new and possibly interested by\nusers. Then, we propose the Knowledge-Enriched Conversational Recommendation\nSystem (KECRS). Specifically, we develop the Bag-of-Entity (BOE) loss and the\ninfusion loss to better integrate KG with CRS for generating more diverse and\ninformative responses. BOE loss provides an additional supervision signal to\nguide CRS to learn from both human-written utterances and KG. Infusion loss\nbridges the gap between the word embeddings and entity embeddings by minimizing\ndistances of the same words in these two embeddings. Moreover, we facilitate\nour study by constructing a high-quality KG, \\ie The Movie Domain Knowledge\nGraph (TMDKG). Experimental results on a large-scale dataset demonstrate that\nKECRS outperforms state-of-the-art chit-chat-based CRS, in terms of both\nrecommendation accuracy and response generation quality.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 03:52:06 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhang", "Tong", ""], ["Liu", "Yong", ""], ["Zhong", "Peixiang", ""], ["Zhang", "Chen", ""], ["Wang", "Hao", ""], ["Miao", "Chunyan", ""]]}, {"id": "2105.08301", "submitter": "Pengjie Ren", "authors": "Pengjie Ren, Zhongkun Liu, Xiaomeng Song, Hongtao Tian, Zhumin Chen,\n  Zhaochun Ren and Maarten de Rijke", "title": "Wizard of Search Engine: Access to Information Through Conversations\n  with Search Engines", "comments": "Published in SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational information seeking (CIS) is playing an increasingly important\nrole in connecting people to information. Due to the lack of suitable resource,\nprevious studies on CIS are limited to the study of theoretical/conceptual\nframeworks, laboratory-based user studies, or a particular aspect of CIS (e.g.,\nasking clarifying questions). In this work, we make efforts to facilitate\nresearch on CIS from three aspects. (1) We formulate a pipeline for CIS with\nsix sub-tasks: intent detection (ID), keyphrase extraction (KE), action\nprediction (AP), query selection (QS), passage selection (PS), and response\ngeneration (RG). (2) We release a benchmark dataset, called wizard of search\nengine (WISE), which allows for comprehensive and in-depth research on all\naspects of CIS. (3) We design a neural architecture capable of training and\nevaluating both jointly and separately on the six sub-tasks, and devise a\npre-train/fine-tune learning scheme, that can reduce the requirements of WISE\nin scale by making full use of available data. We report some useful\ncharacteristics of CIS based on statistics of WISE. We also show that our best\nperforming model variant isable to achieve effective CIS as indicated by\nseveral metrics. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research by measuring further improvements in this\nimportant research direction.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 06:35:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ren", "Pengjie", ""], ["Liu", "Zhongkun", ""], ["Song", "Xiaomeng", ""], ["Tian", "Hongtao", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2105.08316", "submitter": "Chujie Zheng", "authors": "Chujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, Minlie Huang", "title": "CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response\n  Generation", "comments": "Accepted to Findings of ACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of empathy is crucial to the success of open-domain dialog\nsystems. Due to its nature of multi-dimensionality, there are various factors\nthat relate to empathy expression, such as communication mechanism, dialog act\nand emotion. However, existing methods for empathetic response generation\nusually either consider only one empathy factor or ignore the hierarchical\nrelationships between different factors, leading to a weak ability of empathy\nmodeling. In this paper, we propose a multi-factor hierarchical framework,\nCoMAE, for empathetic response generation, which models the above three key\nfactors of empathy expression in a hierarchical way. We show experimentally\nthat our CoMAE-based model can generate more empathetic responses than previous\nmethods. We also highlight the importance of hierarchical modeling of different\nfactors through both the empirical analysis on a real-life corpus and the\nextensive experiments. Our codes and used data are available at\nhttps://github.com/chujiezheng/CoMAE.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 07:13:33 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 11:53:16 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 03:07:03 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zheng", "Chujie", ""], ["Liu", "Yong", ""], ["Chen", "Wei", ""], ["Leng", "Yongcai", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.08361", "submitter": "Saloni Dash", "authors": "Saloni Dash, Dibyendu Mishra, Gazal Shekhawat, Joyojeet Pal", "title": "Divided We Rule: Influencer Polarization on Twitter During Political\n  Crises in India", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Influencers are key to the nature and networks of information propagation on\nsocial media. Influencers are particularly important in political discourse\nthrough their engagement with issues, and may derive their legitimacy either\nsolely or partly through online operation, or have an offline sphere of\nexpertise such as entertainers, journalists etc. To quantify influencers'\npolitical engagement and polarity, we use Google's Universal Sentence Encoder\n(USE) to encode the tweets of 6k influencers and 26k Indian politicians during\npolitical crises in India. We then obtain aggregate vector representations of\nthe influencers based on their tweet embeddings, which alongside retweet graphs\nhelp compute the stance and polarity of these influencers with respect to the\npolitical issues. We find that while on COVID-19 there is a confluence of\ninfluencers on the side of the government, on three other contentious issues\naround citizenship, Kashmir's statehood, and farmers' protests, it is mainly\ngovernment-aligned fan accounts that amplify the incumbent's positions. We\npropose that this method offers insight into the political schisms in\npresent-day India, but also offers a means to study influencers and\npolarization in other contexts.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:38:16 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Dash", "Saloni", ""], ["Mishra", "Dibyendu", ""], ["Shekhawat", "Gazal", ""], ["Pal", "Joyojeet", ""]]}, {"id": "2105.08393", "submitter": "Shengfei Lyu", "authors": "Shengfei Lyu, Huanhuan Chen", "title": "Relation Classification with Entity Type Restriction", "comments": "6 pages,3 figures, accepted by ACL-IJCNLP 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification aims to predict a relation between two entities in a\nsentence. The existing methods regard all relations as the candidate relations\nfor the two entities in a sentence. These methods neglect the restrictions on\ncandidate relations by entity types, which leads to some inappropriate\nrelations being candidate relations. In this paper, we propose a novel\nparadigm, RElation Classification with ENtity Type restriction (RECENT), which\nexploits entity types to restrict candidate relations. Specially, the mutual\nrestrictions of relations and entity types are formalized and introduced into\nrelation classification. Besides, the proposed paradigm, RECENT, is\nmodel-agnostic. Based on two representative models GCN and SpanBERT\nrespectively, RECENT_GCN and RECENT_SpanBERT are trained in RECENT.\nExperimental results on a standard dataset indicate that RECENT improves the\nperformance of GCN and SpanBERT by 6.9 and 4.4 F1 points, respectively.\nEspecially, RECENT_SpanBERT achieves a new state-of-the-art on TACRED.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:42:40 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lyu", "Shengfei", ""], ["Chen", "Huanhuan", ""]]}, {"id": "2105.08399", "submitter": "Ond\\v{r}ej C\\'ifka", "authors": "Antoine Liutkus, Ond\\v{r}ej C\\'ifka, Shih-Lun Wu, Umut\n  \\c{S}im\\c{s}ekli, Yi-Hsuan Yang, Ga\\\"el Richard", "title": "Relative Positional Encoding for Transformers with Linear Complexity", "comments": "ICML 2021 (long talk) camera-ready. 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Transformer models allow for unprecedented sequence\nlengths, due to linear space and time complexity. In the meantime, relative\npositional encoding (RPE) was proposed as beneficial for classical Transformers\nand consists in exploiting lags instead of absolute positions for inference.\nStill, RPE is not available for the recent linear-variants of the Transformer,\nbecause it requires the explicit computation of the attention matrix, which is\nprecisely what is avoided by such methods. In this paper, we bridge this gap\nand present Stochastic Positional Encoding as a way to generate PE that can be\nused as a replacement to the classical additive (sinusoidal) PE and provably\nbehaves like RPE. The main theoretical contribution is to make a connection\nbetween positional encoding and cross-covariance structures of correlated\nGaussian processes. We illustrate the performance of our approach on the\nLong-Range Arena benchmark and on music generation.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:52:32 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 08:55:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liutkus", "Antoine", ""], ["C\u00edfka", "Ond\u0159ej", ""], ["Wu", "Shih-Lun", ""], ["\u015eim\u015fekli", "Umut", ""], ["Yang", "Yi-Hsuan", ""], ["Richard", "Ga\u00ebl", ""]]}, {"id": "2105.08445", "submitter": "Kyra Ahrens", "authors": "Kyra Ahrens, Fares Abawi, Stefan Wermter", "title": "DRILL: Dynamic Representations for Imbalanced Lifelong Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continual or lifelong learning has been a long-standing challenge in machine\nlearning to date, especially in natural language processing (NLP). Although\nstate-of-the-art language models such as BERT have ushered in a new era in this\nfield due to their outstanding performance in multitask learning scenarios,\nthey suffer from forgetting when being exposed to a continuous stream of data\nwith shifting data distributions. In this paper, we introduce DRILL, a novel\ncontinual learning architecture for open-domain text classification. DRILL\nleverages a biologically inspired self-organizing neural architecture to\nselectively gate latent language representations from BERT in a\ntask-incremental manner. We demonstrate in our experiments that DRILL\noutperforms current methods in a realistic scenario of imbalanced,\nnon-stationary data without prior knowledge about task boundaries. To the best\nof our knowledge, DRILL is the first of its kind to use a self-organizing\nneural architecture for open-domain lifelong learning in NLP.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:36:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ahrens", "Kyra", ""], ["Abawi", "Fares", ""], ["Wermter", "Stefan", ""]]}, {"id": "2105.08481", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Liangli Zhen, Joey Tianyi Zhou, Rick\n  Siow Mong Goh", "title": "Parallel Attention Network with Sequence Matching for Video Grounding", "comments": "15 pages, 10 figures, 7 tables, Findings at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video, video grounding aims to retrieve a temporal moment that\nsemantically corresponds to a language query. In this work, we propose a\nParallel Attention Network with Sequence matching (SeqPAN) to address the\nchallenges in this task: multi-modal representation learning, and target moment\nboundary prediction. We design a self-guided parallel attention module to\neffectively capture self-modal contexts and cross-modal attentive information\nbetween video and text. Inspired by sequence labeling tasks in natural language\nprocessing, we split the ground truth moment into begin, inside, and end\nregions. We then propose a sequence matching strategy to guide start/end\nboundary predictions using region labels. Experimental results on three\ndatasets show that SeqPAN is superior to state-of-the-art methods. Furthermore,\nthe effectiveness of the self-guided parallel attention module and the sequence\nmatching module is verified.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:43:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Zhen", "Liangli", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2105.08504", "submitter": "Mathias M\\\"uller", "authors": "Mathias M\\\"uller and Rico Sennrich", "title": "Understanding the Properties of Minimum Bayes Risk Decoding in Neural\n  Machine Translation", "comments": "V1: ACL 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Machine Translation (NMT) currently exhibits biases such as producing\ntranslations that are too short and overgenerating frequent words, and shows\npoor robustness to copy noise in training data or domain shift. Recent work has\ntied these shortcomings to beam search -- the de facto standard inference\nalgorithm in NMT -- and Eikema & Aziz (2020) propose to use Minimum Bayes Risk\n(MBR) decoding on unbiased samples instead.\n  In this paper, we empirically investigate the properties of MBR decoding on a\nnumber of previously reported biases and failure cases of beam search. We find\nthat MBR still exhibits a length and token frequency bias, owing to the MT\nmetrics used as utility functions, but that MBR also increases robustness\nagainst copy noise in the training data and domain shift.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:31:05 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["M\u00fcller", "Mathias", ""], ["Sennrich", "Rico", ""]]}, {"id": "2105.08551", "submitter": "S{\\l}awomir Lasota", "authors": "S{\\l}awomir Lasota", "title": "Improved Ackermannian lower bound for the Petri nets reachability\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Petri nets, equivalently presentable as vector addition systems with states,\nare an established model of concurrency with widespread applications. The\nreachability problem, where we ask whether from a given initial configuration\nthere exists a sequence of valid execution steps reaching a given final\nconfiguration, is the central algorithmic problem for this model. The\ncomplexity of the problem has remained, until recently, one of the hardest open\nquestions in verification of concurrent systems. A first upper bound has been\nprovided only in 2015 by Leroux and Schmitz, then refined by the same authors\nto non-primitive recursive Ackermannian upper bound in 2019. The exponential\nspace lower bound, shown by Lipton already in 1976, remained the only known for\nover 40 years until a breakthrough non-elementary lower bound by\nCzerwi{\\'n}ski, Lasota, Lazic, Leroux and Mazowiecki in 2019. Finally, a\nmatching Ackermannian lower bound announced this year by Czerwi{\\'n}ski and\nOrlikowski, and independently by Leroux, established the complexity of the\nproblem.\n  Our contribution is an improvement of the former construction, making it\nconceptually simpler and more direct. On the way we improve the lower bound for\nvector addition systems with states in fixed dimension (or, equivalently, Petri\nnets with fixed number of places): while Czerwi{\\'n}ski and Orlikowski prove\n$F_k$-hardness (hardness for $k$th level in Grzegorczyk Hierarchy) in dimension\n$6k$, and Leroux in dimension $4k+5$, our simplified construction yields\n$F_k$-hardness already in dimension $3k+2$.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:36:25 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 08:18:02 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Lasota", "S\u0142awomir", ""]]}, {"id": "2105.08581", "submitter": "Matthias Hagen", "authors": "Vaibhav Kasturia, Marcel Gohsen, Matthias Hagen", "title": "Entity-Based Query Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search queries can be rather ambiguous: Is \"paris hilton\" meant to find\nthe latest news on the celebrity or to find a specific hotel in Paris? And in\nwhich of the worldwide more than 20 \"Parises\"? We propose to solve this\nambiguity problem by deriving entity-based query interpretations: given some\nquery, the task is to link suitable parts of the query to semantically\ncompatible entities in a background knowledge base. Our suggested approach to\nidentify the most reasonable interpretations of a query based on the contained\nentities focuses on effectiveness but also on efficiency since web search\nresponse times should not exceed some hundreds of milliseconds. In our\napproach, we propose to use query segmentation as a pre-processing step that\nfinds promising segment-based \"skeletons\". These skeletons are then enhanced to\n\"interpretations\" by linking the contained segments to entities from a\nknowledge base and then ranking the interpretations in a final step. An\nexperimental comparison on a corpus of 2,800 queries shows our approach to have\na better interpretation accuracy at a better run time than the previously most\neffective query entity linking methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:07:51 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Kasturia", "Vaibhav", ""], ["Gohsen", "Marcel", ""], ["Hagen", "Matthias", ""]]}, {"id": "2105.08585", "submitter": "Masahiro Naito", "authors": "Masahiro Naito, Sho Yokoi, Geewook Kim, Hidetoshi Shimodaira", "title": "Revisiting Additive Compositionality: AND, OR and NOT Operations with\n  Word Embeddings", "comments": "11 pages, accepted by ACL-IJCNLP 2021 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that typical word embedding methods such as Word2Vec and\nGloVe have the property that the meaning can be composed by adding up the\nembeddings (additive compositionality). Several theories have been proposed to\nexplain additive compositionality, but the following questions remain\nunanswered: (Q1) The assumptions of those theories do not hold for the\npractical word embedding. (Q2) Ordinary additive compositionality can be seen\nas an AND operation of word meanings, but it is not well understood how other\noperations, such as OR and NOT, can be computed by the embeddings. We address\nthese issues by the idea of frequency-weighted centering at its core. This\npaper proposes a post-processing method for bridging the gap between practical\nword embedding and the assumption of theory about additive compositionality as\nan answer to (Q1). It also gives a method for taking OR or NOT of the meaning\nby linear operation of word embedding as an answer to (Q2). Moreover, we\nconfirm experimentally that the accuracy of AND operation, i.e., the ordinary\nadditive compositionality, can be improved by our post-processing method (3.5x\nimprovement in top-100 accuracy) and that OR and NOT operations can be\nperformed correctly.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:13:04 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Naito", "Masahiro", ""], ["Yokoi", "Sho", ""], ["Kim", "Geewook", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "2105.08589", "submitter": "Wei Zhao", "authors": "Wei Zhao, Rahul Singh, Tarun Joshi, Agus Sudjianto, Vijayan N. Nair", "title": "Self-interpretable Convolutional Neural Networks for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models for natural language processing (NLP) are inherently\ncomplex and often viewed as black box in nature. This paper develops an\napproach for interpreting convolutional neural networks for text classification\nproblems by exploiting the local-linear models inherent in ReLU-DNNs. The CNN\nmodel combines the word embedding through convolutional layers, filters them\nusing max-pooling, and optimizes using a ReLU-DNN for classification. To get an\noverall self-interpretable model, the system of local linear models from the\nReLU DNN are mapped back through the max-pool filter to the appropriate\nn-grams. Our results on experimental datasets demonstrate that our proposed\ntechnique produce parsimonious models that are self-interpretable and have\ncomparable performance with respect to a more complex CNN model. We also study\nthe impact of the complexity of the convolutional layers and the classification\nlayers on the model performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:19:59 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 01:22:52 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhao", "Wei", ""], ["Singh", "Rahul", ""], ["Joshi", "Tarun", ""], ["Sudjianto", "Agus", ""], ["Nair", "Vijayan N.", ""]]}, {"id": "2105.08597", "submitter": "Mohammed Ibrahim", "authors": "Mohammed Ibrahim, Susan Gauch, Tyler Gerth, Brandon Cox", "title": "WOVe: Incorporating Word Order in GloVe Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word vector representations open up new opportunities to extract useful\ninformation from unstructured text. Defining a word as a vector made it easy\nfor the machine learning algorithms to understand a text and extract\ninformation from. Word vector representations have been used in many\napplications such word synonyms, word analogy, syntactic parsing, and many\nothers. GloVe, based on word contexts and matrix vectorization, is an\nef-fective vector-learning algorithm. It improves on previous vector-learning\nalgorithms. However, the GloVe model fails to explicitly consider the order in\nwhich words appear within their contexts. In this paper, multiple methods of\nincorporating word order in GloVe word embeddings are proposed. Experimental\nresults show that our Word Order Vector (WOVe) word embeddings approach\noutperforms unmodified GloVe on the natural lan-guage tasks of analogy\ncompletion and word similarity. WOVe with direct concatenation slightly\noutperformed GloVe on the word similarity task, increasing average rank by 2%.\nHowever, it greatly improved on the GloVe baseline on a word analogy task,\nachieving an average 36.34% improvement in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:28:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ibrahim", "Mohammed", ""], ["Gauch", "Susan", ""], ["Gerth", "Tyler", ""], ["Cox", "Brandon", ""]]}, {"id": "2105.08625", "submitter": "Xiangzhe Kong", "authors": "Xiangzhe Kong, Jialiang Huang, Ziquan Tung, Jian Guan and Minlie Huang", "title": "Stylized Story Generation with Style-Guided Planning", "comments": "9 pages, 3 figures. Will be included in Findings of ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current storytelling systems focus more ongenerating stories with coherent\nplots regard-less of the narration style, which is impor-tant for controllable\ntext generation. There-fore, we propose a new task, stylized story gen-eration,\nnamely generating stories with speci-fied style given a leading context. To\ntacklethe problem, we propose a novel generationmodel that first plans the\nstylized keywordsand then generates the whole story with theguidance of the\nkeywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency\nbetween the generated story andthe specified style. Experiments\ndemonstratesthat our model can controllably generateemo-tion-driven\norevent-driven stories based onthe ROCStories dataset (Mostafazadeh et\nal.,2016). Our study presents insights for stylizedstory generation in further\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:55:38 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 15:54:05 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kong", "Xiangzhe", ""], ["Huang", "Jialiang", ""], ["Tung", "Ziquan", ""], ["Guan", "Jian", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.08780", "submitter": "Kai North", "authors": "Abhinandan Desai and Kai North and Marcos Zampieri and Christopher M.\n  Homan", "title": "LCP-RIT at SemEval-2021 Task 1: Exploring Linguistic Features for\n  Lexical Complexity Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes team LCP-RIT's submission to the SemEval-2021 Task 1:\nLexical Complexity Prediction (LCP). The task organizers provided participants\nwith an augmented version of CompLex (Shardlow et al., 2020), an English\nmulti-domain dataset in which words in context were annotated with respect to\ntheir complexity using a five point Likert scale. Our system uses logistic\nregression and a wide range of linguistic features (e.g. psycholinguistic\nfeatures, n-grams, word frequency, POS tags) to predict the complexity of\nsingle words in this dataset. We analyze the impact of different linguistic\nfeatures in the classification performance and we evaluate the results in terms\nof mean absolute error, mean squared error, Pearson correlation, and Spearman\ncorrelation.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:55:04 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Desai", "Abhinandan", ""], ["North", "Kai", ""], ["Zampieri", "Marcos", ""], ["Homan", "Christopher M.", ""]]}, {"id": "2105.08807", "submitter": "Ganesh Jawahar", "authors": "Ganesh Jawahar, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, Laks\n  V.S. Lakshmanan", "title": "Exploring Text-to-Text Transformers for English to Hinglish Machine\n  Translation with Synthetic Code-Mixing", "comments": "Computational Approaches to Linguistic Code-Switching (CALCS 2021)\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe models focused at the understudied problem of translating between\nmonolingual and code-mixed language pairs. More specifically, we offer a wide\nrange of models that convert monolingual English text into Hinglish (code-mixed\nHindi and English). Given the recent success of pretrained language models, we\nalso test the utility of two recent Transformer-based encoder-decoder models\n(i.e., mT5 and mBART) on the task finding both to work well. Given the paucity\nof training data for code-mixing, we also propose a dependency-free method for\ngenerating code-mixed texts from bilingual distributed representations that we\nexploit for improving language model performance. In particular, armed with\nthis additional data, we adopt a curriculum learning approach where we first\nfinetune the language models on synthetic data then on gold code-mixed data. We\nfind that, although simple, our synthetic code-mixing method is competitive\nwith (and in some cases is even superior to) several standard methods\n(backtranslation, method based on equivalence constraint theory) under a\ndiverse set of conditions. Our work shows that the mT5 model, finetuned\nfollowing the curriculum learning procedure, achieves best translation\nperformance (12.67 BLEU). Our models place first in the overall ranking of the\nEnglish-Hinglish official shared task.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:50:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Jawahar", "Ganesh", ""], ["Nagoudi", "El Moatez Billah", ""], ["Abdul-Mageed", "Muhammad", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "2105.08812", "submitter": "Mohammed Ibrahim", "authors": "Mohammed Ibrahim, Susan Gauch, Omar Salman, Mohammed Alqahatani", "title": "An Automated Method to Enrich Consumer Health Vocabularies Using GloVe\n  Word Embeddings and An Auxiliary Lexical Resource", "comments": "24 pages, 7 figures, 7 Tables, Journal", "journal-ref": null, "doi": "10.2196/preprints.26160", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Clear language makes communication easier between any two\nparties. A layman may have difficulty communicating with a professional due to\nnot understanding the specialized terms common to the domain. In healthcare, it\nis rare to find a layman knowledgeable in medical terminology which can lead to\npoor understanding of their condition and/or treatment. To bridge this gap,\nseveral professional vocabularies and ontologies have been created to map\nlaymen medical terms to professional medical terms and vice versa.\n  Objective: Many of the presented vocabularies are built manually or\nsemi-automatically requiring large investments of time and human effort and\nconsequently the slow growth of these vocabularies. In this paper, we present\nan automatic method to enrich laymen's vocabularies that has the benefit of\nbeing able to be applied to vocabularies in any domain.\n  Methods: Our entirely automatic approach uses machine learning, specifically\nGlobal Vectors for Word Embeddings (GloVe), on a corpus collected from a social\nmedia healthcare platform to extend and enhance consumer health vocabularies\n(CHV). Our approach further improves the CHV by incorporating synonyms and\nhyponyms from the WordNet ontology. The basic GloVe and our novel algorithms\nincorporating WordNet were evaluated using two laymen datasets from the\nNational Library of Medicine (NLM), Open-Access Consumer Health Vocabulary (OAC\nCHV) and MedlinePlus Healthcare Vocabulary.\n  Results: The results show that GloVe was able to find new laymen terms with\nan F-score of 48.44%. Furthermore, our enhanced GloVe approach outperformed\nbasic GloVe with an average F-score of 61%, a relative improvement of 25%.\nFurthermore, the enhanced GloVe showed a statistical significance over the two\nground truth datasets with P<.001.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:16:45 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ibrahim", "Mohammed", ""], ["Gauch", "Susan", ""], ["Salman", "Omar", ""], ["Alqahatani", "Mohammed", ""]]}, {"id": "2105.08840", "submitter": "Yunhao Yang", "authors": "Yunhao Yang, Zhaokun Xue", "title": "Representation Learning in Sequence to Sequence Tasks: Multi-filter\n  Gaussian Mixture Autoencoder", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heterogeneity of sentences exists in sequence to sequence tasks such as\nmachine translation. Sentences with largely varied meanings or grammatical\nstructures may increase the difficulty of convergence while training the\nnetwork. In this paper, we introduce a model to resolve the heterogeneity in\nthe sequence to sequence task. The Multi-filter Gaussian Mixture Autoencoder\n(MGMAE) utilizes an autoencoder to learn the representations of the inputs. The\nrepresentations are the outputs from the encoder, lying in the latent space\nwhose dimension is the hidden dimension of the encoder. The representations of\ntraining data in the latent space are used to train Gaussian mixtures. The\nlatent space representations are divided into several mixtures of Gaussian\ndistributions. A filter (decoder) is tuned to fit the data in one of the\nGaussian distributions specifically. Each Gaussian is corresponding to one\nfilter so that the filter is responsible for the heterogeneity within this\nGaussian. Thus the heterogeneity of the training data can be resolved.\nComparative experiments are conducted on the Geo-query dataset and\nEnglish-French translation. Our experiments show that compares to the\ntraditional encoder-decoder model, this network achieves better performance on\nsequence to sequence tasks such as machine translation and question answering.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:42:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yang", "Yunhao", ""], ["Xue", "Zhaokun", ""]]}, {"id": "2105.08855", "submitter": "Ana Marasovi\\'c", "authors": "Kaiser Sun and Ana Marasovi\\'c", "title": "Effective Attention Sheds Light On Interpretability", "comments": "Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An attention matrix of a transformer self-attention sublayer can provably be\ndecomposed into two components and only one of them (effective attention)\ncontributes to the model output. This leads us to ask whether visualizing\neffective attention gives different conclusions than interpretation of standard\nattention. Using a subset of the GLUE tasks and BERT, we carry out an analysis\nto compare the two attention matrices, and show that their interpretations\ndiffer. Effective attention is less associated with the features related to the\nlanguage modeling pretraining such as the separator token, and it has more\npotential to illustrate linguistic features captured by the model for solving\nthe end-task. Given the found differences, we recommend using effective\nattention for studying a transformer's behavior since it is more pertinent to\nthe model output by design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 23:41:26 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sun", "Kaiser", ""], ["Marasovi\u0107", "Ana", ""]]}, {"id": "2105.08882", "submitter": "Enrico Santus", "authors": "Beatrice Portelli, Daniele Passab\\`i, Edoardo Lenzi, Giuseppe Serra,\n  Enrico Santus and Emmanuele Chersoni", "title": "Improving Adverse Drug Event Extraction with SpanBERT on Different Text\n  Typologies", "comments": "11 pages, AAAI, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Internet users are reporting Adverse Drug Events (ADE) on\nsocial media, blogs and health forums. Because of the large volume of reports,\npharmacovigilance is seeking to resort to NLP to monitor these outlets. We\npropose for the first time the use of the SpanBERT architecture for the task of\nADE extraction: this new version of the popular BERT transformer showed\nimproved capabilities with multi-token text spans. We validate our hypothesis\nwith experiments on two datasets (SMM4H and CADEC) with different text\ntypologies (tweets and blog posts), finding that SpanBERT combined with a CRF\noutperforms all the competitors on both of them.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 02:01:09 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Portelli", "Beatrice", ""], ["Passab\u00ec", "Daniele", ""], ["Lenzi", "Edoardo", ""], ["Serra", "Giuseppe", ""], ["Santus", "Enrico", ""], ["Chersoni", "Emmanuele", ""]]}, {"id": "2105.08901", "submitter": "Zeqi Tan", "authors": "Zeqi Tan, Yongliang Shen, Shuai Zhang, Weiming Lu, Yueting Zhuang", "title": "A Sequence-to-Set Network for Nested Named Entity Recognition", "comments": "Accepted to IJCAI 2021, submission version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a widely studied task in natural language\nprocessing. Recently, a growing number of studies have focused on the nested\nNER. The span-based methods, considering the entity recognition as a span\nclassification task, can deal with nested entities naturally. But they suffer\nfrom the huge search space and the lack of interactions between entities. To\naddress these issues, we propose a novel sequence-to-set neural network for\nnested NER. Instead of specifying candidate spans in advance, we provide a\nfixed set of learnable vectors to learn the patterns of the valuable spans. We\nutilize a non-autoregressive decoder to predict the final set of entities in\none pass, in which we are able to capture dependencies between entities.\nCompared with the sequence-to-sequence method, our model is more suitable for\nsuch unordered recognition task as it is insensitive to the label order. In\naddition, we utilize the loss function based on bipartite matching to compute\nthe overall training loss. Experimental results show that our proposed model\nachieves state-of-the-art on three nested NER corpora: ACE 2004, ACE 2005 and\nKBP 2017. The code is available at\nhttps://github.com/zqtan1024/sequence-to-set.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:10:04 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 14:56:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tan", "Zeqi", ""], ["Shen", "Yongliang", ""], ["Zhang", "Shuai", ""], ["Lu", "Weiming", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2105.08920", "submitter": "Jian Guan", "authors": "Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi\n  Mao, Changjie Fan, Minlie Huang", "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics", "comments": "ACL 2021 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic metrics are essential for developing natural language generation\n(NLG) models, particularly for open-ended language generation tasks such as\nstory generation. However, existing automatic metrics are observed to correlate\npoorly with human evaluation. The lack of standardized benchmark datasets makes\nit difficult to fully evaluate the capabilities of a metric and fairly compare\ndifferent metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating\nopen-ended story generation metrics. OpenMEVA provides a comprehensive test\nsuite to assess the capabilities of metrics, including (a) the correlation with\nhuman judgments, (b) the generalization to different model outputs and\ndatasets, (c) the ability to judge story coherence, and (d) the robustness to\nperturbations. To this end, OpenMEVA includes both manually annotated stories\nand auto-constructed test examples. We evaluate existing metrics on OpenMEVA\nand observe that they have poor correlation with human judgments, fail to\nrecognize discourse-level incoherence, and lack inferential knowledge (e.g.,\ncausal order between events), the generalization ability and robustness. Our\nstudy presents insights for developing NLG models and metrics in further\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:45:07 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Guan", "Jian", ""], ["Zhang", "Zhexin", ""], ["Feng", "Zhuoer", ""], ["Liu", "Zitao", ""], ["Ding", "Wenbiao", ""], ["Mao", "Xiaoxi", ""], ["Fan", "Changjie", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.08928", "submitter": "Minghuan Tan", "authors": "Minghuan Tan and Lei Wang and Lingxiao Jiang and Jing Jiang", "title": "Investigating Math Word Problems using Pretrained Multilingual Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we revisit math word problems~(MWPs) from the cross-lingual\nand multilingual perspective. We construct our MWP solvers over pretrained\nmultilingual language models using sequence-to-sequence model with copy\nmechanism. We compare how the MWP solvers perform in cross-lingual and\nmultilingual scenarios. To facilitate the comparison of cross-lingual\nperformance, we first adapt the large-scale English dataset MathQA as a\ncounterpart of the Chinese dataset Math23K. Then we extend several English\ndatasets to bilingual datasets through machine translation plus human\nannotation. Our experiments show that the MWP solvers may not be transferred to\na different language even if the target expressions have the same operator set\nand constants. But for both cross-lingual and multilingual cases, it can be\nbetter generalized if problem types exist on both source language and target\nlanguage.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 05:17:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Tan", "Minghuan", ""], ["Wang", "Lei", ""], ["Jiang", "Lingxiao", ""], ["Jiang", "Jing", ""]]}, {"id": "2105.08956", "submitter": "Ohad Rozen", "authors": "Ohad Rozen, David Carmel, Avihai Mejer, Vitaly Mirkis, and Yftah Ziser", "title": "Answering Product-Questions by Utilizing Questions from Other\n  Contextually Similar Products", "comments": "Accepted by NAACL2021, 9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the answer to a product-related question is an emerging field of\nresearch that recently attracted a lot of attention. Answering subjective and\nopinion-based questions is most challenging due to the dependency on\ncustomer-generated content. Previous works mostly focused on review-aware\nanswer prediction; however, these approaches fail for new or unpopular\nproducts, having no (or only a few) reviews at hand. In this work, we propose a\nnovel and complementary approach for predicting the answer for such questions,\nbased on the answers for similar questions asked on similar products. We\nmeasure the contextual similarity between products based on the answers they\nprovide for the same question. A mixture-of-expert framework is used to predict\nthe answer by aggregating the answers from contextually similar products.\nEmpirical results demonstrate that our model outperforms strong baselines on\nsome segments of questions, namely those that have roughly ten or more similar\nresolved questions in the corpus. We additionally publish two large-scale\ndatasets used in this work, one is of similar product question pairs, and the\nsecond is of product question-answer pairs.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:05:00 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Rozen", "Ohad", ""], ["Carmel", "David", ""], ["Mejer", "Avihai", ""], ["Mirkis", "Vitaly", ""], ["Ziser", "Yftah", ""]]}, {"id": "2105.08961", "submitter": "Jacob Russin", "authors": "Jacob Russin, Roland Fernandez, Hamid Palangi, Eric Rosen, Nebojsa\n  Jojic, Paul Smolensky, Jianfeng Gao", "title": "Compositional Processing Emerges in Neural Networks Solving Math\n  Problems", "comments": "7 pages, 2 figures, Accepted to CogSci 2021 for poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A longstanding question in cognitive science concerns the learning mechanisms\nunderlying compositionality in human cognition. Humans can infer the structured\nrelationships (e.g., grammatical rules) implicit in their sensory observations\n(e.g., auditory speech), and use this knowledge to guide the composition of\nsimpler meanings into complex wholes. Recent progress in artificial neural\nnetworks has shown that when large models are trained on enough linguistic\ndata, grammatical structure emerges in their representations. We extend this\nwork to the domain of mathematical reasoning, where it is possible to formulate\nprecise hypotheses about how meanings (e.g., the quantities corresponding to\nnumerals) should be composed according to structured rules (e.g., order of\noperations). Our work shows that neural networks are not only able to infer\nsomething about the structured relationships implicit in their training data,\nbut can also deploy this knowledge to guide the composition of individual\nmeanings into composite wholes.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:24:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Russin", "Jacob", ""], ["Fernandez", "Roland", ""], ["Palangi", "Hamid", ""], ["Rosen", "Eric", ""], ["Jojic", "Nebojsa", ""], ["Smolensky", "Paul", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2105.08963", "submitter": "Jian Guan", "authors": "Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, Minlie\n  Huang", "title": "Long Text Generation by Modeling Sentence-Level and Discourse-Level\n  Coherence", "comments": "ACL 2021 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating long and coherent text is an important but challenging task,\nparticularly for open-ended language generation tasks such as story generation.\nDespite the success in modeling intra-sentence coherence, existing generation\nmodels (e.g., BART) still struggle to maintain a coherent event sequence\nthroughout the generated text. We conjecture that this is because of the\ndifficulty for the decoder to capture the high-level semantics and discourse\nstructures in the context beyond token-level co-occurrence. In this paper, we\npropose a long text generation model, which can represent the prefix sentences\nat sentence level and discourse level in the decoding process. To this end, we\npropose two pretraining objectives to learn the representations by predicting\ninter-sentence semantic similarity and distinguishing between normal and\nshuffled sentence orders. Extensive experiments show that our model can\ngenerate more coherent texts than state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:29:08 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Guan", "Jian", ""], ["Mao", "Xiaoxi", ""], ["Fan", "Changjie", ""], ["Liu", "Zitao", ""], ["Ding", "Wenbiao", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.09002", "submitter": "Haipeng Gao", "authors": "Haipeng Gao, Kun Yang, Yuxue Yang, Rufai Yusuf Zakari, Jim Wilson\n  Owusu, Ke Qin", "title": "QuatDE: Dynamic Quaternion Embedding for Knowledge Graph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding has been an active research topic for knowledge\nbase completion (KGC), with progressive improvement from the initial TransE,\nTransH, RotatE et al to the current state-of-the-art QuatE. However, QuatE\nignores the multi-faceted nature of the entity and the complexity of the\nrelation, only using rigorous operation on quaternion space to capture the\ninteraction between entitiy pair and relation, leaving opportunities for better\nknowledge representation which will finally help KGC. In this paper, we propose\na novel model, QuatDE, with a dynamic mapping strategy to explicitly capture\nthe variety of relational patterns and separate different semantic information\nof the entity, using transition vectors to adjust the point position of the\nentity embedding vectors in the quaternion space via Hamilton product,\nenhancing the feature interaction capability between elements of the triplet.\nExperiment results show QuatDE achieves state-of-the-art performance on three\nwell-established knowledge graph completion benchmarks. In particular, the MR\nevaluation has relatively increased by 26% on WN18 and 15% on WN18RR, which\nproves the generalization of QuatDE.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:10:39 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 07:00:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gao", "Haipeng", ""], ["Yang", "Kun", ""], ["Yang", "Yuxue", ""], ["Zakari", "Rufai Yusuf", ""], ["Owusu", "Jim Wilson", ""], ["Qin", "Ke", ""]]}, {"id": "2105.09043", "submitter": "Kiet Nguyen", "authors": "Phong Nguyen-Thuan Do, Nhat Duy Nguyen, Tin Van Huynh, Kiet Van\n  Nguyen, Anh Gia-Tuan Nguyen, Ngan Luu-Thuy Nguyen", "title": "Sentence Extraction-Based Machine Reading Comprehension for Vietnamese", "comments": "Accepted by KSEM 2021 (International Conference on Knowledge Science,\n  Engineering and Management)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of natural language processing (NLP) in general and machine\nreading comprehension in particular has attracted the great attention of the\nresearch community. In recent years, there are a few datasets for machine\nreading comprehension tasks in Vietnamese with large sizes, such as UIT-ViQuAD\nand UIT-ViNewsQA. However, the datasets are not diverse in answers to serve the\nresearch. In this paper, we introduce UIT-ViWikiQA, the first dataset for\nevaluating sentence extraction-based machine reading comprehension in the\nVietnamese language. The UIT-ViWikiQA dataset is converted from the UIT-ViQuAD\ndataset, consisting of comprises 23.074 question-answers based on 5.109\npassages of 174 Wikipedia Vietnamese articles. We propose a conversion\nalgorithm to create the dataset for sentence extraction-based machine reading\ncomprehension and three types of approaches for sentence extraction-based\nmachine reading comprehension in Vietnamese. Our experiments show that the best\nmachine model is XLM-R_Large, which achieves an exact match (EM) of 85.97% and\nan F1-score of 88.77% on our dataset. Besides, we analyze experimental results\nin terms of the question type in Vietnamese and the effect of context on the\nperformance of the MRC models, thereby showing the challenges from the\nUIT-ViWikiQA dataset that we propose to the language processing community.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:22:27 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 04:20:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Do", "Phong Nguyen-Thuan", ""], ["Nguyen", "Nhat Duy", ""], ["Van Huynh", "Tin", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Anh Gia-Tuan", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2105.09045", "submitter": "Shengfei Lyu", "authors": "Shengfei Lyu, Xingyu Wu, Jinlong Li, Qiuju Chen, and Huanhuan Chen", "title": "Do Models Learn the Directionality of Relations? A New Evaluation Task:\n  Relation Direction Recognition", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks such as BERT have made great progress in relation\nclassification. Although they can achieve good performance, it is still a\nquestion of concern whether these models recognize the directionality of\nrelations, especially when they may lack interpretability. To explore the\nquestion, a novel evaluation task, called Relation Direction Recognition (RDR),\nis proposed to explore whether models learn the directionality of relations.\nThree metrics for RDR are introduced to measure the degree to which models\nrecognize the directionality of relations. Several state-of-the-art models are\nevaluated on RDR. Experimental results on a real-world dataset indicate that\nthere are clear gaps among them in recognizing the directionality of relations,\neven though these models obtain similar performance in the traditional metric\n(e.g. Macro-F1). Finally, some suggestions are discussed to enhance models to\nrecognize the directionality of relations from the perspective of model design\nor training.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:24:50 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lyu", "Shengfei", ""], ["Wu", "Xingyu", ""], ["Li", "Jinlong", ""], ["Chen", "Qiuju", ""], ["Chen", "Huanhuan", ""]]}, {"id": "2105.09050", "submitter": "Jia-Chen Gu", "authors": "Jia-Chen Gu, Hui Liu, Zhen-Hua Ling, Quan Liu, Zhigang Chen, Xiaodan\n  Zhu", "title": "Partner Matters! An Empirical Study on Fusing Personas for Personalized\n  Response Selection in Retrieval-Based Chatbots", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persona can function as the prior knowledge for maintaining the consistency\nof dialogue systems. Most of previous studies adopted the self persona in\ndialogue whose response was about to be selected from a set of candidates or\ndirectly generated, but few have noticed the role of partner in dialogue. This\npaper makes an attempt to thoroughly explore the impact of utilizing personas\nthat describe either self or partner speakers on the task of response selection\nin retrieval-based chatbots. Four persona fusion strategies are designed, which\nassume personas interact with contexts or responses in different ways. These\nstrategies are implemented into three representative models for response\nselection, which are based on the Hierarchical Recurrent Encoder (HRE),\nInteractive Matching Network (IMN) and Bidirectional Encoder Representations\nfrom Transformers (BERT) respectively. Empirical studies on the Persona-Chat\ndataset show that the partner personas neglected in previous studies can\nimprove the accuracy of response selection in the IMN- and BERT-based models.\nBesides, our BERT-based model implemented with the context-response-aware\npersona fusion strategy outperforms previous methods by margins larger than\n2.7% on original personas and 4.6% on revised personas in terms of hits@1\n(top-1 accuracy), achieving a new state-of-the-art performance on the\nPersona-Chat dataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:32:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 02:43:50 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Gu", "Jia-Chen", ""], ["Liu", "Hui", ""], ["Ling", "Zhen-Hua", ""], ["Liu", "Quan", ""], ["Chen", "Zhigang", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2105.09052", "submitter": "Daryna Dementieva", "authors": "Daryna Dementieva, Daniil Moskovskiy, Varvara Logacheva, David Dale,\n  Olga Kozlova, Nikita Semenov, and Alexander Panchenko", "title": "Methods for Detoxification of Texts for the Russian Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the first study of automatic detoxification of Russian texts to\ncombat offensive language. Such a kind of textual style transfer can be used,\nfor instance, for processing toxic content in social media. While much work has\nbeen done for the English language in this field, it has never been solved for\nthe Russian language yet. We test two types of models - unsupervised approach\nbased on BERT architecture that performs local corrections and supervised\napproach based on pretrained language GPT-2 model - and compare them with\nseveral baselines. In addition, we describe evaluation setup providing training\ndatasets and metrics for automatic evaluation. The results show that the tested\napproaches can be successfully used for detoxification, although there is room\nfor improvement.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:37:44 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dementieva", "Daryna", ""], ["Moskovskiy", "Daniil", ""], ["Logacheva", "Varvara", ""], ["Dale", "David", ""], ["Kozlova", "Olga", ""], ["Semenov", "Nikita", ""], ["Panchenko", "Alexander", ""]]}, {"id": "2105.09081", "submitter": "Rafael Torres Anchieta", "authors": "Jeziel C. Marinho, Rafael T. Anchieta, and Raimundo S. Moura", "title": "Essay-BR: a Brazilian Corpus of Essays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Essay Scoring (AES) is defined as the computer technology that\nevaluates and scores the written essays, aiming to provide computational models\nto grade essays either automatically or with minimal human involvement. While\nthere are several AES studies in a variety of languages, few of them are\nfocused on the Portuguese language. The main reason is the lack of a corpus\nwith manually graded essays. In order to bridge this gap, we create a large\ncorpus with several essays written by Brazilian high school students on an\nonline platform. All of the essays are argumentative and were scored across\nfive competencies by experts. Moreover, we conducted an experiment on the\ncreated corpus and showed challenges posed by the Portuguese language. Our\ncorpus is publicly available at https://github.com/rafaelanchieta/essay.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:59:46 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Marinho", "Jeziel C.", ""], ["Anchieta", "Rafael T.", ""], ["Moura", "Raimundo S.", ""]]}, {"id": "2105.09085", "submitter": "Jinhong Zhang", "authors": "Jinhong Zhang", "title": "Combining GCN and Transformer for Chinese Grammatical Error Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our system at NLPTEA-2020 Task: Chinese Grammatical\nError Diagnosis (CGED). The goal of CGED is to diagnose four types of\ngrammatical errors: word selection (S), redundant words (R), missing words (M),\nand disordered words (W). The automatic CGED system contains two parts\nincluding error detection and error correction and our system is designed to\nsolve the error detection problem. Our system is built on three models: 1) a\nBERT-based model leveraging syntactic information; 2) a BERT-based model\nleveraging contextual embeddings; 3) a lexicon-based graph neural network\nleveraging lexical information. We also design an ensemble mechanism to improve\nthe single model's performance. Finally, our system achieves the highest F1\nscores at detection level and identification level among all teams\nparticipating in the CGED 2020 task.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 12:17:07 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 03:38:13 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 16:07:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Jinhong", ""]]}, {"id": "2105.09114", "submitter": "Bimal Bhattarai", "authors": "Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao", "title": "Explainable Tsetlin Machine framework for fake news detection with\n  credibility score assessment", "comments": "11 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of fake news, i.e., news intentionally spread for\nmisinformation, poses a threat to individuals and society. Despite various\nfact-checking websites such as PolitiFact, robust detection techniques are\nrequired to deal with the increase in fake news. Several deep learning models\nshow promising results for fake news classification, however, their black-box\nnature makes it difficult to explain their classification decisions and\nquality-assure the models. We here address this problem by proposing a novel\ninterpretable fake news detection framework based on the recently introduced\nTsetlin Machine (TM). In brief, we utilize the conjunctive clauses of the TM to\ncapture lexical and semantic properties of both true and fake news text.\nFurther, we use the clause ensembles to calculate the credibility of fake news.\nFor evaluation, we conduct experiments on two publicly available datasets,\nPolitiFact and GossipCop, and demonstrate that the TM framework significantly\noutperforms previously published baselines by at least $5\\%$ in terms of\naccuracy, with the added benefit of an interpretable logic-based\nrepresentation. Further, our approach provides higher F1-score than BERT and\nXLNet, however, we obtain slightly lower accuracy. We finally present a case\nstudy on our model's explainability, demonstrating how it decomposes into\nmeaningful words and their negations.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:18:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bhattarai", "Bimal", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2105.09127", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon and F. Vagaggini", "title": "Robustness and stability of enterprise intranet social networks: The\n  impact of moderators", "comments": null, "journal-ref": "Information Processing and Management 53(6), 1287-1298 (2017)", "doi": "10.1016/j.ipm.2017.07.001", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study, we tested the robustness of three communication networks\nextracted from the online forums included in the intranet platforms of three\nlarge companies. For each company we analyzed the communication among employees\nboth in terms of network structure and content (language used). Over a period\nof eight months, we analyzed more than 52,000 messages posted by approximately\n12,000 employees. Specifically, we tested the network robustness and the\nstability of a set of structural and semantic metrics, while applying several\ndifferent node removal strategies. We removed the forum moderators, the\nspammers, the overly connected nodes and the nodes lying at the network\nperiphery, also testing different combinations of these selections. Results\nindicate that removing spammers and very peripheral nodes can be a relatively\nlow impact strategy in this context; accordingly, it could be used to clean the\nnoise generated by these types of social actor and to reduce the computation\ncomplexity of the analysis. On the other hand, the removal of moderators seems\nto have a significant impact on the network connectivity and the shared\ncontent. The most affected variables are closeness centrality and contribution\nindex. We also found that the removal of overly connected nodes can\nsignificantly change the network structure. Lastly, we compared the behavior of\nmoderators with the other users, finding distinctive characteristics by which\nmoderators can be identified when their list is unknown. Our findings can help\nonline community managers to understand the role of moderators within intranet\nforums and can be useful for social network analysts who are interested in\nevaluating the effects of graph simplification techniques.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:43:03 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Vagaggini", "F.", ""]]}, {"id": "2105.09137", "submitter": "Saumya Banthia", "authors": "Saumya Banthia, Anantha Sharma, Ravi Mangipudi", "title": "TableZa -- A classical Computer Vision approach to Tabular Extraction", "comments": "14 pages, 16 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer aided Tabular Data Extraction has always been a very challenging and\nerror prone task because it demands both Spectral and Spatial Sanity of data.\nIn this paper we discuss an approach for Tabular Data Extraction in the realm\nof document comprehension. Given the different kinds of the Tabular formats\nthat are often found across various documents, we discuss a novel approach\nusing Computer Vision for extraction of tabular data from images or vector\npdf(s) converted to image(s).\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:55:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Banthia", "Saumya", ""], ["Sharma", "Anantha", ""], ["Mangipudi", "Ravi", ""]]}, {"id": "2105.09142", "submitter": "Maxime Peyrard", "authors": "Maxime Peyrard, Beatriz Borges, Kristina Gligori\\'c and Robert West", "title": "Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?", "comments": "Published at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The automatic detection of humor poses a grand challenge for natural language\nprocessing. Transformer-based systems have recently achieved remarkable results\non this task, but they usually (1)~were evaluated in setups where serious vs\nhumorous texts came from entirely different sources, and (2)~focused on\nbenchmarking performance without providing insights into how the models work.\nWe make progress in both respects by training and analyzing transformer-based\nhumor recognition models on a recently introduced dataset consisting of minimal\npairs of aligned sentences, one serious, the other humorous. We find that,\nalthough our aligned dataset is much harder than previous datasets,\ntransformer-based models recognize the humorous sentence in an aligned pair\nwith high accuracy (78%). In a careful error analysis, we characterize easy vs\nhard instances. Finally, by analyzing attention weights, we obtain important\ninsights into the mechanisms by which transformers recognize humor. Most\nremarkably, we find clear evidence that one single attention head learns to\nrecognize the words that make a test sentence humorous, even without access to\nthis information at training time.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:02:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Peyrard", "Maxime", ""], ["Borges", "Beatriz", ""], ["Gligori\u0107", "Kristina", ""], ["West", "Robert", ""]]}, {"id": "2105.09154", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "M. Elshendy, A. Fronzetti Colladon, E. Battistoni, P. A. Gloor", "title": "Using four different online media sources to forecast the crude oil\n  price", "comments": null, "journal-ref": "Journal of Information Science 44(3), 408-421 (2018)", "doi": "10.1177/0165551517698298", "report-no": null, "categories": "econ.GN cs.CL q-fin.EC q-fin.GN", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study looks for signals of economic awareness on online social media and\ntests their significance in economic predictions. The study analyses, over a\nperiod of two years, the relationship between the West Texas Intermediate daily\ncrude oil price and multiple predictors extracted from Twitter, Google Trends,\nWikipedia, and the Global Data on Events, Language, and Tone database (GDELT).\nSemantic analysis is applied to study the sentiment, emotionality and\ncomplexity of the language used. Autoregressive Integrated Moving Average with\nExplanatory Variable (ARIMAX) models are used to make predictions and to\nconfirm the value of the study variables. Results show that the combined\nanalysis of the four media platforms carries valuable information in making\nfinancial forecasting. Twitter language complexity, GDELT number of articles\nand Wikipedia page reads have the highest predictive power. This study also\nallows a comparison of the different fore-sighting abilities of each platform,\nin terms of how many days ahead a platform can predict a price movement before\nit happens. In comparison with previous work, more media sources and more\ndimensions of the interaction and of the language used are combined in a joint\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:19:18 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Elshendy", "M.", ""], ["Colladon", "A. Fronzetti", ""], ["Battistoni", "E.", ""], ["Gloor", "P. A.", ""]]}, {"id": "2105.09198", "submitter": "Isar Nejadgholi", "authors": "Rajitha Hathurusinghe, Isar Nejadgholi, Miodrag Bolic", "title": "A Privacy-Preserving Approach to Extraction of Personal Information\n  through Automatic Annotation and Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We curated WikiPII, an automatically labeled dataset composed of Wikipedia\nbiography pages, annotated for personal information extraction. Although\nautomatic annotation can lead to a high degree of label noise, it is an\ninexpensive process and can generate large volumes of annotated documents. We\ntrained a BERT-based NER model with WikiPII and showed that with an adequately\nlarge training dataset, the model can significantly decrease the cost of manual\ninformation extraction, despite the high level of label noise. In a similar\napproach, organizations can leverage text mining techniques to create\ncustomized annotated datasets from their historical data without sharing the\nraw data for human annotation. Also, we explore collaborative training of NER\nmodels through federated learning when the annotation is noisy. Our results\nsuggest that depending on the level of trust to the ML operator and the volume\nof the available data, distributed training can be an effective way of training\na personal information identifier in a privacy-preserved manner. Research\nmaterial is available at https://github.com/ratmcu/wikipiifed.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:17:44 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Hathurusinghe", "Rajitha", ""], ["Nejadgholi", "Isar", ""], ["Bolic", "Miodrag", ""]]}, {"id": "2105.09208", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "P. A. Gloor, A. Fronzetti Colladon, F. Grippa, G. Giacomelli", "title": "Forecasting managerial turnover through e-mail based social network\n  analysis", "comments": null, "journal-ref": "Computers in Human Behavior 71, 343-352 (2017)", "doi": "10.1016/j.chb.2017.02.017", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study we propose a method based on e-mail social network analysis to\ncompare the communication behavior of managers who voluntarily quit their job\nand managers who decide to stay. Collecting 18 months of e-mail, we analyzed\nthe communication behavior of 866 managers, out of which 111 left a large\nglobal service company. We compared differences in communication patterns by\ncomputing social network metrics, such as betweenness and closeness centrality,\nand content analysis indicators, such as emotionality and complexity of the\nlanguage used. To study the emergence of managers' disengagement, we made a\ndistinction based on the period of e-mail data examined. We observed\ncommunications during months 5 and 4 before managers left, and found\nsignificant variations in both their network structure and use of language.\nResults indicate that on average managers who quit had lower closeness\ncentrality and less engaged conversations. In addition, managers who chose to\nquit tended to shift their communication behavior starting from 5 months before\nleaving, by increasing their degree and closeness centrality, the complexity of\ntheir language, as well as their oscillations in betweenness centrality and the\nnumber of \"nudges\" they need to send to peers before getting an answer.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:39:55 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Gloor", "P. A.", ""], ["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Giacomelli", "G.", ""]]}, {"id": "2105.09226", "submitter": "Divyansh Singh", "authors": "Divyansh Singh", "title": "Detection of Emotions in Hindi-English Code Mixed Text Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent times, we have seen an increased use of text chat for communication\non social networks and smartphones. This particularly involves the use of\nHindi-English code-mixed text which contains words which are not recognized in\nEnglish vocabulary. We have worked on detecting emotions in these mixed data\nand classify the sentences in human emotions which are angry, fear, happy or\nsad. We have used state of the art natural language processing models and\ncompared their performance on the dataset comprising sentences in this mixed\ndata. The dataset was collected and annotated from sources and then used to\ntrain the models.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:12:01 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 08:17:59 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 17:40:23 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Singh", "Divyansh", ""]]}, {"id": "2105.09235", "submitter": "Giovanni Bonetta", "authors": "Giovanni Bonetta, Rossella Cancelliere, Ding Liu, Paul Vozila", "title": "Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation", "comments": "The International FLAIRS Conference Proceedings volume 34 issue 1", "journal-ref": null, "doi": "10.32473/flairs.v34i1.128369", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models have demonstrated excellent capabilities of\ncapturing patterns and structures in natural language generation and achieved\nstate-of-the-art results in many tasks. In this paper we present a\ntransformer-based model for multi-turn dialog response generation. Our solution\nis based on a hybrid approach which augments a transformer-based generative\nmodel with a novel retrieval mechanism, which leverages the memorized\ninformation in the training data via k-Nearest Neighbor search. Our system is\nevaluated on two datasets made by customer/assistant dialogs: the Taskmaster-1,\nreleased by Google and holding high quality, goal-oriented conversational data\nand a proprietary dataset collected from a real customer service call center.\nBoth achieve better BLEU scores over strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:34:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bonetta", "Giovanni", ""], ["Cancelliere", "Rossella", ""], ["Liu", "Ding", ""], ["Vozila", "Paul", ""]]}, {"id": "2105.09259", "submitter": "Zehui Lin", "authors": "Zehui Lin, Liwei Wu, Mingxuan Wang, Lei Li", "title": "Learning Language Specific Sub-network for Multilingual Machine\n  Translation", "comments": "ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual neural machine translation aims at learning a single translation\nmodel for multiple languages. These jointly trained models often suffer from\nperformance degradation on rich-resource language pairs. We attribute this\ndegeneration to parameter interference. In this paper, we propose LaSS to\njointly train a single unified multilingual MT model. LaSS learns Language\nSpecific Sub-network (LaSS) for each language pair to counter parameter\ninterference. Comprehensive experiments on IWSLT and WMT datasets with various\nTransformer architectures show that LaSS obtains gains on 36 language pairs by\nup to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at\neasy extension to new language pairs and zero-shot translation.LaSS boosts\nzero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes\nand trained models are available at https://github.com/NLP-Playground/LaSS.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:08:38 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 07:31:13 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lin", "Zehui", ""], ["Wu", "Liwei", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2105.09284", "submitter": "Preslav Nakov", "authors": "Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio\n  Silvestri, Hamed Firooz, Preslav Nakov, Giovanni Da San Martino", "title": "SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and\n  Images", "comments": "propaganda, disinformation, misinformation, fake news, memes,\n  multimodality", "journal-ref": "SemEval-2021", "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in\nTexts and Images: the data, the annotation guidelines, the evaluation setup,\nthe results, and the participating systems. The task focused on memes and had\nthree subtasks: (i) detecting the techniques in the text, (ii) detecting the\ntext spans where the techniques are used, and (iii) detecting techniques in the\nentire meme, i.e., both in the text and in the image. It was a popular task,\nattracting 71 registrations, and 22 teams that eventually made an official\nsubmission on the test set. The evaluation results for the third subtask\nconfirmed the importance of both modalities, the text and the image. Moreover,\nsome teams reported benefits when not just combining the two modalities, e.g.,\nby using early or late fusion, but rather modeling the interaction between them\nin a joint model.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 05:00:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dimitrov", "Dimitar", ""], ["Ali", "Bishr Bin", ""], ["Shaar", "Shaden", ""], ["Alam", "Firoj", ""], ["Silvestri", "Fabrizio", ""], ["Firooz", "Hamed", ""], ["Nakov", "Preslav", ""], ["Martino", "Giovanni Da San", ""]]}, {"id": "2105.09392", "submitter": "Gengchen Mai", "authors": "Gengchen Mai, Krzysztof Janowicz, Rui Zhu, Ling Cai, and Ni Lao", "title": "Geographic Question Answering: Challenges, Uniqueness, Classification,\n  and Future Directions", "comments": "20 pages, 3 figure, Full paper accepted to AGILE 2021", "journal-ref": "AGILE 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an important part of Artificial Intelligence (AI), Question Answering (QA)\naims at generating answers to questions phrased in natural language. While\nthere has been substantial progress in open-domain question answering, QA\nsystems are still struggling to answer questions which involve geographic\nentities or concepts and that require spatial operations. In this paper, we\ndiscuss the problem of geographic question answering (GeoQA). We first\ninvestigate the reasons why geographic questions are difficult to answer by\nanalyzing challenges of geographic questions. We discuss the uniqueness of\ngeographic questions compared to general QA. Then we review existing work on\nGeoQA and classify them by the types of questions they can address. Based on\nthis survey, we provide a generic classification framework for geographic\nquestions. Finally, we conclude our work by pointing out unique future research\ndirections for GeoQA.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:47:36 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Zhu", "Rui", ""], ["Cai", "Ling", ""], ["Lao", "Ni", ""]]}, {"id": "2105.09404", "submitter": "Ling Liu", "authors": "Ling Liu", "title": "Computational Morphology with Neural Network Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network approaches have been applied to computational morphology with\ngreat success, improving the performance of most tasks by a large margin and\nproviding new perspectives for modeling. This paper starts with a brief\nintroduction to computational morphology, followed by a review of recent work\non computational morphology with neural network approaches, to provide an\noverview of the area. In the end, we will analyze the advantages and problems\nof neural network approaches to computational morphology, and point out some\ndirections to be explored by future research and study.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 21:17:53 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Liu", "Ling", ""]]}, {"id": "2105.09428", "submitter": "Chuhong Lahlou", "authors": "Chuhong Lahlou, Ancil Crayton, Caroline Trier, Evan Willett", "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim\n  Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:39:15 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lahlou", "Chuhong", ""], ["Crayton", "Ancil", ""], ["Trier", "Caroline", ""], ["Willett", "Evan", ""]]}, {"id": "2105.09458", "submitter": "Ningyu Zhang", "authors": "Dongfang Lou, Zhilin Liao, Shumin Deng, Ningyu Zhang, Huajun Chen", "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of collectively detecting multiple events,\nparticularly in cross-sentence settings. The key to dealing with the problem is\nto encode semantic information and model event inter-dependency at a\ndocument-level. In this paper, we reformulate it as a Seq2Seq task and propose\na Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level\nassociation of events and semantic information simultaneously. Specifically, a\nbidirectional decoder is firstly devised to model event inter-dependency within\na sentence when decoding the event tag vector sequence. Secondly, an\ninformation aggregation module is employed to aggregate sentence-level semantic\nand event tag information. Finally, we stack multiple bidirectional decoders\nand feed cross-sentence information, forming a multi-layer bidirectional\ntagging architecture to iteratively propagate information across sentences. We\nshow that our approach provides significant improvement in performance compared\nto the current state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 02:29:03 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 10:16:22 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 04:40:06 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lou", "Dongfang", ""], ["Liao", "Zhilin", ""], ["Deng", "Shumin", ""], ["Zhang", "Ningyu", ""], ["Chen", "Huajun", ""]]}, {"id": "2105.09501", "submitter": "Xiao Pan", "authors": "Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li", "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine\n  Translation", "comments": "accepted as long paper in ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:59:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 03:47:28 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 12:01:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Pan", "Xiao", ""], ["Wang", "Mingxuan", ""], ["Wu", "Liwei", ""], ["Li", "Lei", ""]]}, {"id": "2105.09509", "submitter": "Shirong Shen", "authors": "Shirong Shen and Tongtong Wu and Guilin Qi and Yuan-Fang Li and\n  Gholamreza Haffari and Sheng Bi", "title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event\n  Detection", "comments": "Accepted by ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection (ED) aims at detecting event trigger words in sentences and\nclassifying them into specific event types. In real-world applications, ED\ntypically does not have sufficient labelled data, thus can be formulated as a\nfew-shot learning problem. To tackle the issue of low sample diversity in\nfew-shot ED, we propose a novel knowledge-based few-shot event detection method\nwhich uses a definition-based encoder to introduce external event knowledge as\nthe knowledge prior of event types. Furthermore, as external knowledge\ntypically provides limited and imperfect coverage of event types, we introduce\nan adaptive knowledge-enhanced Bayesian meta-learning method to dynamically\nadjust the knowledge prior of event types. Experiments show our method\nconsistently and substantially outperforms a number of baselines by at least 15\nabsolute F1 points under the same few-shot settings.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:26:26 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 14:17:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shen", "Shirong", ""], ["Wu", "Tongtong", ""], ["Qi", "Guilin", ""], ["Li", "Yuan-Fang", ""], ["Haffari", "Gholamreza", ""], ["Bi", "Sheng", ""]]}, {"id": "2105.09543", "submitter": "Tianyu Gao", "authors": "Tianyu Gao, Xu Han, Keyue Qiu, Yuzhuo Bai, Zhiyu Xie, Yankai Lin,\n  Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou", "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly\n  Supervised Relation Extraction", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:55:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gao", "Tianyu", ""], ["Han", "Xu", ""], ["Qiu", "Keyue", ""], ["Bai", "Yuzhuo", ""], ["Xie", "Zhiyu", ""], ["Lin", "Yankai", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.09567", "submitter": "Lianwei Wu", "authors": "Lianwei Wu, Yuan Rao, Yuqian Lan, Ling Sun and Zhaoyin Qi", "title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies constructing direct interactions between the claim and each\nsingle user response (a comment or a relevant article) to capture evidence have\nshown remarkable success in interpretable claim verification. Owing to\ndifferent single responses convey different cognition of individual users\n(i.e., audiences), the captured evidence belongs to the perspective of\nindividual cognition. However, individuals' cognition of social things is not\nalways able to truly reflect the objective. There may be one-sided or biased\nsemantics in their opinions on a claim. The captured evidence correspondingly\ncontains some unobjective and biased evidence fragments, deteriorating task\nperformance. In this paper, we propose a Dual-view model based on the views of\nCollective and Individual Cognition (CICD) for interpretable claim\nverification. From the view of the collective cognition, we not only capture\nthe word-level semantics based on individual users, but also focus on\nsentence-level semantics (i.e., the overall responses) among all users and\nadjust the proportion between them to generate global evidence. From the view\nof individual cognition, we select the top-$k$ articles with high degree of\ndifference and interact with the claim to explore the local key evidence\nfragments. To weaken the bias of individual cognition-view evidence, we devise\ninconsistent loss to suppress the divergence between global and local evidence\nfor strengthening the consistent shared evidence between the both. Experiments\non three benchmark datasets confirm that CICD achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:44:03 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wu", "Lianwei", ""], ["Rao", "Yuan", ""], ["Lan", "Yuqian", ""], ["Sun", "Ling", ""], ["Qi", "Zhaoyin", ""]]}, {"id": "2105.09571", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "P. Gloor, A. Fronzetti Colladon, G. Giacomelli, T. Saran, F. Grippa", "title": "The impact of virtual mirroring on customer satisfaction", "comments": null, "journal-ref": "Journal of Business Research 75, 67-76 (2017)", "doi": "10.1016/j.jbusres.2017.02.010", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We investigate the impact of a novel method called \"virtual mirroring\" to\npromote employee self-reflection and impact customer satisfaction. The method\nis based on measuring communication patterns, through social network and\nsemantic analysis, and mirroring them back to the individual. Our goal is to\ndemonstrate that self-reflection can trigger a change in communication\nbehaviors, which lead to increased customer satisfaction. We illustrate and\ntest our approach analyzing e-mails of a large global services company by\ncomparing changes in customer satisfaction associated with team leaders exposed\nto virtual mirroring (the experimental group). We find an increase in customer\nsatisfaction in the experimental group and a decrease in the control group\n(team leaders not involved in the virtual mirroring process). With regard to\nthe individual communication indicators, we find that customer satisfaction is\nhigher when employees are more responsive, use a simpler language, are embedded\nin less centralized communication networks, and show more stable leadership\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:51:13 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gloor", "P.", ""], ["Colladon", "A. Fronzetti", ""], ["Giacomelli", "G.", ""], ["Saran", "T.", ""], ["Grippa", "F.", ""]]}, {"id": "2105.09601", "submitter": "Yash Kumar Atri", "authors": "Yash Kumar Atri, Shraman Pramanick, Vikram Goyal, Tanmoy Chakraborty", "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for\n  Abstractive Text Summarization", "comments": "Journal paper accepted in Knowledge Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:56:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Atri", "Yash Kumar", ""], ["Pramanick", "Shraman", ""], ["Goyal", "Vikram", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2105.09611", "submitter": "Daniel Fern\\'andez-Gonz\\'alez", "authors": "Daniel Fern\\'andez-Gonz\\'alez and Carlos G\\'omez-Rodr\\'iguez", "title": "Dependency Parsing with Bottom-up Hierarchical Pointer Networks", "comments": "15 pages (incl. appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency parsing is a crucial step towards deep language understanding and,\ntherefore, widely demanded by numerous Natural Language Processing\napplications. In particular, left-to-right and top-down transition-based\nalgorithms that rely on Pointer Networks are among the most accurate approaches\nfor performing dependency parsing. Additionally, it has been observed for the\ntop-down algorithm that Pointer Networks' sequential decoding can be improved\nby implementing a hierarchical variant, more adequate to model dependency\nstructures. Considering all this, we develop a bottom-up-oriented Hierarchical\nPointer Network for the left-to-right parser and propose two novel\ntransition-based alternatives: an approach that parses a sentence in\nright-to-left order and a variant that does it from the outside in. We\nempirically test the proposed neural architecture with the different algorithms\non a wide variety of languages, outperforming the original approach in\npractically all of them and setting new state-of-the-art results on the English\nand Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 09:10:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Fern\u00e1ndez-Gonz\u00e1lez", "Daniel", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2105.09632", "submitter": "Vivek Kumar Mr.", "authors": "Danilo Dessi, Rim Helaoui, Vivek Kumar, Diego Reforgiato Recupero, and\n  Daniele Riboni", "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical\n  Notes: An Initial Study", "comments": "12 pages, 2 figures, 2 tables, SmartPhil 2020-First Workshop on Smart\n  Personal Health Interfaces, Associated to ACM IUI 2020", "journal-ref": null, "doi": "10.5281/zenodo.4777594", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 09:57:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:51:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dessi", "Danilo", ""], ["Helaoui", "Rim", ""], ["Kumar", "Vivek", ""], ["Recupero", "Diego Reforgiato", ""], ["Riboni", "Daniele", ""]]}, {"id": "2105.09649", "submitter": "Zixiu Wu", "authors": "Zixiu Wu, Rim Helaoui, Vivek Kumar, Diego Reforgiato Recupero and\n  Daniele Riboni", "title": "Towards Detecting Need for Empathetic Response in Motivational\n  Interviewing", "comments": "Accepted to ICMI '20 Companion: Companion Publication of the 2020\n  International Conference on Multimodal Interaction (SAMIH'20 Workshop)", "journal-ref": null, "doi": "10.1145/3395035.3425228", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Empathetic response from the therapist is key to the success of clinical\npsychotherapy, especially motivational interviewing. Previous work on\ncomputational modelling of empathy in motivational interviewing has focused on\noffline, session-level assessment of therapist empathy, where empathy captures\nall efforts that the therapist makes to understand the client's perspective and\nconvey that understanding to the client. In this position paper, we propose a\nnovel task of turn-level detection of client need for empathy. Concretely, we\npropose to leverage pre-trained language models and empathy-related general\nconversation corpora in a unique labeller-detector framework, where the\nlabeller automatically annotates a motivational interviewing conversation\ncorpus with empathy labels to train the detector that determines the need for\ntherapist empathy. We also lay out our strategies of extending the detector\nwith additional-input and multi-task setups to improve its detection and\nexplainability.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:28:46 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wu", "Zixiu", ""], ["Helaoui", "Rim", ""], ["Kumar", "Vivek", ""], ["Recupero", "Diego Reforgiato", ""], ["Riboni", "Daniele", ""]]}, {"id": "2105.09653", "submitter": "Yves Bestgen", "authors": "Yves Bestgen", "title": "LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction\n  Using Bigram Association Measures", "comments": "Accepted at SemEval-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the system developed by the Laboratoire d'analyse\nstatistique des textes (LAST) for the Lexical Complexity Prediction shared task\nat SemEval-2021. The proposed system is made up of a LightGBM model fed with\nfeatures obtained from many word frequency lists, published lexical norms and\npsychometric data. For tackling the specificity of the multi-word task, it uses\nbigram association measures. Despite that the only contextual feature used was\nsentence length, the system achieved an honorable performance in the multi-word\ntask, but poorer in the single word task. The bigram association measures were\nfound useful, but to a limited extent.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:32:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bestgen", "Yves", ""]]}, {"id": "2105.09660", "submitter": "Felix Hamborg", "authors": "Felix Hamborg and Karsten Donnay and Bela Gipp", "title": "Towards Target-dependent Sentiment Classification in News Articles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-71305-8_12", "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Extensive research on target-dependent sentiment classification (TSC) has led\nto strong classification performances in domains where authors tend to\nexplicitly express sentiment about specific entities or topics, such as in\nreviews or on social media. We investigate TSC in news articles, a much less\nresearched domain, despite the importance of news as an essential information\nsource in individual and societal decision making. This article introduces\nNewsTSC, a manually annotated dataset to explore TSC on news articles.\nInvestigating characteristics of sentiment in news and contrasting them to\npopular TSC domains, we find that sentiment in the news is expressed less\nexplicitly, is more dependent on context and readership, and requires a greater\ndegree of interpretation. In an extensive evaluation, we find that the state of\nthe art in TSC performs worse on news articles than on other domains (average\nrecall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on\nestablished TSC datasets). Reasons include incorrectly resolved relation of\ntarget and sentiment-bearing phrases and off-context dependence. As a major\nimprovement over previous news TSC, we find that BERT's natural language\nunderstanding capabilities capture the less explicit sentiment used in news\narticles.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:48:03 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hamborg", "Felix", ""], ["Donnay", "Karsten", ""], ["Gipp", "Bela", ""]]}, {"id": "2105.09680", "submitter": "Sungjoon Park", "authors": "Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Jiyoon Han,\n  Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, Joohong\n  Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo,\n  Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung\n  Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy\n  Park, Alice Oh, Jung-Woo Ha, Kyunghyun Cho", "title": "KLUE: Korean Language Understanding Evaluation", "comments": "76 pages, 10 figures, 36 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE\nis a collection of 8 Korean natural language understanding (NLU) tasks,\nincluding Topic Classification, SemanticTextual Similarity, Natural Language\nInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,\nMachine Reading Comprehension, and Dialogue State Tracking. We build all of the\ntasks from scratch from diverse source corpora while respecting copyrights, to\nensure accessibility for anyone without any restrictions. With ethical\nconsiderations in mind, we carefully design annotation protocols. Along with\nthe benchmark tasks and data, we provide suitable evaluation metrics and\nfine-tuning recipes for pretrained language models for each task. We\nfurthermore release the pretrained language models (PLM), KLUE-BERT and\nKLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby\nfacilitate future research. We make a few interesting observations from the\npreliminary experiments using the proposed KLUE benchmark suite, already\ndemonstrating the usefulness of this new benchmark suite. First, we find\nKLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs and\nexisting open-source Korean PLMs. Second, we see minimal degradation in\nperformance even when we replace personally identifiable information from the\npretraining corpus, suggesting that privacy and NLU capability are not at odds\nwith each other. Lastly, we find that using BPE tokenization in combination\nwith morpheme-level pre-tokenization is effective in tasks involving\nmorpheme-level tagging, detection and generation. In addition to accelerating\nKorean NLP research, our comprehensive documentation on creating KLUE will\nfacilitate creating similar resources for other languages in the future. KLUE\nis available at https://klue-benchmark.com.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:40:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 05:54:22 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 17:17:26 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Park", "Sungjoon", ""], ["Moon", "Jihyung", ""], ["Kim", "Sungdong", ""], ["Cho", "Won Ik", ""], ["Han", "Jiyoon", ""], ["Park", "Jangwon", ""], ["Song", "Chisung", ""], ["Kim", "Junseong", ""], ["Song", "Yongsook", ""], ["Oh", "Taehwan", ""], ["Lee", "Joohong", ""], ["Oh", "Juhyun", ""], ["Lyu", "Sungwon", ""], ["Jeong", "Younghoon", ""], ["Lee", "Inkwon", ""], ["Seo", "Sangwoo", ""], ["Lee", "Dongjun", ""], ["Kim", "Hyunwoo", ""], ["Lee", "Myeonghwa", ""], ["Jang", "Seongbo", ""], ["Do", "Seungwon", ""], ["Kim", "Sunkyoung", ""], ["Lim", "Kyungtae", ""], ["Lee", "Jongwon", ""], ["Park", "Kyumin", ""], ["Shin", "Jamin", ""], ["Kim", "Seonghyun", ""], ["Park", "Lucy", ""], ["Oh", "Alice", ""], ["Ha", "Jung-Woo", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2105.09702", "submitter": "Hans-J\\\"urgen Profitlich", "authors": "Hans-J\\\"urgen Profitlich and Daniel Sonntag", "title": "A Case Study on Pros and Cons of Regular Expression Detection and\n  Dependency Parsing for Negation Extraction from German Medical Documents.\n  Technical Report", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our work on information extraction in medical documents written\nin German, especially detecting negations using an architecture based on the\nUIMA pipeline. Based on our previous work on software modules to cover medical\nconcepts like diagnoses, examinations, etc. we employ a version of the NegEx\nregular expression algorithm with a large set of triggers as a baseline. We\nshow how a significantly smaller trigger set is sufficient to achieve similar\nresults, in order to reduce adaptation times to new text types. We elaborate on\nthe question whether dependency parsing (based on the Stanford CoreNLP model)\nis a good alternative and describe the potentials and shortcomings of both\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:21:09 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Profitlich", "Hans-J\u00fcrgen", ""], ["Sonntag", "Daniel", ""]]}, {"id": "2105.09742", "submitter": "Aashish Agarwal", "authors": "Aashish Agarwal and Torsten Zesch", "title": "Robustness of end-to-end Automatic Speech Recognition Models -- A Case\n  Study using Mozilla DeepSpeech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When evaluating the performance of automatic speech recognition models,\nusually word error rate within a certain dataset is used. Special care must be\ntaken in understanding the dataset in order to report realistic performance\nnumbers. We argue that many performance numbers reported probably underestimate\nthe expected error rate. We conduct experiments controlling for selection bias,\ngender as well as overlap (between training and test data) in content, voices,\nand recording conditions. We find that content overlap has the biggest impact,\nbut other factors like gender also play a role.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:46:44 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Agarwal", "Aashish", ""], ["Zesch", "Torsten", ""]]}, {"id": "2105.09816", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Bhaskar Mitra, Hamed Zamani, Nick Craswell,\n  Allan Hanbury", "title": "Intra-Document Cascading: Learning to Select Passages for Neural\n  Document Ranking", "comments": "Accepted at SIGIR 2021 (Full Paper Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:10:13 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Mitra", "Bhaskar", ""], ["Zamani", "Hamed", ""], ["Craswell", "Nick", ""], ["Hanbury", "Allan", ""]]}, {"id": "2105.09825", "submitter": "Alessandro Lenci", "authors": "Alessandro Lenci and Magnus Sahlgren and Patrick Jeuniaux and Amaru\n  Cuba Gyllensten and Martina Miliani", "title": "A comprehensive comparative evaluation and analysis of Distributional\n  Semantic Models", "comments": "Submitted to Language Resources and Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributional semantics has deeply changed in the last decades. First,\npredict models stole the thunder from traditional count ones, and more recently\nboth of them were replaced in many NLP applications by contextualized vectors\nproduced by Transformer neural language models. Although an extensive body of\nresearch has been devoted to Distributional Semantic Model (DSM) evaluation, we\nstill lack a thorough comparison with respect to tested models, semantic tasks,\nand benchmark datasets. Moreover, previous work has mostly focused on\ntask-driven evaluation, instead of exploring the differences between the way\nmodels represent the lexical semantic space. In this paper, we perform a\ncomprehensive evaluation of type distributional vectors, either produced by\nstatic DSMs or obtained by averaging the contextualized vectors generated by\nBERT. First of all, we investigate the performance of embeddings in several\nsemantic tasks, carrying out an in-depth statistical analysis to identify the\nmajor factors influencing the behavior of DSMs. The results show that i.) the\nalleged superiority of predict based models is more apparent than real, and\nsurely not ubiquitous and ii.) static DSMs surpass contextualized\nrepresentations in most out-of-context semantic tasks and datasets.\nFurthermore, we borrow from cognitive neuroscience the methodology of\nRepresentational Similarity Analysis (RSA) to inspect the semantic spaces\ngenerated by distributional models. RSA reveals important differences related\nto the frequency and part-of-speech of lexical items.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:18:06 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lenci", "Alessandro", ""], ["Sahlgren", "Magnus", ""], ["Jeuniaux", "Patrick", ""], ["Gyllensten", "Amaru Cuba", ""], ["Miliani", "Martina", ""]]}, {"id": "2105.09835", "submitter": "Zuchao Li", "authors": "Zuchao Li, Junru Zhou, Hai Zhao, Kevin Parnow", "title": "Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constituent and dependency parsing, the two classic forms of syntactic\nparsing, have been found to benefit from joint training and decoding under a\nuniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,\ndecoding this unified grammar has a higher time complexity ($O(n^5)$) than\ndecoding either form individually ($O(n^3)$) since more factors have to be\nconsidered during decoding. We thus propose an improved head scorer that helps\nachieve a novel performance-preserved parser in $O$($n^3$) time complexity.\nFurthermore, on the basis of this proposed practical HPSG parser, we\ninvestigated the strengths of HPSG-based parsing and explored the general\nmethod of training an HPSG-based parser from only a constituent or dependency\nannotations in a multilingual scenario. We thus present a more effective, more\nin-depth, and general work on HPSG parsing.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:33:51 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Li", "Zuchao", ""], ["Zhou", "Junru", ""], ["Zhao", "Hai", ""], ["Parnow", "Kevin", ""]]}, {"id": "2105.09856", "submitter": "Patrick Lumban Tobing", "authors": "Patrick Lumban Tobing, Tomoki Toda", "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on\n  Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform\n  Modeling", "comments": "Accepted for INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:02:45 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 00:43:39 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tobing", "Patrick Lumban", ""], ["Toda", "Tomoki", ""]]}, {"id": "2105.09858", "submitter": "Patrick Lumban Tobing", "authors": "Patrick Lumban Tobing, Tomoki Toda", "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic\n  Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear\n  Prediction", "comments": "Accepted for SSW11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:06:11 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 00:59:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tobing", "Patrick Lumban", ""], ["Toda", "Tomoki", ""]]}, {"id": "2105.09867", "submitter": "Gregory Scontras", "authors": "Gregory Scontras, Michael Henry Tessler, Michael Franke", "title": "A practical introduction to the Rational Speech Act modeling framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in computational cognitive science (i.e., simulation-based\nprobabilistic programs) have paved the way for significant progress in formal,\nimplementable models of pragmatics. Rather than describing a pragmatic\nreasoning process in prose, these models formalize and implement one, deriving\nboth qualitative and quantitative predictions of human behavior -- predictions\nthat consistently prove correct, demonstrating the viability and value of the\nframework. The current paper provides a practical introduction to and critical\nassessment of the Bayesian Rational Speech Act modeling framework, unpacking\ntheoretical foundations, exploring technological innovations, and drawing\nconnections to issues beyond current applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:08:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Scontras", "Gregory", ""], ["Tessler", "Michael Henry", ""], ["Franke", "Michael", ""]]}, {"id": "2105.09930", "submitter": "Ellie Chio", "authors": "Sukhdeep S. Sodhi, Ellie Ka-In Chio, Ambarish Jash, Santiago\n  Onta\\~n\\'on, Ajit Apte, Ankit Kumar, Ayooluwakunmi Jeje, Dima Kuzmin, Harry\n  Fung, Heng-Tze Cheng, Jon Effrat, Tarush Bali, Nitin Jindal, Pei Cao,\n  Sarvjeet Singh, Senqiang Zhou, Tameen Khan, Amol Wankhede, Moustafa Alzantot,\n  Allen Wu, Tushar Chandra", "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error\n  Correction for Voice Search Queries", "comments": "Accepted in KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:45:46 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Sodhi", "Sukhdeep S.", ""], ["Chio", "Ellie Ka-In", ""], ["Jash", "Ambarish", ""], ["Onta\u00f1\u00f3n", "Santiago", ""], ["Apte", "Ajit", ""], ["Kumar", "Ankit", ""], ["Jeje", "Ayooluwakunmi", ""], ["Kuzmin", "Dima", ""], ["Fung", "Harry", ""], ["Cheng", "Heng-Tze", ""], ["Effrat", "Jon", ""], ["Bali", "Tarush", ""], ["Jindal", "Nitin", ""], ["Cao", "Pei", ""], ["Singh", "Sarvjeet", ""], ["Zhou", "Senqiang", ""], ["Khan", "Tameen", ""], ["Wankhede", "Amol", ""], ["Alzantot", "Moustafa", ""], ["Wu", "Allen", ""], ["Chandra", "Tushar", ""]]}, {"id": "2105.09938", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika\n  and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He\n  and Dawn Song and Jacob Steinhardt", "title": "Measuring Coding Challenge Competence With APPS", "comments": "Code and the APPS dataset is available at\n  https://github.com/hendrycks/apps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. Despite its importance, there has been surprisingly little work on\nevaluating code generation, and it can be difficult to accurately assess code\ngeneration performance rigorously. To meet this challenge, we introduce APPS, a\nbenchmark for code generation. Unlike prior work in more restricted settings,\nour benchmark measures the ability of models to take an arbitrary natural\nlanguage specification and generate satisfactory Python code. Similar to how\ncompanies assess candidate software developers, we then evaluate models by\nchecking their generated code on test cases. Our benchmark includes 10,000\nproblems, which range from having simple one-line solutions to being\nsubstantial algorithmic challenges. We fine-tune large language models on both\nGitHub and our training set, and we find that the prevalence of syntax errors\nis decreasing exponentially as models improve. Recent models such as GPT-Neo\ncan pass approximately 20% of the test cases of introductory problems, so we\nfind that machine learning models are now beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:58:42 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 19:41:58 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hendrycks", "Dan", ""], ["Basart", "Steven", ""], ["Kadavath", "Saurav", ""], ["Mazeika", "Mantas", ""], ["Arora", "Akul", ""], ["Guo", "Ethan", ""], ["Burns", "Collin", ""], ["Puranik", "Samir", ""], ["He", "Horace", ""], ["Song", "Dawn", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2105.09967", "submitter": "Boaz Shmueli", "authors": "Boaz Shmueli, Soumya Ray, Lun-Wei Ku", "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on\n  Twitter", "comments": "To be published in ACL 2021. 7 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:01:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Shmueli", "Boaz", ""], ["Ray", "Soumya", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2105.09984", "submitter": "Shivani Kumar", "authors": "Manjot Bedi, Shivani Kumar, Md Shad Akhtar, and Tanmoy Chakraborty", "title": "Multi-modal Sarcasm Detection and Humor Classification in Code-mixed\n  Conversations", "comments": "13 pages, 4 figures, 9 tables", "journal-ref": null, "doi": "10.1109/TAFFC.2021.3083522", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm detection and humor classification are inherently subtle problems,\nprimarily due to their dependence on the contextual and non-verbal information.\nFurthermore, existing studies in these two topics are usually constrained in\nnon-English languages such as Hindi, due to the unavailability of qualitative\nannotated datasets. In this work, we make two major contributions considering\nthe above limitations: (1) we develop a Hindi-English code-mixed dataset,\nMaSaC, for the multi-modal sarcasm detection and humor classification in\nconversational dialog, which to our knowledge is the first dataset of its kind;\n(2) we propose MSH-COMICS, a novel attention-rich neural architecture for the\nutterance classification. We learn efficient utterance representation utilizing\na hierarchical attention mechanism that attends to a small portion of the input\nsentence at a time. Further, we incorporate dialog-level contextual attention\nmechanism to leverage the dialog history for the multi-modal classification. We\nperform extensive experiments for both the tasks by varying multi-modal inputs\nand various submodules of MSH-COMICS. We also conduct comparative analysis\nagainst existing approaches. We observe that MSH-COMICS attains superior\nperformance over the existing models by > 1 F1-score point for the sarcasm\ndetection and 10 F1-score points in humor classification. We diagnose our model\nand perform thorough analysis of the results to understand the superiority and\npitfalls.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:33:55 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:06:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bedi", "Manjot", ""], ["Kumar", "Shivani", ""], ["Akhtar", "Md Shad", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2105.09996", "submitter": "Hu Xu", "authors": "Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh,\n  Christoph Feichtenhofer, Florian Metze, Luke Zettlemoyer", "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video\n  Understanding", "comments": "9 pages, ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:13:27 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Hu", ""], ["Ghosh", "Gargi", ""], ["Huang", "Po-Yao", ""], ["Arora", "Prahal", ""], ["Aminzadeh", "Masoumeh", ""], ["Feichtenhofer", "Christoph", ""], ["Metze", "Florian", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2105.10023", "submitter": "Geetanjali Rakshit", "authors": "Geetanjali Rakshit and Jeffrey Flanigan", "title": "ASQ: Automatically Generating Question-Answer Pairs using AMRs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we introduce ASQ, a tool to automatically mine questions and\nanswers from a sentence, using its Abstract Meaning Representation (AMR).\nPrevious work has made a case for using question-answer pairs to specify\npredicate-argument structure of a sentence using natural language, which does\nnot require linguistic expertise or training. This has resulted in the creation\nof datasets such as QA-SRL and QAMR, for both of which, the question-answer\npair annotations were crowdsourced. Our approach has the same end-goal, but is\nautomatic, making it faster and cost-effective, without compromising on the\nquality and validity of the question-answer pairs thus obtained. A qualitative\nevaluation of the output generated by ASQ from the AMR 2.0 data shows that the\nquestion-answer pairs are natural and valid, and demonstrate good coverage of\nthe content. We run ASQ on the sentences from the QAMR dataset, to observe that\nthe semantic roles in QAMR are also captured by ASQ.We intend to make this tool\nand the results publicly available for others to use and build upon.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:38:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rakshit", "Geetanjali", ""], ["Flanigan", "Jeffrey", ""]]}, {"id": "2105.10026", "submitter": "Adyasha Maharana", "authors": "Adyasha Maharana, Darryl Hannan, Mohit Bansal", "title": "Improving Generation and Evaluation of Visual Stories via Semantic\n  Consistency", "comments": "NAACL 2021 (16 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story visualization is an under-explored task that falls at the intersection\nof many important research directions in both computer vision and natural\nlanguage processing. In this task, given a series of natural language captions\nwhich compose a story, an agent must generate a sequence of images that\ncorrespond to the captions. Prior work has introduced recurrent generative\nmodels which outperform text-to-image synthesis models on this task. However,\nthere is room for improvement of generated images in terms of visual quality,\ncoherence and relevance. We present a number of improvements to prior modeling\napproaches, including (1) the addition of a dual learning framework that\nutilizes video captioning to reinforce the semantic alignment between the story\nand generated images, (2) a copy-transform mechanism for\nsequentially-consistent story visualization, and (3) MART-based transformers to\nmodel complex interactions between frames. We present ablation studies to\ndemonstrate the effect of each of these techniques on the generative power of\nthe model for both individual images as well as the entire narrative.\nFurthermore, due to the complexity and generative nature of the task, standard\nevaluation metrics do not accurately reflect performance. Therefore, we also\nprovide an exploration of evaluation metrics for the model, focused on aspects\nof the generated frames such as the presence/quality of generated characters,\nthe relevance to captions, and the diversity of the generated images. We also\npresent correlation experiments of our proposed automated metrics with human\nevaluations. Code and data available at:\nhttps://github.com/adymaharana/StoryViz\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:42:42 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Maharana", "Adyasha", ""], ["Hannan", "Darryl", ""], ["Bansal", "Mohit", ""]]}, {"id": "2105.10042", "submitter": "Anderson Avila", "authors": "Nihal Potdar, Anderson R. Avila, Chao Xing, Dong Wang, Yiran Cao, Xiao\n  Chen", "title": "A Streaming End-to-End Framework For Spoken Language Understanding", "comments": "Accepted at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 21:37:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 19:57:45 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 18:26:24 GMT"}, {"version": "v4", "created": "Sat, 17 Jul 2021 14:19:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Potdar", "Nihal", ""], ["Avila", "Anderson R.", ""], ["Xing", "Chao", ""], ["Wang", "Dong", ""], ["Cao", "Yiran", ""], ["Chen", "Xiao", ""]]}, {"id": "2105.10080", "submitter": "Bin Ji", "authors": "Bin Ji, Shasha Li, Jie Yu, Jun Ma, Huijun Liu", "title": "Boosting Span-based Joint Entity and Relation Extraction via Squence\n  Tagging Mechanism", "comments": "10pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Span-based joint extraction simultaneously conducts named entity recognition\n(NER) and relation extraction (RE) in text span form. Recent studies have shown\nthat token labels can convey crucial task-specific information and enrich token\nsemantics. However, as far as we know, due to completely abstain from sequence\ntagging mechanism, all prior span-based work fails to use token label\nin-formation. To solve this problem, we pro-pose Sequence Tagging enhanced\nSpan-based Network (STSN), a span-based joint extrac-tion network that is\nenhanced by token BIO label information derived from sequence tag-ging based\nNER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral\narchitecture to build STSN, and each atten-tion layer consists of three basic\nattention units. The deep neural architecture first learns seman-tic\nrepresentations for token labels and span-based joint extraction, and then\nconstructs in-formation interactions between them, which also realizes\nbidirectional information interac-tions between span-based NER and RE.\nFur-thermore, we extend the BIO tagging scheme to make STSN can extract\noverlapping en-tity. Experiments on three benchmark datasets show that our\nmodel consistently outperforms previous optimal models by a large margin,\ncreating new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:10:03 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ji", "Bin", ""], ["Li", "Shasha", ""], ["Yu", "Jie", ""], ["Ma", "Jun", ""], ["Liu", "Huijun", ""]]}, {"id": "2105.10117", "submitter": "Kornraphop Kawintiranon", "authors": "Kornraphop Kawintiranon and Yaguang Liu", "title": "Towards Automatic Comparison of Data Privacy Documents: A Preliminary\n  Experiment on GDPR-like Laws", "comments": "This is a preliminary work, see repo at\n  https://github.com/kornosk/GDPR-similarity-comparison", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  General Data Protection Regulation (GDPR) becomes a standard law for data\nprotection in many countries. Currently, twelve countries adopt the regulation\nand establish their GDPR-like regulation. However, to evaluate the differences\nand similarities of these GDPR-like regulations is time-consuming and needs a\nlot of manual effort from legal experts. Moreover, GDPR-like regulations from\ndifferent countries are written in their languages leading to a more difficult\ntask since legal experts who know both languages are essential. In this paper,\nwe investigate a simple natural language processing (NLP) approach to tackle\nthe problem. We first extract chunks of information from GDPR-like documents\nand form structured data from natural language. Next, we use NLP methods to\ncompare documents to measure their similarity. Finally, we manually label a\nsmall set of data to evaluate our approach. The empirical result shows that the\nBERT model with cosine similarity outperforms other baselines. Our data and\ncode are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 03:59:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kawintiranon", "Kornraphop", ""], ["Liu", "Yaguang", ""]]}, {"id": "2105.10146", "submitter": "Harsh Kohli Mr.", "authors": "Harsh Kohli", "title": "Training Bi-Encoders for Word Sense Disambiguation", "comments": "15 pages, 5 figures. Accepted at the 16th International Conference on\n  Document Analysis and Recognition ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern transformer-based neural architectures yield impressive results in\nnearly every NLP task and Word Sense Disambiguation, the problem of discerning\nthe correct sense of a word in a given context, is no exception.\nState-of-the-art approaches in WSD today leverage lexical information along\nwith pre-trained embeddings from these models to achieve results comparable to\nhuman inter-annotator agreement on standard evaluation benchmarks. In the same\nvein, we experiment with several strategies to optimize bi-encoders for this\nspecific task and propose alternative methods of presenting lexical information\nto our model. Through our multi-stage pre-training and fine-tuning pipeline we\nfurther the state of the art in Word Sense Disambiguation.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:06:03 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kohli", "Harsh", ""]]}, {"id": "2105.10155", "submitter": "Alexios Gidiotis", "authors": "Alexios Gidiotis and Grigorios Tsoumakas", "title": "Uncertainty-Aware Abstractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach to summarization based on Bayesian deep learning.\nWe approximate Bayesian summary generation by first extending state-of-the-art\nsummarization models with Monte Carlo dropout and then using them to perform\nmultiple stochastic forward passes. This method allows us to improve\nsummarization performance by simply using the median of multiple stochastic\nsummaries. We show that our variational equivalents of BART and PEGASUS can\noutperform their deterministic counterparts on multiple benchmark datasets. In\naddition, we rely on Bayesian inference to measure the uncertainty of the model\nwhen generating summaries. Having a reliable uncertainty measure, we can\nimprove the experience of the end user by filtering out generated summaries of\nhigh uncertainty. Furthermore, our proposed metric could be used as a criterion\nfor selecting samples for annotation, and can be paired nicely with active\nlearning and human-in-the-loop approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:36:40 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Gidiotis", "Alexios", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2105.10158", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Jiaqing Liang, Jingping Liu, Chengsong Huang, Wenhao\n  Huang, Yanghua Xiao", "title": "Revisiting the Negative Data of Distantly Supervised Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Distantly supervision automatically generates plenty of training samples for\nrelation extraction. However, it also incurs two major problems: noisy labels\nand imbalanced training data. Previous works focus more on reducing wrongly\nlabeled relations (false positives) while few explore the missing relations\nthat are caused by incompleteness of knowledge base (false negatives).\nFurthermore, the quantity of negative labels overwhelmingly surpasses the\npositive ones in previous problem formulations. In this paper, we first provide\na thorough analysis of the above challenges caused by negative data. Next, we\nformulate the problem of relation extraction into as a positive unlabeled\nlearning task to alleviate false negative problem. Thirdly, we propose a\npipeline approach, dubbed \\textsc{ReRe}, that performs sentence-level relation\ndetection then subject/object extraction to achieve sample-efficient training.\nExperimental results show that the proposed method consistently outperforms\nexisting approaches and remains excellent performance even learned with a large\nquantity of false positive samples.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:44:19 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xie", "Chenhao", ""], ["Liang", "Jiaqing", ""], ["Liu", "Jingping", ""], ["Huang", "Chengsong", ""], ["Huang", "Wenhao", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2105.10165", "submitter": "Dipendra Misra", "authors": "Andrew Bennett, Dipendra Misra, and Nga Than", "title": "Have you tried Neural Topic Models? Comparative Analysis of Neural and\n  Non-Neural Topic Models with Application to COVID-19 Twitter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are widely used in studying social phenomena. We conduct a\ncomparative study examining state-of-the-art neural versus non-neural topic\nmodels, performing a rigorous quantitative and qualitative assessment on a\ndataset of tweets about the COVID-19 pandemic. Our results show that not only\ndo neural topic models outperform their classical counterparts on standard\nevaluation metrics, but they also produce more coherent topics, which are of\ngreat benefit when studying complex social problems. We also propose a novel\nregularization term for neural topic models, which is designed to address the\nwell-documented problem of mode collapse, and demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:24:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bennett", "Andrew", ""], ["Misra", "Dipendra", ""], ["Than", "Nga", ""]]}, {"id": "2105.10185", "submitter": "Tiago Pimentel", "authors": "Jennifer C. White, Tiago Pimentel, Naomi Saphra, Ryan Cotterell", "title": "A Non-Linear Structural Probe", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probes are models devised to investigate the encoding of knowledge -- e.g.\nsyntactic structure -- in contextual representations. Probes are often designed\nfor simplicity, which has led to restrictions on probe design that may not\nallow for the full exploitation of the structure of encoded information; one\nsuch restriction is linearity. We examine the case of a structural probe\n(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic\nstructure in contextual representations through learning only linear\ntransformations. By observing that the structural probe learns a metric, we are\nable to kernelize it and develop a novel non-linear variant with an identical\nnumber of parameters. We test on 6 languages and find that the radial-basis\nfunction (RBF) kernel, in conjunction with regularization, achieves a\nstatistically significant improvement over the baseline in all languages --\nimplying that at least part of the syntactic knowledge is encoded non-linearly.\nWe conclude by discussing how the RBF kernel resembles BERT's self-attention\nlayers and speculate that this resemblance leads to the RBF-based probe's\nstronger performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:53:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["White", "Jennifer C.", ""], ["Pimentel", "Tiago", ""], ["Saphra", "Naomi", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2105.10188", "submitter": "Xuefeng Bai", "authors": "Xuefeng Bai, Yulong Chen, Linfeng Song, Yue Zhang", "title": "Semantic Representation for Dialogue Modeling", "comments": "Final camera ready version, to appear in ACL2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although neural models have achieved competitive results in dialogue systems,\nthey have shown limited ability in representing core semantics, such as\nignoring important entities. To this end, we exploit Abstract Meaning\nRepresentation (AMR) to help dialogue modeling. Compared with the textual\ninput, AMR explicitly provides core semantic knowledge and reduces data\nsparsity. We develop an algorithm to construct dialogue-level AMR graphs from\nsentence-level AMRs and explore two ways to incorporate AMRs into dialogue\nsystems. Experimental results on both dialogue understanding and response\ngeneration tasks show the superiority of our model. To our knowledge, we are\nthe first to leverage a formal semantic representation into neural dialogue\nmodeling.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:55:07 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 16:03:54 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bai", "Xuefeng", ""], ["Chen", "Yulong", ""], ["Song", "Linfeng", ""], ["Zhang", "Yue", ""]]}, {"id": "2105.10193", "submitter": "Ayush Maheshwari", "authors": "Atul Sahay, Anshul Nasery, Ayush Maheshwari, Ganesh Ramakrishnan and\n  Rishabh Iyer", "title": "Rule Augmented Unsupervised Constituency Parsing", "comments": "Accepted at Findings of ACL 2021. 10 Pages, 5 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, unsupervised parsing of syntactic trees has gained considerable\nattention. A prototypical approach to such unsupervised parsing employs\nreinforcement learning and auto-encoders. However, no mechanism ensures that\nthe learnt model leverages the well-understood language grammar. We propose an\napproach that utilizes very generic linguistic knowledge of the language\npresent in the form of syntactic rules, thus inducing better syntactic\nstructures. We introduce a novel formulation that takes advantage of the\nsyntactic grammar rules and is independent of the base system. We achieve new\nstate-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source\ncode of the paper is available at https://github.com/anshuln/Diora_with_rules.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:06:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sahay", "Atul", ""], ["Nasery", "Anshul", ""], ["Maheshwari", "Ayush", ""], ["Ramakrishnan", "Ganesh", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2105.10256", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon and P. A. Gloor", "title": "Measuring the impact of spammers on e-mail and Twitter networks", "comments": null, "journal-ref": "International Journal of Information Management 48, 254-262 (2019)", "doi": "10.1016/j.ijinfomgt.2018.09.009", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper investigates the research question if senders of large amounts of\nirrelevant or unsolicited information - commonly called \"spammers\" - distort\nthe network structure of social networks. Two large social networks are\nanalyzed, the first extracted from the Twitter discourse about a big\ntelecommunication company, and the second obtained from three years of email\ncommunication of 200 managers working for a large multinational company. This\nwork compares network robustness and the stability of centrality and\ninteraction metrics, as well as the use of language, after removing spammers\nand the most and least connected nodes. The results show that spammers do not\nsignificantly alter the structure of the information-carrying network, for most\nof the social indicators. The authors additionally investigate the correlation\nbetween e-mail subject line and content by tracking language sentiment,\nemotionality, and complexity, addressing the cases where collecting email\nbodies is not permitted for privacy reasons. The findings extend the research\nabout robustness and stability of social networks metrics, after the\napplication of graph simplification strategies. The results have practical\nimplication for network analysts and for those company managers who rely on\nnetwork analytics (applied to company emails and social media data) to support\ntheir decision-making processes.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:13:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Gloor", "P. A.", ""]]}, {"id": "2105.10267", "submitter": "Philipp Ennen", "authors": "Philipp Ennen, Yen-Ting Lin, Ali Girayhan Ozbay, Ferdinando Insalata,\n  Maolin Li, Ye Tian, Sepehr Jalali, Da-shan Shiu", "title": "Towards a Universal NLG for Dialogue Systems and Simulators with Future\n  Bridging", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a dialogue system pipeline, a natural language generation (NLG) unit\nconverts the dialogue direction and content to a corresponding natural language\nrealization. A recent trend for dialogue systems is to first pre-train on large\ndatasets and then fine-tune in a supervised manner using datasets annotated\nwith application-specific features. Though novel behaviours can be learned from\ncustom annotation, the required effort severely bounds the quantity of the\ntraining set, and the application-specific nature limits the reuse. In light of\nthe recent success of data-driven approaches, we propose the novel future\nbridging NLG (FBNLG) concept for dialogue systems and simulators. The critical\nstep is for an FBNLG to accept a future user or system utterance to bridge the\npresent context towards. Future bridging enables self supervised training over\nannotation-free datasets, decoupled the training of NLG from the rest of the\nsystem. An FBNLG, pre-trained with massive datasets, is expected to apply in\nclassical or new dialogue scenarios with minimal adaptation effort. We evaluate\na prototype FBNLG to show that future bridging can be a viable approach to a\nuniversal few-shot NLG for task-oriented and chit-chat dialogues.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:37:10 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 10:33:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ennen", "Philipp", ""], ["Lin", "Yen-Ting", ""], ["Ozbay", "Ali Girayhan", ""], ["Insalata", "Ferdinando", ""], ["Li", "Maolin", ""], ["Tian", "Ye", ""], ["Jalali", "Sepehr", ""], ["Shiu", "Da-shan", ""]]}, {"id": "2105.10311", "submitter": "Junyi Li", "authors": "Junyi Li, Tianyi Tang, Wayne Xin Zhao and Ji-Rong Wen", "title": "Pretrained Language Models for Text Generation: A Survey", "comments": "Accepted by IJCAI 2021 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation has become one of the most important yet challenging tasks in\nnatural language processing (NLP). The resurgence of deep learning has greatly\nadvanced this field by neural generation models, especially the paradigm of\npretrained language models (PLMs). In this paper, we present an overview of the\nmajor advances achieved in the topic of PLMs for text generation. As the\npreliminaries, we present the general task definition and briefly describe the\nmainstream architectures of PLMs for text generation. As the core content, we\ndiscuss how to adapt existing PLMs to model different input data and satisfy\nspecial properties in the generated text. We further summarize several\nimportant fine-tuning strategies for text generation. Finally, we present\nseveral future directions and conclude this paper. Our survey aims to provide\ntext generation researchers a synthesis and pointer to related research.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:27:44 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 01:19:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Junyi", ""], ["Tang", "Tianyi", ""], ["Zhao", "Wayne Xin", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2105.10323", "submitter": "Zhiliang Tian", "authors": "Zhiliang Tian, Wei Bi, Zihan Zhang, Dongkyu Lee, Yiping Song, Nevin L.\n  Zhang", "title": "Learning from My Friends: Few-Shot Personalized Conversation Systems via\n  Social Networks", "comments": "Published by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized conversation models (PCMs) generate responses according to\nspeaker preferences. Existing personalized conversation tasks typically require\nmodels to extract speaker preferences from user descriptions or their\nconversation histories, which are scarce for newcomers and inactive users. In\nthis paper, we propose a few-shot personalized conversation task with an\nauxiliary social network. The task requires models to generate personalized\nresponses for a speaker given a few conversations from the speaker and a social\nnetwork. Existing methods are mainly designed to incorporate descriptions or\nconversation histories. Those methods can hardly model speakers with so few\nconversations or connections between speakers. To better cater for newcomers\nwith few resources, we propose a personalized conversation model (PCM) that\nlearns to adapt to new speakers as well as enabling new speakers to learn from\nresource-rich speakers. Particularly, based on a meta-learning based PCM, we\npropose a task aggregator (TA) to collect other speakers' information from the\nsocial network. The TA provides prior knowledge of the new speaker in its\nmeta-learning. Experimental results show our methods outperform all baselines\nin appropriateness, diversity, and consistency with speakers.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:54:50 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Tian", "Zhiliang", ""], ["Bi", "Wei", ""], ["Zhang", "Zihan", ""], ["Lee", "Dongkyu", ""], ["Song", "Yiping", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "2105.10334", "submitter": "Siru Ouyang", "authors": "Siru Ouyang, Zhuosheng Zhang and Hai Zhao", "title": "Fact-driven Logical Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical reasoning, which is closely related to human cognition, is of vital\nimportance in human's understanding of texts. Recent years have witnessed\nincreasing attentions on machine's logical reasoning abilities. However,\nprevious studies commonly apply ad-hoc methods to model pre-defined relation\npatterns, such as linking named entities, which only considers global knowledge\ncomponents that are related to commonsense, without local perception of\ncomplete facts or events. Such methodology is obviously insufficient to deal\nwith complicated logical structures. Therefore, we argue that the natural logic\nunits would be the group of backbone constituents of the sentence such as the\nsubject-verb-object formed \"facts\", covering both global and local knowledge\npieces that are necessary as the basis for logical reasoning. Beyond building\nthe ad-hoc graphs, we propose a more general and convenient fact-driven\napproach to construct a supergraph on top of our newly defined fact units, and\nenhance the supergraph with further explicit guidance of local question and\noption interactions. Experiments on two challenging logical reasoning benchmark\ndatasets, ReClor and LogiQA, show that our proposed model, \\textsc{Focal\nReasoner}, outperforms the baseline models dramatically. It can also be\nsmoothly applied to other downstream tasks such as MuTual, a dialogue reasoning\ndataset, achieving competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:11:13 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ouyang", "Siru", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2105.10344", "submitter": "Muhammad Usman Sanwal", "authors": "Usman Sanwal, Thai Son Hoang, Luigia Petre and Ion Petre", "title": "Towards Scalable Modeling of Biology in Event-B", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biology offers many examples of large-scale, complex, concurrent systems:\nmany processes take place in parallel, compete on resources and influence each\nother's behavior. The scalable modeling of biological systems continues to be a\nvery active field of research. In this paper we introduce a new approach based\non Event-B, a state-based formal method with refinement as its central\ningredient, allowing us to check for model consistency step-by-step in an\nautomated way. Our approach based on functions leads to an elegant and concise\nmodeling method. We demonstrate this approach by constructing what is, to our\nknowledge, the largest ever built Event-B model, describing the ErbB signaling\npathway, a key evolutionary pathway with a significant role in development and\nin many types of cancer. The Event-B model for the ErbB pathway describes 1320\nmolecular reactions through 242 events.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:37:06 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sanwal", "Usman", ""], ["Hoang", "Thai Son", ""], ["Petre", "Luigia", ""], ["Petre", "Ion", ""]]}, {"id": "2105.10362", "submitter": "Stanis{\\l}aw Ambroszkiewicz", "authors": "Stanislaw Ambroszkiewicz, Waldemar Bartyna and Stanislaw Bylka", "title": "Functionals in the Clouds: An abstract architecture of serverless\n  Cloud-Native Apps", "comments": "Work in progress. Three figures (related to functionals) were added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cloud Native Application CNApp (as a distributed system) is a collection of\nindependent components (micro-services) interacting via communication\nprotocols. This gives rise to present an abstract architecture of CNApp as\ndynamically re-configurable acyclic directed multi graph where vertices are\nmicroservices, and edges are the protocols. Generic mechanisms for such\nreconfigurations evidently correspond to higher-level functions (functionals).\nThis implies also internal abstract architecture of microservice as a\ncollection of event-triggered serverless functions (including functions\nimplementing the protocols) that are dynamically composed into event-dependent\ndata-flow graphs. Again, generic mechanisms for such compositions correspond to\ncalculus of functionals and relations.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:28:49 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 19:10:02 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ambroszkiewicz", "Stanislaw", ""], ["Bartyna", "Waldemar", ""], ["Bylka", "Stanislaw", ""]]}, {"id": "2105.10396", "submitter": "Matthew Walter", "authors": "Matthew R. Walter, Siddharth Patki, Andrea F. Daniele, Ethan\n  Fahnestock, Felix Duvallet, Sachithra Hemachandra, Jean Oh, Anthony Stentz,\n  Nicholas Roy, and Thomas M. Howard", "title": "Language Understanding for Field and Service Robots in a Priori Unknown\n  Environments", "comments": "Field Robotics (accepted, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Contemporary approaches to perception, planning, estimation, and control have\nallowed robots to operate robustly as our remote surrogates in uncertain,\nunstructured environments. There is now an opportunity for robots to operate\nnot only in isolation, but also with and alongside humans in our complex\nenvironments. Natural language provides an efficient and flexible medium\nthrough which humans can communicate with collaborative robots. Through\nsignificant progress in statistical methods for natural language understanding,\nrobots are now able to interpret a diverse array of free-form navigation,\nmanipulation, and mobile manipulation commands. However, most contemporary\napproaches require a detailed prior spatial-semantic map of the robot's\nenvironment that models the space of possible referents of the utterance.\nConsequently, these methods fail when robots are deployed in new, previously\nunknown, or partially observed environments, particularly when mental models of\nthe environment differ between the human operator and the robot. This paper\nprovides a comprehensive description of a novel learning framework that allows\nfield and service robots to interpret and correctly execute natural language\ninstructions in a priori unknown, unstructured environments. Integral to our\napproach is its use of language as a \"sensor\" -- inferring spatial,\ntopological, and semantic information implicit in natural language utterances\nand then exploiting this information to learn a distribution over a latent\nenvironment model. We incorporate this distribution in a probabilistic language\ngrounding model and infer a distribution over a symbolic representation of the\nrobot's action space. We use imitation learning to identify a belief space\npolicy that reasons over the environment and behavior distributions. We\nevaluate our framework through a variety of different navigation and mobile\nmanipulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:13:05 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Walter", "Matthew R.", ""], ["Patki", "Siddharth", ""], ["Daniele", "Andrea F.", ""], ["Fahnestock", "Ethan", ""], ["Duvallet", "Felix", ""], ["Hemachandra", "Sachithra", ""], ["Oh", "Jean", ""], ["Stentz", "Anthony", ""], ["Roy", "Nicholas", ""], ["Howard", "Thomas M.", ""]]}, {"id": "2105.10419", "submitter": "Ivana Kvapilikova", "authors": "Ivana Kvapil{\\i}kova, Mikel Artetxe, Gorka Labaka, Eneko Agirre,\n  Ond\\v{r}ej Bojar", "title": "Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining", "comments": "ACL SRW 2020", "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics - Student Research Workshop, pages 255-262,\n  Association for Computational Linguistics, 2020", "doi": "10.18653/v1/2020.acl-srw.34", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models of multilingual sentence embeddings require large parallel\ndata resources which are not available for low-resource languages. We propose a\nnovel unsupervised method to derive multilingual sentence embeddings relying\nonly on monolingual data. We first produce a synthetic parallel corpus using\nunsupervised machine translation, and use it to fine-tune a pretrained\ncross-lingual masked language model (XLM) to derive the multilingual sentence\nrepresentations. The quality of the representations is evaluated on two\nparallel corpus mining tasks with improvements of up to 22 F1 points over\nvanilla XLM. In addition, we observe that a single synthetic bilingual corpus\nis able to improve results for other language pairs.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:39:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kvapil\u0131kova", "Ivana", ""], ["Artetxe", "Mikel", ""], ["Labaka", "Gorka", ""], ["Agirre", "Eneko", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2105.10606", "submitter": "Parag Dakle", "authors": "Parag Pravin Dakle and Dan I. Moldovan", "title": "CEREC: A Corpus for Entity Resolution in Email Conversations", "comments": null, "journal-ref": "Proceedings of the 28th International Conference on Computational\n  Linguistics, pp. 339-349. 2020", "doi": "10.18653/v1/2020.coling-main.30", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first large scale corpus for entity resolution in email\nconversations (CEREC). The corpus consists of 6001 email threads from the Enron\nEmail Corpus containing 36,448 email messages and 60,383 entity coreference\nchains. The annotation is carried out as a two-step process with minimal manual\neffort. Experiments are carried out for evaluating different features and\nperformance of four baselines on the created corpus. For the task of mention\nidentification and coreference resolution, a best performance of 59.2 F1 is\nreported, highlighting the room for improvement. An in-depth qualitative and\nquantitative error analysis is presented to understand the limitations of the\nbaselines considered.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:40:12 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:08:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Dakle", "Parag Pravin", ""], ["Moldovan", "Dan I.", ""]]}, {"id": "2105.10861", "submitter": "Tung Nguyen Thanh", "authors": "Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li", "title": "RST Parsing from Scratch", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel top-down end-to-end formulation of document-level\ndiscourse parsing in the Rhetorical Structure Theory (RST) framework. In this\nformulation, we consider discourse parsing as a sequence of splitting decisions\nat token boundaries and use a seq2seq network to model the splitting decisions.\nOur framework facilitates discourse parsing from scratch without requiring\ndiscourse segmentation as a prerequisite; rather, it yields segmentation as\npart of the parsing process. Our unified parsing model adopts a beam search to\ndecode the best tree structure by searching through a space of high-scoring\ntrees. With extensive experiments on the standard English RST discourse\ntreebank, we demonstrate that our parser outperforms existing methods by a good\nmargin in both end-to-end parsing and parsing with gold segmentation. More\nimportantly, it does so without using any handcrafted features, making it\nfaster and easily adaptable to new languages and domains.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 06:19:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nguyen", "Thanh-Tung", ""], ["Nguyen", "Xuan-Phi", ""], ["Joty", "Shafiq", ""], ["Li", "Xiaoli", ""]]}, {"id": "2105.10878", "submitter": "Guandong Xu", "authors": "Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu", "title": "DepressionNet: A Novel Summarization Boosted Deep Framework for\n  Depression Detection on Social Media", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3462938", "report-no": null, "categories": "cs.LG cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is currently a popular online social media platform which allows\nusers to share their user-generated content. This publicly-generated user data\nis also crucial to healthcare technologies because the discovered patterns\nwould hugely benefit them in several ways. One of the applications is in\nautomatically discovering mental health problems, e.g., depression. Previous\nstudies to automatically detect a depressed user on online social media have\nlargely relied upon the user behaviour and their linguistic patterns including\nuser's social interactions. The downside is that these models are trained on\nseveral irrelevant content which might not be crucial towards detecting a\ndepressed user. Besides, these content have a negative impact on the overall\nefficiency and effectiveness of the model. To overcome the shortcomings in the\nexisting automatic depression detection methods, we propose a novel\ncomputational framework for automatic depression detection that initially\nselects relevant content through a hybrid extractive and abstractive\nsummarization strategy on the sequence of all user tweets leading to a more\nfine-grained and relevant content. The content then goes to our novel deep\nlearning framework comprising of a unified learning machinery comprising of\nConvolutional Neural Network (CNN) coupled with attention-enhanced Gated\nRecurrent Units (GRU) models leading to better empirical performance than\nexisting strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:05:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zogan", "Hamad", ""], ["Razzak", "Imran", ""], ["Jameel", "Shoaib", ""], ["Xu", "Guandong", ""]]}, {"id": "2105.10909", "submitter": "Lingjuan Lyu", "authors": "Lingjuan Lyu, Xuanli He, Fangzhao Wu, Lichao Sun", "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute\n  from BERT-based APIs", "comments": "paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances in pre-trained models (e.g., BERT, XLNET and etc) have largely\nrevolutionized the predictive performance of various modern natural language\nprocessing tasks. This allows corporations to provide machine learning as a\nservice (MLaaS) by encapsulating fine-tuned BERT-based models as commercial\nAPIs. However, previous works have discovered a series of vulnerabilities in\nBERT- based APIs. For example, BERT-based APIs are vulnerable to both model\nextraction attack and adversarial example transferrability attack. However, due\nto the high capacity of BERT-based APIs, the fine-tuned model is easy to be\noverlearned, what kind of information can be leaked from the extracted model\nremains unknown and is lacking. To bridge this gap, in this work, we first\npresent an effective model extraction attack, where the adversary can\npractically steal a BERT-based API (the target/victim model) by only querying a\nlimited number of queries. We further develop an effective attribute inference\nattack to expose the sensitive attribute of the training data used by the\nBERT-based APIs. Our extensive experiments on benchmark datasets under various\nrealistic settings demonstrate the potential vulnerabilities of BERT-based\nAPIs.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 10:38:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lyu", "Lingjuan", ""], ["He", "Xuanli", ""], ["Wu", "Fangzhao", ""], ["Sun", "Lichao", ""]]}, {"id": "2105.10912", "submitter": "Dustin Wright", "authors": "Dustin Wright and Isabelle Augenstein", "title": "CiteWorth: Cite-Worthiness Detection for Improved Scientific Document\n  Understanding", "comments": "12 pages, 9 tables, 1 figure", "journal-ref": "Findings of ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific document understanding is challenging as the data is highly domain\nspecific and diverse. However, datasets for tasks with scientific text require\nexpensive manual annotation and tend to be small and limited to only one or a\nfew fields. At the same time, scientific documents contain many potential\ntraining signals, such as citations, which can be used to build large labelled\ndatasets. Given this, we present an in-depth study of cite-worthiness detection\nin English, where a sentence is labelled for whether or not it cites an\nexternal source. To accomplish this, we introduce CiteWorth, a large,\ncontextualized, rigorously cleaned labelled dataset for cite-worthiness\ndetection built from a massive corpus of extracted plain-text scientific\ndocuments. We show that CiteWorth is high-quality, challenging, and suitable\nfor studying problems such as domain adaptation. Our best performing\ncite-worthiness detection model is a paragraph-level contextualized sentence\nlabelling model based on Longformer, exhibiting a 5 F1 point improvement over\nSciBERT which considers only individual sentences. Finally, we demonstrate that\nlanguage model fine-tuning with cite-worthiness as a secondary task leads to\nimproved performance on downstream scientific document understanding tasks.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:08:45 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 09:20:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wright", "Dustin", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2105.10922", "submitter": "Shumin Deng", "authors": "Shumin Deng, Ningyu Zhang, Luoqiu Li, Hui Chen, Huaixiao Tou, Mosha\n  Chen, Fei Huang, Huajun Chen", "title": "OntoED: Low-resource Event Detection with Ontology Embedding", "comments": "Accepted to appear at the ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event Detection (ED) aims to identify event trigger words from a given text\nand classify it into an event type. Most of current methods to ED rely heavily\non training instances, and almost ignore the correlation of event types. Hence,\nthey tend to suffer from data scarcity and fail to handle new unseen event\ntypes. To address these problems, we formulate ED as a process of event\nontology population: linking event instances to pre-defined event types in\nevent ontology, and propose a novel ED framework entitled OntoED with ontology\nembedding. We enrich event ontology with linkages among event types, and\nfurther induce more event-event correlations. Based on the event ontology,\nOntoED can leverage and propagate correlation knowledge, particularly from\ndata-rich to data-poor event types. Furthermore, OntoED can be applied to new\nunseen event types, by establishing linkages to existing ones. Experiments\nindicate that OntoED is more predominant and robust than previous approaches to\nED, especially in data-scarce scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 12:00:22 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 03:57:12 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 15:11:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Deng", "Shumin", ""], ["Zhang", "Ningyu", ""], ["Li", "Luoqiu", ""], ["Chen", "Hui", ""], ["Tou", "Huaixiao", ""], ["Chen", "Mosha", ""], ["Huang", "Fei", ""], ["Chen", "Huajun", ""]]}, {"id": "2105.10956", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Hai Zhao", "title": "Structural Pre-training for Dialogue Comprehension", "comments": "Accepted by ACL-IJCNLP 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (PrLMs) have demonstrated superior performance\ndue to their strong ability to learn universal language representations from\nself-supervised pre-training. However, even with the help of the powerful\nPrLMs, it is still challenging to effectively capture task-related knowledge\nfrom dialogue texts which are enriched by correlations among speaker-aware\nutterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE\nReader, to capture dialogue exclusive features. To simulate the dialogue-like\nfeatures, we propose two training objectives in addition to the original LM\nobjectives: 1) utterance order restoration, which predicts the order of the\npermuted utterances in dialogue context; 2) sentence backbone regularization,\nwhich regularizes the model to improve the factual correctness of summarized\nsubject-verb-object triplets. Experimental results on widely used dialogue\nbenchmarks verify the effectiveness of the newly introduced self-supervised\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 15:16:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2105.10966", "submitter": "Oana Cocarascu", "authors": "Joel Oksanen, Oana Cocarascu, Francesca Toni", "title": "Automatic Product Ontology Extraction from Textual Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ontologies have proven beneficial in different settings that make use of\ntextual reviews. However, manually constructing ontologies is a laborious and\ntime-consuming process in need of automation. We propose a novel methodology\nfor automatically extracting ontologies, in the form of meronomies, from\nproduct reviews, using a very limited amount of hand-annotated training data.\nWe show that the ontologies generated by our method outperform hand-crafted\nontologies (WordNet) and ontologies extracted by existing methods (Text2Onto\nand COMET) in several, diverse settings. Specifically, our generated ontologies\noutperform the others when evaluated by human annotators as well as on an\nexisting Q&A dataset from Amazon. Moreover, our method is better able to\ngeneralise, in capturing knowledge about unseen products. Finally, we consider\na real-world setting, showing that our method is better able to determine\nrecommended products based on their reviews, in alternative to using Amazon's\nstandard score aggregations.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 16:06:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Oksanen", "Joel", ""], ["Cocarascu", "Oana", ""], ["Toni", "Francesca", ""]]}, {"id": "2105.11018", "submitter": "Lei Sha", "authors": "Lei Sha, Patrick Hohenecker, Thomas Lukasiewicz", "title": "Controlling Text Edition by Changing Answers of Specific Questions", "comments": null, "journal-ref": "ACL 2021 findings", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce the new task of controllable text edition, in\nwhich we take as input a long text, a question, and a target answer, and the\noutput is a minimally modified text, so that it fits the target answer. This\ntask is very important in many situations, such as changing some conditions,\nconsequences, or properties in a legal document, or changing some key\ninformation of an event in a news text. This is very challenging, as it is hard\nto obtain a parallel corpus for training, and we need to first find all text\npositions that should be changed and then decide how to change them. We\nconstructed the new dataset WikiBioCTE for this task based on the existing\ndataset WikiBio (originally created for table-to-text generation). We use\nWikiBioCTE for training, and manually labeled a test set for testing. We also\npropose novel evaluation metrics and a novel method for solving the new task.\nExperimental results on the test set show that our proposed method is a good\nfit for this novel NLP task.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:44:15 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sha", "Lei", ""], ["Hohenecker", "Patrick", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2105.11084", "submitter": "Michael Auli", "authors": "Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, Michael Auli", "title": "Unsupervised Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid progress in the recent past, current speech recognition systems\nstill require labeled training data which limits this technology to a small\nfraction of the languages spoken around the globe. This paper describes\nwav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition\nmodels without any labeled data. We leverage self-supervised speech\nrepresentations to segment unlabeled audio and learn a mapping from these\nrepresentations to phonemes via adversarial training. The right representations\nare key to the success of our method. Compared to the best previous\nunsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT\nbenchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,\nwav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the\nbest published systems trained on 960 hours of labeled data from only two years\nago. We also experiment on nine other languages, including low-resource\nlanguages such as Kyrgyz, Swahili and Tatar.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 04:10:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Baevski", "Alexei", ""], ["Hsu", "Wei-Ning", ""], ["Conneau", "Alexis", ""], ["Auli", "Michael", ""]]}, {"id": "2105.11098", "submitter": "Fandong Meng", "authors": "Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, Jie Zhou", "title": "Prevent the Language Model from being Overconfident in Neural Machine\n  Translation", "comments": "Accepted as a long paper at ACL 2021. Code is available at:\n  https://github.com/Mlair77/nmt_adequacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Machine Translation (NMT) model is essentially a joint language\nmodel conditioned on both the source sentence and partial translation.\nTherefore, the NMT model naturally involves the mechanism of the Language Model\n(LM) that predicts the next token only based on partial translation. Despite\nits success, NMT still suffers from the hallucination problem, generating\nfluent but inadequate translations. The main reason is that NMT pays excessive\nattention to the partial translation while neglecting the source sentence to\nsome extent, namely overconfidence of the LM. Accordingly, we define the Margin\nbetween the NMT and the LM, calculated by subtracting the predicted probability\nof the LM from that of the NMT model for each token. The Margin is negatively\ncorrelated to the overconfidence degree of the LM. Based on the property, we\npropose a Margin-based Token-level Objective (MTO) and a Margin-based\nSentencelevel Objective (MSO) to maximize the Margin for preventing the LM from\nbeing overconfident. Experiments on WMT14 English-to-German, WMT19\nChinese-to-English, and WMT14 English-to-French translation tasks demonstrate\nthe effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements,\nrespectively, compared to the Transformer baseline. The human evaluation\nfurther verifies that our approaches improve translation adequacy as well as\nfluency.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 05:34:09 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 15:57:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Miao", "Mengqi", ""], ["Meng", "Fandong", ""], ["Liu", "Yijin", ""], ["Zhou", "Xiao-Hua", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.11115", "submitter": "Shunyu Yao", "authors": "Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan", "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages", "comments": "ACL 2021. 19 pages with extended appendix. Code:\n  https://github.com/princeton-nlp/dyck-transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:42:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yao", "Shunyu", ""], ["Peng", "Binghui", ""], ["Papadimitriou", "Christos", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2105.11119", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Alberto Valido, Katherine M. Ingram, Giulia Fanti, Suma\n  Bhat, Dorothy L. Espelage", "title": "Abusive Language Detection in Heterogeneous Contexts: Dataset Collection\n  and the Role of Supervised Attention", "comments": "AAAI 2021 (AI for Social Impact track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abusive language is a massive problem in online social platforms. Existing\nabusive language detection techniques are particularly ill-suited to comments\ncontaining heterogeneous abusive language patterns, i.e., both abusive and\nnon-abusive parts. This is due in part to the lack of datasets that explicitly\nannotate heterogeneity in abusive language. We tackle this challenge by\nproviding an annotated dataset of abusive language in over 11,000 comments from\nYouTube. We account for heterogeneity in this dataset by separately annotating\nboth the comment as a whole and the individual sentences that comprise each\ncomment. We then propose an algorithm that uses a supervised attention\nmechanism to detect and categorize abusive content using multi-task learning.\nWe empirically demonstrate the challenges of using traditional techniques on\nheterogeneous content and the comparative gains in performance of the proposed\napproach over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:50:19 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gong", "Hongyu", ""], ["Valido", "Alberto", ""], ["Ingram", "Katherine M.", ""], ["Fanti", "Giulia", ""], ["Bhat", "Suma", ""], ["Espelage", "Dorothy L.", ""]]}, {"id": "2105.11134", "submitter": "Jiacheng Ye", "authors": "Jiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, Qi Zhang", "title": "One2Set: Generating Diverse Keyphrases as a Set", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, the sequence-to-sequence models have made remarkable progress on\nthe task of keyphrase generation (KG) by concatenating multiple keyphrases in a\npredefined order as a target sequence during training. However, the keyphrases\nare inherently an unordered set rather than an ordered sequence. Imposing a\npredefined order will introduce wrong bias during training, which can highly\npenalize shifts in the order between keyphrases. In this work, we propose a new\ntraining paradigm One2Set without predefining an order to concatenate the\nkeyphrases. To fit this paradigm, we propose a novel model that utilizes a\nfixed set of learned control codes as conditions to generate a set of\nkeyphrases in parallel. To solve the problem that there is no correspondence\nbetween each prediction and target during training, we propose a $K$-step\ntarget assignment mechanism via bipartite matching, which greatly increases the\ndiversity and reduces the duplication ratio of generated keyphrases. The\nexperimental results on multiple benchmarks demonstrate that our approach\nsignificantly outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:29:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ye", "Jiacheng", ""], ["Gui", "Tao", ""], ["Luo", "Yichao", ""], ["Xu", "Yige", ""], ["Zhang", "Qi", ""]]}, {"id": "2105.11136", "submitter": "Jieyu Lin", "authors": "Jieyu Lin, Jiajie Zou and Nai Ding", "title": "Using Adversarial Attacks to Reveal the Statistical Bias in Machine\n  Reading Comprehension Models", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models have achieved human-level performance on many\nMachine Reading Comprehension (MRC) tasks, but it remains unclear whether these\nmodels truly understand language or answer questions by exploiting statistical\nbiases in datasets. Here, we demonstrate a simple yet effective method to\nattack MRC models and reveal the statistical biases in these models. We apply\nthe method to the RACE dataset, for which the answer to each MRC question is\nselected from 4 options. It is found that several pre-trained language models,\nincluding BERT, ALBERT, and RoBERTa, show consistent preference to some\noptions, even when these options are irrelevant to the question. When\ninterfered by these irrelevant options, the performance of MRC models can be\nreduced from human-level performance to the chance-level performance. Human\nreaders, however, are not clearly affected by these irrelevant options.\nFinally, we propose an augmented training method that can greatly reduce\nmodels' statistical biases.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:35:56 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 08:24:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Jieyu", ""], ["Zou", "Jiajie", ""], ["Ding", "Nai", ""]]}, {"id": "2105.11174", "submitter": "Han Wang", "authors": "Han Wang, Yang Liu, Chenguang Zhu, Linjun Shou, Ming Gong, Yichong Xu,\n  Michael Zeng", "title": "Retrieval Enhanced Model for Commonsense Generation", "comments": "Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense generation is a challenging task of generating a plausible\nsentence describing an everyday scenario using provided concepts. Its\nrequirement of reasoning over commonsense knowledge and compositional\ngeneralization ability even puzzles strong pre-trained language generation\nmodels. We propose a novel framework using retrieval methods to enhance both\nthe pre-training and fine-tuning for commonsense generation. We retrieve\nprototype sentence candidates by concept matching and use them as auxiliary\ninput. For fine-tuning, we further boost its performance with a trainable\nsentence retriever. We demonstrate experimentally on the large-scale CommonGen\nbenchmark that our approach achieves new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:49:17 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Han", ""], ["Liu", "Yang", ""], ["Zhu", "Chenguang", ""], ["Shou", "Linjun", ""], ["Gong", "Ming", ""], ["Xu", "Yichong", ""], ["Zeng", "Michael", ""]]}, {"id": "2105.11178", "submitter": "Christina Niklaus", "authors": "Christina Niklaus, Matthias Cetto, Andr\\'e Freitas, Siegfried\n  Handschuh", "title": "Context-Preserving Text Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a context-preserving text simplification (TS) approach that\nrecursively splits and rephrases complex English sentences into a semantic\nhierarchy of simplified sentences. Using a set of linguistically principled\ntransformation patterns, input sentences are converted into a hierarchical\nrepresentation in the form of core sentences and accompanying contexts that are\nlinked via rhetorical relations. Hence, as opposed to previously proposed\nsentence splitting approaches, which commonly do not take into account\ndiscourse-level aspects, our TS approach preserves the semantic relationship of\nthe decomposed constituents in the output. A comparative analysis with the\nannotations contained in the RST-DT shows that we are able to capture the\ncontextual hierarchy between the split sentences with a precision of 89% and\nreach an average precision of 69% for the classification of the rhetorical\nrelations that hold between them.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:54:56 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Niklaus", "Christina", ""], ["Cetto", "Matthias", ""], ["Freitas", "Andr\u00e9", ""], ["Handschuh", "Siegfried", ""]]}, {"id": "2105.11197", "submitter": "Hongru Liang", "authors": "Hongru Liang and Huaqing Li", "title": "Towards Standard Criteria for human evaluation of Chatbots: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human evaluation is becoming a necessity to test the performance of Chatbots.\nHowever, off-the-shelf settings suffer the severe reliability and replication\nissues partly because of the extremely high diversity of criteria. It is high\ntime to come up with standard criteria and exact definitions. To this end, we\nconduct a through investigation of 105 papers involving human evaluation for\nChatbots. Deriving from this, we propose five standard criteria along with\nprecise definitions.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 10:49:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liang", "Hongru", ""], ["Li", "Huaqing", ""]]}, {"id": "2105.11210", "submitter": "Chenliang Li", "authors": "Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang\n  and Luo Si", "title": "StructuralLM: Structural Pre-training for Form Understanding", "comments": "Accepted by ACL2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models achieve state-of-the-art results when\nfine-tuned on downstream NLP tasks. However, they almost exclusively focus on\ntext-only representation, while neglecting cell-level layout information that\nis important for form image understanding. In this paper, we propose a new\npre-training approach, StructuralLM, to jointly leverage cell and layout\ninformation from scanned documents. Specifically, we pre-train StructuralLM\nwith two new designs to make the most of the interactions of cell and layout\ninformation: 1) each cell as a semantic unit; 2) classification of cell\npositions. The pre-trained StructuralLM achieves new state-of-the-art results\nin different types of downstream tasks, including form understanding (from\n78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and\ndocument image classification (from 94.43 to 96.08).\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:33:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Chenliang", ""], ["Bi", "Bin", ""], ["Yan", "Ming", ""], ["Wang", "Wei", ""], ["Huang", "Songfang", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""]]}, {"id": "2105.11219", "submitter": "Parth Patwa", "authors": "Parth Patwa, Srinivas PYKL, Amitava Das, Prerana Mukherjee, Viswanath\n  Pulabaigari", "title": "Hater-O-Genius Aggression Classification using Capsule Networks", "comments": "Accepted at the 17th International Conference on Natural Language\n  Processing (ICON 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contending hate speech in social media is one of the most challenging social\nproblems of our time. There are various types of anti-social behavior in social\nmedia. Foremost of them is aggressive behavior, which is causing many social\nissues such as affecting the social lives and mental health of social media\nusers. In this paper, we propose an end-to-end ensemble-based architecture to\nautomatically identify and classify aggressive tweets. Tweets are classified\ninto three categories - Covertly Aggressive, Overtly Aggressive, and\nNon-Aggressive. The proposed architecture is an ensemble of smaller subnetworks\nthat are able to characterize the feature embeddings effectively. We\ndemonstrate qualitatively that each of the smaller subnetworks is able to learn\nunique features. Our best model is an ensemble of Capsule Networks and results\nin a 65.2% F1 score on the Facebook test set, which results in a performance\ngain of 0.95% over the TRAC-2018 winners. The code and the model weights are\npublicly available at\nhttps://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:53:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Patwa", "Parth", ""], ["PYKL", "Srinivas", ""], ["Das", "Amitava", ""], ["Mukherjee", "Prerana", ""], ["Pulabaigari", "Viswanath", ""]]}, {"id": "2105.11223", "submitter": "Mike Zhang", "authors": "Kristian N{\\o}rgaard Jensen, Mike Zhang, Barbara Plank", "title": "De-identification of Privacy-related Entities in Job Postings", "comments": "12 pages, 1 figure, 6 tables, accepted in NoDaLiDa 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De-identification is the task of detecting privacy-related entities in text,\nsuch as person names, emails and contact data. It has been well-studied within\nthe medical domain. The need for de-identification technology is increasing, as\nprivacy-preserving data handling is in high demand in many domains. In this\npaper, we focus on job postings. We present JobStack, a new corpus for\nde-identification of personal data in job vacancies on Stackoverflow. We\nintroduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer\nmodels. To improve upon these baselines, we experiment with contextualized\nembeddings and distantly related auxiliary data via multi-task learning. Our\nresults show that auxiliary data improves de-identification performance.\nSurprisingly, vanilla BERT turned out to be more effective than a BERT model\ntrained on other portions of Stackoverflow.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:01:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jensen", "Kristian N\u00f8rgaard", ""], ["Zhang", "Mike", ""], ["Plank", "Barbara", ""]]}, {"id": "2105.11225", "submitter": "Tianming Liang", "authors": "Tianming Liang, Yang Liu, Xiaoyan Liu, Gaurav Sharma and Maozu Guo", "title": "Distantly-Supervised Long-Tailed Relation Extraction Using Constraint\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Label noise and long-tailed distributions are two major challenges in\ndistantly supervised relation extraction. Recent studies have shown great\nprogress on denoising, but pay little attention to the problem of long-tailed\nrelations. In this paper, we introduce constraint graphs to model the\ndependencies between relation labels. On top of that, we further propose a\nnovel constraint graph-based relation extraction framework(CGRE) to handle the\ntwo challenges simultaneously. CGRE employs graph convolution networks (GCNs)\nto propagate information from data-rich relation nodes to data-poor relation\nnodes, and thus boosts the representation learning of long-tailed relations. To\nfurther improve the noise immunity, a constraint-aware attention module is\ndesigned in CGRE to integrate the constraint information. Experimental results\non a widely-used benchmark dataset indicate that our approach achieves\nsignificant improvements over the previous methods for both denoising and\nlong-tailed relation extraction.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:02:32 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 09:09:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liang", "Tianming", ""], ["Liu", "Yang", ""], ["Liu", "Xiaoyan", ""], ["Sharma", "Gaurav", ""], ["Guo", "Maozu", ""]]}, {"id": "2105.11246", "submitter": "Ziyun Wang", "authors": "Ziyun Wang, Xuan Liu, Peiji Yang, Shixing Liu, Zhisheng Wang", "title": "Cross-lingual Text Classification with Heterogeneous Graph Neural\n  Network", "comments": "Accepted by ACL 2021 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cross-lingual text classification aims at training a classifier on the source\nlanguage and transferring the knowledge to target languages, which is very\nuseful for low-resource languages. Recent multilingual pretrained language\nmodels (mPLM) achieve impressive results in cross-lingual classification tasks,\nbut rarely consider factors beyond semantic similarity, causing performance\ndegradation between some language pairs. In this paper we propose a simple yet\neffective method to incorporate heterogeneous information within and across\nlanguages for cross-lingual text classification using graph convolutional\nnetworks (GCN). In particular, we construct a heterogeneous graph by treating\ndocuments and words as nodes, and linking nodes with different relations, which\ninclude part-of-speech roles, semantic similarity, and document translations.\nExtensive experiments show that our graph-based method significantly\noutperforms state-of-the-art models on all tasks, and also achieves consistent\nperformance gain over baselines in low-resource settings where external tools\nlike translators are unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:45:42 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Ziyun", ""], ["Liu", "Xuan", ""], ["Yang", "Peiji", ""], ["Liu", "Shixing", ""], ["Wang", "Zhisheng", ""]]}, {"id": "2105.11259", "submitter": "Han Xu", "authors": "Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, Maosong Sun", "title": "PTR: Prompt Tuning with Rules for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream task. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is challenging for prompt tuning to address many-class classification\ntasks. To this end, we propose prompt tuning with rules (PTR) for many-class\ntext classification, and apply logic rules to construct prompts with several\nsub-prompts. In this way, PTR is able to encode prior knowledge of each class\ninto prompt tuning. We conduct experiments on relation classification, a\ntypical many-class classification task, and the results on benchmarks show that\nPTR can significantly and consistently outperform existing state-of-the-art\nbaselines. This indicates that PTR is a promising approach to take advantage of\nPLMs for those complicated classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:24:02 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:32:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Han", "Xu", ""], ["Zhao", "Weilin", ""], ["Ding", "Ning", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2105.11260", "submitter": "Andrew Halterman", "authors": "Andrew Halterman, Benjamin J. Radford", "title": "Few-Shot Upsampling for Protest Size Detection", "comments": "Accepted into Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new task and dataset for a common problem in social science\nresearch: \"upsampling\" coarse document labels to fine-grained labels or spans.\nWe pose the problem in a question answering format, with the answers providing\nthe fine-grained labels. We provide a benchmark dataset and baselines on a\nsocially impactful task: identifying the exact crowd size at protests and\ndemonstrations in the United States given only order-of-magnitude information\nabout protest attendance, a very small sample of fine-grained examples, and\nEnglish-language news text. We evaluate several baseline models, including\nzero-shot results from rule-based and question-answering models, few-shot\nmodels fine-tuned on a small set of documents, and weakly supervised models\nusing a larger set of coarsely-labeled documents. We find that our rule-based\nmodel initially outperforms a zero-shot pre-trained transformer language model\nbut that further fine-tuning on a very small subset of 25 examples\nsubstantially improves out-of-sample performance. We also demonstrate a method\nfor fine-tuning the transformer span on only the coarse labels that performs\nsimilarly to our rule-based approach. This work will contribute to social\nscientists' ability to generate data to understand the causes and successes of\ncollective action.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:27:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Halterman", "Andrew", ""], ["Radford", "Benjamin J.", ""]]}, {"id": "2105.11263", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon, P. Gloor, D. F. Iezzi", "title": "Editorial introduction: The power of words and networks", "comments": null, "journal-ref": "International Journal of Information Management 51, 102031 (2020)", "doi": "10.1016/j.ijinfomgt.2019.10.016", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  According to Freud \"words were originally magic and to this day words have\nretained much of their ancient magical power\". By words, behaviors are\ntransformed and problems are solved. The way we use words reveals our\nintentions, goals and values. Novel tools for text analysis help understand the\nmagical power of words. This power is multiplied, if it is combined with the\nstudy of social networks, i.e. with the analysis of relationships among social\nunits. This special issue of the International Journal of Information\nManagement, entitled \"Combining Social Network Analysis and Text Mining: from\nTheory to Practice\", includes heterogeneous and innovative research at the\nnexus of text mining and social network analysis. It aims to enrich work at the\nintersection of these fields, which still lags behind in theoretical,\nempirical, and methodological foundations. The nine articles accepted for\ninclusion in this special issue all present methods and tools that have\nbusiness applications. They are summarized in this editorial introduction.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:29:17 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Gloor", "P.", ""], ["Iezzi", "D. F.", ""]]}, {"id": "2105.11269", "submitter": "Deng Cai", "authors": "Deng Cai and Yan Wang and Huayang Li and Wai Lam and Lemao Liu", "title": "Neural Machine Translation with Monolingual Translation Memory", "comments": "ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work has proved that Translation memory (TM) can boost the performance\nof Neural Machine Translation (NMT). In contrast to existing work that uses\nbilingual corpus as TM and employs source-side similarity search for memory\nretrieval, we propose a new framework that uses monolingual memory and performs\nlearnable memory retrieval in a cross-lingual manner. Our framework has unique\nadvantages. First, the cross-lingual memory retriever allows abundant\nmonolingual data to be TM. Second, the memory retriever and NMT model can be\njointly optimized for the ultimate translation goal. Experiments show that the\nproposed method obtains substantial improvements. Remarkably, it even\noutperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the\nability to leverage monolingual data, our model also demonstrates effectiveness\nin low-resource and domain adaptation scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:35:19 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 07:41:52 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Cai", "Deng", ""], ["Wang", "Yan", ""], ["Li", "Huayang", ""], ["Lam", "Wai", ""], ["Liu", "Lemao", ""]]}, {"id": "2105.11276", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. La Bella, A. Fronzetti Colladon, E. Battistoni, S. Castellan, M.\n  Francucci", "title": "Assessing perceived organizational leadership styles through twitter\n  text mining", "comments": null, "journal-ref": "Journal of the Association for Information Science and Technology\n  61(1), 21-31 (2018)", "doi": "10.1002/asi.23918", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a text classification tool based on support vector machines for\nthe assessment of organizational leadership styles, as appearing to Twitter\nusers. We collected Twitter data over 51 days, related to the first 30 Italian\norganizations in the 2015 ranking of Forbes Global 2000-out of which we\nselected the five with the most relevant volumes of tweets. We analyzed the\ncommunication of the company leaders, together with the dialogue among the\nstakeholders of each company, to understand the association with perceived\nleadership styles and dimensions. To assess leadership profiles, we referred to\nthe 10-factor model developed by Barchiesi and La Bella in 2007. We maintain\nthe distinctiveness of the approach we propose, as it allows a rapid assessment\nof the perceived leadership capabilities of an enterprise, as they emerge from\nits social media interactions. It can also be used to show how companies\nrespond and manage their communication when specific events take place, and to\nassess their stakeholder's reactions.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:45:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["La Bella", "A.", ""], ["Colladon", "A. Fronzetti", ""], ["Battistoni", "E.", ""], ["Castellan", "S.", ""], ["Francucci", "M.", ""]]}, {"id": "2105.11294", "submitter": "Peter Wallis", "authors": "Peter Wallis", "title": "Introducing the Talk Markup Language (TalkML):Adding a little social\n  intelligence to industrial speech interfaces", "comments": "24 pages, 7 figures, 67 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Virtual Personal Assistants like Siri have great potential but such\ndevelopments hit the fundamental problem of how to make computational devices\nthat understand human speech. Natural language understanding is one of the more\ndisappointing failures of AI research and it seems there is something we\ncomputer scientists don't get about the nature of language. Of course\nphilosophers and linguists think quite differently about language and this\npaper describes how we have taken ideas from other disciplines and implemented\nthem. The background to the work is to take seriously the notion of language as\naction and look at what people actually do with language using the techniques\nof Conversation Analysis. The observation has been that human communication is\n(behind the scenes) about the management of social relations as well as the\n(foregrounded) passing of information. To claim this is one thing but to\nimplement it requires a mechanism. The mechanism described here is based on the\nnotion of language being intentional - we think intentionally, talk about them\nand recognise them in others - and cooperative in that we are compelled to help\nout. The way we are compelled points to a solution to the ever present problem\nof keeping the human on topic. The approach has led to a recent success in\nwhich we significantly improve user satisfaction independent of task\ncompletion. Talk Markup Language (TalkML) is a draft alternative to VoiceXML\nthat, we propose, greatly simplifies the scripting of interaction by providing\ndefault behaviours for no input and not recognised speech events.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:25:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wallis", "Peter", ""]]}, {"id": "2105.11301", "submitter": "Barbara Plank", "authors": "Barbara Plank, Kristian N{\\o}rgaard Jensen and Rob van der Goot", "title": "DaN+: Danish Nested Named Entities and Lexical Normalization", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces DaN+, a new multi-domain corpus and annotation\nguidelines for Danish nested named entities (NEs) and lexical normalization to\nsupport research on cross-lingual cross-domain learning for a less-resourced\nlanguage. We empirically assess three strategies to model the two-layer Named\nEntity Recognition (NER) task. We compare transfer capabilities from German\nversus in-language annotation from scratch. We examine language-specific versus\nmultilingual BERT, and study the effect of lexical normalization on NER. Our\nresults show that 1) the most robust strategy is multi-task learning which is\nrivaled by multi-label decoding, 2) BERT-based NER models are sensitive to\ndomain shifts, and 3) in-language BERT and lexical normalization are the most\nbeneficial on the least canonical data. Our results also show that an\nout-of-domain setup remains challenging, while performance on news plateaus\nquickly. This highlights the importance of cross-domain evaluation of\ncross-lingual transfer.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:35:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Plank", "Barbara", ""], ["Jensen", "Kristian N\u00f8rgaard", ""], ["van der Goot", "Rob", ""]]}, {"id": "2105.11314", "submitter": "Milan Straka", "authors": "Milan Straka, Jakub N\\'aplava, Jana Strakov\\'a, David Samuel", "title": "RobeCzech: Czech RoBERTa, a monolingual contextualized language\n  representation model", "comments": "Accepted to TSD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RobeCzech, a monolingual RoBERTa language representation model\ntrained on Czech data. RoBERTa is a robustly optimized Transformer-based\npretraining approach. We show that RobeCzech considerably outperforms\nequally-sized multilingual and Czech-trained contextualized language\nrepresentation models, surpasses current state of the art in all five evaluated\nNLP tasks and reaches state-of-theart results in four of them. The RobeCzech\nmodel is released publicly at https://hdl.handle.net/11234/1-3691 and\nhttps://huggingface.co/ufal/robeczech-base.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:50:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Straka", "Milan", ""], ["N\u00e1plava", "Jakub", ""], ["Strakov\u00e1", "Jana", ""], ["Samuel", "David", ""]]}, {"id": "2105.11321", "submitter": "Kasra Hosseini", "authors": "Kasra Hosseini, Kaspar Beelen, Giovanni Colavizza, Mariona Coll\n  Ardanuy", "title": "Neural Language Models for Nineteenth-Century English", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present four types of neural language models trained on a large historical\ndataset of books in English, published between 1760-1900 and comprised of ~5.1\nbillion tokens. The language model architectures include static (word2vec and\nfastText) and contextualized models (BERT and Flair). For each architecture, we\ntrained a model instance using the whole dataset. Additionally, we trained\nseparate instances on text published before 1850 for the two static models, and\nfour instances considering different time slices for BERT. Our models have\nalready been used in various downstream tasks where they consistently improved\nperformance. In this paper, we describe how the models have been created and\noutline their reuse potential.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:57:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hosseini", "Kasra", ""], ["Beelen", "Kaspar", ""], ["Colavizza", "Giovanni", ""], ["Ardanuy", "Mariona Coll", ""]]}, {"id": "2105.11343", "submitter": "Jia Shen", "authors": "Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan,\n  Xintao Wu, Sean McGrew, Dongwon Lee", "title": "Classifying Math KCs via Task-Adaptive Pre-Trained BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Educational content labeled with proper knowledge components (KCs) are\nparticularly useful to teachers or content organizers. However, manually\nlabeling educational content is labor intensive and error-prone. To address\nthis challenge, prior research proposed machine learning based solutions to\nauto-label educational content with limited success. In this work, we\nsignificantly improve prior research by (1) expanding the input types to\ninclude KC descriptions, instructional video titles, and problem descriptions\n(i.e., three types of prediction task), (2) doubling the granularity of the\nprediction from 198 to 385 KC labels (i.e., more practical setting but much\nharder multinomial classification problem), (3) improving the prediction\naccuracies by 0.5-2.3% using Task-adaptive Pre-trained BERT, outperforming six\nbaselines, and (4) proposing a simple evaluation measure by which we can\nrecover 56-73% of mispredicted KC labels. All codes and data sets in the\nexperiments are available at:https://github.com/tbs17/TAPT-BERT\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:27:33 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shen", "Jia Tracy", ""], ["Yamashita", "Michiharu", ""], ["Prihar", "Ethan", ""], ["Heffernan", "Neil", ""], ["Wu", "Xintao", ""], ["McGrew", "Sean", ""], ["Lee", "Dongwon", ""]]}, {"id": "2105.11347", "submitter": "Baban Gain", "authors": "Baban Gain, Dibyanayan Bandyopadhyay, Arkadipta De, Tanik Saikh, Asif\n  Ekbal", "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal\n  Assistance Shared Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:31:24 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 12:07:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gain", "Baban", ""], ["Bandyopadhyay", "Dibyanayan", ""], ["De", "Arkadipta", ""], ["Saikh", "Tanik", ""], ["Ekbal", "Asif", ""]]}, {"id": "2105.11354", "submitter": "Payam Karisani", "authors": "Payam Karisani, Jinho D. Choi, Li Xiong", "title": "View Distillation with Unlabeled Data for Extracting Adverse Drug\n  Effects from User-Generated Data", "comments": "NAACL 2021 (workshops)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm based on multi-layer transformers for identifying\nAdverse Drug Reactions (ADR) in social media data. Our model relies on the\nproperties of the problem and the characteristics of contextual word embeddings\nto extract two views from documents. Then a classifier is trained on each view\nto label a set of unlabeled documents to be used as an initializer for a new\nclassifier in the other view. Finally, the initialized classifier in each view\nis further trained using the initial training examples. We evaluated our model\nin the largest publicly available ADR dataset. The experiments testify that our\nmodel significantly outperforms the transformer-based models pretrained on\ndomain-specific data.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:38:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Karisani", "Payam", ""], ["Choi", "Jinho D.", ""], ["Xiong", "Li", ""]]}, {"id": "2105.11407", "submitter": "Debanjali Biswas", "authors": "Debanjali Biswas, Mohnish Dubey, Md Rashad Al Hasan Rony and Jens\n  Lehmann", "title": "VANiLLa : Verbalized Answers in Natural Language at Large Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last years, there have been significant developments in the area of\nQuestion Answering over Knowledge Graphs (KGQA). Despite all the notable\nadvancements, current KGQA datasets only provide the answers as the direct\noutput result of the formal query, rather than full sentences incorporating\nquestion context. For achieving coherent answers sentence with the question's\nvocabulary, template-based verbalization so are usually employed for a better\nrepresentation of answers, which in turn require extensive expert intervention.\nThus, making way for machine learning approaches; however, there is a scarcity\nof datasets that empower machine learning models in this area. Hence, we\nprovide the VANiLLa dataset which aims at reducing this gap by offering answers\nin natural language sentences. The answer sentences in this dataset are\nsyntactically and semantically closer to the question than to the triple fact.\nOur dataset consists of over 100k simple questions adapted from the CSQA and\nSimpleQuestionsWikidata datasets and generated using a semi-automatic\nframework. We also present results of training our dataset on multiple baseline\nmodels adapted from current state-of-the-art Natural Language Generation (NLG)\narchitectures. We believe that this dataset will allow researchers to focus on\nfinding suitable methodologies and architectures for answer verbalization.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:57:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Biswas", "Debanjali", ""], ["Dubey", "Mohnish", ""], ["Rony", "Md Rashad Al Hasan", ""], ["Lehmann", "Jens", ""]]}, {"id": "2105.11408", "submitter": "Milan Straka", "authors": "Jakub N\\'aplava, Milan Straka, Jana Strakov\\'a", "title": "Diacritics Restoration using BERT with Analysis on Czech language", "comments": null, "journal-ref": "The Prague Bulletin of Mathematical Linguistics No. 116, 2021, pp.\n  27-42", "doi": "10.14712/00326585.013", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture for diacritics restoration based on\ncontextualized embeddings, namely BERT, and we evaluate it on 12 languages with\ndiacritics. Furthermore, we conduct a detailed error analysis on Czech, a\nmorphologically rich language with a high level of diacritization. Notably, we\nmanually annotate all mispredictions, showing that roughly 44% of them are\nactually not errors, but either plausible variants (19%), or the system\ncorrections of erroneous data (25%). Finally, we categorize the real errors in\ndetail. We release the code at\nhttps://github.com/ufal/bert-diacritics-restoration.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:58:27 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["N\u00e1plava", "Jakub", ""], ["Straka", "Milan", ""], ["Strakov\u00e1", "Jana", ""]]}, {"id": "2105.11412", "submitter": "Kiran Purohit", "authors": "Kiran Purohit, Owais Iqbal and Ankan Mullick", "title": "Reproducibility Report: Contextualizing Hate Speech Classifiers with\n  Post-hoc Explanation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented report evaluates Contextualizing Hate Speech Classifiers with\nPost-hoc Explanation paper within the scope of ML Reproducibility Challenge\n2020. Our work focuses on both aspects constituting the paper: the method\nitself and the validity of the stated results. In the following sections, we\nhave described the paper, related works, algorithmic frameworks, our\nexperiments and evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:02:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Purohit", "Kiran", ""], ["Iqbal", "Owais", ""], ["Mullick", "Ankan", ""]]}, {"id": "2105.11447", "submitter": "Ethan Perez", "authors": "Ethan Perez, Douwe Kiela, Kyunghyun Cho", "title": "True Few-Shot Learning with Language Models", "comments": "Code at https://github.com/ethanjperez/true_few_shot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:55:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Perez", "Ethan", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2105.11519", "submitter": "Ramon Ferrer-i-Cancho", "authors": "David Carrera-Casado and Ramon Ferrer-i-Cancho", "title": "The advent and fall of a vocabulary learning bias from communicative\n  efficiency", "comments": "In press in Biosemiotics; version improved with reviewer comments;\n  typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biosemiosis is a process of choice-making between simultaneously alternative\noptions. It is well-known that, when sufficiently young children encounter a\nnew word, they tend to interpret it as pointing to a meaning that does not have\na word yet in their lexicon rather than to a meaning that already has a word\nattached. In previous research, the strategy was shown to be optimal from an\ninformation theoretic standpoint. In that framework, interpretation is\nhypothesized to be driven by the minimization of a cost function: the option of\nleast communication cost is chosen. However, the information theoretic model\nemployed in that research neither explains the weakening of that vocabulary\nlearning bias in older children or polylinguals nor reproduces Zipf's\nmeaning-frequency law, namely the non-linear relationship between the number of\nmeanings of a word and its frequency. Here we consider a generalization of the\nmodel that is channeled to reproduce that law. The analysis of the new model\nreveals regions of the phase space where the bias disappears consistently with\nthe weakening or loss of the bias in older children or polylinguals. The model\nis abstract enough to support future research on other levels of life that are\nrelevant to biosemiotics. In the deep learning era, the model is a transparent\nlow-dimensional tool for future experimental research and illustrates the\npredictive power of a theoretical framework originally designed to shed light\non the origins of Zipf's rank-frequency law.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 20:13:27 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 14:51:01 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 15:05:03 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Carrera-Casado", "David", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "2105.11589", "submitter": "Karthik Gopalakrishnan", "authors": "Ayush Shrivastava, Karthik Gopalakrishnan, Yang Liu, Robinson\n  Piramuthu, Gokhan T\\\"ur, Devi Parikh, Dilek Hakkani-T\\\"ur", "title": "VISITRON: Visual Semantics-Aligned Interactively Trained\n  Object-Navigator", "comments": "Accepted at NAACL 2021, Visually Grounded Interaction and Language\n  (ViGIL) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive robots navigating photo-realistic environments face challenges\nunderlying vision-and-language navigation (VLN), but in addition, they need to\nbe trained to handle the dynamic nature of dialogue. However, research in\nCooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts\nwith a guide in natural language in order to reach a goal, treats the dialogue\nhistory as a VLN-style static instruction. In this paper, we present VISITRON,\na navigator better suited to the interactive regime inherent to CVDN by being\ntrained to: i) identify and associate object-level concepts and semantics\nbetween the environment and dialogue history, ii) identify when to interact vs.\nnavigate via imitation learning of a binary classification head. We perform\nextensive ablations with VISITRON to gain empirical insights and improve\nperformance on CVDN. VISITRON is competitive with models on the static CVDN\nleaderboard. We also propose a generalized interactive regime to fine-tune and\nevaluate VISITRON and future such models with pre-trained guides for\nadaptability.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:21:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shrivastava", "Ayush", ""], ["Gopalakrishnan", "Karthik", ""], ["Liu", "Yang", ""], ["Piramuthu", "Robinson", ""], ["T\u00fcr", "Gokhan", ""], ["Parikh", "Devi", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "2105.11601", "submitter": "Lei Li", "authors": "Lei Li, Yongfeng Zhang, Li Chen", "title": "Personalized Transformer for Explainable Recommendation", "comments": "Published as a conference paper at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalization of natural language generation plays a vital role in a large\nspectrum of tasks, such as explainable recommendation, review summarization and\ndialog systems. In these tasks, user and item IDs are important identifiers for\npersonalization. Transformer, which is demonstrated with strong language\nmodeling capability, however, is not personalized and fails to make use of the\nuser and item IDs since the ID tokens are not even in the same semantic space\nas the words. To address this problem, we present a PErsonalized Transformer\nfor Explainable Recommendation (PETER), on which we design a simple and\neffective learning objective that utilizes the IDs to predict the words in the\ntarget explanation, so as to endow the IDs with linguistic meanings and to\nachieve personalized Transformer. Besides generating explanations, PETER can\nalso make recommendations, which makes it a unified model for the whole\nrecommendation-explanation pipeline. Extensive experiments show that our small\nunpretrained model outperforms fine-tuned BERT on the generation task, in terms\nof both effectiveness and efficiency, which highlights the importance and the\nnice utility of our design.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:42:47 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 01:19:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Lei", ""], ["Zhang", "Yongfeng", ""], ["Chen", "Li", ""]]}, {"id": "2105.11618", "submitter": "Deming Ye", "authors": "Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun", "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference", "comments": "Accepted by NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing pre-trained language models (PLMs) are often computationally\nexpensive in inference, making them impractical in various resource-limited\nreal-world applications. To address this issue, we propose a dynamic token\nreduction approach to accelerate PLMs' inference, named TR-BERT, which could\nflexibly adapt the layer number of each token in inference to avoid redundant\ncalculation. Specially, TR-BERT formulates the token reduction process as a\nmulti-step token selection problem and automatically learns the selection\nstrategy via reinforcement learning. The experimental results on several\ndownstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to\nsatisfy various performance demands. Moreover, TR-BERT can also achieve better\nperformance with less computation in a suite of long-text tasks since its\ntoken-level layer number adaption greatly accelerates the self-attention\noperation in PLMs. The source code and experiment details of this paper can be\nobtained from https://github.com/thunlp/TR-BERT.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:28:51 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ye", "Deming", ""], ["Lin", "Yankai", ""], ["Huang", "Yufei", ""], ["Sun", "Maosong", ""]]}, {"id": "2105.11644", "submitter": "Gaole He", "authors": "Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao and\n  Ji-Rong Wen", "title": "A Survey on Complex Knowledge Base Question Answering: Methods,\n  Challenges and Solutions", "comments": "Accepted by IJCAI 2021 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Recently, a large number of studies focus on semantically\nor syntactically complicated questions. In this paper, we elaborately summarize\nthe typical challenges and solutions for complex KBQA. We begin with\nintroducing the background about the KBQA task. Next, we present the two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. We then review the advanced methods comprehensively from the\nperspective of the two categories. Specifically, we explicate their solutions\nto the typical challenges. Finally, we conclude and discuss some promising\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 03:45:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lan", "Yunshi", ""], ["He", "Gaole", ""], ["Jiang", "Jinhao", ""], ["Jiang", "Jing", ""], ["Zhao", "Wayne Xin", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2105.11672", "submitter": "Weihong Lin", "authors": "Weihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong, Kai Hu, Qin Ren,\n  Qiang Huo", "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for\n  Key Information Extraction from Documents", "comments": "To be published at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:12:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Weihong", ""], ["Gao", "Qifang", ""], ["Sun", "Lei", ""], ["Zhong", "Zhuoyao", ""], ["Hu", "Kai", ""], ["Ren", "Qin", ""], ["Huo", "Qiang", ""]]}, {"id": "2105.11696", "submitter": "Tatsuya Ide", "authors": "Tatsuya Ide and Daisuke Kawahara", "title": "Multi-Task Learning of Generation and Classification for Emotion-Aware\n  Dialogue Response Generation", "comments": "NAACL Student Research Workshop (SRW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a computer to naturally interact with a human, it needs to be human-like.\nIn this paper, we propose a neural response generation model with multi-task\nlearning of generation and classification, focusing on emotion. Our model based\non BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model,\nis trained to generate responses and recognize emotions simultaneously.\nFurthermore, we weight the losses for the tasks to control the update of\nparameters. Automatic evaluations and crowdsourced manual evaluations show that\nthe proposed model makes generated responses more emotionally aware.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:41:20 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ide", "Tatsuya", ""], ["Kawahara", "Daisuke", ""]]}, {"id": "2105.11698", "submitter": "Yi Cheng", "authors": "Yi Cheng, Siyao Li, Bang Liu, Ruihui Zhao, Sujian Li, Chenghua Lin and\n  Yefeng Zheng", "title": "Guiding the Growth: Difficulty-Controllable Question Generation through\n  Step-by-Step Rewriting", "comments": "Accepted by ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the task of Difficulty-Controllable Question Generation\n(DCQG), which aims at generating questions with required difficulty levels.\nPrevious research on this task mainly defines the difficulty of a question as\nwhether it can be correctly answered by a Question Answering (QA) system,\nlacking interpretability and controllability. In our work, we redefine question\ndifficulty as the number of inference steps required to answer it and argue\nthat Question Generation (QG) systems should have stronger control over the\nlogic of generated questions. To this end, we propose a novel framework that\nprogressively increases question difficulty through step-by-step rewriting\nunder the guidance of an extracted reasoning chain. A dataset is automatically\nconstructed to facilitate the research, on which extensive experiments are\nconducted to test the performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 06:43:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cheng", "Yi", ""], ["Li", "Siyao", ""], ["Liu", "Bang", ""], ["Zhao", "Ruihui", ""], ["Li", "Sujian", ""], ["Lin", "Chenghua", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2105.11741", "submitter": "Yuanmeng Yan", "authors": "Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu and Weiran\n  Xu", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence\n  Representation Transfer", "comments": "Accepted by ACL2021, 10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning high-quality sentence representations benefits a wide range of\nnatural language processing tasks. Though BERT-based pre-trained language\nmodels achieve high performance on many downstream tasks, the native derived\nsentence representations are proved to be collapsed and thus produce a poor\nperformance on the semantic textual similarity (STS) tasks. In this paper, we\npresent ConSERT, a Contrastive Framework for Self-Supervised Sentence\nRepresentation Transfer, that adopts contrastive learning to fine-tune BERT in\nan unsupervised and effective way. By making use of unlabeled texts, ConSERT\nsolves the collapse issue of BERT-derived sentence representations and make\nthem more applicable for downstream tasks. Experiments on STS datasets\ndemonstrate that ConSERT achieves an 8\\% relative improvement over the previous\nstate-of-the-art, even comparable to the supervised SBERT-NLI. And when further\nincorporating NLI supervision, we achieve new state-of-the-art performance on\nSTS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples\navailable, showing its robustness in data scarcity scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:15:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yan", "Yuanmeng", ""], ["Li", "Rumei", ""], ["Wang", "Sirui", ""], ["Zhang", "Fuzheng", ""], ["Wu", "Wei", ""], ["Xu", "Weiran", ""]]}, {"id": "2105.11752", "submitter": "Milad Alshomary", "authors": "Milad Alshomary, Shahbaz Syed, Arkajit Dhar, Martin Potthast and\n  Henning Wachsmuth", "title": "Argument Undermining: Counter-Argument Generation by Attacking Weak\n  Premises", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text generation has received a lot of attention in computational\nargumentation research as of recent. A particularly challenging task is the\ngeneration of counter-arguments. So far, approaches primarily focus on\nrebutting a given conclusion, yet other ways to counter an argument exist. In\nthis work, we go beyond previous research by exploring argument undermining,\nthat is, countering an argument by attacking one of its premises. We\nhypothesize that identifying the argument's weak premises is key to effective\ncountering. Accordingly, we propose a pipeline approach that first assesses the\npremises' strength and then generates a counter-argument targeting the weak\nones. On the one hand, both manual and automatic evaluation proves the\nimportance of identifying weak premises in counter-argument generation. On the\nother hand, when considering correctness and content richness, human annotators\nfavored our approach over state-of-the-art counter-argument generation.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:39:14 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 08:14:09 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 08:12:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Alshomary", "Milad", ""], ["Syed", "Shahbaz", ""], ["Dhar", "Arkajit", ""], ["Potthast", "Martin", ""], ["Wachsmuth", "Henning", ""]]}, {"id": "2105.11776", "submitter": "Weiwen Xu", "authors": "Weiwen Xu, Huihui Zhang, Deng Cai and Wai Lam", "title": "Dynamic Semantic Graph Construction and Reasoning for Explainable\n  Multi-hop Science Question Answering", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge retrieval and reasoning are two key stages in multi-hop question\nanswering (QA) at web scale. Existing approaches suffer from low confidence\nwhen retrieving evidence facts to fill the knowledge gap and lack transparent\nreasoning process. In this paper, we propose a new framework to exploit more\nvalid facts while obtaining explainability for multi-hop QA by dynamically\nconstructing a semantic graph and reasoning over it. We employ Abstract Meaning\nRepresentation (AMR) as semantic graph representation. Our framework contains\nthree new ideas: (a) {\\tt AMR-SG}, an AMR-based Semantic Graph, constructed by\ncandidate fact AMRs to uncover any hop relations among question, answer and\nmultiple facts. (b) A novel path-based fact analytics approach exploiting {\\tt\nAMR-SG} to extract active facts from a large fact pool to answer questions. (c)\nA fact-level relation modeling leveraging graph convolution network (GCN) to\nguide the reasoning process. Results on two scientific multi-hop QA datasets\nshow that we can surpass recent approaches including those using additional\nknowledge graphs while maintaining high explainability on OpenBookQA and\nachieve a new state-of-the-art result on ARC-Challenge in a computationally\npracticable setting.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:14:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Xu", "Weiwen", ""], ["Zhang", "Huihui", ""], ["Cai", "Deng", ""], ["Lam", "Wai", ""]]}, {"id": "2105.11780", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon, G. Scettri", "title": "Look inside. Predicting stock prices by analysing an enterprise intranet\n  social network and using word co-occurrence networks", "comments": null, "journal-ref": "International Journal of Entrepreneurship and Small Business\n  36(4), 378-391 (2019)", "doi": "10.1504/IJESB.2019.098986", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study looks into employees' communication, offering novel metrics which\ncan help to predict a company's stock price. We studied the intranet forum of a\nlarge Italian company, exploring the interactions and the use of language of\nabout 8,000 employees. We built a network linking words included in the general\ndiscourse. In this network, we focused on the position of the node representing\nthe company brand. We found that a lower sentiment, a higher betweenness\ncentrality of the company brand, a denser word co-occurrence network and more\nequally distributed centrality scores of employees (lower group betweenness\ncentrality) are all significant predictors of higher stock prices. Our findings\noffers new metrics that can be helpful for scholars, company managers and\nprofessional investors and could be integrated into existing forecasting models\nto improve their accuracy. Lastly, we contribute to the research on word\nco-occurrence networks by extending their field of application.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:17:22 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Scettri", "G.", ""]]}, {"id": "2105.11798", "submitter": "Carlos Basto", "authors": "Carlos Basto", "title": "Extending the Abstraction of Personality Types based on MBTI with\n  Machine Learning and Natural Language Processing", "comments": "23 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A data-centric approach with Natural Language Processing (NLP) to predict\npersonality types based on the MBTI (an introspective self-assessment\nquestionnaire that indicates different psychological preferences about how\npeople perceive the world and make decisions) through systematic enrichment of\ntext representation, based on the domain of the area, under the generation of\nfeatures based on three types of analysis: sentimental, grammatical and\naspects. The experimentation had a robust baseline of stacked models, with\npremature optimization of hyperparameters through grid search, with gradual\nfeedback, for each of the four classifiers (dichotomies) of MBTI. The results\nshowed that attention to the data iteration loop focused on quality,\nexplanatory power and representativeness for the abstraction of more\nrelevant/important resources for the studied phenomenon made it possible to\nimprove the evaluation metrics results more quickly and less costly than\ncomplex models such as the LSTM or state of the art ones as BERT, as well as\nthe importance of these results by comparisons made from various perspectives.\nIn addition, the study demonstrated a broad spectrum for the evolution and\ndeepening of the task and possible approaches for a greater extension of the\nabstraction of personality types.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:00:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Basto", "Carlos", ""]]}, {"id": "2105.11832", "submitter": "Thomas Searle", "authors": "Thomas Searle, Zina Ibrahim, James Teo, Richard JB Dobson", "title": "Estimating Redundancy in Clinical Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current mode of use of Electronic Health Record (EHR) elicits text\nredundancy. Clinicians often populate new documents by duplicating existing\nnotes, then updating accordingly. Data duplication can lead to a propagation of\nerrors, inconsistencies and misreporting of care. Therefore, quantifying\ninformation redundancy can play an essential role in evaluating innovations\nthat operate on clinical narratives.\n  This work is a quantitative examination of information redundancy in EHR\nnotes. We present and evaluate two strategies to measure redundancy: an\ninformation-theoretic approach and a lexicosyntactic and semantic model. We\nevaluate the measures by training large Transformer-based language models using\nclinical text from a large openly available US-based ICU dataset and a large\nmulti-site UK based Trust. By comparing the information-theoretic content of\nthe trained models with open-domain language models, the language models\ntrained using clinical text have shown ~1.5x to ~3x less efficient than\nopen-domain corpora. Manual evaluation shows a high correlation with\nlexicosyntactic and semantic redundancy, with averages ~43 to ~65%.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:01:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Searle", "Thomas", ""], ["Ibrahim", "Zina", ""], ["Teo", "James", ""], ["Dobson", "Richard JB", ""]]}, {"id": "2105.11872", "submitter": "Marcin Namysl", "authors": "Marcin Namysl, Sven Behnke, Joachim K\\\"ohler", "title": "Empirical Error Modeling Improves Robustness of Noisy Neural Sequence\n  Labeling", "comments": "Accepted to appear in Findings of ACL 2021 (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:15:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Namysl", "Marcin", ""], ["Behnke", "Sven", ""], ["K\u00f6hler", "Joachim", ""]]}, {"id": "2105.11902", "submitter": "Yong Dai", "authors": "Yong Dai and Jian Liu and Jian Zhang and Hongguang Fu and Zenglin Xu", "title": "Unsupervised Sentiment Analysis by Transferring Multi-source Knowledge", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (SA) is an important research area in cognitive\ncomputation-thus in-depth studies of patterns of sentiment analysis are\nnecessary. At present, rich resource data-based SA has been well developed,\nwhile the more challenging and practical multi-source unsupervised SA (i.e. a\ntarget domain SA by transferring from multiple source domains) is seldom\nstudied. The challenges behind this problem mainly locate in the lack of\nsupervision information, the semantic gaps among domains (i.e., domain shifts),\nand the loss of knowledge. However, existing methods either lack the\ndistinguishable capacity of the semantic gaps among domains or lose private\nknowledge. To alleviate these problems, we propose a two-stage domain\nadaptation framework. In the first stage, a multi-task methodology-based\nshared-private architecture is employed to explicitly model the domain common\nfeatures and the domain-specific features for the labeled source domains. In\nthe second stage, two elaborate mechanisms are embedded in the shared private\narchitecture to transfer knowledge from multiple source domains. The first\nmechanism is a selective domain adaptation (SDA) method, which transfers\nknowledge from the closest source domain. And the second mechanism is a\ntarget-oriented ensemble (TOE) method, in which knowledge is transferred\nthrough a well-designed ensemble method. Extensive experiment evaluations\nverify that the performance of the proposed framework outperforms unsupervised\nstate-of-the-art competitors. What can be concluded from the experiments is\nthat transferring from very different distributed source domains may degrade\nthe target-domain performance, and it is crucial to choose the proper source\ndomains to transfer from.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 03:02:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Dai", "Yong", ""], ["Liu", "Jian", ""], ["Zhang", "Jian", ""], ["Fu", "Hongguang", ""], ["Xu", "Zenglin", ""]]}, {"id": "2105.11903", "submitter": "Yanran Li", "authors": "Yanran Li and Ke Li and Hongke Ning and xiaoqiang Xia and Yalong Guo\n  and Chen Wei and Jianwei Cui and Bin Wang", "title": "Towards an Online Empathetic Chatbot with Emotion Causes", "comments": "SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463042", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing emotion-aware conversational models usually focus on controlling the\nresponse contents to align with a specific emotion class, whereas empathy is\nthe ability to understand and concern the feelings and experience of others.\nHence, it is critical to learn the causes that evoke the users' emotion for\nempathetic responding, a.k.a. emotion causes. To gather emotion causes in\nonline environments, we leverage counseling strategies and develop an\nempathetic chatbot to utilize the causal emotion information. On a real-world\nonline dataset, we verify the effectiveness of the proposed approach by\ncomparing our chatbot with several SOTA methods using automatic metrics,\nexpert-based human judgements as well as user-based online evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 02:52:46 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Yanran", ""], ["Li", "Ke", ""], ["Ning", "Hongke", ""], ["Xia", "xiaoqiang", ""], ["Guo", "Yalong", ""], ["Wei", "Chen", ""], ["Cui", "Jianwei", ""], ["Wang", "Bin", ""]]}, {"id": "2105.11904", "submitter": "Yongbin Liu", "authors": "Qing Lin, Yongbin Liu, Wen Wen, Zhihua Tao", "title": "Ensemble Making Few-Shot Learning Stronger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning has been proposed and rapidly emerging as a viable means\nfor completing various tasks. Many few-shot models have been widely used for\nrelation learning tasks. However, each of these models has a shortage of\ncapturing a certain aspect of semantic features, for example, CNN on long-range\ndependencies part, Transformer on local features. It is difficult for a single\nmodel to adapt to various relation learning, which results in the high variance\nproblem. Ensemble strategy could be competitive on improving the accuracy of\nfew-shot relation extraction and mitigating high variance risks. This paper\nexplores an ensemble approach to reduce the variance and introduces fine-tuning\nand feature attention strategies to calibrate relation-level features. Results\non several few-shot relation learning tasks show that our model significantly\noutperforms the previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:11:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Qing", ""], ["Liu", "Yongbin", ""], ["Wen", "Wen", ""], ["Tao", "Zhihua", ""]]}, {"id": "2105.11905", "submitter": "Jindong Wang", "authors": "Wenxin Hou, Han Zhu, Yidong Wang, Jindong Wang, Tao Qin, Renjun Xu,\n  Takahiro Shinozaki", "title": "Exploiting Adapters for Cross-lingual Low-resource Speech Recognition", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-lingual speech adaptation aims to solve the problem of leveraging\nmultiple rich-resource languages to build models for a low-resource target\nlanguage. Since the low-resource language has limited training data, speech\nrecognition models can easily overfit. In this paper, we propose to use\nadapters to investigate the performance of multiple adapters for\nparameter-efficient cross-lingual speech adaptation. Based on our previous\nMetaAdapter that implicitly leverages adapters, we propose a novel algorithms\ncalled SimAdapter for explicitly learning knowledge from adapters. Our\nalgorithm leverages adapters which can be easily integrated into the\nTransformer structure.MetaAdapter leverages meta-learning to transfer the\ngeneral knowledge from training data to the test language. SimAdapter aims to\nlearn the similarities between the source and target languages during\nfine-tuning using the adapters. We conduct extensive experiments on\nfive-low-resource languages in Common Voice dataset. Results demonstrate that\nour MetaAdapter and SimAdapter methods can reduce WER by 2.98% and 2.55% with\nonly 2.5% and 15.5% of trainable parameters compared to the strong full-model\nfine-tuning baseline. Moreover, we also show that these two novel algorithms\ncan be integrated for better performance with up to 3.55% relative WER\nreduction.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:30:37 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hou", "Wenxin", ""], ["Zhu", "Han", ""], ["Wang", "Yidong", ""], ["Wang", "Jindong", ""], ["Qin", "Tao", ""], ["Xu", "Renjun", ""], ["Shinozaki", "Takahiro", ""]]}, {"id": "2105.11908", "submitter": "Ansgar Scherp", "authors": "M. Lautaro Hickmann and Fabian Wurzberger and Megi Hoxhalli and Arne\n  Lochner and Jessica T\\\"ollich and Ansgar Scherp", "title": "Analysis of GraphSum's Attention Weights to Improve the Explainability\n  of Multi-Document Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multi-document summarization (MDS) methods are based on transformer\narchitectures. They generate state of the art summaries, but lack\nexplainability. We focus on graph-based transformer models for MDS as they\ngained recent popularity. We aim to improve the explainability of the\ngraph-based MDS by analyzing their attention weights. In a graph-based MDS such\nas GraphSum, vertices represent the textual units, while the edges form some\nsimilarity graph over the units. We compare GraphSum's performance utilizing\ndifferent textual units, i. e., sentences versus paragraphs, on two news\nbenchmark datasets, namely WikiSum and MultiNews. Our experiments show that\nparagraph-level representations provide the best summarization performance.\nThus, we subsequently focus oAnalysisn analyzing the paragraph-level attention\nweights of GraphSum's multi-heads and decoding layers in order to improve the\nexplainability of a transformer-based MDS model. As a reference metric, we\ncalculate the ROUGE scores between the input paragraphs and each sentence in\nthe generated summary, which indicate source origin information via text\nsimilarity. We observe a high correlation between the attention weights and\nthis reference metric, especially on the the later decoding layers of the\ntransformer architecture. Finally, we investigate if the generated summaries\nfollow a pattern of positional bias by extracting which paragraph provided the\nmost information for each generated summary. Our results show that there is a\nhigh correlation between the position in the summary and the source origin.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:18:59 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hickmann", "M. Lautaro", ""], ["Wurzberger", "Fabian", ""], ["Hoxhalli", "Megi", ""], ["Lochner", "Arne", ""], ["T\u00f6llich", "Jessica", ""], ["Scherp", "Ansgar", ""]]}, {"id": "2105.11910", "submitter": "Timo Spinde", "authors": "T. Spinde, L. Rudnitckaia, K. Sinha, F. Hamborg, B. Gipp, K. Donnay", "title": "MBIC -- A Media Bias Annotation Dataset Including Annotator\n  Characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people consider news articles to be a reliable source of information on\ncurrent events. However, due to the range of factors influencing news agencies,\nsuch coverage may not always be impartial. Media bias, or slanted news\ncoverage, can have a substantial impact on public perception of events, and,\naccordingly, can potentially alter the beliefs and views of the public. The\nmain data gap in current research on media bias detection is a robust,\nrepresentative, and diverse dataset containing annotations of biased words and\nsentences. In particular, existing datasets do not control for the individual\nbackground of annotators, which may affect their assessment and, thus,\nrepresents critical information for contextualizing their annotations. In this\nposter, we present a matrix-based methodology to crowdsource such data using a\nself-developed annotation platform. We also present MBIC (Media Bias Including\nCharacteristics) - the first sample of 1,700 statements representing various\nmedia bias instances. The statements were reviewed by ten annotators each and\ncontain labels for media bias identification both on the word and sentence\nlevel. MBIC is the first available dataset about media bias reporting detailed\ninformation on annotator characteristics and their individual background. The\ncurrent dataset already significantly extends existing data in this domain\nproviding unique and more reliable insights into the perception of bias. In\nfuture, we will further extend it both with respect to the number of articles\nand annotators per article.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:05:17 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Spinde", "T.", ""], ["Rudnitckaia", "L.", ""], ["Sinha", "K.", ""], ["Hamborg", "F.", ""], ["Gipp", "B.", ""], ["Donnay", "K.", ""]]}, {"id": "2105.11921", "submitter": "Rahul Aralikatte", "authors": "Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, Ryan\n  McDonald", "title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional summaries are written with document-level information, such as\nthe theme of the document, in mind. This is in contrast with most seq2seq\ndecoders which simultaneously learn to focus on salient content, while deciding\nwhat to generate, at each decoding step. With the motivation to narrow this\ngap, we introduce Focus Attention Mechanism, a simple yet effective method to\nencourage decoders to proactively generate tokens that are similar or topical\nto the input document. Further, we propose a Focus Sampling method to enable\ngeneration of diverse summaries, an area currently understudied in\nsummarization. When evaluated on the BBC extreme summarization task, two\nstate-of-the-art models augmented with Focus Attention generate summaries that\nare closer to the target and more faithful to their input documents,\noutperforming their vanilla counterparts on \\rouge and multiple faithfulness\nmeasures. We also empirically demonstrate that Focus Sampling is more effective\nin generating diverse and faithful summaries than top-$k$ or nucleus\nsampling-based decoding methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:25:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Aralikatte", "Rahul", ""], ["Narayan", "Shashi", ""], ["Maynez", "Joshua", ""], ["Rothe", "Sascha", ""], ["McDonald", "Ryan", ""]]}, {"id": "2105.11950", "submitter": "Theodore Sumers", "authors": "Theodore R. Sumers, Robert D. Hawkins, Mark K. Ho, Thomas L. Griffiths", "title": "Extending rational models of communication from beliefs to actions", "comments": "7 pages, 4 figures. Proceedings for the 43rd Annual Meeting of the\n  Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speakers communicate to influence their partner's beliefs and shape their\nactions. Belief- and action-based objectives have been explored independently\nin recent computational models, but it has been challenging to explicitly\ncompare or integrate them. Indeed, we find that they are conflated in standard\nreferential communication tasks. To distinguish these accounts, we introduce a\nnew paradigm called signaling bandits, generalizing classic Lewis signaling\ngames to a multi-armed bandit setting where all targets in the context have\nsome relative value. We develop three speaker models: a belief-oriented speaker\nwith a purely informative objective; an action-oriented speaker with an\ninstrumental objective; and a combined speaker which integrates the two by\ninducing listener beliefs that generally lead to desirable actions. We then\npresent a series of simulations demonstrating that grounding production choices\nin future listener actions results in relevance effects and flexible uses of\nnonliteral language. More broadly, our findings suggest that language games\nbased on richer decision problems are a promising avenue for insight into\nrational communication.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:58:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sumers", "Theodore R.", ""], ["Hawkins", "Robert D.", ""], ["Ho", "Mark K.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "2105.12002", "submitter": "Chen Liang", "authors": "Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu,\n  Pengcheng He, Tuo Zhao and Weizhu Chen", "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to\n  Improving Generalization", "comments": "The 59th annual meeting of the Association for Computational\n  Linguistics (ACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lottery Ticket Hypothesis suggests that an over-parametrized network\nconsists of ``lottery tickets'', and training a certain collection of them\n(i.e., a subnetwork) can match the performance of the full model. In this\npaper, we study such a collection of tickets, which is referred to as ``winning\ntickets'', in extremely over-parametrized models, e.g., pre-trained language\nmodels. We observe that at certain compression ratios, the generalization\nperformance of the winning tickets can not only match but also exceed that of\nthe full model. In particular, we observe a phase transition phenomenon: As the\ncompression ratio increases, generalization performance of the winning tickets\nfirst improves then deteriorates after a certain threshold. We refer to the\ntickets on the threshold as ``super tickets''. We further show that the phase\ntransition is task and model dependent -- as the model size becomes larger and\nthe training data set becomes smaller, the transition becomes more pronounced.\nOur experiments on the GLUE benchmark show that the super tickets improve\nsingle task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on\nBERT-large, in terms of task-average score. We also demonstrate that adaptively\nsharing the super tickets across tasks benefits multi-task learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:10:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:58:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liang", "Chen", ""], ["Zuo", "Simiao", ""], ["Chen", "Minshuo", ""], ["Jiang", "Haoming", ""], ["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Zhao", "Tuo", ""], ["Chen", "Weizhu", ""]]}, {"id": "2105.12006", "submitter": "Kelly Gothard", "authors": "Kelly Gothard, David Rushing Dewhurst, Joshua R. Minot, Jane Lydia\n  Adams, Christopher M. Danforth, Peter Sheridan Dodds", "title": "The incel lexicon: Deciphering the emergent cryptolect of a global\n  misogynistic community", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolving out of a gender-neutral framing of an involuntary celibate identity,\nthe concept of `incels' has come to refer to an online community of men who\nbear antipathy towards themselves, women, and society-at-large for their\nperceived inability to find and maintain sexual relationships. By exploring\nincel language use on Reddit, a global online message board, we contextualize\nthe incel community's online expressions of misogyny and real-world acts of\nviolence perpetrated against women. After assembling around three million\ncomments from incel-themed Reddit channels, we analyze the temporal dynamics of\na data driven rank ordering of the glossary of phrases belonging to an emergent\nincel lexicon. Our study reveals the generation and normalization of an\nextensive coded misogynist vocabulary in service of the group's identity.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:20:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gothard", "Kelly", ""], ["Dewhurst", "David Rushing", ""], ["Minot", "Joshua R.", ""], ["Adams", "Jane Lydia", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "2105.12041", "submitter": "Wenhao Wu", "authors": "Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li,\n  Hua Wu, Haifeng Wang", "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph", "comments": "Accepted by ACL-IJCNLP 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstractive summarization for long-document or multi-document remains\nchallenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing\nlong-distance relations in text. In this paper, we present BASS, a novel\nframework for Boosting Abstractive Summarization based on a unified Semantic\ngraph, which aggregates co-referent phrases distributing across a long range of\ncontext and conveys rich relations between phrases. Further, a graph-based\nencoder-decoder model is proposed to improve both the document representation\nand summary generation process by leveraging the graph structure. Specifically,\nseveral graph augmentation methods are designed to encode both the explicit and\nimplicit relations in the text while the graph-propagation attention mechanism\nis developed in the decoder to select salient content into the summary.\nEmpirical results show that the proposed architecture brings substantial\nimprovements for both long-document and multi-document summarization tasks.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:20:48 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wu", "Wenhao", ""], ["Li", "Wei", ""], ["Xiao", "Xinyan", ""], ["Liu", "Jiachen", ""], ["Cao", "Ziqiang", ""], ["Li", "Sujian", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2105.12048", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "M. A. Barchiesi, A. Fronzetti Colladon", "title": "Big data and big values: When companies need to rethink themselves", "comments": null, "journal-ref": "Journal of Business Research 129, 714-722 (2021)", "doi": "10.1016/j.jbusres.2019.10.046", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In order to face the complexity of business environments and detect\npriorities while triggering contingency strategies, we propose a new\nmethodological approach that combines text mining, social network and big data\nanalytics, with the assessment of stakeholders' attitudes towards company core\nvalues. This approach was applied in a case study where we considered the\nTwitter discourse about core values in Italy. We collected more than 94,000\ntweets related to the core values of the firms listed in Fortune's ranking of\nthe World's Most Admired Companies (2013-2017). For the Italian scenario, we\nfound three predominant core values orientations (Customers, Employees and\nExcellence) - which should be at the basis of any business strategy - and three\nlatent ones (Economic-Financial Growth, Citizenship and Social Responsibility),\nwhich need periodic attention. Our contribution is mostly methodological and\nextends the research on text mining and on online big data analytics applied in\ncomplex business contexts.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:26:38 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Barchiesi", "M. A.", ""], ["Colladon", "A. Fronzetti", ""]]}, {"id": "2105.12051", "submitter": "Zhixiang Chen", "authors": "Zhixiang Chen, Yikun Lei, Pai Liu, Guibing Guo", "title": "NEUer at SemEval-2021 Task 4: Complete Summary Representation by Filling\n  Answers into Question for Matching Reading Comprehension", "comments": "accepted by SemEval2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SemEval task 4 aims to find a proper option from multiple candidates to\nresolve the task of machine reading comprehension. Most existing approaches\npropose to concat question and option together to form a context-aware model.\nHowever, we argue that straightforward concatenation can only provide a\ncoarse-grained context for the MRC task, ignoring the specific positions of the\noption relative to the question. In this paper, we propose a novel MRC model by\nfilling options into the question to produce a fine-grained context (defined as\nsummary) which can better reveal the relationship between option and question.\nWe conduct a series of experiments on the given dataset, and the results show\nthat our approach outperforms other counterparts to a large extent.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:31:26 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Chen", "Zhixiang", ""], ["Lei", "Yikun", ""], ["Liu", "Pai", ""], ["Guo", "Guibing", ""]]}, {"id": "2105.12068", "submitter": "Tedo Vrbanec", "authors": "Tedo Vrbanec and Ana Mestrovic", "title": "Taxonomy of academic plagiarism methods", "comments": "18 pages, 3 figures, 1 table", "journal-ref": "Journal of the Polytechnic of Rijeka, 2021, Volume 9, Issue 1, pp.\n  283-300", "doi": "10.31784/zvr.9.1.17", "report-no": "This work has been supported in part by the University of Rijeka\n  under the project \"uniri-drustv-18-38\"", "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article gives an overview of the plagiarism domain, with focus on\nacademic plagiarism. The article defines plagiarism, explains the origin of the\nterm, as well as plagiarism related terms. It identifies the extent of the\nplagiarism domain and then focuses on the plagiarism subdomain of text\ndocuments, for which it gives an overview of current classifications and\ntaxonomies and then proposes a more comprehensive classification according to\nseveral criteria: their origin and purpose, technical implementation,\nconsequence, complexity of detection and according to the number of linguistic\nsources. The article suggests the new classification of academic plagiarism,\ndescribes sorts and methods of plagiarism, types and categories, approaches and\nphases of plagiarism detection, the classification of methods and algorithms\nfor plagiarism detection. The title of the article explicitly targets the\nacademic community, but it is sufficiently general and interdisciplinary, so it\ncan be useful for many other professionals like software developers, linguists\nand librarians.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:49:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Vrbanec", "Tedo", ""], ["Mestrovic", "Ana", ""]]}, {"id": "2105.12172", "submitter": "Dongjun Lee", "authors": "Dongjun Lee, Junhyeong Ahn, Heesoo Park, Jaemin Jo", "title": "IntelliCAT: Intelligent Machine Translation Post-Editing with Quality\n  Estimation and Translation Suggestion", "comments": "ACL 2021 (system demonstration)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present IntelliCAT, an interactive translation interface with neural\nmodels that streamline the post-editing process on machine translation output.\nWe leverage two quality estimation (QE) models at different granularities:\nsentence-level QE, to predict the quality of each machine-translated sentence,\nand word-level QE, to locate the parts of the machine-translated sentence that\nneed correction. Additionally, we introduce a novel translation suggestion\nmodel conditioned on both the left and right contexts, providing alternatives\nfor specific words or phrases for correction. Finally, with word alignments,\nIntelliCAT automatically preserves the original document's styles in the\ntranslated document. The experimental results show that post-editing based on\nthe proposed QE and translation suggestions can significantly improve\ntranslation quality. Furthermore, a user study reveals that three features\nprovided in IntelliCAT significantly accelerate the post-editing task,\nachieving a 52.9\\% speedup in translation time compared to translating from\nscratch. The interface is publicly available at\nhttps://intellicat.beringlab.com/.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 19:00:22 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Lee", "Dongjun", ""], ["Ahn", "Junhyeong", ""], ["Park", "Heesoo", ""], ["Jo", "Jaemin", ""]]}, {"id": "2105.12192", "submitter": "Lee Burke", "authors": "Lee Burke, Karl Pazdernik, Daniel Fortin, Benjamin Wilson, Rustam\n  Goychayev, and John Mattingly", "title": "NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and\n  Energy Domains", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) tasks (text classification, named entity\nrecognition, etc.) have seen revolutionary improvements over the last few\nyears. This is due to language models such as BERT that achieve deep knowledge\ntransfer by using a large pre-trained model, then fine-tuning the model on\nspecific tasks. The BERT architecture has shown even better performance on\ndomain-specific tasks when the model is pre-trained using domain-relevant\ntexts. Inspired by these recent advancements, we have developed NukeLM, a\nnuclear-domain language model pre-trained on 1.5 million abstracts from the\nU.S. Department of Energy Office of Scientific and Technical Information (OSTI)\ndatabase. This NukeLM model is then fine-tuned for the classification of\nresearch articles into either binary classes (related to the nuclear fuel cycle\n[NFC] or not) or multiple categories related to the subject of the article. We\nshow that continued pre-training of a BERT-style architecture prior to\nfine-tuning yields greater performance on both article classification tasks.\nThis information is critical for properly triaging manuscripts, a necessary\ntask for better understanding citation networks that publish in the nuclear\nspace, and for uncovering new areas of research in the nuclear (or\nnuclear-relevant) domains.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:00:59 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Burke", "Lee", ""], ["Pazdernik", "Karl", ""], ["Fortin", "Daniel", ""], ["Wilson", "Benjamin", ""], ["Goychayev", "Rustam", ""], ["Mattingly", "John", ""]]}, {"id": "2105.12202", "submitter": "Andrew Dunn", "authors": "Andrew Dunn, Diana Inkpen, R\\u{a}zvan Andonie", "title": "Context-Sensitive Visualization of Deep Learning Natural Language\n  Processing Models", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The introduction of Transformer neural networks has changed the landscape of\nNatural Language Processing (NLP) during the last years. So far, none of the\nvisualization systems has yet managed to examine all the facets of the\nTransformers. This gave us the motivation of the current work. We propose a new\nNLP Transformer context-sensitive visualization method that leverages existing\nNLP tools to find the most significant groups of tokens (words) that have the\ngreatest effect on the output, thus preserving some context from the original\ntext. First, we use a sentence-level dependency parser to highlight promising\nword groups. The dependency parser creates a tree of relationships between the\nwords in the sentence. Next, we systematically remove adjacent and non-adjacent\ntuples of \\emph{n} tokens from the input text, producing several new texts with\nthose tokens missing. The resulting texts are then passed to a pre-trained BERT\nmodel. The classification output is compared with that of the full text, and\nthe difference in the activation strength is recorded. The modified texts that\nproduce the largest difference in the target classification output neuron are\nselected, and the combination of removed words are then considered to be the\nmost influential on the model's output. Finally, the most influential word\ncombinations are visualized in a heatmap.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:26:38 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Dunn", "Andrew", ""], ["Inkpen", "Diana", ""], ["Andonie", "R\u0103zvan", ""]]}, {"id": "2105.12261", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster, Karin Verspoor, Timothy Baldwin, Jey Han Lau,\n  Antonio Jimeno Yepes, David Martinez, Yulia Otmakhova", "title": "Impact of detecting clinical trial elements in exploration of COVID-19\n  literature", "comments": "Accepted at HealthNLP'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has driven ever-greater demand for tools which enable\nefficient exploration of biomedical literature. Although semi-structured\ninformation resulting from concept recognition and detection of the defining\nelements of clinical trials (e.g. PICO criteria) has been commonly used to\nsupport literature search, the contributions of this abstraction remain poorly\nunderstood, especially in relation to text-based retrieval. In this study, we\ncompare the results retrieved by a standard search engine with those filtered\nusing clinically-relevant concepts and their relations. With analysis based on\nthe annotations from the TREC-COVID shared task, we obtain quantitative as well\nas qualitative insights into characteristics of relational and concept-based\nliterature exploration. Most importantly, we find that the relational concept\nselection filters the original retrieved collection in a way that decreases the\nproportion of unjudged documents and increases the precision, which means that\nthe user is likely to be exposed to a larger number of relevant documents.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:41:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["\u0160uster", "Simon", ""], ["Verspoor", "Karin", ""], ["Baldwin", "Timothy", ""], ["Lau", "Jey Han", ""], ["Yepes", "Antonio Jimeno", ""], ["Martinez", "David", ""], ["Otmakhova", "Yulia", ""]]}, {"id": "2105.12297", "submitter": "Hailong Cao", "authors": "Hailong Cao and Tiejun Zhao", "title": "Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon\n  Induction", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Great progress has been made in unsupervised bilingual lexicon induction\n(UBLI) by aligning the source and target word embeddings independently trained\non monolingual corpora. The common assumption of most UBLI models is that the\nembedding spaces of two languages are approximately isomorphic. Therefore the\nperformance is bound by the degree of isomorphism, especially on etymologically\nand typologically distant languages. To address this problem, we propose a\ntransformation-based method to increase the isomorphism. Embeddings of two\nlanguages are made to match with each other by rotating and scaling. The method\ndoes not require any form of supervision and can be applied to any language\npair. On a benchmark data set of bilingual lexicon induction, our approach can\nachieve competitive or superior performance compared to state-of-the-art\nmethods, with particularly strong results being found on distant languages.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:09:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Cao", "Hailong", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2105.12305", "submitter": "Yong Qian", "authors": "Yong Qian, Zhongqing Wang, Rong Xiao, Chen Chen and Haihong Tang", "title": "SGPT: Semantic Graphs based Pre-training for Aspect-based Sentiment\n  Analysis", "comments": "arXiv admin note: text overlap with arXiv:2005.05635 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous studies show effective of pre-trained language models for sentiment\nanalysis. However, most of these studies ignore the importance of sentimental\ninformation for pre-trained models.Therefore, we fully investigate the\nsentimental information for pre-trained models and enhance pre-trained language\nmodels with semantic graphs for sentiment analysis.In particular, we introduce\nSemantic Graphs based Pre-training(SGPT) using semantic graphs to obtain\nsynonym knowledge for aspect-sentiment pairs and similar aspect/sentiment\nterms.We then optimize the pre-trained language model with the semantic\ngraphs.Empirical studies on several downstream tasks show that proposed model\noutperforms strong pre-trained baselines. The results also show the\neffectiveness of proposed semantic graphs for pre-trained model.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:32:50 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Qian", "Yong", ""], ["Wang", "Zhongqing", ""], ["Xiao", "Rong", ""], ["Chen", "Chen", ""], ["Tang", "Haihong", ""]]}, {"id": "2105.12306", "submitter": "Qingyu Zhou", "authors": "Heng-Da Xu, Zhongli Li, Qingyu Zhou, Chao Li, Zizhen Wang, Yunbo Cao,\n  Heyan Huang and Xian-Ling Mao", "title": "Read, Listen, and See: Leveraging Multimodal Information Helps Chinese\n  Spell Checking", "comments": "In ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chinese Spell Checking (CSC) aims to detect and correct erroneous characters\nfor user-generated text in the Chinese language. Most of the Chinese spelling\nerrors are misused semantically, phonetically or graphically similar\ncharacters. Previous attempts noticed this phenomenon and try to use the\nsimilarity for this task. However, these methods use either heuristics or\nhandcrafted confusion sets to predict the correct character. In this paper, we\npropose a Chinese spell checker called ReaLiSe, by directly leveraging the\nmultimodal information of the Chinese characters. The ReaLiSe model tackles the\nCSC task by (1) capturing the semantic, phonetic and graphic information of the\ninput characters, and (2) selectively mixing the information in these\nmodalities to predict the correct output. Experiments on the SIGHAN benchmarks\nshow that the proposed model outperforms strong baselines by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:38:11 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Xu", "Heng-Da", ""], ["Li", "Zhongli", ""], ["Zhou", "Qingyu", ""], ["Li", "Chao", ""], ["Wang", "Zizhen", ""], ["Cao", "Yunbo", ""], ["Huang", "Heyan", ""], ["Mao", "Xian-Ling", ""]]}, {"id": "2105.12392", "submitter": "Ming Shen", "authors": "Ming Shen, Pratyay Banerjee, Chitta Baral", "title": "Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction", "comments": "Accepted to ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training\nstrategy to tackle pronoun resolution in a fully unsupervised setting. Firstly,\nWe evaluate our pre-trained model on various pronoun resolution datasets\nwithout any finetuning. Our method outperforms all previous unsupervised\nmethods on all datasets by large margins. Secondly, we proceed to a few-shot\nsetting where we finetune our pre-trained model on WinoGrande-S and XS\nseparately. Our method outperforms RoBERTa-large baseline with large margins,\nmeanwhile, achieving a higher AUC score after further finetuning on the\nremaining three official splits of WinoGrande.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:30:18 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 08:46:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Shen", "Ming", ""], ["Banerjee", "Pratyay", ""], ["Baral", "Chitta", ""]]}, {"id": "2105.12397", "submitter": "Hao Zhou", "authors": "Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, Houqiang Li", "title": "Improving Sign Language Translation with Monolingual Data by Sign\n  Back-Translation", "comments": "To appear in 2021 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite existing pioneering works on sign language translation (SLT), there\nis a non-trivial obstacle, i.e., the limited quantity of parallel sign-text\ndata. To tackle this parallel data bottleneck, we propose a sign\nback-translation (SignBT) approach, which incorporates massive spoken language\ntexts into SLT training. With a text-to-gloss translation model, we first\nback-translate the monolingual text to its gloss sequence. Then, the paired\nsign sequence is generated by splicing pieces from an estimated gloss-to-sign\nbank at the feature level. Finally, the synthetic parallel data serves as a\nstrong supplement for the end-to-end training of the encoder-decoder SLT\nframework.\n  To promote the SLT research, we further contribute CSL-Daily, a large-scale\ncontinuous SLT dataset. It provides both spoken language translations and\ngloss-level annotations. The topic revolves around people's daily lives (e.g.,\ntravel, shopping, medical care), the most likely SLT application scenario.\nExtensive experimental results and analysis of SLT methods are reported on\nCSL-Daily. With the proposed sign back-translation method, we obtain a\nsubstantial improvement over previous state-of-the-art SLT methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:49:30 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhou", "Hao", ""], ["Zhou", "Wengang", ""], ["Qi", "Weizhen", ""], ["Pu", "Junfu", ""], ["Li", "Houqiang", ""]]}, {"id": "2105.12399", "submitter": "Naman Jain", "authors": "Akhilesh Ravi, Amit Yadav, Jainish Chauhan, Jatin Dholakia, Naman Jain\n  and Mayank Singh", "title": "SentEmojiBot: Empathising Conversations Generation with Emojis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing use of dialogue agents makes it extremely desirable for them\nto understand and acknowledge the implied emotions to respond like humans with\nempathy. Chatbots using traditional techniques analyze emotions based on the\ncontext and meaning of the text and lack the understanding of emotions\nexpressed through face. Emojis representing facial expressions present a\npromising way to express emotions. However, none of the AI systems utilizes\nemojis for empathetic conversation generation. We propose, SentEmojiBot, based\non the SentEmoji dataset, to generate empathetic conversations with a\ncombination of emojis and text. Evaluation metrics show that the BERT-based\nmodel outperforms the vanilla transformer model. A user study indicates that\nthe dialogues generated by our model were understandable and adding emojis\nimproved empathetic traits in conversations by 9.8%\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:51:44 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Ravi", "Akhilesh", ""], ["Yadav", "Amit", ""], ["Chauhan", "Jainish", ""], ["Dholakia", "Jatin", ""], ["Jain", "Naman", ""], ["Singh", "Mayank", ""]]}, {"id": "2105.12400", "submitter": "Fanchao Qi", "authors": "Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu,\n  Yasheng Wang, Maosong Sun", "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger", "comments": "Accepted by ACL-IJCNLP 2021 as a long paper. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Backdoor attacks are a kind of insidious security threat against machine\nlearning models. After being injected with a backdoor in training, the victim\nmodel will produce adversary-specified outputs on the inputs embedded with\npredesigned triggers but behave properly on normal inputs during inference. As\na sort of emergent attack, backdoor attacks in natural language processing\n(NLP) are investigated insufficiently. As far as we know, almost all existing\ntextual backdoor attack methods insert additional contents into normal samples\nas triggers, which causes the trigger-embedded samples to be detected and the\nbackdoor attacks to be blocked without much effort. In this paper, we propose\nto use the syntactic structure as the trigger in textual backdoor attacks. We\nconduct extensive experiments to demonstrate that the syntactic trigger-based\nattack method can achieve comparable attack performance (almost 100% success\nrate) to the insertion-based methods but possesses much higher invisibility and\nstronger resistance to defenses. These results also reveal the significant\ninsidiousness and harmfulness of textual backdoor attacks. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/HiddenKiller.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:54:19 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 08:27:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Qi", "Fanchao", ""], ["Li", "Mukai", ""], ["Chen", "Yangyi", ""], ["Zhang", "Zhengyan", ""], ["Liu", "Zhiyuan", ""], ["Wang", "Yasheng", ""], ["Sun", "Maosong", ""]]}, {"id": "2105.12410", "submitter": "Tatsuya Hiraoka", "authors": "Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki and Naoaki\n  Okazaki", "title": "Joint Optimization of Tokenization and Downstream Model", "comments": "Accepted at ACL-IJCNLP 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since traditional tokenizers are isolated from a downstream task and model,\nthey cannot output an appropriate tokenization depending on the task and model,\nalthough recent studies imply that the appropriate tokenization improves the\nperformance. In this paper, we propose a novel method to find an appropriate\ntokenization to a given downstream model by jointly optimizing a tokenizer and\nthe model. The proposed method has no restriction except for using loss values\ncomputed by the downstream model to train the tokenizer, and thus, we can apply\nthe proposed method to any NLP task. Moreover, the proposed method can be used\nto explore the appropriate tokenization for an already trained model as\npost-processing. Therefore, the proposed method is applicable to various\nsituations. We evaluated whether our method contributes to improving\nperformance on text classification in three languages and machine translation\nin eight language pairs. Experimental results show that our proposed method\nimproves the performance by determining appropriate tokenizations.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:05:10 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hiraoka", "Tatsuya", ""], ["Takase", "Sho", ""], ["Uchiumi", "Kei", ""], ["Keyaki", "Atsushi", ""], ["Okazaki", "Naoaki", ""]]}, {"id": "2105.12428", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen, Niko Partanen, Jack Rueter, Khalid Alnajjar", "title": "Neural Morphology Dataset and Models for Multiple Languages, from the\n  Large to the Endangered", "comments": "The 23rd Nordic Conference on Computational Linguistics (NoDaLiDa\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We train neural models for morphological analysis, generation and\nlemmatization for morphologically rich languages. We present a method for\nautomatically extracting substantially large amount of training data from FSTs\nfor 22 languages, out of which 17 are endangered. The neural models follow the\nsame tagset as the FSTs in order to make it possible to use them as fallback\nsystems together with the FSTs. The source code, models and datasets have been\nreleased on Zenodo.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:35:38 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Partanen", "Niko", ""], ["Rueter", "Jack", ""], ["Alnajjar", "Khalid", ""]]}, {"id": "2105.12437", "submitter": "Johnny Wei", "authors": "Johnny Tian-Zheng Wei and Robin Jia", "title": "The statistical advantage of automatic NLG metrics at the system level", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the expected output quality of generation systems is central to\nNLG. This paper qualifies the notion that automatic metrics are not as good as\nhumans in estimating system-level quality. Statistically, humans are unbiased,\nhigh variance estimators, while metrics are biased, low variance estimators. We\ncompare these estimators by their error in pairwise prediction (which\ngeneration system is better?) using the bootstrap. Measuring this error is\ncomplicated: predictions are evaluated against noisy, human predicted labels\ninstead of the ground truth, and metric predictions fluctuate based on the test\nsets they were calculated on. By applying a bias-variance-noise decomposition,\nwe adjust this error to a noise-free, infinite test set setting. Our analysis\ncompares the adjusted error of metrics to humans and a derived, perfect\nsegment-level annotator, both of which are unbiased estimators dependent on the\nnumber of judgments collected. In MT, we identify two settings where metrics\noutperform humans due to a statistical advantage in variance: when the number\nof human judgments used is small, and when the quality difference between\ncompared systems is small. The data and code to reproduce our analyses are\navailable at https://github.com/johntzwei/metric-statistical-advantage .\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:53:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wei", "Johnny Tian-Zheng", ""], ["Jia", "Robin", ""]]}, {"id": "2105.12449", "submitter": "Daniel Loureiro", "authors": "Daniel Loureiro, Al\\'ipio M\\'ario Jorge, Jose Camacho-Collados", "title": "LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and\n  Beyond", "comments": "Under review, 81 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantics based on neural approaches is a cornerstone of\nNatural Language Processing, with surprising connections to human meaning\nrepresentation as well. Recent Transformer-based Language Models have proven\ncapable of producing contextual word representations that reliably convey\nsense-specific information, simply as a product of self-supervision. Prior work\nhas shown that these contextual representations can be used to accurately\nrepresent large sense inventories as sense embeddings, to the extent that a\ndistance-based solution to Word Sense Disambiguation (WSD) tasks outperforms\nmodels trained specifically for the task. Still, there remains much to\nunderstand on how to use these Neural Language Models (NLMs) to produce sense\nembeddings that can better harness each NLM's meaning representation abilities.\nIn this work we introduce a more principled approach to leverage information\nfrom all layers of NLMs, informed by a probing analysis on 14 NLM variants. We\nalso emphasize the versatility of these sense embeddings in contrast to\ntask-specific models, applying them on several sense-related tasks, besides\nWSD, while demonstrating improved performance using our proposed approach over\nprior work focused on sense embeddings. Finally, we discuss unexpected findings\nregarding layer and model performance variations, and potential applications\nfor downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:14:22 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Loureiro", "Daniel", ""], ["Jorge", "Al\u00edpio M\u00e1rio", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2105.12523", "submitter": "Yangyifan Xu", "authors": "Yangyifan Xu, Yijin Liu, Fandong Meng, Jiajun Zhang, Jinan Xu, Jie\n  Zhou", "title": "Bilingual Mutual Information Based Adaptive Training for Neural Machine\n  Translation", "comments": "Accepted by ACL-IJCNLP 2021 main conference (short paper). Code is\n  available at: https://github.com/xydaytoy/BMI-NMT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, token-level adaptive training has achieved promising improvement in\nmachine translation, where the cross-entropy loss function is adjusted by\nassigning different training weights to different tokens, in order to alleviate\nthe token imbalance problem. However, previous approaches only use static word\nfrequency information in the target language without considering the source\nlanguage, which is insufficient for bilingual tasks like machine translation.\nIn this paper, we propose a novel bilingual mutual information (BMI) based\nadaptive objective, which measures the learning difficulty for each target\ntoken from the perspective of bilingualism, and assigns an adaptive weight\naccordingly to improve token-level adaptive training. This method assigns\nlarger training weights to tokens with higher BMI, so that easy tokens are\nupdated with coarse granularity while difficult tokens are updated with fine\ngranularity. Experimental results on WMT14 English-to-German and WMT19\nChinese-to-English demonstrate the superiority of our approach compared with\nthe Transformer baseline and previous token-level adaptive training approaches.\nFurther analyses confirm that our method can improve the lexical diversity.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:54:24 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 03:26:02 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Xu", "Yangyifan", ""], ["Liu", "Yijin", ""], ["Meng", "Fandong", ""], ["Zhang", "Jiajun", ""], ["Xu", "Jinan", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.12530", "submitter": "Katerina Papantoniou", "authors": "Katerina Papantoniou, Panagiotis Papadakos, Theodore Patkos, Giorgos\n  Flouris, Ion Androutsopoulos, Dimitris Plexousakis", "title": "Deception detection in text and its relation to the cultural dimension\n  of individualism/collectivism", "comments": "Accepted for publication in Natural Language Engineering journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deception detection is a task with many applications both in direct physical\nand in computer-mediated communication. Our focus is on automatic deception\ndetection in text across cultures. We view culture through the prism of the\nindividualism/collectivism dimension and we approximate culture by using\ncountry as a proxy. Having as a starting point recent conclusions drawn from\nthe social psychology discipline, we explore if differences in the usage of\nspecific linguistic features of deception across cultures can be confirmed and\nattributed to norms in respect to the individualism/collectivism divide. We\nalso investigate if a universal feature set for cross-cultural text deception\ndetection tasks exists. We evaluate the predictive power of different feature\nsets and approaches. We create culture/language-aware classifiers by\nexperimenting with a wide range of n-gram features based on phonology,\nmorphology and syntax, other linguistic cues like word and phoneme counts,\npronouns use, etc., and token embeddings. We conducted our experiments over 11\ndatasets from 5 languages i.e., English, Dutch, Russian, Spanish and Romanian,\nfrom six countries (US, Belgium, India, Russia, Mexico and Romania), and we\napplied two classification methods i.e, logistic regression and fine-tuned BERT\nmodels. The results showed that our task is fairly complex and demanding. There\nare indications that some linguistic cues of deception have cultural origins,\nand are consistent in the context of diverse domains and dataset settings for\nthe same language. This is more evident for the usage of pronouns and the\nexpression of sentiment in deceptive language. The results of this work show\nthat the automatic deception detection across cultures and languages cannot be\nhandled in a unified manner, and that such approaches should be augmented with\nknowledge about cultural differences and the domains of interest.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:09:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Papantoniou", "Katerina", ""], ["Papadakos", "Panagiotis", ""], ["Patkos", "Theodore", ""], ["Flouris", "Giorgos", ""], ["Androutsopoulos", "Ion", ""], ["Plexousakis", "Dimitris", ""]]}, {"id": "2105.12544", "submitter": "Xiachong Feng", "authors": "Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu", "title": "Language Model as an Annotator: Exploring DialoGPT for Dialogue\n  Summarization", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current dialogue summarization systems usually encode the text with a number\nof general semantic features (e.g., keywords and topics) to gain more powerful\ndialogue modeling capabilities. However, these features are obtained via\nopen-domain toolkits that are dialog-agnostic or heavily relied on human\nannotations. In this paper, we show how DialoGPT, a pre-trained model for\nconversational response generation, can be developed as an unsupervised\ndialogue annotator, which takes advantage of dialogue background knowledge\nencoded in DialoGPT. We apply DialoGPT to label three types of features on two\ndialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non\npre-trained models as our summarizes. Experimental results show that our\nproposed method can obtain remarkable improvements on both datasets and\nachieves new state-of-the-art performance on the SAMSum dataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:50:13 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 01:34:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Feng", "Xiachong", ""], ["Feng", "Xiaocheng", ""], ["Qin", "Libo", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""]]}, {"id": "2105.12585", "submitter": "Fanchao Qi", "authors": "Fanchao Qi, Yangyi Chen, Fengyu Wang, Zhiyuan Liu, Xiao Chen, Maosong\n  Sun", "title": "Automatic Construction of Sememe Knowledge Bases via Dictionaries", "comments": "Accepted by Findings of ACL at ACL-IJCNLP 2021. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A sememe is defined as the minimum semantic unit in linguistics. Sememe\nknowledge bases (SKBs), which comprise words annotated with sememes, enable\nsememes to be applied to natural language processing. So far a large body of\nresearch has showcased the unique advantages and effectiveness of SKBs in\nvarious tasks. However, most languages have no SKBs, and manual construction of\nSKBs is time-consuming and labor-intensive. To tackle this challenge, we\npropose a simple and fully automatic method of building an SKB via an existing\ndictionary. We use this method to build an English SKB and a French SKB, and\nconduct comprehensive evaluations from both intrinsic and extrinsic\nperspectives. Experimental results demonstrate that the automatically built\nEnglish SKB is even superior to HowNet, the most widely used SKB that takes\ndecades to build manually. And both the English and French SKBs can bring\nobvious performance enhancement in multiple downstream tasks. All the code and\ndata of this paper (except the copyrighted dictionaries) can be obtained at\nhttps://github.com/thunlp/DictSKB.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:41:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 08:21:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Qi", "Fanchao", ""], ["Chen", "Yangyi", ""], ["Wang", "Fengyu", ""], ["Liu", "Zhiyuan", ""], ["Chen", "Xiao", ""], ["Sun", "Maosong", ""]]}, {"id": "2105.12628", "submitter": "Yujia Bao", "authors": "Yujia Bao, Shiyu Chang, Regina Barzilay", "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:37:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bao", "Yujia", ""], ["Chang", "Shiyu", ""], ["Barzilay", "Regina", ""]]}, {"id": "2105.12659", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "G. Antonacci, A. Fronzetti Colladon, A. Stefanini, P. Gloor", "title": "It is rotating leaders who build the swarm: social network determinants\n  of growth for healthcare virtual communities of practice", "comments": null, "journal-ref": "Journal of Knowledge Management 21(5), 1218-1239 (2017)", "doi": "10.1108/JKM-11-2016-0504", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: The purpose of this paper is to identify the factors influencing the\ngrowth of healthcare virtual communities of practice (VCoPs) through a\nseven-year longitudinal study conducted using metrics from social-network and\nsemantic analysis. By studying online communication along the three dimensions\nof social interactions (connectivity, interactivity and language use), the\nauthors aim to provide VCoP managers with valuable insights to improve the\nsuccess of their communities. Design/methodology/approach: Communications over\na period of seven years (April 2008 to April 2015) and between 14,000 members\nof 16 different healthcare VCoPs coexisting on the same web platform were\nanalysed. Multilevel regression models were used to reveal the main\ndeterminants of community growth over time. Independent variables were derived\nfrom social network and semantic analysis measures. Findings: Results show that\nstructural and content-based variables predict the growth of the community.\nProgressively, more people will join a community if its structure is more\ncentralised, leaders are more dynamic (they rotate more) and the language used\nin the posts is less complex. Research limitations/implications: The available\ndata set included one Web platform and a limited number of control variables.\nTo consolidate the findings of the present study, the experiment should be\nreplicated on other healthcare VCoPs. Originality/value: The study provides\nuseful recommendations for setting up and nurturing the growth of professional\ncommunities, considering, at the same time, the interaction patterns among the\ncommunity members, the dynamic evolution of these interactions and the use of\nlanguage. New analytical tools are presented, together with the use of\ninnovative interaction metrics, that can significantly influence community\ngrowth, such as rotating leadership.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:15:31 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Antonacci", "G.", ""], ["Colladon", "A. Fronzetti", ""], ["Stefanini", "A.", ""], ["Gloor", "P.", ""]]}, {"id": "2105.12667", "submitter": "Elizabeth Nielsen", "authors": "Elizabeth Nielsen, Mark Steedman, Sharon Goldwater", "title": "Prosodic segmentation for parsing spoken dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parsing spoken dialogue poses unique difficulties, including disfluencies and\nunmarked boundaries between sentence-like units. Previous work has shown that\nprosody can help with parsing disfluent speech (Tran et al. 2018), but has\nassumed that the input to the parser is already segmented into sentence-like\nunits (SUs), which isn't true in existing speech applications. We investigate\nhow prosody affects a parser that receives an entire dialogue turn as input (a\nturn-based model), instead of gold standard pre-segmented SUs (an SU-based\nmodel). In experiments on the English Switchboard corpus, we find that when\nusing transcripts alone, the turn-based model has trouble segmenting SUs,\nleading to worse parse performance than the SU-based model. However, prosody\ncan effectively replace gold standard SU boundaries: with prosody, the\nturn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1\nscore, respectively), despite performing two tasks (SU segmentation and\nparsing) rather than one (parsing alone). Analysis shows that pitch and\nintensity features are the most important for this corpus, since they allow the\nmodel to correctly distinguish an SU boundary from a speech disfluency -- a\ndistinction that the model otherwise struggles to make.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:30:16 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Nielsen", "Elizabeth", ""], ["Steedman", "Mark", ""], ["Goldwater", "Sharon", ""]]}, {"id": "2105.12682", "submitter": "Luyang Kong", "authors": "Luyang Kong, Christopher Winestock, Parminder Bhatia", "title": "Zero-shot Medical Entity Retrieval without Annotation: Learning From\n  Rich Knowledge Graph Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical entity retrieval is an integral component for understanding and\ncommunicating information across various health systems. Current approaches\ntend to work well on specific medical domains but generalize poorly to unseen\nsub-specialties. This is of increasing concern under a public health crisis as\nnew medical conditions and drug treatments come to light frequently. Zero-shot\nretrieval is challenging due to the high degree of ambiguity and variability in\nmedical corpora, making it difficult to build an accurate similarity measure\nbetween mentions and concepts. Medical knowledge graphs (KG), however, contain\nrich semantics including large numbers of synonyms as well as its curated\ngraphical structures. To take advantage of this valuable information, we\npropose a suite of learning tasks designed for training efficient zero-shot\nentity retrieval models. Without requiring any human annotation, our knowledge\ngraph enriched architecture significantly outperforms common zero-shot\nbenchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across\nmultiple major medical ontologies, such as UMLS, SNOMED, and ICD-10.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:53:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kong", "Luyang", ""], ["Winestock", "Christopher", ""], ["Bhatia", "Parminder", ""]]}, {"id": "2105.12708", "submitter": "Julia Pritzen", "authors": "Julia Pritzen, Michael Gref, Dietlind Z\\\"uhlke, Christoph Schmidt", "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in\n  German Speech Recognition", "comments": "Submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loanwords, such as Anglicisms, are a challenge in German speech recognition.\nDue to their irregular pronunciation compared to native German words,\nautomatically generated pronunciation dictionaries often include faulty phoneme\nsequences for Anglicisms. In this work, we propose a multitask\nsequence-to-sequence approach for grapheme-to-phoneme conversion to improve the\nphonetization of Anglicisms. We extended a grapheme-to-phoneme model with a\nclassifier to distinguish Anglicisms from native German words. With this\napproach, the model learns to generate pronunciations differently depending on\nthe classification result. We used our model to create supplementary Anglicism\npronunciation dictionaries that are added to an existing German speech\nrecognition model. Tested on a dedicated Anglicism evaluation set, we improved\nthe recognition of Anglicisms compared to a baseline model, reducing the word\nerror rate by 1 % and the Anglicism error rate by 3 %. We show that multitask\nlearning can help solving the challenge of loanwords in German speech\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:42:13 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:36:06 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pritzen", "Julia", ""], ["Gref", "Michael", ""], ["Z\u00fchlke", "Dietlind", ""], ["Schmidt", "Christoph", ""]]}, {"id": "2105.12762", "submitter": "Jonathan K Kummerfeld", "authors": "Jonathan K. Kummerfeld", "title": "Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing", "comments": "To appear at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive work has argued in favour of paying crowd workers a wage that is at\nleast equivalent to the U.S. federal minimum wage. Meanwhile, research on\ncollecting high quality annotations suggests using a qualification that\nrequires workers to have previously completed a certain number of tasks. If\nmost requesters who pay fairly require workers to have completed a large number\nof tasks already then workers need to complete a substantial amount of poorly\npaid work before they can earn a fair wage. Through analysis of worker\ndiscussions and guidance for researchers, we estimate that workers spend\napproximately 2.25 months of full time effort on poorly paid tasks in order to\nget the qualifications needed for better paid tasks. We discuss alternatives to\nthis qualification and conduct a study of the correlation between\nqualifications and work quality on two NLP tasks. We find that it is possible\nto reduce the burden on workers while still collecting high quality data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:02:39 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kummerfeld", "Jonathan K.", ""]]}, {"id": "2105.12804", "submitter": "Hugh Perkins", "authors": "Hugh Perkins", "title": "TexRel: a Green Family of Datasets for Emergent Communications on\n  Relations", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new dataset TexRel as a playground for the study of emergent\ncommunications, in particular for relations. By comparison with other relations\ndatasets, TexRel provides rapid training and experimentation, whilst being\nsufficiently large to avoid overfitting in the context of emergent\ncommunications. By comparison with using symbolic inputs, TexRel provides a\nmore realistic alternative whilst remaining efficient and fast to learn. We\ncompare the performance of TexRel with a related relations dataset Shapeworld.\nWe provide baseline performance results on TexRel for sender architectures,\nreceiver architectures and end-to-end architectures. We examine the effect of\nmultitask learning in the context of shapes, colors and relations on accuracy,\ntopological similarity and clustering precision. We investigate whether\nincreasing the size of the latent meaning space improves metrics of\ncompositionality. We carry out a case-study on using TexRel to reproduce the\nresults of an experiment in a recent paper that used symbolic inputs, but using\nour own non-symbolic inputs, from TexRel, instead.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 19:45:33 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Perkins", "Hugh", ""]]}, {"id": "2105.12825", "submitter": "Zhihan Zhou", "authors": "Zhihan Zhou, Liqian Ma, Han Liu", "title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven\n  Trading", "comments": "Accepted to publish in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-fin.CP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an event-driven trading strategy that predicts\nstock movements by detecting corporate events from news articles. Unlike\nexisting models that utilize textual features (e.g., bag-of-words) and\nsentiments to directly make stock predictions, we consider corporate events as\nthe driving force behind stock movements and aim to profit from the temporary\nstock mispricing that may occur when corporate events take place. The core of\nthe proposed strategy is a bi-level event detection model. The low-level event\ndetector identifies events' existences from each token, while the high-level\nevent detector incorporates the entire article's representation and the\nlow-level detected results to discover events at the article-level. We also\ndevelop an elaborately-annotated dataset EDT for corporate event detection and\nnews-based stock prediction benchmark. EDT includes 9721 news articles with\ntoken-level event labels as well as 303893 news articles with minute-level\ntimestamps and comprehensive stock price labels. Experiments on EDT indicate\nthat the proposed strategy outperforms all the baselines in winning rate,\nexcess returns over the market, and the average return on each transaction.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:39:40 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 01:40:55 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhou", "Zhihan", ""], ["Ma", "Liqian", ""], ["Liu", "Han", ""]]}, {"id": "2105.12848", "submitter": "Yinghao Li", "authors": "Yinghao Li, Pranav Shetty, Lucas Liu, Chao Zhang, Le Song", "title": "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised\n  Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of learning a named entity recognition (NER) tagger\nusing noisy labels from multiple weak supervision sources. Though cheap to\nobtain, the labels from weak supervision sources are often incomplete,\ninaccurate, and contradictory, making it difficult to learn an accurate NER\nmodel. To address this challenge, we propose a conditional hidden Markov model\n(CHMM), which can effectively infer true labels from multi-source noisy labels\nin an unsupervised way. CHMM enhances the classic hidden Markov model with the\ncontextual representation power of pre-trained language models. Specifically,\nCHMM learns token-wise transition and emission probabilities from the BERT\nembeddings of the input tokens to infer the latent true labels from noisy\nobservations. We further refine CHMM with an alternate-training approach\n(CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM,\nand this BERT-NER's output is regarded as an additional weak source to train\nthe CHMM in return. Experiments on four NER benchmarks from various domains\nshow that our method outperforms state-of-the-art weakly supervised NER models\nby wide margins.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:18:48 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 13:53:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yinghao", ""], ["Shetty", "Pranav", ""], ["Liu", "Lucas", ""], ["Zhang", "Chao", ""], ["Song", "Le", ""]]}, {"id": "2105.12887", "submitter": "Chuan-An Lin", "authors": "Nazib Sorathiya, Chuan-An Lin, Daniel Chen Daniel Xiong, Scott Zin, Yi\n  Zhang, He Sarina Yang, Sharon Xiaolei Huang", "title": "Multi-turn Dialog System on Single-turn Data in Medical Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently there has been a huge interest in dialog systems. This interest has\nalso been developed in the field of the medical domain where researchers are\nfocusing on building a dialog system in the medical domain. This research is\nfocused on the multi-turn dialog system trained on the multi-turn dialog data.\nIt is difficult to gather a huge amount of multi-turn conversational data in\nthe medical domain that is verified by professionals and can be trusted.\nHowever, there are several frequently asked questions (FAQs) or single-turn QA\npairs that have information that is verified by the experts and can be used to\nbuild a multi-turn dialog system.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 00:42:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sorathiya", "Nazib", ""], ["Lin", "Chuan-An", ""], ["Xiong", "Daniel Chen Daniel", ""], ["Zin", "Scott", ""], ["Zhang", "Yi", ""], ["Yang", "He Sarina", ""], ["Huang", "Sharon Xiaolei", ""]]}, {"id": "2105.12900", "submitter": "Weijia Xu", "authors": "Weijia Xu, Shuming Ma, Dongdong Zhang, Marine Carpuat", "title": "How Does Distilled Data Complexity Impact the Quality and Confidence of\n  Non-Autoregressive Machine Translation?", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While non-autoregressive (NAR) models are showing great promise for machine\ntranslation, their use is limited by their dependence on knowledge distillation\nfrom autoregressive models. To address this issue, we seek to understand why\ndistillation is so effective. Prior work suggests that distilled training data\nis less complex than manual translations. Based on experiments with the\nLevenshtein Transformer and the Mask-Predict NAR models on the WMT14\nGerman-English task, this paper shows that different types of complexity have\ndifferent impacts: while reducing lexical diversity and decreasing reordering\ncomplexity both help NAR learn better alignment between source and target, and\nthus improve translation quality, lexical diversity is the main reason why\ndistillation increases model confidence, which affects the calibration of\ndifferent NAR models differently.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:19:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Xu", "Weijia", ""], ["Ma", "Shuming", ""], ["Zhang", "Dongdong", ""], ["Carpuat", "Marine", ""]]}, {"id": "2105.12907", "submitter": "Weizhou Shen", "authors": "Weizhou Shen, Siyue Wu, Yunyi Yang and Xiaojun Quan", "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition", "comments": "ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The modeling of conversational context plays a vital role in emotion\nrecognition from conversation (ERC). In this paper, we put forward a novel idea\nof encoding the utterances with a directed acyclic graph (DAG) to better model\nthe intrinsic structure within a conversation, and design a directed acyclic\nneural network,~namely DAG-ERC, to implement this idea.~In an attempt to\ncombine the strengths of conventional graph-based neural models and\nrecurrence-based neural models,~DAG-ERC provides a more intuitive way to model\nthe information flow between long-distance conversation background and nearby\ncontext.~Extensive experiments are conducted on four ERC benchmarks with\nstate-of-the-art models employed as baselines for comparison.~The empirical\nresults demonstrate the superiority of this new model and confirm the\nmotivation of the directed acyclic graph architecture for ERC.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:51:37 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shen", "Weizhou", ""], ["Wu", "Siyue", ""], ["Yang", "Yunyi", ""], ["Quan", "Xiaojun", ""]]}, {"id": "2105.12932", "submitter": "Xiaofei Ma", "authors": "Xiaofei Ma, Cicero Nogueira dos Santos and Andrew O. Arnold", "title": "Contrastive Fine-tuning Improves Robustness for Neural Rankers", "comments": null, "journal-ref": "Findings of ACL 2021", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of state-of-the-art neural rankers can deteriorate\nsubstantially when exposed to noisy inputs or applied to a new domain. In this\npaper, we present a novel method for fine-tuning neural rankers that can\nsignificantly improve their robustness to out-of-domain data and query\nperturbations. Specifically, a contrastive loss that compares data points in\nthe representation space is combined with the standard ranking loss during\nfine-tuning. We use relevance labels to denote similar/dissimilar pairs, which\nallows the model to learn the underlying matching semantics across different\nquery-document pairs and leads to improved robustness. In experiments with four\npassage ranking datasets, the proposed contrastive fine-tuning method obtains\nimprovements on robustness to query reformulations, noise perturbations, and\nzero-shot transfer for both BERT and BART based rankers. Additionally, our\nexperiments show that contrastive fine-tuning outperforms data augmentation for\nrobustifying neural rankers.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 04:00:22 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ma", "Xiaofei", ""], ["Santos", "Cicero Nogueira dos", ""], ["Arnold", "Andrew O.", ""]]}, {"id": "2105.12936", "submitter": "Katherine Keith", "authors": "Andrew Halterman, Katherine A. Keith, Sheikh Muhammad Sarwar, Brendan\n  O'Connor", "title": "Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus\n  Covering the 2002 Gujarat Violence", "comments": "To appear in Findings of ACL 2021", "journal-ref": "Findings of ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated event extraction in social science applications often requires\ncorpus-level evaluations: for example, aggregating text predictions across\nmetadata and unbiased estimates of recall. We combine corpus-level evaluation\nrequirements with a real-world, social science setting and introduce the\nIndiaPoliceEvents corpus--all 21,391 sentences from 1,257 English-language\nTimes of India articles about events in the state of Gujarat during March 2002.\nOur trained annotators read and label every document for mentions of police\nactivity events, allowing for unbiased recall evaluations. In contrast to other\ndatasets with structured event representations, we gather annotations by posing\nnatural questions, and evaluate off-the-shelf models for three different tasks:\nsentence classification, document ranking, and temporal aggregation of target\nevents. We present baseline results from zero-shot BERT-based models fine-tuned\non natural language inference and passage retrieval tasks. Our novel\ncorpus-level evaluations and annotation approach can guide creation of similar\nsocial-science-oriented resources in the future.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 04:15:44 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Halterman", "Andrew", ""], ["Keith", "Katherine A.", ""], ["Sarwar", "Sheikh Muhammad", ""], ["O'Connor", "Brendan", ""]]}, {"id": "2105.12967", "submitter": "Fandong Meng", "authors": "Fusheng Wang, Jianhao Yan, Fandong Meng, Jie Zhou", "title": "Selective Knowledge Distillation for Neural Machine Translation", "comments": "Accepted as a long paper at ACL 2021. Code is available at\n  https://github.com/LeslieOverfitting/selective_distillation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) models achieve state-of-the-art performance\non many translation benchmarks. As an active research field in NMT, knowledge\ndistillation is widely applied to enhance the model's performance by\ntransferring teacher model's knowledge on each training sample. However,\nprevious work rarely discusses the different impacts and connections among\nthese samples, which serve as the medium for transferring teacher knowledge. In\nthis paper, we design a novel protocol that can effectively analyze the\ndifferent impacts of samples by comparing various samples' partitions. Based on\nabove protocol, we conduct extensive experiments and find that the teacher's\nknowledge is not the more, the better. Knowledge over specific samples may even\nhurt the whole performance of knowledge distillation. Finally, to address these\nissues, we propose two simple yet effective strategies, i.e., batch-level and\nglobal-level selections, to pick suitable samples for distillation. We evaluate\nour approaches on two large-scale machine translation tasks, WMT'14\nEnglish->German and WMT'19 Chinese->English. Experimental results show that our\napproaches yield up to +1.28 and +0.89 BLEU points improvements over the\nTransformer baseline, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 06:54:12 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wang", "Fusheng", ""], ["Yan", "Jianhao", ""], ["Meng", "Fandong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.12969", "submitter": "Tiezheng Yu", "authors": "Dan Su, Tiezheng Yu, Pascale Fung", "title": "Improve Query Focused Abstractive Summarization by Incorporating Answer\n  Relevance", "comments": "The two authors contribute equally. Accepted as a short paper in\n  Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Query focused summarization (QFS) models aim to generate summaries from\nsource documents that can answer the given query. Most previous work on QFS\nonly considers the query relevance criterion when producing the summary.\nHowever, studying the effect of answer relevance in the summary generating\nprocess is also important. In this paper, we propose QFS-BART, a model that\nincorporates the explicit answer relevance of the source documents given the\nquery via a question answering model, to generate coherent and answer-related\nsummaries. Furthermore, our model can take advantage of large pre-trained\nmodels which improve the summarization performance significantly. Empirical\nresults on the Debatepedia dataset show that the proposed model achieves the\nnew state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 06:58:42 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 04:38:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Su", "Dan", ""], ["Yu", "Tiezheng", ""], ["Fung", "Pascale", ""]]}, {"id": "2105.12980", "submitter": "Tilman Beck", "authors": "Tilman Beck, Ji-Ung Lee, Christina Viehmann, Marcus Maurer, Oliver\n  Quiring, Iryna Gurevych", "title": "Investigating label suggestions for opinion mining in German Covid-19\n  social media", "comments": "To Appear at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the use of interactively updated label suggestions to\nimprove upon the efficiency of gathering annotations on the task of opinion\nmining in German Covid-19 social media data. We develop guidelines to conduct a\ncontrolled annotation study with social science students and find that\nsuggestions from a model trained on a small, expert-annotated dataset already\nlead to a substantial improvement - in terms of inter-annotator agreement(+.14\nFleiss' $\\kappa$) and annotation quality - compared to students that do not\nreceive any label suggestions. We further find that label suggestions from\ninteractively trained models do not lead to an improvement over suggestions\nfrom a static model. Nonetheless, our analysis of suggestion bias shows that\nannotators remain capable of reflecting upon the suggested label in general.\nFinally, we confirm the quality of the annotated data in transfer learning\nexperiments between different annotator groups. To facilitate further research\nin opinion mining on social media data, we release our collected data\nconsisting of 200 expert and 2,785 student annotations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 07:47:53 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 11:01:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Beck", "Tilman", ""], ["Lee", "Ji-Ung", ""], ["Viehmann", "Christina", ""], ["Maurer", "Marcus", ""], ["Quiring", "Oliver", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2105.12995", "submitter": "Thomas Dopierre", "authors": "Thomas Dopierre, Christophe Gravier, Wilfried Logerais", "title": "ProtAugment: Unsupervised diverse short-texts paraphrasing for intent\n  detection meta-learning", "comments": "Accepted at the 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research considers few-shot intent detection as a meta-learning\nproblem: the model is learning to learn from a consecutive set of small tasks\nnamed episodes. In this work, we propose ProtAugment, a meta-learning algorithm\nfor short texts classification (the intent detection task). ProtAugment is a\nnovel extension of Prototypical Networks, that limits overfitting on the bias\nintroduced by the few-shots classification objective at each episode. It relies\non diverse paraphrasing: a conditional language model is first fine-tuned for\nparaphrasing, and diversity is later introduced at the decoding stage at each\nmeta-learning episode. The diverse paraphrasing is unsupervised as it is\napplied to unlabelled data, and then fueled to the Prototypical Network\ntraining objective as a consistency loss. ProtAugment is the state-of-the-art\nmethod for intent detection meta-learning, at no extra labeling efforts and\nwithout the need to fine-tune a conditional language model on a given\napplication domain.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:31:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dopierre", "Thomas", ""], ["Gravier", "Christophe", ""], ["Logerais", "Wilfried", ""]]}, {"id": "2105.13022", "submitter": "Zhirui Zhang", "authors": "Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen,\n  Weihua Luo and Jiajun Chen", "title": "Adaptive Nearest Neighbor Machine Translation", "comments": "Accepted by ACL-IJCNLP 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines\npre-trained neural machine translation (NMT) model with token-level\nk-nearest-neighbor (kNN) retrieval to improve the translation accuracy.\nHowever, the traditional kNN algorithm used in kNN-MT simply retrieves a same\nnumber of nearest neighbors for each target token, which may cause prediction\nerrors when the retrieved neighbors include noises. In this paper, we propose\nAdaptive kNN-MT to dynamically determine the number of k for each target token.\nWe achieve this by introducing a light-weight Meta-k Network, which can be\nefficiently trained with only a few training samples. On four benchmark machine\ntranslation datasets, we demonstrate that the proposed method is able to\neffectively filter out the noises in retrieval results and significantly\noutperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k\nNetwork learned on one domain could be directly applied to other domains and\nobtain consistent improvements, illustrating the generality of our method. Our\nimplementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:27:42 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zheng", "Xin", ""], ["Zhang", "Zhirui", ""], ["Guo", "Junliang", ""], ["Huang", "Shujian", ""], ["Chen", "Boxing", ""], ["Luo", "Weihua", ""], ["Chen", "Jiajun", ""]]}, {"id": "2105.13025", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "Q. Wen, P. A. Gloor, A. Fronzetti Colladon, P. Tickoo, T. Joshi", "title": "Finding top performers through email patterns analysis", "comments": null, "journal-ref": "Journal of Information Science 46(4), 508-527 (2020)", "doi": "10.1177/0165551519849519", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the information economy, individuals' work performance is closely\nassociated with their digital communication strategies. This study combines\nsocial network and semantic analysis to develop a method to identify top\nperformers based on email communication. By reviewing existing literature, we\nidentified the indicators that quantify email communication into measurable\ndimensions. To empirically examine the predictive power of the proposed\nindicators, we collected 2 million email archive of 578 executives in an\ninternational service company. Panel regression was employed to derive\ninterpretable association between email indicators and top performance. The\nresults suggest that top performers tend to assume central network positions\nand have high responsiveness to emails. In email contents, top performers use\nmore positive and complex language, with low emotionality, but rich in\ninfluential words that are probably reused by co-workers. To better explore the\npredictive power of the email indicators, we employed AdaBoost machine learning\nmodels, which achieved 83.56% accuracy in identifying top performers. With\ncluster analysis, we further find three categories of top performers,\n\"networkers\" with central network positions, \"influencers\" with influential\nideas and \"positivists\" with positive sentiments. The findings suggest that top\nperformers have distinctive email communication patterns, laying the foundation\nfor grounding email communication competence in theory. The proposed email\nanalysis method also provides a tool to evaluate the different types of\nindividual communication styles.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:45:02 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wen", "Q.", ""], ["Gloor", "P. A.", ""], ["Colladon", "A. Fronzetti", ""], ["Tickoo", "P.", ""], ["Joshi", "T.", ""]]}, {"id": "2105.13036", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "P. Gloor, A. Fronzetti Colladon, J. M. de Oliveira, P. Rovelli", "title": "Put your money where your mouth is: Using deep learning to identify\n  consumer tribes from word usage", "comments": null, "journal-ref": "International Journal of Information Management 51, 101924 (2020)", "doi": "10.1016/j.ijinfomgt.2019.03.011", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Internet and social media offer firms novel ways of managing their marketing\nstrategy and gain competitive advantage. The groups of users expressing\nthemselves on the Internet about a particular topic, product, or brand are\nfrequently called a virtual tribe or E-tribe. However, there are no automatic\ntools for identifying and studying the characteristics of these virtual tribes.\nTowards this aim, this paper presents Tribefinder, a system to reveal Twitter\nusers' tribal affiliations, by analyzing their tweets and language use. To show\nthe potential of this instrument, we provide an example considering three\nspecific tribal macro-categories: alternative realities, lifestyle, and\nrecreation. In addition, we discuss the different characteristics of each\nidentified tribe, in terms of use of language and social interaction metrics.\nTribefinder illustrates the importance of adopting a new lens for studying\nvirtual tribes, which is crucial for firms to properly design their marketing\nstrategy, and for scholars to extend prior marketing research.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:06:56 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gloor", "P.", ""], ["Colladon", "A. Fronzetti", ""], ["de Oliveira", "J. M.", ""], ["Rovelli", "P.", ""]]}, {"id": "2105.13065", "submitter": "Mark Fi\\v{s}el", "authors": "Maali Tars, Andre T\\\"attar, Mark Fi\\v{s}el", "title": "Extremely low-resource machine translation for closely related languages", "comments": "Accepted at Nodalida'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  An effective method to improve extremely low-resource neural machine\ntranslation is multilingual training, which can be improved by leveraging\nmonolingual data to create synthetic bilingual corpora using the\nback-translation method. This work focuses on closely related languages from\nthe Uralic language family: from Estonian and Finnish geographical regions. We\nfind that multilingual learning and synthetic corpora increase the translation\nquality in every language pair for which we have data. We show that transfer\nlearning and fine-tuning are very effective for doing low-resource machine\ntranslation and achieve the best results. We collected new parallel data for\nV\\~oro, North and South Saami and present first results of neural machine\ntranslation for these languages.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:27:06 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Tars", "Maali", ""], ["T\u00e4ttar", "Andre", ""], ["Fi\u0161el", "Mark", ""]]}, {"id": "2105.13072", "submitter": "Lemao Liu", "authors": "Guoping Huang, Lemao Liu, Xing Wang, Longyue Wang, Huayang Li,\n  Zhaopeng Tu, Chengyan Huang and Shuming Shi", "title": "TranSmart: A Practical Interactive Machine Translation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic machine translation is super efficient to produce translations yet\ntheir quality is not guaranteed. This technique report introduces TranSmart, a\npractical human-machine interactive translation system that is able to trade\noff translation quality and efficiency. Compared to existing publicly available\ninteractive translation systems, TranSmart supports three key features,\nword-level autocompletion, sentence-level autocompletion and translation\nmemory. By word-level and sentence-level autocompletion, TranSmart allows users\nto interactively translate words in their own manners rather than the strict\nmanner from left to right. In addition, TranSmart has the potential to avoid\nsimilar translation mistakes by using translated sentences in history as its\nmemory. This report presents major functions of TranSmart, algorithms for\nachieving these functions, how to use the TranSmart APIs, and evaluation\nresults of some key functions. TranSmart is publicly available at its homepage\n(https://transmart.qq.com).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:40:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Huang", "Guoping", ""], ["Liu", "Lemao", ""], ["Wang", "Xing", ""], ["Wang", "Longyue", ""], ["Li", "Huayang", ""], ["Tu", "Zhaopeng", ""], ["Huang", "Chengyan", ""], ["Shi", "Shuming", ""]]}, {"id": "2105.13073", "submitter": "Zujie Liang", "authors": "Zujie Liang, Huang Hu, Can Xu, Chongyang Tao, Xiubo Geng, Yining Chen,\n  Fan Liang and Daxin Jiang", "title": "Maria: A Visual Experience Powered Conversational Agent", "comments": "Accepted by ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguably, the visual perception of conversational agents to the physical\nworld is a key way for them to exhibit the human-like intelligence.\nImage-grounded conversation is thus proposed to address this challenge.\nExisting works focus on exploring the multimodal dialog models that ground the\nconversation on a given image. In this paper, we take a step further to study\nimage-grounded conversation under a fully open-ended setting where no paired\ndialog and image are assumed available. Specifically, we present Maria, a\nneural conversation agent powered by the visual world experiences which are\nretrieved from a large-scale image index. Maria consists of three flexible\ncomponents, i.e., text-to-image retriever, visual concept detector and\nvisual-knowledge-grounded response generator. The retriever aims to retrieve a\ncorrelated image to the dialog from an image index, while the visual concept\ndetector extracts rich visual knowledge from the image. Then, the response\ngenerator is grounded on the extracted visual knowledge and dialog context to\ngenerate the target response. Extensive experiments demonstrate Maria\noutperforms previous state-of-the-art methods on automatic metrics and human\nevaluation, and can generate informative responses that have some visual\ncommonsense of the physical world.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:45:29 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:48:55 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liang", "Zujie", ""], ["Hu", "Huang", ""], ["Xu", "Can", ""], ["Tao", "Chongyang", ""], ["Geng", "Xiubo", ""], ["Chen", "Yining", ""], ["Liang", "Fan", ""], ["Jiang", "Daxin", ""]]}, {"id": "2105.13074", "submitter": "Yinyu Lan", "authors": "Yinyu Lan, Shizhu He, Xiangrong Zeng, Shengping Liu, Kang Liu, Jun\n  Zhao", "title": "Path-based knowledge reasoning with textual semantic information for\n  medical knowledge graph completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Knowledge graphs (KGs), especially medical knowledge graphs, are\noften significantly incomplete, so it necessitating a demand for medical\nknowledge graph completion (MedKGC). MedKGC can find new facts based on the\nexited knowledge in the KGs. The path-based knowledge reasoning algorithm is\none of the most important approaches to this task. This type of method has\nreceived great attention in recent years because of its high performance and\ninterpretability. In fact, traditional methods such as path ranking algorithm\n(PRA) take the paths between an entity pair as atomic features. However, the\nmedical KGs are very sparse, which makes it difficult to model effective\nsemantic representation for extremely sparse path features. The sparsity in the\nmedical KGs is mainly reflected in the long-tailed distribution of entities and\npaths. Previous methods merely consider the context structure in the paths of\nthe knowledge graph and ignore the textual semantics of the symbols in the\npath. Therefore, their performance cannot be further improved due to the two\naspects of entity sparseness and path sparseness. To address the above issues,\nthis paper proposes two novel path-based reasoning methods to solve the\nsparsity issues of entity and path respectively, which adopts the textual\nsemantic information of entities and paths for MedKGC. By using the pre-trained\nmodel BERT, combining the textual semantic representations of the entities and\nthe relationships, we model the task of symbolic reasoning in the medical KG as\na numerical computing issue in textual semantic representation.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:45:59 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 02:38:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lan", "Yinyu", ""], ["He", "Shizhu", ""], ["Zeng", "Xiangrong", ""], ["Liu", "Shengping", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""]]}, {"id": "2105.13135", "submitter": "Jinbae Im", "authors": "Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, Sehee Chung", "title": "Self-Supervised Multimodal Opinion Summarization", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, opinion summarization, which is the generation of a summary from\nmultiple reviews, has been conducted in a self-supervised manner by considering\na sampled review as a pseudo summary. However, non-text data such as image and\nmetadata related to reviews have been considered less often. To use the\nabundant information contained in non-text data, we propose a self-supervised\nmultimodal opinion summarization framework called MultimodalSum. Our framework\nobtains a representation of each modality using a separate encoder for each\nmodality, and the text decoder generates a summary. To resolve the inherent\nheterogeneity of multimodal data, we propose a multimodal training pipeline. We\nfirst pretrain the text encoder--decoder based solely on text modality data.\nSubsequently, we pretrain the non-text modality encoders by considering the\npretrained text decoder as a pivot for the homogeneous representation of\nmultimodal data. Finally, to fuse multimodal representations, we train the\nentire framework in an end-to-end manner. We demonstrate the superiority of\nMultimodalSum by conducting experiments on Yelp and Amazon datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:29:05 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Im", "Jinbae", ""], ["Kim", "Moonki", ""], ["Lee", "Hoyeop", ""], ["Cho", "Hyunsouk", ""], ["Chung", "Sehee", ""]]}, {"id": "2105.13225", "submitter": "Qing Sun", "authors": "Qing Sun, Parminder Bhatia", "title": "Neural Entity Recognition with Gazetteer based Fusion", "comments": null, "journal-ref": "the Association for Computational Linguistics (ACL) 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating external knowledge into Named Entity Recognition (NER) systems\nhas been widely studied in the generic domain. In this paper, we focus on\nclinical domain where only limited data is accessible and interpretability is\nimportant. Recent advancement in technology and the acceleration of clinical\ntrials has resulted in the discovery of new drugs, procedures as well as\nmedical conditions. These factors motivate towards building robust zero-shot\nNER systems which can quickly adapt to new medical terminology. We propose an\nauxiliary gazetteer model and fuse it with an NER system, which results in\nbetter robustness and interpretability across different clinical datasets. Our\ngazetteer based fusion model is data efficient, achieving +1.7 micro-F1 gains\non the i2b2 dataset using 20% training data, and brings + 4.7 micro-F1 gains on\nnovel entity mentions never presented during training. Moreover, our fusion\nmodel is able to quickly adapt to new mentions in gazetteers without\nre-training and the gains from the proposed fusion model are transferable to\nrelated datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:14:15 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sun", "Qing", ""], ["Bhatia", "Parminder", ""]]}, {"id": "2105.13239", "submitter": "Junjie Huang", "authors": "Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang,\n  Ming Zhou, Nan Duan", "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering", "comments": "ACL 2021 main conference. The CoSQA data and leaderboard are\n  available at\n  https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/NL-code-search-WebQuery.\n  The code is available at https://github.com/Jun-jie-Huang/CoCLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding codes given natural language query isb eneficial to the productivity\nof software developers. Future progress towards better semantic matching\nbetween query and code requires richer supervised training resources. To remedy\nthis, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of\nnatural language queries and codes, each annotated by at least 3 human\nannotators. We further introduce a contrastive learning method dubbed CoCLR to\nenhance query-code matching, which works as a data augmenter to bring more\nartificially generated training instances. We show that evaluated on CodeXGLUE\nwith the same CodeBERT model, training on CoSQA improves the accuracy of code\nquestion answering by 5.1%, and incorporating CoCLR brings a further\nimprovement of 10.5%.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:37:21 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Huang", "Junjie", ""], ["Tang", "Duyu", ""], ["Shou", "Linjun", ""], ["Gong", "Ming", ""], ["Xu", "Ke", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""], ["Duan", "Nan", ""]]}, {"id": "2105.13255", "submitter": "Jie Huang", "authors": "Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-mei Hwu", "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical\n  Core-Fringe Approach", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to measure fine-grained domain relevance - the degree that a term\nis relevant to a broad (e.g., computer science) or narrow (e.g., deep learning)\ndomain. Such measurement is crucial for many downstream tasks in natural\nlanguage processing. To handle long-tail terms, we build a core-anchored\nsemantic graph, which uses core terms with rich description information to\nbridge the vast remaining fringe terms semantically. To support a fine-grained\ndomain without relying on a matching corpus for supervision, we develop\nhierarchical core-fringe learning, which learns core and fringe terms jointly\nin a semi-supervised manner contextualized in the hierarchy of the domain. To\nreduce expensive human efforts, we employ automatic annotation and hierarchical\npositive-unlabeled learning. Our approach applies to big or small domains,\ncovers head or tail terms, and requires little human effort. Extensive\nexperiments demonstrate that our methods outperform strong baselines and even\nsurpass professional human performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:52:34 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Huang", "Jie", ""], ["Chang", "Kevin Chen-Chuan", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2105.13266", "submitter": "Sean Trott", "authors": "Sean Trott and Benjamin Bergen", "title": "RAW-C: Relatedness of Ambiguous Words--in Context (A New Lexical\n  Resource for English)", "comments": "ACL-IJCNLP 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most words are ambiguous--i.e., they convey distinct meanings in different\ncontexts--and even the meanings of unambiguous words are context-dependent.\nBoth phenomena present a challenge for NLP. Recently, the advent of\ncontextualized word embeddings has led to success on tasks involving lexical\nambiguity, such as Word Sense Disambiguation. However, there are few tasks that\ndirectly evaluate how well these contextualized embeddings accommodate the more\ncontinuous, dynamic nature of word meaning--particularly in a way that matches\nhuman intuitions. We introduce RAW-C, a dataset of graded, human relatedness\njudgments for 112 ambiguous words in context (with 672 sentence pairs total),\nas well as human estimates of sense dominance. The average inter-annotator\nagreement (assessed using a leave-one-annotator-out method) was 0.79. We then\nshow that a measure of cosine distance, computed using contextualized\nembeddings from BERT and ELMo, correlates with human judgments, but that cosine\ndistance also systematically underestimates how similar humans find uses of the\nsame sense of a word to be, and systematically overestimates how similar humans\nfind uses of different-sense homonyms. Finally, we propose a synthesis between\npsycholinguistic theories of the mental lexicon and computational models of\nlexical semantics.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:07:13 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Trott", "Sean", ""], ["Bergen", "Benjamin", ""]]}, {"id": "2105.13318", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg and Shankar Kumar", "title": "Synthetic Data Generation for Grammatical Error Correction with Tagged\n  Corruption Models", "comments": "Proceedings of the 16th Workshop on Innovative Use of NLP for\n  Building Educational Applications, 2021.\n  https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthetic data generation is widely known to boost the accuracy of neural\ngrammatical error correction (GEC) systems, but existing methods often lack\ndiversity or are too simplistic to generate the broad range of grammatical\nerrors made by human writers. In this work, we use error type tags from\nautomatic annotation tools such as ERRANT to guide synthetic data generation.\nWe compare several models that can produce an ungrammatical sentence given a\nclean sentence and an error type tag. We use these models to build a new, large\nsynthetic pre-training data set with error tag frequency distributions matching\na given development set. Our synthetic data set yields large and consistent\ngains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We\nalso show that our approach is particularly effective in adapting a GEC system,\ntrained on mixed native and non-native English, to a native English test set,\neven surpassing real training data consisting of high-quality sentence pairs.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:17:21 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Stahlberg", "Felix", ""], ["Kumar", "Shankar", ""]]}, {"id": "2105.13328", "submitter": "Pratyush Muthukumar", "authors": "Pratyush Muthukumar, Karishma Muthukumar, Deepan Muthirayan, Pramod\n  Khargonekar", "title": "Generative Adversarial Imitation Learning for Empathy-based AI", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial imitation learning (GAIL) is a model-free algorithm\nthat has been shown to provide strong results in imitating complex behaviors in\nhigh-dimensional environments. In this paper, we utilize the GAIL model for\ntext generation to develop empathy-based context-aware conversational AI. Our\nmodel uses an expert trajectory of empathetic prompt-response dialogues which\ncan accurately exhibit the correct empathetic emotion when generating a\nresponse. The Generator of the GAIL model uses the GPT-2 sequential pre-trained\nlanguage model trained on 117 million parameters from 40 GB of internet data.\nWe propose a novel application of an approach used in transfer learning to fine\ntune the GPT-2 model in order to generate concise, user-specific empathetic\nresponses validated against the Discriminator. Our novel GAIL model utilizes a\nsentiment analysis history-based reinforcement learning approach to\nempathetically respond to human interactions in a personalized manner. We find\nthat our model's response scores on various human-generated prompts collected\nfrom the Facebook Empathetic Dialogues dataset outperform baseline\ncounterparts. Moreover, our model improves upon various history-based\nconversational AI models developed recently, as our model's performance over a\nsustained conversation of 3 or more interactions outperform similar\nconversational AI models.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:37:37 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Muthukumar", "Pratyush", ""], ["Muthukumar", "Karishma", ""], ["Muthirayan", "Deepan", ""], ["Khargonekar", "Pramod", ""]]}, {"id": "2105.13385", "submitter": "V\\^ania Mendon\\c{c}a", "authors": "V\\^ania Mendon\\c{c}a (1 and 2), Ricardo Rei (1 and 2 and 3), Luisa\n  Coheur (1 and 2), Alberto Sardinha (1 and 2), Ana L\\'ucia Santos (4 and 5)\n  ((1) INESC-ID Lisboa, (2) Instituto Superior T\\'ecnico, (3) Unbabel AI, (4)\n  Centro de Lingu\\'istica da Universidade de Lisboa, (5) Faculdade de Letras da\n  Universidade de Lisboa)", "title": "Online Learning Meets Machine Translation Evaluation: Finding the Best\n  Systems with the Least Human Effort", "comments": "Accepted to ACL-IJCNLP 2021 Main Conference (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Machine Translation, assessing the quality of a large amount of automatic\ntranslations can be challenging. Automatic metrics are not reliable when it\ncomes to high performing systems. In addition, resorting to human evaluators\ncan be expensive, especially when evaluating multiple systems. To overcome the\nlatter challenge, we propose a novel application of online learning that, given\nan ensemble of Machine Translation systems, dynamically converges to the best\nsystems, by taking advantage of the human feedback available. Our experiments\non WMT'19 datasets show that our online approach quickly converges to the top-3\nranked systems for the language pairs considered, despite the lack of human\nfeedback for many translations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:19:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Mendon\u00e7a", "V\u00e2nia", "", "1 and 2"], ["Rei", "Ricardo", "", "1 and 2 and 3"], ["Coheur", "Luisa", "", "1 and 2"], ["Sardinha", "Alberto", "", "1 and 2"], ["Santos", "Ana L\u00facia", "", "4 and 5"]]}, {"id": "2105.13418", "submitter": "Masoumeh Shafieinejad", "authors": "Masoumeh Shafieinejad and Huseyin Inan and Marcello Hasegawa and\n  Robert Sim", "title": "On Privacy and Confidentiality of Communications in Organizational\n  Graphs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learned models trained on organizational communication data, such as\nemails in an enterprise, carry unique risks of breaching confidentiality, even\nif the model is intended only for internal use. This work shows how\nconfidentiality is distinct from privacy in an enterprise context, and aims to\nformulate an approach to preserving confidentiality while leveraging principles\nfrom differential privacy. The goal is to perform machine learning tasks, such\nas learning a language model or performing topic analysis, using interpersonal\ncommunications in the organization, while not learning about confidential\ninformation shared in the organization. Works that apply differential privacy\ntechniques to natural language processing tasks usually assume independently\ndistributed data, and overlook potential correlation among the records.\nIgnoring this correlation results in a fictional promise of privacy. Naively\nextending differential privacy techniques to focus on group privacy instead of\nrecord-level privacy is a straightforward approach to mitigate this issue. This\napproach, although providing a more realistic privacy-guarantee, is\nover-cautious and severely impacts model utility. We show this gap between\nthese two extreme measures of privacy over two language tasks, and introduce a\nmiddle-ground solution. We propose a model that captures the correlation in the\nsocial network graph, and incorporates this correlation in the privacy\ncalculations through Pufferfish privacy principles.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:45:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Shafieinejad", "Masoumeh", ""], ["Inan", "Huseyin", ""], ["Hasegawa", "Marcello", ""], ["Sim", "Robert", ""]]}, {"id": "2105.13449", "submitter": "Chen Zheng", "authors": "Chen Zheng, Parisa Kordjamshidi", "title": "Relational Gating for \"What If\" Reasoning", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of learning to do procedural reasoning\nover text to answer \"What if...\" questions. We propose a novel relational\ngating network that learns to filter the key entities and relationships and\nlearns contextual and cross representations of both procedure and question for\nfinding the answer. Our relational gating network contains an entity gating\nmodule, relation gating module, and contextual interaction module. These\nmodules help in solving the \"What if...\" reasoning problem. We show that\nmodeling pairwise relationships helps to capture higher-order relations and\nfind the line of reasoning for causes and effects in the procedural\ndescriptions. Our proposed approach achieves the state-of-the-art results on\nthe WIQA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:07:30 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zheng", "Chen", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2105.13456", "submitter": "Tuan Manh Lai", "authors": "Tuan Lai, Heng Ji, ChengXiang Zhai, and Quan Hung Tran", "title": "Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced\n  Collective Inference", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the general news domain, information extraction (IE) from\nbiomedical text requires much broader domain knowledge. However, many previous\nIE methods do not utilize any external knowledge during inference. Due to the\nexponential growth of biomedical publications, models that do not go beyond\ntheir fixed set of parameters will likely fall behind. Inspired by how humans\nlook up relevant information to comprehend a scientific text, we present a\nnovel framework that utilizes external knowledge for joint entity and relation\nextraction named KECI (Knowledge-Enhanced Collective Inference). Given an input\ntext, KECI first constructs an initial span graph representing its initial\nunderstanding of the text. It then uses an entity linker to form a knowledge\ngraph containing relevant background knowledge for the the entity mentions in\nthe text. To make the final predictions, KECI fuses the initial span graph and\nthe knowledge graph into a more refined graph using an attention mechanism.\nKECI takes a collective approach to link mention spans to entities by\nintegrating global relational information into local representations using\ngraph convolutional networks. Our experimental results show that the framework\nis highly effective, achieving new state-of-the-art results in two different\nbenchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse\ndrug event extraction). For example, KECI achieves absolute improvements of\n4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity\nand relation extraction tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:33:34 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 01:46:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lai", "Tuan", ""], ["Ji", "Heng", ""], ["Zhai", "ChengXiang", ""], ["Tran", "Quan Hung", ""]]}, {"id": "2105.13465", "submitter": "Kosuke Yamada", "authors": "Kosuke Yamada, Ryohei Sasano, Koichi Takeda", "title": "Verb Sense Clustering using Contextualized Word Representations for\n  Semantic Frame Induction", "comments": "ACL-IJCNLP 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized word representations have proven useful for various natural\nlanguage processing tasks. However, it remains unclear to what extent these\nrepresentations can cover hand-coded semantic information such as semantic\nframes, which specify the semantic role of the arguments associated with a\npredicate. In this paper, we focus on verbs that evoke different frames\ndepending on the context, and we investigate how well contextualized word\nrepresentations can recognize the difference of frames that the same verb\nevokes. We also explore which types of representation are suitable for semantic\nframe induction. In our experiments, we compare seven different contextualized\nword representations for two English frame-semantic resources, FrameNet and\nPropBank. We demonstrate that several contextualized word representations,\nespecially BERT and its variants, are considerably informative for semantic\nframe induction. Furthermore, we examine the extent to which the contextualized\nrepresentation of a verb can estimate the number of frames that the verb can\nevoke.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:53:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yamada", "Kosuke", ""], ["Sasano", "Ryohei", ""], ["Takeda", "Koichi", ""]]}, {"id": "2105.13466", "submitter": "Kosuke Yamada", "authors": "Kosuke Yamada, Ryohei Sasano, Koichi Takeda", "title": "Semantic Frame Induction using Masked Word Embeddings and Two-Step\n  Clustering", "comments": "ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on semantic frame induction show that relatively high\nperformance has been achieved by using clustering-based methods with\ncontextualized word embeddings. However, there are two potential drawbacks to\nthese methods: one is that they focus too much on the superficial information\nof the frame-evoking verb and the other is that they tend to divide the\ninstances of the same verb into too many different frame clusters. To overcome\nthese drawbacks, we propose a semantic frame induction method using masked word\nembeddings and two-step clustering. Through experiments on the English FrameNet\ndata, we demonstrate that using the masked word embeddings is effective for\navoiding too much reliance on the surface information of frame-evoking verbs\nand that two-step clustering can improve the number of resulting frame clusters\nfor the instances of the same verb.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:00:33 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yamada", "Kosuke", ""], ["Sasano", "Ryohei", ""], ["Takeda", "Koichi", ""]]}, {"id": "2105.13468", "submitter": "Abhiroop Sarkar", "authors": "Abhiroop Sarkar, Mary Sheeran", "title": "Hailstorm : A Statically-Typed, Purely Functional Language for IoT\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1145/3414080.3414092", "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the growing ubiquity of Internet of Things(IoT), more complex logic is\nbeing programmed on resource-constrained IoT devices, almost exclusively using\nthe C programming language. While C provides low-level control over memory, it\nlacks a number of high-level programming abstractions such as higher-order\nfunctions, polymorphism, strong static typing, memory safety, and automatic\nmemory management.\n  We present Hailstorm, a statically-typed, purely functional programming\nlanguage that attempts to address the above problem. It is a high-level\nprogramming language with a strict typing discipline. It supports features like\nhigher-order functions, tail-recursion, and automatic memory management, to\nprogram IoT devices in a declarative manner. Applications running on these\ndevices tend to be heavily dominated by I/O. Hailstorm tracks side effects\nlikeI/O in its type system using resource types. This choice allowed us to\nexplore the design of a purely functional standalone language, in an area where\nit is more common to embed a functional core in an imperative shell. The\nlanguage borrows the combinators of arrowized FRP, but has discrete-time\nsemantics. The design of the full set of combinators is work in progress,\ndriven by examples. So far, we have evaluated Hailstorm by writing standard\nexamples from the literature (earthquake detection, a railway crossing system\nand various other clocked systems), and also running examples on the GRiSP\nembedded systems board, through generation of Erlang.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:09:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sarkar", "Abhiroop", ""], ["Sheeran", "Mary", ""]]}, {"id": "2105.13471", "submitter": "Carlos Aspillaga", "authors": "Carlos Aspillaga, Marcelo Mendoza, Alvaro Soto", "title": "Inspecting the concept knowledge graph encoded by modern language models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of natural language understanding has experienced exponential\nprogress in the last few years, with impressive results in several tasks. This\nsuccess has motivated researchers to study the underlying knowledge encoded by\nthese models. Despite this, attempts to understand their semantic capabilities\nhave not been successful, often leading to non-conclusive, or contradictory\nconclusions among different works. Via a probing classifier, we extract the\nunderlying knowledge graph of nine of the most influential language models of\nthe last years, including word embeddings, text generators, and context\nencoders. This probe is based on concept relatedness, grounded on WordNet. Our\nresults reveal that all the models encode this knowledge, but suffer from\nseveral inaccuracies. Furthermore, we show that the different architectures and\ntraining strategies lead to different model biases. We conduct a systematic\nevaluation to discover specific factors that explain why some concepts are\nchallenging. We hope our insights will motivate the development of models that\ncapture concepts more precisely.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:19:19 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:29:09 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Aspillaga", "Carlos", ""], ["Mendoza", "Marcelo", ""], ["Soto", "Alvaro", ""]]}, {"id": "2105.13479", "submitter": "Mingzhi Yu", "authors": "Mingzhi Yu (1), Diane Litman (1), ((1) University of Pittsburgh)", "title": "Leveraging Linguistic Coordination in Reranking N-Best Candidates For\n  End-to-End Response Selection Using BERT", "comments": "The 34th International FLAIRS Conference Proceedings, May 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Retrieval-based dialogue systems select the best response from many\ncandidates. Although many state-of-the-art models have shown promising\nperformance in dialogue response selection tasks, there is still quite a gap\nbetween R@1 and R@10 performance. To address this, we propose to leverage\nlinguistic coordination (a phenomenon that individuals tend to develop similar\nlinguistic behaviors in conversation) to rerank the N-best candidates produced\nby BERT, a state-of-the-art pre-trained language model. Our results show an\nimprovement in R@1 compared to BERT baselines, demonstrating the utility of\nrepairing machine-generated outputs by leveraging a linguistic theory.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:23:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yu", "Mingzhi", "", "University of Pittsburgh"], ["Litman", "Diane", "", "University of Pittsburgh"]]}, {"id": "2105.13496", "submitter": "Shrey Desai", "authors": "Shrey Desai and Ahmed Aly", "title": "Diagnosing Transformers in Task-Oriented Semantic Parsing", "comments": "Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern task-oriented semantic parsing approaches typically use seq2seq\ntransformers to map textual utterances to semantic frames comprised of intents\nand slots. While these models are empirically strong, their specific strengths\nand weaknesses have largely remained unexplored. In this work, we study BART\nand XLM-R, two state-of-the-art parsers, across both monolingual and\nmultilingual settings. Our experiments yield several key results:\ntransformer-based parsers struggle not only with disambiguating intents/slots,\nbut surprisingly also with producing syntactically-valid frames. Though\npre-training imbues transformers with syntactic inductive biases, we find the\nambiguity of copying utterance spans into frames often leads to tree\ninvalidity, indicating span extraction is a major bottleneck for current\nparsers. However, as a silver lining, we show transformer-based parsers give\nsufficient indicators for whether a frame is likely to be correct or incorrect,\nmaking them easier to deploy in production settings.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:08:53 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Desai", "Shrey", ""], ["Aly", "Ahmed", ""]]}, {"id": "2105.13562", "submitter": "Ashutosh Modi", "authors": "Vijit Malik and Rishabh Sanjay and Shubham Kumar Nigam and Kripa Ghosh\n  and Shouvik Kumar Guha and Arnab Bhattacharya and Ashutosh Modi", "title": "ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment\n  Prediction and Explanation", "comments": "Accepted at ACL 2021, 17 Pages (9 Pages main paper, 4 pages\n  references, 4 pages appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An automated system that could assist a judge in predicting the outcome of a\ncase would help expedite the judicial process. For such a system to be\npractically useful, predictions by the system should be explainable. To promote\nresearch in developing such a system, we introduce ILDC (Indian Legal Documents\nCorpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated\nwith original court decisions. A portion of the corpus (a separate test set) is\nannotated with gold standard explanations by legal experts. Based on ILDC, we\npropose the task of Court Judgment Prediction and Explanation (CJPE). The task\nrequires an automated system to predict an explainable outcome of a case. We\nexperiment with a battery of baseline models for case predictions and propose a\nhierarchical occlusion based model for explainability. Our best prediction\nmodel has an accuracy of 78% versus 94% for human legal experts, pointing\ntowards the complexity of the prediction task. The analysis of explanations by\nthe proposed algorithm reveals a significant difference in the point of view of\nthe algorithm and legal experts for explaining the judgments, pointing towards\nscope for future research.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:07:32 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:17:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Malik", "Vijit", ""], ["Sanjay", "Rishabh", ""], ["Nigam", "Shubham Kumar", ""], ["Ghosh", "Kripa", ""], ["Guha", "Shouvik Kumar", ""], ["Bhattacharya", "Arnab", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2105.13573", "submitter": "El Moatez Billah Nagoudi", "authors": "El Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed", "title": "Investigating Code-Mixed Modern Standard Arabic-Egyptian to English\n  Machine Translation", "comments": "CALCS2021, colocated with NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress in neural machine translation (NMT) has made it possible to\ntranslate successfully between monolingual language pairs where large parallel\ndata exist, with pre-trained models improving performance even further.\nAlthough there exists work on translating in code-mixed settings (where one of\nthe pairs includes text from two or more languages), it is still unclear what\nrecent success in NMT and language modeling exactly means for translating\ncode-mixed text. We investigate one such context, namely MT from code-mixed\nModern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop\nmodels under different conditions, employing both (i) standard end-to-end\nsequence-to-sequence (S2S) Transformers trained from scratch and (ii)\npre-trained S2S language models (LMs). We are able to acquire reasonable\nperformance using only MSA-EN parallel data with S2S models trained from\nscratch. We also find LMs fine-tuned on data from various Arabic dialects to\nhelp the MSAEA-EN task. Our work is in the context of the Shared Task on\nMachine Translation in Code-Switching. Our best model achieves $\\bf25.72$ BLEU,\nplacing us first on the official shared task evaluation for MSAEA-EN.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:38:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Nagoudi", "El Moatez Billah", ""], ["Elmadany", "AbdelRahim", ""], ["Abdul-Mageed", "Muhammad", ""]]}, {"id": "2105.13578", "submitter": "Hieu Tran Trung", "authors": "Hieu Tran, Cuong V. Dinh, Long Phan, and Son T. Nguyen", "title": "Hierarchical Transformer Encoders for Vietnamese Spelling Correction", "comments": "Accepted by The 34th International Conference on Industrial,\n  Engineering & Other Applications of Applied Intelligent Systems(IEA/AIE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Hierarchical Transformer model for Vietnamese\nspelling correction problem. The model consists of multiple Transformer\nencoders and utilizes both character-level and word-level to detect errors and\nmake corrections. In addition, to facilitate future work in Vietnamese spelling\ncorrection tasks, we propose a realistic dataset collected from real-life texts\nfor the problem. We compare our method with other methods and publicly\navailable systems. The proposed method outperforms all of the contemporary\nmethods in terms of recall, precision, and f1-score. A demo version is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 04:09:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tran", "Hieu", ""], ["Dinh", "Cuong V.", ""], ["Phan", "Long", ""], ["Nguyen", "Son T.", ""]]}, {"id": "2105.13607", "submitter": "Yi Zhang", "authors": "Yi Zhang, Lei Li, Yunfang Wu, Qi Su, Xu Sun", "title": "Alleviating the Knowledge-Language Inconsistency: A Study for Deep\n  Commonsense Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge facts are typically represented by relational triples, while we\nobserve that some commonsense facts are represented by the triples whose forms\nare inconsistent with the expression of language. This inconsistency puts\nforward a challenge for pre-trained language models to deal with these\ncommonsense knowledge facts. In this paper, we term such knowledge as deep\ncommonsense knowledge and conduct extensive exploratory experiments on it. We\nshow that deep commonsense knowledge occupies a significant part of commonsense\nknowledge while conventional methods fail to capture it effectively. We further\npropose a novel method to mine the deep commonsense knowledge distributed in\nsentences, alleviating the reliance of conventional methods on the triple\nrepresentation form of knowledge. Experiments demonstrate that the proposal\nsignificantly improves the performance in mining deep commonsense knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:26:19 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:09:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Yi", ""], ["Li", "Lei", ""], ["Wu", "Yunfang", ""], ["Su", "Qi", ""], ["Sun", "Xu", ""]]}, {"id": "2105.13608", "submitter": "Ehsan Kamalloo", "authors": "Ehsan Kamalloo, Mehdi Rezagholizadeh, Peyman Passban, Ali Ghodsi", "title": "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data\n  Augmentation via MiniMax", "comments": "Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language Processing (NLP), finding data augmentation techniques\nthat can produce high-quality human-interpretable examples has always been\nchallenging. Recently, leveraging kNN such that augmented examples are\nretrieved from large repositories of unlabelled sentences has made a step\ntoward interpretable augmentation. Inspired by this paradigm, we introduce\nMinimax-kNN, a sample efficient data augmentation strategy tailored for\nKnowledge Distillation (KD). We exploit a semi-supervised approach based on KD\nto train a model on augmented data. In contrast to existing kNN augmentation\ntechniques that blindly incorporate all samples, our method dynamically selects\na subset of augmented samples that maximizes KL-divergence between the teacher\nand student models. This step aims to extract the most efficient samples to\nensure our augmented data covers regions in the input space with maximum loss\nvalue. We evaluated our technique on several text classification tasks and\ndemonstrated that Minimax-kNN consistently outperforms strong baselines. Our\nresults show that Minimax-kNN requires fewer augmented examples and less\ncomputation to achieve superior performance over the state-of-the-art kNN-based\naugmentation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:32:32 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:18:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kamalloo", "Ehsan", ""], ["Rezagholizadeh", "Mehdi", ""], ["Passban", "Peyman", ""], ["Ghodsi", "Ali", ""]]}, {"id": "2105.13626", "submitter": "Noah Constant", "authors": "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang,\n  Mihir Kale, Adam Roberts, Colin Raffel", "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most widely-used pre-trained language models operate on sequences of tokens\ncorresponding to word or subword units. Encoding text as a sequence of tokens\nrequires a tokenizer, which is typically created as an independent artifact\nfrom the model. Token-free models that instead operate directly on raw text\n(bytes or characters) have many benefits: they can process text in any language\nout of the box, they are more robust to noise, and they minimize technical debt\nby removing complex and error-prone text preprocessing pipelines. Since byte or\ncharacter sequences are longer than token sequences, past work on token-free\nmodels has often introduced new model architectures designed to amortize the\ncost of operating directly on raw text. In this paper, we show that a standard\nTransformer architecture can be used with minimal modifications to process byte\nsequences. We carefully characterize the trade-offs in terms of parameter\ncount, training FLOPs, and inference speed, and show that byte-level models are\ncompetitive with their token-level counterparts. We also demonstrate that\nbyte-level models are significantly more robust to noise and perform better on\ntasks that are sensitive to spelling and pronunciation. As part of our\ncontribution, we release a new set of pre-trained byte-level Transformer models\nbased on the T5 architecture, as well as all code and data used in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:03:22 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Xue", "Linting", ""], ["Barua", "Aditya", ""], ["Constant", "Noah", ""], ["Al-Rfou", "Rami", ""], ["Narang", "Sharan", ""], ["Kale", "Mihir", ""], ["Roberts", "Adam", ""], ["Raffel", "Colin", ""]]}, {"id": "2105.13630", "submitter": "Bin Sun", "authors": "Bin Sun, Shaoxiong Feng, Yiwei Li, Jiamou Liu and Kan Li", "title": "THINK: A Novel Conversation Model for Generating Grammatically Correct\n  and Coherent Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing conversation models that are based on the encoder-decoder\nframework have focused on ways to make the encoder more complicated to enrich\nthe context vectors so as to increase the diversity and informativeness of\ngenerated responses. However, these approaches face two problems. First, the\ndecoder is too simple to effectively utilize the previously generated\ninformation and tends to generate duplicated and self-contradicting responses.\nSecond, the complex encoder tends to generate diverse but incoherent responses\nbecause the complex context vectors may deviate from the original semantics of\ncontext. In this work, we proposed a conversation model named \"THINK\" (Teamwork\ngeneration Hover around Impressive Noticeable Keywords) to make the decoder\nmore complicated and avoid generating duplicated and self-contradicting\nresponses. The model simplifies the context vectors and increases the coherence\nof generated responses in a reasonable way. For this model, we propose Teamwork\ngeneration framework and Semantics Extractor. Compared with other baselines,\nboth automatic and human evaluation showed the advantages of our model.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:11:32 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Bin", ""], ["Feng", "Shaoxiong", ""], ["Li", "Yiwei", ""], ["Liu", "Jiamou", ""], ["Li", "Kan", ""]]}, {"id": "2105.13635", "submitter": "Junnan Liu", "authors": "Junnan Liu, Qianren Mao, Bang Liu, Hao Peng, Hongdong Zhu, Jianxin Li", "title": "Noised Consistency Training for Text Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural abstractive summarization methods often require large quantities of\nlabeled training data. However, labeling large amounts of summarization data is\noften prohibitive due to time, financial, and expertise constraints, which has\nlimited the usefulness of summarization systems to practical applications. In\nthis paper, we argue that this limitation can be overcome by a semi-supervised\napproach: consistency training which is to leverage large amounts of unlabeled\ndata to improve the performance of supervised learning over a small corpus. The\nconsistency regularization semi-supervised learning can regularize model\npredictions to be invariant to small noise applied to input articles. By adding\nnoised unlabeled corpus to help regularize consistency training, this framework\nobtains comparative performance without using the full dataset. In particular,\nwe have verified that leveraging large amounts of unlabeled data decently\nimproves the performance of supervised learning over an insufficient labeled\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:21:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Junnan", ""], ["Mao", "Qianren", ""], ["Liu", "Bang", ""], ["Peng", "Hao", ""], ["Zhu", "Hongdong", ""], ["Li", "Jianxin", ""]]}, {"id": "2105.13648", "submitter": "Yu Bai", "authors": "Yu Bai, Yang Gao, Heyan Huang", "title": "Cross-Lingual Abstractive Summarization with Limited Parallel Resources", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel cross-lingual summarization data is scarce, requiring models to\nbetter use the limited available cross-lingual resources. Existing methods to\ndo so often adopt sequence-to-sequence networks with multi-task frameworks.\nSuch approaches apply multiple decoders, each of which is utilized for a\nspecific task. However, these independent decoders share no parameters, hence\nfail to capture the relationships between the discrete phrases of summaries in\ndifferent languages, breaking the connections in order to transfer the\nknowledge of the high-resource languages to low-resource languages. To bridge\nthese connections, we propose a novel Multi-Task framework for Cross-Lingual\nAbstractive Summarization (MCLAS) in a low-resource setting. Employing one\nunified decoder to generate the sequential concatenation of monolingual and\ncross-lingual summaries, MCLAS makes the monolingual summarization task a\nprerequisite of the cross-lingual summarization (CLS) task. In this way, the\nshared decoder learns interactions involving alignments and summary patterns\nacross languages, which encourages attaining knowledge transfer. Experiments on\ntwo CLS datasets demonstrate that our model significantly outperforms three\nbaseline models in both low-resource and full-dataset scenarios. Moreover,\nin-depth analysis on the generated summaries and attention heads verifies that\ninteractions are learned well using MCLAS, which benefits the CLS task under\nlimited parallel resources.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:51:42 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:26:58 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 14:01:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bai", "Yu", ""], ["Gao", "Yang", ""], ["Huang", "Heyan", ""]]}, {"id": "2105.13650", "submitter": "Huayang Li", "authors": "Wei Bi, Huayang Li, Jiacheng Huang", "title": "Data Augmentation for Text Generation Without Any Augmented Data", "comments": "Accepted into the main conference of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an effective way to improve the performance of many\nneural text generation models. However, current data augmentation methods need\nto define or choose proper data mapping functions that map the original samples\ninto the augmented samples. In this work, we derive an objective to formulate\nthe problem of data augmentation on text generation tasks without any use of\naugmented data constructed by specific mapping functions. Our proposed\nobjective can be efficiently optimized and applied to popular loss functions on\ntext generation tasks with a convergence rate guarantee. Experiments on five\ndatasets of two text generation tasks show that our approach can approximate or\neven surpass popular data augmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:56:51 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bi", "Wei", ""], ["Li", "Huayang", ""], ["Huang", "Jiacheng", ""]]}, {"id": "2105.13662", "submitter": "Simon Razniewski", "authors": "Tuan-Phong Nguyen, Simon Razniewski, Gerhard Weikum", "title": "Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage\n  in Question Answering", "comments": "Demo website: https://ascent.mpi-inf.mpg.de; introductory video:\n  https://youtu.be/qMkJXqu_Yd4", "journal-ref": "ACL 2021 system demonstration", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ASCENT is a fully automated methodology for extracting and consolidating\ncommonsense assertions from web contents (Nguyen et al., WWW 2021). It advances\ntraditional triple-based commonsense knowledge representation by capturing\nsemantic facets like locations and purposes, and composite concepts, i.e.,\nsubgroups and related aspects of subjects. In this demo, we present a web\nportal that allows users to understand its construction process, explore its\ncontent, and observe its impact in the use case of question answering. The demo\nwebsite and an introductory video are both available online.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:17:33 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Nguyen", "Tuan-Phong", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2105.13665", "submitter": "Han Wu", "authors": "Han Wu, Kun Xu, Linfeng Song, Lifeng Jin, Haisong Zhang, Linqi Song", "title": "Domain-Adaptive Pretraining Methods for Dialogue Understanding", "comments": "6 pages, to appear in ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Language models like BERT and SpanBERT pretrained on open-domain data have\nobtained impressive gains on various NLP tasks. In this paper, we probe the\neffectiveness of domain-adaptive pretraining objectives on downstream tasks. In\nparticular, three objectives, including a novel objective focusing on modeling\npredicate-argument relations, are evaluated on two challenging dialogue\nunderstanding tasks. Experimental results demonstrate that domain-adaptive\npretraining with proper objectives can significantly improve the performance of\na strong baseline on these tasks, achieving the new state-of-the-art\nperformances.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:25:27 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wu", "Han", ""], ["Xu", "Kun", ""], ["Song", "Linfeng", ""], ["Jin", "Lifeng", ""], ["Zhang", "Haisong", ""], ["Song", "Linqi", ""]]}, {"id": "2105.13704", "submitter": "Rebekah Baglini", "authors": "Rebekah Baglini and Arthur Hjorth", "title": "Natural Language Processing 4 All (NLP4All): A New Online Platform for\n  Teaching and Learning NLP Concepts", "comments": "Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Natural Language Processing offers new insights into language data across\nalmost all disciplines and domains, and allows us to corroborate and/or\nchallenge existing knowledge. The primary hurdles to widening participation in\nand use of these new research tools are, first, a lack of coding skills in\nstudents across K-16, and in the population at large, and second, a lack of\nknowledge of how NLP-methods can be used to answer questions of disciplinary\ninterest outside of linguistics and/or computer science. To broaden\nparticipation in NLP and improve NLP-literacy, we introduced a new tool\nweb-based tool called Natural Language Processing 4 All (NLP4All). The intended\npurpose of NLP4All is to help teachers facilitate learning with and about NLP,\nby providing easy-to-use interfaces to NLP-methods, data, and analyses, making\nit possible for non- and novice-programmers to learn NLP concepts\ninteractively.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 09:57:22 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Baglini", "Rebekah", ""], ["Hjorth", "Arthur", ""]]}, {"id": "2105.13710", "submitter": "Karin Sevegnani", "authors": "Karin Sevegnani, David M. Howcroft, Ioannis Konstas, Verena Rieser", "title": "OTTers: One-turn Topic Transitions for Open-Domain Dialogue", "comments": null, "journal-ref": "ACL2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed initiative in open-domain dialogue requires a system to pro-actively\nintroduce new topics. The one-turn topic transition task explores how a system\nconnects two topics in a cooperative and coherent manner. The goal of the task\nis to generate a \"bridging\" utterance connecting the new topic to the topic of\nthe previous conversation turn. We are especially interested in commonsense\nexplanations of how a new topic relates to what has been mentioned before. We\nfirst collect a new dataset of human one-turn topic transitions, which we call\nOTTers. We then explore different strategies used by humans when asked to\ncomplete such a task, and notice that the use of a bridging utterance to\nconnect the two topics is the approach used the most. We finally show how\nexisting state-of-the-art text generation models can be adapted to this task\nand examine the performance of these baselines on different splits of the\nOTTers data.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:16:59 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sevegnani", "Karin", ""], ["Howcroft", "David M.", ""], ["Konstas", "Ioannis", ""], ["Rieser", "Verena", ""]]}, {"id": "2105.13728", "submitter": "Oana Cocarascu", "authors": "Oana Cocarascu, Andrew McLean, Paul French, Francesca Toni", "title": "An Explanatory Query-Based Framework for Exploring Academic Expertise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of research institutions heavily relies upon identifying the\nright researchers \"for the job\": researchers may need to identify appropriate\ncollaborators, often from across disciplines; students may need to identify\nsuitable supervisors for projects of their interest; administrators may need to\nmatch funding opportunities with relevant researchers, and so on. Usually,\nfinding potential collaborators in institutions is a time-consuming manual\nsearch task prone to bias. In this paper, we propose a novel query-based\nframework for searching, scoring, and exploring research expertise\nautomatically, based upon processing abstracts of academic publications. Given\nuser queries in natural language, our framework finds researchers with relevant\nexpertise, making use of domain-specific knowledge bases and word embeddings.\nIt also generates explanations for its recommendations. We evaluate our\nframework with an institutional repository of papers from a leading university,\nusing, as baselines, artificial neural networks and transformer-based models\nfor a multilabel classification task to identify authors of publication\nabstracts. We also assess the cross-domain effectiveness of our framework with\na (separate) research funding repository for the same institution. We show that\nour simple method is effective in identifying matches, while satisfying\ndesirable properties and being efficient.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:48:08 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 10:46:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cocarascu", "Oana", ""], ["McLean", "Andrew", ""], ["French", "Paul", ""], ["Toni", "Francesca", ""]]}, {"id": "2105.13782", "submitter": "Marco Gaido", "authors": "Marco Gaido, Beatrice Savoldi, Luisa Bentivogli, Matteo Negri, Marco\n  Turchi", "title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech\n  Translation", "comments": "Accepted in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Having recognized gender bias as a major issue affecting current translation\ntechnologies, researchers have primarily attempted to mitigate it by working on\nthe data front. However, whether algorithmic aspects concur to exacerbate\nunwanted outputs remains so far under-investigated. In this work, we bring the\nanalysis on gender bias in automatic translation onto a seemingly neutral yet\ncritical component: word segmentation. Can segmenting methods influence the\nability to translate gender? Do certain segmentation approaches penalize the\nrepresentation of feminine linguistic markings? We address these questions by\ncomparing 5 existing segmentation strategies on the target side of speech\ntranslation systems. Our results on two language pairs (English-Italian/French)\nshow that state-of-the-art sub-word splitting (BPE) comes at the cost of higher\ngender bias. In light of this finding, we propose a combined approach that\npreserves BPE overall translation quality, while leveraging the higher ability\nof character-based segmentation to properly translate gender.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:38:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Gaido", "Marco", ""], ["Savoldi", "Beatrice", ""], ["Bentivogli", "Luisa", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2105.13792", "submitter": "Tianxiang Sun", "authors": "Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang, Hao Jiang,\n  Zhao Cao, Xuanjing Huang, Xipeng Qiu", "title": "Early Exiting with Ensemble Internal Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a simple technique to accelerate inference of large-scale pre-trained\nmodels, early exiting has gained much attention in the NLP community. It allows\nsamples to exit early at internal classifiers without passing through the\nentire model. Most existing work usually trains the internal classifiers\nindependently and employs an exiting strategy to decide whether or not to exit\nbased on the confidence of the current internal classifier. However, none of\nthese works takes full advantage of the fact that the internal classifiers are\ntrained to solve the same task therefore can be used to construct an ensemble.\nIn this paper, we show that a novel objective function for the training of the\nensemble internal classifiers can be naturally induced from the perspective of\nensemble learning and information theory. The proposed training objective\nconsists of two terms: one for accuracy and the other for the diversity of the\ninternal classifiers. In contrast, the objective used in prior work is exactly\nthe accuracy term of our training objective therefore only optimizes the\naccuracy but not diversity. Further, we propose a simple voting-based strategy\nthat considers predictions of all the past internal classifiers to infer the\ncorrect label and decide whether to exit. Experimental results on various NLP\ntasks show that our proposed objective function and voting-based strategy can\nachieve better accuracy-speed trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:54:11 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Tianxiang", ""], ["Zhou", "Yunhua", ""], ["Liu", "Xiangyang", ""], ["Zhang", "Xinyu", ""], ["Jiang", "Hao", ""], ["Cao", "Zhao", ""], ["Huang", "Xuanjing", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2105.13818", "submitter": "Jaap Jumelet", "authors": "Jaap Jumelet, Milica Deni\\'c, Jakub Szymanik, Dieuwke Hupkes, Shane\n  Steinert-Threlkeld", "title": "Language Models Use Monotonicity to Assess NPI Licensing", "comments": "Published in ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the semantic knowledge of language models (LMs), focusing on\n(1) whether these LMs create categories of linguistic environments based on\ntheir semantic monotonicity properties, and (2) whether these categories play a\nsimilar role in LMs as in human language understanding, using negative polarity\nitem licensing as a case study. We introduce a series of experiments consisting\nof probing with diagnostic classifiers (DCs), linguistic acceptability tasks,\nas well as a novel DC ranking method that tightly connects the probing results\nto the inner workings of the LM. By applying our experimental pipeline to LMs\ntrained on various filtered corpora, we are able to gain stronger insights into\nthe semantic generalizations that are acquired by these models.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:32:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Jumelet", "Jaap", ""], ["Deni\u0107", "Milica", ""], ["Szymanik", "Jakub", ""], ["Hupkes", "Dieuwke", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "2105.13856", "submitter": "Zhuoyuan Mao", "authors": "Zhuoyuan Mao, Prakhar Gupta, Chenhui Chu, Martin Jaggi and Sadao\n  Kurohashi", "title": "Lightweight Cross-Lingual Sentence Representation Learning", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale models for learning fixed-dimensional cross-lingual sentence\nrepresentations like LASER (Artetxe and Schwenk, 2019b) lead to significant\nimprovement in performance on downstream tasks. However, further increases and\nmodifications based on such large-scale models are usually impractical due to\nmemory limitations. In this work, we introduce a lightweight dual-transformer\narchitecture with just 2 layers for generating memory-efficient cross-lingual\nsentence representations. We explore different training tasks and observe that\ncurrent cross-lingual training tasks leave a lot to be desired for this shallow\narchitecture. To ameliorate this, we propose a novel cross-lingual language\nmodel, which combines the existing single-word masked language model with the\nnewly proposed cross-lingual token-level reconstruction task. We further\naugment the training task by the introduction of two computationally-lite\nsentence-level contrastive learning tasks to enhance the alignment of\ncross-lingual sentence representation space, which compensates for the learning\nbottleneck of the lightweight transformer for generative tasks. Our comparisons\nwith competing models on cross-lingual sentence retrieval and multilingual\ndocument classification confirm the effectiveness of the newly proposed\ntraining tasks for a shallow model.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:10:48 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 16:03:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mao", "Zhuoyuan", ""], ["Gupta", "Prakhar", ""], ["Chu", "Chenhui", ""], ["Jaggi", "Martin", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "2105.13857", "submitter": "Emil Carlsson", "authors": "Emil Carlsson, Devdatt Dubhashi, Fredrik D. Johansson", "title": "Learning Approximate and Exact Numeral Systems via Reinforcement\n  Learning", "comments": "CogSci 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work (Xu et al., 2020) has suggested that numeral systems in different\nlanguages are shaped by a functional need for efficient communication in an\ninformation-theoretic sense. Here we take a learning-theoretic approach and\nshow how efficient communication emerges via reinforcement learning. In our\nframework, two artificial agents play a Lewis signaling game where the goal is\nto convey a numeral concept. The agents gradually learn to communicate using\nreinforcement learning and the resulting numeral systems are shown to be\nefficient in the information-theoretic framework of Regier et al. (2015);\nGibson et al. (2017). They are also shown to be similar to human numeral\nsystems of same type. Our results thus provide a mechanistic explanation via\nreinforcement learning of the recent results in Xu et al. (2020) and can\npotentially be generalized to other semantic domains.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:12:10 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Carlsson", "Emil", ""], ["Dubhashi", "Devdatt", ""], ["Johansson", "Fredrik D.", ""]]}, {"id": "2105.13868", "submitter": "Shuhuai Ren", "authors": "Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren\n  Zhou, Xu Sun, Hongxia Yang", "title": "Learning Relation Alignment for Calibrated Cross-modal Retrieval", "comments": "Accepted by ACL-IJCNLP 2021 main conference (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the achievements of large-scale multimodal pre-training approaches,\ncross-modal retrieval, e.g., image-text retrieval, remains a challenging task.\nTo bridge the semantic gap between the two modalities, previous studies mainly\nfocus on word-region alignment at the object level, lacking the matching\nbetween the linguistic relation among the words and the visual relation among\nthe regions. The neglect of such relation consistency impairs the\ncontextualized representation of image-text pairs and hinders the model\nperformance and the interpretability. In this paper, we first propose a novel\nmetric, Intra-modal Self-attention Distance (ISD), to quantify the relation\nconsistency by measuring the semantic distance between linguistic and visual\nrelations. In response, we present Inter-modal Alignment on Intra-modal\nSelf-attentions (IAIS), a regularized training method to optimize the ISD and\ncalibrate intra-modal self-attentions from the two modalities mutually via\ninter-modal alignment. The IAIS regularizer boosts the performance of\nprevailing models on Flickr30k and MS COCO datasets by a considerable margin,\nwhich demonstrates the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:25:49 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 05:16:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ren", "Shuhuai", ""], ["Lin", "Junyang", ""], ["Zhao", "Guangxiang", ""], ["Men", "Rui", ""], ["Yang", "An", ""], ["Zhou", "Jingren", ""], ["Sun", "Xu", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.13871", "submitter": "Songxiang Liu", "authors": "Songxiang Liu, Yuewen Cao, Dan Su, Helen Meng", "title": "DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion", "comments": "Preprint. 8 pages, 2 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Singing voice conversion (SVC) is one promising technique which can enrich\nthe way of human-computer interaction by endowing a computer the ability to\nproduce high-fidelity and expressive singing voice. In this paper, we propose\nDiffSVC, an SVC system based on denoising diffusion probabilistic model.\nDiffSVC uses phonetic posteriorgrams (PPGs) as content features. A denoising\nmodule is trained in DiffSVC, which takes destroyed mel spectrogram produced by\nthe diffusion/forward process and its corresponding step information as input\nto predict the added Gaussian noise. We use PPGs, fundamental frequency\nfeatures and loudness features as auxiliary input to assist the denoising\nprocess. Experiments show that DiffSVC can achieve superior conversion\nperformance in terms of naturalness and voice similarity to current\nstate-of-the-art SVC approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:26:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Songxiang", ""], ["Cao", "Yuewen", ""], ["Su", "Dan", ""], ["Meng", "Helen", ""]]}, {"id": "2105.13878", "submitter": "Xiaonan Li", "authors": "Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, Xuanjing\n  Huang", "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit", "comments": "Accepted to the ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both performance and efficiency are crucial factors for sequence labeling\ntasks in many real-world scenarios. Although the pre-trained models (PTMs) have\nsignificantly improved the performance of various sequence labeling tasks,\ntheir computational cost is expensive. To alleviate this problem, we extend the\nrecent successful early-exit mechanism to accelerate the inference of PTMs for\nsequence labeling tasks. However, existing early-exit mechanisms are\nspecifically designed for sequence-level tasks, rather than sequence labeling.\nIn this paper, we first propose a simple extension of sentence-level early-exit\nfor sequence labeling tasks. To further reduce the computational cost, we also\npropose a token-level early-exit mechanism that allows partial tokens to exit\nearly at different layers. Considering the local dependency inherent in\nsequence labeling, we employed a window-based criterion to decide for a token\nwhether or not to exit. The token-level early-exit brings the gap between\ntraining and inference, so we introduce an extra self-sampling fine-tuning\nstage to alleviate it. The extensive experiments on three popular sequence\nlabeling tasks show that our approach can save up to 66%-75% inference cost\nwith minimal performance degradation. Compared with competitive compressed\nmodels such as DistilBERT, our approach can achieve better performance under\nthe same speed-up ratios of 2X, 3X, and 4X.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:39:26 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:31:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Xiaonan", ""], ["Shao", "Yunfan", ""], ["Sun", "Tianxiang", ""], ["Yan", "Hang", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2105.13880", "submitter": "Yujia Qin", "authors": "Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang,\n  Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou", "title": "Knowledge Inheritance for Pre-trained Language Models", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent explorations of large-scale pre-trained language models (PLMs) such as\nGPT-3 have revealed the power of PLMs with huge amounts of parameters, setting\noff a wave of training ever-larger PLMs. However, training a large-scale PLM\nrequires tremendous amounts of computational resources, which is time-consuming\nand expensive. In addition, existing large-scale PLMs are mainly trained from\nscratch individually, ignoring the availability of many existing well-trained\nPLMs. To this end, we explore the question that how can previously trained PLMs\nbenefit training larger PLMs in future. Specifically, we introduce a novel\npre-training framework named \"knowledge inheritance\" (KI), which combines both\nself-learning and teacher-guided learning to efficiently train larger PLMs.\nSufficient experimental results demonstrate the feasibility of our KI\nframework. We also conduct empirical analyses to explore the effects of teacher\nPLMs' pre-training settings, including model architecture, pre-training data,\netc. Finally, we show that KI can well support lifelong learning and knowledge\ntransfer.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:43:26 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Qin", "Yujia", ""], ["Lin", "Yankai", ""], ["Yi", "Jing", ""], ["Zhang", "Jiajie", ""], ["Han", "Xu", ""], ["Zhang", "Zhengyan", ""], ["Su", "Yusheng", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.13947", "submitter": "Anna Rogers", "authors": "Anna Rogers", "title": "Changing the World by Changing the Data", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP community is currently investing a lot more research and resources into\ndevelopment of deep learning models than training data. While we have made a\nlot of progress, it is now clear that our models learn all kinds of spurious\npatterns, social biases, and annotation artifacts. Algorithmic solutions have\nso far had limited success. An alternative that is being actively discussed is\nmore careful design of datasets so as to deliver specific signals. This\nposition paper maps out the arguments for and against data curation, and argues\nthat fundamentally the point is moot: curation already is and will be\nhappening, and it is changing the world. The question is only how much thought\nwe want to invest into that process.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:17:22 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Rogers", "Anna", ""]]}, {"id": "2105.13959", "submitter": "Sonal Kumar", "authors": "Sreyan Ghosh, Sonal Kumar", "title": "Cisco at SemEval-2021 Task 5: What's Toxic?: Leveraging Transformers for\n  Multiple Toxic Span Extraction from Online Comments", "comments": "9 pages, accepted at SemEval-2021 co-located with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social network platforms are generally used to share positive, constructive,\nand insightful content. However, in recent times, people often get exposed to\nobjectionable content like threat, identity attacks, hate speech, insults,\nobscene texts, offensive remarks or bullying. Existing work on toxic speech\ndetection focuses on binary classification or on differentiating toxic speech\namong a small set of categories. This paper describes the system proposed by\nteam Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared\ntask focusing on detecting the spans in the text that attribute to its\ntoxicity, in English language. We approach this problem primarily in two ways:\na sequence tagging approach and a dependency parsing approach. In our sequence\ntagging approach we tag each token in a sentence under a particular tagging\nscheme. Our best performing architecture in this approach also proved to be our\nbest performing architecture overall with an F1 score of 0.6922, thereby\nplacing us 7th on the final evaluation phase leaderboard. We also explore a\ndependency parsing approach where we extract spans from the input sentence\nunder the supervision of target span boundaries and rank our spans using a\nbiaffine model. Finally, we also provide a detailed analysis of our results and\nmodel performance in our paper.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:27:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ghosh", "Sreyan", ""], ["Kumar", "Sonal", ""]]}, {"id": "2105.13995", "submitter": "Sara Rosenthal", "authors": "Nancy X. R. Wang, Diwakar Mahajan, Marina Danilevsky, Sara Rosenthal", "title": "SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular\n  Data in Scientific Documents (SEM-TAB-FACTS)", "comments": "To Appear in SemEval 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding tables is an important and relevant task that involves\nunderstanding table structure as well as being able to compare and contrast\ninformation within cells. In this paper, we address this challenge by\npresenting a new dataset and tasks that addresses this goal in a shared task in\nSemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in\nScientific Documents (SEM-TAB-FACTS). Our dataset contains 981\nmanually-generated tables and an auto-generated dataset of 1980 tables\nproviding over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS\nfeatured two sub-tasks. In sub-task A, the goal was to determine if a statement\nis supported, refuted or unknown in relation to a table. In sub-task B, the\nfocus was on identifying the specific cells of a table that provide evidence\nfor the statement. 69 teams signed up to participate in the task with 19\nsuccessful submissions to subtask A and 12 successful submissions to subtask B.\nWe present our results and main findings from the competition.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:21:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wang", "Nancy X. R.", ""], ["Mahajan", "Diwakar", ""], ["Danilevsky", "Marina", ""], ["Rosenthal", "Sara", ""]]}, {"id": "2105.14002", "submitter": "Mycal Tucker", "authors": "Mycal Tucker, Peng Qian, and Roger Levy", "title": "What if This Modified That? Syntactic Interventions via Counterfactual\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural language models exhibit impressive performance on a variety of tasks,\nbut their internal reasoning may be difficult to understand. Prior art aims to\nuncover meaningful properties within model representations via probes, but it\nis unclear how faithfully such probes portray information that the models\nactually use. To overcome such limitations, we propose a technique, inspired by\ncausal analysis, for generating counterfactual embeddings within models. In\nexperiments testing our technique, we produce evidence that suggests some\nBERT-based models use a tree-distance-like representation of syntax in\ndownstream prediction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:27:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tucker", "Mycal", ""], ["Qian", "Peng", ""], ["Levy", "Roger", ""]]}, {"id": "2105.14013", "submitter": "Ankit Parag Shah", "authors": "Ankit Shah, Srishti Singh, Shih-Yen Tao", "title": "Feature extraction and evaluation for BioMedical Question Answering", "comments": "An exploratory analysis for BioMedical Question Answering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we present our work on the BioASQ pipeline. The goal is to\nanswer four types of questions: summary, yes/no, factoids, and list. Our goal\nis to empirically evaluate different modules involved: the feature extractor\nand the sentence selection block. We used our pipeline to test the\neffectiveness of each module for all kinds of question types and perform error\nanalysis. We defined metrics that are useful for future research related to the\nBioASQ pipeline critical to improve the performance of the training pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:41:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Shah", "Ankit", ""], ["Singh", "Srishti", ""], ["Tao", "Shih-Yen", ""]]}, {"id": "2105.14064", "submitter": "Chien-Sheng Wu", "authors": "Chien-Sheng Wu and Linqing Liu and Wenhao Liu and Pontus Stenetorp and\n  Caiming Xiong", "title": "Controllable Abstractive Dialogue Summarization with Sketch Supervision", "comments": "ACL-Findings 2021. Code is released at\n  https://github.com/salesforce/ConvSumm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to improve abstractive dialogue summarization quality\nand, at the same time, enable granularity control. Our model has two primary\ncomponents and stages: 1) a two-stage generation strategy that generates a\npreliminary summary sketch serving as the basis for the final summary. This\nsummary sketch provides a weakly supervised signal in the form of\npseudo-labeled interrogative pronoun categories and key phrases extracted using\na constituency parser. 2) A simple strategy to control the granularity of the\nfinal summary, in that our model can automatically determine or control the\nnumber of generated summary sentences for a given dialogue by predicting and\nhighlighting different text spans from the source text. Our model achieves\nstate-of-the-art performance on the largest dialogue summarization corpus\nSAMSum, with as high as 50.79 in ROUGE-L score. In addition, we conduct a case\nstudy and show competitive human evaluation results and controllability to\nhuman-annotated summaries.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:05:36 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 05:16:05 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wu", "Chien-Sheng", ""], ["Liu", "Linqing", ""], ["Liu", "Wenhao", ""], ["Stenetorp", "Pontus", ""], ["Xiong", "Caiming", ""]]}, {"id": "2105.14078", "submitter": "Xiaotao Gu", "authors": "Xiaotao Gu, Zihan Wang, Zhenyu Bi, Yu Meng, Liyuan Liu, Jiawei Han,\n  Jingbo Shang", "title": "UCPhrase: Unsupervised Context-aware Quality Phrase Tagging", "comments": "KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467397", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying and understanding quality phrases from context is a fundamental\ntask in text mining. The most challenging part of this task arguably lies in\nuncommon, emerging, and domain-specific phrases. The infrequent nature of these\nphrases significantly hurts the performance of phrase mining methods that rely\non sufficient phrase occurrences in the input corpus. Context-aware tagging\nmodels, though not restricted by frequency, heavily rely on domain experts for\neither massive sentence-level gold labels or handcrafted gazetteers. In this\nwork, we propose UCPhrase, a novel unsupervised context-aware quality phrase\ntagger. Specifically, we induce high-quality phrase spans as silver labels from\nconsistently co-occurring word sequences within each document. Compared with\ntypical context-agnostic distant supervision based on existing knowledge bases\n(KBs), our silver labels root deeply in the input domain and context, thus\nhaving unique advantages in preserving contextual completeness and capturing\nemerging, out-of-KB phrases. Training a conventional neural tagger based on\nsilver labels usually faces the risk of overfitting phrase surface names.\nAlternatively, we observe that the contextualized attention maps generated from\na transformer-based neural language model effectively reveal the connections\nbetween words in a surface-agnostic way. Therefore, we pair such attention maps\nwith the silver labels to train a lightweight span prediction model, which can\nbe applied to new input to recognize (unseen) quality phrases regardless of\ntheir surface names or frequency. Thorough experiments on various tasks and\ndatasets, including corpus-level phrase ranking, document-level keyphrase\nextraction, and sentence-level phrase tagging, demonstrate the superiority of\nour design over state-of-the-art pre-trained, unsupervised, and distantly\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:44:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gu", "Xiaotao", ""], ["Wang", "Zihan", ""], ["Bi", "Zhenyu", ""], ["Meng", "Yu", ""], ["Liu", "Liyuan", ""], ["Han", "Jiawei", ""], ["Shang", "Jingbo", ""]]}, {"id": "2105.14082", "submitter": "Aryaman Arora", "authors": "Aryaman Arora, Adam Farris, Gopalakrishnan R, Samopriya Basu", "title": "Bh\\=a$\\unicode{x1E63}$\\=acitra: Visualising the dialect geography of\n  South Asia", "comments": "5 pages, 4 figures. To appear at LChange'21 workshop located at ACL\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present Bh\\=a$\\unicode{x1E63}$\\=acitra, a dialect mapping system for South\nAsia built on a database of linguistic studies of languages of the region\nannotated for topic and location data. We analyse language coverage and look\ntowards applications to typology by visualising example datasets. The\napplication is not only meant to be useful for feature mapping, but also serves\nas a new kind of interactive bibliography for linguists of South Asian\nlanguages.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:52:42 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 13:37:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Arora", "Aryaman", ""], ["Farris", "Adam", ""], ["R", "Gopalakrishnan", ""], ["Basu", "Samopriya", ""]]}, {"id": "2105.14095", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, Weijie J. Su", "title": "Weighted Training for Cross-Task Learning", "comments": "21 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted\ntraining algorithm for cross-task learning based on minimizing a\nrepresentation-based task distance between the source and target tasks. We show\nthat TAWT is easy to implement, is computationally efficient, requires little\nhyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees.\nThe effectiveness of TAWT is corroborated through extensive experiments with\nBERT on four sequence tagging tasks in natural language processing (NLP),\nincluding part-of-speech (PoS) tagging, chunking, predicate detection, and\nnamed entity recognition (NER). As a byproduct, the proposed\nrepresentation-based task distance allows one to reason in a theoretically\nprincipled way about several critical aspects of cross-task learning, such as\nthe choice of the source data and the impact of fine-tuning\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:27:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Shuxiao", ""], ["Crammer", "Koby", ""], ["He", "Hangfeng", ""], ["Roth", "Dan", ""], ["Su", "Weijie J.", ""]]}, {"id": "2105.14097", "submitter": "Pawe{\\l} Wawrzy\\'nski", "authors": "Grzegorz Rype\\'s\\'c, {\\L}ukasz Lepak, Pawe{\\l} Wawrzy\\'nski", "title": "Reinforcement Learning for on-line Sequence Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems in the processing of sound and natural language, as well\nas in other areas, can be reduced to simultaneously reading an input sequence\nand writing an output sequence of generally different length. There are well\ndeveloped methods that produce the output sequence based on the entirely known\ninput. However, efficient methods that enable such transformations on-line do\nnot exist. In this paper we introduce an architecture that learns with\nreinforcement to make decisions about whether to read a token or write another\ntoken. This architecture is able to transform potentially infinite sequences\non-line. In an experimental study we compare it with state-of-the-art methods\nfor neural machine translation. While it produces slightly worse translations\nthan Transformer, it outperforms the autoencoder with attention, even though\nour architecture translates texts on-line thereby solving a more difficult\nproblem than both reference methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:31:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rype\u015b\u0107", "Grzegorz", ""], ["Lepak", "\u0141ukasz", ""], ["Wawrzy\u0144ski", "Pawe\u0142", ""]]}, {"id": "2105.14103", "submitter": "Hanlin Goh", "authors": "Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin\n  Goh, Ruixiang Zhang, Josh Susskind", "title": "An Attention Free Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:45:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Talbott", "Walter", ""], ["Srivastava", "Nitish", ""], ["Huang", "Chen", ""], ["Goh", "Hanlin", ""], ["Zhang", "Ruixiang", ""], ["Susskind", "Josh", ""]]}, {"id": "2105.14115", "submitter": "Antonios Anastasopoulos", "authors": "Arnab Debnath, Navid Rajabi, Fardina Fathmiul Alam, Antonios\n  Anastasopoulos", "title": "Towards More Equitable Question Answering Systems: How Much More Data Do\n  You Need?", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) in English has been widely explored, but multilingual\ndatasets are relatively new, with several methods attempting to bridge the gap\nbetween high- and low-resourced languages using data augmentation through\ntranslation and cross-lingual transfer. In this project, we take a step back\nand study which approaches allow us to take the most advantage of existing\nresources in order to produce QA systems in many languages. Specifically, we\nperform extensive analysis to measure the efficacy of few-shot approaches\naugmented with automatic translations and permutations of\ncontext-question-answer pairs. In addition, we make suggestions for future\ndataset development efforts that make better use of a fixed annotation budget,\nwith a goal of increasing the language coverage of QA datasets and systems.\nCode and data for reproducing our experiments are available here:\nhttps://github.com/NavidRajabi/EMQA.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:32:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Debnath", "Arnab", ""], ["Rajabi", "Navid", ""], ["Alam", "Fardina Fathmiul", ""], ["Anastasopoulos", "Antonios", ""]]}, {"id": "2105.14150", "submitter": "Kun Qian", "authors": "Kun Qian, Ahmad Beirami, Zhouhan Lin, Ankita De, Alborz Geramifard,\n  Zhou Yu, Chinnadhurai Sankar", "title": "Annotation Inconsistency and Entity Bias in MultiWOZ", "comments": "Accepted by SIGDIAL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:09:06 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 06:06:54 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Qian", "Kun", ""], ["Beirami", "Ahmad", ""], ["Lin", "Zhouhan", ""], ["De", "Ankita", ""], ["Geramifard", "Alborz", ""], ["Yu", "Zhou", ""], ["Sankar", "Chinnadhurai", ""]]}, {"id": "2105.14167", "submitter": "Zeming Chen", "authors": "Zeming Chen, Qiyue Gao, Lawrence S. Moss", "title": "NeuralLog: Natural Language Inference with Joint Neural and Logical\n  Reasoning", "comments": "8 pages, 4 figures, The 10th Joint Conference on Lexical and\n  Computational Semantics (*SEM2021) @ ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning (DL) based language models achieve high performance on various\nbenchmarks for Natural Language Inference (NLI). And at this time, symbolic\napproaches to NLI are receiving less attention. Both approaches (symbolic and\nDL) have their advantages and weaknesses. However, currently, no method\ncombines them in a system to solve the task of NLI. To merge symbolic and deep\nlearning methods, we propose an inference framework called NeuralLog, which\nutilizes both a monotonicity-based logical inference engine and a neural\nnetwork language model for phrase alignment. Our framework models the NLI task\nas a classic search problem and uses the beam search algorithm to search for\noptimal inference paths. Experiments show that our joint logic and neural\ninference system improves accuracy on the NLI task and can achieve state-of-art\naccuracy on the SICK and MED datasets.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:02:40 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 04:52:20 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 05:36:30 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chen", "Zeming", ""], ["Gao", "Qiyue", ""], ["Moss", "Lawrence S.", ""]]}, {"id": "2105.14174", "submitter": "Shiwan Zhao Mr", "authors": "Mengting Hu, Shiwan Zhao, Honglei Guo, Chao Xue, Hang Gao, Tiegang\n  Gao, Renhong Cheng, Zhong Su", "title": "Multi-Label Few-Shot Learning for Aspect Category Detection", "comments": "Accepted by ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect category detection (ACD) in sentiment analysis aims to identify the\naspect categories mentioned in a sentence. In this paper, we formulate ACD in\nthe few-shot learning scenario. However, existing few-shot learning approaches\nmainly focus on single-label predictions. These methods can not work well for\nthe ACD task since a sentence may contain multiple aspect categories.\nTherefore, we propose a multi-label few-shot learning method based on the\nprototypical network. To alleviate the noise, we design two effective attention\nmechanisms. The support-set attention aims to extract better prototypes by\nremoving irrelevant aspects. The query-set attention computes multiple\nprototype-specific representations for each query instance, which are then used\nto compute accurate distances with the corresponding prototypes. To achieve\nmulti-label inference, we further learn a dynamic threshold per instance by a\npolicy network. Extensive experimental results on three datasets demonstrate\nthat the proposed method significantly outperforms strong baselines.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:56:11 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hu", "Mengting", ""], ["Zhao", "Shiwan", ""], ["Guo", "Honglei", ""], ["Xue", "Chao", ""], ["Gao", "Hang", ""], ["Gao", "Tiegang", ""], ["Cheng", "Renhong", ""], ["Su", "Zhong", ""]]}, {"id": "2105.14189", "submitter": "Lingzhi Wang", "authors": "Lingzhi Wang, Xingshan Zeng, Kam-Fai Wong", "title": "Quotation Recommendation and Interpretation Based on Transformation from\n  Queries to Quotations", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help individuals express themselves better, quotation recommendation is\nreceiving growing attention. Nevertheless, most prior efforts focus on modeling\nquotations and queries separately and ignore the relationship between the\nquotations and the queries. In this work, we introduce a transformation matrix\nthat directly maps the query representations to quotation representations. To\nbetter learn the mapping relationship, we employ a mapping loss that minimizes\nthe distance of two semantic spaces (one for quotation and another for\nmapped-query). Furthermore, we explore using the words in history queries to\ninterpret the figurative language of quotations, where quotation-aware\nattention is applied on top of history queries to highlight the indicator\nwords. Experiments on two datasets in English and Chinese show that our model\noutperforms previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 07:25:59 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:07:23 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Lingzhi", ""], ["Zeng", "Xingshan", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "2105.14207", "submitter": "Takuma Udagawa", "authors": "Takuma Udagawa and Akiko Aizawa", "title": "Maintaining Common Ground in Dynamic Environments", "comments": "Accepted at TACL; pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common grounding is the process of creating and maintaining mutual\nunderstandings, which is a critical aspect of sophisticated human\ncommunication. While various task settings have been proposed in existing\nliterature, they mostly focus on creating common ground under static context\nand ignore the aspect of maintaining them overtime under dynamic context. In\nthis work, we propose a novel task setting to study the ability of both\ncreating and maintaining common ground in dynamic environments. Based on our\nminimal task formulation, we collected a large-scale dataset of 5,617 dialogues\nto enable fine-grained evaluation and analysis of various dialogue systems.\nThrough our dataset analyses, we highlight novel challenges introduced in our\nsetting, such as the usage of complex spatio-temporal expressions to create and\nmaintain common ground. Finally, we conduct extensive experiments to assess the\ncapabilities of our baseline dialogue system and discuss future prospects of\nour research.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:14:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Udagawa", "Takuma", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2105.14209", "submitter": "Zuchao Li", "authors": "Kevin Parnow, Zuchao Li, and Hai Zhao", "title": "Grammatical Error Correction as GAN-like Sequence Labeling", "comments": "Accepted by ACL21, Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Grammatical Error Correction (GEC), sequence labeling models enjoy fast\ninference compared to sequence-to-sequence models; however, inference in\nsequence labeling GEC models is an iterative process, as sentences are passed\nto the model for multiple rounds of correction, which exposes the model to\nsentences with progressively fewer errors at each round. Traditional GEC models\nlearn from sentences with fixed error rates. Coupling this with the iterative\ncorrection process causes a mismatch between training and inference that\naffects final performance. In order to address this mismatch, we propose a\nGAN-like sequence labeling model, which consists of a grammatical error\ndetector as a discriminator and a grammatical error labeler with Gumbel-Softmax\nsampling as a generator. By sampling from real error distributions, our errors\nare more genuine compared to traditional synthesized GEC errors, thus\nalleviating the aforementioned mismatch and allowing for better training. Our\nresults on several evaluation benchmarks demonstrate that our proposed approach\nis effective and improves the previous state-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:39:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Parnow", "Kevin", ""], ["Li", "Zuchao", ""], ["Zhao", "Hai", ""]]}, {"id": "2105.14210", "submitter": "Chen Zhang", "authors": "Fang Ma, Chen Zhang, Dawei Song", "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification", "comments": "7 pages, 2 figures, 4 tables, accepted to Findings of ACL 2021. Repo:\n  https://github.com/BD-MF/POS4ASC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect sentiment classification (ASC) aims at determining sentiments\nexpressed towards different aspects in a sentence. While state-of-the-art ASC\nmodels have achieved remarkable performance, they are recently shown to suffer\nfrom the issue of robustness. Particularly in two common scenarios: when\ndomains of test and training data are different (out-of-domain scenario) or\ntest data is adversarially perturbed (adversarial scenario), ASC models may\nattend to irrelevant words and neglect opinion expressions that truly describe\ndiverse aspects. To tackle the challenge, in this paper, we hypothesize that\nposition bias (i.e., the words closer to a concerning aspect would carry a\nhigher degree of importance) is crucial for building more robust ASC models by\nreducing the probability of mis-attending. Accordingly, we propose two\nmechanisms for capturing position bias, namely position-biased weight and\nposition-biased dropout, which can be flexibly injected into existing models to\nenhance representations for classification. Experiments conducted on\nout-of-domain and adversarial datasets demonstrate that our proposed approaches\nlargely improve the robustness and effectiveness of current models.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 04:41:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ma", "Fang", ""], ["Zhang", "Chen", ""], ["Song", "Dawei", ""]]}, {"id": "2105.14214", "submitter": "Qingfeng Lan", "authors": "Qingfeng Lan, Luke Kumar, Martha White, Alona Fyshe", "title": "Predictive Representation Learning for Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To effectively perform the task of next-word prediction, long short-term\nmemory networks (LSTMs) must keep track of many types of information. Some\ninformation is directly related to the next word's identity, but some is more\nsecondary (e.g. discourse-level features or features of downstream words).\nCorrelates of secondary information appear in LSTM representations even though\nthey are not part of an \\emph{explicitly} supervised prediction task. In\ncontrast, in reinforcement learning (RL), techniques that explicitly supervise\nrepresentations to predict secondary information have been shown to be\nbeneficial. Inspired by that success, we propose Predictive Representation\nLearning (PRL), which explicitly constrains LSTMs to encode specific\npredictions, like those that might need to be learned implicitly. We show that\nPRL 1) significantly improves two strong language modeling methods, 2)\nconverges more quickly, and 3) performs better when data is limited. Our work\nshows that explicitly encoding a simple predictive task facilitates the search\nfor a more effective language model.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:03:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lan", "Qingfeng", ""], ["Kumar", "Luke", ""], ["White", "Martha", ""], ["Fyshe", "Alona", ""]]}, {"id": "2105.14220", "submitter": "Rifat Shahriyar", "authors": "Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed\n  Mehrab, Md. Mahim Anjum Haque, Tahmid Hasan, Wasi Uddin Ahmad, Anindya Iqbal,\n  Rifat Shahriyar", "title": "CoDesc: A Large Code-Description Parallel Dataset", "comments": "Findings of the Association for Computational Linguistics, ACL 2021\n  (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translation between natural language and source code can help software\ndevelopment by enabling developers to comprehend, ideate, search, and write\ncomputer programs in natural language. Despite growing interest from the\nindustry and the research community, this task is often difficult due to the\nlack of large standard datasets suitable for training deep neural models,\nstandard noise removal methods, and evaluation benchmarks. This leaves\nresearchers to collect new small-scale datasets, resulting in inconsistencies\nacross published works. In this study, we present CoDesc -- a large parallel\ndataset composed of 4.2 million Java methods and natural language descriptions.\nWith extensive analysis, we identify and remove prevailing noise patterns from\nthe dataset. We demonstrate the proficiency of CoDesc in two complementary\ntasks for code-description pairs: code summarization and code search. We show\nthat the dataset helps improve code search by up to 22\\% and achieves the new\nstate-of-the-art in code summarization. Furthermore, we show CoDesc's\neffectiveness in pre-training--fine-tuning setup, opening possibilities in\nbuilding pretrained language models for Java. To facilitate future research, we\nrelease the dataset, a data processing tool, and a benchmark at\n\\url{https://github.com/csebuetnlp/CoDesc}.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:40:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hasan", "Masum", ""], ["Muttaqueen", "Tanveer", ""], ["Ishtiaq", "Abdullah Al", ""], ["Mehrab", "Kazi Sajeed", ""], ["Haque", "Md. Mahim Anjum", ""], ["Hasan", "Tahmid", ""], ["Ahmad", "Wasi Uddin", ""], ["Iqbal", "Anindya", ""], ["Shahriyar", "Rifat", ""]]}, {"id": "2105.14241", "submitter": "Linzi Xing", "authors": "Linzi Xing, Wen Xiao, Giuseppe Carenini", "title": "Demoting the Lead Bias in News Summarization via Alternating Adversarial\n  Learning", "comments": "Accepted at ACL-IJCNLP 2021 main conference (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In news articles the lead bias is a common phenomenon that usually dominates\nthe learning signals for neural extractive summarizers, severely limiting their\nperformance on data with different or even no bias. In this paper, we introduce\na novel technique to demote lead bias and make the summarizer focus more on the\ncontent semantics. Experiments on two news corpora with different degrees of\nlead bias show that our method can effectively demote the model's learned lead\nbias and improve its generality on out-of-distribution data, with little to no\nperformance loss on in-distribution data.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 07:40:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xing", "Linzi", ""], ["Xiao", "Wen", ""], ["Carenini", "Giuseppe", ""]]}, {"id": "2105.14242", "submitter": "Tae Hwan Jung", "authors": "Tae-Hwan Jung", "title": "CommitBERT: Commit Message Generation Using Pre-Trained Programming\n  Language Model", "comments": "8 pages, 3 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commit message is a document that summarizes source code changes in natural\nlanguage. A good commit message clearly shows the source code changes, so this\nenhances collaboration between developers. Therefore, our work is to develop a\nmodel that automatically writes the commit message.\n  To this end, we release 345K datasets consisting of code modification and\ncommit messages in six programming languages (Python, PHP, Go, Java,\nJavaScript, and Ruby). Similar to the neural machine translation (NMT) model,\nusing our dataset, we feed the code modification to the encoder input and the\ncommit message to the decoder input and measure the result of the generated\ncommit message with BLEU-4.\n  Also, we propose the following two training methods to improve the result of\ngenerating the commit message: (1) A method of preprocessing the input to feed\nthe code modification to the encoder input. (2) A method that uses an initial\nweight suitable for the code domain to reduce the gap in contextual\nrepresentation between programming language (PL) and natural language (NL).\nTraining code, dataset, and pre-trained weights are available at\nhttps://github.com/graykode/commit-autosuggestions\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 07:48:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jung", "Tae-Hwan", ""]]}, {"id": "2105.14274", "submitter": "Dojun Park", "authors": "Dojun Park, Youngjin Jang and Harksoo Kim", "title": "Korean-English Machine Translation with Multiple Tokenization Strategy", "comments": "KCC2021 Undergraduate/Junior Thesis Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work was conducted to find out how tokenization methods affect the\ntraining results of machine translation models. In this work, alphabet\ntokenization, morpheme tokenization, and BPE tokenization were applied to\nKorean as the source language and English as the target language respectively,\nand the comparison experiment was conducted by repeating 50,000 epochs of each\n9 models using the Transformer neural network. As a result of measuring the\nBLEU scores of the experimental models, the model that applied BPE tokenization\nto Korean and morpheme tokenization to English recorded 35.73, showing the best\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 11:31:59 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:03:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Park", "Dojun", ""], ["Jang", "Youngjin", ""], ["Kim", "Harksoo", ""]]}, {"id": "2105.14276", "submitter": "Rossella Arcucci Dr", "authors": "Robin Hendrickx, Rossella Arcucci, Julio Amador D{\\i}az Lopez, Yi-Ke\n  Guo, and Mark Kennedy", "title": "Correcting public opinion trends through Bayesian data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring public opinion is a key focus during democratic elections, enabling\ncandidates to gauge their popularity and alter their campaign strategies\naccordingly. Traditional survey polling remains the most popular estimation\ntechnique, despite its cost and time intensity, measurement errors, lack of\nreal-time capabilities and lagged representation of public opinion. In recent\nyears, Twitter opinion mining has attempted to combat these issues. Despite\nachieving promising results, it experiences its own set of shortcomings such as\nan unrepresentative sample population and a lack of long term stability. This\npaper aims to merge data from both these techniques using Bayesian data\nassimilation to arrive at a more accurate estimate of true public opinion for\nthe Brexit referendum. This paper demonstrates the effectiveness of the\nproposed approach using Twitter opinion data and survey data from trusted\npollsters. Firstly, the possible existence of a time gap of 16 days between the\ntwo data sets is identified. This gap is subsequently incorporated into a\nproposed assimilation architecture. This method was found to adequately\nincorporate information from both sources and measure a strong upward trend in\nLeave support leading up to the Brexit referendum. The proposed technique\nprovides useful estimates of true opinion, which is essential to future opinion\nmeasurement and forecasting research.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 11:39:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hendrickx", "Robin", ""], ["Arcucci", "Rossella", ""], ["Lopez", "Julio Amador D\u0131az", ""], ["Guo", "Yi-Ke", ""], ["Kennedy", "Mark", ""]]}, {"id": "2105.14277", "submitter": "Dojun Park", "authors": "Dojun Park, Youngjin Jang and Harksoo Kim", "title": "Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of\n  Machine Translation Models", "comments": "Journal of KIISE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intrinsic evaluation by humans for the performance of natural language\ngeneration models is conducted to overcome the fact that the quality of\ngenerated sentences cannot be fully represented by only extrinsic evaluation.\nNevertheless, existing intrinsic evaluations have a large score deviation\naccording to the evaluator's criteria. In this paper, we propose Grammar\nAccuracy Evaluation (GAE) that can provide specific evaluating criteria. As a\nresult of analyzing the quality of machine translation by BLEU and GAE, it was\nconfirmed that the BLEU score does not represent the absolute performance of\nmachine translation models and that GAE compensates for the shortcomings of\nBLEU with a flexible evaluation on alternative synonyms and changes in sentence\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 11:40:51 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:07:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Park", "Dojun", ""], ["Jang", "Youngjin", ""], ["Kim", "Harksoo", ""]]}, {"id": "2105.14289", "submitter": "Keqing He", "authors": "Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu,\n  Huixing Jiang and Weiran Xu", "title": "Modeling Discriminative Representations for Out-of-Domain Detection with\n  Supervised Contrastive Learning", "comments": "Accepted by ACL2021", "journal-ref": "ACL2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting Out-of-Domain (OOD) or unknown intents from user queries is\nessential in a task-oriented dialog system. A key challenge of OOD detection is\nto learn discriminative semantic features. Traditional cross-entropy loss only\nfocuses on whether a sample is correctly classified, and does not explicitly\ndistinguish the margins between categories. In this paper, we propose a\nsupervised contrastive learning objective to minimize intra-class variance by\npulling together in-domain intents belonging to the same class and maximize\ninter-class variance by pushing apart samples from different classes. Besides,\nwe employ an adversarial augmentation mechanism to obtain pseudo diverse views\nof a sample in the latent space. Experiments on two public datasets prove the\neffectiveness of our method capturing discriminative representations for OOD\ndetection.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 12:54:22 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zeng", "Zhiyuan", ""], ["He", "Keqing", ""], ["Yan", "Yuanmeng", ""], ["Liu", "Zijun", ""], ["Wu", "Yanan", ""], ["Xu", "Hong", ""], ["Jiang", "Huixing", ""], ["Xu", "Weiran", ""]]}, {"id": "2105.14300", "submitter": "Zujie Liang", "authors": "Zujie Liang, Haifeng Hu and Jiaying Zhu", "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual\n  Question Answering", "comments": "Accepted by ACM SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462981", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 13:48:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:46:24 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liang", "Zujie", ""], ["Hu", "Haifeng", ""], ["Zhu", "Jiaying", ""]]}, {"id": "2105.14309", "submitter": "Chao Feng", "authors": "Chao Feng", "title": "A Simple Voting Mechanism for Online Sexist Content Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the participation of the MiniTrue team in the EXIST 2021\nChallenge on the sexism detection in social media task for English and Spanish.\nOur approach combines the language models with a simple voting mechanism for\nthe sexist label prediction. For this, three BERT based models and a voting\nfunction are used. Experimental results show that our final model with the\nvoting function has achieved the best results among our four models, which\nmeans that our voting mechanism brings an extra benefit to our system.\nNevertheless, we also observe that our system is robust to data sources and\nlanguages.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:25:20 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Feng", "Chao", ""]]}, {"id": "2105.14313", "submitter": "Keqing He", "authors": "Yanan Wu, Zhiyuan Zeng, Keqing He, Hong Xu, Yuanmeng Yan, Huixing\n  Jiang and Weiran Xu", "title": "Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in\n  the Task-Oriented Dialogue System", "comments": "Accepted by ACL2021", "journal-ref": "ACL2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing slot filling models can only recognize pre-defined in-domain slot\ntypes from a limited slot set. In the practical application, a reliable\ndialogue system should know what it does not know. In this paper, we introduce\na new task, Novel Slot Detection (NSD), in the task-oriented dialogue system.\nNSD aims to discover unknown or out-of-domain slot types to strengthen the\ncapability of a dialogue system based on in-domain training data. Besides, we\nconstruct two public NSD datasets, propose several strong NSD baselines, and\nestablish a benchmark for future work. Finally, we conduct exhaustive\nexperiments and qualitative analysis to comprehend key challenges and provide\nnew guidance for future directions.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:46:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Yanan", ""], ["Zeng", "Zhiyuan", ""], ["He", "Keqing", ""], ["Xu", "Hong", ""], ["Yan", "Yuanmeng", ""], ["Jiang", "Huixing", ""], ["Xu", "Weiran", ""]]}, {"id": "2105.14347", "submitter": "Peratham Wiriyathammabhum Mr.", "authors": "Peratham Wiriyathammabhum", "title": "Is Sluice Resolution really just Question Answering?", "comments": "Extended Abstract at the The First Workshop on Understanding Implicit\n  and Underspecified Language @ ACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sluice resolution is a problem where a system needs to output the\ncorresponding antecedents of wh-ellipses. The antecedents are elided contents\nbehind the wh-words but are implicitly referred to using contexts. Previous\nwork frames sluice resolution as question answering where this setting\noutperforms all its preceding works by large margins. Ellipsis and questions\nare referentially dependent expressions (anaphoras) and retrieving the\ncorresponding antecedents are like answering questions to output pieces of\nclarifying information. However, the task is not fully solved. Therefore, we\nwant to further investigate what makes sluice resolution differ to question\nanswering and fill in the error gaps. We also present some results using recent\nstate-of-the-art question answering systems which improve the previous work\n(86.01 to 90.39 F1).\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 17:51:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wiriyathammabhum", "Peratham", ""]]}, {"id": "2105.14357", "submitter": "Kuntal Kumar Pal", "authors": "Kuntal Kumar Pal, Kazuaki Kashihara, Pratyay Banerjee, Swaroop Mishra,\n  Ruoyu Wang, Chitta Baral", "title": "Constructing Flow Graphs from Procedural Cybersecurity Texts", "comments": "13 pages, 5 pages, accepted in the Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Following procedural texts written in natural languages is challenging. We\nmust read the whole text to identify the relevant information or identify the\ninstruction flows to complete a task, which is prone to failures. If such texts\nare structured, we can readily visualize instruction-flows, reason or infer a\nparticular step, or even build automated systems to help novice agents achieve\na goal. However, this structure recovery task is a challenge because of such\ntexts' diverse nature. This paper proposes to identify relevant information\nfrom such texts and generate information flows between sentences. We built a\nlarge annotated procedural text dataset (CTFW) in the cybersecurity domain\n(3154 documents). This dataset contains valuable instructions regarding\nsoftware vulnerability analysis experiences. We performed extensive experiments\non CTFW with our LM-GNN model variants in multiple settings. To show the\ngeneralizability of both this task and our method, we also experimented with\nprocedural texts from two other domains (Maintenance Manual and Cooking), which\nare substantially different from cybersecurity. Our experiments show that Graph\nConvolution Network with BERT sentence embeddings outperforms BERT in all three\ndomains\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 19:06:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pal", "Kuntal Kumar", ""], ["Kashihara", "Kazuaki", ""], ["Banerjee", "Pratyay", ""], ["Mishra", "Swaroop", ""], ["Wang", "Ruoyu", ""], ["Baral", "Chitta", ""]]}, {"id": "2105.14398", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Ivan Vuli\\'c, Anna Korhonen, Nigel Collier", "title": "Learning Domain-Specialised Representations for Cross-Lingual Biomedical\n  Entity Linking", "comments": "ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Injecting external domain-specific knowledge (e.g., UMLS) into pretrained\nlanguage models (LMs) advances their capability to handle specialised in-domain\ntasks such as biomedical entity linking (BEL). However, such abundant expert\nknowledge is available only for a handful of languages (e.g., English). In this\nwork, by proposing a novel cross-lingual biomedical entity linking task\n(XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically\ndiverse languages, we first investigate the ability of standard\nknowledge-agnostic as well as knowledge-enhanced monolingual and multilingual\nLMs beyond the standard monolingual English BEL task. The scores indicate large\ngaps to English performance. We then address the challenge of transferring\ndomain-specific knowledge in resource-rich languages to resource-poor ones. To\nthis end, we propose and evaluate a series of cross-lingual transfer methods\nfor the XL-BEL task, and demonstrate that general-domain bitext helps propagate\nthe available English knowledge to languages with little to no in-domain data.\nRemarkably, we show that our proposed domain-specific transfer methods yield\nconsistent gains across all target languages, sometimes up to 20 Precision@1\npoints, without any in-domain knowledge in the target language, and without any\nin-domain parallel data.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:50:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Fangyu", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""], ["Collier", "Nigel", ""]]}, {"id": "2105.14403", "submitter": "Ryoma Sato", "authors": "Ryoma Sato, Makoto Yamada, Hisashi Kashima", "title": "Re-evaluating Word Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word mover's distance (WMD) is a fundamental technique for measuring the\nsimilarity of two documents. As the crux of WMD, it can take advantage of the\nunderlying geometry of the word space by employing an optimal transport\nformulation. The original study on WMD reported that WMD outperforms classical\nbaselines such as bag-of-words (BOW) and TF-IDF by significant margins in\nvarious datasets. In this paper, we point out that the evaluation in the\noriginal study could be misleading. We re-evaluate the performances of WMD and\nthe classical baselines and find that the classical baselines are competitive\nwith WMD if we employ an appropriate preprocessing, i.e., L1 normalization.\nHowever, this result is not intuitive. WMD should be superior to BOW because\nWMD can take the underlying geometry into account, whereas BOW cannot. Our\nanalysis shows that this is due to the high-dimensional nature of the\nunderlying metric. We find that WMD in high-dimensional spaces behaves more\nsimilarly to BOW than in low-dimensional spaces due to the curse of\ndimensionality.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 01:35:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sato", "Ryoma", ""], ["Yamada", "Makoto", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2105.14444", "submitter": "Jin Xu", "authors": "Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, Tie-Yan\n  Liu", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural\n  Architecture Search", "comments": "Accepted by KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467262", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While pre-trained language models (e.g., BERT) have achieved impressive\nresults on different natural language processing tasks, they have large numbers\nof parameters and suffer from big computational and memory costs, which make\nthem difficult for real-world deployment. Therefore, model compression is\nnecessary to reduce the computation and memory cost of pre-trained models. In\nthis work, we aim to compress BERT and address the following two challenging\npractical issues: (1) The compression algorithm should be able to output\nmultiple compressed models with different sizes and latencies, in order to\nsupport devices with different memory and latency limitations; (2) The\nalgorithm should be downstream task agnostic, so that the compressed models are\ngenerally applicable for different downstream tasks. We leverage techniques in\nneural architecture search (NAS) and propose NAS-BERT, an efficient method for\nBERT compression. NAS-BERT trains a big supernet on a search space containing a\nvariety of architectures and outputs multiple compressed models with adaptive\nsizes and latency. Furthermore, the training of NAS-BERT is conducted on\nstandard self-supervised pre-training tasks (e.g., masked language model) and\ndoes not depend on specific downstream tasks. Thus, the compressed models can\nbe used across various downstream tasks. The technical challenge of NAS-BERT is\nthat training a big supernet on the pre-training task is extremely costly. We\nemploy several techniques including block-wise search, search space pruning,\nand performance approximation to improve search efficiency and accuracy.\nExtensive experiments on GLUE and SQuAD benchmark datasets demonstrate that\nNAS-BERT can find lightweight models with better accuracy than previous\napproaches, and can be directly applied to different downstream tasks with\nadaptive model sizes for different requirements of memory or latency.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:20:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Jin", ""], ["Tan", "Xu", ""], ["Luo", "Renqian", ""], ["Song", "Kaitao", ""], ["Li", "Jian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.14445", "submitter": "Jiwei Li", "authors": "Shuhe Wang, Yuxian Meng, Xiaofei Sun, Fei Wu, Rongbin Ouyang, Rui Yan,\n  Tianwei Zhang, Jiwei Li", "title": "Modeling Text-visual Mutual Dependency for Multi-modal Dialog Generation", "comments": "arXiv admin note: text overlap with arXiv:2012.15015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal dialog modeling is of growing interest. In this work, we propose\nframeworks to resolve a specific case of multi-modal dialog generation that\nbetter mimics multi-modal dialog generation in the real world, where each\ndialog turn is associated with the visual context in which it takes place.\nSpecifically, we propose to model the mutual dependency between text-visual\nfeatures, where the model not only needs to learn the probability of generating\nthe next dialog utterance given preceding dialog utterances and visual\ncontexts, but also the probability of predicting the visual features in which a\ndialog utterance takes place, leading the generated dialog utterance specific\nto the visual context. We observe significant performance boosts over vanilla\nmodels when the mutual dependency between text and visual features is modeled.\nCode is available at https://github.com/ShannonAI/OpenViDial.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:20:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Shuhe", ""], ["Meng", "Yuxian", ""], ["Sun", "Xiaofei", ""], ["Wu", "Fei", ""], ["Ouyang", "Rongbin", ""], ["Yan", "Rui", ""], ["Zhang", "Tianwei", ""], ["Li", "Jiwei", ""]]}, {"id": "2105.14454", "submitter": "Sungdong Kim", "authors": "Sungdong Kim, Minsuk Chang and Sang-Woo Lee", "title": "NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based\n  Simulation", "comments": "Accepted to ACL 2021 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose NeuralWOZ, a novel dialogue collection framework that uses\nmodel-based dialogue simulation. NeuralWOZ has two pipelined models, Collector\nand Labeler. Collector generates dialogues from (1) user's goal instructions,\nwhich are the user context and task constraints in natural language, and (2)\nsystem's API call results, which is a list of possible query responses for user\nrequests from the given knowledge base. Labeler annotates the generated\ndialogue by formulating the annotation as a multiple-choice problem, in which\nthe candidate labels are extracted from goal instructions and API call results.\nWe demonstrate the effectiveness of the proposed method in the zero-shot domain\ntransfer learning for dialogue state tracking. In the evaluation, the synthetic\ndialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with\nimprovements of 4.4% point joint goal accuracy on average across domains, and\nimprovements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:54:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kim", "Sungdong", ""], ["Chang", "Minsuk", ""], ["Lee", "Sang-Woo", ""]]}, {"id": "2105.14462", "submitter": "Zhiyong Wu", "authors": "Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao", "title": "Good for Misconceived Reasons: An Empirical Revisiting on the Need for\n  Visual Context in Multimodal Machine Translation", "comments": "To appear at ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neural multimodal machine translation (MMT) system is one that aims to\nperform better translation by extending conventional text-only translation\nmodels with multimodal information. Many recent studies report improvements\nwhen equipping their models with the multimodal module, despite the controversy\nof whether such improvements indeed come from the multimodal part. We revisit\nthe contribution of multimodal information in MMT by devising two interpretable\nMMT models. To our surprise, although our models replicate similar gains as\nrecently developed multimodal-integrated systems achieved, our models learn to\nignore the multimodal information. Upon further investigation, we discover that\nthe improvements achieved by the multimodal models over text-only counterparts\nare in fact results of the regularization effect. We report empirical findings\nthat highlight the importance of MMT models' interpretability, and discuss how\nour findings will benefit future research.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 08:27:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Zhiyong", ""], ["Kong", "Lingpeng", ""], ["Bi", "Wei", ""], ["Li", "Xiang", ""], ["Kao", "Ben", ""]]}, {"id": "2105.14473", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein", "title": "Determining the Credibility of Science Communication", "comments": null, "journal-ref": "In Proceedings of the Second Workshop on Scholarly Document\n  Processing (SDP at NAACL 2021)", "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on scholarly document processing assumes that the information\nprocessed is trustworthy and factually correct. However, this is not always the\ncase. There are two core challenges, which should be addressed: 1) ensuring\nthat scientific publications are credible -- e.g. that claims are not made\nwithout supporting evidence, and that all relevant supporting evidence is\nprovided; and 2) that scientific findings are not misrepresented, distorted or\noutright misreported when communicated by journalists or the general public. I\nwill present some first steps towards addressing these problems and outline\nremaining challenges.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:16:11 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Augenstein", "Isabelle", ""]]}, {"id": "2105.14478", "submitter": "Yian Li", "authors": "Yian Li, Hai Zhao", "title": "Pre-training Universal Language Representation", "comments": "Accepted by ACL-IJCNLP 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the well-developed cut-edge representation learning for language,\nmost language representation models usually focus on specific levels of\nlinguistic units. This work introduces universal language representation\nlearning, i.e., embeddings of different levels of linguistic units or text with\nquite diverse lengths in a uniform vector space. We propose the training\nobjective MiSAD that utilizes meaningful n-grams extracted from large unlabeled\ncorpus by a simple but effective algorithm for pre-trained language models.\nThen we empirically verify that well designed pre-training scheme may\neffectively yield universal language representation, which will bring great\nconvenience when handling multiple layers of linguistic objects in a unified\nway. Especially, our model achieves the highest accuracy on analogy tasks in\ndifferent language levels and significantly improves the performance on\ndownstream tasks in the GLUE benchmark and a question answering dataset.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:29:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yian", ""], ["Zhao", "Hai", ""]]}, {"id": "2105.14485", "submitter": "Xiaozhi Wang", "authors": "Ziqi Wang, Xiaozhi Wang, Xu Han, Yankai Lin, Lei Hou, Zhiyuan Liu,\n  Peng Li, Juanzi Li, Jie Zhou", "title": "CLEVE: Contrastive Pre-training for Event Extraction", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event extraction (EE) has considerably benefited from pre-trained language\nmodels (PLMs) by fine-tuning. However, existing pre-training methods have not\ninvolved modeling event characteristics, resulting in the developed EE models\ncannot take full advantage of large-scale unsupervised data. To this end, we\npropose CLEVE, a contrastive pre-training framework for EE to better learn\nevent knowledge from large unsupervised data and their semantic structures\n(e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to\nlearn event semantics and a graph encoder to learn event structures\nrespectively. Specifically, the text encoder learns event semantic\nrepresentations by self-supervised contrastive learning to represent the words\nof the same events closer than those unrelated words; the graph encoder learns\nevent structure representations by graph contrastive pre-training on parsed\nevent-related semantic structures. The two complementary representations then\nwork together to improve both the conventional supervised EE and the\nunsupervised \"liberal\" EE, which requires jointly extracting events and\ndiscovering event schemata without any annotated data. Experiments on ACE 2005\nand MAVEN datasets show that CLEVE achieves significant improvements,\nespecially in the challenging unsupervised setting. The source code and\npre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:50:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Ziqi", ""], ["Wang", "Xiaozhi", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Hou", "Lei", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Li", "Juanzi", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.14488", "submitter": "Jun Gao", "authors": "Jun Gao, Wei Bi, Ruifeng Xu and Shuming Shi", "title": "REAM$\\sharp$: An Enhancement Approach to Reference-based Evaluation\n  Metrics for Open-domain Dialog Generation", "comments": "ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of reliable automatic evaluation metrics is a major impediment to\nthe development of open-domain dialogue systems. Various reference-based\nmetrics have been proposed to calculate a score between a predicted response\nand a small set of references. However, these metrics show unsatisfactory\ncorrelations with human judgments. For a reference-based metric, its\nreliability mainly depends on two factors: its ability to measure the\nsimilarity between the predicted response and the reference response, as well\nas the reliability of the given reference set. Yet, there are few discussions\non the latter. Our work attempts to fill this vacancy. We first clarify an\nassumption on reference-based metrics that, if more high-quality references are\nadded into the reference set, the reliability of the metric will increase.\nNext, we present REAM$\\sharp$: an enhancement approach to Reference-based\nEvAluation Metrics for open-domain dialogue systems. A prediction model is\ndesigned to estimate the reliability of the given reference set. We show how\nits predicted results can be helpful to augment the reference set, and thus\nimprove the reliability of the metric. Experiments validate both the\neffectiveness of our prediction model and that the reliability of\nreference-based metrics improves with the augmented reference sets.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 10:04:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gao", "Jun", ""], ["Bi", "Wei", ""], ["Xu", "Ruifeng", ""], ["Shi", "Shuming", ""]]}, {"id": "2105.14504", "submitter": "Jeremy Barnes", "authors": "Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja {\\O}vrelid, Erik\n  Velldal", "title": "Structured Sentiment Analysis as Dependency Graph Parsing", "comments": "Accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured sentiment analysis attempts to extract full opinion tuples from a\ntext, but over time this task has been subdivided into smaller and smaller\nsub-tasks, e,g,, target extraction or targeted polarity classification. We\nargue that this division has become counterproductive and propose a new unified\nframework to remedy the situation. We cast the structured sentiment problem as\ndependency graph parsing, where the nodes are spans of sentiment holders,\ntargets and expressions, and the arcs are the relations between them. We\nperform experiments on five datasets in four languages (English, Norwegian,\nBasque, and Catalan) and show that this approach leads to strong improvements\nover state-of-the-art baselines. Our analysis shows that refining the sentiment\ngraphs with syntactic dependency information further improves results.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:19:46 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Barnes", "Jeremy", ""], ["Kurtz", "Robin", ""], ["Oepen", "Stephan", ""], ["\u00d8vrelid", "Lilja", ""], ["Velldal", "Erik", ""]]}, {"id": "2105.14515", "submitter": "Rachit Bansal", "authors": "Rachit Bansal, Himanshu Choudhary, Ravneet Punia, Niko Schenk, Jacob L\n  Dahl, \\'Emilie Pag\\'e-Perron", "title": "How Low is Too Low? A Computational Perspective on Extremely\n  Low-Resource Languages", "comments": "ACL SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent advancements of attention-based deep learning\narchitectures across a majority of Natural Language Processing tasks, their\napplication remains limited in a low-resource setting because of a lack of\npre-trained models for such languages. In this study, we make the first attempt\nto investigate the challenges of adapting these techniques for an extremely\nlow-resource language -- Sumerian cuneiform -- one of the world's oldest\nwritten languages attested from at least the beginning of the 3rd millennium\nBC. Specifically, we introduce the first cross-lingual information extraction\npipeline for Sumerian, which includes part-of-speech tagging, named entity\nrecognition, and machine translation. We further curate InterpretLR, an\ninterpretability toolkit for low-resource NLP, and use it alongside human\nattributions to make sense of the models. We emphasize on human evaluations to\ngauge all our techniques. Notably, most components of our pipeline can be\ngeneralised to any other language to obtain an interpretable execution of the\ntechniques, especially in a low-resource setting. We publicly release all\nsoftware, model checkpoints, and a novel dataset with domain-specific\npre-processing to promote further research.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 12:09:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bansal", "Rachit", ""], ["Choudhary", "Himanshu", ""], ["Punia", "Ravneet", ""], ["Schenk", "Niko", ""], ["Dahl", "Jacob L", ""], ["Pag\u00e9-Perron", "\u00c9milie", ""]]}, {"id": "2105.14528", "submitter": "Jiwei Li", "authors": "Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei\n  Zhang, Jiwei Li", "title": "Fast Nearest Neighbor Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though nearest neighbor Machine Translation ($k$NN-MT)\n\\cite{khandelwal2020nearest} has proved to introduce significant performance\nboosts over standard neural MT systems, it is prohibitively slow since it uses\nthe entire reference corpus as the datastore for the nearest neighbor search.\nThis means each step for each beam in the beam search has to search over the\nentire reference corpus. $k$NN-MT is thus two-order slower than vanilla MT\nmodels, making it hard to be applied to real-world applications, especially\nonline services. In this work, we propose Fast $k$NN-MT to address this issue.\nFast $k$NN-MT constructs a significantly smaller datastore for the nearest\nneighbor search: for each word in a source sentence, Fast $k$NN-MT first\nselects its nearest token-level neighbors, which is limited to tokens that are\nthe same as the query token. Then at each decoding step, in contrast to using\nthe entire corpus as the datastore, the search space is limited to target\ntokens corresponding to the previously selected reference source tokens. This\nstrategy avoids search through the whole datastore for nearest neighbors and\ndrastically improves decoding efficiency. Without loss of performance, Fast\n$k$NN-MT is two-order faster than $k$NN-MT, and is only two times slower than\nthe standard NMT model. Fast $k$NN-MT enables the practical use of $k$NN-MT\nsystems in real-world MT applications.\\footnote{Code is available at\n\\url{https://github.com/ShannonAI/fast-knn-nmt.}}\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:10:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Meng", "Yuxian", ""], ["Li", "Xiaoya", ""], ["Zheng", "Xiayu", ""], ["Wu", "Fei", ""], ["Sun", "Xiaofei", ""], ["Zhang", "Tianwei", ""], ["Li", "Jiwei", ""]]}, {"id": "2105.14538", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, Marcel Worring", "title": "Longer Version for \"Deep Context-Encoding Network for Retinal Image\n  Captioning\"", "comments": "This paper is a longer version of \"Deep Context-Encoding Network for\n  Retinal Image Captioning\" which is accepted by IEEE International Conference\n  on Image Processing (ICIP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating medical reports for retinal images is one of the\npromising ways to help ophthalmologists reduce their workload and improve work\nefficiency. In this work, we propose a new context-driven encoding network to\nautomatically generate medical reports for retinal images. The proposed model\nis mainly composed of a multi-modal input encoder and a fused-feature decoder.\nOur experimental results show that our proposed method is capable of\neffectively leveraging the interactive information between the input image and\ncontext, i.e., keywords in our case. The proposed method creates more accurate\nand meaningful reports for retinal images than baseline models and achieves\nstate-of-the-art performance. This performance is shown in several commonly\nused metrics for the medical report generation task: BLEU-avg (+16%), CIDEr\n(+10.2%), and ROUGE (+8.6%).\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:37:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Yang", "Chao-Han Huck", ""], ["Worring", "Marcel", ""]]}, {"id": "2105.14553", "submitter": "Rongzhou Bao", "authors": "Rongzhou Bao, Jiayi Wang, Hai Zhao", "title": "Defending Pre-trained Language Models from Adversarial Word\n  Substitutions Without Performance Sacrifice", "comments": "Findings of ACL: ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained contextualized language models (PrLMs) have led to strong\nperformance gains in downstream natural language understanding tasks. However,\nPrLMs can still be easily fooled by adversarial word substitution, which is one\nof the most challenging textual adversarial attack methods. Existing defence\napproaches suffer from notable performance loss and complexities. Thus, this\npaper presents a compact and performance-preserved framework, Anomaly Detection\nwith Frequency-Aware Randomization (ADFAR). In detail, we design an auxiliary\nanomaly detection classifier and adopt a multi-task learning procedure, by\nwhich PrLMs are able to distinguish adversarial input samples. Then, in order\nto defend adversarial word substitution, a frequency-aware randomization\nprocess is applied to those recognized adversarial input samples. Empirical\nresults show that ADFAR significantly outperforms those newly proposed defense\nmethods over various tasks with much higher inference speed. Remarkably, ADFAR\ndoes not impair the overall performance of PrLMs. The code is available at\nhttps://github.com/LilyNLP/ADFAR\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:24:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bao", "Rongzhou", ""], ["Wang", "Jiayi", ""], ["Zhao", "Hai", ""]]}, {"id": "2105.14556", "submitter": "Yinhe Zheng Dr.", "authors": "Yida Wang, Yinhe Zheng, Yong Jiang, Minlie Huang", "title": "Diversifying Dialog Generation via Adaptive Label Smoothing", "comments": "ACL2021 Main Track (Long Paper), Code Available in\n  https://github.com/lemon234071/AdaLabel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural dialogue generation models trained with the one-hot target\ndistribution suffer from the over-confidence issue, which leads to poor\ngeneration diversity as widely reported in the literature. Although existing\napproaches such as label smoothing can alleviate this issue, they fail to adapt\nto diverse dialog contexts. In this paper, we propose an Adaptive Label\nSmoothing (AdaLabel) approach that can adaptively estimate a target label\ndistribution at each time step for different contexts. The maximum probability\nin the predicted distribution is used to modify the soft target distribution\nproduced by a novel light-weight bi-directional decoder module. The resulting\ntarget distribution is aware of both previous and future contexts and is\nadjusted to avoid over-training the dialogue model. Our model can be trained in\nan end-to-end manner. Extensive experiments on two benchmark datasets show that\nour approach outperforms various competitive baselines in producing diverse\nresponses.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:41:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Yida", ""], ["Zheng", "Yinhe", ""], ["Jiang", "Yong", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.14600", "submitter": "Md Shad Akhtar Dr.", "authors": "Ayan Sengupta, Sourabh Kumar Bhattacharjee, Tanmoy Chakraborty, Md\n  Shad Akhtar", "title": "HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed\n  Language Representation", "comments": "15 pages, 13 tables, 6 Figures. Accepted at ACL-IJCNLP-2021\n  (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding linguistics and morphology of resource-scarce code-mixed texts\nremains a key challenge in text processing. Although word embedding comes in\nhandy to support downstream tasks for low-resource languages, there are plenty\nof scopes in improving the quality of language representation particularly for\ncode-mixed languages. In this paper, we propose HIT, a robust representation\nlearning method for code-mixed texts. HIT is a hierarchical transformer-based\nframework that captures the semantic relationship among words and\nhierarchically learns the sentence-level semantics using a fused attention\nmechanism. HIT incorporates two attention modules, a multi-headed\nself-attention and an outer product attention module, and computes their\nweighted sum to obtain the attention weights. Our evaluation of HIT on one\nEuropean (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and\nMalayalam) languages across four NLP tasks on eleven datasets suggests\nsignificant performance improvement against various state-of-the-art systems.\nWe further show the adaptability of learned representation across tasks in a\ntransfer learning setup (with and without fine-tuning).\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 18:53:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sengupta", "Ayan", ""], ["Bhattacharjee", "Sourabh Kumar", ""], ["Chakraborty", "Tanmoy", ""], ["Akhtar", "Md Shad", ""]]}, {"id": "2105.14636", "submitter": "Zhewei Yao", "authors": "Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, Michael W. Mahoney", "title": "MLPruning: A Multilevel Structured Pruning Framework for\n  Transformer-based Models", "comments": "20 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is an effective method to reduce the memory footprint and\ncomputational cost associated with large natural language processing models.\nHowever, current approaches either only explore head pruning, which has a\nlimited pruning ratio, or only focus on unstructured pruning, which has\nnegligible effects on the real inference time and/or power consumption. To\naddress these challenges, we develop a novel MultiLevel structured Pruning\n(MLPruning) framework, which uses three different levels of structured pruning:\nhead pruning, row pruning, and block-wise sparse pruning. We propose using a\nlearnable Top-k threshold, which employs an adaptive regularization to adjust\nthe regularization magnitude adaptively, to select appropriate pruning ratios\nfor different weight matrices. We also propose a two-step pipeline to combine\nblock-wise pruning with head/row pruning to achieve high structured pruning\nratios with minimum accuracy degradation. Our empirical results show that for\n\\bertbase, with \\textapprox20\\% of remaining weights, \\OURS can achieve an\naccuracy that is comparable to the full model on QQP/MNLI/\\squad, with up to\n\\textapprox3.69x speedup. Our framework has been open sourced~\\cite{codebase}.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:00:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yao", "Zhewei", ""], ["Ma", "Linjian", ""], ["Shen", "Sheng", ""], ["Keutzer", "Kurt", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2105.14652", "submitter": "Kawin Ethayarajh", "authors": "Kawin Ethayarajh and Dan Jurafsky", "title": "Attention Flows are Shapley Value Explanations", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley Values, a solution to the credit assignment problem in cooperative\ngame theory, are a popular type of explanation in machine learning, having been\nused to explain the importance of features, embeddings, and even neurons. In\nNLP, however, leave-one-out and attention-based explanations still predominate.\nCan we draw a connection between these different methods? We formally prove\nthat -- save for the degenerate case -- attention weights and leave-one-out\nvalues cannot be Shapley Values. $\\textit{Attention flow}$ is a post-processed\nvariant of attention weights obtained by running the max-flow algorithm on the\nattention graph. Perhaps surprisingly, we prove that attention flows are indeed\nShapley Values, at least at the layerwise level. Given the many desirable\ntheoretical qualities of Shapley Values -- which has driven their adoption\namong the ML community -- we argue that NLP practitioners should, when\npossible, adopt attention flow explanations alongside more traditional ones.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 00:06:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ethayarajh", "Kawin", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2105.14668", "submitter": "Lang Yu", "authors": "Lang Yu and Allyson Ettinger", "title": "On the Interplay Between Fine-tuning and Composition in Transformers", "comments": "To appear in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained transformer language models have shown remarkable performance on\na variety of NLP tasks. However, recent research has suggested that\nphrase-level representations in these models reflect heavy influences of\nlexical content, but lack evidence of sophisticated, compositional phrase\ninformation. Here we investigate the impact of fine-tuning on the capacity of\ncontextualized embeddings to capture phrase meaning information beyond lexical\ncontent. Specifically, we fine-tune models on an adversarial paraphrase\nclassification task with high lexical overlap, and on a sentiment\nclassification task. After fine-tuning, we analyze phrasal representations in\ncontrolled settings following prior work. We find that fine-tuning largely\nfails to benefit compositionality in these representations, though training on\nsentiment yields a small, localized benefit for certain models. In follow-up\nanalyses, we identify confounding cues in the paraphrase dataset that may\nexplain the lack of composition benefits from that task, and we discuss\npotential factors underlying the localized benefits from sentiment training.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 01:49:56 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 01:11:04 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yu", "Lang", ""], ["Ettinger", "Allyson", ""]]}, {"id": "2105.14669", "submitter": "Yuekai Zhao", "authors": "Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, Weizhu Chen", "title": "Memory-Efficient Differentiable Transformer Architecture Search", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiable architecture search (DARTS) is successfully applied in many\nvision tasks. However, directly using DARTS for Transformers is\nmemory-intensive, which renders the search process infeasible. To this end, we\npropose a multi-split reversible network and combine it with DARTS.\nSpecifically, we devise a backpropagation-with-reconstruction algorithm so that\nwe only need to store the last layer's outputs. By relieving the memory burden\nfor DARTS, it allows us to search with larger hidden size and more candidate\noperations. We evaluate the searched architecture on three sequence-to-sequence\ndatasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14\nEnglish-Czech. Experimental results show that our network consistently\noutperforms standard Transformers across the tasks. Moreover, our method\ncompares favorably with big-size Evolved Transformers, reducing search\ncomputation by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 01:52:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhao", "Yuekai", ""], ["Dong", "Li", ""], ["Shen", "Yelong", ""], ["Zhang", "Zhihua", ""], ["Wei", "Furu", ""], ["Chen", "Weizhu", ""]]}, {"id": "2105.14682", "submitter": "Liangming Pan", "authors": "Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, William Yang\n  Wang", "title": "Zero-shot Fact Verification by Claim Generation", "comments": "ACL-IJCNLP 2021 (main conference, short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural models for automated fact verification have achieved promising results\nthanks to the availability of large, human-annotated datasets. However, for\neach new domain that requires fact verification, creating a dataset by manually\nwriting claims and linking them to their supporting evidence is expensive. We\ndevelop QACG, a framework for training a robust fact verification model by\nusing automatically generated claims that can be supported, refuted, or\nunverifiable from evidence from Wikipedia. QACG generates question-answer pairs\nfrom the evidence and then converts them into different types of claims.\nExperiments on the FEVER dataset show that our QACG framework significantly\nreduces the demand for human-annotated training data. In a zero-shot scenario,\nQACG improves a RoBERTa model's F1 from 50% to 77%, equivalent in performance\nto 2K+ manually-curated examples. Our QACG code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:13:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pan", "Liangming", ""], ["Chen", "Wenhu", ""], ["Xiong", "Wenhan", ""], ["Kan", "Min-Yen", ""], ["Wang", "William Yang", ""]]}, {"id": "2105.14686", "submitter": "Weize Chen", "authors": "Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li,\n  Maosong Sun, Jie Zhou", "title": "Fully Hyperbolic Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:36:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 15:24:28 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Weize", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Zhao", "Hexu", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2105.14704", "submitter": "Hao Fang", "authors": "Hao Fang, Chen Gong, Chen Zhang, Yanan Sui, Luming Li", "title": "Parkinsonian Chinese Speech Analysis towards Automatic Classification of\n  Parkinson's Disease", "comments": "12 pages, 5 figures, proceedings of the Machine Learning for Health\n  NeurIPS Workshop, PMLR 136:114-125, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech disorders often occur at the early stage of Parkinson's disease (PD).\nThe speech impairments could be indicators of the disorder for early diagnosis,\nwhile motor symptoms are not obvious. In this study, we constructed a new\nspeech corpus of Mandarin Chinese and addressed classification of patients with\nPD. We implemented classical machine learning methods with ranking algorithms\nfor feature selection, convolutional and recurrent deep networks, and an end to\nend system. Our classification accuracy significantly surpassed\nstate-of-the-art studies. The result suggests that free talk has stronger\nclassification power than standard speech tasks, which could help the design of\nfuture speech tasks for efficient early diagnosis of the disease. Based on\nexisting classification methods and our natural speech study, the automatic\ndetection of PD from daily conversation could be accessible to the majority of\nthe clinical population.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:51:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fang", "Hao", ""], ["Gong", "Chen", ""], ["Zhang", "Chen", ""], ["Sui", "Yanan", ""], ["Li", "Luming", ""]]}, {"id": "2105.14761", "submitter": "Guangsheng Bao", "authors": "Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen and Weihua Luo", "title": "G-Transformer for Document-level Machine Translation", "comments": "Accepted by ACL2021 main track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level MT models are still far from satisfactory. Existing work\nextend translation unit from single sentence to multiple sentences. However,\nstudy shows that when we further enlarge the translation unit to a whole\ndocument, supervised training of Transformer can fail. In this paper, we find\nsuch failure is not caused by overfitting, but by sticking around local minima\nduring training. Our analysis shows that the increased complexity of\ntarget-to-source attention is a reason for the failure. As a solution, we\npropose G-Transformer, introducing locality assumption as an inductive bias\ninto Transformer, reducing the hypothesis space of the attention from target to\nsource. Experiments show that G-Transformer converges faster and more stably\nthan Transformer, achieving new state-of-the-art BLEU scores for both\nnon-pretraining and pre-training settings on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:47:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bao", "Guangsheng", ""], ["Zhang", "Yue", ""], ["Teng", "Zhiyang", ""], ["Chen", "Boxing", ""], ["Luo", "Weihua", ""]]}, {"id": "2105.14762", "submitter": "Kun Zhou", "authors": "Kun Zhou, Berrak Sisman, Rui Liu, Haizhou Li", "title": "Emotional Voice Conversion: Theory, Databases and ESD", "comments": "Submitted to Speech Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we first provide a review of the state-of-the-art emotional\nvoice conversion research, and the existing emotional speech databases. We then\nmotivate the development of a novel emotional speech database (ESD) that\naddresses the increasing research need. With this paper, the ESD database is\nnow made available to the research community. The ESD database consists of 350\nparallel utterances spoken by 10 native English and 10 native Chinese speakers\nand covers 5 emotion categories (neutral, happy, angry, sad and surprise). More\nthan 29 hours of speech data were recorded in a controlled acoustic\nenvironment. The database is suitable for multi-speaker and cross-lingual\nemotional voice conversion studies. As case studies, we implement several\nstate-of-the-art emotional voice conversion systems on the ESD database. This\npaper provides a reference study on ESD in conjunction with its release.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:48:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Kun", ""], ["Sisman", "Berrak", ""], ["Liu", "Rui", ""], ["Li", "Haizhou", ""]]}, {"id": "2105.14774", "submitter": "Erfan Ghadery", "authors": "Erfan Ghadery, Damien Sileo, Marie-Francine Moens", "title": "LIIR at SemEval-2021 task 6: Detection of Persuasion Techniques In Texts\n  and Images using CLIP features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe our approach for SemEval-2021 task 6 on detection of persuasion\ntechniques in multimodal content (memes). Our system combines pretrained\nmultimodal models (CLIP) and chained classifiers. Also, we propose to enrich\nthe data by a data augmentation technique. Our submission achieves a rank of\n8/16 in terms of F1-micro and 9/16 with F1-macro on the test set.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:16:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ghadery", "Erfan", ""], ["Sileo", "Damien", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2105.14778", "submitter": "Peng Wang", "authors": "Peng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang, Jingren\n  Zhou, Hongxia Yang", "title": "Sketch and Refine: Towards Faithful and Informative Table-to-Text\n  Generation", "comments": "13 pages, 2 figures. Accepted in ACL2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table-to-text generation refers to generating a descriptive text from a\nkey-value table. Traditional autoregressive methods, though can generate text\nwith high fluency, suffer from low coverage and poor faithfulness problems. To\nmitigate these problems, we propose a novel Skeleton-based two-stage method\nthat combines both Autoregressive and Non-Autoregressive generations (SANA).\nOur approach includes: (1) skeleton generation with an autoregressive pointer\nnetwork to select key tokens from the source table; (2) edit-based\nnon-autoregressive generation model to produce texts via iterative insertion\nand deletion operations. By integrating hard constraints from the skeleton, the\nnon-autoregressive model improves the generation's coverage over the source\ntable and thus enhances its faithfulness. We conduct automatic and human\nevaluations on both WikiPerson and WikiBio datasets. Experimental results\ndemonstrate that our method outperforms the previous state-of-the-art methods\nin both automatic and human evaluation, especially on coverage and\nfaithfulness. In particular, we achieve PARENT-T recall of 99.47 in WikiPerson,\nimproving over the existing best results by more than 10 points.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:18:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Peng", ""], ["Lin", "Junyang", ""], ["Yang", "An", ""], ["Zhou", "Chang", ""], ["Zhang", "Yichang", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.14779", "submitter": "Shammur Absar Chowdhury", "authors": "Shammur Absar Chowdhury, Amir Hussein, Ahmed Abdelali, Ahmed Ali", "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal\n  Code-Switching Arabic ASR", "comments": "Accepted in INTERSPEECH 2021, Multilingual ASR, Multi-dialectal ASR,\n  Code-Switching ASR, Arabic ASR, Conformer, Transformer, E2E ASR, Speech\n  Recognition, ASR, Arabic, English, French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language and\ndialectal variation of spoken content. Recent studies show its efficacy over\nmonolingual systems. In this study, we design a large multilingual end-to-end\nASR using self-attention based conformer architecture. We trained the system\nusing Arabic (Ar), English (En) and French (Fr) languages. We evaluate the\nsystem performance handling: (i) monolingual (Ar, En and Fr); (ii)\nmulti-dialectal (Modern Standard Arabic, along with dialectal variation such as\nEgyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and\ndialectal (MSA-Egyptian dialect) test cases, and compare with current\nstate-of-the-art systems. Furthermore, we investigate the influence of\ndifferent embedding/character representations including character vs\nword-piece; shared vs distinct input symbol per language. Our findings\ndemonstrate the strength of such a model by outperforming state-of-the-art\nmonolingual dialectal Arabic and code-switching Arabic ASR.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:20:38 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 09:16:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chowdhury", "Shammur Absar", ""], ["Hussein", "Amir", ""], ["Abdelali", "Ahmed", ""], ["Ali", "Ahmed", ""]]}, {"id": "2105.14781", "submitter": "Yilin Niu", "authors": "Yilin Niu, Fei Huang, Jiaming Liang, Wenkai Chen, Xiaoyan Zhu, Minlie\n  Huang", "title": "A Semantic-based Method for Unsupervised Commonsense Question Answering", "comments": "Accepted by ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised commonsense question answering is appealing since it does not\nrely on any labeled task data. Among existing work, a popular solution is to\nuse pre-trained language models to score candidate choices directly conditioned\non the question or context. However, such scores from language models can be\neasily affected by irrelevant factors, such as word frequencies, sentence\nstructures, etc. These distracting factors may not only mislead the model to\nchoose a wrong answer but also make it oversensitive to lexical perturbations\nin candidate answers.\n  In this paper, we present a novel SEmantic-based Question Answering method\n(SEQA) for unsupervised commonsense question answering. Instead of directly\nscoring each answer choice, our method first generates a set of plausible\nanswers with generative models (e.g., GPT-2), and then uses these plausible\nanswers to select the correct choice by considering the semantic similarity\nbetween each plausible answer and each choice. We devise a simple, yet sound\nformalism for this idea and verify its effectiveness and robustness with\nextensive experiments. We evaluate the proposed method on four benchmark\ndatasets, and our method achieves the best results in unsupervised settings.\nMoreover, when attacked by TextFooler with synonym replacement, SEQA\ndemonstrates much less performance drops than baselines, thereby indicating\nstronger robustness.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:21:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Niu", "Yilin", ""], ["Huang", "Fei", ""], ["Liang", "Jiaming", ""], ["Chen", "Wenkai", ""], ["Zhu", "Xiaoyan", ""], ["Huang", "Minlie", ""]]}, {"id": "2105.14802", "submitter": "Yafu Li", "authors": "Yafu Li, Yongjing Yin, Yulong Chen and Yue Zhang", "title": "On Compositional Generalization of Neural Machine Translation", "comments": "To appear at the ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern neural machine translation (NMT) models have achieved competitive\nperformance in standard benchmarks such as WMT. However, there still exist\nsignificant issues such as robustness, domain generalization, etc. In this\npaper, we study NMT models from the perspective of compositional generalization\nby building a benchmark dataset, CoGnition, consisting of 216k clean and\nconsistent sentence pairs. We quantitatively analyze effects of various factors\nusing compound translation error rate, then demonstrate that the NMT model\nfails badly on compositional generalization, although it performs remarkably\nwell under traditional metrics.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:04:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yafu", ""], ["Yin", "Yongjing", ""], ["Chen", "Yulong", ""], ["Zhang", "Yue", ""]]}, {"id": "2105.14809", "submitter": "Xuancheng Huang", "authors": "Xuancheng Huang, Jingfang Xu, Maosong Sun, and Yang Liu", "title": "Transfer Learning for Sequence Generation: from Single-source to\n  Multi-source", "comments": "ACL2021 main track long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-source sequence generation (MSG) is an important kind of sequence\ngeneration tasks that takes multiple sources, including automatic post-editing,\nmulti-source translation, multi-document summarization, etc. As MSG tasks\nsuffer from the data scarcity problem and recent pretrained models have been\nproven to be effective for low-resource downstream tasks, transferring\npretrained sequence-to-sequence models to MSG tasks is essential. Although\ndirectly finetuning pretrained models on MSG tasks and concatenating multiple\nsources into a single long sequence is regarded as a simple method to transfer\npretrained models to MSG tasks, we conjecture that the direct finetuning method\nleads to catastrophic forgetting and solely relying on pretrained\nself-attention layers to capture cross-source information is not sufficient.\nTherefore, we propose a two-stage finetuning method to alleviate the\npretrain-finetune discrepancy and introduce a novel MSG model with a fine\nencoder to learn better representations in MSG tasks. Experiments show that our\napproach achieves new state-of-the-art results on the WMT17 APE task and\nmulti-source translation task using the WMT14 test set. When adapted to\ndocument-level translation, our framework outperforms strong baselines\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:12:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Xuancheng", ""], ["Xu", "Jingfang", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "2105.14813", "submitter": "Chong Li", "authors": "Chong Li, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang", "title": "Exploration and Exploitation: Two Ways to Improve Chinese Spelling\n  Correction Models", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A sequence-to-sequence learning with neural networks has empirically proven\nto be an effective framework for Chinese Spelling Correction (CSC), which takes\na sentence with some spelling errors as input and outputs the corrected one.\nHowever, CSC models may fail to correct spelling errors covered by the\nconfusion sets, and also will encounter unseen ones. We propose a method, which\ncontinually identifies the weak spots of a model to generate more valuable\ntraining instances, and apply a task-specific pre-training strategy to enhance\nthe model. The generated adversarial examples are gradually added to the\ntraining set. Experimental results show that such an adversarial training\nmethod combined with the pretraining strategy can improve both the\ngeneralization and robustness of multiple CSC models across three different\ndatasets, achieving stateof-the-art performance for CSC task.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:17:33 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 15:18:14 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Chong", ""], ["Zhang", "Cenyuan", ""], ["Zheng", "Xiaoqing", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2105.14815", "submitter": "Thiemo Wambsganss", "authors": "Thiemo Wambsganss, Christina Niklaus, Matthias S\\\"ollner, Siegfried\n  Handschuh and Jan Marco Leimeister", "title": "Supporting Cognitive and Emotional Empathic Writing of Students", "comments": "to be published in The Joint Conference of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International\n  Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an annotation approach to capturing emotional and cognitive\nempathy in student-written peer reviews on business models in German. We\npropose an annotation scheme that allows us to model emotional and cognitive\nempathy scores based on three types of review components. Also, we conducted an\nannotation study with three annotators based on 92 student essays to evaluate\nour annotation scheme. The obtained inter-rater agreement of {\\alpha}=0.79 for\nthe components and the multi-{\\pi}=0.41 for the empathy scores indicate that\nthe proposed annotation scheme successfully guides annotators to a substantial\nto moderate agreement. Moreover, we trained predictive models to detect the\nannotated empathy structures and embedded them in an adaptive writing support\nsystem for students to receive individual empathy feedback independent of an\ninstructor, time, and location. We evaluated our tool in a peer learning\nexercise with 58 students and found promising results for perceived empathy\nskill learning, perceived feedback accuracy, and intention to use. Finally, we\npresent our freely available corpus of 500 empathy-annotated, student-written\npeer reviews on business models and our annotation guidelines to encourage\nfuture research on the design and development of empathy support systems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:18:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wambsganss", "Thiemo", ""], ["Niklaus", "Christina", ""], ["S\u00f6llner", "Matthias", ""], ["Handschuh", "Siegfried", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.14822", "submitter": "Hiroshi Noji", "authors": "Hiroshi Noji, Yohei Oseki", "title": "Effective Batching for Recurrent Neural Network Grammars", "comments": "Findings of ACL: ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a language model that integrates traditional symbolic operations and\nflexible neural representations, recurrent neural network grammars (RNNGs) have\nattracted great attention from both scientific and engineering perspectives.\nHowever, RNNGs are known to be harder to scale due to the difficulty of batched\ntraining. In this paper, we propose effective batching for RNNGs, where every\noperation is computed in parallel with tensors across multiple sentences. Our\nPyTorch implementation effectively employs a GPU and achieves x6 speedup\ncompared to the existing C++ DyNet implementation with model-independent\nauto-batching. Moreover, our batched RNNG also accelerates inference and\nachieves x20-150 speedup for beam search depending on beam sizes. Finally, we\nevaluate syntactic generalization performance of the scaled RNNG against the\nLSTM baseline, based on the large training data of 100M tokens from English\nWikipedia and the broad-coverage targeted syntactic evaluation benchmark. Our\nRNNG implementation is available at https://github.com/aistairc/rnng-pytorch/.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:34:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Noji", "Hiroshi", ""], ["Oseki", "Yohei", ""]]}, {"id": "2105.14839", "submitter": "David Peer", "authors": "David Peer, Sebastian Stabinger, Stefan Engl, Antonio\n  Rodriguez-Sanchez", "title": "Greedy Layer Pruning: Decreasing Inference Time of Transformer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-tuning transformer models after unsupervised pre-training reaches a very\nhigh performance on many different NLP tasks. Unfortunately, transformers\nsuffer from long inference times which greatly increases costs in production\nand is a limiting factor for the deployment into embedded devices. One possible\nsolution is to use knowledge distillation, which solves this problem by\ntransferring information from large teacher models to smaller student models,\nbut as it needs an additional expensive pre-training phase, this solution is\ncomputationally expensive and can be financially prohibitive for smaller\nacademic research groups. Another solution is to use layer-wise pruning\nmethods, which reach high compression rates for transformer models and avoids\nthe computational load of the pre-training distillation stage. The price to pay\nis that the performance of layer-wise pruning algorithms is not on par with\nstate-of-the-art knowledge distillation methods. In this paper, greedy layer\npruning (GLP) is introduced to (1) outperform current state-of-the-art for\nlayer-wise pruning (2) close the performance gap when compared to knowledge\ndistillation, while (3) using only a modest budget. More precisely, with the\nmethodology presented it is possible to prune and evaluate competitive models\non the whole GLUE benchmark with a budget of just $\\$300$. Our source code is\navailable on https://github.com/deepopinion/greedy-layer-pruning.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:52:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Peer", "David", ""], ["Stabinger", "Sebastian", ""], ["Engl", "Stefan", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "2105.14849", "submitter": "Albert Zeyer", "authors": "Albert Zeyer and Ralf Schl\\\"uter and Hermann Ney", "title": "Why does CTC result in peaky behavior?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE cs.SD eess.AS math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The peaky behavior of CTC models is well known experimentally. However, an\nunderstanding about why peaky behavior occurs is missing, and whether this is a\ngood property. We provide a formal analysis of the peaky behavior and gradient\ndescent convergence properties of the CTC loss and related training criteria.\nOur analysis provides a deep understanding why peaky behavior occurs and when\nit is suboptimal. On a simple example which should be trivial to learn for any\nmodel, we prove that a feed-forward neural network trained with CTC from\nuniform initialization converges towards peaky behavior with a 100% error rate.\nOur analysis further explains why CTC only works well together with the blank\nlabel. We further demonstrate that peaky behavior does not occur on other\nrelated losses including a label prior model, and that this improves\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:03:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:44:23 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2105.14850", "submitter": "Lin Zheng", "authors": "Lin Zheng, Zhiyong Wu, Lingpeng Kong", "title": "Cascaded Head-colliding Attention", "comments": "ACL 2021 Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers have advanced the field of natural language processing (NLP) on\na variety of important tasks. At the cornerstone of the Transformer\narchitecture is the multi-head attention (MHA) mechanism which models pairwise\ninteractions between the elements of the sequence. Despite its massive success,\nthe current framework ignores interactions among different heads, leading to\nthe problem that many of the heads are redundant in practice, which greatly\nwastes the capacity of the model. To improve parameter efficiency, we\nre-formulate the MHA as a latent variable model from a probabilistic\nperspective. We present cascaded head-colliding attention (CODA) which\nexplicitly models the interactions between attention heads through a\nhierarchical variational distribution. We conduct extensive experiments and\ndemonstrate that CODA outperforms the transformer baseline, by $0.6$ perplexity\non \\texttt{Wikitext-103} in language modeling, and by $0.6$ BLEU on\n\\texttt{WMT14 EN-DE} in machine translation, due to its improvements on the\nparameter efficiency.\\footnote{Our implementation is publicly available at\n\\url{https://github.com/LZhengisme/CODA}.}\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:06:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zheng", "Lin", ""], ["Wu", "Zhiyong", ""], ["Kong", "Lingpeng", ""]]}, {"id": "2105.14875", "submitter": "Jakaria Rabbi", "authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, MD.\n  Kamrul Hasan, Mohammed Baz, Mehedi Masud, Md. Abdul Awal, Awal Ahmed Fime,\n  Md. Tahmid Hasan Fuad, Delowar Sikder, and MD. Akil Raihan Iftee", "title": "Bangla Natural Language Processing: A Comprehensive Review of Classical,\n  Machine Learning, and Deep Learning Based Methods", "comments": "This preprint will be submitted to IEEE Access Journal and it\n  contains total of 43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive study of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough review of 71 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. We discuss Classical, Machine Learning\nand Deep Learning approaches with different datasets while addressing the\nlimitations and current and future trends of the BNLP.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:58:58 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:40:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sen", "Ovishake", ""], ["Fuad", "Mohtasim", ""], ["Islam", "MD. Nazrul", ""], ["Rabbi", "Jakaria", ""], ["Hasan", "MD. Kamrul", ""], ["Baz", "Mohammed", ""], ["Masud", "Mehedi", ""], ["Awal", "Md. Abdul", ""], ["Fime", "Awal Ahmed", ""], ["Fuad", "Md. Tahmid Hasan", ""], ["Sikder", "Delowar", ""], ["Iftee", "MD. Akil Raihan", ""]]}, {"id": "2105.14878", "submitter": "Mingjun Zhao", "authors": "Mingjun Zhao, Haijiang Wu, Di Niu, Zixuan Wang, Xiaoli Wang", "title": "Verdi: Quality Estimation and Error Detection for Bilingual", "comments": "Accepted by The Web Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Translation Quality Estimation is critical to reducing post-editing efforts\nin machine translation and to cross-lingual corpus cleaning. As a research\nproblem, quality estimation (QE) aims to directly estimate the quality of\ntranslation in a given pair of source and target sentences, and highlight the\nwords that need corrections, without referencing to golden translations. In\nthis paper, we propose Verdi, a novel framework for word-level and\nsentence-level post-editing effort estimation for bilingual corpora. Verdi\nadopts two word predictors to enable diverse features to be extracted from a\npair of sentences for subsequent quality estimation, including a\ntransformer-based neural machine translation (NMT) model and a pre-trained\ncross-lingual language model (XLM). We exploit the symmetric nature of\nbilingual corpora and apply model-level dual learning in the NMT predictor,\nwhich handles a primal task and a dual task simultaneously with weight sharing,\nleading to stronger context prediction ability than single-direction NMT\nmodels. By taking advantage of the dual learning scheme, we further design a\nnovel feature to directly encode the translated target information without\nrelying on the source context. Extensive experiments conducted on WMT20 QE\ntasks demonstrate that our method beats the winner of the competition and\noutperforms other baseline methods by a great margin. We further use the\nsentence-level scores provided by Verdi to clean a parallel corpus and observe\nbenefits on both model performance and training efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:04:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhao", "Mingjun", ""], ["Wu", "Haijiang", ""], ["Niu", "Di", ""], ["Wang", "Zixuan", ""], ["Wang", "Xiaoli", ""]]}, {"id": "2105.14879", "submitter": "Boyuan Zheng", "authors": "Boyuan Zheng, Xiaoyu Yang, Yu-Ping Ruan, Zhenhua Ling, Quan Liu, Si\n  Wei, Xiaodan Zhu", "title": "SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the SemEval-2021 shared task 4: Reading Comprehension\nof Abstract Meaning (ReCAM). This shared task is designed to help evaluate the\nability of machines in representing and understanding abstract concepts. Given\na passage and the corresponding question, a participating system is expected to\nchoose the correct answer from five candidates of abstract concepts in a\ncloze-style machine reading comprehension setup. Based on two typical\ndefinitions of abstractness, i.e., the imperceptibility and nonspecificity, our\ntask provides three subtasks to evaluate the participating models.\nSpecifically, Subtask 1 aims to evaluate how well a system can model concepts\nthat cannot be directly perceived in the physical world. Subtask 2 focuses on\nmodels' ability in comprehending nonspecific concepts located high in a\nhypernym hierarchy given the context of a passage. Subtask 3 aims to provide\nsome insights into models' generalizability over the two types of abstractness.\nDuring the SemEval-2021 official evaluation period, we received 23 submissions\nto Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29\nsubmissions to Subtask 3. The leaderboard and competition website can be found\nat https://competitions.codalab.org/competitions/26153. The data and baseline\ncode are available at\nhttps://github.com/boyuanzheng010/SemEval2021-Reading-Comprehension-of-Abstract-Meaning.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:04:17 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:45:27 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zheng", "Boyuan", ""], ["Yang", "Xiaoyu", ""], ["Ruan", "Yu-Ping", ""], ["Ling", "Zhenhua", ""], ["Liu", "Quan", ""], ["Wei", "Si", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2105.14880", "submitter": "Gaochen Wu", "authors": "Gaochen Wu, Bin Xu, Dejie Chang, Bangchang Liu", "title": "A Multilingual Modeling Method for Span-Extraction Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Span-extraction reading comprehension models have made tremendous advances\nenabled by the availability of large-scale, high-quality training datasets.\nDespite such rapid progress and widespread application, extractive reading\ncomprehension datasets in languages other than English remain scarce, and\ncreating such a sufficient amount of training data for each language is costly\nand even impossible. An alternative to creating large-scale high-quality\nmonolingual span-extraction training datasets is to develop multilingual\nmodeling approaches and systems which can transfer to the target language\nwithout requiring training data in that language. In this paper, in order to\nsolve the scarce availability of extractive reading comprehension training data\nin the target language, we propose a multilingual extractive reading\ncomprehension approach called XLRC by simultaneously modeling the existing\nextractive reading comprehension training data in a multilingual environment\nusing self-adaptive attention and multilingual attention. Specifically, we\nfirstly construct multilingual parallel corpora by translating the existing\nextractive reading comprehension datasets (i.e., CMRC 2018) from the target\nlanguage (i.e., Chinese) into different language families (i.e., English).\nSecondly, to enhance the final target representation, we adopt self-adaptive\nattention (SAA) to combine self-attention and inter-attention to extract the\nsemantic relations from each pair of the target and source languages.\nFurthermore, we propose multilingual attention (MLA) to learn the rich\nknowledge from various language families. Experimental results show that our\nmodel outperforms the state-of-the-art baseline (i.e., RoBERTa_Large) on the\nCMRC 2018 task, which demonstrate the effectiveness of our proposed\nmulti-lingual modeling approach and show the potentials in multilingual NLP\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:05:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Gaochen", ""], ["Xu", "Bin", ""], ["Chang", "Dejie", ""], ["Liu", "Bangchang", ""]]}, {"id": "2105.14888", "submitter": "Ana-Maria Bucur", "authors": "Ana-Maria Bucur, Marcos Zampieri, and Liviu P. Dinu", "title": "An Exploratory Analysis of the Relation Between Offensive Language and\n  Mental Health", "comments": "Accepted to Findings of the Association for Computational\n  Linguistics: ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we analyze the interplay between the use of offensive language\nand mental health. We acquired publicly available datasets created for\noffensive language identification and depression detection and we train\ncomputational models to compare the use of offensive language in social media\nposts written by groups of individuals with and without self-reported\ndepression diagnosis. We also look at samples written by groups of individuals\nwhose posts show signs of depression according to recent related studies. Our\nanalysis indicates that offensive language is more frequently used in the\nsamples written by individuals with self-reported depression as well as\nindividuals showing signs of depression. The results discussed here open new\navenues in research in politeness/offensiveness and mental health.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:25:07 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 20:47:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bucur", "Ana-Maria", ""], ["Zampieri", "Marcos", ""], ["Dinu", "Liviu P.", ""]]}, {"id": "2105.14913", "submitter": "Huayang Li", "authors": "Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi", "title": "GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation", "comments": "Accepted into the main conference of ACL 2021. arXiv admin note: text\n  overlap with arXiv:2105.13072", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided translation (CAT), the use of software to assist a human\ntranslator in the translation process, has been proven to be useful in\nenhancing the productivity of human translators. Autocompletion, which suggests\ntranslation results according to the text pieces provided by human translators,\nis a core function of CAT. There are two limitations in previous research in\nthis line. First, most research works on this topic focus on sentence-level\nautocompletion (i.e., generating the whole translation as a sentence based on\nhuman input), but word-level autocompletion is under-explored so far. Second,\nalmost no public benchmarks are available for the autocompletion task of CAT.\nThis might be among the reasons why research progress in CAT is much slower\ncompared to automatic MT. In this paper, we propose the task of general\nword-level autocompletion (GWLAN) from a real-world CAT scenario, and construct\nthe first public benchmark to facilitate research in this topic. In addition,\nwe propose an effective method for GWLAN and compare it with several strong\nbaselines. Experiments demonstrate that our proposed method can give\nsignificantly more accurate predictions than the baseline methods on our\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:27:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Huayang", ""], ["Liu", "Lemao", ""], ["Huang", "Guoping", ""], ["Shi", "Shuming", ""]]}, {"id": "2105.14921", "submitter": "Jussi Karlgren", "authors": "Jussi Karlgren", "title": "How Lexical Gold Standards Have Effects On The Usefulness Of Text\n  Analysis Tools For Digital Scholarship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how the current lexical similarity and analogy gold\nstandards are built to conform to certain ideas about what the models they are\ndesigned to evaluate are used for. Topical relevance has always been the most\nimportant target notion for information access tools and related language\ntechnology technologies, and while this has proven a useful starting point for\nmuch of what information technology is used for, it does not always align well\nwith other uses to which technologies are being put, most notably use cases\nfrom digital scholarship in the humanities or social sciences. This paper\nargues for more systematic formulation of requirements from the digital\nhumanities and social sciences and more explicit description of the assumptions\nunderlying model design.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:40:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Karlgren", "Jussi", ""]]}, {"id": "2105.14924", "submitter": "Runxin Xu", "authors": "Runxin Xu, Tianyu Liu, Lei Li, Baobao Chang", "title": "Document-level Event Extraction via Heterogeneous Graph-based\n  Interaction Model with a Tracker", "comments": "Accepted by ACL-IJCNLP 2021 main conference (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level event extraction aims to recognize event information from a\nwhole piece of article. Existing methods are not effective due to two\nchallenges of this task: a) the target event arguments are scattered across\nsentences; b) the correlation among events in a document is non-trivial to\nmodel. In this paper, we propose Heterogeneous Graph-based Interaction Model\nwith a Tracker (GIT) to solve the aforementioned two challenges. For the first\nchallenge, GIT constructs a heterogeneous graph interaction network to capture\nglobal interactions among different sentences and entity mentions. For the\nsecond, GIT introduces a Tracker module to track the extracted events and hence\ncapture the interdependency among the events. Experiments on a large-scale\ndataset (Zheng et al., 2019) show GIT outperforms the previous methods by 2.8\nF1. Further analysis reveals GIT is effective in extracting multiple correlated\nevents and event arguments that scatter across the document. Our code is\navailable at https://github.com/RunxinXu/GIT.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:45:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Runxin", ""], ["Liu", "Tianyu", ""], ["Li", "Lei", ""], ["Chang", "Baobao", ""]]}, {"id": "2105.14940", "submitter": "Zae Myung Kim", "authors": "Zae Myung Kim, Laurent Besacier, Vassilina Nikoulina, Didier Schwab", "title": "Do Multilingual Neural Machine Translation Models Contain Language Pair\n  Specific Attention Heads?", "comments": "10 pages, accepted at Findings of ACL 2021 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on the analysis of the multilingual representations focus on\nidentifying whether there is an emergence of language-independent\nrepresentations, or whether a multilingual model partitions its weights among\ndifferent languages. While most of such work has been conducted in a\n\"black-box\" manner, this paper aims to analyze individual components of a\nmultilingual neural translation (NMT) model. In particular, we look at the\nencoder self-attention and encoder-decoder attention heads (in a many-to-one\nNMT model) that are more specific to the translation of a certain language pair\nthan others by (1) employing metrics that quantify some aspects of the\nattention weights such as \"variance\" or \"confidence\", and (2) systematically\nranking the importance of attention heads with respect to translation quality.\nExperimental results show that surprisingly, the set of most important\nattention heads are very similar across the language pairs and that it is\npossible to remove nearly one-third of the less important heads without hurting\nthe translation quality greatly.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:15:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kim", "Zae Myung", ""], ["Besacier", "Laurent", ""], ["Nikoulina", "Vassilina", ""], ["Schwab", "Didier", ""]]}, {"id": "2105.14980", "submitter": "Xin Zhang", "authors": "Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Pengjun Xie", "title": "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named\n  Entity Recognition", "comments": "Accepted by ACL-IJCNLP 2021 (long paper), accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is regarded as one prospective solution for effective\nsupervised learning, aiming to build large-scale annotated training data by\ncrowd workers. Previous studies focus on reducing the influences from the\nnoises of the crowdsourced annotations for supervised models. We take a\ndifferent point in this work, regarding all crowdsourced annotations as\ngold-standard with respect to the individual annotators. In this way, we find\nthat crowdsourcing could be highly similar to domain adaptation, and then the\nrecent advances of cross-domain methods can be almost directly applied to\ncrowdsourcing. Here we take named entity recognition (NER) as a study case,\nsuggesting an annotator-aware representation learning model that inspired by\nthe domain adaptation methods which attempt to capture effective domain-aware\nfeatures. We investigate both unsupervised and supervised crowdsourcing\nlearning, assuming that no or only small-scale expert annotations are\navailable. Experimental results on a benchmark crowdsourced NER dataset show\nthat our method is highly effective, leading to a new state-of-the-art\nperformance. In addition, under the supervised setting, we can achieve\nimpressive performance gains with only a very small scale of expert\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:11:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Xin", ""], ["Xu", "Guangwei", ""], ["Sun", "Yueheng", ""], ["Zhang", "Meishan", ""], ["Xie", "Pengjun", ""]]}, {"id": "2105.15021", "submitter": "Songlin Yang", "authors": "Songlin Yang, Yanpeng Zhao, Kewei Tu", "title": "Neural Bi-Lexicalized PCFG Induction", "comments": "To appear in ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar\ninduction. However, to reduce computational complexity, they make a strong\nindependence assumption on the generation of the child word and thus bilexical\ndependencies are ignored. In this paper, we propose an approach to parameterize\nL-PCFGs without making implausible independence assumptions. Our approach\ndirectly models bilexical dependencies and meanwhile reduces both learning and\nrepresentation complexities of L-PCFGs. Experimental results on the English WSJ\ndataset confirm the effectiveness of our approach in improving both running\nspeed and unsupervised parsing performance.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:00:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Songlin", ""], ["Zhao", "Yanpeng", ""], ["Tu", "Kewei", ""]]}, {"id": "2105.15033", "submitter": "Mosha Chen", "authors": "Dejie Chang, Mosha Chen, Chaozhen Liu, Liping Liu, Dongdong Li, Wei\n  Li, Fei Kong, Bangchang Liu, Xiaobin Luo, Ji Qi, Qiao Jin, Bin Xu", "title": "DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph\n  Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph has been proven effective in modeling structured information\nand conceptual knowledge, especially in the medical domain. However, the lack\nof high-quality annotated corpora remains a crucial problem for advancing the\nresearch and applications on this task. In order to accelerate the research for\ndomain-specific knowledge graphs in the medical domain, we introduce DiaKG, a\nhigh-quality Chinese dataset for Diabetes knowledge graph, which contains\n22,050 entities and 6,890 relations in total. We implement recent typical\nmethods for Named Entity Recognition and Relation Extraction as a benchmark to\nevaluate the proposed dataset thoroughly. Empirical results show that the DiaKG\nis challenging for most existing methods and further analysis is conducted to\ndiscuss future research direction for improvements. We hope the release of this\ndataset can assist the construction of diabetes knowledge graphs and facilitate\nAI-based applications.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:12:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chang", "Dejie", ""], ["Chen", "Mosha", ""], ["Liu", "Chaozhen", ""], ["Liu", "Liping", ""], ["Li", "Dongdong", ""], ["Li", "Wei", ""], ["Kong", "Fei", ""], ["Liu", "Bangchang", ""], ["Luo", "Xiaobin", ""], ["Qi", "Ji", ""], ["Jin", "Qiao", ""], ["Xu", "Bin", ""]]}, {"id": "2105.15053", "submitter": "Tom Hosking", "authors": "Tom Hosking, Mirella Lapata", "title": "Factorising Meaning and Form for Intent-Preserving Paraphrasing", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for generating paraphrases of English questions that\nretain the original intent but use a different surface form. Our model combines\na careful choice of training objective with a principled information\nbottleneck, to induce a latent encoding space that disentangles meaning and\nform. We train an encoder-decoder model to reconstruct a question from a\nparaphrase with the same meaning and an exemplar with the same surface form,\nleading to separated encoding spaces. We use a Vector-Quantized Variational\nAutoencoder to represent the surface form as a set of discrete latent\nvariables, allowing us to use a classifier to select a different surface form\nat test time. Crucially, our method does not require access to an external\nsource of target exemplars. Extensive experiments and a human evaluation show\nthat we are able to generate paraphrases with a better tradeoff between\nsemantic preservation and syntactic novelty compared to previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:37:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hosking", "Tom", ""], ["Lapata", "Mirella", ""]]}, {"id": "2105.15054", "submitter": "Prithviraj Ammanabrolu", "authors": "Wai Man Si, Prithviraj Ammanabrolu, Mark O. Riedl", "title": "Telling Stories through Multi-User Dialogue by Modeling Character\n  Relations", "comments": "In Proceedings of SIGDIAL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores character-driven story continuation, in which the story\nemerges through characters' first- and second-person narration as well as\ndialogue -- requiring models to select language that is consistent with a\ncharacter's persona and their relationships with other characters while\nfollowing and advancing the story. We hypothesize that a multi-task model that\ntrains on character dialogue plus character relationship information improves\ntransformer-based story continuation. To this end, we extend the Critical Role\nDungeons and Dragons Dataset (Rameshkumar and Bailey, 2020) -- consisting of\ndialogue transcripts of people collaboratively telling a story while playing\nthe role-playing game Dungeons and Dragons -- with automatically extracted\nrelationships between each pair of interacting characters as well as their\npersonas. A series of ablations lend evidence to our hypothesis, showing that\nour multi-task model using character relationships improves story continuation\naccuracy over strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:39:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Si", "Wai Man", ""], ["Ammanabrolu", "Prithviraj", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2105.15065", "submitter": "Amar Prakash Azad", "authors": "Amar Prakash Azad, Supriyo Ghosh, Ajay Gupta, Harshit Kumar and\n  Prateeti Mohapatra", "title": "Picking Pearl From Seabed: Extracting Artefacts from Noisy Issue\n  Triaging Collaborative Conversations for Hybrid Cloud Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Site Reliability Engineers (SREs) play a key role in issue identification and\nresolution. After an issue is reported, SREs come together in a virtual room\n(collaboration platform) to triage the issue. While doing so, they leave behind\na wealth of information which can be used later for triaging similar issues.\nHowever, usability of the conversations offer challenges due to them being i)\nnoisy and ii) unlabelled. This paper presents a novel approach for issue\nartefact extraction from the noisy conversations with minimal labelled data. We\npropose a combination of unsupervised and supervised model with minimum human\nintervention that leverages domain knowledge to predict artefacts for a small\namount of conversation data and use that for fine-tuning an already pretrained\nlanguage model for artefact prediction on a large amount of conversation data.\nExperimental results on our dataset show that the proposed ensemble of\nunsupervised and supervised model is better than using either one of them\nindividually.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:51:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Azad", "Amar Prakash", ""], ["Ghosh", "Supriyo", ""], ["Gupta", "Ajay", ""], ["Kumar", "Harshit", ""], ["Mohapatra", "Prateeti", ""]]}, {"id": "2105.15071", "submitter": "Wei-Jen Ko", "authors": "Wei-Jen Ko, Ahmed El-Kishky, Adithya Renduchintala, Vishrav Chaudhary,\n  Naman Goyal, Francisco Guzm\\'an, Pascale Fung, Philipp Koehn, Mona Diab", "title": "Adapting High-resource NMT Models to Translate Low-resource Related\n  Languages without Parallel Data", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of parallel data is a major obstacle for training high-quality\nmachine translation systems for low-resource languages. Fortunately, some\nlow-resource languages are linguistically related or similar to high-resource\nlanguages; these related languages may share many lexical or syntactic\nstructures. In this work, we exploit this linguistic overlap to facilitate\ntranslating to and from a low-resource language with only monolingual data, in\naddition to any parallel data in the related high-resource language. Our\nmethod, NMT-Adapt, combines denoising autoencoding, back-translation and\nadversarial objectives to utilize monolingual data for low-resource adaptation.\nWe experiment on 7 languages from three different language families and show\nthat our technique significantly improves translation into low-resource\nlanguage compared to other translation baselines.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:01:18 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:21:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ko", "Wei-Jen", ""], ["El-Kishky", "Ahmed", ""], ["Renduchintala", "Adithya", ""], ["Chaudhary", "Vishrav", ""], ["Goyal", "Naman", ""], ["Guzm\u00e1n", "Francisco", ""], ["Fung", "Pascale", ""], ["Koehn", "Philipp", ""], ["Diab", "Mona", ""]]}, {"id": "2105.15079", "submitter": "Luong Luc Phan", "authors": "Luong Luc Phan, Phuc Huynh Pham, Kim Thi-Thanh Nguyen, Tham Thi\n  Nguyen, Sieu Khai Huynh, Luan Thanh Nguyen, Tin Van Huynh, and Kiet Van\n  Nguyen", "title": "SA2SL: From Aspect-Based Sentiment Analysis to Social Listening System\n  for Business Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a process of building a social listening system\nbased on aspect-based sentiment analysis in Vietnamese from creating a dataset\nto building a real application. Firstly, we create UIT-ViSFD, a Vietnamese\nSmartphone Feedback Dataset as a new benchmark corpus built based on a strict\nannotation schemes for evaluating aspect-based sentiment analysis, consisting\nof 11,122 human-annotated comments for mobile e-commerce, which is freely\navailable for research purposes. We also present a proposed approach based on\nthe Bi-LSTM architecture with the fastText word embeddings for the Vietnamese\naspect based sentiment task. Our experiments show that our approach achieves\nthe best performances with the F1-score of 84.48% for the aspect task and\n63.06% for the sentiment task, which performs several conventional machine\nlearning and deep learning systems. Last but not least, we build SA2SL, a\nsocial listening system based on the best performance model on our dataset,\nwhich will inspire more social listening systems in future.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:09:26 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:49:03 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Phan", "Luong Luc", ""], ["Pham", "Phuc Huynh", ""], ["Nguyen", "Kim Thi-Thanh", ""], ["Nguyen", "Tham Thi", ""], ["Huynh", "Sieu Khai", ""], ["Nguyen", "Luan Thanh", ""], ["Van Huynh", "Tin", ""], ["Van Nguyen", "Kiet", ""]]}, {"id": "2105.15082", "submitter": "An Yang", "authors": "An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang\n  Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren\n  Zhou, Hongxia Yang", "title": "Exploring Sparse Expert Models and Beyond", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:12:44 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 15:28:15 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 13:45:24 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 03:01:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yang", "An", ""], ["Lin", "Junyang", ""], ["Men", "Rui", ""], ["Zhou", "Chang", ""], ["Jiang", "Le", ""], ["Jia", "Xianyan", ""], ["Wang", "Ang", ""], ["Zhang", "Jie", ""], ["Wang", "Jiamang", ""], ["Li", "Yong", ""], ["Zhang", "Di", ""], ["Lin", "Wei", ""], ["Qu", "Lin", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2105.15087", "submitter": "Eleftheria Briakou", "authors": "Eleftheria Briakou and Marine Carpuat", "title": "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences\n  on Neural Machine Translation", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While it has been shown that Neural Machine Translation (NMT) is highly\nsensitive to noisy parallel training samples, prior work treats all types of\nmismatches between source and target as noise. As a result, it remains unclear\nhow samples that are mostly equivalent but contain a small number of\nsemantically divergent tokens impact NMT training. To close this gap, we\nanalyze the impact of different types of fine-grained semantic divergences on\nTransformer models. We show that models trained on synthetic divergences output\ndegenerated text more frequently and are less confident in their predictions.\nBased on these findings, we introduce a divergent-aware NMT framework that uses\nfactors to help NMT recover from the degradation caused by naturally occurring\ndivergences, improving both translation quality and model calibration on EN-FR\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:15:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Briakou", "Eleftheria", ""], ["Carpuat", "Marine", ""]]}, {"id": "2105.15162", "submitter": "Aciel Eshky", "authors": "Aciel Eshky, Joanne Cleland, Manuel Sam Ribeiro, Eleanor Sugden, Korin\n  Richmond, Steve Renals", "title": "Automatic audiovisual synchronisation for ultrasound tongue imaging", "comments": "18 pages, 10 figures. Manuscript accepted at Speech Communication", "journal-ref": null, "doi": "10.1016/j.specom.2021.05.008", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ultrasound tongue imaging is used to visualise the intra-oral articulators\nduring speech production. It is utilised in a range of applications, including\nspeech and language therapy and phonetics research. Ultrasound and speech audio\nare recorded simultaneously, and in order to correctly use this data, the two\nmodalities should be correctly synchronised. Synchronisation is achieved using\nspecialised hardware at recording time, but this approach can fail in practice\nresulting in data of limited usability. In this paper, we address the problem\nof automatically synchronising ultrasound and audio after data collection. We\nfirst investigate the tolerance of expert ultrasound users to synchronisation\nerrors in order to find the thresholds for error detection. We use these\nthresholds to define accuracy scoring boundaries for evaluating our system. We\nthen describe our approach for automatic synchronisation, which is driven by a\nself-supervised neural network, exploiting the correlation between the two\nsignals to synchronise them. We train our model on data from multiple domains\nwith different speaker characteristics, different equipment, and different\nrecording environments, and achieve an accuracy >92.4% on held-out in-domain\ndata. Finally, we introduce a novel resource, the Cleft dataset, which we\ngathered with a new clinical subgroup and for which hardware synchronisation\nproved unreliable. We apply our model to this out-of-domain data, and evaluate\nits performance subjectively with expert users. Results show that users prefer\nour model's output over the original hardware output 79.3% of the time. Our\nresults demonstrate the strength of our approach and its ability to generalise\nto data from new domains.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:11:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Eshky", "Aciel", ""], ["Cleland", "Joanne", ""], ["Ribeiro", "Manuel Sam", ""], ["Sugden", "Eleanor", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "2105.15171", "submitter": "Wangchunshu Zhou", "authors": "Wangchunshu Zhou, Qifei Li, Chenle Li", "title": "Learning from Perturbations: Diverse and Informative Dialogue Generation\n  with Inverse Adversarial Training", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose Inverse Adversarial Training (IAT) algorithm for\ntraining neural dialogue systems to avoid generic responses and model dialogue\nhistory better. In contrast to standard adversarial training algorithms, IAT\nencourages the model to be sensitive to the perturbation in the dialogue\nhistory and therefore learning from perturbations. By giving higher rewards for\nresponses whose output probability reduces more significantly when dialogue\nhistory is perturbed, the model is encouraged to generate more diverse and\nconsistent responses. By penalizing the model when generating the same response\ngiven perturbed dialogue history, the model is forced to better capture\ndialogue history and generate more informative responses. Experimental results\non two benchmark datasets show that our approach can better model dialogue\nhistory and generate more diverse and consistent responses. In addition, we\npoint out a problem of the widely used maximum mutual information (MMI) based\nmethods for improving the diversity of dialogue response generation models and\ndemonstrate it empirically.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:28:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Wangchunshu", ""], ["Li", "Qifei", ""], ["Li", "Chenle", ""]]}, {"id": "2105.15176", "submitter": "Tianyang Xu", "authors": "Tianyang Xu, Chunyun Zhang", "title": "Reinforced Generative Adversarial Network for Abstractive Text\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:34:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Tianyang", ""], ["Zhang", "Chunyun", ""]]}, {"id": "2105.15179", "submitter": "Nadir Durrani Dr", "authors": "Nadir Durrani and Hassan Sajjad and Fahim Dalvi", "title": "How transfer learning impacts linguistic knowledge in deep NLP models?", "comments": "Findings of the ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from pre-trained neural language models towards downstream\ntasks has been a predominant theme in NLP recently. Several researchers have\nshown that deep NLP models learn non-trivial amount of linguistic knowledge,\ncaptured at different layers of the model. We investigate how fine-tuning\ntowards downstream NLP tasks impacts the learned linguistic knowledge. We carry\nout a study across popular pre-trained models BERT, RoBERTa and XLNet using\nlayer and neuron-level diagnostic classifiers. We found that for some GLUE\ntasks, the network relies on the core linguistic information and preserve it\ndeeper in the network, while for others it forgets. Linguistic information is\ndistributed in the pre-trained language models but becomes localized to the\nlower layers post fine-tuning, reserving higher layers for the task specific\nknowledge. The pattern varies across architectures, with BERT retaining\nlinguistic information relatively deeper in the network compared to RoBERTa and\nXLNet, where it is predominantly delegated to the lower layers.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:43:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Durrani", "Nadir", ""], ["Sajjad", "Hassan", ""], ["Dalvi", "Fahim", ""]]}]