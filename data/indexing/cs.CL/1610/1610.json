[{"id": "1610.00030", "submitter": "Marcos Zampieri", "authors": "Marcos Zampieri, Shervin Malmasi, Mark Dras", "title": "Modeling Language Change in Historical Corpora: The Case of Portuguese", "comments": "Proceedings of Language Resources and Evaluation (LREC)", "journal-ref": "Proceedings of Language Resources and Evaluation (LREC). Portoroz,\n  Slovenia. pp. 4098-4104 (2016)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a number of experiments to model changes in a historical\nPortuguese corpus composed of literary texts for the purpose of temporal text\nclassification. Algorithms were trained to classify texts with respect to their\npublication date taking into account lexical variation represented as word\nn-grams, and morphosyntactic variation represented by part-of-speech (POS)\ndistribution. We report results of 99.8% accuracy using word unigram features\nwith a Support Vector Machines classifier to predict the publication date of\ndocuments in time intervals of both one century and half a century. A feature\nanalysis is performed to investigate the most informative features for this\ntask and how they are linked to language change.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 20:57:01 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zampieri", "Marcos", ""], ["Malmasi", "Shervin", ""], ["Dras", "Mark", ""]]}, {"id": "1610.00031", "submitter": "Marcos Zampieri", "authors": "Cyril Goutte, Serge L\\'eger, Shervin Malmasi, Marcos Zampieri", "title": "Discriminating Similar Languages: Evaluations and Explorations", "comments": "Proceedings of Language Resources and Evaluation (LREC)", "journal-ref": "Proceedings of Language Resources and Evaluation (LREC). Portoroz,\n  Slovenia. pp 1800-1807 (2016)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of the performance of machine learning classifiers on\ndiscriminating between similar languages and language varieties. We carried out\na number of experiments using the results of the two editions of the\nDiscriminating between Similar Languages (DSL) shared task. We investigate the\nprogress made between the two tasks, estimate an upper bound on possible\nperformance using ensemble and oracle combination, and provide learning curves\nto help us understand which languages are more challenging. A number of\ndifficult sentences are identified and investigated further with human\nannotation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 20:57:52 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Goutte", "Cyril", ""], ["L\u00e9ger", "Serge", ""], ["Malmasi", "Shervin", ""], ["Zampieri", "Marcos", ""]]}, {"id": "1610.00072", "submitter": "Michael Auli", "authors": "Gurvan L'Hostis, David Grangier, Michael Auli", "title": "Vocabulary Selection Strategies for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical translation models constrain the space of possible outputs by\nselecting a subset of translation rules based on the input sentence. Recent\nwork on improving the efficiency of neural translation models adopted a similar\nstrategy by restricting the output vocabulary to a subset of likely candidates\ngiven the source. In this paper we experiment with context and embedding-based\nselection methods and extend previous work by examining speed and accuracy\ntrade-offs in more detail. We show that decoding time on CPUs can be reduced by\nup to 90% and training time by 25% on the WMT15 English-German and WMT16\nEnglish-Romanian tasks at the same or only negligible change in accuracy. This\nbrings the time to decode with a state of the art neural translation system to\njust over 140 msec per sentence on a single CPU core for English-German.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 02:23:03 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["L'Hostis", "Gurvan", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1610.00211", "submitter": "Marcos Vin\\'icius Treviso", "authors": "Marcos Vin\\'icius Treviso, Christopher Shulby, Sandra Maria Alu\\'isio", "title": "Sentence Segmentation in Narrative Transcripts from Neuropsychological\n  Tests using Recurrent Convolutional Neural Networks", "comments": "EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated discourse analysis tools based on Natural Language Processing (NLP)\naiming at the diagnosis of language-impairing dementias generally extract\nseveral textual metrics of narrative transcripts. However, the absence of\nsentence boundary segmentation in the transcripts prevents the direct\napplication of NLP methods which rely on these marks to function properly, such\nas taggers and parsers. We present the first steps taken towards automatic\nneuropsychological evaluation based on narrative discourse analysis, presenting\na new automatic sentence segmentation method for impaired speech. Our model\nuses recurrent convolutional neural networks with prosodic, Part of Speech\n(PoS) features, and word embeddings. It was evaluated intrinsically on\nimpaired, spontaneous speech, as well as, normal, prepared speech, and presents\nbetter results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive\nImpairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method\n(F1 = 0.55 and 0.53, respectively) used in the same context of our study. The\nresults suggest that our model is robust for impaired speech and can be used in\nautomated discourse analysis tools to differentiate narratives produced by MCI\nand CTL.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 00:49:15 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 22:08:16 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Treviso", "Marcos Vin\u00edcius", ""], ["Shulby", "Christopher", ""], ["Alu\u00edsio", "Sandra Maria", ""]]}, {"id": "1610.00219", "submitter": "Junxian He", "authors": "Junxian He, Ying Huang, Changfeng Liu, Jiaming Shen, Yuting Jia,\n  Xinbing Wang", "title": "Text Network Exploration via Heterogeneous Web of Topics", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A text network refers to a data type that each vertex is associated with a\ntext document and the relationship between documents is represented by edges.\nThe proliferation of text networks such as hyperlinked webpages and academic\ncitation networks has led to an increasing demand for quickly developing a\ngeneral sense of a new text network, namely text network exploration. In this\npaper, we address the problem of text network exploration through constructing\na heterogeneous web of topics, which allows people to investigate a text\nnetwork associating word level with document level. To achieve this, a\nprobabilistic generative model for text and links is proposed, where three\ndifferent relationships in the heterogeneous topic web are quantified. We also\ndevelop a prototype demo system named TopicAtlas to exhibit such heterogeneous\ntopic web, and demonstrate how this system can facilitate the task of text\nnetwork exploration. Extensive qualitative analyses are included to verify the\neffectiveness of this heterogeneous topic web. Besides, we validate our model\non real-life text networks, showing that it preserves good performance on\nobjective evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 03:35:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["He", "Junxian", ""], ["Huang", "Ying", ""], ["Liu", "Changfeng", ""], ["Shen", "Jiaming", ""], ["Jia", "Yuting", ""], ["Wang", "Xinbing", ""]]}, {"id": "1610.00277", "submitter": "Yanmin Qian", "authors": "Yanmin Qian, Philip C Woodland", "title": "Very Deep Convolutional Neural Networks for Robust Speech Recognition", "comments": "accepted by SLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the extension and optimization of our previous work on\nvery deep convolutional neural networks (CNNs) for effective recognition of\nnoisy speech in the Aurora 4 task. The appropriate number of convolutional\nlayers, the sizes of the filters, pooling operations and input feature maps are\nall modified: the filter and pooling sizes are reduced and dimensions of input\nfeature maps are extended to allow adding more convolutional layers.\nFurthermore appropriate input padding and input feature map selection\nstrategies are developed. In addition, an adaptation framework using joint\ntraining of very deep CNN with auxiliary features i-vector and fMLLR features\nis developed. These modifications give substantial word error rate reductions\nover the standard CNN used as baseline. Finally the very deep CNN is combined\nwith an LSTM-RNN acoustic model and it is shown that state-level weighted log\nlikelihood score combination in a joint acoustic model decoding scheme is very\neffective. On the Aurora 4 task, the very deep CNN achieves a WER of 8.81%,\nfurther 7.99% with auxiliary feature joint training, and 7.09% with LSTM-RNN\njoint decoding.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 13:29:14 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Qian", "Yanmin", ""], ["Woodland", "Philip C", ""]]}, {"id": "1610.00311", "submitter": "Matilde Marcolli", "authors": "Kevin Shu and Matilde Marcolli", "title": "Syntactic Structures and Code Parameters", "comments": "14 pages, LaTeX, 12 png figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assign binary and ternary error-correcting codes to the data of syntactic\nstructures of world languages and we study the distribution of code points in\nthe space of code parameters. We show that, while most codes populate the lower\nregion approximating a superposition of Thomae functions, there is a\nsubstantial presence of codes above the Gilbert-Varshamov bound and even above\nthe asymptotic bound and the Plotkin bound. We investigate the dynamics induced\non the space of code parameters by spin glass models of language change, and\nshow that, in the presence of entailment relations between syntactic parameters\nthe dynamics can sometimes improve the code. For large sets of languages and\nsyntactic data, one can gain information on the spin glass dynamics from the\ninduced dynamics in the space of code parameters.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 16:54:41 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Shu", "Kevin", ""], ["Marcolli", "Matilde", ""]]}, {"id": "1610.00369", "submitter": "Asif Hassan", "authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad", "title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 23:45:23 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 02:13:05 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hassan", "A.", ""], ["Amin", "M. R.", ""], ["Mohammed", "N.", ""], ["Azad", "A. K. A.", ""]]}, {"id": "1610.00388", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Graham Neubig, Kyunghyun Cho and Victor O.K. Li", "title": "Learning to Translate in Real-time with Neural Machine Translation", "comments": "10 pages, camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating in real-time, a.k.a. simultaneous translation, outputs\ntranslation words before the input sentence ends, which is a challenging\nproblem for conventional machine translation methods. We propose a neural\nmachine translation (NMT) framework for simultaneous translation in which an\nagent learns to make decisions on when to translate from the interaction with a\npre-trained NMT environment. To trade off quality and delay, we extensively\nexplore various targets for delay and design a method for beam-search\napplicable in the simultaneous MT setting. Experiments against state-of-the-art\nbaselines on two language pairs demonstrate the efficacy of the proposed\nframework both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 02:11:03 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 00:46:39 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 21:07:56 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Gu", "Jiatao", ""], ["Neubig", "Graham", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1610.00479", "submitter": "Hinrich Schuetze", "authors": "Hinrich Schuetze, Heike Adel, Ehsaneddin Asgari", "title": "Nonsymbolic Text Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first generic text representation model that is completely\nnonsymbolic, i.e., it does not require the availability of a segmentation or\ntokenization method that attempts to identify words or other symbolic units in\ntext. This applies to training the parameters of the model on a training corpus\nas well as to applying it when computing the representation of a new text. We\nshow that our model performs better than prior work on an information\nextraction and a text denoising task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 10:30:13 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 11:51:10 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 14:30:00 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Schuetze", "Hinrich", ""], ["Adel", "Heike", ""], ["Asgari", "Ehsaneddin", ""]]}, {"id": "1610.00520", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka and Giampiero Salvi", "title": "Semi-supervised Learning with Sparse Autoencoders in Phone\n  Classification", "comments": "5 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the application of a semi-supervised learning method to improve\nthe performance of acoustic modelling for automatic speech recognition based on\ndeep neural net- works. As opposed to unsupervised initialisation followed by\nsupervised fine tuning, our method takes advantage of both unlabelled and\nlabelled data simultaneously through mini- batch stochastic gradient descent.\nWe tested the method with varying proportions of labelled vs unlabelled\nobservations in frame-based phoneme classification on the TIMIT database. Our\nexperiments show that the method outperforms standard supervised training for\nan equal amount of labelled data and provides competitive error rates compared\nto state-of-the-art graph-based semi-supervised learning techniques.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 12:52:26 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1610.00552", "submitter": "Minjae Lee", "authors": "Minjae Lee, Kyuyeon Hwang, Jinhwan Park, Sungwook Choi, Sungho Shin,\n  Wonyong Sung", "title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks", "comments": "Accepted to SiPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a neural network based real-time speech recognition (SR)\nsystem is developed using an FPGA for very low-power operation. The implemented\nsystem employs two recurrent neural networks (RNNs); one is a\nspeech-to-character RNN for acoustic modeling (AM) and the other is for\ncharacter-level language modeling (LM). The system also employs a statistical\nword-level LM to improve the recognition accuracy. The results of the AM, the\ncharacter-level LM, and the word-level LM are combined using a fairly simple\nN-best search algorithm instead of the hidden Markov model (HMM) based network.\nThe RNNs are implemented using massively parallel processing elements (PEs) for\nlow latency and high throughput. The weights are quantized to 6 bits to store\nall of them in the on-chip memory of an FPGA. The proposed algorithm is\nimplemented on a Xilinx XC7Z045, and the system can operate much faster than\nreal-time.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 10:44:32 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Lee", "Minjae", ""], ["Hwang", "Kyuyeon", ""], ["Park", "Jinhwan", ""], ["Choi", "Sungwook", ""], ["Shin", "Sungho", ""], ["Sung", "Wonyong", ""]]}, {"id": "1610.00572", "submitter": "Mauro Cettolo", "authors": "Mauro Cettolo", "title": "An Arabic-Hebrew parallel corpus of TED talks", "comments": "To appear in Proceedings of the AMTA 2016 Workshop on Semitic Machine\n  Translation (SeMaT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3,\nthe Web inventory that repurposes the original content of the TED website in a\nway which is more convenient for MT researchers. The benchmark consists of\nabout 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately\naligned and rearranged in sentences, for a total of about 3.5M tokens per\nlanguage. Talks have been partitioned in train, development and test sets\nsimilarly in all respects to the MT tasks of the IWSLT 2016 evaluation\ncampaign. In addition to describing the benchmark, we list the problems\nencountered in preparing it and the novel methods designed to solve them.\nBaseline MT results and some measures on sentence length are provided as an\nextrinsic evaluation of the quality of the benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 14:44:58 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Cettolo", "Mauro", ""]]}, {"id": "1610.00602", "submitter": "Nikhil Krishnaswamy", "authors": "Nikhil Krishnaswamy and James Pustejovsky", "title": "Multimodal Semantic Simulations of Linguistically Underspecified Motion\n  Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a system for generating three-dimensional visual\nsimulations of natural language motion expressions. We use a rich formal model\nof events and their participants to generate simulations that satisfy the\nminimal constraints entailed by the associated utterance, relying on semantic\nknowledge of physical objects and motion events. This paper outlines technical\nconsiderations and discusses implementing the aforementioned semantic models\ninto such a system.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 15:40:08 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Krishnaswamy", "Nikhil", ""], ["Pustejovsky", "James", ""]]}, {"id": "1610.00634", "submitter": "Anoop Kunchukuttan", "authors": "Anoop Kunchukuttan and Pushpak Bhattacharyya", "title": "Orthographic Syllable as basic unit for SMT between Related Languages", "comments": "7 pages, 1 figure, compiled with XeTex, to be published at the\n  Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of the orthographic syllable, a variable-length\nconsonant-vowel sequence, as a basic unit of translation between related\nlanguages which use abugida or alphabetic scripts. We show that orthographic\nsyllable level translation significantly outperforms models trained over other\nbasic units (word, morpheme and character) when training over small parallel\ncorpora.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 16:53:10 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Kunchukuttan", "Anoop", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1610.00765", "submitter": "Edoardo Maria Ponti", "authors": "Edoardo Maria Ponti, Elisabetta Jezek, Bernardo Magnini", "title": "Distributed Representations of Lexical Sets and Prototypes in Causal\n  Alternation Verbs", "comments": "5 pages, 4 figures, accepted at: Third Italian Conference on\n  Computational Linguistics (CLIC-it). 5-6 December 2016, Napoli (Italy)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical sets contain the words filling an argument slot of a verb, and are in\npart determined by selectional preferences. The purpose of this paper is to\nunravel the properties of lexical sets through distributional semantics. We\ninvestigate 1) whether lexical set behave as prototypical categories with a\ncentre and a periphery; 2) whether they are polymorphic, i.e. composed by\nsubcategories; 3) whether the distance between lexical sets of different\narguments is explanatory of verb properties. In particular, our case study are\nlexical sets of causative-inchoative verbs in Italian. Having studied several\nvector models, we find that 1) based on spatial distance from the centroid,\nobject fillers are scattered uniformly across the category, whereas\nintransitive subject fillers lie on its edge; 2) a correlation exists between\nthe amount of verb senses and that of clusters discovered automatically,\nespecially for intransitive subjects; 3) the distance between the centroids of\nobject and intransitive subject is correlated with other properties of verbs,\nsuch as their cross-lingual tendency to appear in the intransitive pattern\nrather than transitive one. This paper is noncommittal with respect to the\nhypothesis that this connection is underpinned by a semantic reason, namely the\nspontaneity of the event denoted by the verb.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 21:50:27 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 23:48:14 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ponti", "Edoardo Maria", ""], ["Jezek", "Elisabetta", ""], ["Magnini", "Bernardo", ""]]}, {"id": "1610.00842", "submitter": "Chen Li", "authors": "Yandi Xia, Yang Liu", "title": "Chinese Event Extraction Using DeepNeural Network with Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of prior work on event extraction has exploited a variety of features\nto represent events. Such methods have several drawbacks: 1) the features are\noften specific for a particular domain and do not generalize well; 2) the\nfeatures are derived from various linguistic analyses and are error-prone; and\n3) some features may be expensive and require domain expert. In this paper, we\ndevelop a Chinese event extraction system that uses word embedding vectors to\nrepresent language, and deep neural networks to learn the abstract feature\nrepresentation in order to greatly reduce the effort of feature engineering. In\naddition, in this framework, we leverage large amount of unlabeled data, which\ncan address the problem of limited labeled corpus for this task. Our\nexperiments show that our proposed method performs better compared to the\nsystem using rich language features, and using unlabeled data benefits the word\nembeddings. This study suggests the potential of DNN and word embedding for the\nevent extraction task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 04:56:23 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Xia", "Yandi", ""], ["Liu", "Yang", ""]]}, {"id": "1610.00852", "submitter": "Joey Hong", "authors": "Joey Hong, Chris Mattmann, Paul Ramirez", "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author\n  Age Prediction", "comments": "6 pages, 4 figures", "journal-ref": "2017 IEEE International Conference on Information Reuse and\n  Integration (IRI)", "doi": "10.1109/IRI.2017.48", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the internet has created an abundance of unstructured data\non the web, a significant part of which is textual. The task of author\nprofiling seeks to find the demographics of people solely from their linguistic\nand content-based features in text. The ability to describe traits of authors\nclearly has applications in fields such as security and forensics, as well as\nmarketing. Instead of seeing age as just a classification problem, we also\nframe age as a regression one, but use an ensemble chain method that\nincorporates the power of both classification and regression to learn the\nauthors exact age.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 06:01:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Hong", "Joey", ""], ["Mattmann", "Chris", ""], ["Ramirez", "Paul", ""]]}, {"id": "1610.00879", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Abhijit Mishra, Balamurali AR, Pushpak Bhattacharyya,\n  Mark Carman", "title": "A Computational Approach to Automatic Prediction of Drunk Texting", "comments": "This paper was presented at ACL-IJCNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alcohol abuse may lead to unsociable behavior such as crime, drunk driving,\nor privacy leaks. We introduce automatic drunk-texting prediction as the task\nof identifying whether a text was written when under the influence of alcohol.\nWe experiment with tweets labeled using hashtags as distant supervision. Our\nclassifiers use a set of N-gram and stylistic features to detect drunk tweets.\nOur observations present the first quantitative evidence that text contains\nsignals that can be exploited to detect drunk-texting.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 07:34:24 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Joshi", "Aditya", ""], ["Mishra", "Abhijit", ""], ["AR", "Balamurali", ""], ["Bhattacharyya", "Pushpak", ""], ["Carman", "Mark", ""]]}, {"id": "1610.00883", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak Bhattacharyya,\n  Mark Carman", "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?", "comments": "The paper will be presented at Conference on Empirical Methods in\n  Natural Language Processing (EMNLP) 2016 in November 2016.\n  http://www.emnlp2016.net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes a simple increment to state-of-the-art in sarcasm detection\nresearch. Existing approaches are unable to capture subtle forms of context\nincongruity which lies at the heart of sarcasm. We explore if prior work can be\nenhanced using semantic similarity/discordance between word embeddings. We\naugment word embedding-based features to four feature sets reported in the\npast. We also experiment with four types of word embeddings. We observe an\nimprovement in sarcasm detection, irrespective of the word embedding used or\nthe original feature set to which our features are augmented. For example, this\naugmentation results in an improvement in F-score of around 4\\% for three out\nof these four feature sets, and a minor degradation in case of the fourth, when\nWord2Vec embeddings are used. Finally, a comparison of the four embeddings\nshows that Word2Vec and dependency weight-based features outperform LSA and\nGloVe, in terms of their benefit to sarcasm detection.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 07:50:06 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Joshi", "Aditya", ""], ["Tripathi", "Vaibhav", ""], ["Patel", "Kevin", ""], ["Bhattacharyya", "Pushpak", ""], ["Carman", "Mark", ""]]}, {"id": "1610.00956", "submitter": "Ondrej Bajgar", "authors": "Ondrej Bajgar, Rudolf Kadlec and Jan Kleindienst", "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "comments": "The first two authors contributed equally to this work. Submitted to\n  EACL 2017. Code and dataset are publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a practically unlimited amount of natural language data available.\nStill, recent work in text comprehension has focused on datasets which are\nsmall relative to current computing possibilities. This article is making a\ncase for the community to move to larger data and as a step in that direction\nit is proposing the BookTest, a new dataset similar to the popular Children's\nBook Test (CBT), however more than 60 times larger. We show that training on\nthe new data improves the accuracy of our Attention-Sum Reader model on the\noriginal CBT test data by a much larger margin than many recent attempts to\nimprove the model architecture. On one version of the dataset our ensemble even\nexceeds the human baseline provided by Facebook. We then show in our own human\nstudy that there is still space for further improvement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 12:48:51 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bajgar", "Ondrej", ""], ["Kadlec", "Rudolf", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1610.01030", "submitter": "Muhammad Imran", "authors": "Dat Tien Nguyen, Shafiq Joty, Muhammad Imran, Hassan Sajjad, Prasenjit\n  Mitra", "title": "Applications of Online Deep Learning for Crisis Response Using Social\n  Media Information", "comments": "Accepted at SWDM co-located with CIKM 2016. 6 pages, 2 figures. arXiv\n  admin note: text overlap with arXiv:1608.03902", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During natural or man-made disasters, humanitarian response organizations\nlook for useful information to support their decision-making processes. Social\nmedia platforms such as Twitter have been considered as a vital source of\nuseful information for disaster response and management. Despite advances in\nnatural language processing techniques, processing short and informal Twitter\nmessages is a challenging task. In this paper, we propose to use Deep Neural\nNetwork (DNN) to address two types of information needs of response\norganizations: 1) identifying informative tweets and 2) classifying them into\ntopical classes. DNNs use distributed representation of words and learn the\nrepresentation as well as higher level features automatically for the\nclassification task. We propose a new online algorithm based on stochastic\ngradient descent to train DNNs in an online fashion during disaster situations.\nWe test our models using a crisis-related real-world Twitter dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 14:53:51 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 09:50:15 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Joty", "Shafiq", ""], ["Imran", "Muhammad", ""], ["Sajjad", "Hassan", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1610.01076", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Tutorial on Answering Questions about Images with Deep Learning", "comments": "The tutorial was presented at '2nd Summer School on Integrating\n  Vision and Language: Deep Learning' in Malta, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Together with the development of more accurate methods in Computer Vision and\nNatural Language Understanding, holistic architectures that answer on questions\nabout the content of real-world images have emerged. In this tutorial, we build\na neural-based approach to answer questions about images. We base our tutorial\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\nmodels that we present here can achieve a competitive performance on both\ndatasets, in fact, they are among the best methods that use a combination of\nLSTM with a global, full frame CNN representation of an image. We hope that\nafter reading this tutorial, the reader will be able to use Deep Learning\nframeworks, such as Keras and introduced Kraino, to build various architectures\nthat will lead to a further performance improvement on this challenging task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 16:29:28 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1610.01108", "submitter": "Marcin Junczys-Dowmunt", "authors": "Marcin Junczys-Dowmunt and Tomasz Dwojak and Hieu Hoang", "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30\n  Translation Directions", "comments": "Accepted for presentation at IWSLT 2016, Seattle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide the largest published comparison of translation\nquality for phrase-based SMT and neural machine translation across 30\ntranslation directions. For ten directions we also include hierarchical\nphrase-based MT. Experiments are performed for the recently published United\nNations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus.\nIn the second part of the paper we investigate aspects of translation speed,\nintroducing AmuNMT, our efficient neural machine translation decoder. We\ndemonstrate that current neural machine translation could already be used for\nin-production systems when comparing words-per-second ratios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 17:52:42 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 07:35:47 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 18:37:18 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Junczys-Dowmunt", "Marcin", ""], ["Dwojak", "Tomasz", ""], ["Hoang", "Hieu", ""]]}, {"id": "1610.01247", "submitter": "Nikhil Krishnaswamy", "authors": "Tuan Do, Nikhil Krishnaswamy, James Pustejovsky", "title": "ECAT: Event Capture Annotation Tool", "comments": "4 pages, 4 figures, ISA workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Event Capture Annotation Tool (ECAT), a\nuser-friendly, open-source interface tool for annotating events and their\nparticipants in video, capable of extracting the 3D positions and orientations\nof objects in video captured by Microsoft's Kinect(R) hardware. The modeling\nlanguage VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object,\nprogram, and attribute representations, although ECAT uses its own spec for\nexplicit labeling of motion instances. The demonstration will show the tool's\nworkflow and the options available for capturing event-participant relations\nand browsing visual data. Mapping ECAT's output to VoxML will also be\naddressed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 01:24:42 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Do", "Tuan", ""], ["Krishnaswamy", "Nikhil", ""], ["Pustejovsky", "James", ""]]}, {"id": "1610.01291", "submitter": "Laurent Besacier", "authors": "Christophe Servan and Alexandre Berard and Zied Elloumi and Herv\\'e\n  Blanchon and Laurent Besacier", "title": "Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or\n  Lexical Resources?", "comments": "accepted to COLING 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an approach combining lexico-semantic resources and\ndistributed representations of words applied to the evaluation in machine\ntranslation (MT). This study is made through the enrichment of a well-known MT\nevaluation metric: METEOR. This metric enables an approximate match (synonymy\nor morphological similarity) between an automatic and a reference translation.\nOur experiments are made in the framework of the Metrics task of WMT 2014. We\nshow that distributed representations are a good alternative to lexico-semantic\nresources for MT evaluation and they can even bring interesting additional\ninformation. The augmented versions of METEOR, using vector representations,\nare made available on our Github page.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 07:18:42 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Servan", "Christophe", ""], ["Berard", "Alexandre", ""], ["Elloumi", "Zied", ""], ["Blanchon", "Herv\u00e9", ""], ["Besacier", "Laurent", ""]]}, {"id": "1610.01367", "submitter": "Mahdi Khademian", "authors": "Mahdi Khademian and Mohammad Mehdi Homayounpour", "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech\n  Processing Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Pascal challenge entitled monaural multi-talker speech recognition was\ndeveloped, targeting the problem of robust automatic speech recognition against\nspeech like noises which significantly degrades the performance of automatic\nspeech recognition systems. In this challenge, two competing speakers say a\nsimple command simultaneously and the objective is to recognize speech of the\ntarget speaker. Surprisingly during the challenge, a team from IBM research,\ncould achieve a performance better than human listeners on this task. The\nproposed method of the IBM team, consist of an intermediate speech separation\nand then a single-talker speech recognition. This paper reconsiders the task of\nthis challenge based on gain adapted factorial speech processing models. It\ndevelops a joint-token passing algorithm for direct utterance decoding of both\ntarget and masker speakers, simultaneously. Comparing it to the challenge\nwinner, it uses maximum uncertainty during the decoding which cannot be used in\nthe past two-phased method. It provides detailed derivation of inference on\nthese models based on general inference procedures of probabilistic graphical\nmodels. As another improvement, it uses deep neural networks for joint-speaker\nidentification and gain estimation which makes these two steps easier than\nbefore producing competitive results for these steps. The proposed method of\nthis work outperforms past super-human results and even the results were\nachieved recently by Microsoft research, using deep neural networks. It\nachieved 5.5% absolute task performance improvement compared to the first\nsuper-human system and 2.7% absolute task performance improvement compared to\nits recent competitor.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 11:34:36 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Khademian", "Mahdi", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1610.01382", "submitter": "Jamil Ahmad", "authors": "Abdul Malik Badshah, Jamil Ahmad, Mi Young Lee, Sung Wook Baik", "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC\n  and Random Forest", "comments": "8 pages, conference paper, The 2nd International Integrated\n  Conference & Concert on Convergence (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides spoken words, speech signals also carry information about speaker\ngender, age, and emotional state which can be used in a variety of speech\nanalysis applications. In this paper, a divide and conquer strategy for\nensemble classification has been proposed to recognize emotions in speech.\nIntrinsic hierarchy in emotions has been utilized to construct an emotions\ntree, which assisted in breaking down the emotion recognition task into smaller\nsub tasks. The proposed framework generates predictions in three phases.\nFirstly, emotions are detected in the input speech signal by classifying it as\nneutral or emotional. If the speech is classified as emotional, then in the\nsecond phase, it is further classified into positive and negative classes.\nFinally, individual positive or negative emotions are identified based on the\noutcomes of the previous stages. Several experiments have been performed on a\nwidely used benchmark dataset. The proposed method was able to achieve improved\nrecognition rates as compared to several other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 12:16:35 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Badshah", "Abdul Malik", ""], ["Ahmad", "Jamil", ""], ["Lee", "Mi Young", ""], ["Baik", "Sung Wook", ""]]}, {"id": "1610.01465", "submitter": "Kushal Kafle", "authors": "Kushal Kafle, Christopher Kanan", "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2017.06.005", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a recent problem in computer vision and\nnatural language processing that has garnered a large amount of interest from\nthe deep learning, computer vision, and natural language processing\ncommunities. In VQA, an algorithm needs to answer text-based questions about\nimages. Since the release of the first VQA dataset in 2014, additional datasets\nhave been released and many algorithms have been proposed. In this review, we\ncritically examine the current state of VQA in terms of problem formulation,\nexisting datasets, evaluation metrics, and algorithms. In particular, we\ndiscuss the limitations of current datasets with regard to their ability to\nproperly train and assess VQA algorithms. We then exhaustively review existing\nalgorithms for VQA. Finally, we discuss possible future directions for VQA and\nimage understanding research.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:58:36 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 01:39:40 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 05:39:21 GMT"}, {"version": "v4", "created": "Thu, 15 Jun 2017 01:52:59 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "1610.01486", "submitter": "Tiago Tresoldi", "authors": "Tiago Tresoldi", "title": "A tentative model for dimensionless phoneme distance from binary\n  distinctive features", "comments": "draft, 6 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a tentative model for the calculation of dimensionless\ndistances between phonemes; sounds are described with binary distinctive\nfeatures and distances show linear consistency in terms of such features. The\nmodel can be used as a scoring function for local and global pairwise alignment\nof phoneme sequences, and the distances can be used as prior probabilities for\nBayesian analyses on the phylogenetic relationship between languages,\nparticularly for cognate identification in cases where no empirical prior\nprobability is available.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 15:48:08 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 15:41:08 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Tresoldi", "Tiago", ""]]}, {"id": "1610.01508", "submitter": "Nikhil Krishnaswamy", "authors": "James Pustejovsky and Nikhil Krishnaswamy", "title": "VoxML: A Visualization Modeling Language", "comments": "8 pages, 9 figures, proceedings of LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the specification for a modeling language, VoxML, which encodes\nsemantic knowledge of real-world objects represented as three-dimensional\nmodels, and of events and attributes related to and enacted over these objects.\nVoxML is intended to overcome the limitations of existing 3D visual markup\nlanguages by allowing for the encoding of a broad range of semantic knowledge\nthat can be exploited by a variety of systems and platforms, leading to\nmultimodal simulations of real-world scenarios using conceptual objects that\nrepresent their semantic values.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 16:27:48 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Pustejovsky", "James", ""], ["Krishnaswamy", "Nikhil", ""]]}, {"id": "1610.01520", "submitter": "Edgar Altszyler", "authors": "Edgar Altszyler, Mariano Sigman, Sidarta Ribeiro and Diego Fern\\'andez\n  Slezak", "title": "Comparative study of LSA vs Word2vec embeddings in small corpora: a case\n  study in dreams database", "comments": null, "journal-ref": "Conscious Cogn. 2017 Nov;56:178-187", "doi": "10.1016/j.concog.2017.09.004", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been extensively studied in large text datasets.\nHowever, only a few studies analyze semantic representations of small corpora,\nparticularly relevant in single-person text production studies. In the present\npaper, we compare Skip-gram and LSA capabilities in this scenario, and we test\nboth techniques to extract relevant semantic patterns in single-series dreams\nreports. LSA showed better performance than Skip-gram in small size training\ncorpus in two semantic tests. As a study case, we show that LSA can capture\nrelevant words associations in dream reports series, even in cases of small\nnumber of dreams or low-frequency words. We propose that LSA can be used to\nexplore words associations in dreams reports, which could bring new insight\ninto this classic research area of psychology\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 16:47:17 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:43:33 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Altszyler", "Edgar", ""], ["Sigman", "Mariano", ""], ["Ribeiro", "Sidarta", ""], ["Slezak", "Diego Fern\u00e1ndez", ""]]}, {"id": "1610.01546", "submitter": "Yueming Sun", "authors": "Yueming Sun, Yi Zhang, Yunfei Chen, Roger Jin", "title": "Conversational Recommendation System with Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We will demonstrate a conversational products recommendation agent. This\nsystem shows how we combine research in personalized recommendation systems\nwith research in dialogue systems to build a virtual sales agent. Based on new\ndeep learning technologies we developed, the virtual agent is capable of\nlearning how to interact with users, how to answer user questions, what is the\nnext question to ask, and what to recommend when chatting with a human user.\n  Normally a descent conversational agent for a particular domain requires tens\nof thousands of hand labeled conversational data or hand written rules. This is\na major barrier when launching a conversation agent for a new domain. We will\nexplore and demonstrate the effectiveness of the learning solution even when\nthere is no hand written rules or hand labeled training data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 05:46:49 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Sun", "Yueming", ""], ["Zhang", "Yi", ""], ["Chen", "Yunfei", ""], ["Jin", "Roger", ""]]}, {"id": "1610.01561", "submitter": "Koustav Rudra", "authors": "Koustav Rudra, Siddhartha Banerjee, Niloy Ganguly, Pawan Goyal,\n  Muhammad Imran, Prasenjit Mitra", "title": "Summarizing Situational and Topical Information During Crises", "comments": "7 pages, 9 figures, Accepted in The 4th International Workshop on\n  Social Web for Disaster Management (SWDM'16) will be co-located with CIKM\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of microblogging platforms such as Twitter during crises has become\nwidespread. More importantly, information disseminated by affected people\ncontains useful information like reports of missing and found people, requests\nfor urgent needs etc. For rapid crisis response, humanitarian organizations\nlook for situational awareness information to understand and assess the\nseverity of the crisis. In this paper, we present a novel framework (i) to\ngenerate abstractive summaries useful for situational awareness, and (ii) to\ncapture sub-topics and present a short informative summary for each of these\ntopics. A summary is generated using a two stage framework that first extracts\na set of important tweets from the whole set of information through an\nInteger-linear programming (ILP) based optimization technique and then follows\na word graph and concept event based abstractive summarization technique to\nproduce the final summary. High accuracies obtained for all the tasks show the\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 18:39:53 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Rudra", "Koustav", ""], ["Banerjee", "Siddhartha", ""], ["Ganguly", "Niloy", ""], ["Goyal", "Pawan", ""], ["Imran", "Muhammad", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1610.01588", "submitter": "Yftah Ziser", "authors": "Yftah Ziser and Roi Reichart", "title": "Neural Structural Correspondence Learning for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation, adapting models from domains rich in labeled training data\nto domains poor in such data, is a fundamental NLP challenge. We introduce a\nneural network model that marries together ideas from two prominent strands of\nresearch on domain adaptation through representation learning: structural\ncorrespondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural\nnetworks. Particularly, our model is a three-layer neural network that learns\nto encode the nonpivot features of an input example into a low-dimensional\nrepresentation, so that the existence of pivot features (features that are\nprominent in both domains and convey useful information for the NLP task) in\nthe example can be decoded from that representation. The low-dimensional\nrepresentation is then employed in a learning algorithm for the task. Moreover,\nwe show how to inject pre-trained word embeddings into our model in order to\nimprove generalization across examples with similar pivot features. On the task\nof cross-domain product sentiment classification (Blitzer et al., 2007),\nconsisting of 12 domain pairs, our model outperforms both the SCL and the\nmarginalized stacked denoising autoencoder (MSDA, (Chen et al., 2012)) methods\nby 3.77% and 2.17% respectively, on average across domain pairs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 19:57:21 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 19:03:00 GMT"}, {"version": "v3", "created": "Sat, 17 Jun 2017 22:30:57 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ziser", "Yftah", ""], ["Reichart", "Roi", ""]]}, {"id": "1610.01713", "submitter": "Nikhil Krishnaswamy", "authors": "James Pustejovsky and Nikhil Krishnaswamy", "title": "Generating Simulations of Motion Events from Verbal Descriptions", "comments": "11 pages, 5 figures, *SEM workshop, COLING 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a computational model for motion events in natural\nlanguage that maps from linguistic expressions, through a dynamic event\ninterpretation, into three-dimensional temporal simulations in a model.\nStarting with the model from (Pustejovsky and Moszkowicz, 2011), we analyze\nmotion events using temporally-traced Labelled Transition Systems. We model the\ndistinction between path- and manner-motion in an operational semantics, and\nfurther distinguish different types of manner-of-motion verbs in terms of the\nmereo-topological relations that hold throughout the process of movement. From\nthese representations, we generate minimal models, which are realized as\nthree-dimensional simulations in software developed with the game engine,\nUnity. The generated simulations act as a conceptual \"debugger\" for the\nsemantics of different motion verbs: that is, by testing for consistency and\ninformativeness in the model, simulations expose the presuppositions associated\nwith linguistic expressions and their compositions. Because the model\ngeneration component is still incomplete, this paper focuses on an\nimplementation which maps directly from linguistic interpretations into the\nUnity code snippets that create the simulations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 01:56:23 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Pustejovsky", "James", ""], ["Krishnaswamy", "Nikhil", ""]]}, {"id": "1610.01720", "submitter": "Rouzbeh Shirvani", "authors": "French Pope III, Rouzbeh A. Shirvani, Mugizi Robert Rwebangira,\n  Mohamed Chouikha, Ayo Taylor, Andres Alarcon Ramirez, Amirsina Torfi", "title": "Automatic Detection of Small Groups of Persons, Influential Members,\n  Relations and Hierarchy in Written Conversations Using Fuzzy Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays a lot of data is collected in online forums. One of the key tasks is\nto determine the social structure of these online groups, for example the\nidentification of subgroups within a larger group. We will approach the\ngrouping of individual as a classification problem. The classifier will be\nbased on fuzzy logic. The input to the classifier will be linguistic features\nand degree of relationships (among individuals). The output of the classifiers\nare the groupings of individuals. We also incorporate a method that ranks the\nmembers of the detected subgroup to identify the hierarchies in each subgroup.\nData from the HBO television show The Wire is used to analyze the efficacy and\nusefulness of fuzzy logic based methods as alternative methods to classical\nstatistical methods usually used for these problems. The proposed methodology\ncould detect automatically the most influential members of each organization\nThe Wire with 90% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 02:39:29 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Pope", "French", "III"], ["Shirvani", "Rouzbeh A.", ""], ["Rwebangira", "Mugizi Robert", ""], ["Chouikha", "Mohamed", ""], ["Taylor", "Ayo", ""], ["Ramirez", "Andres Alarcon", ""], ["Torfi", "Amirsina", ""]]}, {"id": "1610.01858", "submitter": "Muhammad Imran", "authors": "Muhammad Imran, Sanjay Chawla, Carlos Castillo", "title": "A Robust Framework for Classifying Evolving Document Streams in an\n  Expert-Machine-Crowd Setting", "comments": "Accepted at ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging challenge in the online classification of social media data\nstreams is to keep the categories used for classification up-to-date. In this\npaper, we propose an innovative framework based on an Expert-Machine-Crowd\n(EMC) triad to help categorize items by continuously identifying novel concepts\nin heterogeneous data streams often riddled with outliers. We unify constrained\nclustering and outlier detection by formulating a novel optimization problem:\nCOD-Means. We design an algorithm to solve the COD-Means problem and show that\nCOD-Means will not only help detect novel categories but also seamlessly\ndiscover human annotation errors and improve the overall quality of the\ncategorization process. Experiments on diverse real data sets demonstrate that\nour approach is both effective and efficient.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:20:07 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Imran", "Muhammad", ""], ["Chawla", "Sanjay", ""], ["Castillo", "Carlos", ""]]}, {"id": "1610.01874", "submitter": "Kim Anh Nguyen", "authors": "Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu", "title": "Neural-based Noise Filtering from Word Embeddings", "comments": "9 pages, 4 figures, COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been demonstrated to benefit NLP tasks impressively.\nYet, there is room for improvement in the vector representations, because\ncurrent word embeddings typically contain unnecessary information, i.e., noise.\nWe propose two novel models to improve word embeddings by unsupervised\nlearning, in order to yield word denoising embeddings. The word denoising\nembeddings are obtained by strengthening salient information and weakening\nnoise in the original word embeddings, based on a deep feed-forward neural\nnetwork filter. Results from benchmark tasks show that the filtered word\ndenoising embeddings outperform the original word embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:52:52 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Nguyen", "Kim Anh", ""], ["Walde", "Sabine Schulte im", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1610.01891", "submitter": "Mohamad Ivan Fanany", "authors": "Sadikin Mujiono, Mohamad Ivan Fanany, Chan Basaruddin", "title": "A New Data Representation Based on Training Data Characteristics to\n  Extract Drug Named-Entity in Medical Text", "comments": "Hindawi Publishing. Computational Intelligence and Neuroscience\n  Volume 2016 (2016), Article ID 3483528, 24 pages Received 27 May 2016;\n  Revised 8 August 2016; Accepted 18 September 2016. Special Issue on \"Smart\n  Data: Where the Big Data Meets the Semantics\". Academic Editor: Trong H.\n  Duong", "journal-ref": "Computational Intelligence and Neuroscience Volume 2016 (2016),\n  Article ID 3483528, 24 pages", "doi": null, "report-no": "3483528", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One essential task in information extraction from the medical corpus is drug\nname recognition. Compared with text sources come from other domains, the\nmedical text is special and has unique characteristics. In addition, the\nmedical text mining poses more challenges, e.g., more unstructured text, the\nfast growing of new terms addition, a wide range of name variation for the same\ndrug. The mining is even more challenging due to the lack of labeled dataset\nsources and external knowledge, as well as multiple token representations for a\nsingle drug name that is more common in the real application setting. Although\nmany approaches have been proposed to overwhelm the task, some problems\nremained with poor F-score performance (less than 0.75). This paper presents a\nnew treatment in data representation techniques to overcome some of those\nchallenges. We propose three data representation techniques based on the\ncharacteristics of word distribution and word similarities as a result of word\nembedding training. The first technique is evaluated with the standard NN\nmodel, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two\ndeep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked\nDenoising Encoders). The third technique represents the sentence as a sequence\nthat is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term\nMemory). In extracting the drug name entities, the third technique gives the\nbest F-score performance compared to the state of the art, with its average\nF-score being 0.8645.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 14:38:09 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Mujiono", "Sadikin", ""], ["Fanany", "Mohamad Ivan", ""], ["Basaruddin", "Chan", ""]]}, {"id": "1610.01910", "submitter": "Caroline Brun", "authors": "Amit Navindgi, Caroline Brun, C\\'ecile Boulard Masson, Scott Nowson", "title": "Toward Automatic Understanding of the Function of Affective Language in\n  Support Groups", "comments": "9 pages, 1 figure, conference workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding expressions of emotions in support forums has considerable\nvalue and NLP methods are key to automating this. Many approaches\nunderstandably use subjective categories which are more fine-grained than a\nstraightforward polarity-based spectrum. However, the definition of such\ncategories is non-trivial and, in fact, we argue for a need to incorporate\ncommunicative elements even beyond subjectivity. To support our position, we\nreport experiments on a sentiment-labelled corpus of posts taken from a medical\nsupport forum. We argue that not only is a more fine-grained approach to text\nanalysis important, but simultaneously recognising the social function behind\naffective expressions enable a more accurate and valuable level of\nunderstanding.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 15:23:31 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Navindgi", "Amit", ""], ["Brun", "Caroline", ""], ["Masson", "C\u00e9cile Boulard", ""], ["Nowson", "Scott", ""]]}, {"id": "1610.02003", "submitter": "Paul Baltescu", "authors": "Paul Baltescu", "title": "Scalable Machine Translation in Memory Constrained Environments", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is the discipline concerned with developing automated\ntools for translating from one human language to another. Statistical machine\ntranslation (SMT) is the dominant paradigm in this field. In SMT, translations\nare generated by means of statistical models whose parameters are learned from\nbilingual data. Scalability is a key concern in SMT, as one would like to make\nuse of as much data as possible to train better translation systems.\n  In recent years, mobile devices with adequate computing power have become\nwidely available. Despite being very successful, mobile applications relying on\nNLP systems continue to follow a client-server architecture, which is of\nlimited use because access to internet is often limited and expensive. The goal\nof this dissertation is to show how to construct a scalable machine translation\nsystem that can operate with the limited resources available on a mobile\ndevice.\n  The main challenge for porting translation systems on mobile devices is\nmemory usage. The amount of memory available on a mobile device is far less\nthan what is typically available on the server side of a client-server\napplication. In this thesis, we investigate alternatives for the two components\nwhich prevent standard translation systems from working on mobile devices due\nto high memory usage. We show that once these standard components are replaced\nwith our proposed alternatives, we obtain a scalable translation system that\ncan work on a device with limited memory.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 19:22:49 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Baltescu", "Paul", ""]]}, {"id": "1610.02124", "submitter": "Courtney Napoles", "authors": "Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault", "title": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical\n  Error Correction", "comments": "to appear in Proceedings of the 2016 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for automatically evaluating grammatical error correction\n(GEC) systems rely on gold-standard references. However, these methods suffer\nfrom penalizing grammatical edits that are correct but not in the gold\nstandard. We show that reference-less grammaticality metrics correlate very\nstrongly with human judgments and are competitive with the leading\nreference-based evaluation metrics. By interpolating both methods, we achieve\nstate-of-the-art correlation with human judgments. Finally, we show that GEC\nmetrics are much more reliable when they are calculated at the sentence level\ninstead of the corpus level. We have set up a CodaLab site for benchmarking GEC\noutput using a common dataset and different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 02:17:17 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Napoles", "Courtney", ""], ["Sakaguchi", "Keisuke", ""], ["Tetreault", "Joel", ""]]}, {"id": "1610.02209", "submitter": "Marta R. Costa-juss\\`a", "authors": "Marta R. Costa-juss\\`a and Carlos Escolano", "title": "Morphology Generation for Statistical Machine Translation using Deep\n  Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphology in unbalanced languages remains a big challenge in the context of\nmachine translation. In this paper, we propose to de-couple machine translation\nfrom morphology generation in order to better deal with the problem. We\ninvestigate the morphology simplification with a reasonable trade-off between\nexpected gain and generation complexity. For the Chinese-Spanish task, optimum\nmorphological simplification is in gender and number. For this purpose, we\ndesign a new classification architecture which, compared to other standard\nmachine learning techniques, obtains the best results. This proposed\nneural-based architecture consists of several layers: an embedding, a\nconvolutional followed by a recurrent neural network and, finally, ends with\nsigmoid and softmax layers. We obtain classification results over 98% accuracy\nin gender classification, over 93% in number classification, and an overall\ntranslation improvement of 0.7 METEOR.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 09:59:13 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 15:15:40 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Costa-juss\u00e0", "Marta R.", ""], ["Escolano", "Carlos", ""]]}, {"id": "1610.02213", "submitter": "Ngoc Thang Vu", "authors": "\\\"Ozlem \\c{C}etino\\u{g}lu and Sarah Schulz and Ngoc Thang Vu", "title": "Challenges of Computational Processing of Code-Switching", "comments": "Will appear in the Proceedings of the 2nd Workshop on Computational\n  Approaches to Linguistic Code Switching @EMNLP, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses challenges of Natural Language Processing (NLP) on\nnon-canonical multilingual data in which two or more languages are mixed. It\nrefers to code-switching which has become more popular in our daily life and\ntherefore obtains an increasing amount of attention from the research\ncommunity. We report our experience that cov- ers not only core NLP tasks such\nas normalisation, language identification, language modelling, part-of-speech\ntagging and dependency parsing but also more downstream ones such as machine\ntranslation and automatic speech recognition. We highlight and discuss the key\nproblems for each of the tasks with supporting examples from different language\npairs and relevant previous work.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 10:22:26 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["\u00c7etino\u011flu", "\u00d6zlem", ""], ["Schulz", "Sarah", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1610.02424", "submitter": "Ashwin Kalyan", "authors": "Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing\n  Sun, Stefan Lee, David Crandall, Dhruv Batra", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\n  Models", "comments": "16 pages; accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence models are widely used to model time-series data. Equally\nubiquitous is the usage of beam search (BS) as an approximate inference\nalgorithm to decode output sequences from these models. BS explores the search\nspace in a greedy left-right fashion retaining only the top-B candidates -\nresulting in sequences that differ only slightly from each other. Producing\nlists of nearly identical sequences is not only computationally wasteful but\nalso typically fails to capture the inherent ambiguity of complex AI tasks. To\novercome this problem, we propose Diverse Beam Search (DBS), an alternative to\nBS that decodes a list of diverse outputs by optimizing for a\ndiversity-augmented objective. We observe that our method finds better top-1\nsolutions by controlling for the exploration and exploitation of the search\nspace - implying that DBS is a better search algorithm. Moreover, these gains\nare achieved with minimal computational or memory over- head as compared to\nbeam search. To demonstrate the broad applicability of our method, we present\nresults on image captioning, machine translation and visual question generation\nusing both standard quantitative metrics and qualitative human studies.\nFurther, we study the role of diversity for image-grounded language generation\ntasks as the complexity of the image changes. We observe that our method\nconsistently outperforms BS and previously proposed techniques for diverse\ndecoding from neural sequence models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 20:56:47 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 13:48:32 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Vijayakumar", "Ashwin K", ""], ["Cogswell", "Michael", ""], ["Selvaraju", "Ramprasath R.", ""], ["Sun", "Qing", ""], ["Lee", "Stefan", ""], ["Crandall", "David", ""], ["Batra", "Dhruv", ""]]}, {"id": "1610.02493", "submitter": "Mourad Mars", "authors": "Mourad Mars, Mounir Zrigui, Mohamed Belgacem, Anis Zouaghi", "title": "A Semantic Analyzer for the Comprehension of the Spontaneous Arabic\n  Speech", "comments": "Advances in Computer Science and Engineering. 12 pages 6 figures", "journal-ref": "Journal: Research in Computing Science Vol34, 2008, pp. 129-140,\n  ISSN: 1870-4069", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is part of a large research project entitled \"Or\\'eodule\" aimed at\ndeveloping tools for automatic speech recognition, translation, and synthesis\nfor Arabic language. Our attention has mainly been focused on an attempt to\nimprove the probabilistic model on which our semantic decoder is based. To\nachieve this goal, we have decided to test the influence of the pertinent\ncontext use, and of the contextual data integration of different types, on the\neffectiveness of the semantic decoder. The findings are quite satisfactory.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 06:33:35 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Mars", "Mourad", ""], ["Zrigui", "Mounir", ""], ["Belgacem", "Mohamed", ""], ["Zouaghi", "Anis", ""]]}, {"id": "1610.02544", "submitter": "Aaron Steven White", "authors": "Aaron Steven White, Drew Reisinger, Rachel Rudinger, Kyle Rawlins,\n  Benjamin Van Durme", "title": "Computational linking theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A linking theory explains how verbs' semantic arguments are mapped to their\nsyntactic arguments---the inverse of the Semantic Role Labeling task from the\nshallow semantic parsing literature. In this paper, we develop the\nComputational Linking Theory framework as a method for implementing and testing\nlinking theories proposed in the theoretical literature. We deploy this\nframework to assess two cross-cutting types of linking theory: local v. global\nmodels and categorical v. featural models. To further investigate the behavior\nof these models, we develop a measurement model in the spirit of previous work\nin semantic role induction: the Semantic Proto-Role Linking Model. We use this\nmodel, which implements a generalization of Dowty's seminal Proto-Role Theory,\nto induce semantic proto-roles, which we compare to those Dowty proposes.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 15:24:50 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["White", "Aaron Steven", ""], ["Reisinger", "Drew", ""], ["Rudinger", "Rachel", ""], ["Rawlins", "Kyle", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1610.02567", "submitter": "Abeed Sarker", "authors": "Abbas Chokor, Abeed Sarker, Graciela Gonzalez", "title": "Mining the Web for Pharmacovigilance: the Case Study of Duloxetine and\n  Venlafaxine", "comments": "Masters project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse reactions caused by drugs following their release into the market are\namong the leading causes of death in many countries. The rapid growth of\nelectronically available health related information, and the ability to process\nlarge volumes of them automatically, using natural language processing (NLP)\nand machine learning algorithms, have opened new opportunities for\npharmacovigilance. Survey found that more than 70% of US Internet users consult\nthe Internet when they require medical information. In recent years, research\nin this area has addressed for Adverse Drug Reaction (ADR) pharmacovigilance\nusing social media, mainly Twitter and medical forums and websites. This paper\nwill show the information which can be collected from a variety of Internet\ndata sources and search engines, mainly Google Trends and Google Correlate.\nWhile considering the case study of two popular Major depressive Disorder (MDD)\ndrugs, Duloxetine and Venlafaxine, we will provide a comparative analysis for\ntheir reactions using publicly-available alternative data sources.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 19:43:23 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Chokor", "Abbas", ""], ["Sarker", "Abeed", ""], ["Gonzalez", "Graciela", ""]]}, {"id": "1610.02633", "submitter": "Preslav Nakov", "authors": "Ahmad Musleh and Nadir Durrani and Irina Temnikova and Preslav Nakov\n  and Stephan Vogel and Osama Alsaad", "title": "Enabling Medical Translation for Low-Resource Languages", "comments": "CICLING-2016: 17th International Conference on Intelligent Text\n  Processing and Computational Linguistics, Keywords: Machine Translation,\n  medical translation, doctor-patient communication, resource-poor languages,\n  Hindi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present research towards bridging the language gap between migrant workers\nin Qatar and medical staff. In particular, we present the first steps towards\nthe development of a real-world Hindi-English machine translation system for\ndoctor-patient communication. As this is a low-resource language pair,\nespecially for speech and for the medical domain, our initial focus has been on\ngathering suitable training data from various sources. We applied a variety of\nmethods ranging from fully automatic extraction from the Web to manual\nannotation of test data. Moreover, we developed a method for automatically\naugmenting the training data with synthetically generated variants, which\nyielded a very sizable improvement of more than 3 BLEU points absolute.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 06:42:02 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Musleh", "Ahmad", ""], ["Durrani", "Nadir", ""], ["Temnikova", "Irina", ""], ["Nakov", "Preslav", ""], ["Vogel", "Stephan", ""], ["Alsaad", "Osama", ""]]}, {"id": "1610.02683", "submitter": "Malika Aubakirova", "authors": "Malika Aubakirova, Mohit Bansal", "title": "Interpreting Neural Networks to Improve Politeness Comprehension", "comments": "To appear at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interpretable neural network approach to predicting and\nunderstanding politeness in natural language requests. Our models are based on\nsimple convolutional neural networks directly on raw text, avoiding any manual\nidentification of complex sentiment or syntactic features, while performing\nbetter than such feature-based models from previous work. More importantly, we\nuse the challenging task of politeness prediction as a testbed to next present\na much-needed understanding of what these successful networks are actually\nlearning. For this, we present several network visualizations based on\nactivation clusters, first derivative saliency, and embedding space\ntransformations, helping us automatically identify several subtle linguistics\nmarkers of politeness theories. Further, this analysis reveals multiple novel,\nhigh-scoring politeness strategies which, when added back as new features,\nreduce the accuracy gap between the original featurized system and the neural\nmodel, thus providing a clear quantitative interpretation of the success of\nthese neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 14:42:58 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Aubakirova", "Malika", ""], ["Bansal", "Mohit", ""]]}, {"id": "1610.02692", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Issey Masuda, Santiago Pascual de la Puente and Xavier Giro-i-Nieto", "title": "Open-Ended Visual Question-Answering", "comments": "Bachelor thesis report graded with A with honours at ETSETB Telecom\n  BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016. Source\n  code and models are publicly available at\n  http://imatge-upc.github.io/vqa-2016-cvprw/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis report studies methods to solve Visual Question-Answering (VQA)\ntasks with a Deep Learning framework. As a preliminary step, we explore Long\nShort-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to\ntackle Question-Answering (text based). We then modify the previous model to\naccept an image as an input in addition to the question. For this purpose, we\nexplore the VGG-16 and K-CNN convolutional neural networks to extract visual\nfeatures from the image. These are merged with the word embedding or with a\nsentence embedding of the question to predict the answer. This work was\nsuccessfully submitted to the Visual Question Answering Challenge 2016, where\nit achieved a 53,62% of accuracy in the test dataset. The developed software\nhas followed the best programming practices and Python code style, providing a\nconsistent baseline in Keras for different configurations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 16:38:31 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Masuda", "Issey", ""], ["de la Puente", "Santiago Pascual", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1610.02736", "submitter": "Lucas Lacasa", "authors": "Ivan Gonzalez Torre, Bartolo Luque, Lucas Lacasa, Jordi Luque and\n  Antoni Hernandez-Fernandez", "title": "Emergence of linguistic laws in human voice", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic laws constitute one of the quantitative cornerstones of modern\ncognitive sciences and have been routinely investigated in written corpora, or\nin the equivalent transcription of oral corpora. This means that inferences of\nstatistical patterns of language in acoustics are biased by the arbitrary,\nlanguage-dependent segmentation of the signal, and virtually precludes the\npossibility of making comparative studies between human voice and other animal\ncommunication systems. Here we bridge this gap by proposing a method that\nallows to measure such patterns in acoustic signals of arbitrary origin,\nwithout needs to have access to the language corpus underneath. The method has\nbeen applied to six different human languages, recovering successfully some\nwell-known laws of human communication at timescales even below the phoneme and\nfinding yet another link between complexity and criticality in a biological\nsystem. These methods further pave the way for new comparative studies in\nanimal communication or the analysis of signals of unknown code.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 22:41:19 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Torre", "Ivan Gonzalez", ""], ["Luque", "Bartolo", ""], ["Lacasa", "Lucas", ""], ["Luque", "Jordi", ""], ["Hernandez-Fernandez", "Antoni", ""]]}, {"id": "1610.02749", "submitter": "Huijia Wu", "authors": "Huijia Wu, Jiajun Zhang and Chengqing Zong", "title": "A Dynamic Window Neural Network for CCG Supertagging", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatory Category Grammar (CCG) supertagging is a task to assign lexical\ncategories to each word in a sentence. Almost all previous methods use fixed\ncontext window sizes as input features. However, it is obvious that different\ntags usually rely on different context window sizes. These motivate us to build\na supertagger with a dynamic window approach, which can be treated as an\nattention mechanism on the local contexts. Applying dropout on the dynamic\nfilters can be seen as drop on words directly, which is superior to the regular\ndropout on word embeddings. We use this approach to demonstrate the\nstate-of-the-art CCG supertagging performance on the standard test set.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 01:47:50 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Wu", "Huijia", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1610.02751", "submitter": "Shiyou Lian", "authors": "Shiyou Lian", "title": "A New Theoretical and Technological System of Imprecise-Information\n  Processing", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imprecise-information processing will play an indispensable role in\nintelligent systems, especially in the anthropomorphic intelligent systems (as\nintelligent robots). A new theoretical and technological system of\nimprecise-information processing has been founded in Principles of\nImprecise-Information Processing: A New Theoretical and Technological System[1]\nwhich is different from fuzzy technology. The system has clear hierarchy and\nrigorous structure, which results from the formation principle of imprecise\ninformation and has solid mathematical and logical bases, and which has many\nadvantages beyond fuzzy technology. The system provides a technological\nplatform for relevant applications and lays a theoretical foundation for\nfurther research.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 02:14:16 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Lian", "Shiyou", ""]]}, {"id": "1610.02806", "submitter": "Yao Zhou", "authors": "Yao Zhou, Cong Liu and Yan Pan", "title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "comments": "10 pages, 3 figures, COLING2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe an attentive encoder that combines tree-structured recursive\nneural networks and sequential recurrent neural networks for modelling sentence\npairs. Since existing attentive models exert attention on the sequential\nstructure, we propose a way to incorporate attention into the tree topology.\nSpecially, given a pair of sentences, our attentive encoder uses the\nrepresentation of one sentence, which generated via an RNN, to guide the\nstructural encoding of the other sentence on the dependency parse tree. We\nevaluate the proposed attentive encoder on three tasks: semantic similarity,\nparaphrase identification and true-false question selection. Experimental\nresults show that our encoder outperforms all baselines and achieves\nstate-of-the-art results on two tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 08:52:36 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhou", "Yao", ""], ["Liu", "Cong", ""], ["Pan", "Yan", ""]]}, {"id": "1610.02891", "submitter": "Kaixiang Mo", "authors": "Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, Qiang Yang", "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to train a personalized task-oriented dialogue system because\nthe data collected from each individual is often insufficient. Personalized\ndialogue systems trained on a small dataset can overfit and make it difficult\nto adapt to different user needs. One way to solve this problem is to consider\na collection of multiple users' data as a source domain and an individual\nuser's data as a target domain, and to perform a transfer learning from the\nsource to the target domain. By following this idea, we propose\n\"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework\nbased on POMDP to learn a personalized dialogue system. The system first learns\ncommon dialogue knowledge from the source domain and then adapts this knowledge\nto the target user. This framework can avoid the negative transfer problem by\nconsidering differences between source and target users. The policy in the\npersonalized POMDP can learn to choose different actions appropriately for\ndifferent users. Experimental results on a real-world coffee-shopping data and\nsimulation data show that our personalized dialogue system can choose different\noptimal actions for different users, and thus effectively improve the dialogue\nquality under the personalized setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 12:51:05 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 14:08:42 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 14:05:07 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Mo", "Kaixiang", ""], ["Li", "Shuangyin", ""], ["Zhang", "Yu", ""], ["Li", "Jiajun", ""], ["Yang", "Qiang", ""]]}, {"id": "1610.02906", "submitter": "Xiaofei Sun", "authors": "Xiaofei Sun, Jiang Guo, Xiao Ding and Ting Liu", "title": "A General Framework for Content-enhanced Network Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of network embedding, which aims at\nlearning low-dimensional vector representation of nodes in networks. Most\nexisting network embedding methods rely solely on the network structure, i.e.,\nthe linkage relationships between nodes, but ignore the rich content\ninformation associated with it, which is common in real world networks and\nbeneficial to describing the characteristics of a node. In this paper, we\npropose content-enhanced network embedding (CENE), which is capable of jointly\nleveraging the network structure and the content information. Our approach\nintegrates text modeling and structure modeling in a general framework by\ntreating the content information as a special kind of node. Experiments on\nseveral real world net- works with application to node classification show that\nour models outperform all existing network embedding methods, demonstrating the\nmerits of content information and joint learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 13:27:01 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 02:06:55 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 13:55:02 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Sun", "Xiaofei", ""], ["Guo", "Jiang", ""], ["Ding", "Xiao", ""], ["Liu", "Ting", ""]]}, {"id": "1610.03009", "submitter": "Ali Khodabakhsh", "authors": "Ali Khodabakhsh, Cenk Demiroglu", "title": "Investigation of Synthetic Speech Detection Using Frame- and\n  Segment-Specific Importance Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker verification systems are vulnerable to spoofing attacks which\npresents a major problem in their real-life deployment. To date, most of the\nproposed synthetic speech detectors (SSDs) have weighted the importance of\ndifferent segments of speech equally. However, different attack methods have\ndifferent strengths and weaknesses and the traces that they leave may be short\nor long term acoustic artifacts. Moreover, those may occur for only particular\nphonemes or sounds. Here, we propose three algorithms that weigh\nlikelihood-ratio scores of individual frames, phonemes, and sound-classes\ndepending on their importance for the SSD. Significant improvement over the\nbaseline system has been obtained for known attack methods that were used in\ntraining the SSDs. However, improvement with unknown attack types was not\nsubstantial. Thus, the type of distortions that were caused by the unknown\nsystems were different and could not be captured better with the proposed SSD\ncompared to the baseline SSD.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 18:03:29 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Khodabakhsh", "Ali", ""], ["Demiroglu", "Cenk", ""]]}, {"id": "1610.03017", "submitter": "Jason Lee", "authors": "Jason Lee, Kyunghyun Cho and Thomas Hofmann", "title": "Fully Character-Level Neural Machine Translation without Explicit\n  Segmentation", "comments": "Transactions of the Association for Computational Linguistics (TACL),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing machine translation systems operate at the level of words,\nrelying on explicit segmentation to extract tokens. We introduce a neural\nmachine translation (NMT) model that maps a source character sequence to a\ntarget character sequence without any segmentation. We employ a character-level\nconvolutional network with max-pooling at the encoder to reduce the length of\nsource representation, allowing the model to be trained at a speed comparable\nto subword-level models while capturing local regularities. Our\ncharacter-to-character model outperforms a recently proposed baseline with a\nsubword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable\nperformance on FI-EN and RU-EN. We then demonstrate that it is possible to\nshare a single character-level encoder across multiple languages by training a\nmodel on a many-to-one translation task. In this multilingual setting, the\ncharacter-level encoder significantly outperforms the subword-level encoder on\nall the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality\nof the multilingual character-level translation even surpasses the models\nspecifically trained on that language pair alone, both in terms of BLEU score\nand human judgment.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 18:19:34 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 17:51:32 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 03:32:34 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Lee", "Jason", ""], ["Cho", "Kyunghyun", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1610.03022", "submitter": "Yu Zhang", "authors": "Yu Zhang, William Chan, Navdeep Jaitly", "title": "Very Deep Convolutional Networks for End-to-End Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have shown success in end-to-end speech\nrecognition. However these models have only used shallow acoustic encoder\nnetworks. In our work, we successively train very deep convolutional networks\nto add more expressive power and better generalization for end-to-end ASR\nmodels. We apply network-in-network principles, batch normalization, residual\nconnections and convolutional LSTMs to build very deep recurrent and\nconvolutional structures. Our models exploit the spectral structure in the\nfeature space and add computational depth without overfitting issues. We\nexperiment with the WSJ ASR task and achieve 10.5\\% word error rate without any\ndictionary or language using a 15 layer deep network.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 18:43:58 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhang", "Yu", ""], ["Chan", "William", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1610.03035", "submitter": "William Chan", "authors": "William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly", "title": "Latent Sequence Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes\nsequences with variable lengthed output units as a function of both the input\nsequence and the output sequence. We present a training algorithm which samples\nvalid extensions and an approximate decoding algorithm. We experiment with the\nWall Street Journal speech recognition task. Our LSD model achieves 12.9% WER\ncompared to a character baseline of 14.8% WER. When combined with a\nconvolutional network on the encoder, we achieve 9.6% WER.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:16:08 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 20:11:21 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 17:23:55 GMT"}, {"version": "v4", "created": "Wed, 30 Nov 2016 19:14:17 GMT"}, {"version": "v5", "created": "Thu, 19 Jan 2017 22:23:44 GMT"}, {"version": "v6", "created": "Tue, 7 Feb 2017 15:52:27 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Chan", "William", ""], ["Zhang", "Yu", ""], ["Le", "Quoc", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1610.03098", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek Datla, Ashequl\n  Qadir, Joey Liu, Oladimeji Farri", "title": "Neural Paraphrase Generation with Stacked Residual LSTM Networks", "comments": "COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel neural approach for paraphrase generation.\nConventional para- phrase generation methods either leverage hand-written rules\nand thesauri-based alignments, or use statistical machine learning principles.\nTo the best of our knowledge, this work is the first to explore deep learning\nmodels for paraphrase generation. Our primary contribution is a stacked\nresidual LSTM network, where we add residual connections between LSTM layers.\nThis allows for efficient training of deep LSTMs. We evaluate our model and\nother state-of-the-art deep learning models on three different datasets: PPDB,\nWikiAnswers and MSCOCO. Evaluation results demonstrate that our model\noutperforms sequence to sequence, attention-based and bi- directional LSTM\nmodels on BLEU, METEOR, TER and an embedding-based sentence similarity metric.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 21:01:00 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 15:02:02 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 00:37:33 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Prakash", "Aaditya", ""], ["Hasan", "Sadid A.", ""], ["Lee", "Kathy", ""], ["Datla", "Vivek", ""], ["Qadir", "Ashequl", ""], ["Liu", "Joey", ""], ["Farri", "Oladimeji", ""]]}, {"id": "1610.03106", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan, Patrice Bellot, Frederic Bechet", "title": "Supervised Term Weighting Metrics for Sentiment Analysis in Short Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term weighting metrics assign weights to terms in order to discriminate the\nimportant terms from the less crucial ones. Due to this characteristic, these\nmetrics have attracted growing attention in text classification and recently in\nsentiment analysis. Using the weights given by such metrics could lead to more\naccurate document representation which may improve the performance of the\nclassification. While previous studies have focused on proposing or comparing\ndifferent weighting metrics at two-classes document level sentiment analysis,\nthis study propose to analyse the results given by each metric in order to find\nout the characteristics of good and bad weighting metrics. Therefore we present\nan empirical study of fifteen global supervised weighting metrics with four\nlocal weighting metrics adopted from information retrieval, we also give an\nanalysis to understand the behavior of each metric by observing and analysing\nhow each metric distributes the terms and deduce some characteristics which may\ndistinguish the good and bad metrics. The evaluation has been done using\nSupport Vector Machine on three different datasets: Twitter, restaurant and\nlaptop reviews.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 21:52:47 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Hamdan", "Hussam", ""], ["Bellot", "Patrice", ""], ["Bechet", "Frederic", ""]]}, {"id": "1610.03112", "submitter": "Ran Zhao", "authors": "Tiancheng Zhao, Ran Zhao, Zhao Meng, Justine Cassell", "title": "Leveraging Recurrent Neural Networks for Multimodal Recognition of\n  Social Norm Violation in Dialog", "comments": "Submitted to NIPS Workshop. arXiv admin note: text overlap with\n  arXiv:1608.02977 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social norms are shared rules that govern and facilitate social interaction.\nViolating such social norms via teasing and insults may serve to upend power\nimbalances or, on the contrary reinforce solidarity and rapport in\nconversation, rapport which is highly situated and context-dependent. In this\nwork, we investigate the task of automatically identifying the phenomena of\nsocial norm violation in discourse. Towards this goal, we leverage the power of\nrecurrent neural networks and multimodal information present in the\ninteraction, and propose a predictive model to recognize social norm violation.\nUsing long-term temporal and contextual information, our model achieves an F1\nscore of 0.705. Implications of our work regarding developing a social-aware\nagent are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 22:08:13 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Zhao", "Tiancheng", ""], ["Zhao", "Ran", ""], ["Meng", "Zhao", ""], ["Cassell", "Justine", ""]]}, {"id": "1610.03120", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan", "title": "Correlation-Based Method for Sentiment Classification", "comments": "I'm not convinced about the significance of this paper in its actual\n  state", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic supervised classification algorithms are efficient, but\ntime-consuming, complicated and not interpretable, which makes it difficult to\nanalyze their results that limits the possibility to improve them based on real\nobservations. In this paper, we propose a new and a simple classifier to\npredict a sentiment label of a short text. This model keeps the capacity of\nhuman interpret-ability and can be extended to integrate NLP techniques in a\nmore interpretable way. Our model is based on a correlation metric which\nmeasures the degree of association between a sentiment label and a word. Ten\ncorrelation metrics are proposed and evaluated intrinsically. And then a\nclassifier based on each metric is proposed, evaluated and compared to the\nclassic classification algorithms which have proved their performance in many\nstudies. Our model outperforms these algorithms with several correlation\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 22:35:21 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:37:49 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Hamdan", "Hussam", ""]]}, {"id": "1610.03164", "submitter": "Andrea Daniele", "authors": "Andrea F. Daniele and Mohit Bansal and Matthew R. Walter", "title": "Navigational Instruction Generation as Inverse Reinforcement Learning\n  with Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern robotics applications that involve human-robot interaction require\nrobots to be able to communicate with humans seamlessly and effectively.\nNatural language provides a flexible and efficient medium through which robots\ncan exchange information with their human partners. Significant advancements\nhave been made in developing robots capable of interpreting free-form\ninstructions, but less attention has been devoted to endowing robots with the\nability to generate natural language. We propose a navigational guide model\nthat enables robots to generate natural language instructions that allow humans\nto navigate a priori unknown environments. We first decide which information to\nshare with the user according to their preferences, using a policy trained from\nhuman demonstrations via inverse reinforcement learning. We then \"translate\"\nthis information into a natural language instruction using a neural\nsequence-to-sequence model that learns to generate free-form instructions from\nnatural language corpora. We evaluate our method on a benchmark route\ninstruction dataset and achieve a BLEU score of 72.18% when compared to\nhuman-generated reference instructions. We additionally conduct navigation\nexperiments with human participants that demonstrate that our method generates\ninstructions that people follow as accurately and easily as those produced by\nhumans.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 02:47:09 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Daniele", "Andrea F.", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1610.03165", "submitter": "Xiangang Li", "authors": "Xiangang Li and Xihong Wu", "title": "Long Short-Term Memory based Convolutional Recurrent Neural Networks for\n  Large Vocabulary Speech Recognition", "comments": "Published in INTERSPEECH 2015, September 6-10, 2015, Dresden, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory (LSTM) recurrent neural networks (RNNs) have been\nshown to give state-of-the-art performance on many speech recognition tasks, as\nthey are able to provide the learned dynamically changing contextual window of\nall sequence history. On the other hand, the convolutional neural networks\n(CNNs) have brought significant improvements to deep feed-forward neural\nnetworks (FFNNs), as they are able to better reduce spectral variation in the\ninput signal. In this paper, a network architecture called as convolutional\nrecurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN.\nIn the proposed CRNNs, each speech frame, without adjacent context frames, is\norganized as a number of local feature patches along the frequency axis, and\nthen a LSTM network is performed on each feature patch along the time axis. We\ntrain and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various\nnumber of configurations. Experimental results show that the LSTM CRNNs can\nexceed state-of-the-art speech recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 02:48:13 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Li", "Xiangang", ""], ["Wu", "Xihong", ""]]}, {"id": "1610.03167", "submitter": "Huijia Wu", "authors": "Huijia Wu, Jiajun Zhang and Chengqing Zong", "title": "An Empirical Exploration of Skip Connections for Sequential Tagging", "comments": "Accepted at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we empirically explore the effects of various kinds of skip\nconnections in stacked bidirectional LSTMs for sequential tagging. We\ninvestigate three kinds of skip connections connecting to LSTM cells: (a) skip\nconnections to the gates, (b) skip connections to the internal states and (c)\nskip connections to the cell outputs. We present comprehensive experiments\nshowing that skip connections to cell outputs outperform the remaining two.\nFurthermore, we observe that using gated identity functions as skip mappings\nworks pretty well. Based on this novel skip connections, we successfully train\ndeep stacked bidirectional LSTM models and obtain state-of-the-art results on\nCCG supertagging and comparable results on POS tagging.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 03:02:38 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Wu", "Huijia", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1610.03246", "submitter": "Ma\\'isa Cristina Duarte", "authors": "Maisa C. Duarte and Pierre Maret", "title": "Toward a new instances of NELL", "comments": "6 pages, 1 figure and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We are developing the method to start new instances of NELL in various\nlanguages and develop then NELL multilingualism. We base our method on our\nexperience on NELL Portuguese and NELL French. This reports explain our method\nand develops some research perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 09:19:06 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Duarte", "Maisa C.", ""], ["Maret", "Pierre", ""]]}, {"id": "1610.03256", "submitter": "Tam\\'as Gr\\'osz", "authors": "G\\'abor Gosztolya, Tam\\'as Gr\\'osz, L\\'aszl\\'o T\\'oth", "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training", "comments": null, "journal-ref": null, "doi": "10.21437/Interspeech.2016-391", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attempts have been made to remove Gaussian mixture models (GMM)\nfrom the training process of deep neural network-based hidden Markov models\n(HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two\nproblems, namely the initial alignment of the frame-level state labels and the\ncreation of context-dependent states. Although flat-start training via\niteratively realigning and retraining the DNN using a frame-level error\nfunction is viable, it is quite cumbersome. Here, we propose to use a\nsequence-discriminative training criterion for flat start. While\nsequence-discriminative training is routinely applied only in the final phase\nof model training, we show that with proper caution it is also suitable for\ngetting an alignment of context-independent DNN models. For the construction of\ntied states we apply a recently proposed KL-divergence-based state clustering\nmethod, hence our whole training process is GMM-free. In the experimental\nevaluation we found that the sequence-discriminative flat start training method\nis not only significantly faster than the straightforward approach of iterative\nretraining and realignment, but the word error rates attained are slightly\nbetter as well.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 09:52:57 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Gosztolya", "G\u00e1bor", ""], ["Gr\u00f3sz", "Tam\u00e1s", ""], ["T\u00f3th", "L\u00e1szl\u00f3", ""]]}, {"id": "1610.03321", "submitter": "Barbara Plank", "authors": "Barbara Plank", "title": "Keystroke dynamics as signal for shallow syntactic parsing", "comments": "In COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keystroke dynamics have been extensively used in psycholinguistic and writing\nresearch to gain insights into cognitive processing. But do keystroke logs\ncontain actual signal that can be used to learn better natural language\nprocessing models?\n  We postulate that keystroke dynamics contain information about syntactic\nstructure that can inform shallow syntactic parsing. To test this hypothesis,\nwe explore labels derived from keystroke logs as auxiliary task in a multi-task\nbidirectional Long Short-Term Memory (bi-LSTM). Our results show promising\nresults on two shallow syntactic parsing tasks, chunking and CCG supertagging.\nOur model is simple, has the advantage that data can come from distinct\nsources, and produces models that are significantly better than models trained\non the text annotations alone.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 13:20:52 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Plank", "Barbara", ""]]}, {"id": "1610.03342", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Lieke Gelderloos and Grzegorz Chrupa{\\l}a", "title": "From phonemes to images: levels of representation in a recurrent neural\n  model of visually-grounded language learning", "comments": "Accepted at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a model of visually-grounded language learning based on stacked\ngated recurrent neural networks which learns to predict visual features given\nan image description in the form of a sequence of phonemes. The learning task\nresembles that faced by human language learners who need to discover both\nstructure and meaning from noisy and ambiguous data across modalities. We show\nthat our model indeed learns to predict features of the visual context given\nphonetically transcribed image descriptions, and show that it represents\nlinguistic information in a hierarchy of levels: lower layers in the stack are\ncomparatively more sensitive to form, whereas higher layers are more sensitive\nto meaning.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 14:00:28 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Gelderloos", "Lieke", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "1610.03349", "submitter": "Yevgeni Berzak", "authors": "Helen O'Horan, Yevgeni Berzak, Ivan Vuli\\'c, Roi Reichart, Anna\n  Korhonen", "title": "Survey on the Use of Typological Information in Natural Language\n  Processing", "comments": null, "journal-ref": "COLING 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years linguistic typology, which classifies the world's languages\naccording to their functional and structural properties, has been widely used\nto support multilingual NLP. While the growing importance of typological\ninformation in supporting multilingual tasks has been recognised, no systematic\nsurvey of existing typological resources and their use in NLP has been\npublished. This paper provides such a survey as well as discussion which we\nhope will both inform and inspire future work in the area.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 14:11:55 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["O'Horan", "Helen", ""], ["Berzak", "Yevgeni", ""], ["Vuli\u0107", "Ivan", ""], ["Reichart", "Roi", ""], ["Korhonen", "Anna", ""]]}, {"id": "1610.03585", "submitter": "Jon Gauthier", "authors": "Jon Gauthier, Igor Mordatch", "title": "A Paradigm for Situated and Goal-Driven Language Learning", "comments": "5 pages, submitted to Machine Intelligence @ NIPS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distinguishing property of human intelligence is the ability to flexibly\nuse language in order to communicate complex ideas with other humans in a\nvariety of contexts. Research in natural language dialogue should focus on\ndesigning communicative agents which can integrate themselves into these\ncontexts and productively collaborate with humans. In this abstract, we propose\na general situated language learning paradigm which is designed to bring about\nrobust language agents able to cooperate productively with humans.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 02:45:45 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Gauthier", "Jon", ""], ["Mordatch", "Igor", ""]]}, {"id": "1610.03708", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Christof Monz, Arnold W.M. Smeulders", "title": "Generating captions without looking beyond objects", "comments": "This paper was presented at the ECCV2016 2nd Workshop on Storytelling\n  with Images and Videos (VisStory)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores new evaluation perspectives for image captioning and\nintroduces a noun translation task that achieves comparative image caption\ngeneration performance by translating from a set of nouns to captions. This\nimplies that in image captioning, all word categories other than nouns can be\nevoked by a powerful language model without sacrificing performance on n-gram\nprecision. The paper also investigates lower and upper bounds of how much\nindividual word categories in the captions contribute to the final BLEU score.\nA large possible improvement exists for nouns, verbs, and prepositions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 13:42:03 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 09:35:03 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Heuer", "Hendrik", ""], ["Monz", "Christof", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1610.03750", "submitter": "Shanshan Zhang", "authors": "Shanshan Zhang, Slobodan Vucetic", "title": "Semi-supervised Discovery of Informative Tweets During the Emerging\n  Disasters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first objective towards the effective use of microblogging services such\nas Twitter for situational awareness during the emerging disasters is discovery\nof the disaster-related postings. Given the wide range of possible disasters,\nusing a pre-selected set of disaster-related keywords for the discovery is\nsuboptimal. An alternative that we focus on in this work is to train a\nclassifier using a small set of labeled postings that are becoming available as\na disaster is emerging. Our hypothesis is that utilizing large quantities of\nhistorical microblogs could improve the quality of classification, as compared\nto training a classifier only on the labeled data. We propose to use unlabeled\nmicroblogs to cluster words into a limited number of clusters and use the word\nclusters as features for classification. To evaluate the proposed\nsemi-supervised approach, we used Twitter data from 6 different disasters. Our\nresults indicate that when the number of labeled tweets is 100 or less, the\nproposed approach is superior to the standard classification based on the bag\nor words feature representation. Our results also reveal that the choice of the\nunlabeled corpus, the choice of word clustering algorithm, and the choice of\nhyperparameters can have a significant impact on the classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 15:26:30 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Zhang", "Shanshan", ""], ["Vucetic", "Slobodan", ""]]}, {"id": "1610.03759", "submitter": "Victor Makarenkov", "authors": "Victor Makarenkov, Bracha Shapira, Lior Rokach", "title": "Language Models with Pre-Trained (GloVe) Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we implement a training of a Language Model (LM), using\nRecurrent Neural Network (RNN) and GloVe word embeddings, introduced by\nPennigton et al. in [1]. The implementation is following the general idea of\ntraining RNNs for LM tasks presented in [2], but is rather using Gated\nRecurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM\n[4].\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 15:53:02 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 11:24:05 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Makarenkov", "Victor", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "1610.03771", "submitter": "Marzieh Saeidi Marzieh Saeidi", "authors": "Marzieh Saeidi, Guillaume Bouchard, Maria Liakata, Sebastian Riedel", "title": "SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban\n  Neighbourhoods", "comments": "Accepted at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the task of targeted aspect-based sentiment\nanalysis. The goal is to extract fine-grained information with respect to\nentities mentioned in user comments. This work extends both aspect-based\nsentiment analysis that assumes a single entity per document and targeted\nsentiment analysis that assumes a single sentiment towards a target entity. In\nparticular, we identify the sentiment towards each aspect of one or more\nentities. As a testbed for this task, we introduce the SentiHood dataset,\nextracted from a question answering (QA) platform where urban neighbourhoods\nare discussed by users. In this context units of text often mention several\naspects of one or more neighbourhoods. This is the first time that a generic\nsocial media platform in this case a QA platform, is used for fine-grained\nopinion mining. Text coming from QA platforms is far less constrained compared\nto text from review specific platforms which current datasets are based on. We\ndevelop several strong baselines, relying on logistic regression and\nstate-of-the-art recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 16:23:11 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Saeidi", "Marzieh", ""], ["Bouchard", "Guillaume", ""], ["Liakata", "Maria", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1610.03807", "submitter": "Linfeng Song", "authors": "Linfeng Song and Lin Zhao", "title": "Question Generation from a Knowledge Base with Web Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question generation from a knowledge base (KB) is the task of generating\nquestions related to the domain of the input KB. We propose a system for\ngenerating fluent and natural questions from a KB, which significantly reduces\nthe human effort by leveraging massive web resources. In more detail, a seed\nquestion set is first generated by applying a small number of hand-crafted\ntemplates on the input KB, then more questions are retrieved by iteratively\nforming already obtained questions as search queries into a standard search\nengine, before finally questions are selected by estimating their fluency and\ndomain relevance. Evaluated by human graders on 500 random-selected triples\nfrom Freebase, questions generated by our system are judged to be more fluent\nthan those of \\newcite{serban-EtAl:2016:P16-1} by human graders.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 17:55:56 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 23:31:47 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Song", "Linfeng", ""], ["Zhao", "Lin", ""]]}, {"id": "1610.03914", "submitter": "Kiran Vodrahalli", "authors": "Kiran Vodrahalli, Po-Hsuan Chen, Yingyu Liang, Christopher Baldassano,\n  Janice Chen, Esther Yong, Christopher Honey, Uri Hasson, Peter Ramadge, Ken\n  Norman, Sanjeev Arora", "title": "Mapping Between fMRI Responses to Movies and their Natural Language\n  Annotations", "comments": "19 pages, 9 figures, in submission to NeuroImage. Prior version\n  presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437) and ICML 2016\n  Workshop on Multi-view Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research groups have shown how to correlate fMRI responses to the\nmeanings of presented stimuli. This paper presents new methods for doing so\nwhen only a natural language annotation is available as the description of the\nstimulus. We study fMRI data gathered from subjects watching an episode of BBCs\nSherlock [1], and learn bidirectional mappings between fMRI responses and\nnatural language representations. We show how to leverage data from multiple\nsubjects watching the same movie to improve the accuracy of the mappings,\nallowing us to succeed at a scene classification task with 72% accuracy (random\nguessing would give 4%) and at a scene ranking task with average rank in the\ntop 4% (random guessing would give 50%). The key ingredients are (a) the use of\nthe Shared Response Model (SRM) and its variant SRM-ICA [2, 3] to aggregate\nfMRI data from multiple subjects, both of which are shown to be superior to\nstandard PCA in producing low-dimensional representations for the tasks in this\npaper; (b) a sentence embedding technique adapted from the natural language\nprocessing (NLP) literature [4] that produces semantic vector representation of\nthe annotations; (c) using previous timestep information in the featurization\nof the predictor data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 02:20:45 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 20:29:41 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 08:41:51 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Vodrahalli", "Kiran", ""], ["Chen", "Po-Hsuan", ""], ["Liang", "Yingyu", ""], ["Baldassano", "Christopher", ""], ["Chen", "Janice", ""], ["Yong", "Esther", ""], ["Honey", "Christopher", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter", ""], ["Norman", "Ken", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1610.03934", "submitter": "Krupakar Hans", "authors": "Hans Krupakar, Keerthika Rajvel, Bharathi B, Angel Deborah S,\n  Vallidevi Krishnamurthy", "title": "A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder", "comments": "(8 pages, 7 figures, IEEE Digital Xplore paper)", "journal-ref": "2016 International Conference on Information Communication and\n  Embedded Systems (ICICES), Chennai, 2016, pp. 1-9", "doi": "10.1109/ICICES.2016.7518940", "report-no": null, "categories": "cs.CL cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech Translation has always been about giving source text or audio input\nand waiting for system to give translated output in desired form. In this\npaper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice\near-piece translation device. We introduce and survey the recent advances made\nin the field of Speech Engineering, to employ in the ADD, particularly focusing\non the three major processing steps of Recognition, Translation and Synthesis.\nWe tackle the problem of machine understanding of natural language by designing\na recognition unit for source audio to text, a translation unit for source\nlanguage text to target language text, and a synthesis unit for target language\ntext to target language speech. Speech from the surroundings will be recorded\nby the recognition unit present on the ear-piece and translation will start as\nsoon as one sentence is successfully read. This way, we hope to give translated\noutput as and when input is being read. The recognition unit will use Hidden\nMarkov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory\ncells, and the synthesis unit, HMM based speech synthesis system HTS. This\nsystem will initially be built as an English to Tamil translation device.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 04:10:58 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Krupakar", "Hans", ""], ["Rajvel", "Keerthika", ""], ["B", "Bharathi", ""], ["S", "Angel Deborah", ""], ["Krishnamurthy", "Vallidevi", ""]]}, {"id": "1610.03946", "submitter": "Jessica Ficler", "authors": "Jessica Ficler, Yoav Goldberg", "title": "A Neural Network for Coordination Boundary Prediction", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural-network based model for coordination boundary prediction.\nThe network is designed to incorporate two signals: the similarity between\nconjuncts and the observation that replacing the whole coordination phrase with\na conjunct tends to produce a coherent sentences. The modeling makes use of\nseveral LSTM networks. The model is trained solely on conjunction annotations\nin a Treebank, without using external resources. We show improvements on\npredicting coordination boundaries on the PTB compared to two state-of-the-art\nparsers; as well as improvement over previous coordination boundary prediction\nsystems on the Genia corpus.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 06:42:51 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Ficler", "Jessica", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1610.03950", "submitter": "Lili Mou", "authors": "Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, Zhi Jin", "title": "Compressing Neural Language Models by Sparse Word Representations", "comments": "ACL-16, pages 226--235", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are among the state-of-the-art techniques for language\nmodeling. Existing neural language models typically map discrete words to\ndistributed, dense vector representations. After information processing of the\npreceding context words by hidden layers, an output layer estimates the\nprobability of the next word. Such approaches are time- and memory-intensive\nbecause of the large numbers of parameters for word embeddings and the output\nlayer. In this paper, we propose to compress neural language models by sparse\nword representations. In the experiments, the number of parameters in our model\nincreases very slowly with the growth of the vocabulary size, which is almost\nimperceptible. Moreover, our approach not only reduces the parameter space to a\nlarge extent, but also improves the performance in terms of the perplexity\nmeasure.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 06:55:54 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Chen", "Yunchuan", ""], ["Mou", "Lili", ""], ["Xu", "Yan", ""], ["Li", "Ge", ""], ["Jin", "Zhi", ""]]}, {"id": "1610.03955", "submitter": "Lili Mou", "authors": "Yiping Song, Lili Mou, Rui Yan, Li Yi, Zinan Zhu, Xiaohua Hu, Ming\n  Zhang", "title": "Dialogue Session Segmentation by Embedding-Enhanced TextTiling", "comments": "INTERSPEECH-16, pages 2706--2710", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-computer conversation systems, the context of a user-issued\nutterance is particularly important because it provides useful background\ninformation of the conversation. However, it is unwise to track all previous\nutterances in the current session as not all of them are equally important. In\nthis paper, we address the problem of session segmentation. We propose an\nembedding-enhanced TextTiling approach, inspired by the observation that\nconversation utterances are highly noisy, and that word embeddings provide a\nrobust way of capturing semantics. Experimental results show that our approach\nachieves better performance than the TextTiling, MMD approaches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 07:07:50 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Song", "Yiping", ""], ["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Yi", "Li", ""], ["Zhu", "Zinan", ""], ["Hu", "Xiaohua", ""], ["Zhang", "Ming", ""]]}, {"id": "1610.04120", "submitter": "Lina Rojas-Barahona", "authors": "Lina M. Rojas Barahona, Milica Gasic, Nikola Mrk\\v{s}i\\'c, Pei-Hao Su,\n  Stefan Ultes, Tsung-Hsien Wen and Steve Young", "title": "Exploiting Sentence and Context Representations in Deep Neural Models\n  for Spoken Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning architecture for the semantic decoder\ncomponent of a Statistical Spoken Dialogue System. In a slot-filling dialogue,\nthe semantic decoder predicts the dialogue act and a set of slot-value pairs\nfrom a set of n-best hypotheses returned by the Automatic Speech Recognition.\nMost current models for spoken language understanding assume (i) word-aligned\nsemantic annotations as in sequence taggers and (ii) delexicalisation, or a\nmapping of input words to domain-specific concepts using heuristics that try to\ncapture morphological variation but that do not scale to other domains nor to\nlanguage variation (e.g., morphology, synonyms, paraphrasing ). In this work\nthe semantic decoder is trained using unaligned semantic annotations and it\nuses distributed semantic representation learning to overcome the limitations\nof explicit delexicalisation. The proposed architecture uses a convolutional\nneural network for the sentence representation and a long-short term memory\nnetwork for the context representation. Results are presented for the publicly\navailable DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a\nsignificantly higher word error rate (WER).\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 15:11:40 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Barahona", "Lina M. Rojas", ""], ["Gasic", "Milica", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Su", "Pei-Hao", ""], ["Ultes", "Stefan", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1610.04211", "submitter": "Julien Perez", "authors": "Julien Perez and Fei Liu", "title": "Gated End-to-End Memory Networks", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading using differentiable reasoning models has recently shown\nremarkable progress. In this context, End-to-End trainable Memory Networks,\nMemN2N, have demonstrated promising performance on simple natural language\nbased reasoning tasks such as factual reasoning and basic deduction. However,\nother tasks, namely multi-fact question-answering, positional reasoning or\ndialog related tasks, remain challenging particularly due to the necessity of\nmore complex interactions between the memory and controller modules composing\nthis family of models. In this paper, we introduce a novel end-to-end memory\naccess regulation mechanism inspired by the current progress on the connection\nshort-cutting principle in the field of computer vision. Concretely, we develop\na Gated End-to-End trainable Memory Network architecture, GMemN2N. From the\nmachine learning perspective, this new capability is learned in an end-to-end\nfashion without the use of any additional supervision signal which is, as far\nas our knowledge goes, the first of its kind. Our experiments show significant\nimprovements on the most challenging tasks in the 20 bAbI dataset, without the\nuse of any domain knowledge. Then, we show improvements on the dialog bAbI\ntasks including the real human-bot conversion-based Dialog State Tracking\nChallenge (DSTC-2) dataset. On these two datasets, our model sets the new state\nof the art.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 19:38:03 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 15:09:29 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Perez", "Julien", ""], ["Liu", "Fei", ""]]}, {"id": "1610.04265", "submitter": "Hieu Hoang", "authors": "Hieu Hoang, Nikolay Bogoychev, Lane Schwartz, Marcin Junczys-Dowmunt", "title": "Fast, Scalable Phrase-Based SMT Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utilization of statistical machine translation (SMT) has grown enormously\nover the last decade, many using open-source software developed by the NLP\ncommunity. As commercial use has increased, there is need for software that is\noptimized for commercial requirements, in particular, fast phrase-based\ndecoding and more efficient utilization of modern multicore servers.\n  In this paper we re-examine the major components of phrase-based decoding and\ndecoder implementation with particular emphasis on speed and scalability on\nmulticore machines. The result is a drop-in replacement for the Moses decoder\nwhich is up to fifteen times faster and scales monotonically with the number of\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 21:25:34 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 22:32:18 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Hoang", "Hieu", ""], ["Bogoychev", "Nikolay", ""], ["Schwartz", "Lane", ""], ["Junczys-Dowmunt", "Marcin", ""]]}, {"id": "1610.04345", "submitter": "Julien Perez", "authors": "Fei Liu and Julien Perez and Scott Nowson", "title": "A Language-independent and Compositional Model for Personality Trait\n  Recognition from Short Texts", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been used to recognize author personality traits from text,\ntypically combining linguistic feature engineering with shallow learning\nmodels, e.g. linear regression or Support Vector Machines. This work uses\ndeep-learning-based models and atomic features of text, the characters, to\nbuild hierarchical, vectorial word and sentence representations for trait\ninference. This method, applied to a corpus of tweets, shows state-of-the-art\nperformance across five traits and three languages (English, Spanish and\nItalian) compared with prior work in author profiling. The results, supported\nby preliminary visualisation work, are encouraging for the ability to detect\ncomplex human traits.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 07:14:44 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Liu", "Fei", ""], ["Perez", "Julien", ""], ["Nowson", "Scott", ""]]}, {"id": "1610.04377", "submitter": "Diptesh Kanojia", "authors": "Diptesh Kanojia, Vishwajeet Kumar, and Krithi Ramamritham", "title": "Civique: Using Social Media to Detect Urban Emergencies", "comments": "This paper was presented at Workshop on Social Data Analytics and\n  Management (SoDAM 2016), at Very Large Databases (VLDB 2016), in September\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Civique system for emergency detection in urban areas by\nmonitoring micro blogs like Tweets. The system detects emergency related\nevents, and classifies them into appropriate categories like \"fire\",\n\"accident\", \"earthquake\", etc. We demonstrate our ideas by classifying Twitter\nposts in real time, visualizing the ongoing event on a map interface and\nalerting users with options to contact relevant authorities, both online and\noffline. We evaluate our classifiers for both the steps, i.e., emergency\ndetection and categorization, and obtain F-scores exceeding 70% and 90%,\nrespectively. We demonstrate Civique using a web interface and on an Android\napplication, in realtime, and show its use for both tweet detection and\nvisualization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 09:06:36 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Kanojia", "Diptesh", ""], ["Kumar", "Vishwajeet", ""], ["Ramamritham", "Krithi", ""]]}, {"id": "1610.04416", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis, Mehrnoosh Sadrzadeh", "title": "Distributional Inclusion Hypothesis for Tensor-based Composition", "comments": "To appear in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to the distributional inclusion hypothesis, entailment between\nwords can be measured via the feature inclusions of their distributional\nvectors. In recent work, we showed how this hypothesis can be extended from\nwords to phrases and sentences in the setting of compositional distributional\nsemantics. This paper focuses on inclusion properties of tensors; its main\ncontribution is a theoretical and experimental analysis of how feature\ninclusion works in different concrete models of verb tensors. We present\nresults for relational, Frobenius, projective, and holistic methods and compare\nthem to the simple vector addition, multiplication, min, and max models. The\ndegrees of entailment thus obtained are evaluated via a variety of existing\nword-based measures, such as Weed's and Clarke's, KL-divergence, APinc,\nbalAPinc, and two of our previously proposed metrics at the phrase/sentence\nlevel. We perform experiments on three entailment datasets, investigating which\nversion of tensor-based composition achieves the highest performance when\ncombined with the sentence-level measures.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 11:52:19 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1610.04533", "submitter": "Issa Atoum", "authors": "Issa Atoum, Ahmed Otoom and Narayanan Kulathuramaiyer", "title": "A Comprehensive Comparative Study of Word and Sentence Similarity\n  Measures", "comments": "7 pages,4 figures", "journal-ref": "International Journal of Computer Applications,2016,135(1),\n  Foundation of Computer Science (FCS), NY, USA", "doi": "10.5120/ijca2016908259", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence similarity is considered the basis of many natural language tasks\nsuch as information retrieval, question answering and text summarization. The\nsemantic meaning between compared text fragments is based on the words semantic\nfeatures and their relationships. This article reviews a set of word and\nsentence similarity measures and compares them on benchmark datasets. On the\nstudied datasets, results showed that hybrid semantic measures perform better\nthan both knowledge and corpus based measures.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 19:33:47 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Atoum", "Issa", ""], ["Otoom", "Ahmed", ""], ["Kulathuramaiyer", "Narayanan", ""]]}, {"id": "1610.04658", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Anna Choromanska and David Sontag", "title": "Simultaneous Learning of Trees and Representations for Extreme\n  Classification and Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-class classification where the predictor has a hierarchical\nstructure that allows for a very large number of labels both at train and test\ntime. The predictive power of such models can heavily depend on the structure\nof the tree, and although past work showed how to learn the tree structure, it\nexpected that the feature vectors remained static. We provide a novel algorithm\nto simultaneously perform representation learning for the input data and\nlearning of the hierarchi- cal predictor. Our approach optimizes an objec- tive\nfunction which favors balanced and easily- separable multi-way node partitions.\nWe theoret- ically analyze this objective, showing that it gives rise to a\nboosting style property and a bound on classification error. We next show how\nto extend the algorithm to conditional density estimation. We empirically\nvalidate both variants of the al- gorithm on text classification and language\nmod- eling, respectively, and show that they compare favorably to common\nbaselines in terms of accu- racy and running time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 22:03:15 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 20:33:14 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jernite", "Yacine", ""], ["Choromanska", "Anna", ""], ["Sontag", "David", ""]]}, {"id": "1610.04718", "submitter": "Andrey Vasnetsov", "authors": "Roman Samarev, Andrey Vasnetsov, Elizaveta Smelkova", "title": "Generalization of metric classification algorithms for sequences\n  classification and labelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article deals with the issue of modification of metric classification\nalgorithms. In particular, it studies the algorithm k-Nearest Neighbours for\nits application to sequential data. A method of generalization of metric\nclassification algorithms is proposed. As a part of it, there has been\ndeveloped an algorithm for solving the problem of classification and labelling\nof sequential data. The advantages of the developed algorithm of classification\nin comparison with the existing one are also discussed in the article. There is\na comparison of the effectiveness of the proposed algorithm with the algorithm\nof CRF in the task of chunking in the open data set CoNLL2000.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 11:04:44 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 05:32:34 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Samarev", "Roman", ""], ["Vasnetsov", "Andrey", ""], ["Smelkova", "Elizaveta", ""]]}, {"id": "1610.04814", "submitter": "Mahamad Suhil", "authors": "D S Guru and Mahamad Suhil", "title": "Term-Class-Max-Support (TCMS): A Simple Text Document Categorization\n  Approach Using Term-Class Relevance Measure", "comments": "4 Pages, 4 Figures; 2016 Intl. Conference on Advances in Computing,\n  Communications and Informatics (ICACCI), Sept. 21-24, 2016, Jaipur, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple text categorization method using term-class relevance\nmeasures is proposed. Initially, text documents are processed to extract\nsignificant terms present in them. For every term extracted from a document, we\ncompute its importance in preserving the content of a class through a novel\nterm-weighting scheme known as Term_Class Relevance (TCR) measure proposed by\nGuru and Suhil (2015) [1]. In this way, for every term, its relevance for all\nthe classes present in the corpus is computed and stored in the knowledgebase.\nDuring testing, the terms present in the test document are extracted and the\nterm-class relevance of each term is obtained from the stored knowledgebase. To\nachieve quick search of term weights, Btree indexing data structure has been\nadapted. Finally, the class which receives maximum support in terms of\nterm-class relevance is decided to be the class of the given test document. The\nproposed method works in logarithmic complexity in testing time and simple to\nimplement when compared to any other text categorization techniques available\nin literature. The experiments conducted on various benchmarking datasets have\nrevealed that the performance of the proposed method is satisfactory and\nencouraging.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 03:40:13 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Guru", "D S", ""], ["Suhil", "Mahamad", ""]]}, {"id": "1610.04841", "submitter": "Raj Nath Patel", "authors": "Raj Nath Patel and Sasikumar M", "title": "Translation Quality Estimation using Recurrent Neural Network", "comments": "7 pages, published at First Conference on Machine Translation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to the shared task on word/phrase level\nQuality Estimation (QE) in the First Conference on Statistical Machine\nTranslation (WMT16). The objective of the shared task was to predict if the\ngiven word/phrase is a correct/incorrect (OK/BAD) translation in the given\nsentence. In this paper, we propose a novel approach for word level Quality\nEstimation using Recurrent Neural Network Language Model (RNN-LM) architecture.\nRNN-LMs have been found very effective in different Natural Language Processing\n(NLP) applications. RNN-LM is mainly used for vector space language modeling\nfor different NLP problems. For this task, we modify the architecture of\nRNN-LM. The modified system predicts a label (OK/BAD) in the slot rather than\npredicting the word. The input to the system is a word sequence, similar to the\nstandard RNN-LM. The approach is language independent and requires only the\ntranslated text for QE. To estimate the phrase level quality, we use the output\nof the word level QE system.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 10:54:23 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 07:01:05 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Patel", "Raj Nath", ""], ["M", "Sasikumar", ""]]}, {"id": "1610.04989", "submitter": "Jiacheng Xu", "authors": "Jiacheng Xu, Danlu Chen, Xipeng Qiu and Xuangjing Huang", "title": "Cached Long Short-Term Memory Neural Networks for Document-Level\n  Sentiment Classification", "comments": "Published as long paper of EMNLP2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural networks have achieved great success on sentiment\nclassification due to their ability to alleviate feature engineering. However,\none of the remaining challenges is to model long texts in document-level\nsentiment classification under a recurrent architecture because of the\ndeficiency of the memory unit. To address this problem, we present a Cached\nLong Short-Term Memory neural networks (CLSTM) to capture the overall semantic\ninformation in long texts. CLSTM introduces a cache mechanism, which divides\nmemory into several groups with different forgetting rates and thus enables the\nnetwork to keep sentiment information better within a recurrent unit. The\nproposed CLSTM outperforms the state-of-the-art models on three publicly\navailable document-level sentiment analysis datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 07:28:06 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Xu", "Jiacheng", ""], ["Chen", "Danlu", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuangjing", ""]]}, {"id": "1610.05011", "submitter": "Fandong Meng", "authors": "Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu", "title": "Interactive Attention for Neural Machine Translation", "comments": "Accepted at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional attention-based Neural Machine Translation (NMT) conducts\ndynamic alignment in generating the target sentence. By repeatedly reading the\nrepresentation of source sentence, which keeps fixed after generated by the\nencoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced\nstate-of-the-art NMT. In this paper, we propose a new attention mechanism,\ncalled INTERACTIVE ATTENTION, which models the interaction between the decoder\nand the representation of source sentence during translation by both reading\nand writing operations. INTERACTIVE ATTENTION can keep track of the interaction\nhistory and therefore improve the translation performance. Experiments on NIST\nChinese-English translation task show that INTERACTIVE ATTENTION can achieve\nsignificant improvements over both the previous attention-based NMT baseline\nand some state-of-the-art variants of attention-based NMT (i.e., coverage\nmodels (Tu et al., 2016)). And neural machine translator with our INTERACTIVE\nATTENTION can outperform the open source attention-based NMT system Groundhog\nby 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU\npoints averagely on multiple test sets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 08:33:20 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Meng", "Fandong", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Liu", "Qun", ""]]}, {"id": "1610.05150", "submitter": "Xing Wang", "authors": "Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang Li, Deyi Xiong, Min Zhang", "title": "Neural Machine Translation Advised by Statistical Machine Translation", "comments": "Accepted by AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) is a new approach to machine translation\nthat has made great progress in recent years. However, recent studies show that\nNMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu\net al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to\nconventional Statistical Machine Translation (SMT), which usually yields\nadequate but non-fluent translations. It is natural, therefore, to leverage the\nadvantages of both models for better translations, and in this work we propose\nto incorporate SMT model into NMT framework. More specifically, at each\ndecoding step, SMT offers additional recommendations of generated words based\non the decoding information from NMT (e.g., the generated partial translation\nand attention history). Then we employ an auxiliary classifier to score the SMT\nrecommendations and a gating function to combine the SMT recommendations with\nNMT generations, both of which are jointly trained within the NMT architecture\nin an end-to-end manner. Experimental results on Chinese-English translation\nshow that the proposed approach achieves significant and consistent\nimprovements over state-of-the-art NMT and SMT systems on multiple NIST test\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:50:58 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 02:38:35 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Wang", "Xing", ""], ["Lu", "Zhengdong", ""], ["Tu", "Zhaopeng", ""], ["Li", "Hang", ""], ["Xiong", "Deyi", ""], ["Zhang", "Min", ""]]}, {"id": "1610.05243", "submitter": "Jan Niehues", "authors": "Jan Niehues, Eunah Cho, Thanh-Le Ha and Alex Waibel", "title": "Pre-Translation for Neural Machine Translation", "comments": "9 pages. To appear in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the development of neural machine translation (NMT) has\nsignificantly improved the translation quality of automatic machine\ntranslation. While most sentences are more accurate and fluent than\ntranslations by statistical machine translation (SMT)-based systems, in some\ncases, the NMT system produces translations that have a completely different\nmeaning. This is especially the case when rare words occur.\n  When using statistical machine translation, it has already been shown that\nsignificant gains can be achieved by simplifying the input in a preprocessing\nstep. A commonly used example is the pre-reordering approach.\n  In this work, we used phrase-based machine translation to pre-translate the\ninput into the target language. Then a neural machine translation system\ngenerates the final hypothesis using the pre-translation. Thereby, we use\neither only the output of the phrase-based machine translation (PBMT) system or\na combination of the PBMT output and the source sentence.\n  We evaluate the technique on the English to German translation task. Using\nthis approach we are able to outperform the PBMT system as well as the baseline\nneural MT system by up to 2 BLEU points. We analyzed the influence of the\nquality of the initial system on the final result.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:14:24 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Niehues", "Jan", ""], ["Cho", "Eunah", ""], ["Ha", "Thanh-Le", ""], ["Waibel", "Alex", ""]]}, {"id": "1610.05256", "submitter": "Andreas Stolcke", "authors": "W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu\n  and G. Zweig", "title": "Achieving Human Parity in Conversational Speech Recognition", "comments": "Revised for publication, updated results", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2016-71, revised Feb. 2017", "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational speech recognition has served as a flagship speech recognition\ntask since the release of the Switchboard corpus in the 1990s. In this paper,\nwe measure the human error rate on the widely used NIST 2000 test set, and find\nthat our latest automated system has reached human parity. The error rate of\nprofessional transcribers is 5.9% for the Switchboard portion of the data, in\nwhich newly acquainted pairs of people discuss an assigned topic, and 11.3% for\nthe CallHome portion where friends and family members have open-ended\nconversations. In both cases, our automated system establishes a new state of\nthe art, and edges past the human benchmark, achieving error rates of 5.8% and\n11.0%, respectively. The key to our system's performance is the use of various\nconvolutional and LSTM acoustic model architectures, combined with a novel\nspatial smoothing method and lattice-free MMI acoustic training, multiple\nrecurrent neural network language modeling approaches, and a systematic use of\nsystem combination.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:40:50 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 07:32:58 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Xiong", "W.", ""], ["Droppo", "J.", ""], ["Huang", "X.", ""], ["Seide", "F.", ""], ["Seltzer", "M.", ""], ["Stolcke", "A.", ""], ["Yu", "D.", ""], ["Zweig", "G.", ""]]}, {"id": "1610.05361", "submitter": "Hassan Taherian", "authors": "Hassan Taherian", "title": "End-to-end attention-based distant speech recognition with Highway LSTM", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  End-to-end attention-based models have been shown to be competitive\nalternatives to conventional DNN-HMM models in the Speech Recognition Systems.\nIn this paper, we extend existing end-to-end attention-based models that can be\napplied for Distant Speech Recognition (DSR) task. Specifically, we propose an\nend-to-end attention-based speech recognizer with multichannel input that\nperforms sequence prediction directly at the character level. To gain a better\nperformance, we also incorporate Highway long short-term memory (HLSTM) which\noutperforms previous models on AMI distant speech recognition task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 21:16:56 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Taherian", "Hassan", ""]]}, {"id": "1610.05461", "submitter": "Ella Rabinovich", "authors": "Ella Rabinovich, Shachar Mirkin, Raj Nath Patel, Lucia Specia and\n  Shuly Wintner", "title": "Personalized Machine Translation: Preserving Original Author Traits", "comments": "EACL 2017, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The language that we produce reflects our personality, and various personal\nand demographic characteristics can be detected in natural language texts. We\nfocus on one particular personal trait of the author, gender, and study how it\nis manifested in original texts and in translations. We show that author's\ngender has a powerful, clear signal in originals texts, but this signal is\nobfuscated in human and machine translation. We then propose simple\ndomain-adaptation techniques that help retain the original gender traits in the\ntranslation, without harming the quality of the translation, thereby creating\nmore personalized machine translation systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 07:39:40 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:43:05 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Rabinovich", "Ella", ""], ["Mirkin", "Shachar", ""], ["Patel", "Raj Nath", ""], ["Specia", "Lucia", ""], ["Wintner", "Shuly", ""]]}, {"id": "1610.05522", "submitter": "Giovanni Da San Martino", "authors": "Giovanni Da San Martino, Alberto Barr\\'on-Cede\\~no, Salvatore Romeo,\n  Alessandro Moschitti, Shafiq Joty, Fahad A. Al Obaidli, Kateryna Tymoshenko,\n  Antonio Uva", "title": "Addressing Community Question Answering in English and Arabic", "comments": "presented at Second WebQA workshop, SIGIR2016\n  (http://plg2.cs.uwaterloo.ca/~avtyurin/WebQA2016/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the impact of different types of features applied to\nlearning to re-rank questions in community Question Answering. We tested our\nmodels on two datasets released in SemEval-2016 Task 3 on \"Community Question\nAnswering\". Task 3 targeted real-life Web fora both in English and Arabic. Our\nmodels include bag-of-words features (BoW), syntactic tree kernels (TKs), rank\nfeatures, embeddings, and machine translation evaluation features. To the best\nof our knowledge, structural kernels have barely been applied to the question\nreranking task, where they have to model paraphrase relations. In the case of\nthe English question re-ranking task, we compare our learning to rank (L2R)\nalgorithms against a strong baseline given by the Google-generated ranking\n(GR). The results show that i) the shallow structures used in our TKs are\nrobust enough to noisy data and ii) improving GR is possible, but effective BoW\nfeatures and TKs along with an accurate model of GR features in the used L2R\nalgorithm are required. In the case of the Arabic question re-ranking task, for\nthe first time we applied tree kernels on syntactic trees of Arabic sentences.\nOur approaches to both tasks obtained the second best results on SemEval-2016\nsubtasks B on English and D on Arabic.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 10:22:46 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Romeo", "Salvatore", ""], ["Moschitti", "Alessandro", ""], ["Joty", "Shafiq", ""], ["Obaidli", "Fahad A. Al", ""], ["Tymoshenko", "Kateryna", ""], ["Uva", "Antonio", ""]]}, {"id": "1610.05540", "submitter": "Josep Crego", "authors": "Josep Crego, Jungi Kim, Guillaume Klein, Anabel Rebollo, Kathy Yang,\n  Jean Senellart, Egor Akhanov, Patrice Brunelle, Aurelien Coquard, Yongchao\n  Deng, Satoshi Enoue, Chiyo Geiss, Joshua Johanson, Ardas Khalsa, Raoum\n  Khiari, Byeongil Ko, Catherine Kobus, Jean Lorieux, Leidiana Martins,\n  Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia Segal,\n  Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou,\n  Peter Zoldan", "title": "SYSTRAN's Pure Neural Machine Translation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first online demonstration of Neural Machine Translation (NMT) by\nLISA, NMT development has recently moved from laboratory to production systems\nas demonstrated by several entities announcing roll-out of NMT engines to\nreplace their existing technologies. NMT systems have a large number of\ntraining configurations and the training process of such systems is usually\nvery long, often a few weeks, so role of experimentation is critical and\nimportant to share. In this work, we present our approach to production-ready\nsystems simultaneously with release of online demonstrators covering a large\nvariety of languages (12 languages, for 32 language pairs). We explore\ndifferent practical choices: an efficient and evolutive open-source framework;\ndata preparation; network architecture; additional implemented features; tuning\nfor production; etc. We discuss about evaluation methodology, present our first\nfindings and we finally outline further work.\n  Our ultimate goal is to share our expertise to build competitive production\nsystems for \"generic\" translation. We aim at contributing to set up a\ncollaborative framework to speed-up adoption of the technology, foster further\nresearch efforts and enable the delivery and adoption to/by industry of\nuse-case specific engines integrated in real production workflows. Mastering of\nthe technology would allow us to build translation engines suited for\nparticular needs, outperforming current simplest/uniform systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 11:32:42 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Crego", "Josep", ""], ["Kim", "Jungi", ""], ["Klein", "Guillaume", ""], ["Rebollo", "Anabel", ""], ["Yang", "Kathy", ""], ["Senellart", "Jean", ""], ["Akhanov", "Egor", ""], ["Brunelle", "Patrice", ""], ["Coquard", "Aurelien", ""], ["Deng", "Yongchao", ""], ["Enoue", "Satoshi", ""], ["Geiss", "Chiyo", ""], ["Johanson", "Joshua", ""], ["Khalsa", "Ardas", ""], ["Khiari", "Raoum", ""], ["Ko", "Byeongil", ""], ["Kobus", "Catherine", ""], ["Lorieux", "Jean", ""], ["Martins", "Leidiana", ""], ["Nguyen", "Dang-Chuan", ""], ["Priori", "Alexandra", ""], ["Riccardi", "Thomas", ""], ["Segal", "Natalia", ""], ["Servan", "Christophe", ""], ["Tiquet", "Cyril", ""], ["Wang", "Bo", ""], ["Yang", "Jin", ""], ["Zhang", "Dakun", ""], ["Zhou", "Jing", ""], ["Zoldan", "Peter", ""]]}, {"id": "1610.05652", "submitter": "Phuong Le-Hong", "authors": "Phuong Le-Hong", "title": "Vietnamese Named Entity Recognition using Token Regular Expressions and\n  Bidirectional Inference", "comments": "Submitted to the VLSP Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an efficient approach to improve the accuracy of a named\nentity recognition system for Vietnamese. The approach combines regular\nexpressions over tokens and a bidirectional inference method in a sequence\nlabelling model. The proposed method achieves an overall $F_1$ score of 89.66%\non a test set of an evaluation campaign, organized in late 2016 by the\nVietnamese Language and Speech Processing (VLSP) community.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 14:44:01 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 14:25:42 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Le-Hong", "Phuong", ""]]}, {"id": "1610.05654", "submitter": "Ramon Ferrer i Cancho", "authors": "Antoni Hern\\'andez-Fern\\'andez and Ramon Ferrer-i-Cancho", "title": "The infochemical core", "comments": "Little corrections of format and language", "journal-ref": "Journal of Quantitative Linguistics 23 (2), 133-153 (2016)", "doi": "10.1080/09296174.2016.1142323", "report-no": null, "categories": "q-bio.NC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vocalizations and less often gestures have been the object of linguistic\nresearch over decades. However, the development of a general theory of\ncommunication with human language as a particular case requires a clear\nunderstanding of the organization of communication through other means.\nInfochemicals are chemical compounds that carry information and are employed by\nsmall organisms that cannot emit acoustic signals of optimal frequency to\nachieve successful communication. Here the distribution of infochemicals across\nspecies is investigated when they are ranked by their degree or the number of\nspecies with which it is associated (because they produce or they are sensitive\nto it). The quality of the fit of different functions to the dependency between\ndegree and rank is evaluated with a penalty for the number of parameters of the\nfunction. Surprisingly, a double Zipf (a Zipf distribution with two regimes\nwith a different exponent each) is the model yielding the best fit although it\nis the function with the largest number of parameters. This suggests that the\nworld wide repertoire of infochemicals contains a chemical nucleus shared by\nmany species and reminiscent of the core vocabularies found for human language\nin dictionaries or large corpora.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 14:53:20 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 11:22:52 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Hern\u00e1ndez-Fern\u00e1ndez", "Antoni", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1610.05670", "submitter": "Mark Eisen", "authors": "Mark Eisen, Santiago Segarra, Gabriel Egan, Alejandro Ribeiro", "title": "Stylometric Analysis of Early Modern Period English Plays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function word adjacency networks (WANs) are used to study the authorship of\nplays from the Early Modern English period. In these networks, nodes are\nfunction words and directed edges between two nodes represent the relative\nfrequency of directed co-appearance of the two words. For every analyzed play,\na WAN is constructed and these are aggregated to generate author profile\nnetworks. We first study the similarity of writing styles between Early English\nplaywrights by comparing the profile WANs. The accuracy of using WANs for\nauthorship attribution is then demonstrated by attributing known plays among\nsix popular playwrights. Moreover, the WAN method is shown to outperform other\nfrequency-based methods on attributing Early English plays. In addition, WANs\nare shown to be reliable classifiers even when attributing collaborative plays.\nFor several plays of disputed co-authorship, a deeper analysis is performed by\nattributing every act and scene separately, in which we both corroborate\nexisting breakdowns and provide evidence of new assignments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 15:22:14 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 21:00:00 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Eisen", "Mark", ""], ["Segarra", "Santiago", ""], ["Egan", "Gabriel", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1610.05688", "submitter": "Pranay Dighe", "authors": "Pranay Dighe, Afsaneh Asaei and Herve Bourlard", "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2017.7953161", "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional deep neural networks (DNN) for speech acoustic modeling rely on\nGaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary\nclass labels as the targets for DNN training. Subword classes in speech\nrecognition systems correspond to context-dependent tied states or senones. The\npresent work addresses some limitations of GMM-HMM senone alignments for DNN\ntraining. We hypothesize that the senone probabilities obtained from a DNN\ntrained with binary labels can provide more accurate targets to learn better\nacoustic models. However, DNN outputs bear inaccuracies which are exhibited as\nhigh dimensional unstructured noise, whereas the informative components are\nstructured and low-dimensional. We exploit principle component analysis (PCA)\nand sparse coding to characterize the senone subspaces. Enhanced probabilities\nobtained from low-rank and sparse reconstructions are used as soft-targets for\nDNN acoustic modeling, that also enables training with untranscribed data.\nExperiments conducted on AMI corpus shows 4.6% relative reduction in word error\nrate.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 16:02:10 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Dighe", "Pranay", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herve", ""]]}, {"id": "1610.05812", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "comments": "9 pages, 6 figures. Accepted to IEEE/ACM Transactions on Audio,\n  Speech and Language Processing, 2017. arXiv admin note: text overlap with\n  arXiv:1608.00892, arXiv:1607.01963", "journal-ref": null, "doi": "10.1109/TASLP.2017.2698723", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art speech recognition systems typically employ neural network\nacoustic models. However, compared to Gaussian mixture models, deep neural\nnetwork (DNN) based acoustic models often have many more model parameters,\nmaking it challenging for them to be deployed on resource-constrained\nplatforms, such as mobile devices. In this paper, we study the application of\nthe recently proposed highway deep neural network (HDNN) for training\nsmall-footprint acoustic models. HDNNs are a depth-gated feedforward neural\nnetwork, which include two types of gate functions to facilitate the\ninformation flow through different layers. Our study demonstrates that HDNNs\nare more compact than regular DNNs for acoustic modeling, i.e., they can\nachieve comparable recognition accuracy with many fewer model parameters.\nFurthermore, HDNNs are more controllable than DNNs: the gate functions of an\nHDNN can control the behavior of the whole network using a very small number of\nmodel parameters. Finally, we show that HDNNs are more adaptable than DNNs. For\nexample, simply updating the gate functions using adaptation data can result in\nconsiderable gains in accuracy. We demonstrate these aspects by experiments\nusing the publicly available AMI corpus, which has around 80 hours of training\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:06:01 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 21:12:56 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 15:45:22 GMT"}, {"version": "v4", "created": "Tue, 25 Apr 2017 19:48:41 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1610.05858", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy, Ehsan Zare Borzeshi, Massimo Piccardi", "title": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "comments": "This paper \"Bidirectional LSTM-CRF for Clinical Concept Extraction\"\n  is accepted for short paper presentation at Clinical Natural Language\n  Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of concepts present in patient clinical records is an essential\nstep in clinical research. The 2010 i2b2/VA Workshop on Natural Language\nProcessing Challenges for clinical records presented concept extraction (CE)\ntask, with aim to identify concepts (such as treatments, tests, problems) and\nclassify them into predefined categories. State-of-the-art CE approaches\nheavily rely on hand crafted features and domain specific resources which are\nhard to collect and tune. For this reason, this paper employs bidirectional\nLSTM with CRF decoding initialized with general purpose off-the-shelf word\nembeddings for CE. The experimental results achieved on 2010 i2b2/VA reference\nstandard corpora using bidirectional LSTM CRF ranks closely with top ranked\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 03:59:30 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Chalapathy", "Raghavendra", ""], ["Borzeshi", "Ehsan Zare", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1610.05948", "submitter": "Dhananjay Ram", "authors": "Dhananjay Ram, Debasis Kundu, Rajesh M. Hegde", "title": "A Bayesian Approach to Estimation of Speaker Normalization Parameters", "comments": "23 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a Bayesian approach to speaker normalization is proposed to\ncompensate for the degradation in performance of a speaker independent speech\nrecognition system. The speaker normalization method proposed herein uses the\ntechnique of vocal tract length normalization (VTLN). The VTLN parameters are\nestimated using a novel Bayesian approach which utilizes the Gibbs sampler, a\nspecial type of Markov Chain Monte Carlo method. Additionally the\nhyperparameters are estimated using maximum likelihood approach. This model is\nused assuming that human vocal tract can be modeled as a tube of uniform cross\nsection. It captures the variation in length of the vocal tract of different\nspeakers more effectively, than the linear model used in literature. The work\nhas also investigated different methods like minimization of Mean Square Error\n(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both\nsingle pass and two pass approaches are then used to build a VTLN based speech\nrecognizer. Experimental results on recognition of vowels and Hindi phrases\nfrom a medium vocabulary indicate that the Bayesian method improves the\nperformance by a considerable margin.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:16:46 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ram", "Dhananjay", ""], ["Kundu", "Debasis", ""], ["Hegde", "Rajesh M.", ""]]}, {"id": "1610.06053", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama", "title": "Chinese Restaurant Process for cognate clustering: A threshold free\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a threshold free approach, motivated from Chinese\nRestaurant Process, for the purpose of cognate clustering. We show that our\napproach yields similar results to a linguistically motivated cognate\nclustering system known as LexStat. Our Chinese Restaurant Process system is\nfast and does not require any threshold and can be applied to any language\nfamily of the world.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:09:21 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Rama", "Taraka", ""]]}, {"id": "1610.06210", "submitter": "Rik Koncel-Kedziorski", "authors": "Rik Koncel-Kedziorski, Ioannis Konstas, Luke Zettlemoyer, and Hannaneh\n  Hajishirzi", "title": "A Theme-Rewriting Approach for Generating Algebra Word Problems", "comments": "To appear EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texts present coherent stories that have a particular theme or overall\nsetting, for example science fiction or western. In this paper, we present a\ntext generation method called {\\it rewriting} that edits existing\nhuman-authored narratives to change their theme without changing the underlying\nstory. We apply the approach to math word problems, where it might help\nstudents stay more engaged by quickly transforming all of their homework\nassignments to the theme of their favorite movie without changing the math\nconcepts that are being taught. Our rewriting method uses a two-stage decoding\nprocess, which proposes new words from the target theme and scores the\nresulting stories according to a number of factors defining aspects of\nsyntactic, semantic, and thematic coherence. Experiments demonstrate that the\nfinal stories typically represent the new theme well while still testing the\noriginal math concepts, outperforming a number of baselines. We also release a\nnew dataset of human-authored rewrites of math word problems in several themes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 20:49:23 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Koncel-Kedziorski", "Rik", ""], ["Konstas", "Ioannis", ""], ["Zettlemoyer", "Luke", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1610.06227", "submitter": "Mohammad Sadegh Rasooli", "authors": "Mohammad Sadegh Rasooli, Michael Collins", "title": "Cross-Lingual Syntactic Transfer with Limited Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple but effective method for cross-lingual syntactic\ntransfer of dependency parsers, in the scenario where a large amount of\ntranslation data is not available. The method makes use of three steps: 1) a\nmethod for deriving cross-lingual word clusters, which can then be used in a\nmultilingual parser; 2) a method for transferring lexical information from a\ntarget language to source language treebanks; 3) a method for integrating these\nsteps with the density-driven annotation projection method of Rasooli and\nCollins (2015). Experiments show improvements over the state-of-the-art in\nseveral languages used in previous work, in a setting where the only source of\ntranslation data is the Bible, a considerably smaller corpus than the Europarl\ncorpus used in previous work. Results using the Europarl corpus as a source of\ntranslation data show additional improvements over the results of Rasooli and\nCollins (2015). We conclude with results on 38 datasets from the Universal\nDependencies corpora.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 21:25:39 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 04:05:00 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Rasooli", "Mohammad Sadegh", ""], ["Collins", "Michael", ""]]}, {"id": "1610.06272", "submitter": "Bonggun Shin", "authors": "Bonggun Shin, Timothy Lee and Jinho D. Choi", "title": "Lexicon Integrated CNN Models with Attention for Sentiment Analysis", "comments": "In Proceedings of the EMNLP Workshop on Computational Approaches to\n  Subjectivity, Sentiment and Social Media Analysis, of WASSA'17, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the advent of word embeddings, lexicons are no longer fully utilized for\nsentiment analysis although they still provide important features in the\ntraditional setting. This paper introduces a novel approach to sentiment\nanalysis that integrates lexicon embeddings and an attention mechanism into\nConvolutional Neural Networks. Our approach performs separate convolutions for\nword and lexicon embeddings and provides a global view of the document using\nattention. Our models are experimented on both the SemEval'16 Task 4 dataset\nand the Stanford Sentiment Treebank, and show comparative or better results\nagainst the existing state-of-the-art systems. Our analysis shows that lexicon\nembeddings allow to build high-performing models with much smaller word\nembeddings, and the attention mechanism effectively dims out noisy words for\nsentiment analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 03:10:57 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 21:21:30 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Shin", "Bonggun", ""], ["Lee", "Timothy", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1610.06370", "submitter": "Georgios Spithourakis", "authors": "Georgios P. Spithourakis and Steffen E. Petersen and Sebastian Riedel", "title": "Clinical Text Prediction with Numerically Grounded Conditional Language\n  Models", "comments": "Accepted at the 7th International Workshop on Health Text Mining and\n  Information Analysis (LOUHI) EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assisted text input techniques can save time and effort and improve text\nquality. In this paper, we investigate how grounded and conditional extensions\nto standard neural language models can bring improvements in the tasks of word\nprediction and completion. These extensions incorporate a structured knowledge\nbase and numerical values from the text into the context used to predict the\nnext word. Our automated evaluation on a clinical dataset shows extended models\nsignificantly outperform standard models. Our best system uses both\nconditioning and grounding, because of their orthogonal benefits. For word\nprediction with a list of 5 suggestions, it improves recall from 25.03% to\n71.28% and for word completion it improves keystroke savings from 34.35% to\n44.81%, where theoretical bound for this dataset is 58.78%. We also perform a\nqualitative investigation of how models with lower perplexity occasionally fare\nbetter at the tasks. We found that at test time numbers have more influence on\nthe document level than on individual word probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 11:48:30 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Spithourakis", "Georgios P.", ""], ["Petersen", "Steffen E.", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1610.06454", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Reasoning with Memory Augmented Neural Networks for Language\n  Comprehension", "comments": "Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is an important cognitive process that supports human\nreasoning. In this paper, we introduce a computational hypothesis testing\napproach based on memory augmented neural networks. Our approach involves a\nhypothesis testing loop that reconsiders and progressively refines a previously\nformed hypothesis in order to generate new hypotheses to test. We apply the\nproposed approach to language comprehension task by using Neural Semantic\nEncoders (NSE). Our NSE models achieve the state-of-the-art results showing an\nabsolute improvement of 1.2% to 2.6% accuracy over previous results obtained by\nsingle and ensemble systems on standard machine comprehension benchmarks such\nas the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:17:04 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:06:17 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1610.06498", "submitter": "Diego Amancio Dr.", "authors": "Jeaneth Machicao, Edilson A. Corr\\^ea Jr., Gisele H. B. Miranda, Diego\n  R. Amancio, Odemir M. Bruno", "title": "Authorship Attribution Based on Life-Like Network Automata", "comments": null, "journal-ref": "PLoS ONE 13(3): e0193703, 2018", "doi": "10.1371/journal.pone.0193703", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authorship attribution is a problem of considerable practical and\ntechnical interest. Several methods have been designed to infer the authorship\nof disputed documents in multiple contexts. While traditional statistical\nmethods based solely on word counts and related measurements have provided a\nsimple, yet effective solution in particular cases; they are prone to\nmanipulation. Recently, texts have been successfully modeled as networks, where\nwords are represented by nodes linked according to textual similarity\nmeasurements. Such models are useful to identify informative topological\npatterns for the authorship recognition task. However, there is no consensus on\nwhich measurements should be used. Thus, we proposed a novel method to\ncharacterize text networks, by considering both topological and dynamical\naspects of networks. Using concepts and methods from cellular automata theory,\nwe devised a strategy to grasp informative spatio-temporal patterns from this\nmodel. Our experiments revealed an outperformance over traditional analysis\nrelying only on topological measurements. Remarkably, we have found a\ndependence of pre-processing steps (such as the lemmatization) on the obtained\nresults, a feature that has mostly been disregarded in related works. The\noptimized results obtained here pave the way for a better characterization of\ntextual networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 17:00:42 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Machicao", "Jeaneth", ""], ["Corr\u00eaa", "Edilson A.", "Jr."], ["Miranda", "Gisele H. B.", ""], ["Amancio", "Diego R.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1610.06510", "submitter": "Anoop Kunchukuttan", "authors": "Anoop Kunchukuttan and Pushpak Bhattacharyya", "title": "Learning variable length units for SMT between related languages via\n  Byte Pair Encoding", "comments": "Accepted at First Workshop on Subword and Character LEvel Models in\n  NLP (SCLeM) to be held at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of segments learnt using Byte Pair Encoding (referred to\nas BPE units) as basic units for statistical machine translation between\nrelated languages and compare it with orthographic syllables, which are\ncurrently the best performing basic units for this translation task. BPE\nidentifies the most frequent character sequences as basic units, while\northographic syllables are linguistically motivated pseudo-syllables. We show\nthat BPE units modestly outperform orthographic syllables as units of\ntranslation, showing up to 11% increase in BLEU score. While orthographic\nsyllables can be used only for languages whose writing systems use vowel\nrepresentations, BPE is writing system independent and we show that BPE\noutperforms other units for non-vowel writing systems too. Our results are\nsupported by extensive experimentation spanning multiple language families and\nwriting systems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 17:32:32 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 17:22:45 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 21:33:00 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Kunchukuttan", "Anoop", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1610.06540", "submitter": "Shubham Toshniwal", "authors": "Shubham Toshniwal, Karen Livescu", "title": "Jointly Learning to Align and Convert Graphemes to Phonemes with Neural\n  Attention Models", "comments": "Accepted in SLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an attention-enabled encoder-decoder model for the problem of\ngrapheme-to-phoneme conversion. Most previous work has tackled the problem via\njoint sequence models that require explicit alignments for training. In\ncontrast, the attention-enabled encoder-decoder model allows for jointly\nlearning to align and convert characters to phonemes. We explore different\ntypes of attention models, including global and local attention, and our best\nmodels achieve state-of-the-art results on three standard data sets (CMUDict,\nPronlex, and NetTalk).\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:00:48 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Toshniwal", "Shubham", ""], ["Livescu", "Karen", ""]]}, {"id": "1610.06542", "submitter": "Graham Neubig", "authors": "Graham Neubig", "title": "Lexicons and Minimum Risk Training for Neural Machine Translation:\n  NAIST-CMU at WAT2016", "comments": "To Appear in the Workshop on Asian Translation (WAT). arXiv admin\n  note: text overlap with arXiv:1606.02006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This year, the Nara Institute of Science and Technology (NAIST)/Carnegie\nMellon University (CMU) submission to the Japanese-English translation track of\nthe 2016 Workshop on Asian Translation was based on attentional neural machine\ntranslation (NMT) models. In addition to the standard NMT model, we make a\nnumber of improvements, most notably the use of discrete translation lexicons\nto improve probability estimates, and the use of minimum risk training to\noptimize the MT system for BLEU score. As a result, our system achieved the\nhighest translation evaluation scores for the task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:10:09 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Neubig", "Graham", ""]]}, {"id": "1610.06550", "submitter": "Alexander Rosenberg Johansen", "authors": "Alexander Rosenberg Johansen, Jonas Meinertz Hansen, Elias Khazen\n  Obeid, Casper Kaae S{\\o}nderby, Ole Winther", "title": "Neural Machine Translation with Characters and Hierarchical Encoding", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Neural Machine Translation models use groups of characters or\nwhole words as their unit of input and output. We propose a model with a\nhierarchical char2word encoder, that takes individual characters both as input\nand output. We first argue that this hierarchical representation of the\ncharacter encoder reduces computational complexity, and show that it improves\ntranslation performance. Secondly, by qualitatively studying attention plots\nfrom the decoder we find that the model learns to compress common words into a\nsingle embedding whereas rare words, such as names and places, are represented\ncharacter by character.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:33:02 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Johansen", "Alexander Rosenberg", ""], ["Hansen", "Jonas Meinertz", ""], ["Obeid", "Elias Khazen", ""], ["S\u00f8nderby", "Casper Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1610.06601", "submitter": "Alok Pal", "authors": "Alok Ranjan Pal, Anupam Munshi and Diganta Saha", "title": "An Approach to Speed-up the Word Sense Disambiguation Procedure through\n  Sense Filtering", "comments": "13 pages in International Journal of Instrumentation and Control\n  Systems (IJICS) Vol.3, No.4, October 2013", "journal-ref": null, "doi": "10.5121/ijics.2013.3403", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are going to focus on speed up of the Word Sense\nDisambiguation procedure by filtering the relevant senses of an ambiguous word\nthrough Part-of-Speech Tagging. First, this proposed approach performs the\nPart-of-Speech Tagging operation before the disambiguation procedure using\nBigram approximation. As a result, the exact Part-of-Speech of the ambiguous\nword at a particular text instance is derived. In the next stage, only those\ndictionary definitions (glosses) are retrieved from an online dictionary, which\nare associated with that particular Part-of-Speech to disambiguate the exact\nsense of the ambiguous word. In the training phase, we have used Brown Corpus\nfor Part-of-Speech Tagging and WordNet as an online dictionary. The proposed\napproach reduces the execution time upto half (approximately) of the normal\nexecution time for a text, containing around 200 sentences. Not only that, we\nhave found several instances, where the correct sense of an ambiguous word is\nfound for using the Part-of-Speech Tagging before the Disambiguation procedure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 11:29:10 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Pal", "Alok Ranjan", ""], ["Munshi", "Anupam", ""], ["Saha", "Diganta", ""]]}, {"id": "1610.06602", "submitter": "Roman Novak", "authors": "Roman Novak, Michael Auli, David Grangier", "title": "Iterative Refinement for Machine Translation", "comments": "Presented as a poster at BayLearn 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing machine translation decoding algorithms generate translations in a\nstrictly monotonic fashion and never revisit previous decisions. As a result,\nearlier mistakes cannot be corrected at a later stage. In this paper, we\npresent a translation scheme that starts from an initial guess and then makes\niterative improvements that may revisit previous decisions. We parameterize our\nmodel as a convolutional neural network that predicts discrete substitutions to\nan existing translation based on an attention mechanism over both the source\nsentence as well as the current translation output. By making less than one\nmodification per sentence, we improve the output of a phrase-based translation\nsystem by up to 0.4 BLEU on WMT15 German-English translation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 20:54:07 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 16:23:30 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 23:47:55 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Novak", "Roman", ""], ["Auli", "Michael", ""], ["Grangier", "David", ""]]}, {"id": "1610.06620", "submitter": "Omid Bakhshandeh", "authors": "Omid Bakhshandeh, Trung Bui, Zhe Lin, Walter Chang", "title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering open-ended questions is an essential capability for any intelligent\nagent. One of the most interesting recent open-ended question answering\nchallenges is Visual Question Answering (VQA) which attempts to evaluate a\nsystem's visual understanding through its answers to natural language questions\nabout images. There exist many approaches to VQA, the majority of which do not\nexhibit deeper semantic understanding of the candidate answers they produce. We\nstudy the importance of generating plausible answers to a given question by\nintroducing the novel task of `Answer Proposal': for a given open-ended\nquestion, a system should generate a ranked list of candidate answers informed\nby the semantics of the question. We experiment with various models including a\nneural generative model as well as a semantic graph matching one. We provide\nboth intrinsic and extrinsic evaluations for the task of Answer Proposal,\nshowing that our best model learns to propose plausible answers with a high\nrecall and performs competitively with some other solutions to VQA.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 22:01:36 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 00:12:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Bakhshandeh", "Omid", ""], ["Bui", "Trung", ""], ["Lin", "Zhe", ""], ["Chang", "Walter", ""]]}, {"id": "1610.06700", "submitter": "Hao Tang", "authors": "Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu", "title": "End-to-End Training Approaches for Discriminative Segmental Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on discriminative segmental models has shown that they can\nachieve competitive speech recognition performance, using features based on\ndeep neural frame classifiers. However, segmental models can be more\nchallenging to train than standard frame-based approaches. While some segmental\nmodels have been successfully trained end to end, there is a lack of\nunderstanding of their training under different settings and with different\nlosses.\n  We investigate a model class based on recent successful approaches,\nconsisting of a linear model that combines segmental features based on an LSTM\nframe classifier. Similarly to hybrid HMM-neural network models, segmental\nmodels of this class can be trained in two stages (frame classifier training\nfollowed by linear segmental model weight training), end to end (joint training\nof both frame classifier and linear weights), or with end-to-end fine-tuning\nafter two-stage training.\n  We study segmental models trained end to end with hinge loss, log loss,\nlatent hinge loss, and marginal log loss. We consider several losses for the\ncase where training alignments are available as well as where they are not.\n  We find that in general, marginal log loss provides the most consistent\nstrong performance without requiring ground-truth alignments. We also find that\ntraining with dropout is very important in obtaining good performance with\nend-to-end training. Finally, the best results are typically obtained by a\ncombination of two-stage training and fine-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 08:45:35 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Weiran", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1610.06856", "submitter": "Ethan Rudd", "authors": "Khudran Alzhrani, Ethan M. Rudd, Terrance E. Boult, and C. Edward Chow", "title": "Automated Big Text Security Classification", "comments": "Pre-print of Best Paper Award IEEE Intelligence and Security\n  Informatics (ISI) 2016 Manuscript", "journal-ref": "2016 IEEE International Conference on Intelligence and Security\n  Informatics (ISI)", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, traditional cybersecurity safeguards have proven ineffective\nagainst insider threats. Famous cases of sensitive information leaks caused by\ninsiders, including the WikiLeaks release of diplomatic cables and the Edward\nSnowden incident, have greatly harmed the U.S. government's relationship with\nother governments and with its own citizens. Data Leak Prevention (DLP) is a\nsolution for detecting and preventing information leaks from within an\norganization's network. However, state-of-art DLP detection models are only\nable to detect very limited types of sensitive information, and research in the\nfield has been hindered due to the lack of available sensitive texts. Many\nresearchers have focused on document-based detection with artificially labeled\n\"confidential documents\" for which security labels are assigned to the entire\ndocument, when in reality only a portion of the document is sensitive. This\ntype of whole-document based security labeling increases the chances of\npreventing authorized users from accessing non-sensitive information within\nsensitive documents. In this paper, we introduce Automated Classification\nEnabled by Security Similarity (ACESS), a new and innovative detection model\nthat penetrates the complexity of big text security classification/detection.\nTo analyze the ACESS system, we constructed a novel dataset, containing\nformerly classified paragraphs from diplomatic cables made public by the\nWikiLeaks organization. To our knowledge this paper is the first to analyze a\ndataset that contains actual formerly sensitive information annotated at\nparagraph granularity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 16:53:09 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Alzhrani", "Khudran", ""], ["Rudd", "Ethan M.", ""], ["Boult", "Terrance E.", ""], ["Chow", "C. Edward", ""]]}, {"id": "1610.07091", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Pranav Goel, Pushpak Bhattacharyya, Mark Carman", "title": "Automatic Identification of Sarcasm Target: An Introductory Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past work in computational sarcasm deals primarily with sarcasm detection. In\nthis paper, we introduce a novel, related problem: sarcasm target\nidentification i.e., extracting the target of ridicule in a sarcastic\nsentence). We present an introductory approach for sarcasm target\nidentification. Our approach employs two types of extractors: one based on\nrules, and another consisting of a statistical classifier. To compare our\napproach, we use two baselines: a na\\\"ive baseline and another baseline based\non work in sentiment target identification. We perform our experiments on book\nsnippets and tweets, and show that our hybrid approach performs better than the\ntwo baselines and also, in comparison with using the two extractors\nindividually. Our introductory approach establishes the viability of sarcasm\ntarget identification, and will serve as a baseline for future work.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 19:50:01 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 05:01:11 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Joshi", "Aditya", ""], ["Goel", "Pranav", ""], ["Bhattacharyya", "Pushpak", ""], ["Carman", "Mark", ""]]}, {"id": "1610.07149", "submitter": "Yiping Song", "authors": "Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, Ming Zhang", "title": "Two are Better than One: An Ensemble of Retrieval- and Generation-Based\n  Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain human-computer conversation has attracted much attention in the\nfield of NLP. Contrary to rule- or template-based domain-specific dialog\nsystems, open-domain conversation usually requires data-driven approaches,\nwhich can be roughly divided into two categories: retrieval-based and\ngeneration-based systems. Retrieval systems search a user-issued utterance\n(called a query) in a large database, and return a reply that best matches the\nquery. Generative approaches, typically based on recurrent neural networks\n(RNNs), can synthesize new replies, but they suffer from the problem of\ngenerating short, meaningless utterances. In this paper, we propose a novel\nensemble of retrieval-based and generation-based dialog systems in the open\ndomain. In our approach, the retrieved candidate, in addition to the original\nquery, is fed to an RNN-based reply generator, so that the neural model is\naware of more information. The generated reply is then fed back as a new\ncandidate for post-reranking. Experimental results show that such ensemble\noutperforms each single part of it by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 11:22:40 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Song", "Yiping", ""], ["Yan", "Rui", ""], ["Li", "Xiang", ""], ["Zhao", "Dongyan", ""], ["Zhang", "Ming", ""]]}, {"id": "1610.07272", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang and Chengqing Zong", "title": "Bridging Neural Machine Translation and Bilingual Dictionaries", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) has become the new state-of-the-art in\nseveral language pairs. However, it remains a challenging problem how to\nintegrate NMT with a bilingual dictionary which mainly contains words rarely or\nnever seen in the bilingual training data. In this paper, we propose two\nmethods to bridge NMT and the bilingual dictionaries. The core idea behind is\nto design novel models that transform the bilingual dictionaries into adequate\nsentence pairs, so that NMT can distil latent bilingual mappings from the ample\nand repetitive phenomena. One method leverages a mixed word/character model and\nthe other attempts at synthesizing parallel sentences guaranteeing massive\noccurrence of the translation lexicon. Extensive experiments demonstrate that\nthe proposed methods can remarkably improve the translation quality, and most\nof the rare words in the test sentences can obtain correct translations if they\nare covered by the dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 03:39:22 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1610.07363", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Maria Liakata, Rob Procter", "title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breaking news leads to situations of fast-paced reporting in social media,\nproducing all kinds of updates related to news stories, albeit with the caveat\nthat some of those early updates tend to be rumours, i.e., information with an\nunverified status at the time of posting. Flagging information that is\nunverified can be helpful to avoid the spread of information that may turn out\nto be false. Detection of rumours can also feed a rumour tracking system that\nultimately determines their veracity. In this paper we introduce a novel\napproach to rumour detection that learns from the sequential dynamics of\nreporting during breaking news in social media to detect rumours in new\nstories. Using Twitter datasets collected during five breaking news stories, we\nexperiment with Conditional Random Fields as a sequential classifier that\nleverages context learnt during an event for rumour detection, which we compare\nwith the state-of-the-art rumour detection system as well as other baselines.\nIn contrast to existing work, our classifier does not need to observe tweets\nquerying a piece of information to deem it a rumour, but instead we detect\nrumours from the tweet alone by exploiting context learnt during the event. Our\nclassifier achieves competitive performance, beating the state-of-the-art\nclassifier that relies on querying tweets with improved precision and recall,\nas well as outperforming our best baseline with nearly 40% improvement in terms\nof F1 score. The scale and diversity of our experiments reinforces the\ngeneralisability of our classifier.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 11:25:24 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1610.07365", "submitter": "Thierry Poibeau", "authors": "Thierry Poibeau (LaTTICe), Shravan Vasishth", "title": "Introduction: Cognitive Issues in Natural Language Processing", "comments": null, "journal-ref": "Traitement Automatique des Langues, ATALA, 2014, Traitement\n  Automatique des Langues et Sciences Cognitives, 55 (3), pp.7-19", "doi": "10.1201/b21583-2", "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This special issue is dedicated to get a better picture of the relationships\nbetween computational linguistics and cognitive science. It specifically raises\ntwo questions: \"what is the potential contribution of computational language\nmodeling to cognitive science?\" and conversely: \"what is the influence of\ncognitive science in contemporary computational linguistics?\"\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 11:30:22 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Poibeau", "Thierry", "", "LaTTICe"], ["Vasishth", "Shravan", ""]]}, {"id": "1610.07418", "submitter": "Raj Nath Patel", "authors": "Raj Nath Patel, Prakash B. Pimpale, Sasikumar M", "title": "Statistical Machine Translation for Indian Languages: Mission Hindi", "comments": "5 pages, Published at NLP Tools Contest: Statistical Machine\n  Translation in Indian Languages, ICON-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Statistical Machine Translation\nin Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of\nthe contest was to explore the effectiveness of Statistical Machine Translation\n(SMT) for Indian language to Indian language and English-Hindi machine\ntranslation. In this paper, we have proposed that suffix separation and word\nsplitting for SMT from agglutinative languages to Hindi significantly improves\nover the baseline (BL). We have also shown that the factored model with\nreordering outperforms the phrase-based SMT for English-Hindi (\\enhi). We\nreport our work on all five pairs of languages, namely Bengali-Hindi (\\bnhi),\nMarathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and \\enhi for\nHealth, Tourism, and General domains.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:06:31 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Patel", "Raj Nath", ""], ["Pimpale", "Prakash B.", ""], ["M", "Sasikumar", ""]]}, {"id": "1610.07420", "submitter": "Raj Nath Patel", "authors": "Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale, Sasikumar M", "title": "Reordering rules for English-Hindi SMT", "comments": "8 pages, Published at the Second Workshop on Hybrid Approaches to\n  Translation, ACL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reordering is a preprocessing stage for Statistical Machine Translation (SMT)\nsystem where the words of the source sentence are reordered as per the syntax\nof the target language. We are proposing a rich set of rules for better\nreordering. The idea is to facilitate the training process by better alignments\nand parallel phrase extraction for a phrase-based SMT system. Reordering also\nhelps the decoding process and hence improving the machine translation quality.\nWe have observed significant improvements in the translation quality by using\nour approach over the baseline SMT. We have used BLEU, NIST, multi-reference\nword error rate, multi-reference position independent error rate for judging\nthe improvements. We have exploited open source SMT toolkit MOSES to develop\nthe system.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:08:59 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Patel", "Raj Nath", ""], ["Gupta", "Rohit", ""], ["Pimpale", "Prakash B.", ""], ["M", "Sasikumar", ""]]}, {"id": "1610.07432", "submitter": "Douwe Kiela", "authors": "Douwe Kiela and Luana Bulat and Anita L. Vero and Stephen Clark", "title": "Virtual Embodiment: A Scalable Long-Term Strategy for Artificial\n  Intelligence Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meaning has been called the \"holy grail\" of a variety of scientific\ndisciplines, ranging from linguistics to philosophy, psychology and the\nneurosciences. The field of Artifical Intelligence (AI) is very much a part of\nthat list: the development of sophisticated natural language semantics is a\nsine qua non for achieving a level of intelligence comparable to humans.\nEmbodiment theories in cognitive science hold that human semantic\nrepresentation depends on sensori-motor experience; the abundant evidence that\nhuman meaning representation is grounded in the perception of physical reality\nleads to the conclusion that meaning must depend on a fusion of multiple\n(perceptual) modalities. Despite this, AI research in general, and its\nsubdisciplines such as computational linguistics and computer vision in\nparticular, have focused primarily on tasks that involve a single modality.\nHere, we propose virtual embodiment as an alternative, long-term strategy for\nAI research that is multi-modal in nature and that allows for the kind of\nscalability required to develop the field coherently and incrementally, in an\nethically responsible fashion.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:37:27 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Kiela", "Douwe", ""], ["Bulat", "Luana", ""], ["Vero", "Anita L.", ""], ["Clark", "Stephen", ""]]}, {"id": "1610.07569", "submitter": "Jiaqi Mu Jiaqi Mu", "authors": "Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "Geometry of Polysemy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representations of words have heralded a transformational approach to\nclassical problems in NLP; the most popular example is word2vec. However, a\nsingle vector does not suffice to model the polysemous nature of many\n(frequent) words, i.e., words with multiple meanings. In this paper, we propose\na three-fold approach for unsupervised polysemy modeling: (a) context\nrepresentations, (b) sense induction and disambiguation and (c) lexeme (as a\nword and sense pair) representations. A key feature of our work is the finding\nthat a sentence containing a target word is well represented by a low rank\nsubspace, instead of a point in a vector space. We then show that the subspaces\nassociated with a particular sense of the target word tend to intersect over a\nline (one-dimensional subspace), which we use to disambiguate senses using a\nclustering algorithm that harnesses the Grassmannian geometry of the\nrepresentations. The disambiguation algorithm, which we call $K$-Grassmeans,\nleads to a procedure to label the different senses of the target word in the\ncorpus -- yielding lexeme vector representations, all in an unsupervised manner\nstarting from a large (Wikipedia) corpus in English. Apart from several\nprototypical target (word,sense) examples and a host of empirical studies to\nintuit and justify the various geometric representations, we validate our\nalgorithms on standard sense induction and disambiguation datasets and present\nnew state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:35:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1610.07647", "submitter": "Mark Neumann", "authors": "Mark Neumann, Pontus Stenetorp, Sebastian Riedel", "title": "Learning to Reason With Adaptive Computation", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop inference is necessary for machine learning systems to successfully\nsolve tasks such as Recognising Textual Entailment and Machine Reading. In this\nwork, we demonstrate the effectiveness of adaptive computation for learning the\nnumber of inference steps required for examples of different complexity and\nthat learning the correct number of inference steps is difficult. We introduce\nthe first model involving Adaptive Computation Time which provides a small\nperformance benefit on top of a similar model without an adaptive component as\nwell as enabling considerable insight into the reasoning process of the model.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:48:04 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 19:35:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Neumann", "Mark", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1610.07651", "submitter": "Chunlei Zhang", "authors": "Chunlei Zhang, Fahimeh Bahmaninezhad, Shivesh Ranjan, Chengzhu Yu,\n  Navid Shokouhi, John H.L. Hansen", "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document briefly describes the systems submitted by the Center for\nRobust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to\nthe 2016 National Institute of Standards and Technology (NIST) Speaker\nRecognition Evaluation (SRE). We developed several UBM and DNN i-Vector based\nspeaker recognition systems with different data sets and feature\nrepresentations. Given that the emphasis of the NIST SRE 2016 is on language\nmismatch between training and enrollment/test data, so-called domain mismatch,\nin our system development we focused on: (1) using unlabeled in-domain data for\ncentralizing data to alleviate the domain mismatch problem, (2) finding the\nbest data set for training LDA/PLDA, (3) using newly proposed dimension\nreduction technique incorporating unlabeled in-domain data before PLDA\ntraining, (4) unsupervised speaker clustering of unlabeled data and using them\nalone or with previous SREs for PLDA training, (5) score calibration using only\nunlabeled data and combination of unlabeled and development (Dev) data as\nseparate experiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 21:05:05 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Zhang", "Chunlei", ""], ["Bahmaninezhad", "Fahimeh", ""], ["Ranjan", "Shivesh", ""], ["Yu", "Chengzhu", ""], ["Shokouhi", "Navid", ""], ["Hansen", "John H. L.", ""]]}, {"id": "1610.07708", "submitter": "Amit Sheth", "authors": "Amit Sheth, Sujan Perera, Sanjaya Wijeratne", "title": "Knowledge will Propel Machine Understanding of Content: Extrapolating\n  from Current Examples", "comments": "There is a new version of this paper with new authors uploaded as\n  arXiv:1707.05308, so this is an invalid entry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning has been a big success story during the AI resurgence. One\nparticular stand out success relates to unsupervised learning from a massive\namount of data, albeit much of it relates to one modality/type of data at a\ntime. In spite of early assertions of the unreasonable effectiveness of data,\nthere is increasing recognition of utilizing knowledge whenever it is available\nor can be created purposefully. In this paper, we focus on discussing the\nindispensable role of knowledge for deeper understanding of complex text and\nmultimodal data in situations where (i) large amounts of training data\n(labeled/unlabeled) are not available or labor intensive to create, (ii) the\nobjects (particularly text) to be recognized are complex (i.e., beyond simple\nentity-person/location/organization names), such as implicit entities and\nhighly subjective content, and (iii) applications need to use complementary or\nrelated data in multiple modalities/media. What brings us to the cusp of rapid\nprogress is our ability to (a) create knowledge, varying from comprehensive or\ncross domain to domain or application specific, and (b) carefully exploit the\nknowledge to further empower or extend the applications of ML/NLP techniques.\nUsing the early results in several diverse situations - both in data types and\napplications - we seek to foretell unprecedented progress in our ability for\ndeeper understanding and exploitation of multimodal data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 02:13:53 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 18:37:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sheth", "Amit", ""], ["Perera", "Sujan", ""], ["Wijeratne", "Sanjaya", ""]]}, {"id": "1610.07710", "submitter": "Amit Sheth", "authors": "Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran", "title": "EmojiNet: Building a Machine Readable Sense Inventory for Emoji", "comments": "15 pages, 4 figures, 3 tables, Accepted to publish at the 8th\n  International Conference on Social Informatics (SocInfo 2016) as a full\n  research track paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emoji are a contemporary and extremely popular way to enhance electronic\ncommunication. Without rigid semantics attached to them, emoji symbols take on\ndifferent meanings based on the context of a message. Thus, like the word sense\ndisambiguation task in natural language processing, machines also need to\ndisambiguate the meaning or sense of an emoji. In a first step toward achieving\nthis goal, this paper presents EmojiNet, the first machine readable sense\ninventory for emoji. EmojiNet is a resource enabling systems to link emoji with\ntheir context-specific meaning. It is automatically constructed by integrating\nmultiple emoji resources with BabelNet, which is the most comprehensive\nmultilingual sense inventory available to date. The paper discusses its\nconstruction, evaluates the automatic resource creation process, and presents a\nuse case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is\navailable online for use at http://emojinet.knoesis.org.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 02:36:52 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wijeratne", "Sanjaya", ""], ["Balasuriya", "Lakshika", ""], ["Sheth", "Amit", ""], ["Doran", "Derek", ""]]}, {"id": "1610.07796", "submitter": "Steffen Eger", "authors": "Carsten Schnober and Steffen Eger and Erik-L\\^an Do Dinh and Iryna\n  Gurevych", "title": "Still not there? Comparing Traditional Sequence-to-Sequence Models to\n  Encoder-Decoder Neural Networks on Monotone String Translation Tasks", "comments": "Accepted for publication at COLING 2016. See also:\n  https://www.ukp.tu-darmstadt.de/publications/details/?no_cache=1&tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450\n  Version 2: corrected spelling of third author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of encoder-decoder neural models and compare them\nwith well-known established methods. The latter represent different classes of\ntraditional approaches that are applied to the monotone sequence-to-sequence\ntasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion,\nand lemmatization. Such tasks are of practical relevance for various\nhigher-level research fields including digital humanities, automatic text\ncorrection, and speech recognition. We investigate how well generic\ndeep-learning approaches adapt to these tasks, and how they perform in\ncomparison with established and more specialized methods, including our own\nadaptation of pruned CRFs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 09:14:05 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 13:05:39 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Schnober", "Carsten", ""], ["Eger", "Steffen", ""], ["Dinh", "Erik-L\u00e2n Do", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1610.07809", "submitter": "Florian Boudin", "authors": "Florian Boudin, Hugo Mougard, Damien Cram", "title": "How Document Pre-processing affects Keyphrase Extraction Performance", "comments": "Accepted at the COLING 2016 Workshop on Noisy User-generated Text\n  (WNUT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The SemEval-2010 benchmark dataset has brought renewed attention to the task\nof automatic keyphrase extraction. This dataset is made up of scientific\narticles that were automatically converted from PDF format to plain text and\nthus require careful preprocessing so that irrevelant spans of text do not\nnegatively affect keyphrase extraction performance. In previous work, a wide\nrange of document preprocessing techniques were described but their impact on\nthe overall performance of keyphrase extraction models is still unexplored.\nHere, we re-assess the performance of several keyphrase extraction models and\nmeasure their robustness against increasingly sophisticated levels of document\npreprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 09:59:13 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Boudin", "Florian", ""], ["Mougard", "Hugo", ""], ["Cram", "Damien", ""]]}, {"id": "1610.07844", "submitter": "Marcel Bollmann", "authors": "Marcel Bollmann and Anders S{\\o}gaard", "title": "Improving historical spelling normalization with bi-directional LSTMs\n  and multi-task learning", "comments": "Accepted to COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural-language processing of historical documents is complicated by the\nabundance of variant spellings and lack of annotated data. A common approach is\nto normalize the spelling of historical words to modern forms. We explore the\nsuitability of a deep neural network architecture for this task, particularly a\ndeep bi-LSTM network applied on a character level. Our model compares well to\npreviously established normalization algorithms when evaluated on a diverse set\nof texts from Early New High German. We show that multi-task learning with\nadditional normalization data can improve our model's performance further.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 12:30:26 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Bollmann", "Marcel", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1610.07918", "submitter": "Yossi Adi", "authors": "Yossi Adi, Joseph Keshet, Emily Cibelli, Matthew Goldrick", "title": "Sequence Segmentation Using Joint RNN and Structured Prediction Models", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a simple and effective algorithm for sequence\nsegmentation applied to speech processing tasks. We propose a neural\narchitecture that is composed of two modules trained jointly: a recurrent\nneural network (RNN) module and a structured prediction model. The RNN outputs\nare considered as feature functions to the structured model. The overall model\nis trained with a structured loss function which can be designed to the given\nsegmentation task. We demonstrate the effectiveness of our method by applying\nit to two simple tasks commonly used in phonetic studies: word segmentation and\nvoice onset time segmentation. Results sug- gest the proposed model is superior\nto previous methods, ob- taining state-of-the-art results on the tested\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:21:25 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Adi", "Yossi", ""], ["Keshet", "Joseph", ""], ["Cibelli", "Emily", ""], ["Goldrick", "Matthew", ""]]}, {"id": "1610.08000", "submitter": "Raj Nath Patel", "authors": "Raj Nath Patel, Prakash B. Pimpale", "title": "Statistical Machine Translation for Indian Languages: Mission Hindi 2", "comments": "4 pages, Published in the Proceedings of NLP Tools Contest:\n  Statistical Machine Translation in Indian Languages", "journal-ref": "In the Proceedings of the 12th International Conference on Natural\n  Language Processing (ICON 2015)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to NLP Tools Contest on Statistical Machine Translation in\nIndian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the\ncontest was to collectively explore the effectiveness of Statistical Machine\nTranslation (SMT) while translating within Indian languages and between English\nand Indian languages. In this paper, we report our work on all five language\npairs, namely Bengali-Hindi (\\bnhi), Marathi-Hindi (\\mrhi), Tamil-Hindi\n(\\tahi), Telugu-Hindi (\\tehi), and English-Hindi (\\enhi) for Health, Tourism,\nand General domains. We have used suffix separation, compound splitting and\npreordering prior to SMT training and testing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 18:20:08 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Patel", "Raj Nath", ""], ["Pimpale", "Prakash B.", ""]]}, {"id": "1610.08078", "submitter": "Tanay Kumar Saha", "authors": "Tanay Kumar Saha, Shafiq Joty, Naeemul Hassan and Mohammad Al Hasan", "title": "Dis-S2V: Discourse Informed Sen2Vec", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representation of sentences is important for many text processing\ntasks that involve clustering, classifying, or ranking sentences. Recently,\ndistributed representation of sentences learned by neural models from unlabeled\ndata has been shown to outperform the traditional bag-of-words representation.\nHowever, most of these learning methods consider only the content of a sentence\nand disregard the relations among sentences in a discourse by and large.\n  In this paper, we propose a series of novel models for learning latent\nrepresentations of sentences (Sen2Vec) that consider the content of a sentence\nas well as inter-sentence relations. We first represent the inter-sentence\nrelations with a language network and then use the network to induce contextual\ninformation into the content-based Sen2Vec models. Two different approaches are\nintroduced to exploit the information in the network. Our first approach\nretrofits (already trained) Sen2Vec vectors with respect to the network in two\ndifferent ways: (1) using the adjacency relations of a node, and (2) using a\nstochastic sampling method which is more flexible in sampling neighbors of a\nnode. The second approach uses a regularizer to encode the information in the\nnetwork into the existing Sen2Vec model. Experimental results show that our\nproposed models outperform existing methods in three fundamental information\nsystem tasks demonstrating the effectiveness of our approach. The models\nleverage the computational power of multi-core CPUs to achieve fine-grained\ncomputational efficiency. We make our code publicly available upon acceptance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:19:35 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Saha", "Tanay Kumar", ""], ["Joty", "Shafiq", ""], ["Hassan", "Naeemul", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1610.08095", "submitter": "Mengting Wan", "authors": "Mengting Wan, Julian McAuley", "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion\n  Question Answering Systems", "comments": "10 pages, accepted by ICDM'2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product review websites provide an incredible lens into the wide variety of\nopinions and experiences of different people, and play a critical role in\nhelping users discover products that match their personal needs and\npreferences. To help address questions that can't easily be answered by reading\nothers' reviews, some review websites also allow users to pose questions to the\ncommunity via a question-answering (QA) system. As one would expect, just as\nopinions diverge among different reviewers, answers to such questions may also\nbe subjective, opinionated, and divergent. This means that answering such\nquestions automatically is quite different from traditional QA tasks, where it\nis assumed that a single `correct' answer is available. While recent work\nintroduced the idea of question-answering using product reviews, it did not\naccount for two aspects that we consider in this paper: (1) Questions have\nmultiple, often divergent, answers, and this full spectrum of answers should\nsomehow be used to train the system; and (2) What makes a `good' answer depends\non the asker and the answerer, and these factors should be incorporated in\norder for the system to be more personalized. Here we build a new QA dataset\nwith 800 thousand questions---and over 3.1 million answers---and show that\nexplicitly accounting for personalization and ambiguity leads both to\nquantitatively better answers, but also a more nuanced view of the range of\nsupporting, but subjective, opinions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 21:08:15 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Wan", "Mengting", ""], ["McAuley", "Julian", ""]]}, {"id": "1610.08229", "submitter": "Amit Mandelbaum", "authors": "Amit Mandelbaum and Adi Shalev", "title": "Word Embeddings and Their Use In Sentence Classification Tasks", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper have two parts. In the first part we discuss word embeddings. We\ndiscuss the need for them, some of the methods to create them, and some of\ntheir interesting properties. We also compare them to image embeddings and see\nhow word embedding and image embedding can be combined to perform different\ntasks. In the second part we implement a convolutional neural network trained\non top of pre-trained word vectors. The network is used for several\nsentence-level classification tasks, and achieves state-of-art (or comparable)\nresults, demonstrating the great power of pre-trainted word embeddings over\nrandom ones.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 08:48:10 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Mandelbaum", "Amit", ""], ["Shalev", "Adi", ""]]}, {"id": "1610.08375", "submitter": "Dimitra Gkatzia", "authors": "Dimitra Gkatzia", "title": "Content Selection in Data-to-Text Systems: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-to-text systems are powerful in generating reports from data\nautomatically and thus they simplify the presentation of complex data. Rather\nthan presenting data using visualisation techniques, data-to-text systems use\nnatural (human) language, which is the most common way for human-human\ncommunication. In addition, data-to-text systems can adapt their output content\nto users' preferences, background or interests and therefore they can be\npleasant for users to interact with. Content selection is an important part of\nevery data-to-text system, because it is the module that determines which from\nthe available information should be conveyed to the user. This survey initially\nintroduces the field of data-to-text generation, describes the general\ndata-to-text system architecture and then it reviews the state-of-the-art\ncontent selection methods. Finally, it provides recommendations for choosing an\napproach and discusses opportunities for future research.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 15:20:47 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Gkatzia", "Dimitra", ""]]}, {"id": "1610.08431", "submitter": "Zewei Chu", "authors": "Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester", "title": "Broad Context Language Modeling as Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Progress in text understanding has been driven by large datasets that test\nparticular capabilities, like recent datasets for reading comprehension\n(Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al.,\n2016), a word prediction task requiring broader context than the immediate\nsentence. We view LAMBADA as a reading comprehension problem and apply\ncomprehension models based on neural networks. Though these models are\nconstrained to choose a word from the context, they improve the state of the\nart on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural\nnetwork readers perform well in cases that involve selecting a name from the\ncontext based on dialogue or discourse cues but struggle when coreference\nresolution or external knowledge is needed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 17:25:38 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 18:54:44 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 21:33:30 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Chu", "Zewei", ""], ["Wang", "Hai", ""], ["Gimpel", "Kevin", ""], ["McAllester", "David", ""]]}, {"id": "1610.08462", "submitter": "Qian Chen", "authors": "Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang", "title": "Distraction-Based Neural Networks for Document Summarization", "comments": "Published in IJCAI-2016: the 25th International Joint Conference on\n  Artificial Intelligence", "journal-ref": "IJCAI, 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representation learned with neural networks has recently shown to\nbe effective in modeling natural languages at fine granularities such as words,\nphrases, and even sentences. Whether and how such an approach can be extended\nto help model larger spans of text, e.g., documents, is intriguing, and further\ninvestigation would still be desirable. This paper aims to enhance neural\nnetwork models for such a purpose. A typical problem of document-level modeling\nis automatic summarization, which aims to model documents in order to generate\nsummaries. In this paper, we propose neural models to train computers not just\nto pay attention to specific regions and content of input documents with\nattention models, but also distract them to traverse between different content\nof a document so as to better grasp the overall meaning for summarization.\nWithout engineering any features, we train the models on two large datasets.\nThe models achieve the state-of-the-art performance, and they significantly\nbenefit from the distraction modeling, particularly when input documents are\nlong.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:57:00 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Chen", "Qian", ""], ["Zhu", "Xiaodan", ""], ["Ling", "Zhenhua", ""], ["Wei", "Si", ""], ["Jiang", "Hui", ""]]}, {"id": "1610.08557", "submitter": "Ramakanth Kavuluru", "authors": "A.K.M. Sabbir, Antonio Jimeno Yepes, and Ramakanth Kavuluru", "title": "Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept\n  Embeddings", "comments": "8 pages, accepted to appear in proceedings of IEEE BIBE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical word sense disambiguation (WSD) is an important intermediate task\nin many natural language processing applications such as named entity\nrecognition, syntactic parsing, and relation extraction. In this paper, we\nemploy knowledge-based approaches that also exploit recent advances in neural\nword/concept embeddings to improve over the state-of-the-art in biomedical WSD\nusing the MSH WSD dataset as the test set. Our methods involve weak supervision\n- we do not use any hand-labeled examples for WSD to build our prediction\nmodels; however, we employ an existing well known named entity recognition and\nconcept mapping program, MetaMap, to obtain our concept vectors. Over the MSH\nWSD dataset, our linear time (in terms of numbers of senses and words in the\ntest instance) method achieves an accuracy of 92.24% which is an absolute 3%\nimprovement over the best known results obtained via unsupervised or\nknowledge-based means. A more expensive approach that we developed relies on a\nnearest neighbor framework and achieves an accuracy of 94.34%. Employing dense\nvector representations learned from unlabeled free text has been shown to\nbenefit many language processing tasks recently and our efforts show that\nbiomedical WSD is no exception to this trend. For a complex and rapidly\nevolving domain such as biomedicine, building labeled datasets for larger sets\nof ambiguous terms may be impractical. Here, we show that weak supervision that\nleverages recent advances in representation learning can rival supervised\napproaches in biomedical WSD. However, external knowledge bases (here sense\ninventories) play a key role in the improvements achieved.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 21:49:15 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 00:57:16 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 20:38:45 GMT"}, {"version": "v4", "created": "Wed, 28 Jun 2017 02:13:13 GMT"}, {"version": "v5", "created": "Sat, 30 Sep 2017 01:01:50 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Sabbir", "A. K. M.", ""], ["Yepes", "Antonio Jimeno", ""], ["Kavuluru", "Ramakanth", ""]]}, {"id": "1610.08597", "submitter": "Sanjaya Wijeratne", "authors": "Sanjaya Wijeratne, Lakshika Balasuriya, Derek Doran, Amit Sheth", "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification", "comments": "7 pages, 1 figure, 2 tables, Published at IJCAI Workshop on Semantic\n  Machine Learning (SML 2016)", "journal-ref": "IJCAI Workshop on Semantic Machine Learning (SML 2016). pp. 18-24.\n  CEUR-WS, New York City, NY (07 2016)", "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gang affiliates have joined the masses who use social media to share thoughts\nand actions publicly. Interestingly, they use this public medium to express\nrecent illegal actions, to intimidate others, and to share outrageous images\nand statements. Agencies able to unearth these profiles may thus be able to\nanticipate, stop, or hasten the investigation of gang-related crimes. This\npaper investigates the use of word embeddings to help identify gang members on\nTwitter. Building on our previous work, we generate word embeddings that\ntranslate what Twitter users post in their profile descriptions, tweets,\nprofile images, and linked YouTube content to a real vector format amenable for\nmachine learning classification. Our experimental results show that pre-trained\nword embeddings can boost the accuracy of supervised learning algorithms\ntrained over gang members social media posts.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 03:21:49 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Wijeratne", "Sanjaya", ""], ["Balasuriya", "Lakshika", ""], ["Doran", "Derek", ""], ["Sheth", "Amit", ""]]}, {"id": "1610.08613", "submitter": "{\\L}ukasz Kaiser", "authors": "{\\L}ukasz Kaiser and Samy Bengio", "title": "Can Active Memory Replace Attention?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several mechanisms to focus attention of a neural network on selected parts\nof its input or memory have been used successfully in deep learning models in\nrecent years. Attention has improved image classification, image captioning,\nspeech recognition, generative models, and learning algorithmic tasks, but it\nhad probably the largest impact on neural machine translation.\n  Recently, similar improvements have been obtained using alternative\nmechanisms that do not focus on a single part of a memory but operate on all of\nit in parallel, in a uniform way. Such mechanism, which we call active memory,\nimproved over attention in algorithmic tasks, image processing, and in\ngenerative modelling.\n  So far, however, active memory has not improved over attention for most\nnatural language processing tasks, in particular for machine translation. We\nanalyze this shortcoming in this paper and propose an extended model of active\nmemory that matches existing attention models on neural machine translation and\ngeneralizes better to longer sentences. We investigate this model and explain\nwhy previous active memory models did not succeed. Finally, we discuss when\nactive memory brings most benefits and where attention can be a better choice.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 04:28:29 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 04:04:33 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Kaiser", "\u0141ukasz", ""], ["Bengio", "Samy", ""]]}, {"id": "1610.08694", "submitter": "Vered Shwartz", "authors": "Vered Shwartz and Ido Dagan", "title": "CogALex-V Shared Task: LexNET - Integrated Path-based and Distributional\n  Method for the Identification of Semantic Relations", "comments": "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the\n  Lexicon (CogALex-V), in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a submission to the CogALex 2016 shared task on the corpus-based\nidentification of semantic relations, using LexNET (Shwartz and Dagan, 2016),\nan integrated path-based and distributional method for semantic relation\nclassification. The reported results in the shared task bring this submission\nto the third place on subtask 1 (word relatedness), and the first place on\nsubtask 2 (semantic relation classification), demonstrating the utility of\nintegrating the complementary path-based and distributional information sources\nin recognizing concrete semantic relations. Combined with a common similarity\nmeasure, LexNET performs fairly good on the word relatedness task (subtask 1).\nThe relatively low performance of LexNET and all other systems on subtask 2,\nhowever, confirms the difficulty of the semantic relation classification task,\nand stresses the need to develop additional methods for this task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 10:49:00 GMT"}, {"version": "v2", "created": "Sun, 30 Oct 2016 09:42:41 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 10:10:01 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Shwartz", "Vered", ""], ["Dagan", "Ido", ""]]}, {"id": "1610.08763", "submitter": "Xiang Ren", "authors": "Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek\n  F. Abdelzaher, Jiawei Han", "title": "CoType: Joint Extraction of Typed Entities and Relations with Knowledge\n  Bases", "comments": "WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting entities and relations for types of interest from text is\nimportant for understanding massive text corpora. Traditionally, systems of\nentity relation extraction have relied on human-annotated corpora for training\nand adopted an incremental pipeline. Such systems require additional human\nexpertise to be ported to a new domain, and are vulnerable to errors cascading\ndown the pipeline. In this paper, we investigate joint extraction of typed\nentities and relations with labeled data heuristically obtained from knowledge\nbases (i.e., distant supervision). As our algorithm for type labeling via\ndistant supervision is context-agnostic, noisy training data poses unique\nchallenges for the task. We propose a novel domain-independent framework,\ncalled CoType, that runs a data-driven text segmentation algorithm to extract\nentity mentions, and jointly embeds entity mentions, relation mentions, text\nfeatures and type labels into two low-dimensional spaces (for entity and\nrelation mentions respectively), where, in each space, objects whose types are\nclose will also have similar representations. CoType, then using these learned\nembeddings, estimates the types of test (unlinkable) mentions. We formulate a\njoint optimization problem to learn embeddings from text corpora and knowledge\nbases, adopting a novel partial-label loss function for noisy labeled data and\nintroducing an object \"translation\" function to capture the cross-constraints\nof entities and relations on each other. Experiments on three public datasets\ndemonstrate the effectiveness of CoType across different domains (e.g., news,\nbiomedical), with an average of 25% improvement in F1 score compared to the\nnext best method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 13:20:25 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 19:28:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ren", "Xiang", ""], ["Wu", "Zeqiu", ""], ["He", "Wenqi", ""], ["Qu", "Meng", ""], ["Voss", "Clare R.", ""], ["Ji", "Heng", ""], ["Abdelzaher", "Tarek F.", ""], ["Han", "Jiawei", ""]]}, {"id": "1610.08815", "submitter": "Soujanya Poria", "authors": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Prateek Vij", "title": "A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural\n  Networks", "comments": "Published in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sarcasm detection is a key task for many natural language processing tasks.\nIn sentiment analysis, for example, sarcasm can flip the polarity of an\n\"apparently positive\" sentence and, hence, negatively affect polarity detection\nperformance. To date, most approaches to sarcasm detection have treated the\ntask primarily as a text categorization problem. Sarcasm, however, can be\nexpressed in very subtle ways and requires a deeper understanding of natural\nlanguage that standard text categorization techniques cannot grasp. In this\nwork, we develop models based on a pre-trained convolutional neural network for\nextracting sentiment, emotion and personality features for sarcasm detection.\nSuch features, along with the network's baseline features, allow the proposed\nmodels to outperform the state of the art on benchmark datasets. We also\naddress the often ignored generalizability issue of classifying data that have\nnot been seen by the models at learning phase.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 14:50:43 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 02:38:59 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Poria", "Soujanya", ""], ["Cambria", "Erik", ""], ["Hazarika", "Devamanyu", ""], ["Vij", "Prateek", ""]]}, {"id": "1610.08914", "submitter": "Lucas Dixon", "authors": "Ellery Wulczyn, Nithum Thain, Lucas Dixon", "title": "Ex Machina: Personal Attacks Seen at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The damage personal attacks cause to online discourse motivates many\nplatforms to try to curb the phenomenon. However, understanding the prevalence\nand impact of personal attacks in online platforms at scale remains\nsurprisingly difficult. The contribution of this paper is to develop and\nillustrate a method that combines crowdsourcing and machine learning to analyze\npersonal attacks at scale. We show an evaluation method for a classifier in\nterms of the aggregated number of crowd-workers it can approximate. We apply\nour methodology to English Wikipedia, generating a corpus of over 100k high\nquality human-labeled comments and 63M machine-labeled ones from a classifier\nthat is as good as the aggregate of 3 crowd-workers, as measured by the area\nunder the ROC curve and Spearman correlation. Using this corpus of\nmachine-labeled scores, our methodology allows us to explore some of the open\nquestions about the nature of online personal attacks. This reveals that the\nmajority of personal attacks on Wikipedia are not the result of a few malicious\nusers, nor primarily the consequence of allowing anonymous contributions from\nunregistered users.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 18:18:18 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 18:38:16 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wulczyn", "Ellery", ""], ["Thain", "Nithum", ""], ["Dixon", "Lucas", ""]]}, {"id": "1610.09091", "submitter": "Shijia E", "authors": "Shijia E, Yang Xiang, Mohan Zhang", "title": "Representation Learning Models for Entity Search", "comments": "This paper has been withdrawn by the author because the proposed\n  model need to be re-evaluate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of learning distributed representations for entity\nsearch queries, named entities, and their short descriptions. With our\nrepresentation learning models, the entity search query, named entity and\ndescription can be represented as low-dimensional vectors. Our goal is to\ndevelop a simple but effective model that can make the distributed\nrepresentations of query related entities similar to the query in the vector\nspace. Hence, we propose three kinds of learning strategies, and the difference\nbetween them mainly lies in how to deal with the relationship between an entity\nand its description. We analyze the strengths and weaknesses of each learning\nstrategy and validate our methods on public datasets which contain four kinds\nof named entities, i.e., movies, TV shows, restaurants and celebrities. The\nexperimental results indicate that our proposed methods can adapt to different\ntypes of entity search queries, and outperform the current state-of-the-art\nmethods based on keyword matching and vanilla word2vec models. Besides, the\nproposed methods can be trained fast and be easily extended to other similar\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 06:33:33 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 02:19:01 GMT"}, {"version": "v3", "created": "Sun, 15 Jan 2017 13:57:23 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["E", "Shijia", ""], ["Xiang", "Yang", ""], ["Zhang", "Mohan", ""]]}, {"id": "1610.09158", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "Towards a continuous modeling of natural language domains", "comments": "5 pages, 3 figures, published in Uphill Battles in Language\n  Processing workshop, EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans continuously adapt their style and language to a variety of domains.\nHowever, a reliable definition of `domain' has eluded researchers thus far.\nAdditionally, the notion of discrete domains stands in contrast to the\nmultiplicity of heterogeneous domains that humans navigate, many of which\noverlap. In order to better understand the change and variation of human\nlanguage, we draw on research in domain adaptation and extend the notion of\ndiscrete domains to the continuous spectrum. We propose representation\nlearning-based models that can adapt to continuous domains and detail how these\ncan be used to investigate variation in language. To this end, we propose to\nuse dialogue modeling as a test bed due to its proximity to language modeling\nand its social component.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 10:26:40 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1610.09225", "submitter": "Venkata Sasank Pagolu", "authors": "Venkata Sasank Pagolu, Kamal Nayan Reddy Challa, Ganapati Panda,\n  Babita Majhi", "title": "Sentiment Analysis of Twitter Data for Predicting Stock Market Movements", "comments": "6 pages 4 figures Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting stock market movements is a well-known problem of interest.\nNow-a-days social media is perfectly representing the public sentiment and\nopinion about current events. Especially, twitter has attracted a lot of\nattention from researchers for studying the public sentiments. Stock market\nprediction on the basis of public sentiments expressed on twitter has been an\nintriguing field of research. Previous studies have concluded that the\naggregate public mood collected from twitter may well be correlated with Dow\nJones Industrial Average Index (DJIA). The thesis of this work is to observe\nhow well the changes in stock prices of a company, the rises and falls, are\ncorrelated with the public opinions being expressed in tweets about that\ncompany. Understanding author's opinion from a piece of text is the objective\nof sentiment analysis. The present paper have employed two different textual\nrepresentations, Word2vec and N-gram, for analyzing the public sentiments in\ntweets. In this paper, we have applied sentiment analysis and supervised\nmachine learning principles to the tweets extracted from twitter and analyze\nthe correlation between stock market movements of a company and sentiments in\ntweets. In an elaborate way, positive news and tweets in social media about a\ncompany would definitely encourage people to invest in the stocks of that\ncompany and as a result the stock price of that company would increase. At the\nend of the paper, it is shown that a strong correlation exists between the rise\nand falls in stock prices with the public sentiments in tweets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 14:07:43 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Pagolu", "Venkata Sasank", ""], ["Challa", "Kamal Nayan Reddy", ""], ["Panda", "Ganapati", ""], ["Majhi", "Babita", ""]]}, {"id": "1610.09226", "submitter": "Pavlina Fragkou", "authors": "Pavlina Fragkou", "title": "Text Segmentation using Named Entity Recognition and Co-reference\n  Resolution in English and Greek Texts", "comments": "32 pages. arXiv admin note: text overlap with arXiv:1308.0661,\n  arXiv:1204.2847 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the benefit of performing named entity recognition\n(NER) and co-reference resolution to an English and a Greek corpus used for\ntext segmentation. The aim here is to examine whether the combination of text\nsegmentation and information extraction can be beneficial for the\nidentification of the various topics that appear in a document. NER was\nperformed manually in the English corpus and was compared with the output\nproduced by publicly available annotation tools while, an already existing tool\nwas used for the Greek corpus. Produced annotations from both corpora were\nmanually corrected and enriched to cover four types of named entities.\nCo-reference resolution i.e., substitution of every reference of the same\ninstance with the same named entity identifier was subsequently performed. The\nevaluation, using five text segmentation algorithms for the English corpus and\nfour for the Greek corpus leads to the conclusion that, the benefit highly\ndepends on the segment's topic, the number of named entity instances appearing\nin it, as well as the segment's length.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 14:09:33 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Fragkou", "Pavlina", ""]]}, {"id": "1610.09333", "submitter": "Antoine Tixier", "authors": "Antoine J.-P. Tixier, Michalis Vazirgiannis, Matthew R. Hallowell", "title": "Word Embeddings for the Construction Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce word vectors for the construction domain. Our vectors were\nobtained by running word2vec on an 11M-word corpus that we created from scratch\nby leveraging freely-accessible online sources of construction-related text. We\nfirst explore the embedding space and show that our vectors capture meaningful\nconstruction-specific concepts. We then evaluate the performance of our vectors\nagainst that of ones trained on a 100B-word corpus (Google News) within the\nframework of an injury report classification task. Without any parameter\ntuning, our embeddings give competitive results, and outperform the Google News\nvectors in many cases. Using a keyword-based compression of the reports also\nleads to a significant speed-up with only a limited loss in performance. We\nrelease our corpus and the data set we created for the classification task as\npublicly available, in the hope that they will be used by future studies for\nbenchmarking and building on our work.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 18:15:08 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Tixier", "Antoine J. -P.", ""], ["Vazirgiannis", "Michalis", ""], ["Hallowell", "Matthew R.", ""]]}, {"id": "1610.09516", "submitter": "Sanjaya Wijeratne", "authors": "Lakshika Balasuriya, Sanjaya Wijeratne, Derek Doran, Amit Sheth", "title": "Finding Street Gang Members on Twitter", "comments": "8 pages, 9 figures, 2 tables, Published as a full paper at 2016\n  IEEE/ACM International Conference on Advances in Social Networks Analysis and\n  Mining (ASONAM 2016)", "journal-ref": "The 2016 IEEE/ACM Int. Conf. on Advances in Social Networks\n  Analysis and Mining. vol. 8, pp. 685-692. San Francisco, CA, USA (2016)", "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most street gang members use Twitter to intimidate others, to present\noutrageous images and statements to the world, and to share recent illegal\nactivities. Their tweets may thus be useful to law enforcement agencies to\ndiscover clues about recent crimes or to anticipate ones that may occur.\nFinding these posts, however, requires a method to discover gang member Twitter\nprofiles. This is a challenging task since gang members represent a very small\npopulation of the 320 million Twitter users. This paper studies the problem of\nautomatically finding gang members on Twitter. It outlines a process to curate\none of the largest sets of verifiable gang member profiles that have ever been\nstudied. A review of these profiles establishes differences in the language,\nimages, YouTube links, and emojis gang members use compared to the rest of the\nTwitter population. Features from this review are used to train a series of\nsupervised classifiers. Our classifier achieves a promising F1 score with a low\nfalse positive rate.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:30:57 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Balasuriya", "Lakshika", ""], ["Wijeratne", "Sanjaya", ""], ["Doran", "Derek", ""], ["Sheth", "Amit", ""]]}, {"id": "1610.09565", "submitter": "Mihaela Rosca", "authors": "Mihaela Rosca, Thomas Breuel", "title": "Sequence-to-sequence neural network models for transliteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transliteration is a key component of machine translation systems and\nsoftware internationalization. This paper demonstrates that neural\nsequence-to-sequence models obtain state of the art or close to state of the\nart results on existing datasets. In an effort to make machine transliteration\naccessible, we open source a new Arabic to English transliteration dataset and\nour trained models.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 19:21:19 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Rosca", "Mihaela", ""], ["Breuel", "Thomas", ""]]}, {"id": "1610.09704", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Ozlem Uzuner, Peter Szolovits", "title": "Feature-Augmented Neural Networks for Patient Note De-identification", "comments": "Accepted as a conference paper at COLING ClinicalNLP 2016. The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient notes contain a wealth of information of potentially great interest\nto medical investigators. However, to protect patients' privacy, Protected\nHealth Information (PHI) must be removed from the patient notes before they can\nbe legally released, a process known as patient note de-identification. The\nmain objective for a de-identification system is to have the highest possible\nrecall. Recently, the first neural-network-based de-identification system has\nbeen proposed, yielding state-of-the-art results. Unlike other systems, it does\nnot rely on human-engineered features, which allows it to be quickly deployed,\nbut does not leverage knowledge from human experts or from electronic health\nrecords (EHRs). In this work, we explore a method to incorporate\nhuman-engineered features as well as features derived from EHRs to a\nneural-network-based de-identification system. Our results show that the\naddition of features, especially the EHR-derived features, further improves the\nstate-of-the-art in patient note de-identification, including for some of the\nmost sensitive PHI types such as patient names. Since in a real-life setting\npatient notes typically come with EHRs, we recommend developers of\nde-identification systems to leverage the information EHRs contain.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 20:09:46 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Uzuner", "Ozlem", ""], ["Szolovits", "Peter", ""]]}, {"id": "1610.09722", "submitter": "Jason Naradowsky", "authors": "Jason Naradowsky and Sebastian Riedel", "title": "Represent, Aggregate, and Constrain: A Novel Architecture for Machine\n  Reading from Noisy Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to extract event information from text, a machine reading model must\nlearn to accurately read and interpret the ways in which that information is\nexpressed. But it must also, as the human reader must, aggregate numerous\nindividual value hypotheses into a single coherent global analysis, applying\nglobal constraints which reflect prior knowledge of the domain.\n  In this work we focus on the task of extracting plane crash event information\nfrom clusters of related news articles whose labels are derived via distant\nsupervision. Unlike previous machine reading work, we assume that while most\ntarget values will occur frequently in most clusters, they may also be missing\nor incorrect.\n  We introduce a novel neural architecture to explicitly model the noisy nature\nof the data and to deal with these aforementioned learning issues. Our models\nare trained end-to-end and achieve an improvement of more than 12.1 F$_1$ over\nprevious work, despite using far less linguistic annotation. We apply factor\ngraph constraints to promote more coherent event analyses, with belief\npropagation inference formulated within the transitions of a recurrent neural\nnetwork. We show this technique additionally improves maximum F$_1$ by up to\n2.8 points, resulting in a relative improvement of $50\\%$ over the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 22:33:47 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Naradowsky", "Jason", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1610.09756", "submitter": "Vinayak Athavale", "authors": "Vinayak Athavale, Shreenivas Bharadwaj, Monik Pamecha, Ameya Prabhu\n  and Manish Shrivastava", "title": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled\n  Data Scarcity", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": "https://aclweb.org/anthology/W/W16/W16-6320.pdf", "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an end to end Neural Model for Named Entity\nRecognition NER) which is based on Bi-Directional RNN-LSTM. Almost all NER\nsystems for Hindi use Language Specific features and handcrafted rules with\ngazetteers. Our model is language independent and uses no domain specific\nfeatures or any handcrafted rules. Our models rely on semantic information in\nthe form of word vectors which are learnt by an unsupervised learning algorithm\non an unannotated corpus. Our model attained state of the art performance in\nboth English and Hindi without the use of any morphological analysis or without\nusing gazetteers of any sort.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 01:31:52 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 17:15:14 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Athavale", "Vinayak", ""], ["Bharadwaj", "Shreenivas", ""], ["Pamecha", "Monik", ""], ["Prabhu", "Ameya", ""], ["Shrivastava", "Manish", ""]]}, {"id": "1610.09799", "submitter": "Raj Nath Patel", "authors": "Prakash B. Pimpale, Raj Nath Patel", "title": "Experiments with POS Tagging Code-mixed Indian Social Media Text", "comments": "3 Pages, Published in the Proceedings of the Tool Contest on POS\n  Tagging for Code-mixed Indian Social Media (Facebook, Twitter, and Whatsapp)\n  Text", "journal-ref": "In the Proceedings of the 12th International Conference on Natural\n  Language Processing (ICON 2015)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 06:13:31 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Pimpale", "Prakash B.", ""], ["Patel", "Raj Nath", ""]]}, {"id": "1610.09889", "submitter": "Zhe Wang", "authors": "Zhe Wang, Wei He, Hua Wu, Haiyang Wu, Wei Li, Haifeng Wang, Enhong\n  Chen", "title": "Chinese Poetry Generation with Planning based Neural Network", "comments": "Accepted paper at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese poetry generation is a very challenging task in natural language\nprocessing. In this paper, we propose a novel two-stage poetry generating\nmethod which first plans the sub-topics of the poem according to the user's\nwriting intent, and then generates each line of the poem sequentially, using a\nmodified recurrent neural network encoder-decoder framework. The proposed\nplanning-based method can ensure that the generated poem is coherent and\nsemantically consistent with the user's intent. A comprehensive evaluation with\nhuman judgments demonstrates that our proposed approach outperforms the\nstate-of-the-art poetry generating methods and the poem quality is somehow\ncomparable to human poets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:16:39 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:56:19 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Wang", "Zhe", ""], ["He", "Wei", ""], ["Wu", "Hua", ""], ["Wu", "Haiyang", ""], ["Li", "Wei", ""], ["Wang", "Haifeng", ""], ["Chen", "Enhong", ""]]}, {"id": "1610.09893", "submitter": "Tao Qin Dr.", "authors": "Xiang Li and Tao Qin and Jian Yang and Tie-Yan Liu", "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have achieved state-of-the-art performances\nin many natural language processing tasks, such as language modeling and\nmachine translation. However, when the vocabulary is large, the RNN model will\nbecome very big (e.g., possibly beyond the memory capacity of a GPU device) and\nits training will become very inefficient. In this work, we propose a novel\ntechnique to tackle this challenge. The key idea is to use 2-Component (2C)\nshared embedding for word representations. We allocate every word in the\nvocabulary into a table, each row of which is associated with a vector, and\neach column associated with another vector. Depending on its position in the\ntable, a word is jointly represented by two components: a row vector and a\ncolumn vector. Since the words in the same row share the row vector and the\nwords in the same column share the column vector, we only need $2 \\sqrt{|V|}$\nvectors to represent a vocabulary of $|V|$ unique words, which are far less\nthan the $|V|$ vectors required by existing approaches. Based on the\n2-Component shared embedding, we design a new RNN algorithm and evaluate it\nusing the language modeling task on several benchmark datasets. The results\nshow that our algorithm significantly reduces the model size and speeds up the\ntraining process, without sacrifice of accuracy (it achieves similar, if not\nbetter, perplexity as compared to state-of-the-art language models).\nRemarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves\ncomparable perplexity to previous language models, whilst reducing the model\nsize by a factor of 40-100, and speeding up the training process by a factor of\n2. We name our proposed algorithm \\emph{LightRNN} to reflect its very small\nmodel size and very high training speed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:24:13 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Xiang", ""], ["Qin", "Tao", ""], ["Yang", "Jian", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1610.09914", "submitter": "Lizhen Qu", "authors": "Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, Timothy Baldwin", "title": "Named Entity Recognition for Novel Types by Transfer Learning", "comments": null, "journal-ref": "EMNLP 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In named entity recognition, we often don't have a large in-domain training\ncorpus or a knowledge base with adequate coverage to train a model directly. In\nthis paper, we propose a method where, given training data in a related domain\nwith similar (but not identical) named entity (NE) types and a small amount of\nin-domain training data, we use transfer learning to learn a domain-specific NE\nmodel. That is, the novelty in the task setup is that we assume not just domain\nmismatch, but also label mismatch.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 13:36:35 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Qu", "Lizhen", ""], ["Ferraro", "Gabriela", ""], ["Zhou", "Liyuan", ""], ["Hou", "Weiwei", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1610.09935", "submitter": "Dominic Seyler", "authors": "Dominic Seyler, Mohamed Yahya, Klaus Berberich", "title": "Knowledge Questions from Knowledge Graphs", "comments": null, "journal-ref": "ICTIR (2017) 11-18", "doi": "10.1145/3121050.3121073", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the novel problem of automatically generating quiz-style knowledge\nquestions from a knowledge graph such as DBpedia. Questions of this kind have\nample applications, for instance, to educate users about or to evaluate their\nknowledge in a specific domain. To solve the problem, we propose an end-to-end\napproach. The approach first selects a named entity from the knowledge graph as\nan answer. It then generates a structured triple-pattern query, which yields\nthe answer as its sole result. If a multiple-choice question is desired, the\napproach selects alternative answer options. Finally, our approach uses a\ntemplate-based method to verbalize the structured query and yield a natural\nlanguage question. A key challenge is estimating how difficult the generated\nquestion is to human users. To do this, we make use of historical data from the\nJeopardy! quiz show and a semantically annotated Web-scale document collection,\nengineer suitable features, and train a logistic regression classifier to\npredict question difficulty. Experiments demonstrate the viability of our\noverall approach.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 14:27:07 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 05:39:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Seyler", "Dominic", ""], ["Yahya", "Mohamed", ""], ["Berberich", "Klaus", ""]]}, {"id": "1610.09964", "submitter": "Vinu E V", "authors": "Vinu E.V and P Sreenivasa Kumar", "title": "Ontology Verbalization using Semantic-Refinement", "comments": "Currently this paper is under review in the Semantic Web Journal.\n  arXiv admin note: text overlap with arXiv:1607.07027", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a rule-based technique to generate redundancy-free NL descriptions\nof OWL entities.The existing approaches which address the problem of\nverbalizing OWL ontologies generate NL text segments which are close to their\ncounterpart OWL statements.Some of these approaches also perform grouping and\naggregating of these NL text segments to generate a more fluent and\ncomprehensive form of the content.Restricting our attention to description of\nindividuals and concepts, we find that the approach currently followed in the\navailable tools is that of determining the set of all logical conditions that\nare satisfied by the given individual/concept name and translate these\nconditions verbatim into corresponding NL descriptions.Human-understandability\nof such descriptions is affected by the presence of repetitions and\nredundancies, as they have high fidelity to their OWL representation.In the\nliterature, no efforts had been taken to remove redundancies and repetitions at\nthe logical-level before generating the NL descriptions of entities and we find\nthis to be the main reason for lack of readability of the generated\ntext.Herein, we propose a technique called semantic-refinement(SR) to generate\nmeaningful and easily-understandable descriptions of individuals and concepts\nof a given OWLontology.We identify the combinations of OWL/DL constructs that\nlead to repetitive/redundant descriptions and propose a series of refinement\nrules to rewrite the conditions that are satisfied by an individual/concept in\na meaning-preserving manner.The reduced set of conditions are then employed for\ngenerating NL descriptions.Our experiments show that, SR leads to significantly\nimproved descriptions of ontology entities.We also test the effectiveness and\nusefulness of the the generated descriptions for the purpose of validating the\nontologies and find that the proposed technique is indeed helpful in the\ncontext.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 15:20:14 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["E.", "Vinu", "V"], ["Kumar", "P Sreenivasa", ""]]}, {"id": "1610.09975", "submitter": "Hasim Sak", "authors": "Hagen Soltau, Hank Liao, Hasim Sak", "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large\n  Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results that show it is possible to build a competitive, greatly\nsimplified, large vocabulary continuous speech recognition system with whole\nwords as acoustic units. We model the output vocabulary of about 100,000 words\ndirectly using deep bi-directional LSTM RNNs with CTC loss. The model is\ntrained on 125,000 hours of semi-supervised acoustic training data, which\nenables us to alleviate the data sparsity problem for word models. We show that\nthe CTC word models work very well as an end-to-end all-neural speech\nrecognition model without the use of traditional context-dependent sub-word\nphone units that require a pronunciation lexicon, and without any language\nmodel removing the need to decode. We demonstrate that the CTC word models\nperform better than a strong, more complex, state-of-the-art baseline with\nsub-word units.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 15:36:42 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Soltau", "Hagen", ""], ["Liao", "Hank", ""], ["Sak", "Hasim", ""]]}, {"id": "1610.09982", "submitter": "Sanjay Chakraborty", "authors": "Lopamudra Dey, Sanjay Chakraborty, Anuraag Biswas, Beepa Bose, Sweta\n  Tiwari", "title": "Sentiment Analysis of Review Datasets Using Naive Bayes and K-NN\n  Classifier", "comments": "Volume-8, Issue-4, pp.54-62, 2016", "journal-ref": null, "doi": "10.5815/ijieeb.2016.04.07", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Web 2.0 has led to an increase in the amount of sentimental\ncontent available in the Web. Such content is often found in social media web\nsites in the form of movie or product reviews, user comments, testimonials,\nmessages in discussion forums etc. Timely discovery of the sentimental or\nopinionated web content has a number of advantages, the most important of all\nbeing monetization. Understanding of the sentiments of human masses towards\ndifferent entities and products enables better services for contextual\nadvertisements, recommendation systems and analysis of market trends. The focus\nof our project is sentiment focussed web crawling framework to facilitate the\nquick discovery of sentimental contents of movie reviews and hotel reviews and\nanalysis of the same. We use statistical methods to capture elements of\nsubjective style and the sentence polarity. The paper elaborately discusses two\nsupervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Naive\nBayes and compares their overall accuracy, precisions as well as recall values.\nIt was seen that in case of movie reviews Naive Bayes gave far better results\nthan K-NN but for hotel reviews these algorithms gave lesser, almost same\naccuracies.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 15:45:41 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Dey", "Lopamudra", ""], ["Chakraborty", "Sanjay", ""], ["Biswas", "Anuraag", ""], ["Bose", "Beepa", ""], ["Tiwari", "Sweta", ""]]}, {"id": "1610.09995", "submitter": "Wladimir Sidorenko", "authors": "Uladzimir Sidarenka and Manfred Stede", "title": "Generating Sentiment Lexicons for German Twitter", "comments": "This paper is the first in a planned series of articles on an\n  automatic generation of sentiment lexicons for non-English Twitter. It will\n  be presented as a poster at the PEOPLES workshop\n  (https://peoples2016.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite a substantial progress made in developing new sentiment lexicon\ngeneration (SLG) methods for English, the task of transferring these approaches\nto other languages and domains in a sound way still remains open. In this\npaper, we contribute to the solution of this problem by systematically\ncomparing semi-automatic translations of common English polarity lists with the\nresults of the original automatic SLG algorithms, which were applied directly\nto German data. We evaluate these lexicons on a corpus of 7,992 manually\nannotated tweets. In addition to that, we also collate the results of\ndictionary- and corpus-based SLG methods in order to find out which of these\nparadigms is better suited for the inherently noisy domain of social media. Our\nexperiments show that semi-automatic translations notably outperform automatic\nsystems (reaching a macro-averaged F1-score of 0.589), and that\ndictionary-based techniques produce much better polarity lists as compared to\ncorpus-based approaches (whose best F1-scores run up to 0.479 and 0.419\nrespectively) even for the non-standard Twitter genre.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 16:12:16 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sidarenka", "Uladzimir", ""], ["Stede", "Manfred", ""]]}, {"id": "1610.09996", "submitter": "Yang Yu", "authors": "Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, Bowen Zhou", "title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "comments": "Submitted to AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading\ncomprehension (RC) model that is able to extract and rank a set of answer\ncandidates from a given document to answer questions. DCR is able to predict\nanswers of variable lengths, whereas previous neural RC models primarily\nfocused on predicting single tokens or entities. DCR encodes a document and an\ninput question with recurrent neural networks, and then applies a word-by-word\nattention mechanism to acquire question-aware representations for the document,\nfollowed by the generation of chunk representations and a ranking module to\npropose the top-ranked chunk as the answer. Experimental results show that DCR\nachieves state-of-the-art exact match and F1 scores on the SQuAD dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 16:14:08 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 17:55:32 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Yu", "Yang", ""], ["Zhang", "Wei", ""], ["Hasan", "Kazi", ""], ["Yu", "Mo", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1610.10099", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord,\n  Alex Graves, Koray Kavukcuoglu", "title": "Neural Machine Translation in Linear Time", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 19:56:39 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 18:09:51 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Espeholt", "Lasse", ""], ["Simonyan", "Karen", ""], ["Oord", "Aaron van den", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}]