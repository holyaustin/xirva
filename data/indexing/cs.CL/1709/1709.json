[{"id": "1709.00023", "submitter": "Mo Yu", "authors": "Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei\n  Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, Jing Jiang", "title": "R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering", "comments": "8 pages, accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years researchers have achieved considerable success applying\nneural network methods to question answering (QA). These approaches have\nachieved state of the art results in simplified closed-domain settings such as\nthe SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected\npassage, from which the answer to a given question may be extracted. More\nrecently, researchers have begun to tackle open-domain QA, in which the model\nis given a question and access to a large corpus (e.g., wikipedia) instead of a\npre-selected passage (Chen et al., 2017a). This setting is more complex as it\nrequires large-scale search for relevant passages by an information retrieval\ncomponent, combined with a reading comprehension model that \"reads\" the\npassages to generate an answer to the question. Performance in this setting\nlags considerably behind closed-domain performance. In this paper, we present a\nnovel open-domain QA system called Reinforced Ranker-Reader $(R^3)$, based on\ntwo algorithmic innovations. First, we propose a new pipeline for open-domain\nQA with a Ranker component, which learns to rank retrieved passages in terms of\nlikelihood of generating the ground-truth answer to a given question. Second,\nwe propose a novel method that jointly trains the Ranker along with an\nanswer-generation Reader model, based on reinforcement learning. We report\nextensive experimental results showing that our method significantly improves\non the state of the art for multiple open-domain QA datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 18:08:35 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 16:38:30 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wang", "Shuohang", ""], ["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Wang", "Zhiguo", ""], ["Klinger", "Tim", ""], ["Zhang", "Wei", ""], ["Chang", "Shiyu", ""], ["Tesauro", "Gerald", ""], ["Zhou", "Bowen", ""], ["Jiang", "Jing", ""]]}, {"id": "1709.00028", "submitter": "Falcon Dai", "authors": "Falcon Z. Dai and Zheng Cai", "title": "Glyph-aware Embedding of Chinese Characters", "comments": "Workshop on Subword and Character level models in NLP at EMNLP 2017.\n  Source code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the advantage and recent success of English character-level and\nsubword-unit models in several NLP tasks, we consider the equivalent modeling\nproblem for Chinese. Chinese script is logographic and many Chinese logograms\nare composed of common substructures that provide semantic, phonetic and\nsyntactic hints. In this work, we propose to explicitly incorporate the visual\nappearance of a character's glyph in its representation, resulting in a novel\nglyph-aware embedding of Chinese characters. Being inspired by the success of\nconvolutional neural networks in computer vision, we use them to incorporate\nthe spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In\nthe context of two basic Chinese NLP tasks of language modeling and word\nsegmentation, the model learns to represent each character's task-relevant\nsemantic and syntactic information in the character-level embedding.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 18:19:08 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Dai", "Falcon Z.", ""], ["Cai", "Zheng", ""]]}, {"id": "1709.00071", "submitter": "Nick Obradovich", "authors": "Patrick Baylis, Nick Obradovich, Yury Kryvasheyeu, Haohui Chen,\n  Lorenzo Coviello, Esteban Moro, Manuel Cebrian, James H. Fowler", "title": "Weather impacts expressed sentiment", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0195750", "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct the largest ever investigation into the relationship between\nmeteorological conditions and the sentiment of human expressions. To do this,\nwe employ over three and a half billion social media posts from tens of\nmillions of individuals from both Facebook and Twitter between 2009 and 2016.\nWe find that cold temperatures, hot temperatures, precipitation, narrower daily\ntemperature ranges, humidity, and cloud cover are all associated with worsened\nexpressions of sentiment, even when excluding weather-related posts. We compare\nthe magnitude of our estimates with the effect sizes associated with notable\nhistorical events occurring within our data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 20:36:07 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Baylis", "Patrick", ""], ["Obradovich", "Nick", ""], ["Kryvasheyeu", "Yury", ""], ["Chen", "Haohui", ""], ["Coviello", "Lorenzo", ""], ["Moro", "Esteban", ""], ["Cebrian", "Manuel", ""], ["Fowler", "James H.", ""]]}, {"id": "1709.00094", "submitter": "Jiaqi Wu", "authors": "Jiaqi Wu, Marilyn Walker, Pranav Anand and Steve Whittaker", "title": "Linguistic Reflexes of Well-Being and Happiness in Echo", "comments": "11 pages, 8th Workshop on Computational Approaches to Subjectivity,\n  Sentiment and Social Media Analysis (WASSA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different theories posit different sources for feelings of well-being and\nhappiness. Appraisal theory grounds our emotional responses in our goals and\ndesires and their fulfillment, or lack of fulfillment. Self Determination\ntheory posits that the basis for well-being rests on our assessment of our\ncompetence, autonomy, and social connection. And surveys that measure happiness\nempirically note that people require their basic needs to be met for food and\nshelter, but beyond that tend to be happiest when socializing, eating or having\nsex. We analyze a corpus of private microblogs from a well-being application\ncalled ECHO, where users label each written post about daily events with a\nhappiness score between 1 and 9. Our goal is to ground the linguistic\ndescriptions of events that users experience in theories of well-being and\nhappiness, and then examine the extent to which different theoretical accounts\ncan explain the variance in the happiness scores. We show that recurrent event\ntypes, such as OBLIGATION and INCOMPETENCE, which affect people's feelings of\nwell-being are not captured in current lexical or semantic resources.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 22:03:53 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Wu", "Jiaqi", ""], ["Walker", "Marilyn", ""], ["Anand", "Pranav", ""], ["Whittaker", "Steve", ""]]}, {"id": "1709.00103", "submitter": "Victor Zhong", "authors": "Victor Zhong, Caiming Xiong, and Richard Socher", "title": "Seq2SQL: Generating Structured Queries from Natural Language using\n  Reinforcement Learning", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of the world's knowledge is stored in relational\ndatabases. However, the ability for users to retrieve facts from a database is\nlimited due to a lack of understanding of query languages such as SQL. We\npropose Seq2SQL, a deep neural network for translating natural language\nquestions to corresponding SQL queries. Our model leverages the structure of\nSQL queries to significantly reduce the output space of generated queries.\nMoreover, we use rewards from in-the-loop query execution over the database to\nlearn a policy to generate unordered parts of the query, which we show are less\nsuitable for optimization via cross entropy loss. In addition, we will publish\nWikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL\nqueries distributed across 24241 tables from Wikipedia. This dataset is\nrequired to train our model and is an order of magnitude larger than comparable\ndatasets. By applying policy-based reinforcement learning with a query\nexecution environment to WikiSQL, our model Seq2SQL outperforms attentional\nsequence to sequence models, improving execution accuracy from 35.9% to 59.4%\nand logical form accuracy from 23.4% to 48.3%.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 23:12:15 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 19:14:04 GMT"}, {"version": "v3", "created": "Wed, 18 Oct 2017 01:34:46 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 21:13:56 GMT"}, {"version": "v5", "created": "Tue, 31 Oct 2017 20:49:31 GMT"}, {"version": "v6", "created": "Tue, 7 Nov 2017 16:53:10 GMT"}, {"version": "v7", "created": "Thu, 9 Nov 2017 23:06:14 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Zhong", "Victor", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1709.00149", "submitter": "Clayton Morrison", "authors": "Enrique Noriega-Atala, Marco A. Valenzuela-Escarcega, Clayton T.\n  Morrison, Mihai Surdeanu", "title": "Learning what to read: Focused machine reading", "comments": "6 pages, 1 figure, 1 algorithm, 2 tables, accepted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts in bioinformatics have achieved tremendous progress in the\nmachine reading of biomedical literature, and the assembly of the extracted\nbiochemical interactions into large-scale models such as protein signaling\npathways. However, batch machine reading of literature at today's scale (PubMed\nalone indexes over 1 million papers per year) is unfeasible due to both cost\nand processing overhead. In this work, we introduce a focused reading approach\nto guide the machine reading of biomedical literature towards what literature\nshould be read to answer a biomedical query as efficiently as possible. We\nintroduce a family of algorithms for focused reading, including an intuitive,\nstrong baseline, and a second approach which uses a reinforcement learning (RL)\nframework that learns when to explore (widen the search) or exploit (narrow\nit). We demonstrate that the RL approach is capable of answering more queries\nthan the baseline, while being more efficient, i.e., reading fewer documents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:09:42 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Noriega-Atala", "Enrique", ""], ["Valenzuela-Escarcega", "Marco A.", ""], ["Morrison", "Clayton T.", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1709.00155", "submitter": "Lei Sha", "authors": "Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li, Baobao\n  Chang, Zhifang Sui", "title": "Order-Planning Neural Text Generation From Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating texts from structured data (e.g., a table) is important for\nvarious natural language processing tasks such as question answering and dialog\nsystems. In recent studies, researchers use neural language models and\nencoder-decoder frameworks for table-to-text generation. However, these neural\nnetwork-based approaches do not model the order of contents during text\ngeneration. When a human writes a summary based on a given table, he or she\nwould probably consider the content order before wording. In a biography, for\nexample, the nationality of a person is typically mentioned before occupation\nin a biography. In this paper, we propose an order-planning text generation\nmodel to capture the relationship between different fields and use such\nrelationship to make the generated text more fluent and smooth. We conducted\nexperiments on the WikiBio dataset and achieve significantly higher performance\nthan previous methods in terms of BLEU, ROUGE, and NIST scores.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:46:10 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Sha", "Lei", ""], ["Mou", "Lili", ""], ["Liu", "Tianyu", ""], ["Poupart", "Pascal", ""], ["Li", "Sujian", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "1709.00224", "submitter": "Guy Edward Toh Emerson", "authors": "Guy Emerson and Ann Copestake", "title": "Variational Inference for Logical Inference", "comments": "Conference on Logic and Machine Learning in Natural Language (LaML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Distributional Semantics is a framework that aims to learn, from\ntext, semantic representations which can be interpreted in terms of truth. Here\nwe make two contributions to this framework. The first is to show how a type of\nlogical inference can be performed by evaluating conditional probabilities. The\nsecond is to make these calculations tractable by means of a variational\napproximation. This approximation also enables faster convergence during\ntraining, allowing us to close the gap with state-of-the-art vector space\nmodels when evaluating on semantic similarity. We demonstrate promising\nperformance on two tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 09:55:44 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Emerson", "Guy", ""], ["Copestake", "Ann", ""]]}, {"id": "1709.00226", "submitter": "Guy Edward Toh Emerson", "authors": "Guy Emerson and Ann Copestake", "title": "Semantic Composition via Probabilistic Model Theory", "comments": "International Conference on Computational Semantics (IWCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic composition remains an open problem for vector space models of\nsemantics. In this paper, we explain how the probabilistic graphical model used\nin the framework of Functional Distributional Semantics can be interpreted as a\nprobabilistic version of model theory. Building on this, we explain how various\nsemantic phenomena can be recast in terms of conditional probabilities in the\ngraphical model. This connection between formal semantics and machine learning\nis helpful in both directions: it gives us an explicit mechanism for modelling\ncontext-dependent meanings (a challenge for formal semantics), and also gives\nus well-motivated techniques for composing distributed representations (a\nchallenge for distributional semantics). We present results on two datasets\nthat go beyond word similarity, showing how these semantically-motivated\ntechniques improve on the performance of vector models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 10:01:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Emerson", "Guy", ""], ["Copestake", "Ann", ""]]}, {"id": "1709.00345", "submitter": "Ian Stewart", "authors": "Ian Stewart, Jacob Eisenstein", "title": "Making \"fetch\" happen: The influence of social and linguistic context on\n  nonstandard word growth and decline", "comments": null, "journal-ref": "EMNLP 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In an online community, new words come and go: today's \"haha\" may be replaced\nby tomorrow's \"lol.\" Changes in online writing are usually studied as a social\nprocess, with innovations diffusing through a network of individuals in a\nspeech community. But unlike other types of innovation, language change is\nshaped and constrained by the system in which it takes part. To investigate the\nlinks between social and structural factors in language change, we undertake a\nlarge-scale analysis of nonstandard word growth in the online community Reddit.\nWe find that dissemination across many linguistic contexts is a sign of growth:\nwords that appear in more linguistic contexts grow faster and survive longer.\nWe also find that social dissemination likely plays a less important role in\nexplaining word growth and decline than previously hypothesized.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:38:21 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 18:25:48 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 19:07:18 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2018 13:41:25 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Stewart", "Ian", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1709.00354", "submitter": "Chia-Wei Ao", "authors": "Chia-Wei Ao, Hung-yi Lee", "title": "Query-by-example Spoken Term Detection using Attention-based Multi-hop\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving spoken content with spoken queries, or query-by- example spoken\nterm detection (STD), is attractive because it makes possible the matching of\nsignals directly on the acoustic level without transcribing them into text.\nHere, we propose an end-to-end query-by-example STD model based on an\nattention-based multi-hop network, whose input is a spoken query and an audio\nsegment containing several utterances; the output states whether the audio\nsegment includes the query. The model can be trained in either a supervised\nscenario using labeled data, or in an unsupervised fashion. In the supervised\nscenario, we find that the attention mechanism and multiple hops improve\nperformance, and that the attention weights indicate the time span of the\ndetected terms. In the unsupervised setting, the model mimics the behavior of\nthe existing query-by-example STD system, yielding performance comparable to\nthe existing system but with a lower search time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:56:53 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 08:33:39 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Ao", "Chia-Wei", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1709.00387", "submitter": "Suwon Shon", "authors": "Suwon Shon, Ahmed Ali and James Glass", "title": "MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre\n  Broadcast Challenge", "comments": "Submitted to the 2017 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to successfully annotate the Arabic speech con- tent found in\nopen-domain media broadcasts, it is essential to be able to process a diverse\nset of Arabic dialects. For the 2017 Multi-Genre Broadcast challenge (MGB-3)\nthere were two possible tasks: Arabic speech recognition, and Arabic Dialect\nIdentification (ADI). In this paper, we describe our efforts to create an ADI\nsystem for the MGB-3 challenge, with the goal of distinguishing amongst four\nmajor Arabic dialects, as well as Modern Standard Arabic. Our research fo-\ncused on dialect variability and domain mismatches between the training and\ntest domain. In order to achieve a robust ADI system, we explored both Siamese\nneural network models to learn similarity and dissimilarities among Arabic\ndialects, as well as i-vector post-processing to adapt domain mismatches. Both\nAcoustic and linguistic features were used for the final MGB-3 submissions,\nwith the best primary system achieving 75% accuracy on the official 10hr test\nset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:20:02 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Shon", "Suwon", ""], ["Ali", "Ahmed", ""], ["Glass", "James", ""]]}, {"id": "1709.00389", "submitter": "Jian Tang", "authors": "Jian Tang, Yue Wang, Kai Zheng, Qiaozhu Mei", "title": "End-to-end Learning for Short Text Expansion", "comments": "KDD'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Effectively making sense of short texts is a critical task for many real\nworld applications such as search engines, social media services, and\nrecommender systems. The task is particularly challenging as a short text\ncontains very sparse information, often too sparse for a machine learning\nalgorithm to pick up useful signals. A common practice for analyzing short text\nis to first expand it with external information, which is usually harvested\nfrom a large collection of longer texts. In literature, short text expansion\nhas been done with all kinds of heuristics. We propose an end-to-end solution\nthat automatically learns how to expand short text to optimize a given learning\ntask. A novel deep memory network is proposed to automatically find relevant\ninformation from a collection of longer documents and reformulate the short\ntext through a gating mechanism. Using short text classification as a\ndemonstrating task, we show that the deep memory network significantly\noutperforms classical text expansion methods with comprehensive experiments on\nreal world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 04:24:06 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tang", "Jian", ""], ["Wang", "Yue", ""], ["Zheng", "Kai", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1709.00489", "submitter": "Miguel Ballesteros", "authors": "Miguel Ballesteros and Xavier Carreras", "title": "Arc-Standard Spinal Parsing with Stack-LSTMs", "comments": "IWPT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural transition-based parser for spinal trees, a dependency\nrepresentation of constituent trees. The parser uses Stack-LSTMs that compose\nconstituent nodes with dependency-based derivations. In experiments, we show\nthat this model adapts to different styles of dependency relations, but this\nchoice has little effect for predicting constituent structure, suggesting that\nLSTMs induce useful states by themselves.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 21:38:28 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ballesteros", "Miguel", ""], ["Carreras", "Xavier", ""]]}, {"id": "1709.00541", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov and Zhenisbek Assylbekov", "title": "Patterns versus Characters in Subword-aware Neural Language Modeling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words in some natural languages can have a composite structure. Elements of\nthis structure include the root (that could also be composite), prefixes and\nsuffixes with which various nuances and relations to other words can be\nexpressed. Thus, in order to build a proper word representation one must take\ninto account its internal structure. From a corpus of texts we extract a set of\nfrequent subwords and from the latter set we select patterns, i.e. subwords\nwhich encapsulate information on character $n$-gram regularities. The selection\nis made using the pattern-based Conditional Random Field model with $l_1$\nregularization. Further, for every word we construct a new sequence over an\nalphabet of patterns. The new alphabet's symbols confine a local statistical\ncontext stronger than the characters, therefore they allow better\nrepresentations in ${\\mathbb{R}}^n$ and are better building blocks for word\nrepresentation. In the task of subword-aware language modeling, pattern-based\nmodels outperform character-based analogues by 2-20 perplexity points. Also, a\nrecurrent neural network in which a word is represented as a sum of embeddings\nof its patterns is on par with a competitive and significantly more\nsophisticated character-based convolutional architecture.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 07:00:22 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Takhanov", "Rustem", ""], ["Assylbekov", "Zhenisbek", ""]]}, {"id": "1709.00575", "submitter": "Marek Rei", "authors": "Marek Rei, Luana Bulat, Douwe Kiela, Ekaterina Shutova", "title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor\n  Detection", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of metaphor in our everyday communication makes it an important\nproblem for natural language understanding. Yet, the majority of metaphor\nprocessing systems to date rely on hand-engineered features and there is still\nno consensus in the field as to which features are optimal for this task. In\nthis paper, we present the first deep learning architecture designed to capture\nmetaphorical composition. Our results demonstrate that it outperforms the\nexisting approaches in the metaphor identification task.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 13:13:06 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Rei", "Marek", ""], ["Bulat", "Luana", ""], ["Kiela", "Douwe", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "1709.00616", "submitter": "Hassan Sajjad", "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Ahmed Abdelali, Yonatan\n  Belinkov, Stephan Vogel", "title": "Challenging Language-Dependent Segmentation for Arabic: An Application\n  to Machine Translation and Part-of-Speech Tagging", "comments": "ACL 2017 pages 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word segmentation plays a pivotal role in improving any Arabic NLP\napplication. Therefore, a lot of research has been spent in improving its\naccuracy. Off-the-shelf tools, however, are: i) complicated to use and ii)\ndomain/dialect dependent. We explore three language-independent alternatives to\nmorphological segmentation using: i) data-driven sub-word units, ii) characters\nas a unit of learning, and iii) word embeddings learned using a character CNN\n(Convolution Neural Network). On the tasks of Machine Translation and POS\ntagging, we found these methods to achieve close to, and occasionally surpass\nstate-of-the-art performance. In our analysis, we show that a neural machine\ntranslation system is sensitive to the ratio of source and target tokens, and a\nratio close to 1 or greater, gives optimal performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 18:45:48 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Sajjad", "Hassan", ""], ["Dalvi", "Fahim", ""], ["Durrani", "Nadir", ""], ["Abdelali", "Ahmed", ""], ["Belinkov", "Yonatan", ""], ["Vogel", "Stephan", ""]]}, {"id": "1709.00659", "submitter": "Sunil Sahu", "authors": "Kushal Chawla, Sunil Kumar Sahu, Ashish Anand", "title": "Investigating how well contextual features are captured by\n  bi-directional recurrent neural network models", "comments": "Camera ready version of ICON-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Learning algorithms for natural language processing (NLP) tasks traditionally\nrely on manually defined relevant contextual features. On the other hand,\nneural network models using an only distributional representation of words have\nbeen successfully applied for several NLP tasks. Such models learn features\nautomatically and avoid explicit feature engineering. Across several domains,\nneural models become a natural choice specifically when limited characteristics\nof data are known. However, this flexibility comes at the cost of\ninterpretability. In this paper, we define three different methods to\ninvestigate ability of bi-directional recurrent neural networks (RNNs) in\ncapturing contextual features. In particular, we analyze RNNs for sequence\ntagging tasks. We perform a comprehensive analysis on general as well as\nbiomedical domain datasets. Our experiments focus on important contextual words\nas features, which can easily be extended to analyze various other feature\ntypes. We also investigate positional effects of context words and show how the\ndeveloped methods can be used for error analysis.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 04:00:49 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:02:36 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chawla", "Kushal", ""], ["Sahu", "Sunil Kumar", ""], ["Anand", "Ashish", ""]]}, {"id": "1709.00661", "submitter": "Amita Misra", "authors": "Amita Misra and Marilyn Walker", "title": "Topic Independent Identification of Agreement and Disagreement in Social\n  Media Dialogue", "comments": "@inproceedings{Misra2013TopicII, title={Topic Independent\n  Identification of Agreement and Disagreement in Social Media Dialogue},\n  author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},\n  year={2013}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on the structure of dialogue has been hampered for years because\nlarge dialogue corpora have not been available. This has impacted the dialogue\nresearch community's ability to develop better theories, as well as good off\nthe shelf tools for dialogue processing. Happily, an increasing amount of\ninformation and opinion exchange occur in natural dialogue in online forums,\nwhere people share their opinions about a vast range of topics. In particular\nwe are interested in rejection in dialogue, also called disagreement and\ndenial, where the size of available dialogue corpora, for the first time,\noffers an opportunity to empirically test theoretical accounts of the\nexpression and inference of rejection in dialogue. In this paper, we test\nwhether topic-independent features motivated by theoretical predictions can be\nused to recognize rejection in online forums in a topic independent way. Our\nresults show that our theoretically motivated features achieve 66% accuracy, an\nimprovement over a unigram baseline of an absolute 6%.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 04:16:15 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Misra", "Amita", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.00662", "submitter": "Amita Misra", "authors": "Amita Misra, Pranav Anand, Jean E Fox Tree and Marilyn Walker", "title": "Using Summarization to Discover Argument Facets in Online Ideological\n  Dialog", "comments": "@inproceedings{Misra2015UsingST,title={Using Summarization to\n  Discover Argument Facets in Online Idealogical Dialog},author={Amita Misra\n  and Pranav Anand and Jean E. Fox Tree and Marilyn A.\n  Walker},booktitle={HLT-NAACL},year={2015}}", "journal-ref": null, "doi": "10.3115/v1/n15-1046", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more of the information available on the web is dialogic, and a\nsignificant portion of it takes place in online forum conversations about\ncurrent social and political topics. We aim to develop tools to summarize what\nthese conversations are about. What are the CENTRAL PROPOSITIONS associated\nwith different stances on an issue, what are the abstract objects under\ndiscussion that are central to a speaker's argument? How can we recognize that\ntwo CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize\nthat the CENTRAL PROPOSITIONS are exactly those arguments that people find most\nsalient, and use human summarization as a probe for discovering them. We\ndescribe our corpus of human summaries of opinionated dialogs, then show how we\ncan identify similar repeated arguments, and group them into FACETS across many\ndiscussions of a topic. We define a new task, ARGUMENT FACET SIMILARITY (AFS),\nand show that we can predict AFS with a .54 correlation score, versus an ngram\nsystem baseline of .39 and a semantic textual similarity system baseline of\n.45.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 04:16:25 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Misra", "Amita", ""], ["Anand", "Pranav", ""], ["Tree", "Jean E Fox", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.00678", "submitter": "Laurent Besacier", "authors": "Ngoc-Tien Le and Benjamin Lecouteux and Laurent Besacier", "title": "Disentangling ASR and MT Errors in Speech Translation", "comments": "Accepted to MT Summit 2017 (Japan)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main aim of this paper is to investigate automatic quality assessment for\nspoken language translation (SLT). More precisely, we investigate SLT errors\nthat can be due to transcription (ASR) or to translation (MT) modules. This\npaper investigates automatic detection of SLT errors using a single classifier\nbased on joint ASR and MT features. We evaluate both 2-class (good/bad) and\n3-class (good/badASR/badMT ) labeling tasks. The 3-class problem necessitates\nto disentangle ASR and MT errors in the speech translation output and we\npropose two label extraction methods for this non trivial step. This enables -\nas a by-product - qualitative analysis on the SLT errors and their origin (are\nthey due to transcription or to translation step?) on our large in-house corpus\nfor French-to-English speech translation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 07:42:17 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Le", "Ngoc-Tien", ""], ["Lecouteux", "Benjamin", ""], ["Besacier", "Laurent", ""]]}, {"id": "1709.00728", "submitter": "Wen Kokke", "authors": "Wen Kokke", "title": "Formalising Type-Logical Grammars in Agda", "comments": "In 1st Workshop on Type Theory and Lexical Semantics at ESSLLI'15,\n  Barcelona, Spain, August, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, the interest in using proof assistants to formalise and\nreason about mathematics and programming languages has grown. Type-logical\ngrammars, being closely related to type theories and systems used in functional\nprogramming, are a perfect candidate to next apply this curiosity to. The\nadvantages of using proof assistants is that they allow one to write formally\nverified proofs about one's type-logical systems, and that any theory, once\nimplemented, can immediately be computed with. The downside is that in many\ncases the formal proofs are written as an afterthought, are incomplete, or use\nobtuse syntax. This makes it that the verified proofs are often much more\ndifficult to read than the pen-and-paper proofs, and almost never directly\npublished. In this paper, we will try to remedy that by example.\n  Concretely, we use Agda to model the Lambek-Grishin calculus, a grammar logic\nwith a rich vocabulary of type-forming operations. We then present a verified\nprocedure for cut elimination in this system. Then we briefly outline a CPS\ntranslation from proofs in the Lambek-Grishin calculus to programs in Agda. And\nfinally, we will put our system to use in the analysis of a simple example\nsentence.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 14:54:29 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kokke", "Wen", ""]]}, {"id": "1709.00770", "submitter": "Muhammad Mahbubur Rahman", "authors": "Muhammad Mahbubur Rahman, Tim Finin", "title": "Understanding the Logical and Semantic Structure of Large Documents", "comments": "10 pages, 15 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current language understanding approaches focus on small documents, such as\nnewswire articles, blog posts, product reviews and discussion forum entries.\nUnderstanding and extracting information from large documents like legal\nbriefs, proposals, technical manuals and research articles is still a\nchallenging task. We describe a framework that can analyze a large document and\nhelp people to know where a particular information is in that document. We aim\nto automatically identify and classify semantic sections of documents and\nassign consistent and human-understandable labels to similar sections across\ndocuments. A key contribution of our research is modeling the logical and\nsemantic structure of an electronic document. We apply machine learning\ntechniques, including deep learning, in our prototype system. We also make\navailable a dataset of information about a collection of scholarly articles\nfrom the arXiv eprints collection that includes a wide range of metadata for\neach article, including a table of contents, section labels, section\nsummarizations and more. We hope that this dataset will be a useful resource\nfor the machine learning and NLP communities in information retrieval,\ncontent-based question answering and language modeling.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 21:38:56 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Rahman", "Muhammad Mahbubur", ""], ["Finin", "Tim", ""]]}, {"id": "1709.00813", "submitter": "Samuel Cunningham-Nelson", "authors": "Samuel Cunningham-Nelson, Mahsa Baktashmotlagh, Wageeh Boles", "title": "From Review to Rating: Exploring Dependency Measures for Text\n  Classification", "comments": "8 pages", "journal-ref": "Under Consideration by Pattern Recognition Letters (PRL) 2018", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 05:38:35 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Cunningham-Nelson", "Samuel", ""], ["Baktashmotlagh", "Mahsa", ""], ["Boles", "Wageeh", ""]]}, {"id": "1709.00831", "submitter": "Nishant Gurnani", "authors": "Nishant Gurnani", "title": "Hypothesis Testing based Intrinsic Evaluation of Word Embeddings", "comments": "Accepted to RepEval 2017: The Second Workshop on Evaluating Vector\n  Space Representations for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the cross-match test - an exact, distribution free,\nhigh-dimensional hypothesis test as an intrinsic evaluation metric for word\nembeddings. We show that cross-match is an effective means of measuring\ndistributional similarity between different vector representations and of\nevaluating the statistical significance of different vector embedding models.\nAdditionally, we find that cross-match can be used to provide a quantitative\nmeasure of linguistic similarity for selecting bridge languages for machine\ntranslation. We demonstrate that the results of the hypothesis test align with\nour expectations and note that the framework of two sample hypothesis testing\nis not limited to word embeddings and can be extended to all vector\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 06:29:36 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Gurnani", "Nishant", ""]]}, {"id": "1709.00893", "submitter": "Dehong Ma", "authors": "Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang", "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-level sentiment classification aims at identifying the sentiment\npolarity of specific target in its context. Previous approaches have realized\nthe importance of targets in sentiment classification and developed various\nmethods with the goal of precisely modeling their contexts via generating\ntarget-specific representations. However, these studies always ignore the\nseparate modeling of targets. In this paper, we argue that both targets and\ncontexts deserve special treatment and need to be learned their own\nrepresentations via interactive learning. Then, we propose the interactive\nattention networks (IAN) to interactively learn attentions in the contexts and\ntargets, and generate the representations for targets and contexts separately.\nWith this design, the IAN model can well represent a target and its collocative\ncontext, which is helpful to sentiment classification. Experimental results on\nSemEval 2014 Datasets demonstrate the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 10:34:34 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ma", "Dehong", ""], ["Li", "Sujian", ""], ["Zhang", "Xiaodong", ""], ["Wang", "Houfeng", ""]]}, {"id": "1709.00917", "submitter": "Hao Li", "authors": "Shasha Xia, Hao Li and Xueliang Zhang", "title": "Using Optimal Ratio Mask as Training Target for Supervised Speech\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised speech separation uses supervised learning algorithms to learn a\nmapping from an input noisy signal to an output target. With the fast\ndevelopment of deep learning, supervised separation has become the most\nimportant direction in speech separation area in recent years. For the\nsupervised algorithm, training target has a significant impact on the\nperformance. Ideal ratio mask is a commonly used training target, which can\nimprove the speech intelligibility and quality of the separated speech.\nHowever, it does not take into account the correlation between noise and clean\nspeech. In this paper, we use the optimal ratio mask as the training target of\nthe deep neural network (DNN) for speech separation. The experiments are\ncarried out under various noise environments and signal to noise ratio (SNR)\nconditions. The results show that the optimal ratio mask outperforms other\ntraining targets in general.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 12:25:18 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Xia", "Shasha", ""], ["Li", "Hao", ""], ["Zhang", "Xueliang", ""]]}, {"id": "1709.00947", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Lu\\'is Sarmento, Eduarda Mendes Rodrigues, Carlos\n  Soares, Eug\\'enio Oliveira", "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of\n  some Practical Aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a preliminary study for producing and distributing a\nlarge-scale database of embeddings from the Portuguese Twitter stream. We start\nby experimenting with a relatively small sample and focusing on three\nchallenges: volume of training data, vocabulary size and intrinsic evaluation\nmetrics. Using a single GPU, we were able to scale up vocabulary size from 2048\nwords embedded and 500K training examples to 32768 words over 10M training\nexamples while keeping a stable validation loss and approximately linear trend\non training time per epoch. We also observed that using less than 50\\% of the\navailable training examples for each vocabulary size might result in\noverfitting. Results on intrinsic evaluation show promising performance for a\nvocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics\nsuffer from over-sensitivity to their corresponding cosine similarity\nthresholds, indicating that a wider range of metrics need to be developed to\ntrack progress.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 13:30:23 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Saleiro", "Pedro", ""], ["Sarmento", "Lu\u00eds", ""], ["Rodrigues", "Eduarda Mendes", ""], ["Soares", "Carlos", ""], ["Oliveira", "Eug\u00e9nio", ""]]}, {"id": "1709.01042", "submitter": "Stephanie Lukin", "authors": "Reid Swanson and Stephanie Lukin and Luke Eisenberg and Thomas Chase\n  Corcoran and Marilyn A.Walker", "title": "Getting Reliable Annotations for Sarcasm in Online Dialogues", "comments": "International Conference on Language Resources and Evaluation (LREC\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The language used in online forums differs in many ways from that of\ntraditional language resources such as news. One difference is the use and\nfrequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the\naim is to develop a theory of sarcasm in dialogue, or engineer automatic\nmethods for reliably detecting sarcasm, a major challenge is simply the\ndifficulty of getting enough reliably labelled examples. In this paper we\ndescribe our work on methods for achieving highly reliable sarcasm annotations\nfrom untrained annotators on Mechanical Turk. We explore the use of a number of\ncommon statistical reliability measures, such as Kappa, Karger's, Majority\nClass, and EM. We show that more sophisticated measures do not appear to yield\nbetter results for our data than simple measures such as assuming that the\ncorrect label is the one that a majority of Turkers apply.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 16:54:35 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Swanson", "Reid", ""], ["Lukin", "Stephanie", ""], ["Eisenberg", "Luke", ""], ["Corcoran", "Thomas Chase", ""], ["Walker", "Marilyn A.", ""]]}, {"id": "1709.01058", "submitter": "Linfeng Song", "authors": "Linfeng Song, Zhiguo Wang and Wael Hamza", "title": "A Unified Query-based Generative Model for Question Generation and\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a query-based generative model for solving both tasks of question\ngeneration (QG) and question an- swering (QA). The model follows the classic\nencoder- decoder framework. The encoder takes a passage and a query as input\nthen performs query understanding by matching the query with the passage from\nmultiple per- spectives. The decoder is an attention-based Long Short Term\nMemory (LSTM) model with copy and coverage mechanisms. In the QG task, a\nquestion is generated from the system given the passage and the target answer,\nwhereas in the QA task, the answer is generated given the question and the\npassage. During the training stage, we leverage a policy-gradient reinforcement\nlearning algorithm to overcome exposure bias, a major prob- lem resulted from\nsequence learning with cross-entropy loss. For the QG task, our experiments\nshow higher per- formances than the state-of-the-art results. When used as\nadditional training data, the automatically generated questions even improve\nthe performance of a strong ex- tractive QA system. In addition, our model\nshows bet- ter performance than the state-of-the-art baselines of the\ngenerative QA task.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 17:54:49 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 14:04:39 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Song", "Linfeng", ""], ["Wang", "Zhiguo", ""], ["Hamza", "Wael", ""]]}, {"id": "1709.01121", "submitter": "Adina Williams", "authors": "Adina Williams, and Andrew Drozdov and Samuel R. Bowman", "title": "Do latent tree learning models identify meaningful structure in\n  sentences?", "comments": "15 pages, 6 figures, 4 tables. v1. was submitted to TACL, v2. was\n  accepted to TACL, name change, additional baselines (R/L branching)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on the problem of latent tree learning has made it possible to\ntrain neural networks that learn to both parse a sentence and use the resulting\nparse to interpret the sentence, all without exposure to ground-truth parse\ntrees at training time. Surprisingly, these models often perform better at\nsentence understanding tasks than models that use parse trees from conventional\nparsers. This paper aims to investigate what these latent tree learning models\nlearn. We replicate two such models in a shared codebase and find that (i) only\none of these models outperforms conventional tree-structured models on sentence\nclassification, (ii) its parsing strategies are not especially consistent\nacross random restarts, (iii) the parses it produces tend to be shallower than\nstandard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB\nor any other semantic or syntactic formalism that the authors are aware of.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 19:05:39 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 15:59:38 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Williams", "Adina", ""], ["Drozdov", "Andrew", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1709.01144", "submitter": "Pranay Dighe", "authors": "Pranay Dighe, Afsaneh Asaei, Herv\\'e Bourlard", "title": "Information Theoretic Analysis of DNN-HMM Acoustic Modeling", "comments": "Theoretical flaw, needs major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an information theoretic framework for quantitative assessment of\nacoustic modeling for hidden Markov model (HMM) based automatic speech\nrecognition (ASR). Acoustic modeling yields the probabilities of HMM sub-word\nstates for a short temporal window of speech acoustic features. We cast ASR as\na communication channel where the input sub-word probabilities convey the\ninformation about the output HMM state sequence. The quality of the acoustic\nmodel is thus quantified in terms of the information transmitted through this\nchannel. The process of inferring the most likely HMM state sequence from the\nsub-word probabilities is known as decoding. HMM based decoding assumes that an\nacoustic model yields accurate state-level probabilities and the data\ndistribution given the underlying hidden state is independent of any other\nstate in the sequence. We quantify 1) the acoustic model accuracy and 2) its\nrobustness to mismatch between data and the HMM conditional independence\nassumption in terms of some mutual information quantities. In this context,\nexploiting deep neural network (DNN) posterior probabilities leads to a simple\nand straightforward analysis framework to assess shortcomings of the acoustic\nmodel for HMM based decoding. This analysis enables us to evaluate the Gaussian\nmixture acoustic model (GMM) and the importance of many hidden layers in DNNs\nwithout any need of explicit speech recognition. In addition, it sheds light on\nthe contribution of low-dimensional models to enhance acoustic modeling for\nbetter compliance with the HMM based decoding requirements.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:03:05 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 15:52:53 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Dighe", "Pranay", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herv\u00e9", ""]]}, {"id": "1709.01186", "submitter": "Danushka Bollegala", "authors": "Krasen Samardzhiev and Andrew Gargett and Danushka Bollegala", "title": "Learning Neural Word Salience Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the salience of a word is an essential step in numerous NLP tasks.\nHeuristic approaches such as tfidf have been used so far to estimate the\nsalience of words. We propose \\emph{Neural Word Salience} (NWS) scores, unlike\nheuristics, are learnt from a corpus. Specifically, we learn word salience\nscores such that, using pre-trained word embeddings as the input, can\naccurately predict the words that appear in a sentence, given the words that\nappear in the sentences preceding or succeeding that sentence. Experimental\nresults on sentence similarity prediction show that the learnt word salience\nscores perform comparably or better than some of the state-of-the-art\napproaches for representing sentences on benchmark datasets for sentence\nsimilarity, while using only a fraction of the training and prediction times\nrequired by prior methods. Moreover, our NWS scores positively correlate with\npsycholinguistic measures such as concreteness, and imageability implying a\nclose connection to the salience as perceived by humans.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 22:52:59 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Samardzhiev", "Krasen", ""], ["Gargett", "Andrew", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1709.01188", "submitter": "Zhichao Hu", "authors": "Zhichao Hu, Marilyn A. Walker, Michael Neff and Jean E. Fox Tree", "title": "Storytelling Agents with Personality and Adaptivity", "comments": "Related dataset: https://nlds.soe.ucsc.edu/sdg", "journal-ref": "In International Conference on Intelligent Virtual Agents, pp.\n  181-193. Springer, Cham, 2015", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the expression of personality and adaptivity through the gestures\nof virtual agents in a storytelling task. We conduct two experiments using four\ndifferent dialogic stories. We manipulate agent personality on the extraversion\nscale, whether the agents adapt to one another in their gestural performance\nand agent gender. Our results show that subjects are able to perceive the\nintended variation in extraversion between different virtual agents,\nindependently of the story they are telling and the gender of the agent. A\nsecond study shows that subjects also prefer adaptive to nonadaptive virtual\nagents.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:06:05 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Hu", "Zhichao", ""], ["Walker", "Marilyn A.", ""], ["Neff", "Michael", ""], ["Tree", "Jean E. Fox", ""]]}, {"id": "1709.01189", "submitter": "Fan Yang", "authors": "Fan Yang, Arjun Mukherjee, Eduard Dragut", "title": "Satirical News Detection and Analysis using Attention Mechanism and\n  Linguistic Features", "comments": "EMNLP 2017, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satirical news is considered to be entertainment, but it is potentially\ndeceptive and harmful. Despite the embedded genre in the article, not everyone\ncan recognize the satirical cues and therefore believe the news as true news.\nWe observe that satirical cues are often reflected in certain paragraphs rather\nthan the whole document. Existing works only consider document-level features\nto detect the satire, which could be limited. We consider paragraph-level\nlinguistic features to unveil the satire by incorporating neural network and\nattention mechanism. We investigate the difference between paragraph-level\nfeatures and document-level features, and analyze them on a large satirical\nnews dataset. The evaluation shows that the proposed model detects satirical\nnews effectively and reveals what features are important at which level.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:06:36 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Yang", "Fan", ""], ["Mukherjee", "Arjun", ""], ["Dragut", "Eduard", ""]]}, {"id": "1709.01193", "submitter": "Danushka Bollegala", "authors": "Huda Hakami and Danushka Bollegala", "title": "Compositional Approaches for Representing Relations Between Words: A\n  Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the relations that exist between words (or entities) is important\nfor various natural language processing tasks such as, relational search,\nnoun-modifier classification and analogy detection. A popular approach to\nrepresent the relations between a pair of words is to extract the patterns in\nwhich the words co-occur with from a corpus, and assign each word-pair a vector\nof pattern frequencies. Despite the simplicity of this approach, it suffers\nfrom data sparseness, information scalability and linguistic creativity as the\nmodel is unable to handle previously unseen word pairs in a corpus. In\ncontrast, a compositional approach for representing relations between words\novercomes these issues by using the attributes of each individual word to\nindirectly compose a representation for the common relations that hold between\nthe two words. This study aims to compare different operations for creating\nrelation representations from word-level representations. We investigate the\nperformance of the compositional methods by measuring the relational\nsimilarities using several benchmark datasets for word analogy. Moreover, we\nevaluate the different relation representations in a knowledge base completion\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:30:22 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Hakami", "Huda", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1709.01199", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala and Yuichi Yoshida and Ken-ichi Kawarabayashi", "title": "Using $k$-way Co-occurrences for Learning Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-occurrences between two words provide useful insights into the semantics\nof those words. Consequently, numerous prior work on word embedding learning\nhave used co-occurrences between two words as the training signal for learning\nword embeddings. However, in natural language texts it is common for multiple\nwords to be related and co-occurring in the same context. We extend the notion\nof co-occurrences to cover $k(\\geq\\!\\!2)$-way co-occurrences among a set of\n$k$-words. Specifically, we prove a theoretical relationship between the joint\nprobability of $k(\\geq\\!\\!2)$ words, and the sum of $\\ell_2$ norms of their\nembeddings. Next, we propose a learning objective motivated by our theoretical\nresult that utilises $k$-way co-occurrences for learning word embeddings. Our\nexperimental results show that the derived theoretical relationship does indeed\nhold empirically, and despite data sparsity, for some smaller $k$ values,\n$k$-way embeddings perform comparably or better than $2$-way embeddings in a\nrange of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 00:25:58 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Bollegala", "Danushka", ""], ["Yoshida", "Yuichi", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1709.01256", "submitter": "Xiaofeng Zhu", "authors": "Xiaofeng Zhu, Diego Klabjan, Patrick Bless", "title": "Semantic Document Distance Measures and Unsupervised Document Revision\n  Detection", "comments": null, "journal-ref": "IJCNLP 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model the document revision detection problem as a minimum\ncost branching problem that relies on computing document distances.\nFurthermore, we propose two new document distance measures, word vector-based\nDynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED).\nOur revision detection system is designed for a large scale corpus and\nimplemented in Apache Spark. We demonstrate that our system can more precisely\ndetect revisions than state-of-the-art methods by utilizing the Wikipedia\nrevision dumps https://snap.stanford.edu/data/wiki-meta.html and simulated data\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 06:47:03 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 03:36:40 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhu", "Xiaofeng", ""], ["Klabjan", "Diego", ""], ["Bless", "Patrick", ""]]}, {"id": "1709.01562", "submitter": "Alexander Bauer", "authors": "Alexander Bauer, Shinichi Nakajima, Nico G\\\"ornitz, Klaus-Robert\n  M\\\"uller", "title": "Optimizing for Measure of Performance in Max-Margin Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical learning problems in the area of natural language processing\nincluding sequence tagging, sequence segmentation and syntactic parsing has\nbeen successfully approached by means of structured prediction methods. An\nappealing property of the corresponding discriminative learning algorithms is\ntheir ability to integrate the loss function of interest directly into the\noptimization process, which potentially can increase the resulting performance\naccuracy. Here, we demonstrate on the example of constituency parsing how to\noptimize for F1-score in the max-margin framework of structural SVM. In\nparticular, the optimization is with respect to the original (not binarized)\ntrees.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 19:27:22 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 13:28:52 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Bauer", "Alexander", ""], ["Nakajima", "Shinichi", ""], ["G\u00f6rnitz", "Nico", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1709.01572", "submitter": "Hao Tang", "authors": "Hao Tang", "title": "Sequence Prediction with Neural Segmental Models", "comments": "Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segments that span contiguous parts of inputs, such as phonemes in speech,\nnamed entities in sentences, actions in videos, occur frequently in sequence\nprediction problems. Segmental models, a class of models that explicitly\nhypothesizes segments, have allowed the exploration of rich segment features\nfor sequence prediction. However, segmental models suffer from slow decoding,\nhampering the use of computationally expensive features.\n  In this thesis, we introduce discriminative segmental cascades, a multi-pass\ninference framework that allows us to improve accuracy by adding higher-order\nfeatures and neural segmental features while maintaining efficiency. We also\nshow that instead of including more features to obtain better accuracy,\nsegmental cascades can be used to speed up training and decoding.\n  Segmental models, similarly to conventional speech recognizers, are typically\ntrained in multiple stages. In the first stage, a frame classifier is trained\nwith manual alignments, and then in the second stage, segmental models are\ntrained with manual alignments and the out- puts of the frame classifier.\nHowever, obtaining manual alignments are time-consuming and expensive. We\nexplore end-to-end training for segmental models with various loss functions,\nand show how end-to-end training with marginal log loss can eliminate the need\nfor detailed manual alignments.\n  We draw the connections between the marginal log loss and a popular\nend-to-end training approach called connectionist temporal classification. We\npresent a unifying framework for various end-to-end graph search-based models,\nsuch as hidden Markov models, connectionist temporal classification, and\nsegmental models. Finally, we discuss possible extensions of segmental models\nto large-vocabulary sequence prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 19:53:35 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 14:25:43 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 03:43:47 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Tang", "Hao", ""]]}, {"id": "1709.01634", "submitter": "J. Michael Herrmann", "authors": "J. Michael Herrmann", "title": "The Voynich Manuscript is Written in Natural Language: The Pahlavi\n  Hypothesis", "comments": "23 pages, 15 figures, 4 tables. Second version: one reference added,\n  duplicate text removed from introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The late medieval Voynich Manuscript (VM) has resisted decryption and was\nconsidered a meaningless hoax or an unsolvable cipher. Here, we provide\nevidence that the VM is written in natural language by establishing a relation\nof the Voynich alphabet and the Iranian Pahlavi script. Many of the Voynich\ncharacters are upside-down versions of their Pahlavi counterparts, which may be\nan effect of different writing directions. Other Voynich letters can be\nexplained as ligatures or departures from Pahlavi with the intent to cope with\nknown problems due to the stupendous ambiguity of Pahlavi text. While a\ntranslation of the VM text is not attempted here, we can confirm the\nVoynich-Pahlavi relation at the character level by the transcription of many\nwords from the VM illustrations and from parts of the main text. Many of the\ntranscribed words can be identified as terms from Zoroastrian cosmology which\nis in line with the use of Pahlavi script in Zoroastrian communities from\nmedieval times.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 00:15:37 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 16:52:11 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Herrmann", "J. Michael", ""]]}, {"id": "1709.01679", "submitter": "Sosuke Kobayashi", "authors": "Sosuke Kobayashi, Naoaki Okazaki, Kentaro Inui", "title": "A Neural Language Model for Dynamically Representing the Meanings of\n  Unknown Words and Entities in a Discourse", "comments": "11 pages. To appear in IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the problem of identifying the meaning of unknown words\nor entities in a discourse with respect to the word embedding approaches used\nin neural language models. We proposed a method for on-the-fly construction and\nexploitation of word embeddings in both the input and output layers of a neural\nmodel by tracking contexts. This extends the dynamic entity representation used\nin Kobayashi et al. (2016) and incorporates a copy mechanism proposed\nindependently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we\nconstruct a new task and dataset called Anonymized Language Modeling for\nevaluating the ability to capture word meanings while reading. Experiments\nconducted using our novel dataset show that the proposed variant of RNN\nlanguage model outperformed the baseline model. Furthermore, the experiments\nalso demonstrate that dynamic updates of an output layer help a model predict\nreappearing entities, whereas those of an input layer are effective to predict\nwords following reappearing entities.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 05:23:37 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 12:35:33 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kobayashi", "Sosuke", ""], ["Okazaki", "Naoaki", ""], ["Inui", "Kentaro", ""]]}, {"id": "1709.01687", "submitter": "Shashank Gupta", "authors": "Shashank Gupta, Sachin Pawar, Nitin Ramrakhiyani, Girish Palshikar and\n  Vasudeva Varma", "title": "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction\n  Mention Extraction", "comments": "Accepted at DTMBIO workshop, CIKM 2017. To appear in BMC\n  Bioinformatics. Pls cite that version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is an useful platform to share health-related information due to\nits vast reach. This makes it a good candidate for public-health monitoring\ntasks, specifically for pharmacovigilance. We study the problem of extraction\nof Adverse-Drug-Reaction (ADR) mentions from social media, particularly from\ntwitter. Medical information extraction from social media is challenging,\nmainly due to short and highly information nature of text, as compared to more\ntechnical and formal medical reports.\n  Current methods in ADR mention extraction relies on supervised learning\nmethods, which suffers from labeled data scarcity problem. The State-of-the-art\nmethod uses deep neural networks, specifically a class of Recurrent Neural\nNetwork (RNN) which are Long-Short-Term-Memory networks (LSTMs)\n\\cite{hochreiter1997long}. Deep neural networks, due to their large number of\nfree parameters relies heavily on large annotated corpora for learning the end\ntask. But in real-world, it is hard to get large labeled data, mainly due to\nheavy cost associated with manual annotation. Towards this end, we propose a\nnovel semi-supervised learning based RNN model, which can leverage unlabeled\ndata also present in abundance on social media. Through experiments we\ndemonstrate the effectiveness of our method, achieving state-of-the-art\nperformance in ADR mention extraction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:42:22 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Gupta", "Shashank", ""], ["Pawar", "Sachin", ""], ["Ramrakhiyani", "Nitin", ""], ["Palshikar", "Girish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1709.01713", "submitter": "James Salsman", "authors": "Yuan Gao, Brij Mohan Lal Srivastava, and James Salsman", "title": "Spoken English Intelligibility Remediation with PocketSphinx Alignment\n  and Feature Extraction Improves Substantially over the State of the Art", "comments": "8 pages, 3 figures", "journal-ref": "IEEE Advanced Information Management, Communications, Electronic\n  and Automation Control Conference 2 (2018)", "doi": "10.1109/IMCEC.2018.8469649", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We use automatic speech recognition to assess spoken English learner\npronunciation based on the authentic intelligibility of the learners' spoken\nresponses determined from support vector machine (SVM) classifier or deep\nlearning neural network model predictions of transcription correctness. Using\nnumeric features produced by PocketSphinx alignment mode and many recognition\npasses searching for the substitution and deletion of each expected phoneme and\ninsertion of unexpected phonemes in sequence, the SVM models achieve 82 percent\nagreement with the accuracy of Amazon Mechanical Turk crowdworker\ntranscriptions, up from 75 percent reported by multiple independent\nresearchers. Using such features with SVM classifier probability prediction\nmodels can help computer-aided pronunciation teaching (CAPT) systems provide\nintelligibility remediation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:27:59 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 22:59:37 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 09:59:26 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gao", "Yuan", ""], ["Srivastava", "Brij Mohan Lal", ""], ["Salsman", "James", ""]]}, {"id": "1709.01766", "submitter": "Wen Zhang", "authors": "Wen Zhang, Jiawei Hu, Yang Feng and Qun Liu", "title": "Information-Propogation-Enhanced Neural Machine Translation by Relation\n  Model", "comments": "i am planned to improve my experiments and modified our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though sequence-to-sequence neural machine translation (NMT) model have\nachieved state-of-art performance in the recent fewer years, but it is widely\nconcerned that the recurrent neural network (RNN) units are very hard to\ncapture the long-distance state information, which means RNN can hardly find\nthe feature with long term dependency as the sequence becomes longer.\nSimilarly, convolutional neural network (CNN) is introduced into NMT for\nspeeding recently, however, CNN focus on capturing the local feature of the\nsequence; To relieve this issue, we incorporate a relation network into the\nstandard encoder-decoder framework to enhance information-propogation in neural\nnetwork, ensuring that the information of the source sentence can flow into the\ndecoder adequately. Experiments show that proposed framework outperforms the\nstatistical MT model and the state-of-art NMT model significantly on two data\nsets with different scales.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:13:50 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:22:10 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 13:36:58 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Zhang", "Wen", ""], ["Hu", "Jiawei", ""], ["Feng", "Yang", ""], ["Liu", "Qun", ""]]}, {"id": "1709.01848", "submitter": "Andrew Yates", "authors": "Andrew Yates, Arman Cohan, Nazli Goharian", "title": "Depression and Self-Harm Risk Assessment in Online Forums", "comments": "Expanded version of EMNLP17 paper. Added sections 6.1, 6.2, 6.4,\n  FastText baseline, and CNN-R", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users suffering from mental health conditions often turn to online resources\nfor support, including specialized online support communities or general\ncommunities such as Twitter and Reddit. In this work, we present a neural\nframework for supporting and studying users in both types of communities. We\npropose methods for identifying posts in support communities that may indicate\na risk of self-harm, and demonstrate that our approach outperforms strong\npreviously proposed methods for identifying such posts. Self-harm is closely\nrelated to depression, which makes identifying depressed users on general\nforums a crucial related task. We introduce a large-scale general forum dataset\n(\"RSDD\") consisting of users with self-reported depression diagnoses matched\nwith control users. We show how our method can be applied to effectively\nidentify depressed users from their use of language alone. We demonstrate that\nour method outperforms strong baselines on this general forum dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:50:42 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Yates", "Andrew", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1709.01887", "submitter": "Amita Misra", "authors": "Amita Misra, Brian Ecker, and Marilyn A. Walker", "title": "Measuring the Similarity of Sentential Arguments in Dialog", "comments": "Measuring the Similarity of Sentential Arguments in Dialog, by Misra,\n  Amita and Ecker, Brian and Walker, Marilyn A, 17th Annual Meeting of the\n  Special Interest Group on Discourse and Dialogue, pages={276}, year={2016}\n  The dataset is available at https://nlds.soe.ucsc.edu/node/44", "journal-ref": null, "doi": "10.18653/v1/w16-3636", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people converse about social or political topics, similar arguments are\noften paraphrased by different speakers, across many different conversations.\nDebate websites produce curated summaries of arguments on such topics; these\nsummaries typically consist of lists of sentences that represent frequently\nparaphrased propositions, or labels capturing the essence of one particular\naspect of an argument, e.g. Morality or Second Amendment. We call these\nfrequently paraphrased propositions ARGUMENT FACETS. Like these curated sites,\nour goal is to induce and identify argument facets across multiple\nconversations, and produce summaries. However, we aim to do this automatically.\nWe frame the problem as consisting of two steps: we first extract sentences\nthat express an argument from raw social media dialogs, and then rank the\nextracted arguments in terms of their similarity to one another. Sets of\nsimilar arguments are used to represent argument facets. We show here that we\ncan predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63\ncompared to a human topline averaging 0.68 over three debate topics, easily\nbeating several reasonable baselines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 17:15:49 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Misra", "Amita", ""], ["Ecker", "Brian", ""], ["Walker", "Marilyn A.", ""]]}, {"id": "1709.01888", "submitter": "Miriam Cha", "authors": "Miriam Cha, Youngjune Gwon, H.T. Kung", "title": "Language Modeling by Clustering with Word Embeddings for Text\n  Readability Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a clustering-based language model using word embeddings for text\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\nholds true for word embeddings whose training is done by observing word\nco-occurrences. We argue that clustering with word embeddings in the metric\nspace should yield feature representations in a higher semantic space\nappropriate for text regression. Also, by representing features in terms of\nhistograms, our approach can naturally address documents of varying lengths. An\nempirical evaluation using the Common Core Standards corpus reveals that the\nfeatures formed on our clustering-based language model significantly improve\nthe previously known results for the same corpus in readability prediction. We\nalso evaluate the task of sentence matching based on semantic relatedness using\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 02:38:44 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Cha", "Miriam", ""], ["Gwon", "Youngjune", ""], ["Kung", "H. T.", ""]]}, {"id": "1709.01895", "submitter": "Amita Misra", "authors": "Amita Misra, Brian Ecker, Theodore Handleman, Nicolas Hahn and Marilyn\n  Walker", "title": "A Semi-Supervised Approach to Detecting Stance in Tweets", "comments": "@InProceedings{S16-1068, title = \"NLDS-UCSC at SemEval-2016 Task 6: A\n  Semi-Supervised Approach to Detecting Stance in Tweets\", \"Misra, Amita and\n  Ecker, Brian and Handleman, Theodore and Hahn, Nicolas and Walker,\n  Marilyn\",booktitle = \"Proceedings of the 10th International Workshop on\n  Semantic Evaluation (SemEval-2016) \", year = \"2016\",publisher = \"Association\n  for Computational Linguistics\"}", "journal-ref": "International Workshop on Semantic Evaluation 2016", "doi": "10.18653/v1/s16-1068", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance classification aims to identify, for a particular issue under\ndiscussion, whether the speaker or author of a conversational turn has Pro\n(Favor) or Con (Against) stance on the issue. Detecting stance in tweets is a\nnew task proposed for SemEval-2016 Task6, involving predicting stance for a\ndataset of tweets on the topics of abortion, atheism, climate change, feminism\nand Hillary Clinton. Given the small size of the dataset, our team created our\nown topic-specific training corpus by developing a set of high precision\nhashtags for each topic that were used to query the twitter API, with the aim\nof developing a large training corpus without additional human labeling of\ntweets for stance. The hashtags selected for each topic were predicted to be\nstance-bearing on their own. Experimental results demonstrate good performance\nfor our features for opinion-target pairs based on generalizing dependency\nfeatures using sentiment lexicons.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 04:31:11 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Misra", "Amita", ""], ["Ecker", "Brian", ""], ["Handleman", "Theodore", ""], ["Hahn", "Nicolas", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.01915", "submitter": "James Bradbury", "authors": "James Bradbury, Richard Socher", "title": "Towards Neural Machine Translation with Latent Tree Attention", "comments": "Presented at SPNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building models that take advantage of the hierarchical structure of language\nwithout a priori annotation is a longstanding goal in natural language\nprocessing. We introduce such a model for the task of machine translation,\npairing a recurrent neural network grammar encoder with a novel attentional\nRNNG decoder and applying policy gradient reinforcement learning to induce\nunsupervised tree structures on both the source and target. When trained on\ncharacter-level datasets with no explicit segmentation or parse annotation, the\nmodel learns a plausible segmentation and shallow parse, obtaining performance\nclose to an attentional baseline.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 17:44:53 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Bradbury", "James", ""], ["Socher", "Richard", ""]]}, {"id": "1709.01950", "submitter": "Lakshya Kumar", "authors": "Lakshya Kumar, Arpan Somani, Pushpak Bhattacharyya", "title": "\"Having 2 hours to write a paper is fun!\": Detecting Sarcasm in\n  Numerical Portions of Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sarcasm occurring due to the presence of numerical portions in text has been\nquoted as an error made by automatic sarcasm detection approaches in the past.\nWe present a first study in detecting sarcasm in numbers, as in the case of the\nsentence 'Love waking up at 4 am'. We analyze the challenges of the problem,\nand present Rule-based, Machine Learning and Deep Learning approaches to detect\nsarcasm in numerical portions of text. Our Deep Learning approach outperforms\nfour past works for sarcasm detection and Rule-based and Machine learning\napproaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows\nthat special attention to text containing numbers may be useful to improve\nstate-of-the-art in sarcasm detection.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 18:09:15 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Kumar", "Lakshya", ""], ["Somani", "Arpan", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1709.01991", "submitter": "Monika Rani", "authors": "Monika Rani, Amit Kumar Dhar and O. P. Vyas", "title": "Semi-Automatic Terminology Ontology Learning Based on Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies provide features like a common vocabulary, reusability,\nmachine-readable content, and also allows for semantic search, facilitate agent\ninteraction and ordering & structuring of knowledge for the Semantic Web (Web\n3.0) application. However, the challenge in ontology engineering is automatic\nlearning, i.e., the there is still a lack of fully automatic approach from a\ntext corpus or dataset of various topics to form ontology using machine\nlearning techniques. In this paper, two topic modeling algorithms are explored,\nnamely LSI & SVD and Mr.LDA for learning topic ontology. The objective is to\ndetermine the statistical relationship between document and terms to build a\ntopic ontology and ontology graph with minimum human intervention. Experimental\nanalysis on building a topic ontology and semantic retrieving corresponding\ntopic ontology for the user's query demonstrating the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:30:48 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Rani", "Monika", ""], ["Dhar", "Amit Kumar", ""], ["Vyas", "O. P.", ""]]}, {"id": "1709.02076", "submitter": "Clayton Morrison", "authors": "Donya Quick, Clayton T. Morrison", "title": "Composition by Conversation", "comments": "6 pages, 8 figures, accepted to ICMC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most musical programming languages are developed purely for coding virtual\ninstruments or algorithmic compositions. Although there has been some work in\nthe domain of musical query languages for music information retrieval, there\nhas been little attempt to unify the principles of musical programming and\nquery languages with cognitive and natural language processing models that\nwould facilitate the activity of composition by conversation. We present a\nprototype framework, called MusECI, that merges these domains, permitting\nscore-level algorithmic composition in a text editor while also supporting\nconnectivity to existing natural language processing frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:39:00 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Quick", "Donya", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1709.02184", "submitter": "Mihael Arcan", "authors": "Mihael Arcan, Daniel Torregrosa, Paul Buitelaar", "title": "Translating Terminological Expressions in Knowledge Bases with Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work presented in this paper focuses on the translation of terminological\nexpressions represented in semantically structured resources, like ontologies\nor knowledge graphs. The challenge of translating ontology labels or\nterminological expressions documented in knowledge bases lies in the highly\nspecific vocabulary and the lack of contextual information, which can guide a\nmachine translation system to translate ambiguous words into the targeted\ndomain. Due to these challenges, we evaluate the translation quality of\ndomain-specific expressions in the medical and financial domain with\nstatistical as well as with neural machine translation methods and experiment\ndomain adaptation of the translation models with terminological expressions\nonly. Furthermore, we perform experiments on the injection of external\nterminological expressions into the translation systems. Through these\nexperiments, we observed a significant advantage in domain adaptation for the\ndomain-specific resource in the medical and financial domain and the benefit of\nsubword models over word-based neural machine translation models for\nterminology translation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 11:31:09 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:58:54 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 16:40:22 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Arcan", "Mihael", ""], ["Torregrosa", "Daniel", ""], ["Buitelaar", "Paul", ""]]}, {"id": "1709.02271", "submitter": "Su Wang", "authors": "Su Wang, Elisa Ferracane, Raymond J. Mooney", "title": "Leveraging Discourse Information Effectively for Authorship Attribution", "comments": "Accepted at IJCNLP 2017 as a conference paper", "journal-ref": "The 8th International Joint Conference on Natural Language\n  Processing (IJCNLP 2017)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore techniques to maximize the effectiveness of discourse information\nin the task of authorship attribution. We present a novel method to embed\ndiscourse features in a Convolutional Neural Network text classifier, which\nachieves a state-of-the-art result by a substantial margin. We empirically\ninvestigate several featurization methods to understand the conditions under\nwhich discourse features contribute non-trivial performance gains, and analyze\ndiscourse embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 14:22:50 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Wang", "Su", ""], ["Ferracane", "Elisa", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1709.02279", "submitter": "Amittai Axelrod", "authors": "Amittai Axelrod", "title": "Cynical Selection of Language Model Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Moore-Lewis method of \"intelligent selection of language model training\ndata\" is very effective, cheap, efficient... and also has structural problems.\n(1) The method defines relevance by playing language models trained on the\nin-domain and the out-of-domain (or data pool) corpora against each other. This\npowerful idea-- which we set out to preserve-- treats the two corpora as the\nopposing ends of a single spectrum. This lack of nuance does not allow for the\ntwo corpora to be very similar. In the extreme case where the come from the\nsame distribution, all of the sentences have a Moore-Lewis score of zero, so\nthere is no resulting ranking. (2) The selected sentences are not guaranteed to\nbe able to model the in-domain data, nor to even cover the in-domain data. They\nare simply well-liked by the in-domain model; this is necessary, but not\nsufficient. (3) There is no way to tell what is the optimal number of sentences\nto select, short of picking various thresholds and building the systems.\n  We present a greedy, lazy, approximate, and generally efficient\ninformation-theoretic method of accomplishing the same goal using only\nvocabulary counts. The method has the following properties: (1) Is responsive\nto the extent to which two corpora differ. (2) Quickly reaches near-optimal\nvocabulary coverage. (3) Takes into account what has already been selected. (4)\nDoes not involve defining any kind of domain, nor any kind of classifier. (6)\nKnows approximately when to stop. This method can be used as an\ninherently-meaningful measure of similarity, as it measures the bits of\ninformation to be gained by adding one text to another.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 14:30:50 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Axelrod", "Amittai", ""]]}, {"id": "1709.02349", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng\n  Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath\n  Chandar, Nan Rosemary Ke, Sai Rajeshwar, Alexandre de Brebisson, Jose M. R.\n  Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau,\n  Yoshua Bengio", "title": "A Deep Reinforcement Learning Chatbot", "comments": "40 pages, 9 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MILABOT: a deep reinforcement learning chatbot developed by the\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\ncompetition. MILABOT is capable of conversing with humans on popular small talk\ntopics through both speech and text. The system consists of an ensemble of\nnatural language generation and retrieval models, including template-based\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\nvariable neural network models. By applying reinforcement learning to\ncrowdsourced data and real-world user interactions, the system has been trained\nto select an appropriate response from the models in its ensemble. The system\nhas been evaluated through A/B testing with real-world users, where it\nperformed significantly better than many competing systems. Due to its machine\nlearning architecture, the system is likely to improve with additional data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:51:09 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:02:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sankar", "Chinnadhurai", ""], ["Germain", "Mathieu", ""], ["Zhang", "Saizheng", ""], ["Lin", "Zhouhan", ""], ["Subramanian", "Sandeep", ""], ["Kim", "Taesup", ""], ["Pieper", "Michael", ""], ["Chandar", "Sarath", ""], ["Ke", "Nan Rosemary", ""], ["Rajeshwar", "Sai", ""], ["de Brebisson", "Alexandre", ""], ["Sotelo", "Jose M. R.", ""], ["Suhubdy", "Dendi", ""], ["Michalski", "Vincent", ""], ["Nguyen", "Alexandre", ""], ["Pineau", "Joelle", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1709.02755", "submitter": "Tao Lei", "authors": "Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai and Yoav Artzi", "title": "Simple Recurrent Units for Highly Parallelizable Recurrence", "comments": "EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common recurrent neural architectures scale poorly due to the intrinsic\ndifficulty in parallelizing their state computations. In this work, we propose\nthe Simple Recurrent Unit (SRU), a light recurrent unit that balances model\ncapacity and scalability. SRU is designed to provide expressive recurrence,\nenable highly parallelized implementation, and comes with careful\ninitialization to facilitate training of deep models. We demonstrate the\neffectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over\ncuDNN-optimized LSTM on classification and question answering datasets, and\ndelivers stronger results than LSTM and convolutional models. We also obtain an\naverage of 0.7 BLEU improvement over the Transformer model on translation by\nincorporating SRU into the architecture.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 16:02:30 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 20:13:56 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 12:44:43 GMT"}, {"version": "v4", "created": "Tue, 26 Dec 2017 03:58:02 GMT"}, {"version": "v5", "created": "Fri, 7 Sep 2018 17:17:02 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Lei", "Tao", ""], ["Zhang", "Yu", ""], ["Wang", "Sida I.", ""], ["Dai", "Hui", ""], ["Artzi", "Yoav", ""]]}, {"id": "1709.02783", "submitter": "Richard Futrell", "authors": "Richard Futrell, Roger Levy and Matthew Dryer", "title": "A Statistical Comparison of Some Theories of NP Word Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A frequent object of study in linguistic typology is the order of elements\n{demonstrative, adjective, numeral, noun} in the noun phrase. The goal is to\npredict the relative frequencies of these orders across languages. Here we use\nPoisson regression to statistically compare some prominent accounts of this\nvariation. We compare feature systems derived from Cinque (2005) to feature\nsystems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not\nfind clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but\nwe find both of these models have substantially better fit to the typological\ndata than the model from Cysouw (2010).\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 17:12:16 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Futrell", "Richard", ""], ["Levy", "Roger", ""], ["Dryer", "Matthew", ""]]}, {"id": "1709.02828", "submitter": "Jonathan Raiman", "authors": "Jonathan Raiman and John Miller", "title": "Globally Normalized Reader", "comments": "Presented at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid progress has been made towards question answering (QA) systems that can\nextract answers from text. Existing neural approaches make use of expensive\nbi-directional attention mechanisms or score all possible answer spans,\nlimiting scalability. We propose instead to cast extractive QA as an iterative\nsearch problem: select the answer's sentence, start word, and end word. This\nrepresentation reduces the space of each search step and allows computation to\nbe conditionally allocated to promising search paths. We show that globally\nnormalizing the decision process and back-propagating through beam search makes\nthis representation viable and learning efficient. We empirically demonstrate\nthe benefits of this approach using our model, Globally Normalized Reader\n(GNR), which achieves the second highest single model performance on the\nStanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster\nthan bi-attention-flow. We also introduce a data-augmentation method to produce\nsemantically valid examples by aligning named entities to a knowledge base and\nswapping them with new entities of the same type. This method improves the\nperformance of all models considered in this work and is of independent\ninterest for a variety of NLP tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 18:27:50 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Raiman", "Jonathan", ""], ["Miller", "John", ""]]}, {"id": "1709.02842", "submitter": "Lisa Lee", "authors": "Yohan Jo, Lisa Lee, Shruti Palaskar", "title": "Combining LSTM and Latent Topic Modeling for Mortality Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great need for technologies that can predict the mortality of\npatients in intensive care units with both high accuracy and accountability. We\npresent joint end-to-end neural network architectures that combine long\nshort-term memory (LSTM) and a latent topic model to simultaneously train a\nclassifier for mortality prediction and learn latent topics indicative of\nmortality from textual clinical notes. For topic interpretability, the topic\nmodeling layer has been carefully designed as a single-layer network with\nconstraints inspired by LDA. Experiments on the MIMIC-III dataset show that our\nmodels significantly outperform prior models that are based on LDA topics in\nmortality prediction. However, we achieve limited success with our method for\ninterpreting topics from the trained models by looking at the neural network\nweights.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 19:30:09 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Jo", "Yohan", ""], ["Lee", "Lisa", ""], ["Palaskar", "Shruti", ""]]}, {"id": "1709.02843", "submitter": "Leila Kosseim", "authors": "Elnaz Davoodi and Leila Kosseim", "title": "CLaC at SemEval-2016 Task 11: Exploring linguistic and psycho-linguistic\n  Features for Complex Word Identification", "comments": "In Proceedings of the International Workshop on Semantic Evaluation\n  (SemEval-2016), a workshop of the 15th Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics: Human\n  Language Technologies (NAACL-2016) pp 982-985. June 16-17, San Diego,\n  California", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the system deployed by the CLaC-EDLK team to the\n\"SemEval 2016, Complex Word Identification task\". The goal of the task is to\nidentify if a given word in a given context is \"simple\" or \"complex\". Our\nsystem relies on linguistic features and cognitive complexity. We used several\nsupervised models, however the Random Forest model outperformed the others.\nOverall our best configuration achieved a G-score of 68.8% in the task, ranking\nour system 21 out of 45.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 19:34:02 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Davoodi", "Elnaz", ""], ["Kosseim", "Leila", ""]]}, {"id": "1709.02911", "submitter": "Vindula Jayawardana", "authors": "Vindula Jayawardana, Dimuthu Lakmal, Nisansa de Silva, Amal Shehan\n  Perera, Keet Sugathadasa, Buddhi Ayesha, Madhavi Perera", "title": "Semi-Supervised Instance Population of an Ontology using Word Vector\n  Embeddings", "comments": null, "journal-ref": null, "doi": "10.1109/ICTER.2017.8257822", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern day systems such as information extraction and knowledge\nmanagement agents, ontologies play a vital role in maintaining the concept\nhierarchies of the selected domain. However, ontology population has become a\nproblematic process due to its nature of heavy coupling with manual human\nintervention. With the use of word embeddings in the field of natural language\nprocessing, it became a popular topic due to its ability to cope up with\nsemantic sensitivity. Hence, in this study, we propose a novel way of\nsemi-supervised ontology population through word embeddings as the basis. We\nbuilt several models including traditional benchmark models and new types of\nmodels which are based on word embeddings. Finally, we ensemble them together\nto come up with a synergistic model with better accuracy. We demonstrate that\nour ensemble model can outperform the individual models.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 05:04:19 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Jayawardana", "Vindula", ""], ["Lakmal", "Dimuthu", ""], ["de Silva", "Nisansa", ""], ["Perera", "Amal Shehan", ""], ["Sugathadasa", "Keet", ""], ["Ayesha", "Buddhi", ""], ["Perera", "Madhavi", ""]]}, {"id": "1709.02968", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu and Hongsu Wang", "title": "Matrix and Graph Operations for Relationship Inference: An Illustration\n  with the Kinship Inference in the China Biographical Database", "comments": "3 pages, 3 figures, 2017 Annual Meeting of the Japanese Association\n  for Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biographical databases contain diverse information about individuals. Person\nnames, birth information, career, friends, family and special achievements are\nsome possible items in the record for an individual. The relationships between\nindividuals, such as kinship and friendship, provide invaluable insights about\nhidden communities which are not directly recorded in databases. We show that\nsome simple matrix and graph-based operations are effective for inferring\nrelationships among individuals, and illustrate the main ideas with the China\nBiographical Database (CBDB).\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 16:01:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Wang", "Hongsu", ""]]}, {"id": "1709.02984", "submitter": "Nicole Novielli", "authors": "Fabio Calefato, Filippo Lanubile, Federico Maiorano, Nicole Novielli", "title": "Sentiment Polarity Detection for Software Development", "comments": "Cite as: Calefato, F., Lanubile, F., Maiorano, F., Novielli N. Empir\n  Software Eng (2017). https://doi.org/10.1007/s10664-017-9546-9 Full-text\n  view-only version here: http://rdcu.be/vZrG, Empir Software Eng (2017)", "journal-ref": "Empirical Software Engineering, June 2018, Volume 23, Issue 3, pp\n  1352 - 1382", "doi": "10.1007/s10664-017-9546-9", "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of sentiment analysis is increasingly emerging to study software\ndevelopers' emotions by mining crowd-generated content within social software\nengineering tools. However, off-the-shelf sentiment analysis tools have been\ntrained on non-technical domains and general-purpose social media, thus\nresulting in misclassifications of technical jargon and problem reports. Here,\nwe present Senti4SD, a classifier specifically trained to support sentiment\nanalysis in developers' communication channels. Senti4SD is trained and\nvalidated using a gold standard of Stack Overflow questions, answers, and\ncomments manually annotated for sentiment polarity. It exploits a suite of both\nlexicon- and keyword-based features, as well as semantic features based on word\nembedding. With respect to a mainstream off-the-shelf tool, which we use as a\nbaseline, Senti4SD reduces the misclassifications of neutral and positive posts\nas emotionally negative. To encourage replications, we release a lab package\nincluding the classifier, the word embedding space, and the gold standard with\nannotation guidelines.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 17:28:10 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 14:37:55 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Calefato", "Fabio", ""], ["Lanubile", "Filippo", ""], ["Maiorano", "Federico", ""], ["Novielli", "Nicole", ""]]}, {"id": "1709.03010", "submitter": "Di Wang", "authors": "Di Wang, Nebojsa Jojic, Chris Brockett, Eric Nyberg", "title": "Steering Output Style and Topic in Neural Response Generation", "comments": "EMNLP 2017 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose simple and flexible training and decoding methods for influencing\noutput style and topic in neural encoder-decoder based language generation.\nThis capability is desirable in a variety of applications, including\nconversational systems, where successful agents need to produce language in a\nspecific style and generate responses steered by a human puppeteer or external\nknowledge. We decompose the neural generation process into empirically easier\nsub-problems: a faithfulness model and a decoding method based on\nselective-sampling. We also describe training and sampling algorithms that bias\nthe generation process with a specific language style restriction, or a topic\nrestriction. Human evaluation results show that our proposed methods are able\nto restrict style and topic without degrading output quality in conversational\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 22:03:11 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Wang", "Di", ""], ["Jojic", "Nebojsa", ""], ["Brockett", "Chris", ""], ["Nyberg", "Eric", ""]]}, {"id": "1709.03036", "submitter": "Kevin McCurley", "authors": "Kedar Dhamdhere and Kevin S. McCurley and Mukund Sundararajan and\n  Ankur Taly", "title": "Abductive Matching in Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study question-answering over semi-structured data. We introduce a new way\nto apply the technique of semantic parsing by applying machine learning only to\nprovide annotations that the system infers to be missing; all the other parsing\nlogic is in the form of manually authored rules. In effect, the machine\nlearning is used to provide non-syntactic matches, a step that is ill-suited to\nmanual rules. The advantage of this approach is in its debuggability and in its\ntransparency to the end-user. We demonstrate the effectiveness of the approach\nby achieving state-of-the-art performance of 40.42% accuracy on a standard\nbenchmark dataset over tables from Wikipedia.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 03:42:43 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Dhamdhere", "Kedar", ""], ["McCurley", "Kevin S.", ""], ["Sundararajan", "Mukund", ""], ["Taly", "Ankur", ""]]}, {"id": "1709.03064", "submitter": "Sanyam Agarwal", "authors": "Mayank Singh, Soham Dan, Sanyam Agarwal, Pawan Goyal, Animesh\n  Mukherjee", "title": "AppTechMiner: Mining Applications and Techniques from Scientific\n  Articles", "comments": "JCDL 2017, 6th International Workshop on Mining Scientific\n  Publications. arXiv admin note: substantial text overlap with\n  arXiv:1608.06386", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents AppTechMiner, a rule-based information extraction\nframework that automatically constructs a knowledge base of all application\nareas and problem solving techniques. Techniques include tools, methods,\ndatasets or evaluation metrics. We also categorize individual research articles\nbased on their application areas and the techniques proposed/improved in the\narticle. Our system achieves high average precision (~82%) and recall (~84%) in\nknowledge base creation. It also performs well in application and technique\nassignment to an individual article (average accuracy ~66%). In the end, we\nfurther present two use cases presenting a trivial information retrieval system\nand an extensive temporal analysis of the usage of techniques and application\nareas. At present, we demonstrate the framework for the domain of computational\nlinguistics but this can be easily generalized to any other field of research.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 07:56:44 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 06:13:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Singh", "Mayank", ""], ["Dan", "Soham", ""], ["Agarwal", "Sanyam", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1709.03167", "submitter": "Kevin Bowden", "authors": "Geetanjali Rakshit, Kevin K. Bowden, Lena Reed, Amita Misra, Marilyn\n  Walker", "title": "Debbie, the Debate Bot of the Future", "comments": "IWSDS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots are a rapidly expanding application of dialogue systems with\ncompanies switching to bot services for customer support, and new applications\nfor users interested in casual conversation. One style of casual conversation\nis argument, many people love nothing more than a good argument. Moreover,\nthere are a number of existing corpora of argumentative dialogues, annotated\nfor agreement and disagreement, stance, sarcasm and argument quality. This\npaper introduces Debbie, a novel arguing bot, that selects arguments from\nconversational corpora, and aims to use them appropriately in context. We\npresent an initial working prototype of Debbie, with some preliminary\nevaluation and describe future work.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 20:22:30 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Rakshit", "Geetanjali", ""], ["Bowden", "Kevin K.", ""], ["Reed", "Lena", ""], ["Misra", "Amita", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.03190", "submitter": "Kevin Bowden", "authors": "Kevin K. Bowden, Shereen Oraby, Amita Misra, Jiaqi Wu, and Stephanie\n  Lukin", "title": "Data-Driven Dialogue Systems for Social Agents", "comments": "IWSDS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to build dialogue systems to tackle the ambitious task of holding\nsocial conversations, we argue that we need a data driven approach that\nincludes insight into human conversational chit chat, and which incorporates\ndifferent natural language processing modules. Our strategy is to analyze and\nindex large corpora of social media data, including Twitter conversations,\nonline debates, dialogues between friends, and blog posts, and then to couple\nthis data retrieval with modules that perform tasks such as sentiment and style\nanalysis, topic modeling, and summarization. We aim for personal assistants\nthat can learn more nuanced human language, and to grow from task-oriented\nagents to more personable social bots.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 22:37:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Oraby", "Shereen", ""], ["Misra", "Amita", ""], ["Wu", "Jiaqi", ""], ["Lukin", "Stephanie", ""]]}, {"id": "1709.03406", "submitter": "Jo\\~ao Pereira Sr.", "authors": "Jo\\~ao Filipe Figueiredo Pereira", "title": "Social Media Text Processing and Semantic Analysis for Smart Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of Social Media, people obtain and share information almost\ninstantly on a 24/7 basis. Many research areas have tried to gain valuable\ninsights from these large volumes of freely available user generated content.\n  With the goal of extracting knowledge from social media streams that might be\nuseful in the context of intelligent transportation systems and smart cities,\nwe designed and developed a framework that provides functionalities for\nparallel collection of geo-located tweets from multiple pre-defined bounding\nboxes (cities or regions), including filtering of non-complying tweets, text\npre-processing for Portuguese and English language, topic modeling, and\ntransportation-specific text classifiers, as well as, aggregation and data\nvisualization.\n  We performed an exploratory data analysis of geo-located tweets in 5\ndifferent cities: Rio de Janeiro, S\\~ao Paulo, New York City, London and\nMelbourne, comprising a total of more than 43 million tweets in a period of 3\nmonths. Furthermore, we performed a large scale topic modelling comparison\nbetween Rio de Janeiro and S\\~ao Paulo. Interestingly, most of the topics are\nshared between both cities which despite being in the same country are\nconsidered very different regarding population, economy and lifestyle.\n  We take advantage of recent developments in word embeddings and train such\nrepresentations from the collections of geo-located tweets. We then use a\ncombination of bag-of-embeddings and traditional bag-of-words to train\ntravel-related classifiers in both Portuguese and English to filter\ntravel-related content from non-related. We created specific gold-standard data\nto perform empirical evaluation of the resulting classifiers. Results are in\nline with research work in other application areas by showing the robustness of\nusing word embeddings to learn word similarities that bag-of-words is not able\nto capture.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 14:30:35 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Pereira", "Jo\u00e3o Filipe Figueiredo", ""]]}, {"id": "1709.03544", "submitter": "Dominic Seyler", "authors": "Dominic Seyler, Tatiana Dembelova, Luciano Del Corro, Johannes\n  Hoffart, Gerhard Weikum", "title": "KnowNER: Incremental Multilingual Knowledge in Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KnowNER is a multilingual Named Entity Recognition (NER) system that\nleverages different degrees of external knowledge. A novel modular framework\ndivides the knowledge into four categories according to the depth of knowledge\nthey convey. Each category consists of a set of features automatically\ngenerated from different information sources (such as a knowledge-base, a list\nof names or document-specific semantic annotations) and is used to train a\nconditional random field (CRF). Since those information sources are usually\nmultilingual, KnowNER can be easily trained for a wide range of languages. In\nthis paper, we show that the incorporation of deeper knowledge systematically\nboosts accuracy and compare KnowNER with state-of-the-art NER approaches across\nthree languages (i.e., English, German and Spanish) performing amongst\nstate-of-the art systems in all of them.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 18:54:15 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Seyler", "Dominic", ""], ["Dembelova", "Tatiana", ""], ["Del Corro", "Luciano", ""], ["Hoffart", "Johannes", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1709.03637", "submitter": "Fei Liu", "authors": "Fei Liu, Timothy Baldwin and Trevor Cohn", "title": "Capturing Long-range Contextual Dependencies with Memory-enhanced\n  Conditional Random Fields", "comments": "Accepted to IJCNLP 2017 (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite successful applications across a broad range of NLP tasks,\nconditional random fields (\"CRFs\"), in particular the linear-chain variant, are\nonly able to model local features. While this has important benefits in terms\nof inference tractability, it limits the ability of the model to capture\nlong-range dependencies between items. Attempts to extend CRFs to capture\nlong-range dependencies have largely come at the cost of computational\ncomplexity and approximate inference. In this work, we propose an extension to\nCRFs by integrating external memory, taking inspiration from memory networks,\nthereby allowing CRFs to incorporate information far beyond neighbouring steps.\nExperiments across two tasks show substantial improvements over strong CRF and\nLSTM baselines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 00:57:04 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 03:31:10 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Liu", "Fei", ""], ["Baldwin", "Timothy", ""], ["Cohn", "Trevor", ""]]}, {"id": "1709.03665", "submitter": "Zhiming Wang", "authors": "Zhiming Wang, Xiaolong Li, Jun Zhou", "title": "Small-footprint Keyword Spotting Using Deep Neural Network and\n  Connectionist Temporal Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainly for the sake of solving the lack of keyword-specific data, we propose\none Keyword Spotting (KWS) system using Deep Neural Network (DNN) and\nConnectionist Temporal Classifier (CTC) on power-constrained small-footprint\nmobile devices, taking full advantage of general corpus from continuous speech\nrecognition which is of great amount. DNN is to directly predict the posterior\nof phoneme units of any personally customized key-phrase, and CTC to produce a\nconfidence score of the given phoneme sequence as responsive decision-making\nmechanism. The CTC-KWS has competitive performance in comparison with purely\nDNN based keyword specific KWS, but not increasing any computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 02:52:54 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Wang", "Zhiming", ""], ["Li", "Xiaolong", ""], ["Zhou", "Jun", ""]]}, {"id": "1709.03742", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma", "title": "Dependencies: Formalising Semantic Catenae for Information Retrieval", "comments": "This document is a doktordisputats - a dissertation within the Danish\n  academic system required to obtain the degree of \\textit{Doctor Scientiarum},\n  in form and function equivalent to the French and German Habilitation and the\n  Higher Doctorate of the Commonwealth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machines that can understand text like humans is an AI-complete\nproblem. A great deal of research has already gone into this, with astounding\nresults, allowing everyday people to discuss with their telephones, or have\ntheir reading materials analysed and classified by computers. A prerequisite\nfor processing text semantics, common to the above examples, is having some\ncomputational representation of text as an abstract object. Operations on this\nrepresentation practically correspond to making semantic inferences, and by\nextension simulating understanding text. The complexity and granularity of\nsemantic processing that can be realised is constrained by the mathematical and\ncomputational robustness, expressiveness, and rigour of the tools used.\n  This dissertation contributes a series of such tools, diverse in their\nmathematical formulation, but common in their application to model semantic\ninferences when machines process text. These tools are principally expressed in\nnine distinct models that capture aspects of semantic dependence in highly\ninterpretable and non-complex ways. This dissertation further reflects on\npresent and future problems with the current research paradigm in this area,\nand makes recommendations on how to overcome them.\n  The amalgamation of the body of work presented in this dissertation advances\nthe complexity and granularity of semantic inferences that can be made\nautomatically by machines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:54:02 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lioma", "Christina", ""]]}, {"id": "1709.03756", "submitter": "Yan Shao", "authors": "Yan Shao", "title": "Cross-lingual Word Segmentation and Morpheme Segmentation as Sequence\n  Labelling", "comments": "6 pages, presented at the First Workshop on Multi-Language Processing\n  in a Globalising World (MLP 2017), Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our segmentation system developed for the MLP 2017 shared\ntasks on cross-lingual word segmentation and morpheme segmentation. We model\nboth word and morpheme segmentation as character-level sequence labelling\ntasks. The prevalent bidirectional recurrent neural network with conditional\nrandom fields as the output interface is adapted as the baseline system, which\nis further improved via ensemble decoding. Our universal system is applied to\nand extensively evaluated on all the official data sets without any\nlanguage-specific adjustment. The official evaluation results indicate that the\nproposed model achieves outstanding accuracies both for word and morpheme\nsegmentation on all the languages in various types when compared to the other\nparticipating systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 09:23:55 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Shao", "Yan", ""]]}, {"id": "1709.03759", "submitter": "Lyan Verwimp", "authors": "Lyan Verwimp, Joris Pelemans, Marieke Lycke, Hugo Van hamme, Patrick\n  Wambacq", "title": "Language Models of Spoken Dutch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Flanders, all TV shows are subtitled. However, the process of subtitling\nis a very time-consuming one and can be sped up by providing the output of a\nspeech recognizer run on the audio of the TV show, prior to the subtitling.\nNaturally, this speech recognition will perform much better if the employed\nlanguage model is adapted to the register and the topic of the program. We\npresent several language models trained on subtitles of television shows\nprovided by the Flemish public-service broadcaster VRT. This data was gathered\nin the context of the project STON which has as purpose to facilitate the\nprocess of subtitling TV shows. One model is trained on all available data (46M\nword tokens), but we also trained models on a specific type of TV show or\ndomain/topic. Language models of spoken language are quite rare due to the lack\nof training data. The size of this corpus is relatively large for a corpus of\nspoken language (compare with e.g. CGN which has 9M words), but still rather\nsmall for a language model. Thus, in practice it is advised to interpolate\nthese models with a large background language model trained on written\nlanguage. The models can be freely downloaded on\nhttp://www.esat.kuleuven.be/psi/spraak/downloads/.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 09:27:12 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Verwimp", "Lyan", ""], ["Pelemans", "Joris", ""], ["Lycke", "Marieke", ""], ["Van hamme", "Hugo", ""], ["Wambacq", "Patrick", ""]]}, {"id": "1709.03814", "submitter": "Josep Crego", "authors": "Yongchao Deng, Jungi Kim, Guillaume Klein, Catherine Kobus, Natalia\n  Segal, Christophe Servan, Bo Wang, Dakun Zhang, Josep Crego, Jean Senellart", "title": "SYSTRAN Purely Neural MT Engines for WMT2017", "comments": "Published in WMT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news\ntranslation task for English-German, in both translation directions. Our\nsystems are built using OpenNMT, an open-source neural machine translation\nsystem, implementing sequence-to-sequence models with LSTM encoder/decoders and\nattention. We experimented using monolingual data automatically\nback-translated. Our resulting models are further hyper-specialised with an\nadaptation technique that finely tunes models according to the evaluation test\nsentences.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 12:47:05 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Deng", "Yongchao", ""], ["Kim", "Jungi", ""], ["Klein", "Guillaume", ""], ["Kobus", "Catherine", ""], ["Segal", "Natalia", ""], ["Servan", "Christophe", ""], ["Wang", "Bo", ""], ["Zhang", "Dakun", ""], ["Crego", "Josep", ""], ["Senellart", "Jean", ""]]}, {"id": "1709.03815", "submitter": "Josep Crego", "authors": "Guillaume Klein, Yoon Kim, Yuntian Deng, Josep Crego, Jean Senellart,\n  Alexander M. Rush", "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation", "comments": "Published in EAMT 2017 User Studies and Project/Product Descriptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an open-source toolkit for neural machine translation (NMT) to\nsupport research into model architectures, feature representations, and source\nmodalities, while maintaining competitive performance, modularity and\nreasonable training requirements.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 12:58:07 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Klein", "Guillaume", ""], ["Kim", "Yoon", ""], ["Deng", "Yuntian", ""], ["Crego", "Josep", ""], ["Senellart", "Jean", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1709.03856", "submitter": "Jason  Weston", "authors": "Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes and\n  Jason Weston", "title": "StarSpace: Embed All The Things!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present StarSpace, a general-purpose neural embedding model that can solve\na wide variety of problems: labeling tasks such as text classification, ranking\ntasks such as information retrieval/web search, collaborative filtering-based\nor content-based recommendation, embedding of multi-relational graphs, and\nlearning word, sentence or document level embeddings. In each case the model\nworks by embedding those entities comprised of discrete features and comparing\nthem against each other -- learning similarities dependent on the task.\nEmpirical results on a number of tasks show that StarSpace is highly\ncompetitive with existing methods, whilst also being generally applicable to\nnew cases where those methods are not.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 14:16:56 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 12:19:23 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 13:06:43 GMT"}, {"version": "v4", "created": "Tue, 26 Sep 2017 15:00:56 GMT"}, {"version": "v5", "created": "Tue, 21 Nov 2017 02:59:57 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wu", "Ledell", ""], ["Fisch", "Adam", ""], ["Chopra", "Sumit", ""], ["Adams", "Keith", ""], ["Bordes", "Antoine", ""], ["Weston", "Jason", ""]]}, {"id": "1709.03925", "submitter": "Natalia Loukachevitch", "authors": "Natalia Loukachevitch, Anastasia Gerasimova", "title": "Human Associations Help to Detect Conventionalized Multiword Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that if we want to obtain human evidence about\nconventionalization of some phrases, we should ask native speakers about\nassociations they have to a given phrase and its component words. We have shown\nthat if component words of a phrase have each other as frequent associations,\nthen this phrase can be considered as conventionalized. Another type of\nconventionalized phrases can be revealed using two factors: low entropy of\nphrase associations and low intersection of component word and phrase\nassociations. The association experiments were performed for the Russian\nlanguage.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:57:22 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Loukachevitch", "Natalia", ""], ["Gerasimova", "Anastasia", ""]]}, {"id": "1709.03933", "submitter": "Dan Svenstrup", "authors": "Dan Svenstrup, Jonas Meinertz Hansen and Ole Winther", "title": "Hash Embeddings for Efficient Word Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight\nvector. The final $d$ dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of $B$ embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 16:13:10 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Svenstrup", "Dan", ""], ["Hansen", "Jonas Meinertz", ""], ["Winther", "Ole", ""]]}, {"id": "1709.03968", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar, Pascal Poupart, Jesse Hoey, Xin Jiang, Lili Mou", "title": "Affective Neural Response Generation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural conversational models process natural language primarily on a\nlexico-syntactic level, thereby ignoring one of the most crucial components of\nhuman-to-human dialogue: its affective content. We take a step in this\ndirection by proposing three novel ways to incorporate affective/emotional\naspects into long short term memory (LSTM) encoder-decoder neural conversation\nmodels: (1) affective word embeddings, which are cognitively engineered, (2)\naffect-based objective functions that augment the standard cross-entropy loss,\nand (3) affectively diverse beam search for decoding. Experiments show that\nthese techniques improve the open-domain conversational prowess of\nencoder-decoder networks by enabling them to produce emotionally rich responses\nthat are more interesting and natural.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 17:41:30 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Asghar", "Nabiha", ""], ["Poupart", "Pascal", ""], ["Hoey", "Jesse", ""], ["Jiang", "Xin", ""], ["Mou", "Lili", ""]]}, {"id": "1709.03980", "submitter": "Wen Zhang", "authors": "Wen Zhang, Jiawei Hu, Yang Feng, Qun Liu", "title": "Refining Source Representations with Relation Networks for Neural\n  Machine Translation", "comments": "i am planned to improve my experiments and modified our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural machine translation (NMT) with the encoder-decoder framework\nhas achieved great success in recent times, it still suffers from some\ndrawbacks: RNNs tend to forget old information which is often useful and the\nencoder only operates through words without considering word relationship. To\nsolve these problems, we introduce a relation networks (RN) into NMT to refine\nthe encoding representations of the source. In our method, the RN first\naugments the representation of each source word with its neighbors and reasons\nall the possible pairwise relations between them. Then the source\nrepresentations and all the relations are fed to the attention module and the\ndecoder together, keeping the main encoder-decoder architecture unchanged.\nExperiments on two Chinese-to-English data sets in different scales both show\nthat our method can outperform the competitive baselines significantly.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 13:38:11 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:20:13 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 13:36:08 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Zhang", "Wen", ""], ["Hu", "Jiawei", ""], ["Feng", "Yang", ""], ["Liu", "Qun", ""]]}, {"id": "1709.04005", "submitter": "Rui Zhang", "authors": "Rui Zhang, Honglak Lee, Lazaros Polymenakos, Dragomir Radev", "title": "Addressee and Response Selection in Multi-Party Conversations with\n  Speaker Interaction RNNs", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of addressee and response selection in\nmulti-party conversations. Understanding multi-party conversations is\nchallenging because of complex speaker interactions: multiple speakers exchange\nmessages with each other, playing different roles (sender, addressee,\nobserver), and these roles vary across turns. To tackle this challenge, we\npropose the Speaker Interaction Recurrent Neural Network (SI-RNN). Whereas the\nprevious state-of-the-art system updated speaker embeddings only for the\nsender, SI-RNN uses a novel dialog encoder to update speaker embeddings in a\nrole-sensitive way. Additionally, unlike the previous work that selected the\naddressee and response separately, SI-RNN selects them jointly by viewing the\ntask as a sequence prediction problem. Experimental results show that SI-RNN\nsignificantly improves the accuracy of addressee and response selection,\nparticularly in complex conversations with many speakers and responses to\ndistant messages many turns in the past.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 18:19:51 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 16:40:57 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Zhang", "Rui", ""], ["Lee", "Honglak", ""], ["Polymenakos", "Lazaros", ""], ["Radev", "Dragomir", ""]]}, {"id": "1709.04071", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, Le Song", "title": "Variational Reasoning for Question Answering with Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph (KG) is known to be helpful for the task of question\nanswering (QA), since it provides well-structured relational information\nbetween entities, and allows one to further infer indirect facts. However, it\nis challenging to build QA systems which can learn to reason over knowledge\ngraphs based on question-answer pairs alone. First, when people ask questions,\ntheir expressions are noisy (for example, typos in texts, or variations in\npronunciations), which is non-trivial for the QA system to match those\nmentioned entities to the knowledge graph. Second, many questions require\nmulti-hop logic reasoning over the knowledge graph to retrieve the answers. To\naddress these challenges, we propose a novel and unified deep learning\narchitecture, and an end-to-end variational learning algorithm which can handle\nnoise in questions, and learn multi-hop reasoning simultaneously. Our method\nachieves state-of-the-art performance on a recent benchmark dataset in the\nliterature. We also derive a series of new benchmark datasets, including\nquestions for multi-hop reasoning, questions paraphrased by neural translation\nmodel, and questions in human voice. Our method yields very promising results\non all these challenging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 22:27:34 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 06:24:24 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 23:18:47 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 00:33:53 GMT"}, {"version": "v5", "created": "Mon, 27 Nov 2017 21:58:40 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Zhang", "Yuyu", ""], ["Dai", "Hanjun", ""], ["Kozareva", "Zornitsa", ""], ["Smola", "Alexander J.", ""], ["Song", "Le", ""]]}, {"id": "1709.04109", "submitter": "Liyuan Liu", "authors": "Liyuan Liu, Jingbo Shang, Frank F. Xu, Xiang Ren, Huan Gui, Jian Peng,\n  Jiawei Han", "title": "Empower Sequence Labeling with Task-Aware Neural Language Model", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic sequence labeling is a general modeling approach that encompasses\na variety of problems, such as part-of-speech tagging and named entity\nrecognition. Recent advances in neural networks (NNs) make it possible to build\nreliable models without handcrafted features. However, in many cases, it is\nhard to obtain sufficient annotations to train these models. In this study, we\ndevelop a novel neural framework to extract abundant knowledge hidden in raw\ntexts to empower the sequence labeling task. Besides word-level knowledge\ncontained in pre-trained word embeddings, character-aware neural language\nmodels are incorporated to extract character-level knowledge. Transfer learning\ntechniques are further adopted to mediate different components and guide the\nlanguage model towards the key knowledge. Comparing to previous methods, these\ntask-specific knowledge allows us to adopt a more concise model and conduct\nmore efficient training. Different from most transfer learning methods, the\nproposed framework does not rely on any additional supervision. It extracts\nknowledge from self-contained order information of training sequences.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nleveraging character-level knowledge and the efficiency of co-training. For\nexample, on the CoNLL03 NER task, model training completes in about 6 hours on\na single GPU, reaching F1 score of 91.71$\\pm$0.10 without using any extra\nannotation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 02:13:25 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 03:39:38 GMT"}, {"version": "v3", "created": "Fri, 15 Sep 2017 06:51:06 GMT"}, {"version": "v4", "created": "Thu, 23 Nov 2017 23:12:40 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Liu", "Liyuan", ""], ["Shang", "Jingbo", ""], ["Xu", "Frank F.", ""], ["Ren", "Xiang", ""], ["Gui", "Huan", ""], ["Peng", "Jian", ""], ["Han", "Jiawei", ""]]}, {"id": "1709.04219", "submitter": "Jeremy Barnes", "authors": "Jeremy Barnes, Roman Klinger and Sabine Schulte im Walde", "title": "Assessing State-of-the-Art Sentiment Models on State-of-the-Art\n  Sentiment Datasets", "comments": "Presented at WASSA 2017", "journal-ref": "In Proceedings of WASSA (2017). 2 - 12", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a good amount of progress in sentiment analysis over the past\n10 years, including the proposal of new methods and the creation of benchmark\ndatasets. In some papers, however, there is a tendency to compare models only\non one or two datasets, either because of time restraints or because the model\nis tailored to a specific task. Accordingly, it is hard to understand how well\na certain model generalizes across different tasks and datasets. In this paper,\nwe contribute to this situation by comparing several models on six different\nbenchmarks, which belong to different domains and additionally have different\nlevels of granularity (binary, 3-class, 4-class and 5-class). We show that\nBi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are\nparticularly good at fine-grained sentiment tasks (i. e., with more than two\nclasses). Incorporating sentiment information into word embeddings during\ntraining gives good results for datasets that are lexically similar to the\ntraining data. With our experiments, we contribute to a better understanding of\nthe performance of different model architectures on different data sets.\nConsequently, we detect novel state-of-the-art results on the SenTube datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 09:43:02 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Barnes", "Jeremy", ""], ["Klinger", "Roman", ""], ["Walde", "Sabine Schulte im", ""]]}, {"id": "1709.04250", "submitter": "Harshit Kumar", "authors": "Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta, Sachindra Joshi,\n  Arun Kumar", "title": "Dialogue Act Sequence Labeling using Hierarchical encoder with CRF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue Act recognition associate dialogue acts (i.e., semantic labels) to\nutterances in a conversation. The problem of associating semantic labels to\nutterances can be treated as a sequence labeling problem. In this work, we\nbuild a hierarchical recurrent neural network using bidirectional LSTM as a\nbase unit and the conditional random field (CRF) as the top layer to classify\neach utterance into its corresponding dialogue act. The hierarchical network\nlearns representations at multiple levels, i.e., word level, utterance level,\nand conversation level. The conversation level representations are input to the\nCRF layer, which takes into account not only all previous utterances but also\ntheir dialogue acts, thus modeling the dependency among both, labels and\nutterances, an important consideration of natural dialogue. We validate our\napproach on two different benchmark data sets, Switchboard and Meeting Recorder\nDialogue Act, and show performance improvement over the state-of-the-art\nmethods by $2.2\\%$ and $4.1\\%$ absolute points, respectively. It is worth\nnoting that the inter-annotator agreement on Switchboard data set is $84\\%$,\nand our method is able to achieve the accuracy of about $79\\%$ despite being\ntrained on the noisy data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 11:01:02 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 13:49:10 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kumar", "Harshit", ""], ["Agarwal", "Arvind", ""], ["Dasgupta", "Riddhiman", ""], ["Joshi", "Sachindra", ""], ["Kumar", "Arun", ""]]}, {"id": "1709.04264", "submitter": "Wenya Zhu", "authors": "Wenya Zhu, Kaixiang Mo, Yu Zhang, Zhangbin Zhu, Xuezheng Peng, Qiang\n  Yang", "title": "Flexible End-to-End Dialogue System for Knowledge Grounded Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In knowledge grounded conversation, domain knowledge plays an important role\nin a special domain such as Music. The response of knowledge grounded\nconversation might contain multiple answer entities or no entity at all.\nAlthough existing generative question answering (QA) systems can be applied to\nknowledge grounded conversation, they either have at most one entity in a\nresponse or cannot deal with out-of-vocabulary entities. We propose a fully\ndata-driven generative dialogue system GenDS that is capable of generating\nresponses based on input message and related knowledge base (KB). To generate\narbitrary number of answer entities even when these entities never appear in\nthe training set, we design a dynamic knowledge enquirer which selects\ndifferent answer entities at different positions in a single response,\naccording to different local context. It does not rely on the representations\nof entities, enabling our model deal with out-of-vocabulary entities. We\ncollect a human-human conversation data (ConversMusic) with knowledge\nannotations. The proposed method is evaluated on CoversMusic and a public\nquestion answering dataset. Our proposed GenDS system outperforms baseline\nmethods significantly in terms of the BLEU, entity accuracy, entity recall and\nhuman evaluation. Moreover,the experiments also demonstrate that GenDS works\nbetter even on small datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 11:59:06 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zhu", "Wenya", ""], ["Mo", "Kaixiang", ""], ["Zhang", "Yu", ""], ["Zhu", "Zhangbin", ""], ["Peng", "Xuezheng", ""], ["Yang", "Qiang", ""]]}, {"id": "1709.04348", "submitter": "Yichen Gong", "authors": "Yichen Gong, Heng Luo and Jian Zhang", "title": "Natural Language Inference over Interaction Space", "comments": "15 pages, 2 figures, under review as ICLR proceeding, Published at\n  Sixth International Conference on Learning Representations, ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It's noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:22:14 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 20:55:39 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Gong", "Yichen", ""], ["Luo", "Heng", ""], ["Zhang", "Jian", ""]]}, {"id": "1709.04359", "submitter": "Marcos Zampieri", "authors": "Ekaterina Lapshninova-Koltunski, Marcos Zampieri", "title": "Linguistic Features of Genre and Method Variation in Translation: A\n  Computational Perspective", "comments": "To appear as a book chapter in Grammar of Genres and Styles. De\n  Gruyter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the use of text classification methods to\ninvestigate genre and method variation in an English - German translation\ncorpus. For this purpose we use linguistically motivated features representing\ntexts using a combination of part-of-speech tags arranged in bigrams, trigrams,\nand 4-grams. The classification method used in this paper is a Bayesian\nclassifier with Laplace smoothing. We use the output of the classifiers to\ncarry out an extensive feature analysis on the main difference between genres\nand methods of translation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:46:50 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Lapshninova-Koltunski", "Ekaterina", ""], ["Zampieri", "Marcos", ""]]}, {"id": "1709.04380", "submitter": "Tianyu Li", "authors": "Tianyu Li, Guillaume Rabusseau, Doina Precup", "title": "Neural Network Based Nonlinear Weighted Finite Automata", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weighted finite automata (WFA) can expressively model functions defined over\nstrings but are inherently linear models. Given the recent successes of\nnonlinear models in machine learning, it is natural to wonder whether\nex-tending WFA to the nonlinear setting would be beneficial. In this paper, we\npropose a novel model of neural network based nonlinearWFA model (NL-WFA) along\nwith a learning algorithm. Our learning algorithm is inspired by the spectral\nlearning algorithm for WFAand relies on a nonlinear decomposition of the\nso-called Hankel matrix, by means of an auto-encoder network. The expressive\npower of NL-WFA and the proposed learning algorithm are assessed on both\nsynthetic and real-world data, showing that NL-WFA can lead to smaller model\nsizes and infer complex grammatical structures from data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 15:26:50 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 13:42:43 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Li", "Tianyu", ""], ["Rabusseau", "Guillaume", ""], ["Precup", "Doina", ""]]}, {"id": "1709.04409", "submitter": "Amanda Cercas Curry", "authors": "Amanda Cercas Curry, Helen Hastie and Verena Rieser", "title": "A Review of Evaluation Techniques for Social Dialogue Systems", "comments": "2 pages", "journal-ref": null, "doi": "10.1145/3139491.3139504", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast with goal-oriented dialogue, social dialogue has no clear measure\nof task success. Consequently, evaluation of these systems is notoriously hard.\nIn this paper, we review current evaluation methods, focusing on automatic\nmetrics. We conclude that turn-based metrics often ignore the context and do\nnot account for the fact that several replies are valid, while end-of-dialogue\nrewards are mainly hand-crafted. Both lack grounding in human perceptions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:31:44 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Curry", "Amanda Cercas", ""], ["Hastie", "Helen", ""], ["Rieser", "Verena", ""]]}, {"id": "1709.04482", "submitter": "Yonatan Belinkov", "authors": "Yonatan Belinkov, James Glass", "title": "Analyzing Hidden Representations in End-to-End Automatic Speech\n  Recognition Systems", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have become ubiquitous in automatic speech recognition systems.\nWhile neural networks are typically used as acoustic models in more complex\nsystems, recent studies have explored end-to-end speech recognition systems\nbased on neural networks, which can be trained to directly predict text from\ninput acoustic features. Although such systems are conceptually elegant and\nsimpler than traditional systems, it is less obvious how to interpret the\ntrained models. In this work, we analyze the speech representations learned by\na deep end-to-end model that is based on convolutional and recurrent layers,\nand trained with a connectionist temporal classification (CTC) loss. We use a\npre-trained model to generate frame-level features which are given to a\nclassifier that is trained on frame classification into phones. We evaluate\nrepresentations from different layers of the deep model and compare their\nquality for predicting phone labels. Our experiments shed light on important\naspects of the end-to-end model such as layer depth, model complexity, and\nother design choices.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 18:02:53 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Belinkov", "Yonatan", ""], ["Glass", "James", ""]]}, {"id": "1709.04491", "submitter": "Lukasz Augustyniak", "authors": "{\\L}ukasz Augustyniak, Krzysztof Rajda, Tomasz Kajdanowicz", "title": "Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-54472-4_72", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper fills a gap in aspect-based sentiment analysis and aims to present\na new method for preparing and analysing texts concerning opinion and\ngenerating user-friendly descriptive reports in natural language. We present a\ncomprehensive set of techniques derived from Rhetorical Structure Theory and\nsentiment analysis to extract aspects from textual opinions and then build an\nabstractive summary of a set of opinions. Moreover, we propose aspect-aspect\ngraphs to evaluate the importance of aspects and to filter out unimportant ones\nfrom the summary. Additionally, the paper presents a prototype solution of data\nflow with interesting and valuable results. The proposed method's results\nproved the high accuracy of aspect detection when applied to the gold standard\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 18:17:56 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Augustyniak", "\u0141ukasz", ""], ["Rajda", "Krzysztof", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1709.04558", "submitter": "John Ball", "authors": "John S. Ball", "title": "Using NLU in Context for Question Answering: Improving on Facebook's\n  bAbI Tasks", "comments": "38 Pages, 10 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the next step in human to machine interaction, Artificial Intelligence\n(AI) should interact predominantly using natural language because, if it\nworked, it would be the fastest way to communicate. Facebook's toy tasks (bAbI)\nprovide a useful benchmark to compare implementations for conversational AI.\nWhile the published experiments so far have been based on exploiting the\ndistributional hypothesis with machine learning, our model exploits natural\nlanguage understanding (NLU) with the decomposition of language based on Role\nand Reference Grammar (RRG) and the brain-based Patom theory. Our combinatorial\nsystem for conversational AI based on linguistics has many advantages: passing\nbAbI task tests without parsing or statistics while increasing scalability. Our\nmodel validates both the training and test data to find 'garbage' input and\noutput (GIGO). It is not rules-based, nor does it use parts of speech, but\ninstead relies on meaning. While Deep Learning is difficult to debug and fix,\nevery step in our model can be understood and changed like any non-statistical\ncomputer program. Deep Learning's lack of explicable reasoning has raised\nopposition to AI, partly due to fear of the unknown. To support the goals of\nAI, we propose extended tasks to use human-level statements with tense, aspect\nand voice, and embedded clauses with junctures: and answers to be natural\nlanguage generation (NLG) instead of keywords. While machine learning permits\ninvalid training data to produce incorrect test responses, our system cannot\nbecause the context tracking would need to be intentionally broken. We believe\nno existing learning systems can currently solve these extended natural\nlanguage tests. There appears to be a knowledge gap between NLP researchers and\nlinguists, but ongoing competitive results such as these promise to narrow that\ngap.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 22:48:39 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 01:19:52 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ball", "John S.", ""]]}, {"id": "1709.04625", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Cuong Duc Dao, Modar Alfadly, C. Huck Yang, Bernard\n  Ghanem", "title": "Robustness Analysis of Visual QA Models by Basic Questions", "comments": "Accepted by CVPR 2018 VQA Challenge and Visual Dialog Workshop.\n  (Acknowledgement updating)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) models should have both high robustness and\naccuracy. Unfortunately, most of the current VQA research only focuses on\naccuracy because there is a lack of proper methods to measure the robustness of\nVQA models. There are two main modules in our algorithm. Given a natural\nlanguage question about an image, the first module takes the question as input\nand then outputs the ranked basic questions, with similarity scores, of the\nmain given question. The second module takes the main question, image and these\nbasic questions as input and then outputs the text-based answer of the main\nquestion about the given image. We claim that a robust VQA model is one, whose\nperformance is not changed much when related basic questions as also made\navailable to it as input. We formulate the basic questions generation problem\nas a LASSO optimization, and also propose a large scale Basic Question Dataset\n(BQD) and Rscore (novel robustness measure), for analyzing the robustness of\nVQA models. We hope our BQD will be used as a benchmark for to evaluate the\nrobustness of VQA models, so as to help the community build more robust and\naccurate VQA models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 06:11:09 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 06:56:47 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 05:14:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Dao", "Cuong Duc", ""], ["Alfadly", "Modar", ""], ["Yang", "C. Huck", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1709.04682", "submitter": "Neama Abdulaziz Ms.", "authors": "Neama Abdulaziz Dahan, Fadl Mutaher Ba-Alwi, Ibrahim Ahmed Al-Baltah,\n  Ghaleb H. Al-gapheri", "title": "Towards an Arabic-English Machine-Translation Based on Semantic Web", "comments": "6 pages, 4 figures, Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication tools make the world like a small village and as a consequence\npeople can contact with others who are from different societies or who speak\ndifferent languages. This communication cannot happen effectively without\nMachine Translation because they can be found anytime and everywhere. There are\na number of studies that have developed Machine Translation for the English\nlanguage with so many other languages except the Arabic it has not been\nconsidered yet. Therefore we aim to highlight a roadmap for our proposed\ntranslation machine to provide an enhanced Arabic English translation based on\nSemantic.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 09:36:48 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Dahan", "Neama Abdulaziz", ""], ["Ba-Alwi", "Fadl Mutaher", ""], ["Al-Baltah", "Ibrahim Ahmed", ""], ["Al-gapheri", "Ghaleb H.", ""]]}, {"id": "1709.04685", "submitter": "Neama Abdulaziz Ms.", "authors": "Nabeel T. Alsohybe, Neama Abdulaziz Dahan, Fadl Mutaher Ba-Alwi", "title": "Machine-Translation History and Evolution: Survey for Arabic-English\n  Translations", "comments": "19 pages, 5 figures, 3 tables, survey article paper", "journal-ref": "CJAST 23(4): 2017", "doi": "10.9734/CJAST/2017/36124", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a result of the rapid changes in information and communication technology\n(ICT), the world has become a small village where people from all over the\nworld connect with each other in dialogue and communication via the Internet.\nAlso, communications have become a daily routine activity due to the new\nglobalization where companies and even universities become global residing\ncross countries borders. As a result, translation becomes a needed activity in\nthis connected world. ICT made it possible to have a student in one country\ntake a course or even a degree from a different country anytime anywhere\neasily. The resulted communication still needs a language as a means that helps\nthe receiver understands the contents of the sent message. People need an\nautomated translation application because human translators are hard to find\nall the times, and the human translations are very expensive comparing to the\ntranslations automated process. Several types of research describe the\nelectronic process of the Machine-Translation. In this paper, the authors are\ngoing to study some of these previous researches, and they will explore some of\nthe needed tools for the Machine-Translation. This research is going to\ncontribute to the Machine-Translation area by helping future researchers to\nhave a summary for the Machine-Translation groups of research and to let lights\non the importance of the translation mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 09:46:15 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Alsohybe", "Nabeel T.", ""], ["Dahan", "Neama Abdulaziz", ""], ["Ba-Alwi", "Fadl Mutaher", ""]]}, {"id": "1709.04696", "submitter": "Tao Shen", "authors": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan and\n  Chengqi Zhang", "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language\n  Understanding", "comments": "10 pages, 8 figures; Accepted in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely\nused on NLP tasks to capture the long-term and local dependencies,\nrespectively. Attention mechanisms have recently attracted enormous interest\ndue to their highly parallelizable computation, significantly less training\ntime, and flexibility in modeling dependencies. We propose a novel attention\nmechanism in which the attention between elements from input sequence(s) is\ndirectional and multi-dimensional (i.e., feature-wise). A light-weight neural\nnet, \"Directional Self-Attention Network (DiSAN)\", is then proposed to learn\nsentence embedding, based solely on the proposed attention without any RNN/CNN\nstructure. DiSAN is only composed of a directional self-attention with temporal\norder encoded, followed by a multi-dimensional attention that compresses the\nsequence into a vector representation. Despite its simple form, DiSAN\noutperforms complicated RNN models on both prediction quality and time\nefficiency. It achieves the best test accuracy among all sentence encoding\nmethods and improves the most recent best result by 1.02% on the Stanford\nNatural Language Inference (SNLI) dataset, and shows state-of-the-art test\naccuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language\ninference (MultiNLI), Sentences Involving Compositional Knowledge (SICK),\nCustomer Review, MPQA, TREC question-type classification and Subjectivity\n(SUBJ) datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 10:42:44 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 02:53:35 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 23:39:11 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Shen", "Tao", ""], ["Zhou", "Tianyi", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Pan", "Shirui", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1709.04710", "submitter": "Atsushi Yokoyama", "authors": "Atsushi Yokoyama", "title": "Embedded-Graph Theory", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new type of graph, denoted as \"embedded-graph\",\nand its theory, which employs a distributed representation to describe the\nrelations on the graph edges. Embedded-graphs can express linguistic and\ncomplicated relations, which cannot be expressed by the existing edge-graphs or\nweighted-graphs. We introduce the mathematical definition of embedded-graph,\ntranslation, edge distance, and graph similarity. We can transform an\nembedded-graph into a weighted-graph and a weighted-graph into an edge-graph by\nthe translation method and by threshold calculation, respectively. The edge\ndistance of an embedded-graph is a distance based on the components of a target\nvector, and it is calculated through cosine similarity with the target vector.\nThe graph similarity is obtained considering the relations with linguistic\ncomplexity. In addition, we provide some examples and data structures for\nembedded-graphs in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:31:19 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Yokoyama", "Atsushi", ""]]}, {"id": "1709.04820", "submitter": "Damien Sileo", "authors": "Damien Sileo, Camille Pradel, Philippe Muller, Tim Van de Cruys", "title": "Synapse at CAp 2017 NER challenge: Fasttext CRF", "comments": null, "journal-ref": "CAP2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our system for the CAp 2017 NER challenge which is about named\nentity recognition on French tweets. Our system leverages unsupervised learning\non a larger dataset of French tweets to learn features feeding a CRF model. It\nwas ranked first without using any gazetteer or structured external data, with\nan F-measure of 58.89\\%. To the best of our knowledge, it is the first system\nto use fasttext embeddings (which include subword representations) and an\nembedding-based sentence representation for NER.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 14:44:30 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Sileo", "Damien", ""], ["Pradel", "Camille", ""], ["Muller", "Philippe", ""], ["Van de Cruys", "Tim", ""]]}, {"id": "1709.04849", "submitter": "Lesly Miculicich Werlen", "authors": "Lesly Miculicich Werlen, Nikolaos Pappas, Dhananjay Ram and Andrei\n  Popescu-Belis", "title": "Self-Attentive Residual Decoder for Neural Machine Translation", "comments": "Accepted on NAACL-HLT 2018, Volume: Proceedings of the 2018\n  Conference of the North American Chapter of the Association for Computational\n  Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "journal-ref": null, "doi": "10.18653/v1/N18-1124", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence networks with attention have achieved remarkable\nperformance for machine translation. One of the reasons for their effectiveness\nis their ability to capture relevant source-side contextual information at each\ntime-step prediction through an attention mechanism. However, the target-side\ncontext is solely based on the sequence model which, in practice, is prone to a\nrecency bias and lacks the ability to capture effectively non-sequential\ndependencies among words. To address this limitation, we propose a\ntarget-side-attentive residual recurrent network for decoding, where attention\nover previous words contributes directly to the prediction of the next word.\nThe residual learning facilitates the flow of information from the distant past\nand is able to emphasize any of the previously translated words, hence it gains\naccess to a wider context. The proposed model outperforms a neural MT baseline\nas well as a memory and self-attention network on three language pairs. The\nanalysis of the attention learned by the decoder confirms that it emphasizes a\nwider context, and that it captures syntactic-like structures.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:06:45 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 15:51:21 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 11:19:28 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 16:37:45 GMT"}, {"version": "v5", "created": "Mon, 1 Oct 2018 09:16:57 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Werlen", "Lesly Miculicich", ""], ["Pappas", "Nikolaos", ""], ["Ram", "Dhananjay", ""], ["Popescu-Belis", "Andrei", ""]]}, {"id": "1709.04857", "submitter": "Kun Xing", "authors": "Kun Xing", "title": "A New Semantic Theory of Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Semantics and Distributional Semantics are two important semantic\nframeworks in Natural Language Processing (NLP). Cognitive Semantics belongs to\nthe movement of Cognitive Linguistics, which is based on contemporary cognitive\nscience. Each framework could deal with some meaning phenomena, but none of\nthem fulfills all requirements proposed by applications. A unified semantic\ntheory characterizing all important language phenomena has both theoretical and\npractical significance; however, although many attempts have been made in\nrecent years, no existing theory has achieved this goal yet.\n  This article introduces a new semantic theory that has the potential to\ncharacterize most of the important meaning phenomena of natural language and to\nfulfill most of the necessary requirements for philosophical analysis and for\nNLP applications. The theory is based on a unified representation of\ninformation, and constructs a kind of mathematical model called cognitive model\nto interpret natural language expressions in a compositional manner. It accepts\nthe empirical assumption of Cognitive Semantics, and overcomes most\nshortcomings of Formal Semantics and of Distributional Semantics. The theory,\nhowever, is not a simple combination of existing theories, but an extensive\ngeneralization of classic logic and Formal Semantics. It inherits nearly all\nadvantages of Formal Semantics, and also provides descriptive contents for\nobjects and events as fine-gram as possible, descriptive contents which\nrepresent the results of human cognition.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 13:18:58 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Xing", "Kun", ""]]}, {"id": "1709.04969", "submitter": "Fred Morstatter", "authors": "Fred Morstatter, Kai Shu, Suhang Wang, and Huan Liu", "title": "Cross-Platform Emoji Interpretation: Analysis, a Solution, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most social media platforms are largely based on text, and users often write\nposts to describe where they are, what they are seeing, and how they are\nfeeling. Because written text lacks the emotional cues of spoken and\nface-to-face dialogue, ambiguities are common in written language. This problem\nis exacerbated in the short, informal nature of many social media posts. To\nbypass this issue, a suite of special characters called \"emojis,\" which are\nsmall pictograms, are embedded within the text. Many emojis are small\ndepictions of facial expressions designed to help disambiguate the emotional\nmeaning of the text. However, a new ambiguity arises in the way that emojis are\nrendered. Every platform (Windows, Mac, and Android, to name a few) renders\nemojis according to their own style. In fact, it has been shown that some\nemojis can be rendered so differently that they look \"happy\" on some platforms,\nand \"sad\" on others. In this work, we use real-world data to verify the\nexistence of this problem. We verify that the usage of the same emoji can be\nsignificantly different across platforms, with some emojis exhibiting different\nsentiment polarities on different platforms. We propose a solution to identify\nthe intended emoji based on the platform-specific nature of the emoji used by\nthe author of a social media post. We apply our solution to sentiment analysis,\na task that can benefit from the emoji calibration technique we use in this\nwork. We conduct experiments to evaluate the effectiveness of the mapping in\nthis task.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 20:28:27 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Morstatter", "Fred", ""], ["Shu", "Kai", ""], ["Wang", "Suhang", ""], ["Liu", "Huan", ""]]}, {"id": "1709.05014", "submitter": "Gonzalo Estr\\'an Buyo", "authors": "Gonzalo Estr\\'an Buyo", "title": "WOAH: Preliminaries to Zero-shot Ontology Learning for Conversational\n  Agents", "comments": "ICTIR' 17 Workshop on Search-Oriented Conversational AI (SCAI' 2017),\n  5 pages, LaTeX; typo corrected in the diagram", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper presents the Weighted Ontology Approximation Heuristic\n(WOAH), a novel zero-shot approach to ontology estimation for conversational\nagents development environments. This methodology extracts verbs and nouns\nseparately from data by distilling the dependencies obtained and applying\nsimilarity and sparsity metrics to generate an ontology estimation configurable\nin terms of the level of generalization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 00:12:08 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 22:33:18 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Buyo", "Gonzalo Estr\u00e1n", ""]]}, {"id": "1709.05027", "submitter": "Wei Wen", "authors": "Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang,\n  Fang Liu, Bin Hu, Yiran Chen, Hai Li", "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory", "comments": "Published in ICLR 2018 ( the Sixth International Conference on\n  Learning Representations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression is significant for the wide adoption of Recurrent Neural\nNetworks (RNNs) in both user devices possessing limited resources and business\nclusters requiring quick responses to large-scale service requests. This work\naims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the\nsizes of basic structures within LSTM units, including input updates, gates,\nhidden states, cell states and outputs. Independently reducing the sizes of\nbasic structures can result in inconsistent dimensions among them, and\nconsequently, end up with invalid LSTM units. To overcome the problem, we\npropose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS\nwill simultaneously decrease the sizes of all basic structures by one and\nthereby always maintain the dimension consistency. By learning ISS within LSTM\nunits, the obtained LSTMs remain regular while having much smaller basic\nstructures. Based on group Lasso regularization, our method achieves 10.59x\nspeedup without losing any perplexity of a language modeling of Penn TreeBank\ndataset. It is also successfully evaluated through a compact model with only\n2.69M weights for machine Question Answering of SQuAD dataset. Our approach is\nsuccessfully extended to non- LSTM RNNs, like Recurrent Highway Networks\n(RHNs). Our source code is publicly available at\nhttps://github.com/wenwei202/iss-rnns\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 01:10:23 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 17:23:05 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 15:49:22 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 19:24:23 GMT"}, {"version": "v5", "created": "Fri, 5 Jan 2018 16:23:10 GMT"}, {"version": "v6", "created": "Tue, 30 Jan 2018 04:42:43 GMT"}, {"version": "v7", "created": "Sun, 11 Feb 2018 16:36:32 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wen", "Wei", ""], ["He", "Yuxiong", ""], ["Rajbhandari", "Samyam", ""], ["Zhang", "Minjia", ""], ["Wang", "Wenhan", ""], ["Liu", "Fang", ""], ["Hu", "Bin", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1709.05036", "submitter": "Yu Hsueh Wu", "authors": "Tzu-Chien Liu, Yu-Hsueh Wu, Hung-Yi Lee", "title": "Query-based Attention CNN for Text Similarity Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Query-based Attention CNN(QACNN) for Text\nSimilarity Map, an end-to-end neural network for question answering. This\nnetwork is composed of compare mechanism, two-staged CNN architecture with\nattention mechanism, and a prediction layer. First, the compare mechanism\ncompares between the given passage, query, and multiple answer choices to build\nsimilarity maps. Then, the two-staged CNN architecture extracts features\nthrough word-level and sentence-level. At the same time, attention mechanism\nhelps CNN focus more on the important part of the passage based on the query\ninformation. Finally, the prediction layer find out the most possible answer\nchoice. We conduct this model on the MovieQA dataset using Plot Synopses only,\nand achieve 79.99% accuracy which is the state of the art on the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 02:25:57 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 14:01:20 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Liu", "Tzu-Chien", ""], ["Wu", "Yu-Hsueh", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1709.05038", "submitter": "Yang Xian", "authors": "Yang Xian, Yingli Tian", "title": "Self-Guiding Multimodal LSTM - when we do not have a perfect training\n  dataset for image captioning", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning\nmodel is proposed to handle uncontrolled imbalanced real-world image-sentence\ndataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165\nimages and the original text descriptions uploaded by the users are utilized as\nthe ground truth for training. Descriptions in FlickrNYC dataset vary\ndramatically ranging from short term-descriptions to long\nparagraph-descriptions and can describe any visual aspects, or even refer to\nobjects that are not depicted. To deal with the imbalanced and noisy situation\nand to fully explore the dataset itself, we propose a novel guiding textual\nfeature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of\nm-LSTM is based on the portion of data in which the image content and the\ncorresponding descriptions are strongly bonded. Afterwards, during the training\nof sg-LSTM on the rest training data, this guiding information serves as\nadditional input to the network along with the image representations and the\nground-truth descriptions. By integrating these input components into a\nmultimodal block, we aim to form a training scheme with the textual information\ntightly coupled with the image content. The experimental results demonstrate\nthat the proposed sg-LSTM model outperforms the traditional state-of-the-art\nmultimodal RNN captioning framework in successfully describing the key\ncomponents of the input images.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 02:53:16 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Xian", "Yang", ""], ["Tian", "Yingli", ""]]}, {"id": "1709.05074", "submitter": "Arvind Agarwal", "authors": "Ankush Gupta, Arvind Agarwal, Prawaan Singh, Piyush Rai", "title": "A Deep Generative Framework for Paraphrase Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paraphrase generation is an important problem in NLP, especially in question\nanswering, information retrieval, information extraction, conversation systems,\nto name a few. In this paper, we address the problem of generating paraphrases\nautomatically. Our proposed method is based on a combination of deep generative\nmodels (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,\ngiven an input sentence. Traditional VAEs when combined with recurrent neural\nnetworks can generate free text but they are not suitable for paraphrase\ngeneration for a given sentence. We address this problem by conditioning the\nboth, encoder and decoder sides of VAE, on the original sentence, so that it\ncan generate the given sentence's paraphrases. Unlike most existing models, our\nmodel is simple, modular and can generate multiple paraphrases, for a given\nsentence. Quantitative evaluation of the proposed method on a benchmark\nparaphrase dataset demonstrates its efficacy, and its performance improvement\nover the state-of-the-art methods by a significant margin, whereas qualitative\nhuman evaluation indicate that the generated paraphrases are well-formed,\ngrammatically correct, and are relevant to the input sentence. Furthermore, we\nevaluate our method on a newly released question paraphrase dataset, and\nestablish a new baseline for future research.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 06:58:13 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Gupta", "Ankush", ""], ["Agarwal", "Arvind", ""], ["Singh", "Prawaan", ""], ["Rai", "Piyush", ""]]}, {"id": "1709.05094", "submitter": "Athanasios Giannakopoulos", "authors": "Athanasios Giannakopoulos, Claudiu Musat, Andreea Hossmann and Michael\n  Baeriswyl", "title": "Unsupervised Aspect Term Extraction with B-LSTM & CRF using\n  Automatically Labelled Datasets", "comments": "9 pages, 3 figures, 2 tables 8th Workshop on Computational Approaches\n  to Subjectivity, Sentiment & Social Media Analysis (WASSA), EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts and\nis one of the tasks in the SemEval Aspect Based Sentiment Analysis (ABSA)\ncontest. The small amount of available datasets for supervised ATE and the\ncostly human annotation for aspect term labelling give rise to the need for\nunsupervised ATE. In this paper, we introduce an architecture that achieves\ntop-ranking performance for supervised ATE. Moreover, it can be used\nefficiently as feature extractor and classifier for unsupervised ATE. Our\nsecond contribution is a method to automatically construct datasets for ATE. We\ntrain a classifier on our automatically labelled datasets and evaluate it on\nthe human annotated SemEval ABSA test sets. Compared to a strong rule-based\nbaseline, we obtain a dramatically higher F-score and attain precision values\nabove 80%. Our unsupervised method beats the supervised ABSA baseline from\nSemEval, while preserving high precision scores.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 08:12:58 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Giannakopoulos", "Athanasios", ""], ["Musat", "Claudiu", ""], ["Hossmann", "Andreea", ""], ["Baeriswyl", "Michael", ""]]}, {"id": "1709.05227", "submitter": "Matthias Sperber", "authors": "Matthias Sperber, Graham Neubig, Jan Niehues, Satoshi Nakamura, Alex\n  Waibel", "title": "Transcribing Against Time", "comments": "Speech Communication, Oct 2017 (preprint)", "journal-ref": null, "doi": "10.1016/j.specom.2017.07.006", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of manually correcting errors from an automatic\nspeech transcript in a cost-sensitive fashion. This is done by specifying a\nfixed time budget, and then automatically choosing location and size of\nsegments for correction such that the number of corrected errors is maximized.\nThe core components, as suggested by previous research [1], are a utility model\nthat estimates the number of errors in a particular segment, and a cost model\nthat estimates annotation effort for the segment. In this work we propose a\ndynamic updating framework that allows for the training of cost models during\nthe ongoing transcription process. This removes the need for transcriber\nenrollment prior to the actual transcription, and improves correction\nefficiency by allowing highly transcriber-adaptive cost modeling. We first\nconfirm and analyze the improvements afforded by this method in a simulated\nstudy. We then conduct a realistic user study, observing efficiency\nimprovements of 15% relative on average, and 42% for the participants who\ndeviated most strongly from our initial, transcriber-agnostic cost model.\nMoreover, we find that our updating framework can capture dynamically changing\nfactors, such as transcriber fatigue and topic familiarity, which we observe to\nhave a large influence on the transcriber's working behavior.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 14:19:29 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Sperber", "Matthias", ""], ["Neubig", "Graham", ""], ["Niehues", "Jan", ""], ["Nakamura", "Satoshi", ""], ["Waibel", "Alex", ""]]}, {"id": "1709.05278", "submitter": "Daoud Clarke", "authors": "Dion Bailey, Tom Pajak, Daoud Clarke, Carlos Rodriguez", "title": "Algorithms and Architecture for Real-time Recommendations at News UK", "comments": "Accepted for presentation at AI-2017 Thirty-seventh SGAI\n  International Conference on Artificial Intelligence. Cambridge, England 12-14\n  December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are recognised as being hugely important in industry,\nand the area is now well understood. At News UK, there is a requirement to be\nable to quickly generate recommendations for users on news items as they are\npublished. However, little has been published about systems that can generate\nrecommendations in response to changes in recommendable items and user\nbehaviour in a very short space of time. In this paper we describe a new\nalgorithm for updating collaborative filtering models incrementally, and\ndemonstrate its effectiveness on clickstream data from The Times. We also\ndescribe the architecture that allows recommendations to be generated on the\nfly, and how we have made each component scalable. The system is currently\nbeing used in production at News UK.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:47:23 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Bailey", "Dion", ""], ["Pajak", "Tom", ""], ["Clarke", "Daoud", ""], ["Rodriguez", "Carlos", ""]]}, {"id": "1709.05295", "submitter": "Shereen Oraby", "authors": "Shereen Oraby, Lena Reed, Ryan Compton, Ellen Riloff, Marilyn Walker,\n  and Steve Whittaker", "title": "And That's A Fact: Distinguishing Factual and Emotional Argumentation in\n  Online Dialogue", "comments": "11 pages, 6 figures, Proceedings of the 2nd Workshop on Argumentation\n  Mining at NAACL 2015", "journal-ref": null, "doi": "10.3115/v1/W15-0515", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the characteristics of factual and emotional argumentation\nstyles observed in online debates. Using an annotated set of \"factual\" and\n\"feeling\" debate forum posts, we extract patterns that are highly correlated\nwith factual and emotional arguments, and then apply a bootstrapping\nmethodology to find new patterns in a larger pool of unannotated forum posts.\nThis process automatically produces a large set of patterns representing\nlinguistic expressions that are highly correlated with factual and emotional\nlanguage. Finally, we analyze the most discriminating patterns to better\nunderstand the defining characteristics of factual and emotional arguments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:28:46 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Oraby", "Shereen", ""], ["Reed", "Lena", ""], ["Compton", "Ryan", ""], ["Riloff", "Ellen", ""], ["Walker", "Marilyn", ""], ["Whittaker", "Steve", ""]]}, {"id": "1709.05305", "submitter": "Shereen Oraby", "authors": "Shereen Oraby, Vrindavan Harrison, Amita Misra, Ellen Riloff, and\n  Marilyn Walker", "title": "Are you serious?: Rhetorical Questions and Sarcasm in Social Media\n  Dialog", "comments": "10 pages, 1 figure, SIGDIAL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective models of social dialog must understand a broad range of rhetorical\nand figurative devices. Rhetorical questions (RQs) are a type of figurative\nlanguage whose aim is to achieve a pragmatic goal, such as structuring an\nargument, being persuasive, emphasizing a point, or being ironic. While there\nare computational models for other forms of figurative language, rhetorical\nquestions have received little attention to date. We expand a small dataset\nfrom previous work, presenting a corpus of 10,270 RQs from debate forums and\nTwitter that represent different discourse functions. We show that we can\nclearly distinguish between RQs and sincere questions (0.76 F1). We then show\nthat RQs can be used both sarcastically and non-sarcastically, observing that\nnon-sarcastic (other) uses of RQs are frequently argumentative in forums, and\npersuasive in tweets. We present experiments to distinguish between these uses\nof RQs using SVM and LSTM models that represent linguistic features and\npost-level context, achieving results as high as 0.76 F1 for \"sarcastic\" and\n0.77 F1 for \"other\" in forums, and 0.83 F1 for both \"sarcastic\" and \"other\" in\ntweets. We supplement our quantitative experiments with an in-depth\ncharacterization of the linguistic variation in RQs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:54:02 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Oraby", "Shereen", ""], ["Harrison", "Vrindavan", ""], ["Misra", "Amita", ""], ["Riloff", "Ellen", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.05308", "submitter": "Shereen Oraby", "authors": "Shereen Oraby, Sheideh Homayon, and Marilyn Walker", "title": "Harvesting Creative Templates for Generating Stylistically Varied\n  Restaurant Reviews", "comments": "9 pages, 2 figures, Stylistic Variation Workshop at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the creative and figurative elements that make language exciting are\nlost in translation in current natural language generation engines. In this\npaper, we explore a method to harvest templates from positive and negative\nreviews in the restaurant domain, with the goal of vastly expanding the types\nof stylistic variation available to the natural language generator. We learn\nhyperbolic adjective patterns that are representative of the strongly-valenced\nexpressive language commonly used in either positive or negative reviews. We\nthen identify and delexicalize entities, and use heuristics to extract\ngeneration templates from review sentences. We evaluate the learned templates\nagainst more traditional review templates, using subjective measures of\n\"convincingness\", \"interestingness\", and \"naturalness\". Our results show that\nthe learned templates score highly on these measures. Finally, we analyze the\nlinguistic categories that characterize the learned positive and negative\ntemplates. We plan to use the learned templates to improve the conversational\nstyle of dialogue systems in the restaurant domain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:59:20 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Oraby", "Shereen", ""], ["Homayon", "Sheideh", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.05404", "submitter": "Shereen Oraby", "authors": "Shereen Oraby, Vrindavan Harrison, Lena Reed, Ernesto Hernandez, Ellen\n  Riloff, and Marilyn Walker", "title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "comments": "11 pages, 4 figures, SIGDIAL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of irony and sarcasm in social media allows us to study them at scale\nfor the first time. However, their diversity has made it difficult to construct\na high-quality corpus of sarcasm in dialogue. Here, we describe the process of\ncreating a large- scale, highly-diverse corpus of online debate forums\ndialogue, and our novel methods for operationalizing classes of sarcasm in the\nform of rhetorical questions and hyperbole. We show that we can use\nlexico-syntactic cues to reliably retrieve sarcastic utterances with high\naccuracy. To demonstrate the properties and quality of our corpus, we conduct\nsupervised learning experiments with simple features, and show that we achieve\nboth higher precision and F than previous work on sarcasm in debate forums\ndialogue. We apply a weakly-supervised linguistic pattern learner and\nqualitatively analyze the linguistic differences in each class.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:01:57 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Oraby", "Shereen", ""], ["Harrison", "Vrindavan", ""], ["Reed", "Lena", ""], ["Hernandez", "Ernesto", ""], ["Riloff", "Ellen", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.05411", "submitter": "Kevin Bowden", "authors": "Kevin K. Bowden, Shereen Oraby, Jiaqi Wu, Amita Misra, and Marilyn\n  Walker", "title": "Combining Search with Structured Data to Create a More Engaging User\n  Experience in Open Domain Dialogue", "comments": "SCAI 2017", "journal-ref": "Search-Oriented Conversational AI (SCAI) 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greatest challenges in building sophisticated open-domain conversational\nagents arise directly from the potential for ongoing mixed-initiative\nmulti-turn dialogues, which do not follow a particular plan or pursue a\nparticular fixed information need. In order to make coherent conversational\ncontributions in this context, a conversational agent must be able to track the\ntypes and attributes of the entities under discussion in the conversation and\nknow how they are related. In some cases, the agent can rely on structured\ninformation sources to help identify the relevant semantic relations and\nproduce a turn, but in other cases, the only content available comes from\nsearch, and it may be unclear which semantic relations hold between the search\nresults and the discourse context. A further constraint is that the system must\nproduce its contribution to the ongoing conversation in real-time. This paper\ndescribes our experience building SlugBot for the 2017 Alexa Prize, and\ndiscusses how we leveraged search and structured data from different sources to\nhelp SlugBot produce dialogic turns and carry on conversations whose length\nover the semi-finals user evaluation period averaged 8:17 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:12:59 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Oraby", "Shereen", ""], ["Wu", "Jiaqi", ""], ["Misra", "Amita", ""], ["Walker", "Marilyn", ""]]}, {"id": "1709.05413", "submitter": "Shereen Oraby", "authors": "Shereen Oraby, Pritam Gundecha, Jalal Mahmud, Mansurul Bhuiyan, and\n  Rama Akkiraju", "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations\n  Using Fine-Grained Dialogue Acts", "comments": "13 pages, 6 figures, IUI 2017", "journal-ref": null, "doi": "10.1145/3025171.3025191", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the increasing popularity of customer service dialogue on Twitter,\nanalysis of conversation data is essential to understand trends in customer and\nagent behavior for the purpose of automating customer service interactions. In\nthis work, we develop a novel taxonomy of fine-grained \"dialogue acts\"\nfrequently observed in customer service, showcasing acts that are more suited\nto the domain than the more generic existing taxonomies. Using a sequential\nSVM-HMM model, we model conversation flow, predicting the dialogue act of a\ngiven turn in real-time. We characterize differences between customer and agent\nbehavior in Twitter customer service conversations, and investigate the effect\nof testing our system on different customer service industries. Finally, we use\na data-driven approach to predict important conversation outcomes: customer\nsatisfaction, customer frustration, and overall problem resolution. We show\nthat the type and location of certain dialogue acts in a conversation have a\nsignificant effect on the probability of desirable and undesirable outcomes,\nand present actionable rules based on our findings. The patterns and rules we\nderive can be used as guidelines for outcome-driven automated customer service\nplatforms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:24:48 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Oraby", "Shereen", ""], ["Gundecha", "Pritam", ""], ["Mahmud", "Jalal", ""], ["Bhuiyan", "Mansurul", ""], ["Akkiraju", "Rama", ""]]}, {"id": "1709.05453", "submitter": "Tom Young", "authors": "Tom Young, Erik Cambria, Iti Chaturvedi, Minlie Huang, Hao Zhou,\n  Subham Biswas", "title": "Augmenting End-to-End Dialog Systems with Commonsense Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building dialog agents that can converse naturally with humans is a\nchallenging yet intriguing problem of artificial intelligence. In open-domain\nhuman-computer conversation, where the conversational agent is expected to\nrespond to human responses in an interesting and engaging way, commonsense\nknowledge has to be integrated into the model effectively. In this paper, we\ninvestigate the impact of providing commonsense knowledge about the concepts\ncovered in the dialog. Our model represents the first attempt to integrating a\nlarge commonsense knowledge base into end-to-end conversational models. In the\nretrieval-based scenario, we propose the Tri-LSTM model to jointly take into\naccount message and commonsense for selecting an appropriate response. Our\nexperiments suggest that the knowledge-augmented models are superior to their\nknowledge-free counterparts in automatic evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 04:14:53 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 05:52:45 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 14:13:21 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Young", "Tom", ""], ["Cambria", "Erik", ""], ["Chaturvedi", "Iti", ""], ["Huang", "Minlie", ""], ["Zhou", "Hao", ""], ["Biswas", "Subham", ""]]}, {"id": "1709.05467", "submitter": "Ying Lin", "authors": "Ying Lin, Joe Hoover, Morteza Dehghani, Marlon Mooijman, Heng Ji", "title": "Acquiring Background Knowledge to Improve Moral Value Prediction", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of detecting expressions of moral\nvalues in tweets using content analysis. This is a particularly challenging\nproblem because moral values are often only implicitly signaled in language,\nand tweets contain little contextual information due to length constraints. To\naddress these obstacles, we present a novel approach to automatically acquire\nbackground knowledge from an external knowledge base to enrich input texts and\nthus improve moral value prediction. By combining basic text features with\nbackground knowledge, our overall context-aware framework achieves performance\ncomparable to a single human annotator. To the best of our knowledge, this is\nthe first attempt to incorporate background knowledge for the prediction of\nimplicit psychological variables in the area of computational social science.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 05:54:54 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Lin", "Ying", ""], ["Hoover", "Joe", ""], ["Dehghani", "Morteza", ""], ["Mooijman", "Marlon", ""], ["Ji", "Heng", ""]]}, {"id": "1709.05475", "submitter": "Bo Ru Lu", "authors": "Bo-Ru Lu, Frank Shyu, Yun-Nung Chen, Hung-Yi Lee, and Lin-shan Lee", "title": "Order-Preserving Abstractive Summarization for Spoken Content Based on\n  Connectionist Temporal Classification", "comments": "Accepted by Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectionist temporal classification (CTC) is a powerful approach for\nsequence-to-sequence learning, and has been popularly used in speech\nrecognition. The central ideas of CTC include adding a label \"blank\" during\ntraining. With this mechanism, CTC eliminates the need of segment alignment,\nand hence has been applied to various sequence-to-sequence learning problems.\nIn this work, we applied CTC to abstractive summarization for spoken content.\nThe \"blank\" in this case implies the corresponding input data are less\nimportant or noisy; thus it can be ignored. This approach was shown to\noutperform the existing methods in term of ROUGE scores over Chinese Gigaword\nand MATBN corpora. This approach also has the nice property that the ordering\nof words or characters in the input documents can be better preserved in the\ngenerated summaries.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 07:39:53 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 03:26:35 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Lu", "Bo-Ru", ""], ["Shyu", "Frank", ""], ["Chen", "Yun-Nung", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1709.05487", "submitter": "Sreelekha S", "authors": "Sreelekha S, Pushpak Bhattacharyya", "title": "Role of Morphology Injection in Statistical Machine Translation", "comments": "36 pages, 12 figures, 15 tables, Modified version Published in: ACM\n  Transactions on Asian and Low-Resource Language Information Processing\n  (TALLIP) TALLIP Homepage archive Volume 17 Issue 1, September 2017\n  Issue-in-Progress,Article No. 1", "journal-ref": "ACM Transactions on Asian and Low-Resource Language Information\n  Processing (TALLIP), Volume 17 Issue 1, September 2017 Issue-in-Progress,\n  Article No. 1", "doi": "10.1145/3129208", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phrase-based Statistical models are more commonly used as they perform\noptimally in terms of both, translation quality and complexity of the system.\nHindi and in general all Indian languages are morphologically richer than\nEnglish. Hence, even though Phrase-based systems perform very well for the less\ndivergent language pairs, for English to Indian language translation, we need\nmore linguistic information (such as morphology, parse tree, parts of speech\ntags, etc.) on the source side. Factored models seem to be useful in this case,\nas Factored models consider word as a vector of factors. These factors can\ncontain any information about the surface word and use it while translating.\nHence, the objective of this work is to handle morphological inflections in\nHindi and Marathi using Factored translation models while translating from\nEnglish. SMT approaches face the problem of data sparsity while translating\ninto a morphologically rich language. It is very unlikely for a parallel corpus\nto contain all morphological forms of words. We propose a solution to generate\nthese unseen morphological forms and inject them into original training\ncorpora. In this paper, we study factored models and the problem of sparseness\nin context of translation to morphologically rich languages. We propose a\nsimple and effective solution which is based on enriching the input with\nvarious morphological forms of words. We observe that morphology injection\nimproves the quality of translation in terms of both adequacy and fluency. We\nverify this with the experiments on two morphologically rich languages: Hindi\nand Marathi, while translating from English.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 09:40:36 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["S", "Sreelekha", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1709.05522", "submitter": "Xingyu Na", "authors": "Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, Hao Zheng", "title": "AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech\n  Recognition Baseline", "comments": "Oriental COCOSDA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open-source Mandarin speech corpus called AISHELL-1 is released. It is by\nfar the largest corpus which is suitable for conducting the speech recognition\nresearch and building speech recognition systems for Mandarin. The recording\nprocedure, including audio capturing devices and environments are presented in\ndetails. The preparation of the related resources, including transcriptions and\nlexicon are described. The corpus is released with a Kaldi recipe. Experimental\nresults implies that the quality of audio recordings and transcriptions are\npromising.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 14:33:27 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bu", "Hui", ""], ["Du", "Jiayu", ""], ["Na", "Xingyu", ""], ["Wu", "Bengu", ""], ["Zheng", "Hao", ""]]}, {"id": "1709.05563", "submitter": "Slava Mikhaylov", "authors": "Philipp Broniecki and Anna Hanchar and Slava J. Mikhaylov", "title": "Data Innovation for International Development: An overview of natural\n  language processing for qualitative data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability, collection and access to quantitative data, as well as its\nlimitations, often make qualitative data the resource upon which development\nprograms heavily rely. Both traditional interview data and social media\nanalysis can provide rich contextual information and are essential for\nresearch, appraisal, monitoring and evaluation. These data may be difficult to\nprocess and analyze both systematically and at scale. This, in turn, limits the\nability of timely data driven decision-making which is essential in fast\nevolving complex social systems. In this paper, we discuss the potential of\nusing natural language processing to systematize analysis of qualitative data,\nand to inform quick decision-making in the development context. We illustrate\nthis with interview data generated in a format of micro-narratives for the UNDP\nFragments of Impact project.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 20:16:56 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Broniecki", "Philipp", ""], ["Hanchar", "Anna", ""], ["Mikhaylov", "Slava J.", ""]]}, {"id": "1709.05576", "submitter": "Manolis Peponakis", "authors": "Anna Mastora, Manolis Peponakis, Sarantos Kapidakis", "title": "SKOS Concepts and Natural Language Concepts: an Analysis of Latent\n  Relationships in KOSs", "comments": "18 pages, 5 tables", "journal-ref": "Journal of Information Science, 43(4), 492-508 (2017)", "doi": "10.1177/0165551516648108", "report-no": null, "categories": "cs.DL cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The vehicle to represent Knowledge Organization Systems (KOSs) in the\nenvironment of the Semantic Web and linked data is the Simple Knowledge\nOrganization System (SKOS). SKOS provides a way to assign a URI to each\nconcept, and this URI functions as a surrogate for the concept. This fact makes\nof main concern the need to clarify the URIs' ontological meaning. The aim of\nthis study is to investigate the relation between the ontological substance of\nKOS concepts and concepts revealed through the grammatical and syntactic\nformalisms of natural language. For this purpose, we examined the dividableness\nof concepts in specific KOSs (i.e. a thesaurus, a subject headings system and a\nclassification scheme) by applying Natural Language Processing (NLP) techniques\n(i.e. morphosyntactic analysis) to the lexical representations (i.e. RDF\nliterals) of SKOS concepts. The results of the comparative analysis reveal\nthat, despite the use of multi-word units, thesauri tend to represent concepts\nin a way that can hardly be further divided conceptually, while Subject\nHeadings and Classification Schemes - to a certain extent - comprise terms that\ncan be decomposed into more conceptual constituents. Consequently, SKOS\nconcepts deriving from thesauri are more likely to represent atomic conceptual\nunits and thus be more appropriate tools for inference and reasoning. Since\nidentifiers represent the meaning of a concept, complex concepts are neither\nthe most appropriate nor the most efficient way of modelling a KOS for the\nSemantic Web.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 22:58:13 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Mastora", "Anna", ""], ["Peponakis", "Manolis", ""], ["Kapidakis", "Sarantos", ""]]}, {"id": "1709.05587", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Shuhua Zhang, Yuanli Geng, Huei-ling Lai, Hongsu Wang", "title": "Character Distributions of Classical Chinese Literary Texts: Zipf's Law,\n  Genres, and Epochs", "comments": "12 pages, 2 tables, 6 figures, 2017 International Conference on\n  Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We collect 14 representative corpora for major periods in Chinese history in\nthis study. These corpora include poetic works produced in several dynasties,\nnovels of the Ming and Qing dynasties, and essays and news reports written in\nmodern Chinese. The time span of these corpora ranges between 1046 BCE and 2007\nCE. We analyze their character and word distributions from the viewpoint of the\nZipf's law, and look for factors that affect the deviations and similarities\nbetween their Zipfian curves. Genres and epochs demonstrated their influences\nin our analyses. Specifically, the character distributions for poetic works of\nbetween 618 CE and 1644 CE exhibit striking similarity. In addition, although\ntexts of the same dynasty may tend to use the same set of characters, their\ncharacter distributions still deviate from each other.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 01:15:03 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Zhang", "Shuhua", ""], ["Geng", "Yuanli", ""], ["Lai", "Huei-ling", ""], ["Wang", "Hongsu", ""]]}, {"id": "1709.05599", "submitter": "Wei Li", "authors": "Wei Li, Yunfang Wu", "title": "Hierarchical Gated Recurrent Neural Tensor Network for Answer Triggering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of answer triggering ad-dressed by\nYang et al. (2015), which is a critical component for a real-world question\nanswering system. We employ a hierarchical gated recurrent neural tensor\n(HGRNT) model to capture both the context information and the deep\nin-teractions between the candidate answers and the question. Our result on F\nval-ue achieves 42.6%, which surpasses the baseline by over 10 %.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 02:56:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Li", "Wei", ""], ["Wu", "Yunfang", ""]]}, {"id": "1709.05631", "submitter": "Laurent Besacier", "authors": "Marcely Zanon Boito, Alexandre Berard, Aline Villavicencio and Laurent\n  Besacier", "title": "Unwritten Languages Demand Attention Too! Word Discovery with\n  Encoder-Decoder Models", "comments": "Accepted to IEEE ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Word discovery is the task of extracting words from unsegmented text. In this\npaper we examine to what extent neural networks can be applied to this task in\na realistic unwritten language scenario, where only small corpora and limited\nannotations are available. We investigate two scenarios: one with no\nsupervision and another with limited supervision with access to the most\nfrequent words. Obtained results show that it is possible to retrieve at least\n27% of the gold standard vocabulary by training an encoder-decoder neural\nmachine translation system with only 5,157 sentences. This result is close to\nthose obtained with a task-specific Bayesian nonparametric model. Moreover, our\napproach has the advantage of generating translation alignments, which could be\nused to create a bilingual lexicon. As a future perspective, this approach is\nalso well suited to work directly from speech.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 09:36:40 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 05:30:55 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Boito", "Marcely Zanon", ""], ["Berard", "Alexandre", ""], ["Villavicencio", "Aline", ""], ["Besacier", "Laurent", ""]]}, {"id": "1709.05700", "submitter": "Amin Jaber", "authors": "Amin Jaber and Fadi A. Zaraket", "title": "Morphology-based Entity and Relational Entity Extraction Framework for\n  Arabic", "comments": null, "journal-ref": "Traitement Automatique des Langues (TAL). 58.3 (2017): 97-121", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based techniques to extract relational entities from documents allow\nusers to specify desired entities with natural language questions, finite state\nautomata, regular expressions and structured query language. They require\nlinguistic and programming expertise and lack support for Arabic morphological\nanalysis. We present a morphology-based entity and relational entity extraction\nframework for Arabic (MERF). MERF requires basic knowledge of linguistic\nfeatures and regular expressions, and provides the ability to interactively\nspecify Arabic morphological and synonymity features, tag types associated with\nregular expressions, and relations and code actions defined over matches of\nsubexpressions. MERF constructs entities and relational entities from matches\nof the specifications. We evaluated MERF with several case studies. The results\nshow that MERF requires shorter development time and effort compared to\nexisting application specific techniques and produces reasonably accurate\nresults within a reasonable overhead in run time.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 17:56:24 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 17:01:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Jaber", "Amin", ""], ["Zaraket", "Fadi A.", ""]]}, {"id": "1709.05729", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu", "title": "Flexible Computing Services for Comparisons and Analyses of Classical\n  Chinese Poetry", "comments": "6 pages, 2 tables, 1 figure, 2017 International Conference on Digital\n  Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We collect nine corpora of representative Chinese poetry for the time span of\n1046 BCE and 1644 CE for studying the history of Chinese words, collocations,\nand patterns. By flexibly integrating our own tools, we are able to provide new\nperspectives for approaching our goals. We illustrate the ideas with two\nexamples. The first example show a new way to compare word preferences of\npoets, and the second example demonstrates how we can utilize our corpora in\nhistorical studies of the Chinese words. We show the viability of the tools for\nacademic research, and we wish to make it helpful for enriching existing\nChinese dictionary as well.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 00:01:07 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Liu", "Chao-Lin", ""]]}, {"id": "1709.05743", "submitter": "Jan R. Benetka", "authors": "Jan R. Benetka, Krisztian Balog, Kjetil N{\\o}rv{\\aa}g", "title": "Towards Building a Knowledge Base of Monetary Transactions from a News\n  Collection", "comments": "Proceedings of the 17th ACM/IEEE-CS Joint Conference on Digital\n  Libraries (JCDL '17), 2017", "journal-ref": null, "doi": "10.1109/JCDL.2017.7991575", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of extracting structured representations of economic\nevents from a large corpus of news articles, using a combination of natural\nlanguage processing and machine learning techniques. The developed techniques\nallow for semi-automatic population of a financial knowledge base, which, in\nturn, may be used to support a range of data mining and exploration tasks. The\nkey challenge we face in this domain is that the same event is often reported\nmultiple times, with varying correctness of details. We address this challenge\nby first collecting all information pertinent to a given event from the entire\ncorpus, then considering all possible representations of the event, and\nfinally, using a supervised learning method, to rank these representations by\nthe associated confidence scores. A main innovative element of our approach is\nthat it jointly extracts and stores all attributes of the event as a single\nrepresentation (quintuple). Using a purpose-built test set we demonstrate that\nour supervised learning approach can achieve 25% improvement in F1-score over\nbaseline methods that consider the earliest, the latest or the most frequent\nreporting of the event.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:09:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Benetka", "Jan R.", ""], ["Balog", "Krisztian", ""], ["N\u00f8rv\u00e5g", "Kjetil", ""]]}, {"id": "1709.05778", "submitter": "Bradford Heap", "authors": "Bradford Heap, Michael Bain, Wayne Wobcke, Alfred Krzywicki and\n  Susanne Schmeidl", "title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model\n  for Short Text Multi-class Classification Problems", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag-of-words model is a standard representation of text for many linear\nclassifier learners. In many problem domains, linear classifiers are preferred\nover more complex models due to their efficiency, robustness and\ninterpretability, and the bag-of-words text representation can capture\nsufficient information for linear classifiers to make highly accurate\npredictions. However in settings where there is a large vocabulary, large\nvariance in the frequency of terms in the training corpus, many classes and\nvery short text (e.g., single sentences or document titles) the bag-of-words\nrepresentation becomes extremely sparse, and this can reduce the accuracy of\nclassifiers. A particular issue in such settings is that short texts tend to\ncontain infrequently occurring or rare terms which lack class-conditional\nevidence. In this work we introduce a method for enriching the bag-of-words\nmodel by complementing such rare term information with related terms from both\ngeneral and domain-specific Word Vector models. By reducing sparseness in the\nbag-of-words models, our enrichment approach achieves improved classification\nover several baseline classifiers in a variety of text classification problems.\nOur approach is also efficient because it requires no change to the linear\nclassifier before or during training, since bag-of-words enrichment applies\nonly to text being classified.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 05:00:34 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Heap", "Bradford", ""], ["Bain", "Michael", ""], ["Wobcke", "Wayne", ""], ["Krzywicki", "Alfred", ""], ["Schmeidl", "Susanne", ""]]}, {"id": "1709.05820", "submitter": "Maxim Khalilov", "authors": "Pavel Levin, Nishikant Dhanuka, Talaat Khalil, Fedor Kovalev, Maxim\n  Khalilov", "title": "Toward a full-scale neural machine translation in production: the\n  Booking.com use case", "comments": "11 pages, 4 figures, presented at MT Summit XVI, Commercial MT Users\n  and Translators Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While some remarkable progress has been made in neural machine translation\n(NMT) research, there have not been many reports on its development and\nevaluation in practice. This paper tries to fill this gap by presenting some of\nour findings from building an in-house travel domain NMT system in a large\nscale E-commerce setting. The three major topics that we cover are optimization\nand training (including different optimization strategies and corpus sizes),\nhandling real-world content and evaluating results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 08:46:20 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 10:26:44 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Levin", "Pavel", ""], ["Dhanuka", "Nishikant", ""], ["Khalil", "Talaat", ""], ["Kovalev", "Fedor", ""], ["Khalilov", "Maxim", ""]]}, {"id": "1709.05914", "submitter": "Mareike Hartmann", "authors": "Mareike Hartmann and Anders Soegaard", "title": "Limitations of Cross-Lingual Learning from Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual representation learning is an important step in making NLP\nscale to all the world's languages. Recent work on bilingual lexicon induction\nsuggests that it is possible to learn cross-lingual representations of words\nbased on similarities between images associated with these words. However, that\nwork focused on the translation of selected nouns only. In our work, we\ninvestigate whether the meaning of other parts-of-speech, in particular\nadjectives and verbs, can be learned in the same way. We also experiment with\ncombining the representations learned from visual data with embeddings learned\nfrom textual data. Our experiments across five language pairs indicate that\nprevious work does not scale to the problem of learning cross-lingual\nrepresentations beyond simple nouns.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:28:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Hartmann", "Mareike", ""], ["Soegaard", "Anders", ""]]}, {"id": "1709.06033", "submitter": "Dai Quoc Nguyen", "authors": "Dai Quoc Nguyen, Dat Quoc Nguyen, Cuong Xuan Chu, Stefan Thater and\n  Manfred Pinkal", "title": "Sequence to Sequence Learning for Event Prediction", "comments": "To appear in Proceedings of IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to the task of predicting an event\ndescription from a preceding sentence in a text. Our approach explores\nsequence-to-sequence learning using a bidirectional multi-layer recurrent\nneural network. Our approach substantially outperforms previous work in terms\nof the BLEU score on two datasets derived from WikiHow and DeScript\nrespectively. Since the BLEU score is not easy to interpret as a measure of\nevent prediction, we complement our study with a second evaluation that\nexploits the rich linguistic annotation of gold paraphrase sets of events.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 16:34:18 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Nguyen", "Dai Quoc", ""], ["Nguyen", "Dat Quoc", ""], ["Chu", "Cuong Xuan", ""], ["Thater", "Stefan", ""], ["Pinkal", "Manfred", ""]]}, {"id": "1709.06136", "submitter": "Bing Liu", "authors": "Bing Liu, Ian Lane", "title": "Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural\n  Dialog Models", "comments": "Accepted at ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep reinforcement learning (RL) framework for\niterative dialog policy optimization in end-to-end task-oriented dialog\nsystems. Popular approaches in learning dialog policy with RL include letting a\ndialog agent to learn against a user simulator. Building a reliable user\nsimulator, however, is not trivial, often as difficult as building a good\ndialog agent. We address this challenge by jointly optimizing the dialog agent\nand the user simulator with deep RL by simulating dialogs between the two\nagents. We first bootstrap a basic dialog agent and a basic user simulator by\nlearning directly from dialog corpora with supervised training. We then improve\nthem further by letting the two agents to conduct task-oriented dialogs and\niteratively optimizing their policies with deep RL. Both the dialog agent and\nthe user simulator are designed with neural network models that can be trained\nend-to-end. Our experiment results show that the proposed method leads to\npromising improvements on task success rate and total task reward comparing to\nsupervised training and single-agent RL training baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:45:51 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Liu", "Bing", ""], ["Lane", "Ian", ""]]}, {"id": "1709.06162", "submitter": "Alberto Mor\\'on Hern\\'andez", "authors": "Alberto Mor\\'on Hern\\'andez", "title": "Paraphrasing verbal metonymy through computational methods", "comments": "49 pages, 5 figures, BA dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Verbal metonymy has received relatively scarce attention in the field of\ncomputational linguistics despite the fact that a model to accurately\nparaphrase metonymy has applications both in academia and the technology\nsector. The method described in this paper makes use of data from the British\nNational Corpus in order to create word vectors, find instances of verbal\nmetonymy and generate potential paraphrases. Two different ways of creating\nword vectors are evaluated in this study: Continuous bag of words and\nSkip-grams. Skip-grams are found to outperform the Continuous bag of words\napproach. Furthermore, the Skip-gram model is found to operate with\nbetter-than-chance accuracy and there is a strong positive relationship (phi\ncoefficient = 0.61) between the model's classification and human judgement of\nthe ranked paraphrases. This study lends credence to the viability of modelling\nverbal metonymy through computational methods based on distributional\nsemantics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:40:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Hern\u00e1ndez", "Alberto Mor\u00f3n", ""]]}, {"id": "1709.06265", "submitter": "Zi-Yi Dou", "authors": "Zi-Yi Dou, Hao Zhou, Shu-Jian Huang, Xin-Yu Dai, Jia-Jun Chen", "title": "Dynamic Oracle for Neural Machine Translation in Decoding Phase", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past several years have witnessed the rapid progress of end-to-end Neural\nMachine Translation (NMT). However, there exists discrepancy between training\nand inference in NMT when decoding, which may lead to serious problems since\nthe model might be in a part of the state space it has never seen during\ntraining. To address the issue, Scheduled Sampling has been proposed. However,\nthere are certain limitations in Scheduled Sampling and we propose two dynamic\noracle-based methods to improve it. We manage to mitigate the discrepancy by\nchanging the training process towards a less guided scheme and meanwhile\naggregating the oracle's demonstrations. Experimental results show that the\nproposed approaches improve translation quality over standard NMT system.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 06:07:12 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 21:30:25 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Dou", "Zi-Yi", ""], ["Zhou", "Hao", ""], ["Huang", "Shu-Jian", ""], ["Dai", "Xin-Yu", ""], ["Chen", "Jia-Jun", ""]]}, {"id": "1709.06307", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Dai Quoc Nguyen, Thanh Vu, Mark Dras, Mark Johnson", "title": "A Fast and Accurate Vietnamese Word Segmenter", "comments": "In Proceedings of the 11th International Conference on Language\n  Resources and Evaluation (LREC 2018), to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to Vietnamese word segmentation. Our approach is\nbased on the Single Classification Ripple Down Rules methodology (Compton and\nJansen, 1990), where rules are stored in an exception structure and new rules\nare only added to correct segmentation errors given by existing rules.\nExperimental results on the benchmark Vietnamese treebank show that our\napproach outperforms previous state-of-the-art approaches JVnSegmenter,\nvnTokenizer, DongDu and UETsegmenter in terms of both accuracy and performance\nspeed. Our code is open-source and available at:\nhttps://github.com/datquocnguyen/RDRsegmenter.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:16:09 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 08:51:17 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Nguyen", "Dai Quoc", ""], ["Vu", "Thanh", ""], ["Dras", "Mark", ""], ["Johnson", "Mark", ""]]}, {"id": "1709.06309", "submitter": "Soufian Jebbara", "authors": "Soufian Jebbara, Philipp Cimiano", "title": "Aspect-Based Relational Sentiment Analysis Using a Stacked Neural\n  Network Architecture", "comments": null, "journal-ref": null, "doi": "10.3233/978-1-61499-672-9-1123", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis can be regarded as a relation extraction problem in which\nthe sentiment of some opinion holder towards a certain aspect of a product,\ntheme or event needs to be extracted. We present a novel neural architecture\nfor sentiment analysis as a relation extraction problem that addresses this\nproblem by dividing it into three subtasks: i) identification of aspect and\nopinion terms, ii) labeling of opinion terms with a sentiment, and iii)\nextraction of relations between opinion terms and aspect terms. For each\nsubtask, we propose a neural network based component and combine all of them\ninto a complete system for relational sentiment analysis. The component for\naspect and opinion term extraction is a hybrid architecture consisting of a\nrecurrent neural network stacked on top of a convolutional neural network. This\napproach outperforms a standard convolutional deep neural architecture as well\nas a recurrent network architecture and performs competitively compared to\nother methods on two datasets of annotated customer reviews. To extract\nsentiments for individual opinion terms, we propose a recurrent architecture in\ncombination with word distance features and achieve promising results,\noutperforming a majority baseline by 18% accuracy and providing the first\nresults for the USAGE dataset. Our relation extraction component outperforms\nthe current state-of-the-art in aspect-opinion relation extraction by 15%\nF-Measure.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:22:12 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Jebbara", "Soufian", ""], ["Cimiano", "Philipp", ""]]}, {"id": "1709.06311", "submitter": "Soufian Jebbara", "authors": "Soufian Jebbara, Philipp Cimiano", "title": "Aspect-Based Sentiment Analysis Using a Two-Step Neural Network\n  Architecture", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-46565-4_12", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web holds a wealth of information in the form of unstructured\ntexts such as customer reviews for products, events and more. By extracting and\nanalyzing the expressed opinions in customer reviews in a fine-grained way,\nvaluable opportunities and insights for customers and businesses can be gained.\nWe propose a neural network based system to address the task of Aspect-Based\nSentiment Analysis to compete in Task 2 of the ESWC-2016 Challenge on Semantic\nSentiment Analysis. Our proposed architecture divides the task in two subtasks:\naspect term extraction and aspect-specific sentiment extraction. This approach\nis flexible in that it allows to address each subtask independently. As a first\nstep, a recurrent neural network is used to extract aspects from a text by\nframing the problem as a sequence labeling task. In a second step, a recurrent\nnetwork processes each extracted aspect with respect to its context and\npredicts a sentiment label. The system uses pretrained semantic word embedding\nfeatures which we experimentally enhance with semantic knowledge extracted from\nWordNet. Further features extracted from SenticNet prove to be beneficial for\nthe extraction of sentiment labels. As the best performing system in its\ncategory, our proposed system proves to be an effective approach for the\nAspect-Based Sentiment Analysis.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:30:35 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Jebbara", "Soufian", ""], ["Cimiano", "Philipp", ""]]}, {"id": "1709.06317", "submitter": "Soufian Jebbara", "authors": "Soufian Jebbara, Philipp Cimiano", "title": "Improving Opinion-Target Extraction with Character-Level Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained sentiment analysis is receiving increasing attention in recent\nyears. Extracting opinion target expressions (OTE) in reviews is often an\nimportant step in fine-grained, aspect-based sentiment analysis. Retrieving\nthis information from user-generated text, however, can be difficult. Customer\nreviews, for instance, are prone to contain misspelled words and are difficult\nto process due to their domain-specific language. In this work, we investigate\nwhether character-level models can improve the performance for the\nidentification of opinion target expressions. We integrate information about\nthe character structure of a word into a sequence labeling system using\ncharacter-level word embeddings and show their positive impact on the system's\nperformance. Specifically, we obtain an increase by 3.3 points F1-score with\nrespect to our baseline model. In further experiments, we reveal encoded\ncharacter patterns of the learned embeddings and give a nuanced view of the\nperformance differences of both models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:46:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Jebbara", "Soufian", ""], ["Cimiano", "Philipp", ""]]}, {"id": "1709.06365", "submitter": "Wray Buntine", "authors": "He Zhao, Lan Du, Wray Buntine, Gang Liu", "title": "MetaLDA: a Topic Model that Efficiently Incorporates Meta information", "comments": "To appear in ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the text content, documents and their associated words usually come\nwith rich sets of meta informa- tion, such as categories of documents and\nsemantic/syntactic features of words, like those encoded in word embeddings.\nIncorporating such meta information directly into the generative process of\ntopic models can improve modelling accuracy and topic quality, especially in\nthe case where the word-occurrence information in the training data is\ninsufficient. In this paper, we present a topic model, called MetaLDA, which is\nable to leverage either document or word meta information, or both of them\njointly. With two data argumentation techniques, we can derive an efficient\nGibbs sampling algorithm, which benefits from the fully local conjugacy of the\nmodel. Moreover, the algorithm is favoured by the sparsity of the meta\ninformation. Extensive experiments on several real world datasets demonstrate\nthat our model achieves comparable or improved performance in terms of both\nperplexity and topic quality, particularly in handling sparse texts. In\naddition, compared with other models using meta information, our model runs\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 12:09:21 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Zhao", "He", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""], ["Liu", "Gang", ""]]}, {"id": "1709.06429", "submitter": "Shaona Ghosh", "authors": "Shaona Ghosh, Per Ola Kristensson", "title": "Neural Networks for Text Correction and Completion in Keyboard Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the ubiquity of mobile and wearable text messaging applications, the\nproblem of keyboard text decoding is not tackled sufficiently in the light of\nthe enormous success of the deep learning Recurrent Neural Network (RNN) and\nConvolutional Neural Networks (CNN) for natural language understanding. In\nparticular, considering that the keyboard decoders should operate on devices\nwith memory and processor resource constraints, makes it challenging to deploy\nindustrial scale deep neural network (DNN) models. This paper proposes a\nsequence-to-sequence neural attention network system for automatic text\ncorrection and completion. Given an erroneous sequence, our model encodes\ncharacter level hidden representations and then decodes the revised sequence\nthus enabling auto-correction and completion. We achieve this by a combination\nof character level CNN and gated recurrent unit (GRU) encoder along with and a\nword level gated recurrent unit (GRU) attention decoder. Unlike traditional\nlanguage models that learn from billions of words, our corpus size is only 12\nmillion words; an order of magnitude smaller. The memory footprint of our\nlearnt model for inference and prediction is also an order of magnitude smaller\nthan the conventional language model based text decoders. We report baseline\nperformance for neural keyboard decoders in such limited domain. Our models\nachieve a word level accuracy of $90\\%$ and a character error rate CER of\n$2.4\\%$ over the Twitter typo dataset. We present a novel dataset of noisy to\ncorrected mappings by inducing the noise distribution from the Twitter data\nover the OpenSubtitles 2009 dataset; on which our model predicts with a word\nlevel accuracy of $98\\%$ and sequence accuracy of $68.9\\%$. In our user study,\nour model achieved an average CER of $2.6\\%$ with the state-of-the-art\nnon-neural touch-screen keyboard decoder at CER of $1.6\\%$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 13:52:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Ghosh", "Shaona", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "1709.06436", "submitter": "Gakuto Kurata", "authors": "Gakuto Kurata, Bhuvana Ramabhadran, George Saon, Abhinav Sethy", "title": "Language Modeling with Highway LSTM", "comments": "to appear in 2017 IEEE Automatic Speech Recognition and Understanding\n  Workshop (ASRU 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) based on Long Short Term Memory (LSTM) have shown good\ngains in many automatic speech recognition tasks. In this paper, we extend an\nLSTM by adding highway networks inside an LSTM and use the resulting Highway\nLSTM (HW-LSTM) model for language modeling. The added highway networks increase\nthe depth in the time dimension. Since a typical LSTM has two internal states,\na memory cell and a hidden state, we compare various types of HW-LSTM by adding\nhighway networks onto the memory cell and/or the hidden state. Experimental\nresults on English broadcast news and conversational telephone speech\nrecognition show that the proposed HW-LSTM LM improves speech recognition\naccuracy on top of a strong LSTM LM baseline. We report 5.1% and 9.9% on the\nSwitchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the\nbest performance numbers reported on these tasks to date.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 14:04:15 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kurata", "Gakuto", ""], ["Ramabhadran", "Bhuvana", ""], ["Saon", "George", ""], ["Sethy", "Abhinav", ""]]}, {"id": "1709.06438", "submitter": "Shachar Mirkin", "authors": "Shachar Mirkin, Michal Jacovi, Tamar Lavee, Hong-Kwang Kuo, Samuel\n  Thomas, Leslie Sager, Lili Kotlerman, Elad Venezian, Noam Slonim", "title": "A Recorded Debating Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an English audio and textual dataset of debating\nspeeches, a unique resource for the growing research field of computational\nargumentation and debating technologies. We detail the process of speech\nrecording by professional debaters, the transcription of the speeches with an\nAutomatic Speech Recognition (ASR) system, their consequent automatic\nprocessing to produce a text that is more \"NLP-friendly\", and in parallel --\nthe manual transcription of the speeches in order to produce gold-standard\n\"reference\" transcripts. We release 60 speeches on various controversial\ntopics, each in five formats corresponding to the different stages in the\nproduction of the data. The intention is to allow utilizing this resource for\nmultiple research purposes, be it the addition of in-domain training data for a\ndebate-specific ASR system, or applying argumentation mining on either noisy or\nclean debate transcripts. We intend to make further releases of this data in\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 14:09:03 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:32:00 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Mirkin", "Shachar", ""], ["Jacovi", "Michal", ""], ["Lavee", "Tamar", ""], ["Kuo", "Hong-Kwang", ""], ["Thomas", "Samuel", ""], ["Sager", "Leslie", ""], ["Kotlerman", "Lili", ""], ["Venezian", "Elad", ""], ["Slonim", "Noam", ""]]}, {"id": "1709.06671", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala, Kohei Hayashi and Ken-ichi Kawarabayashi", "title": "Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word embeddings have shown superior performances in numerous\nNatural Language Processing (NLP) tasks. However, their performances vary\nsignificantly across different tasks, implying that the word embeddings learnt\nby those methods capture complementary aspects of lexical semantics. Therefore,\nwe believe that it is important to combine the existing word embeddings to\nproduce more accurate and complete \\emph{meta-embeddings} of words. For this\npurpose, we propose an unsupervised locally linear meta-embedding learning\nmethod that takes pre-trained word embeddings as the input, and produces more\naccurate meta embeddings. Unlike previously proposed meta-embedding learning\nmethods that learn a global projection over all words in a vocabulary, our\nproposed method is sensitive to the differences in local neighbourhoods of the\nindividual source word embeddings. Moreover, we show that vector concatenation,\na previously proposed highly competitive baseline approach for integrating word\nembeddings, can be derived as a special case of the proposed method.\nExperimental results on semantic similarity, word analogy, relation\nclassification, and short-text classification tasks show that our\nmeta-embeddings to significantly outperform prior methods in several benchmark\ndatasets, establishing a new state of the art for meta-embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 22:58:02 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Bollegala", "Danushka", ""], ["Hayashi", "Kohei", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1709.06673", "submitter": "Danushka Bollegala", "authors": "Huda Hakami and Danushka Bollegala and Hayashi Kohei", "title": "Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational\n  Compositional Operators for Analogy Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the semantic relations that exist between two given words (or\nentities) is an important first step in a wide-range of NLP applications such\nas analogical reasoning, knowledge base completion and relational information\nretrieval. A simple, yet surprisingly accurate method for representing a\nrelation between two words is to compute the vector offset (\\PairDiff) between\ntheir corresponding word embeddings. Despite the empirical success, it remains\nunclear as to whether \\PairDiff is the best operator for obtaining a relational\nrepresentation from word embeddings. We conduct a theoretical analysis of\ngeneralised bilinear operators that can be used to measure the $\\ell_{2}$\nrelational distance between two word-pairs. We show that, if the word\nembeddings are standardised and uncorrelated, such an operator will be\nindependent of bilinear terms, and can be simplified to a linear form, where\n\\PairDiff is a special case. For numerous word embedding types, we empirically\nverify the uncorrelation assumption, demonstrating the general applicability of\nour theoretical result. Moreover, we experimentally discover \\PairDiff from the\nbilinear relation composition operator on several benchmark analogy datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 23:09:15 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 15:41:23 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Hakami", "Huda", ""], ["Bollegala", "Danushka", ""], ["Kohei", "Hayashi", ""]]}, {"id": "1709.06818", "submitter": "Zhibin Niu", "authors": "Yan Ji, Licheng Liu, Hongcui Wang, Zhilei Liu, Zhibin Niu, Bruce Denby", "title": "Updating the silent speech challenge benchmark with deep learning", "comments": "25 pages, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2010 Silent Speech Challenge benchmark is updated with new results\nobtained in a Deep Learning strategy, using the same input features and\ndecoding strategy as in the original article. A Word Error Rate of 6.4% is\nobtained, compared to the published value of 17.4%. Additional results\ncomparing new auto-encoder-based features with the original features at reduced\ndimensionality, as well as decoding scenarios on two different language models,\nare also presented. The Silent Speech Challenge archive has been updated to\ncontain both the original and the new auto-encoder features, in addition to the\noriginal raw data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 11:28:40 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Ji", "Yan", ""], ["Liu", "Licheng", ""], ["Wang", "Hongcui", ""], ["Liu", "Zhilei", ""], ["Niu", "Zhibin", ""], ["Denby", "Bruce", ""]]}, {"id": "1709.06901", "submitter": "Chao Zhao", "authors": "Zhipeng Jiang, Chao Zhao, Bin He, Yi Guan, Jingchi Jiang", "title": "De-identification of medical records using conditional random fields and\n  long short-term memory networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CEGS N-GRID 2016 Shared Task 1 in Clinical Natural Language Processing\nfocuses on the de-identification of psychiatric evaluation records. This paper\ndescribes two participating systems of our team, based on conditional random\nfields (CRFs) and long short-term memory networks (LSTMs). A pre-processing\nmodule was introduced for sentence detection and tokenization before\nde-identification. For CRFs, manually extracted rich features were utilized to\ntrain the model. For LSTMs, a character-level bi-directional LSTM network was\napplied to represent tokens and classify tags for each token, following which a\ndecoding layer was stacked to decode the most probable protected health\ninformation (PHI) terms. The LSTM-based system attained an i2b2 strict\nmicro-F_1 measure of 89.86%, which was higher than that of the CRF-based\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:30:17 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 12:03:57 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Jiang", "Zhipeng", ""], ["Zhao", "Chao", ""], ["He", "Bin", ""], ["Guan", "Yi", ""], ["Jiang", "Jingchi", ""]]}, {"id": "1709.06907", "submitter": "Simon Razniewski", "authors": "Simon Razniewski, Vevake Balaraman, Werner Nutt", "title": "Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings\n  of Knowledge Base Properties [Extended Version]", "comments": "Extended version of an ADMA 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In knowledge bases such as Wikidata, it is possible to assert a large set of\nproperties for entities, ranging from generic ones such as name and place of\nbirth to highly profession-specific or background-specific ones such as\ndoctoral advisor or medical condition. Determining a preference or ranking in\nthis large set is a challenge in tasks such as prioritisation of edits or\nnatural-language generation. Most previous approaches to ranking knowledge base\nproperties are purely data-driven, that is, as we show, mistake frequency for\ninterestingness.\n  In this work, we have developed a human-annotated dataset of 350 preference\njudgments among pairs of knowledge base properties for fixed entities. From\nthis set, we isolate a subset of pairs for which humans show a high level of\nagreement (87.5% on average). We show, however, that baseline and\nstate-of-the-art techniques achieve only 61.3% precision in predicting human\npreferences for this subset.\n  We then analyze what contributes to one property being rated as more\nimportant than another one, and identify that at least three factors play a\nrole, namely (i) general frequency, (ii) applicability to similar entities and\n(iii) semantic similarity between property and entity. We experimentally\nanalyze the contribution of each factor and show that a combination of\ntechniques addressing all the three factors achieves 74% precision on the task.\n  The dataset is available at\nwww.kaggle.com/srazniewski/wikidatapropertyranking.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:43:08 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Razniewski", "Simon", ""], ["Balaraman", "Vevake", ""], ["Nutt", "Werner", ""]]}, {"id": "1709.06918", "submitter": "Chao Zhao", "authors": "Chao Zhao, Min Zhao, Yi Guan", "title": "Constructing a Hierarchical User Interest Structure based on User\n  Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interests of individual internet users fall into a hierarchical structure\nwhich is useful in regards to building personalized searches and\nrecommendations. Most studies on this subject construct the interest hierarchy\nof a single person from the document perspective. In this study, we constructed\nthe user interest hierarchy via user profiles. We organized 433,397 user\ninterests, referred to here as \"attentions\", into a user attention network\n(UAN) from 200 million user profiles; we then applied the Louvain algorithm to\ndetect hierarchical clusters in these attentions. Finally, a 26-level hierarchy\nwith 34,676 clusters was obtained. We found that these attention clusters were\naggregated according to certain topics as opposed to the hyponymy-relation\nbased conceptual ontologies. The topics can be entities or concepts, and the\nrelations were not restrained by hyponymy. The concept relativity encapsulated\nin the user's interest can be captured by labeling the attention clusters with\ncorresponding concepts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 15:03:51 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Zhao", "Chao", ""], ["Zhao", "Min", ""], ["Guan", "Yi", ""]]}, {"id": "1709.06990", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq and Bruce A. Bassett", "title": "Text Compression for Sentiment Analysis via Evolutionary Algorithms", "comments": "8 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Can textual data be compressed intelligently without losing accuracy in\nevaluating sentiment? In this study, we propose a novel evolutionary\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\nwhich makes use of Parts-of-Speech tags to compress text in a way that\nsacrifices minimal classification accuracy when used in conjunction with\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\nsentiment analysis algorithms are more severely affected by compression. We\nconclude that significant compression of text data is possible for sentiment\nanalysis depending on the accuracy demands of the specific application and the\nspecific sentiment analysis algorithm used.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 17:57:16 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1709.07104", "submitter": "Hoang Pham", "authors": "Thai-Hoang Pham, Xuan-Khoai Pham, Phuong Le-Hong", "title": "On the Use of Machine Translation-Based Approaches for Vietnamese\n  Diacritic Restoration", "comments": "4 pages, 2 figures, 4 tables, accepted to IALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an empirical study of two machine translation-based\napproaches for Vietnamese diacritic restoration problem, including phrase-based\nand neural-based machine translation models. This is the first work that\napplies neural-based machine translation method to this problem and gives a\nthorough comparison to the phrase-based machine translation method which is the\ncurrent state-of-the-art method for this problem. On a large dataset, the\nphrase-based approach has an accuracy of 97.32% while that of the neural-based\napproach is 96.15%. While the neural-based method has a slightly lower\naccuracy, it is about twice faster than the phrase-based method in terms of\ninference speed. Moreover, neural-based machine translation method has much\nroom for future improvement such as incorporating pre-trained word embeddings\nand collecting more training data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 23:56:24 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 15:22:47 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Pham", "Thai-Hoang", ""], ["Pham", "Xuan-Khoai", ""], ["Le-Hong", "Phuong", ""]]}, {"id": "1709.07109", "submitter": "Dinghan Shen", "authors": "Dinghan Shen, Yizhe Zhang, Ricardo Henao, Qinliang Su, Lawrence Carin", "title": "Deconvolutional Latent-Variable Model for Text Sequence Matching", "comments": "Accepted by AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A latent-variable model is introduced for text matching, inferring sentence\nrepresentations by jointly optimizing generative and discriminative objectives.\nTo alleviate typical optimization challenges in latent-variable models for\ntext, we employ deconvolutional networks as the sequence decoder (generator),\nproviding learned latent codes with more semantic information and better\ngeneralization. Our model, trained in an unsupervised manner, yields stronger\nempirical predictive performance than a decoder based on Long Short-Term Memory\n(LSTM), with less parameters and considerably faster training. Further, we\napply it to text sequence-matching problems. The proposed model significantly\noutperforms several strong sentence-encoding baselines, especially in the\nsemi-supervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 00:23:55 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 01:07:34 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 02:18:24 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Shen", "Dinghan", ""], ["Zhang", "Yizhe", ""], ["Henao", "Ricardo", ""], ["Su", "Qinliang", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.07276", "submitter": "Ahmed Ali", "authors": "Ahmed Ali, Stephan Vogel, Steve Renals", "title": "Speech Recognition Challenge in the Wild: Arabic MGB-3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Arabic MGB-3 Challenge - Arabic Speech Recognition\nin the Wild. Unlike last year's Arabic MGB-2 Challenge, for which the\nrecognition task was based on more than 1,200 hours broadcast TV news\nrecordings from Aljazeera Arabic TV programs, MGB-3 emphasises dialectal Arabic\nusing a multi-genre collection of Egyptian YouTube videos. Seven genres were\nused for the data collection: comedy, cooking, family/kids, fashion, drama,\nsports, and science (TEDx). A total of 16 hours of videos, split evenly across\nthe different genres, were divided into adaptation, development and evaluation\ndata sets. The Arabic MGB-Challenge comprised two tasks: A) Speech\ntranscription, evaluated on the MGB-3 test set, along with the 10 hour MGB-2\ntest set to report progress on the MGB-2 evaluation; B) Arabic dialect\nidentification, introduced this year in order to distinguish between four major\nArabic dialects - Egyptian, Levantine, North African, Gulf, as well as Modern\nStandard Arabic. Two hours of audio per dialect were released for development\nand a further two hours were used for evaluation. For dialect identification,\nboth lexical features and i-vector bottleneck features were shared with\nparticipants in addition to the raw audio recordings. Overall, thirteen teams\nsubmitted ten systems to the challenge. We outline the approaches adopted in\neach system, and summarise the evaluation results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 12:10:43 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ali", "Ahmed", ""], ["Vogel", "Stephan", ""], ["Renals", "Steve", ""]]}, {"id": "1709.07357", "submitter": "Trevor Cohen", "authors": "Zhiguo Yu, Byron C. Wallace, Todd Johnson, Trevor Cohen", "title": "Retrofitting Concept Vector Representations of Medical Concepts to\n  Improve Estimates of Semantic Similarity and Relatedness", "comments": "To appear in: Proceedings of the 16th World Congress on Medical and\n  Health Informatics 21st-25th August Hangzhou, China (2017). Please visit and\n  cite the canonical version once available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of semantic similarity and relatedness between biomedical concepts\nhas utility for many informatics applications. Automated methods fall into two\ncategories: methods based on distributional statistics drawn from text corpora,\nand methods using the structure of existing knowledge resources. Methods in the\nformer category disregard taxonomic structure, while those in the latter fail\nto consider semantically relevant empirical information. In this paper, we\npresent a method that retrofits distributional context vector representations\nof biomedical concepts using structural information from the UMLS\nMetathesaurus, such that the similarity between vector representations of\nlinked concepts is augmented. We evaluated it on the UMNSRS benchmark. Our\nresults demonstrate that retrofitting of concept vector representations leads\nto better correlation with human raters for both similarity and relatedness,\nsurpassing the best results reported to date. They also demonstrate a clear\nimprovement in performance on this reference standard for retrofitted vector\nrepresentations, as compared to those without retrofitting.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:51:51 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Yu", "Zhiguo", ""], ["Wallace", "Byron C.", ""], ["Johnson", "Todd", ""], ["Cohen", "Trevor", ""]]}, {"id": "1709.07403", "submitter": "Sapna Negi", "authors": "Sapna Negi, Paul Buitelaar", "title": "Inducing Distant Supervision in Suggestion Mining through Part-of-Speech\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining suggestion expressing sentences from a given text is a less\ninvestigated sentence classification task, and therefore lacks hand labeled\nbenchmark datasets. In this work, we propose and evaluate two approaches for\ndistant supervision in suggestion mining. The distant supervision is obtained\nthrough a large silver standard dataset, constructed using the text from\nwikiHow and Wikipedia. Both the approaches use a LSTM based neural network\narchitecture to learn a classification model for suggestion mining, but vary in\ntheir method to use the silver standard dataset. The first approach directly\ntrains the classifier using this dataset, while the second approach only learns\nword embeddings from this dataset. In the second approach, we also learn POS\nembeddings, which interestingly gives the best classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 16:40:46 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 14:49:27 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Negi", "Sapna", ""], ["Buitelaar", "Paul", ""]]}, {"id": "1709.07432", "submitter": "Benjamin Krause", "authors": "Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals", "title": "Dynamic Evaluation of Neural Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methodology for using dynamic evaluation to improve neural\nsequence models. Models are adapted to recent history via a gradient descent\nbased mechanism, causing them to assign higher probabilities to re-occurring\nsequential patterns. Dynamic evaluation outperforms existing adaptation\napproaches in our comparisons. Dynamic evaluation improves the state-of-the-art\nword-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1\nand 44.3 respectively, and the state-of-the-art character-level cross-entropies\non the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 17:50:04 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 22:31:34 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Krause", "Ben", ""], ["Kahembwe", "Emmanuel", ""], ["Murray", "Iain", ""], ["Renals", "Steve", ""]]}, {"id": "1709.07434", "submitter": "Preeti Bhargava", "authors": "Guoning Hu, Preeti Bhargava, Saul Fuhrmann, Sarah Ellinger, Nemanja\n  Spasojevic", "title": "Analyzing users' sentiment towards popular consumer industries and\n  brands on Twitter", "comments": "8 pages, 11 figures, 1 table, 2017 IEEE International Conference on\n  Data Mining Workshops (ICDMW 2017), ICDM Sentiment Elicitation from Natural\n  Text for Information Retrieval and Extraction (ICDM SENTIRE) 2017 workshop", "journal-ref": "2017 IEEE International Conference on Data Mining Workshops (ICDMW\n  2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media serves as a unified platform for users to express their thoughts\non subjects ranging from their daily lives to their opinion on consumer brands\nand products. These users wield an enormous influence in shaping the opinions\nof other consumers and influence brand perception, brand loyalty and brand\nadvocacy. In this paper, we analyze the opinion of 19M Twitter users towards 62\npopular industries, encompassing 12,898 enterprise and consumer brands, as well\nas associated subject matter topics, via sentiment analysis of 330M tweets over\na period spanning a month. We find that users tend to be most positive towards\nmanufacturing and most negative towards service industries. In addition, they\ntend to be more positive or negative when interacting with brands than\ngenerally on Twitter. We also find that sentiment towards brands within an\nindustry varies greatly and we demonstrate this using two industries as use\ncases. In addition, we discover that there is no strong correlation between\ntopic sentiments of different industries, demonstrating that topic sentiments\nare highly dependent on the context of the industry that they are mentioned in.\nWe demonstrate the value of such an analysis in order to assess the impact of\nbrands on social media. We hope that this initial study will prove valuable for\nboth researchers and companies in understanding users' perception of\nindustries, brands and associated topics and encourage more research in this\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 17:50:13 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Hu", "Guoning", ""], ["Bhargava", "Preeti", ""], ["Fuhrmann", "Saul", ""], ["Ellinger", "Sarah", ""], ["Spasojevic", "Nemanja", ""]]}, {"id": "1709.07470", "submitter": "Arpita Roy", "authors": "Arpita Roy, Youngja Park, SHimei Pan", "title": "Learning Domain-Specific Word Embeddings from Sparse Cybersecurity Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding is a Natural Language Processing (NLP) technique that\nautomatically maps words from a vocabulary to vectors of real numbers in an\nembedding space. It has been widely used in recent years to boost the\nperformance of a vari-ety of NLP tasks such as Named Entity Recognition,\nSyntac-tic Parsing and Sentiment Analysis. Classic word embedding methods such\nas Word2Vec and GloVe work well when they are given a large text corpus. When\nthe input texts are sparse as in many specialized domains (e.g.,\ncybersecurity), these methods often fail to produce high-quality vectors. In\nthis pa-per, we describe a novel method to train domain-specificword embeddings\nfrom sparse texts. In addition to domain texts, our method also leverages\ndiverse types of domain knowledge such as domain vocabulary and semantic\nrelations. Specifi-cally, we first propose a general framework to encode\ndiverse types of domain knowledge as text annotations. Then we de-velop a novel\nWord Annotation Embedding (WAE) algorithm to incorporate diverse types of text\nannotations in word em-bedding. We have evaluated our method on two\ncybersecurity text corpora: a malware description corpus and a Common\nVulnerability and Exposure (CVE) corpus. Our evaluation re-sults have\ndemonstrated the effectiveness of our method in learning domain-specific word\nembeddings.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 18:17:36 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Roy", "Arpita", ""], ["Park", "Youngja", ""], ["Pan", "SHimei", ""]]}, {"id": "1709.07484", "submitter": "Preslav Nakov", "authors": "Ahmed Ali, Preslav Nakov, Peter Bell, Steve Renals", "title": "WERd: Using Social Text Spelling Variants for Evaluating Dialectal\n  Speech Recognition", "comments": "ASRU-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of evaluating automatic speech recognition (ASR) systems\nthat target dialectal speech input. A major challenge in this case is that the\northography of dialects is typically not standardized. From an ASR evaluation\nperspective, this means that there is no clear gold standard for the expected\noutput, and several possible outputs could be considered correct according to\ndifferent human annotators, which makes standard word error rate (WER)\ninadequate as an evaluation metric. Such a situation is typical for machine\ntranslation (MT), and thus we borrow ideas from an MT evaluation metric, namely\nTERp, an extension of translation error rate which is closely-related to WER.\nIn particular, in the process of comparing a hypothesis to a reference, we make\nuse of spelling variants for words and phrases, which we mine from Twitter in\nan unsupervised fashion. Our experiments with evaluating ASR output for\nEgyptian Arabic, and further manual analysis, show that the resulting WERd\n(i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER\nfor evaluating dialectal ASR.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 18:37:36 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Ali", "Ahmed", ""], ["Nakov", "Preslav", ""], ["Bell", "Peter", ""], ["Renals", "Steve", ""]]}, {"id": "1709.07642", "submitter": "Wenhao Zheng", "authors": "Wenhao Zheng, Hong-Yu Zhou, Ming Li and Jianxin Wu", "title": "Code Attention: Translating Code to Comments by Exploiting Domain\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate comments of code snippets provide insight for code functionality,\nwhich are helpful for program comprehension. However, due to the great cost of\nauthoring with the comments, many code projects do not contain adequate\ncomments. Automatic comment generation techniques have been proposed to\ngenerate comments from pieces of code in order to alleviate the human efforts\nin annotating the code. Most existing approaches attempt to exploit certain\ncorrelations (usually manually given) between code and generated comments,\nwhich could be easily violated if the coding patterns change and hence the\nperformance of comment generation declines. In this paper, we first build\nC2CGit, a large dataset from open projects in GitHub, which is more than\n20$\\times$ larger than existing datasets. Then we propose a new attention\nmodule called Code Attention to translate code to comments, which is able to\nutilize the domain features of code snippets, such as symbols and identifiers.\nWe make ablation studies to determine effects of different parts in Code\nAttention. Experimental results demonstrate that the proposed module has better\nperformance over existing approaches in both BLEU and METEOR.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 09:08:47 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 11:32:04 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zheng", "Wenhao", ""], ["Zhou", "Hong-Yu", ""], ["Li", "Ming", ""], ["Wu", "Jianxin", ""]]}, {"id": "1709.07758", "submitter": "Farhana Ferdousi Liza", "authors": "Farhana Ferdousi Liza and Marek Grzes", "title": "Improving Language Modelling with Noise-contrastive estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models do not scale well when the vocabulary is large.\nNoise-contrastive estimation (NCE) is a sampling-based method that allows for\nfast learning with large vocabularies. Although NCE has shown promising\nperformance in neural machine translation, it was considered to be an\nunsuccessful approach for language modelling. A sufficient investigation of the\nhyperparameters in the NCE-based neural language models was also missing. In\nthis paper, we showed that NCE can be a successful approach in neural language\nmodelling when the hyperparameters of a neural network are tuned appropriately.\nWe introduced the 'search-then-converge' learning rate schedule for NCE and\ndesigned a heuristic that specifies how to use this schedule. The impact of the\nother important hyperparameters, such as the dropout rate and the weight\ninitialisation range, was also demonstrated. We showed that appropriate tuning\nof NCE-based neural language models outperforms the state-of-the-art\nsingle-model methods on a popular benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 13:59:17 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Liza", "Farhana Ferdousi", ""], ["Grzes", "Marek", ""]]}, {"id": "1709.07777", "submitter": "Ji Wen", "authors": "Ji Wen", "title": "Sentence Correction Based on Large-scale Language Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the further development of informatization, more and more data is stored\nin the form of text. There are some loss of text during their generation and\ntransmission. The paper aims to establish a language model based on the\nlarge-scale corpus to complete the restoration of missing text. In this paper,\nwe introduce a novel measurement to find the missing words, and a way of\nestablishing a comprehensive candidate lexicon to insert the correct choice of\nwords. The paper also introduces some effective optimization methods, which\nlargely improve the efficiency of the text restoration and shorten the time of\ndealing with 1000 sentences into 3.6 seconds. \\keywords{ language model,\nsentence correction, word imputation, parallel optimization\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:28:44 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 07:46:11 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Wen", "Ji", ""]]}, {"id": "1709.07809", "submitter": "Philipp Koehn", "authors": "Philipp Koehn", "title": "Neural Machine Translation", "comments": "100+ pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges. Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:28:24 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Koehn", "Philipp", ""]]}, {"id": "1709.07814", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura", "title": "Attention-based Wav2Text with Feature Transfer Learning", "comments": "Accepted at ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional automatic speech recognition (ASR) typically performs\nmulti-level pattern recognition tasks that map the acoustic speech waveform\ninto a hierarchy of speech units. But, it is widely known that information loss\nin the earlier stage can propagate through the later stages. After the\nresurgence of deep learning, interest has emerged in the possibility of\ndeveloping a purely end-to-end ASR system from the raw waveform to the\ntranscription without any predefined alignments and hand-engineered models.\nHowever, the successful attempts in end-to-end architecture still used\nspectral-based features, while the successful attempts in using raw waveform\nwere still based on the hybrid deep neural network - Hidden Markov model\n(DNN-HMM) framework. In this paper, we construct the first end-to-end\nattention-based encoder-decoder model to process directly from raw speech\nwaveform to the text transcription. We called the model as \"Attention-based\nWav2Text\". To assist the training process of the end-to-end model, we propose\nto utilize a feature transfer learning. Experimental results also reveal that\nthe proposed Attention-based Wav2Text model directly with raw waveform could\nachieve a better result in comparison with the attentional encoder-decoder\nmodel trained on standard front-end filterbank features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:38:09 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1709.07840", "submitter": "Igor Shalyminov", "authors": "Igor Shalyminov, Arash Eshghi, Oliver Lemon", "title": "Challenging Neural Dialogue Models with Natural Data: Memory Networks\n  Fail on Incremental Phenomena", "comments": "9 pages, 3 figures, 2 tables. Accepted as a full paper for SemDial\n  2017", "journal-ref": "Proceedings of the 21st Workshop on the Semantics and Pragmatics\n  of Dialogue (ISSN 2308-2275), pp 125-133. Saarbrucken, Germany, 15-17 August\n  2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural, spontaneous dialogue proceeds incrementally on a word-by-word basis;\nand it contains many sorts of disfluency such as mid-utterance/sentence\nhesitations, interruptions, and self-corrections. But training data for machine\nlearning approaches to dialogue processing is often either cleaned-up or wholly\nsynthetic in order to avoid such phenomena. The question then arises of how\nwell systems trained on such clean data generalise to real spontaneous\ndialogue, or indeed whether they are trainable at all on naturally occurring\ndialogue data. To answer this question, we created a new corpus called bAbI+ by\nsystematically adding natural spontaneous incremental dialogue phenomena such\nas restarts and self-corrections to the Facebook AI Research's bAbI dialogues\ndataset. We then explore the performance of a state-of-the-art retrieval model,\nMemN2N, on this more natural dataset. Results show that the semantic accuracy\nof the MemN2N model drops drastically; and that although it is in principle\nable to learn to process the constructions in bAbI+, it needs an impractical\namount of training data to do so. Finally, we go on to show that an\nincremental, semantic parser -- DyLan -- shows 100% semantic accuracy on both\nbAbI and bAbI+, highlighting the generalisation properties of linguistically\ninformed dialogue models.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 16:43:48 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Shalyminov", "Igor", ""], ["Eshghi", "Arash", ""], ["Lemon", "Oliver", ""]]}, {"id": "1709.07858", "submitter": "Igor Shalyminov", "authors": "Arash Eshghi, Igor Shalyminov, Oliver Lemon", "title": "Bootstrapping incremental dialogue systems from minimal data: the\n  generalisation power of dialogue grammars", "comments": "11 pages, 4 figures, 2 tables. Accepted as a long paper for EMNLP\n  2017", "journal-ref": "Proceedings of the 2017 Conference on Empirical Methods in Natural\n  Language Processing (ISBN 978-1-945626-83-8), pp 2210-2220. Copenhagen,\n  Denmark September 7-11, 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an end-to-end method for automatically inducing task-based\ndialogue systems from small amounts of unannotated dialogue data. It combines\nan incremental semantic grammar - Dynamic Syntax and Type Theory with Records\n(DS-TTR) - with Reinforcement Learning (RL), where language generation and\ndialogue management are a joint decision problem. The systems thus produced are\nincremental: dialogues are processed word-by-word, shown previously to be\nessential in supporting natural, spontaneous dialogue. We hypothesised that the\nrich linguistic knowledge within the grammar should enable a combinatorially\nlarge number of dialogue variations to be processed, even when trained on very\nfew dialogues. Our experiments show that our model can process 74% of the\nFacebook AI bAbI dataset even when trained on only 0.13% of the data (5\ndialogues). It can in addition process 65% of bAbI+, a corpus we created by\nsystematically adding incremental dialogue phenomena such as restarts and\nself-corrections to bAbI. We compare our model with a state-of-the-art\nretrieval model, MemN2N. We find that, in terms of semantic accuracy, MemN2N\nshows very poor robustness to the bAbI+ transformations even when trained on\nthe full bAbI dataset.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:24:33 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Eshghi", "Arash", ""], ["Shalyminov", "Igor", ""], ["Lemon", "Oliver", ""]]}, {"id": "1709.07862", "submitter": "Pin-Jung Chen", "authors": "Pin-Jung Chen, I-Hung Hsu, Yi-Yao Huang, Hung-Yi Lee", "title": "Mitigating the Impact of Speech Recognition Errors on Chatbot using\n  Sequence-to-Sequence Model", "comments": "Accepted at ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply sequence-to-sequence model to mitigate the impact of speech\nrecognition errors on open domain end-to-end dialog generation. We cast the\ntask as a domain adaptation problem where ASR transcriptions and original text\nare in two different domains. In this paper, our proposed model includes two\nindividual encoders for each domain data and make their hidden states similar\nto ensure the decoder predict the same dialog text. The method shows that the\nsequence-to-sequence model can learn the ASR transcriptions and original text\npair having the same meaning and eliminate the speech recognition errors.\nExperimental results on Cornell movie dialog dataset demonstrate that the\ndomain adaption system help the spoken dialog system generate more similar\nresponses with the original text answers.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:33:32 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 09:27:03 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Chen", "Pin-Jung", ""], ["Hsu", "I-Hung", ""], ["Huang", "Yi-Yao", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1709.07871", "submitter": "Ethan Perez", "authors": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron\n  Courville", "title": "FiLM: Visual Reasoning with a General Conditioning Layer", "comments": "AAAI 2018. Code available at http://github.com/ethanjperez/film .\n  Extends arXiv:1707.03017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:54:12 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:25:53 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Perez", "Ethan", ""], ["Strub", "Florian", ""], ["de Vries", "Harm", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1709.07902", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, and James Glass", "title": "Unsupervised Learning of Disentangled and Interpretable Representations\n  from Sequential Data", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a factorized hierarchical variational autoencoder, which learns\ndisentangled and interpretable representations from sequential data without\nsupervision. Specifically, we exploit the multi-scale nature of information in\nsequential data by formulating it explicitly within a factorized hierarchical\ngraphical model that imposes sequence-dependent priors and sequence-independent\npriors to different sets of latent variables. The model is evaluated on two\nspeech corpora to demonstrate, qualitatively, its ability to transform speakers\nor linguistic content by manipulating different sets of latent variables; and\nquantitatively, its ability to outperform an i-vector baseline for speaker\nverification and reduce the word error rate by as much as 35% in mismatched\ntrain/test scenarios for automatic speech recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 18:36:50 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1709.07915", "submitter": "Amir Karami", "authors": "George Shaw Jr., and Amir Karami", "title": "Computational Content Analysis of Negative Tweets for Obesity, Diet,\n  Diabetes, and Exercise", "comments": "The 2017 Annual Meeting of the Association for Information Science\n  and Technology (ASIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media based digital epidemiology has the potential to support faster\nresponse and deeper understanding of public health related threats. This study\nproposes a new framework to analyze unstructured health related textual data\nvia Twitter users' post (tweets) to characterize the negative health sentiments\nand non-health related concerns in relations to the corpus of negative\nsentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the\ncollection of 6 million Tweets for one month, this study identified the\nprominent topics of users as it relates to the negative sentiments. Our\nproposed framework uses two text mining methods, sentiment analysis and topic\nmodeling, to discover negative topics. The negative sentiments of Twitter users\nsupport the literature narratives and the many morbidity issues that are\nassociated with DDEO and the linkage between obesity and diabetes. The\nframework offers a potential method to understand the publics' opinions and\nsentiments regarding DDEO. More importantly, this research provides new\nopportunities for computational social scientists, medical experts, and public\nhealth professionals to collectively address DDEO-related issues.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:18:42 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Shaw", "George", "Jr."], ["Karami", "Amir", ""]]}, {"id": "1709.07916", "submitter": "Amir Karami", "authors": "Amir Karami, Alicia A. Dahl, Gabrielle Turner-McGrievy, Hadi Kharrazi,\n  Jr., George Shaw", "title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "comments": "International Journal of Information Management (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media provide a platform for users to express their opinions and share\ninformation. Understanding public health opinions on social media, such as\nTwitter, offers a unique approach to characterizing common health issues such\nas diabetes, diet, exercise, and obesity (DDEO), however, collecting and\nanalyzing a large scale conversational public health data set is a challenging\nresearch task. The goal of this research is to analyze the characteristics of\nthe general public's opinions in regard to diabetes, diet, exercise and obesity\n(DDEO) as expressed on Twitter. A multi-component semantic and linguistic\nframework was developed to collect Twitter data, discover topics of interest\nabout DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8%\nof tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity.\nThe strongest correlation among the topics was determined between exercise and\nobesity. Other notable correlations were: diabetes and obesity, and diet and\nobesity DDEO terms were also identified as subtopics of each of the DDEO\ntopics. The frequent subtopics discussed along with Diabetes, excluding the\nDDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer.\nThe non-DDEO subtopics for Diet included vegetarian, pregnancy, celebrities,\nweight loss, religious, and mental health, while subtopics for Exercise\nincluded computer games, brain, fitness, and daily plan. Non-DDEO subtopics for\nObesity included Alzheimer, cancer, and children. With 2.67 billion social\nmedia users in 2016, publicly available data such as Twitter posts can be\nutilized to support clinical providers, public health experts, and social\nscientists in better understanding common public opinions in regard to\ndiabetes, diet, exercise, and obesity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:19:49 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Karami", "Amir", ""], ["Dahl", "Alicia A.", ""], ["Turner-McGrievy", "Gabrielle", ""], ["Kharrazi,", "Hadi", "Jr."], ["Shaw", "George", ""]]}, {"id": "1709.08011", "submitter": "Mamoru Komachi", "authors": "Yoshiaki Kitagawa and Mamoru Komachi", "title": "Long Short-Term Memory for Japanese Word Segmentation", "comments": "10 pages; PACLIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This study presents a Long Short-Term Memory (LSTM) neural network approach\nto Japanese word segmentation (JWS). Previous studies on Chinese word\nsegmentation (CWS) succeeded in using recurrent neural networks such as LSTM\nand gated recurrent units (GRU). However, in contrast to Chinese, Japanese\nincludes several character types, such as hiragana, katakana, and kanji, that\nproduce orthographic variations and increase the difficulty of word\nsegmentation. Additionally, it is important for JWS tasks to consider a global\ncontext, and yet traditional JWS approaches rely on local features. In order to\naddress this problem, this study proposes employing an LSTM-based approach to\nJWS. The experimental results indicate that the proposed model achieves\nstate-of-the-art accuracy with respect to various Japanese corpora.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 06:15:37 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 20:26:49 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 03:52:05 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Kitagawa", "Yoshiaki", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1709.08074", "submitter": "Md Faisal Mahbub Chowdhury", "authors": "Michael R. Glass, Md Faisal Mahbub Chowdhury and Alfio M. Gliozzo", "title": "Language Independent Acquisition of Abbreviations", "comments": "9 pages, 7 figues, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses automatic extraction of abbreviations (encompassing\nacronyms and initialisms) and corresponding long-form expansions from plain\nunstructured text. We create and are going to release a multilingual resource\nfor abbreviations and their corresponding expansions, built automatically by\nexploiting Wikipedia redirect and disambiguation pages, that can be used as a\nbenchmark for evaluation. We address a shortcoming of previous work where only\nthe redirect pages were used, and so every abbreviation had only a single\nexpansion, even though multiple different expansions are possible for many of\nthe abbreviations. We also develop a principled machine learning based approach\nto scoring expansion candidates using different techniques such as indicators\nof near synonymy, topical relatedness, and surface similarity. We show improved\nperformance over seven languages, including two with a non-Latin alphabet,\nrelative to strong baselines.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 16:43:31 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Glass", "Michael R.", ""], ["Chowdhury", "Md Faisal Mahbub", ""], ["Gliozzo", "Alfio M.", ""]]}, {"id": "1709.08196", "submitter": "Johannes Gra\\\"en", "authors": "Johannes Gra\\\"en", "title": "Identifying Phrasemes via Interlingual Association Measures -- A\n  Data-driven Approach on Dependency-parsed and Word-aligned Parallel Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a preprint of the article \"Identifying Phrasemes via Interlingual\nAssociation Measures\" that was presented in February 2016 at the LeKo (Lexical\ncombinations and typified speech in a multilingual context) conference in\nInnsbruck.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 13:48:36 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Gra\u00ebn", "Johannes", ""]]}, {"id": "1709.08267", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari\n  Meimandi, Matthew S. Gerber, Laura E. Barnes", "title": "HDLTex: Hierarchical Deep Learning for Text Classification", "comments": "ICMLA 2017", "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-134", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continually increasing number of documents produced each year\nnecessitates ever improving information processing methods for searching,\nretrieving, and organizing text. Central to these information processing\nmethods is document classification, which has become an important application\nfor supervised learning. Recently the performance of these traditional\nclassifiers has degraded as the number of documents has increased. This is\nbecause along with this growth in the number of documents has come an increase\nin the number of categories. This paper approaches this problem differently\nfrom current document classification methods that view the problem as\nmulti-class classification. Instead we perform hierarchical classification\nusing an approach we call Hierarchical Deep Learning for Text classification\n(HDLTex). HDLTex employs stacks of deep learning architectures to provide\nspecialized understanding at each level of the document hierarchy.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 21:58:12 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 18:16:31 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Heidarysafa", "Mojtaba", ""], ["Meimandi", "Kiana Jafari", ""], ["Gerber", "Matthew S.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1709.08294", "submitter": "Dinghan Shen", "authors": "Dinghan Shen, Martin Renqiang Min, Yitong Li, Lawrence Carin", "title": "Learning Context-Sensitive Convolutional Filters for Text Processing", "comments": "Accepted by EMNLP 2018 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have recently emerged as a popular\nbuilding block for natural language processing (NLP). Despite their success,\nmost existing CNN models employed in NLP share the same learned (and static)\nset of filters for all input sentences. In this paper, we consider an approach\nof using a small meta network to learn context-sensitive convolutional filters\nfor text processing. The role of meta network is to abstract the contextual\ninformation of a sentence or document into a set of input-aware filters. We\nfurther generalize this framework to model sentence pairs, where a\nbidirectional filter generation mechanism is introduced to encapsulate\nco-dependent sentence representations. In our benchmarks on four different\ntasks, including ontology classification, sentiment analysis, answer sentence\nselection, and paraphrase identification, our proposed model, a modified CNN\nwith context-sensitive filters, consistently outperforms the standard CNN and\nattention-based CNN baselines. By visualizing the learned context-sensitive\nfilters, we further validate and rationalize the effectiveness of proposed\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 02:29:26 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 04:15:40 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 16:29:50 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Shen", "Dinghan", ""], ["Min", "Martin Renqiang", ""], ["Li", "Yitong", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.08299", "submitter": "Yiming Cui", "authors": "Yiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shijin Wang and Guoping\n  Hu", "title": "Dataset for the First Evaluation on Chinese Machine Reading\n  Comprehension", "comments": "5 pages, published at LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Reading Comprehension (MRC) has become enormously popular recently\nand has attracted a lot of attention. However, existing reading comprehension\ndatasets are mostly in English. To add diversity in reading comprehension\ndatasets, in this paper we propose a new Chinese reading comprehension dataset\nfor accelerating related research in the community. The proposed dataset\ncontains two different types: cloze-style reading comprehension and user query\nreading comprehension, associated with large-scale training data as well as\nhuman-annotated validation and hidden test set. Along with this dataset, we\nalso hosted the first Evaluation on Chinese Machine Reading Comprehension\n(CMRC-2017) and successfully attracted tens of participants, which suggest the\npotential impact of this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 03:14:01 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 09:22:50 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Chen", "Zhipeng", ""], ["Ma", "Wentao", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1709.08366", "submitter": "Anush Sankaran", "authors": "Vitobha Munigala, Srikanth Tamilselvam, Anush Sankaran", "title": "\"Let me convince you to buy my product ... \": A Case Study of an\n  Automated Persuasive System for Fashion Products", "comments": "ML4Creativity workshop at SIGKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persuasivenes is a creative art aimed at making people believe in certain set\nof beliefs. Many a times, such creativity is about adapting richness of one\ndomain into another to strike a chord with the target audience. In this\nresearch, we present PersuAIDE! - A persuasive system based on linguistic\ncreativity to transform given sentence to generate various forms of persuading\nsentences. These various forms cover multiple focus of persuasion such as\nmemorability and sentiment. For a given simple product line, the algorithm is\ncomposed of several steps including: (i) select an appropriate well-known\nexpression for the target domain to add memorability, (ii) identify keywords\nand entities in the given sentence and expression and transform it to produce\ncreative persuading sentence, and (iii) adding positive or negative sentiment\nfor further persuasion. The persuasive conversion were manually verified using\nqualitative results and the effectiveness of the proposed approach is\nempirically discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:28:23 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Munigala", "Vitobha", ""], ["Tamilselvam", "Srikanth", ""], ["Sankaran", "Anush", ""]]}, {"id": "1709.08448", "submitter": "Kevin Alex Mathews", "authors": "Kevin Alex Mathews, P Sreenivasa Kumar", "title": "Extracting Ontological Knowledge from Textual Descriptions", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authoring of OWL-DL ontologies is intellectually challenging and to make this\nprocess simpler, many systems accept natural language text as input. A\ntext-based ontology authoring approach can be successful only when it is\ncombined with an effective method for extracting ontological axioms from text.\nExtracting axioms from unrestricted English input is a substantially\nchallenging task due to the richness of the language. Controlled natural\nlanguages (CNLs) have been proposed in this context and these tend to be highly\nrestrictive. In this paper, we propose a new CNL called TEDEI (TExtual\nDEscription Identifier) whose grammar is inspired by the different ways OWL-DL\nconstructs are expressed in English. We built a system that transforms TEDEI\nsentences into corresponding OWL-DL axioms. Now, ambiguity due to different\npossible lexicalizations of sentences and semantic ambiguity present in\nsentences are challenges in this context. We find that the best way to handle\nthese challenges is to construct axioms corresponding to alternative\nformalizations of the sentence so that the end-user can make an appropriate\nchoice. The output is compared against human-authored axioms and in substantial\nnumber of cases, human-authored axiom is indeed one of the alternatives given\nby the system. The proposed system substantially enhances the types of sentence\nstructures that can be used for ontology authoring.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 12:22:51 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 08:41:06 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 19:37:02 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Mathews", "Kevin Alex", ""], ["Kumar", "P Sreenivasa", ""]]}, {"id": "1709.08521", "submitter": "Omar Al-Harbi", "authors": "Omar Al-Harbi", "title": "Using objective words in the reviews to improve the colloquial arabic\n  sentiment analysis", "comments": "14 pages, 1 figure, International Journal on Natural Language\n  Computing (IJNLC)", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  6, No.3, June 2017", "doi": "10.5121/ijnlc", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main difficulties in sentiment analysis of the Arabic language is\nthe presence of the colloquialism. In this paper, we examine the effect of\nusing objective words in conjunction with sentimental words on sentiment\nclassification for the colloquial Arabic reviews, specifically Jordanian\ncolloquial reviews. The reviews often include both sentimental and objective\nwords, however, the most existing sentiment analysis models ignore the\nobjective words as they are considered useless. In this work, we created two\nlexicons: the first includes the colloquial sentimental words and compound\nphrases, while the other contains the objective words associated with values of\nsentiment tendency based on a particular estimation method. We used these\nlexicons to extract sentiment features that would be training input to the\nSupport Vector Machines (SVM) to classify the sentiment polarity of the\nreviews. The reviews dataset have been collected manually from JEERAN website.\nThe results of the experiments show that the proposed approach improves the\npolarity classification in comparison to two baseline models, with accuracy\n95.6%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:40:28 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Al-Harbi", "Omar", ""]]}, {"id": "1709.08600", "submitter": "Maxim Grechkin", "authors": "Maxim Grechkin, Hoifung Poon, Bill Howe", "title": "EZLearn: Exploiting Organic Supervision in Large-Scale Data Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications require automated data annotation, such as\nidentifying tissue origins based on gene expressions and classifying images\ninto semantic categories. Annotation classes are often numerous and subject to\nchanges over time, and annotating examples has become the major bottleneck for\nsupervised learning methods. In science and other high-value domains, large\nrepositories of data samples are often available, together with two sources of\norganic supervision: a lexicon for the annotation classes, and text\ndescriptions that accompany some data samples. Distant supervision has emerged\nas a promising paradigm for exploiting such indirect supervision by\nautomatically annotating examples where the text description contains a class\nmention in the lexicon. However, due to linguistic variations and ambiguities,\nsuch training data is inherently noisy, which limits the accuracy of this\napproach. In this paper, we introduce an auxiliary natural language processing\nsystem for the text modality, and incorporate co-training to reduce noise and\naugment signal in distant supervision. Without using any manually labeled data,\nour EZLearn system learned to accurately annotate data samples in functional\ngenomics and scientific figure comprehension, substantially outperforming\nstate-of-the-art supervised methods trained on tens of thousands of annotated\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 17:10:46 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 16:16:57 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 00:03:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Grechkin", "Maxim", ""], ["Poon", "Hoifung", ""], ["Howe", "Bill", ""]]}, {"id": "1709.08624", "submitter": "Weinan Zhang", "authors": "Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang", "title": "Long Text Generation via Adversarial Training with Leaked Information", "comments": "14 pages, AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating coherent and semantically meaningful text has many\napplications in machine translation, dialogue systems, image captioning, etc.\nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN)\nthat use a discriminative model to guide the training of the generative model\nas a reinforcement learning policy has shown promising results in text\ngeneration. However, the scalar guiding signal is only available after the\nentire text has been generated and lacks intermediate information about text\nstructure during the generative process. As such, it limits its success when\nthe length of the generated text samples is long (more than 20 words). In this\npaper, we propose a new framework, called LeakGAN, to address the problem for\nlong text generation. We allow the discriminative net to leak its own\nhigh-level extracted features to the generative net to further help the\nguidance. The generator incorporates such informative signals into all\ngeneration steps through an additional Manager module, which takes the\nextracted features of current generated words and outputs a latent vector to\nguide the Worker module for next-word generation. Our extensive experiments on\nsynthetic data and various real-world tasks with Turing test demonstrate that\nLeakGAN is highly effective in long text generation and also improves the\nperformance in short text generation scenarios. More importantly, without any\nsupervision, LeakGAN would be able to implicitly learn sentence structures only\nthrough the interaction between Manager and Worker.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 13:35:08 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 18:53:52 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Guo", "Jiaxian", ""], ["Lu", "Sidi", ""], ["Cai", "Han", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "1709.08694", "submitter": "Paulo Cavalin", "authors": "Luciano Barbosa, Paulo R. Cavalin, Victor Guimaraes and Matthias\n  Kormaksson", "title": "Methodology and Results for the Competition on Semantic Similarity\n  Evaluation and Entailment Recognition for PROPOR 2016", "comments": "Original submission in English, further translated to Portuguese and\n  publised at Linguamatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the methodology and the results obtained by our\nteams, dubbed Blue Man Group, in the ASSIN (from the Portuguese {\\it\nAvalia\\c{c}\\~ao de Similaridade Sem\\^antica e Infer\\^encia Textual})\ncompetition, held at PROPOR 2016\\footnote{International Conference on the\nComputational Processing of the Portuguese Language -\nhttp://propor2016.di.fc.ul.pt/}. Our team's strategy consisted of evaluating\nmethods based on semantic word vectors, following two distinct directions: 1)\nto make use of low-dimensional, compact, feature sets, and 2) deep\nlearning-based strategies dealing with high-dimensional feature vectors.\nEvaluation results demonstrated that the first strategy was more promising, so\nthat the results from the second strategy have been discarded. As a result, by\nconsidering the best run of each of the six teams, we have been able to achieve\nthe best accuracy and F1 values in entailment recognition, in the Brazilian\nPortuguese set, and the best F1 score overall. In the semantic similarity task,\nour team was ranked second in the Brazilian Portuguese set, and third\nconsidering both sets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 18:02:51 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Barbosa", "Luciano", ""], ["Cavalin", "Paulo R.", ""], ["Guimaraes", "Victor", ""], ["Kormaksson", "Matthias", ""]]}, {"id": "1709.08698", "submitter": "Boya Yu", "authors": "Boya Yu, Jiaxu Zhou, Yi Zhang, Yunong Cao", "title": "Identifying Restaurant Features via Sentiment Analysis on Yelp Reviews", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people use Yelp to find a good restaurant. Nonetheless, with only an\noverall rating for each restaurant, Yelp offers not enough information for\nindependently judging its various aspects such as environment, service or\nflavor. In this paper, we introduced a machine learning based method to\ncharacterize such aspects for particular types of restaurants. The main\napproach used in this paper is to use a support vector machine (SVM) model to\ndecipher the sentiment tendency of each review from word frequency. Word scores\ngenerated from the SVM models are further processed into a polarity index\nindicating the significance of each word for special types of restaurant.\nCustomers overall tend to express more sentiment regarding service. As for the\ndistinction between different cuisines, results that match the common sense are\nobtained: Japanese cuisines are usually fresh, some French cuisines are\noverpriced while Italian Restaurants are often famous for their pizzas.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 03:51:19 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Yu", "Boya", ""], ["Zhou", "Jiaxu", ""], ["Zhang", "Yi", ""], ["Cao", "Yunong", ""]]}, {"id": "1709.08716", "submitter": "Lei Shu", "authors": "Lei Shu and Hu Xu and Bing Liu", "title": "DOC: Deep Open Classification of Text Documents", "comments": "accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional supervised learning makes the closed-world assumption that the\nclasses appeared in the test data must have appeared in training. This also\napplies to text learning or text classification. As learning is used\nincreasingly in dynamic open environments where some new/test documents may not\nbelong to any of the training classes, identifying these novel documents during\nclassification presents an important problem. This problem is called open-world\nclassification or open classification. This paper proposes a novel deep\nlearning based approach. It outperforms existing state-of-the-art techniques\ndramatically.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 20:36:42 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Shu", "Lei", ""], ["Xu", "Hu", ""], ["Liu", "Bing", ""]]}, {"id": "1709.08853", "submitter": "Xianggen Liu", "authors": "Zhengdong Lu and Xianggen Liu and Haotian Cui and Yukun Yan and Daqi\n  Zheng", "title": "Object-oriented Neural Programming (OONP) for Document Understanding", "comments": "accepted by ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Object-oriented Neural Programming (OONP), a framework for\nsemantically parsing documents in specific domains. Basically, OONP reads a\ndocument and parses it into a predesigned object-oriented data structure\n(referred to as ontology in this paper) that reflects the domain-specific\nsemantics of the document. An OONP parser models semantic parsing as a decision\nprocess: a neural net-based Reader sequentially goes through the document, and\nduring the process it builds and updates an intermediate ontology to summarize\nits partial understanding of the text it covers. OONP supports a rich family of\noperations (both symbolic and differentiable) for composing the ontology, and a\nbig variety of forms (both symbolic and differentiable) for representing the\nstate and the document. An OONP parser can be trained with supervision of\ndifferent forms and strength, including supervised learning (SL) ,\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\nsynthetic and real-world document parsing tasks have shown that OONP can learn\nto handle fairly complicated ontology with training data of modest sizes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 06:17:35 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 06:56:18 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 15:07:54 GMT"}, {"version": "v4", "created": "Sun, 8 Oct 2017 07:36:03 GMT"}, {"version": "v5", "created": "Thu, 19 Jul 2018 11:21:07 GMT"}, {"version": "v6", "created": "Wed, 25 Jul 2018 08:56:09 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Lu", "Zhengdong", ""], ["Liu", "Xianggen", ""], ["Cui", "Haotian", ""], ["Yan", "Yukun", ""], ["Zheng", "Daqi", ""]]}, {"id": "1709.08858", "submitter": "Mitsuo Yoshida", "authors": "Kana Oomoto, Haruka Oikawa, Eiko Yamamoto, Mitsuo Yoshida, Masayuki\n  Okabe, Kyoji Umemura", "title": "Polysemy Detection in Distributed Representation of Word Sense", "comments": "The 9th International Conference on Knowledge and Smart Technology\n  (KST-2017)", "journal-ref": null, "doi": "10.1109/KST.2017.7886073", "report-no": null, "categories": "cs.DS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a statistical test to determine whether a given\nword is used as a polysemic word or not. The statistic of the word in this test\nroughly corresponds to the fluctuation in the senses of the neighboring words a\nnd the word itself. Even though the sense of a word corresponds to a single\nvector, we discuss how polysemy of the words affects the position of vectors.\nFinally, we also explain the method to detect this effect.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 06:51:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Oomoto", "Kana", ""], ["Oikawa", "Haruka", ""], ["Yamamoto", "Eiko", ""], ["Yoshida", "Mitsuo", ""], ["Okabe", "Masayuki", ""], ["Umemura", "Kyoji", ""]]}, {"id": "1709.08878", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, Percy Liang", "title": "Generating Sentences by Editing Prototypes", "comments": "14 pages, Transactions of the Association for Computational\n  Linguistics (TACL), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generative model of sentences that first samples a prototype\nsentence from the training corpus and then edits it into a new sentence.\nCompared to traditional models that generate from scratch either left-to-right\nor by first sampling a latent sentence vector, our prototype-then-edit model\nimproves perplexity on language modeling and generates higher quality outputs\naccording to human evaluation. Furthermore, the model gives rise to a latent\nedit vector that captures interpretable semantics such as sentence similarity\nand sentence-level analogies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 08:11:33 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 04:57:15 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Guu", "Kelvin", ""], ["Hashimoto", "Tatsunori B.", ""], ["Oren", "Yonatan", ""], ["Liang", "Percy", ""]]}, {"id": "1709.08898", "submitter": "Gyu Hyeon Choi", "authors": "Gyu-Hyeon Choi, Jong-Hun Shin and Young-Kil Kim", "title": "Improving a Multi-Source Neural Machine Translation Model with Corpus\n  Extension for Low-Resource Languages", "comments": "5 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine translation, we often try to collect resources to improve\nperformance. However, most of the language pairs, such as Korean-Arabic and\nKorean-Vietnamese, do not have enough resources to train machine translation\nsystems. In this paper, we propose the use of synthetic methods for extending a\nlow-resource corpus and apply it to a multi-source neural machine translation\nmodel. We showed the improvement of machine translation performance through\ncorpus extension using the synthetic method. We specifically focused on how to\ncreate source sentences that can make better target sentences, including the\nuse of synthetic methods. We found that the corpus extension could also improve\nthe performance of multi-source neural machine translation. We showed the\ncorpus extension and multi-source model to be efficient methods for a\nlow-resource language pair. Furthermore, when both methods were used together,\nwe found better machine translation performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 09:04:29 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 05:26:45 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Choi", "Gyu-Hyeon", ""], ["Shin", "Jong-Hun", ""], ["Kim", "Young-Kil", ""]]}, {"id": "1709.08907", "submitter": "Sho Takase", "authors": "Sho Takase, Jun Suzuki and Masaaki Nagata", "title": "Input-to-Output Gate to Improve RNN Language Models", "comments": "Accepted as a conference paper in IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a reinforcing method that refines the output layers of\nexisting Recurrent Neural Network (RNN) language models. We refer to our\nproposed method as Input-to-Output Gate (IOG). IOG has an extremely simple\nstructure, and thus, can be easily combined with any RNN language models. Our\nexperiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG\nconsistently boosts the performance of several different types of current\ntopline RNN language models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 09:28:49 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 06:40:39 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Takase", "Sho", ""], ["Suzuki", "Jun", ""], ["Nagata", "Masaaki", ""]]}, {"id": "1709.09118", "submitter": "Qiuyuan Huang", "authors": "Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu", "title": "Tensor Product Generation Networks for Deep NLP Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the design of deep networks for natural language\nprocessing (NLP), based on the general technique of Tensor Product\nRepresentations (TPRs) for encoding and processing symbol structures in\ndistributed neural networks. A network architecture --- the Tensor Product\nGeneration Network (TPGN) --- is proposed which is capable in principle of\ncarrying out TPR computation, but which uses unconstrained deep learning to\ndesign its internal representations. Instantiated in a model for image-caption\ngeneration, TPGN outperforms LSTM baselines when evaluated on the COCO dataset.\nThe TPR-capable structure enables interpretation of internal representations\nand operations, which prove to contain considerable grammatical content. Our\ncaption-generation model can be interpreted as generating sequences of\ngrammatical categories and retrieving words by their categories from a plan\nencoded as a distributed representation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:32:20 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 07:50:59 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 02:45:39 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 06:46:31 GMT"}, {"version": "v5", "created": "Sat, 16 Dec 2017 12:01:09 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Huang", "Qiuyuan", ""], ["Smolensky", "Paul", ""], ["He", "Xiaodong", ""], ["Deng", "Li", ""], ["Wu", "Dapeng", ""]]}, {"id": "1709.09119", "submitter": "Paul Christian Sommerhoff", "authors": "Paul Christian Sommerhoff", "title": "Integration of Japanese Papers Into the DBLP Data Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If someone is looking for a certain publication in the field of computer\nscience, the searching person is likely to use the DBLP to find the desired\npublication. The DBLP data set is continuously extended with new publications,\nor rather their metadata, for example the names of involved authors, the title\nand the publication date. While the size of the data set is already remarkable,\nspecific areas can still be improved. The DBLP offers a huge collection of\nEnglish papers because most papers concerning computer science are published in\nEnglish. Nevertheless, there are official publications in other languages which\nare supposed to be added to the data set. One kind of these are Japanese\npapers. This diploma thesis will show a way to automatically process\npublication lists of Japanese papers and to make them ready for an import into\nthe DBLP data set. Especially important are the problems along the way of\nprocessing, such as transcription handling and Personal Name Matching with\nJapanese names.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:33:59 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Sommerhoff", "Paul Christian", ""]]}, {"id": "1709.09220", "submitter": "Athanasios Giannakopoulos", "authors": "Athanasios Giannakopoulos, Diego Antognini, Claudiu Musat, Andreea\n  Hossmann and Michael Baeriswyl", "title": "Dataset Construction via Attention for Aspect Term Extraction with\n  Distant Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Term Extraction (ATE) detects opinionated aspect terms in sentences or\ntext spans, with the end goal of performing aspect-based sentiment analysis.\nThe small amount of available datasets for supervised ATE and the fact that\nthey cover only a few domains raise the need for exploiting other data sources\nin new and creative ways. Publicly available review corpora contain a plethora\nof opinionated aspect terms and cover a larger domain spectrum. In this paper,\nwe first propose a method for using such review corpora for creating a new\ndataset for ATE. Our method relies on an attention mechanism to select\nsentences that have a high likelihood of containing actual opinionated aspects.\nWe thus improve the quality of the extracted aspects. We then use the\nconstructed dataset to train a model and perform ATE with distant supervision.\nBy evaluating on human annotated datasets, we prove that our method achieves a\nsignificantly improved performance over various unsupervised and supervised\nbaselines. Finally, we prove that sentence selection matters when it comes to\ncreating new datasets for ATE. Specifically, we show that, using a set of\nselected sentences leads to higher ATE performance compared to using the whole\nsentence set.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 18:54:39 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Giannakopoulos", "Athanasios", ""], ["Antognini", "Diego", ""], ["Musat", "Claudiu", ""], ["Hossmann", "Andreea", ""], ["Baeriswyl", "Michael", ""]]}, {"id": "1709.09239", "submitter": "Matthias Hartung", "authors": "Hendrik ter Horst, Matthias Hartung, Roman Klinger, Matthias Zwick,\n  Philipp Cimiano", "title": "Predicting Disease-Gene Associations using Cross-Document Graph-based\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of personalized medicine, text mining methods pose an\ninteresting option for identifying disease-gene associations, as they can be\nused to generate novel links between diseases and genes which may complement\nknowledge from structured databases. The most straightforward approach to\nextract such links from text is to rely on a simple assumption postulating an\nassociation between all genes and diseases that co-occur within the same\ndocument. However, this approach (i) tends to yield a number of spurious\nassociations, (ii) does not capture different relevant types of associations,\nand (iii) is incapable of aggregating knowledge that is spread across\ndocuments. Thus, we propose an approach in which disease-gene co-occurrences\nand gene-gene interactions are represented in an RDF graph. A machine\nlearning-based classifier is trained that incorporates features extracted from\nthe graph to separate disease-gene pairs into valid disease-gene associations\nand spurious ones. On the manually curated Genetic Testing Registry, our\napproach yields a 30 points increase in F1 score over a plain co-occurrence\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 19:59:16 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["ter Horst", "Hendrik", ""], ["Hartung", "Matthias", ""], ["Klinger", "Roman", ""], ["Zwick", "Matthias", ""], ["Cimiano", "Philipp", ""]]}, {"id": "1709.09250", "submitter": "Omar Al-Harbi Mohammad", "authors": "Omar Al-Harbi, Shaidah Jusoh, Norita Md Norwawi", "title": "Lexical Disambiguation in Natural Language Questions (NLQs)", "comments": "8 pages, 4 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No 2, July 2011 (143-150)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question processing is a fundamental step in a question answering (QA)\napplication, and its quality impacts the performance of QA application. The\nmajor challenging issue in processing question is how to extract semantic of\nnatural language questions (NLQs). A human language is ambiguous. Ambiguity may\noccur at two levels; lexical and syntactic. In this paper, we propose a new\napproach for resolving lexical ambiguity problem by integrating context\nknowledge and concepts knowledge of a domain, into shallow natural language\nprocessing (SNLP) techniques. Concepts knowledge is modeled using ontology,\nwhile context knowledge is obtained from WordNet, and it is determined based on\nneighborhood words in a question. The approach will be applied to a university\nQA system.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 20:24:10 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Al-Harbi", "Omar", ""], ["Jusoh", "Shaidah", ""], ["Norwawi", "Norita Md", ""]]}, {"id": "1709.09254", "submitter": "William Yang Wang", "authors": "Ke Ni, William Yang Wang", "title": "Learning to Explain Non-Standard English Words and Phrases", "comments": "IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a data-driven approach for automatically explaining new,\nnon-standard English expressions in a given sentence, building on a large\ndataset that includes 15 years of crowdsourced examples from\nUrbanDictionary.com. Unlike prior studies that focus on matching keywords from\na slang dictionary, we investigate the possibility of learning a neural\nsequence-to-sequence model that generates explanations of unseen non-standard\nEnglish expressions given context. We propose a dual encoder approach---a\nword-level encoder learns the representation of context, and a second\ncharacter-level encoder to learn the hidden representation of the target\nnon-standard expression. Our model can produce reasonable definitions of new\nnon-standard English expressions given their context with certain confidence.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 20:28:18 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Ni", "Ke", ""], ["Wang", "William Yang", ""]]}, {"id": "1709.09345", "submitter": "Seil Na", "authors": "Seil Na, Sangho Lee, Jisung Kim, Gunhee Kim", "title": "A Read-Write Memory Network for Movie Story Understanding", "comments": "accepted paper at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel memory network model named Read-Write Memory Network\n(RWMN) to perform question and answering tasks for large-scale, multimodal\nmovie story understanding. The key focus of our RWMN model is to design the\nread network and the write network that consist of multiple convolutional\nlayers, which enable memory read and write operations to have high capacity and\nflexibility. While existing memory-augmented network models treat each memory\nslot as an independent block, our use of multi-layered CNNs allows the model to\nread and write sequential memory cells as chunks, which is more reasonable to\nrepresent a sequential story because adjacent memory blocks often have strong\ncorrelations. For evaluation, we apply our model to all the six tasks of the\nMovieQA benchmark, and achieve the best accuracies on several tasks, especially\non the visual QA task. Our model shows a potential to better understand not\nonly the content in the story, but also more abstract information, such as\nrelationships between characters and the reasons for their actions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 06:02:57 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 11:36:34 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 08:40:50 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 13:43:15 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Na", "Seil", ""], ["Lee", "Sangho", ""], ["Kim", "Jisung", ""], ["Kim", "Gunhee", ""]]}, {"id": "1709.09360", "submitter": "Lyndon White", "authors": "Lyndon White, Roberto Togneri, Wei Liu, Mohammed Bennamoun", "title": "Learning of Colors from Color Names: Distribution and Point Estimation", "comments": "Implementation available at\n  https://github.com/oxinabox/ColoringNames.jl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color names are often made up of multiple words. As a task in natural\nlanguage understanding we investigate in depth the capacity of neural networks\nbased on sums of word embeddings (SOWE), recurrence (LSTM and GRU based RNNs)\nand convolution (CNN), to estimate colors from sequences of terms. We consider\nboth point and distribution estimates of color. We argue that the latter has a\nparticular value as there is no clear agreement between people as to what a\nparticular color describes -- different people have a different idea of what it\nmeans to be ``very dark orange'', for example. Surprisingly, despite it's\nsimplicity, the sum of word embeddings generally performs the best on almost\nall evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 07:06:05 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 06:27:09 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 10:50:37 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["White", "Lyndon", ""], ["Togneri", "Roberto", ""], ["Liu", "Wei", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1709.09373", "submitter": "Giovanni Siragusa", "authors": "Luigi Di Caro, Marco Guerzoni, Massimiliano Nuccio, Giovanni Siragusa", "title": "A Bimodal Network Approach to Model Topic Dynamics", "comments": "topic modeling, LDA, bimodal network, topic dynamics, economic\n  thought 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an intertemporal bimodal network to analyze the evolution\nof the semantic content of a scientific field within the framework of topic\nmodeling, namely using the Latent Dirichlet Allocation (LDA). The main\ncontribution is the conceptualization of the topic dynamics and its\nformalization and codification into an algorithm. To benchmark the\neffectiveness of this approach, we propose three indexes which track the\ntransformation of topics over time, their rate of birth and death, and the\nnovelty of their content. Applying the LDA, we test the algorithm both on a\ncontrolled experiment and on a corpus of several thousands of scientific papers\nover a period of more than 100 years which account for the history of the\neconomic thought.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 07:49:03 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Di Caro", "Luigi", ""], ["Guerzoni", "Marco", ""], ["Nuccio", "Massimiliano", ""], ["Siragusa", "Giovanni", ""]]}, {"id": "1709.09404", "submitter": "Patrice Bellot", "authors": "Wided Bakari (1), Patrice Bellot (1), Mahmoud Neji ((1) LSIS)", "title": "A Preliminary Study for Building an Arabic Corpus of Pair\n  Questions-Texts from the Web: AQA-Webcorp", "comments": null, "journal-ref": "International Journal of Recent Contributions from Engineering,\n  Science \\& IT (iJES), kassel university press GmbH, 2016, 4 (2), pp.38-45", "doi": "10.3991/ijes.v4i2.5345", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of electronic media and the heterogeneity of Arabic data\non the Web, the idea of building a clean corpus for certain applications of\nnatural language processing, including machine translation, information\nretrieval, question answer, become more and more pressing. In this manuscript,\nwe seek to create and develop our own corpus of pair's questions-texts. This\nconstitution then will provide a better base for our experimentation step.\nThus, we try to model this constitution by a method for Arabic insofar as it\nrecovers texts from the web that could prove to be answers to our factual\nquestions. To do this, we had to develop a java script that can extract from a\ngiven query a list of html pages. Then clean these pages to the extent of\nhaving a data base of texts and a corpus of pair's question-texts. In addition,\nwe give preliminary results of our proposal method. Some investigations for the\nconstruction of Arabic corpus are also presented in this document.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 09:20:06 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Bakari", "Wided", "", "LSIS"], ["Bellot", "Patrice", "", "LSIS"], ["Neji", "Mahmoud", ""]]}, {"id": "1709.09443", "submitter": "Lea Frermann", "authors": "Lea Frermann and Michael C. Frank", "title": "Prosodic Features from Large Corpora of Child-Directed Speech as\n  Predictors of the Age of Acquisition of Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The impressive ability of children to acquire language is a widely studied\nphenomenon, and the factors influencing the pace and patterns of word learning\nremains a subject of active research. Although many models predicting the age\nof acquisition of words have been proposed, little emphasis has been directed\nto the raw input children achieve. In this work we present a comparatively\nlarge-scale multi-modal corpus of prosody-text aligned child directed speech.\nOur corpus contains automatically extracted word-level prosodic features, and\nwe investigate the utility of this information as predictors of age of\nacquisition. We show that prosody features boost predictive power in a\nregularized regression, and demonstrate their utility in the context of a\nmulti-modal factorized language models trained and tested on child-directed\nspeech.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 10:50:12 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Frermann", "Lea", ""], ["Frank", "Michael C.", ""]]}, {"id": "1709.09500", "submitter": "Rotem Dror", "authors": "Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi Reichart", "title": "Replicability Analysis for Natural Language Processing: Testing\n  Significance with Multiple Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-growing amounts of textual data from a large variety of\nlanguages, domains, and genres, it has become standard to evaluate NLP\nalgorithms on multiple datasets in order to ensure consistent performance\nacross heterogeneous setups. However, such multiple comparisons pose\nsignificant challenges to traditional statistical analysis methods in NLP and\ncan lead to erroneous conclusions. In this paper, we propose a Replicability\nAnalysis framework for a statistically sound analysis of multiple comparisons\nbetween algorithms for NLP tasks. We discuss the theoretical advantages of this\nframework over the current, statistically unjustified, practice in the NLP\nliterature, and demonstrate its empirical value across four applications:\nmulti-domain dependency parsing, multilingual POS tagging, cross-domain\nsentiment classification and word similarity prediction.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:31:41 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Dror", "Rotem", ""], ["Baumer", "Gili", ""], ["Bogomolov", "Marina", ""], ["Reichart", "Roi", ""]]}, {"id": "1709.09587", "submitter": "Tal Baumel", "authors": "Tal Baumel, Jumana Nassour-Kassis, Raphael Cohen, Michael Elhadad and\n  No`emie Elhadad", "title": "Multi-Label Classification of Patient Notes a Case Study on ICD Code\n  Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the Electronic Health Record, automated diagnosis coding of\npatient notes is a useful task, but a challenging one due to the large number\nof codes and the length of patient notes. We investigate four models for\nassigning multiple ICD codes to discharge summaries taken from both MIMIC II\nand III. We present Hierarchical Attention-GRU (HA-GRU), a hierarchical\napproach to tag a document by identifying the sentences relevant for each\nlabel. HA-GRU achieves state-of-the art results. Furthermore, the learned\nsentence-level attention layer highlights the model decision process, allows\neasier error analysis, and suggests future directions for improvement.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 15:46:07 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 12:04:57 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 20:32:26 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Baumel", "Tal", ""], ["Nassour-Kassis", "Jumana", ""], ["Cohen", "Raphael", ""], ["Elhadad", "Michael", ""], ["Elhadad", "No`emie", ""]]}, {"id": "1709.09590", "submitter": "Giannis Bekoulis", "authors": "Giannis Bekoulis, Johannes Deleu, Thomas Demeester, Chris Develder", "title": "An attentive neural architecture for joint segmentation and parsing and\n  its application to real estate ads", "comments": "Preprint - Accepted for publication in Expert Systems with\n  Applications", "journal-ref": "Expert Systems with Applications, Volume 102, 15 July 2018, Pages\n  100-112, ISSN 0957-4174", "doi": "10.1016/j.eswa.2018.02.031", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In processing human produced text using natural language processing (NLP)\ntechniques, two fundamental subtasks that arise are (i) segmentation of the\nplain text into meaningful subunits (e.g., entities), and (ii) dependency\nparsing, to establish relations between subunits. In this paper, we develop a\nrelatively simple and effective neural joint model that performs both\nsegmentation and dependency parsing together, instead of one after the other as\nin most state-of-the-art works. We will focus in particular on the real estate\nad setting, aiming to convert an ad to a structured description, which we name\nproperty tree, comprising the tasks of (1) identifying important entities of a\nproperty (e.g., rooms) from classifieds and (2) structuring them into a tree\nformat. In this work, we propose a new joint model that is able to tackle the\ntwo tasks simultaneously and construct the property tree by (i) avoiding the\nerror propagation that would arise from the subtasks one after the other in a\npipelined fashion, and (ii) exploiting the interactions between the subtasks.\nFor this purpose, we perform an extensive comparative study of the pipeline\nmethods and the new proposed joint model, reporting an improvement of over\nthree percentage points in the overall edge F1 score of the property tree.\nAlso, we propose attention methods, to encourage our model to focus on salient\ntokens during the construction of the property tree. Thus we experimentally\ndemonstrate the usefulness of attentive neural architectures for the proposed\njoint model, showcasing a further improvement of two percentage points in edge\nF1 score for our application.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 15:50:53 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 13:51:54 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Bekoulis", "Giannis", ""], ["Deleu", "Johannes", ""], ["Demeester", "Thomas", ""], ["Develder", "Chris", ""]]}, {"id": "1709.09686", "submitter": "Le The Anh", "authors": "L. T. Anh, M. Y. Arkhipov, M. S. Burtsev", "title": "Application of a Hybrid Bi-LSTM-CRF model to the task of Russian Named\n  Entity Recognition", "comments": "Artificial Intelligence and Natural Language Conference (AINL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is one of the most common tasks of the natural\nlanguage processing. The purpose of NER is to find and classify tokens in text\ndocuments into predefined categories called tags, such as person names,\nquantity expressions, percentage expressions, names of locations,\norganizations, as well as expression of time, currency and others. Although\nthere is a number of approaches have been proposed for this task in Russian\nlanguage, it still has a substantial potential for the better solutions. In\nthis work, we studied several deep neural network models starting from vanilla\nBi-directional Long Short-Term Memory (Bi-LSTM) then supplementing it with\nConditional Random Fields (CRF) as well as highway networks and finally adding\nexternal word embeddings. All models were evaluated across three datasets:\nGareev's dataset, Person-1000, FactRuEval-2016. We found that extension of\nBi-LSTM model with CRF significantly increased the quality of predictions.\nEncoding input tokens with external word embeddings reduced training time and\nallowed to achieve state of the art for the Russian NER task.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 18:18:32 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 09:13:41 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Anh", "L. T.", ""], ["Arkhipov", "M. Y.", ""], ["Burtsev", "M. S.", ""]]}, {"id": "1709.09741", "submitter": "Raj Korpan", "authors": "Raj Korpan, Susan L. Epstein, Anoop Aroor, Gil Dekel", "title": "WHY: Natural Explanations from a Robot Navigator", "comments": "Accepted at AAAI 2017 Fall Symposium on Natural Communication for\n  Human-Robot Collaboration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective collaboration between a robot and a person requires natural\ncommunication. When a robot travels with a human companion, the robot should be\nable to explain its navigation behavior in natural language. This paper\nexplains how a cognitively-based, autonomous robot navigation system produces\ninformative, intuitive explanations for its decisions. Language generation here\nis based upon the robot's commonsense, its qualitative reasoning, and its\nlearned spatial model. This approach produces natural explanations in real time\nfor a robot as it navigates in a large, complex indoor environment.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 21:30:53 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Korpan", "Raj", ""], ["Epstein", "Susan L.", ""], ["Aroor", "Anoop", ""], ["Dekel", "Gil", ""]]}, {"id": "1709.09749", "submitter": "Bin Bi", "authors": "Bin Bi and Hao Ma", "title": "KeyVec: Key-semantics Preserving Document Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have demonstrated the empirical success of word embeddings\nin various applications. In this paper, we investigate the problem of learning\ndistributed representations for text documents which many machine learning\nalgorithms take as input for a number of NLP tasks.\n  We propose a neural network model, KeyVec, which learns document\nrepresentations with the goal of preserving key semantics of the input text. It\nenables the learned low-dimensional vectors to retain the topics and important\ninformation from the documents that will flow to downstream tasks. Our\nempirical evaluations show the superior quality of KeyVec representations in\ntwo different document understanding tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 22:05:59 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Bi", "Bin", ""], ["Ma", "Hao", ""]]}, {"id": "1709.09783", "submitter": "Francis Gr\\'egoire", "authors": "Francis Gr\\'egoire, Philippe Langlais", "title": "A Deep Neural Network Approach To Parallel Sentence Extraction", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentence extraction is a task addressing the data sparsity problem\nfound in multilingual natural language processing applications. We propose an\nend-to-end deep neural network approach to detect translational equivalence\nbetween sentences in two different languages. In contrast to previous\napproaches, which typically rely on multiples models and various word alignment\nfeatures, by leveraging continuous vector representation of sentences we remove\nthe need of any domain specific feature engineering. Using a siamese\nbidirectional recurrent neural networks, our results against a strong baseline\nbased on a state-of-the-art parallel sentence extraction system show a\nsignificant improvement in both the quality of the extracted parallel sentences\nand the translation performance of statistical machine translation systems. We\nbelieve this study is the first one to investigate deep learning for the\nparallel sentence extraction task.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 02:09:04 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Gr\u00e9goire", "Francis", ""], ["Langlais", "Philippe", ""]]}, {"id": "1709.09816", "submitter": "Federico Fancellu", "authors": "Ben Krause, Marco Damonte, Mihai Dobre, Daniel Duma, Joachim Fainberg,\n  Federico Fancellu, Emmanuel Kahembwe, Jianpeng Cheng, Bonnie Webber", "title": "Edina: Building an Open Domain Socialbot with Self-dialogues", "comments": "10 pages; submitted to the 1st Proceedings of the Alexa Prize", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Edina, the University of Edinburgh's social bot for the Amazon\nAlexa Prize competition. Edina is a conversational agent whose responses\nutilize data harvested from Amazon Mechanical Turk (AMT) through an innovative\nnew technique we call self-dialogues. These are conversations in which a single\nAMT Worker plays both participants in a dialogue. Such dialogues are\nsurprisingly natural, efficient to collect and reflective of relevant and/or\ntrending topics. These self-dialogues provide training data for a generative\nneural network as well as a basis for soft rules used by a matching score\ncomponent. Each match of a soft rule against a user utterance is associated\nwith a confidence score which we show is strongly indicative of reply quality,\nallowing this component to self-censor and be effectively integrated with other\ncomponents. Edina's full architecture features a rule-based system backing off\nto a matching score, backing off to a generative neural network. Our hybrid\ndata-driven methodology thus addresses both coverage limitations of a strictly\nrule-based approach and the lack of guarantees of a strictly machine-learning\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 06:13:33 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Krause", "Ben", ""], ["Damonte", "Marco", ""], ["Dobre", "Mihai", ""], ["Duma", "Daniel", ""], ["Fainberg", "Joachim", ""], ["Fancellu", "Federico", ""], ["Kahembwe", "Emmanuel", ""], ["Cheng", "Jianpeng", ""], ["Webber", "Bonnie", ""]]}, {"id": "1709.09885", "submitter": "Gichang Lee", "authors": "Gichang Lee, Jaeyun Jeong, Seungwan Seo, CzangYeob Kim, Pilsung Kang", "title": "Sentiment Classification with Word Attention based on Weakly Supervised\n  Learning with a Convolutional Neural Network", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to maximize the applicability of sentiment analysis results, it is\nnecessary to not only classify the overall sentiment (positive/negative) of a\ngiven document but also to identify the main words that contribute to the\nclassification. However, most datasets for sentiment analysis only have the\nsentiment label for each document or sentence. In other words, there is no\ninformation about which words play an important role in sentiment\nclassification. In this paper, we propose a method for identifying key words\ndiscriminating positive and negative sentences by using a weakly supervised\nlearning method based on a convolutional neural network (CNN). In our model,\neach word is represented as a continuous-valued vector and each sentence is\nrepresented as a matrix whose rows correspond to the word vector used in the\nsentence. Then, the CNN model is trained using these sentence matrices as\ninputs and the sentiment labels as the output. Once the CNN model is trained,\nwe implement the word attention mechanism that identifies high-contributing\nwords to classification results with a class activation map, using the weights\nfrom the fully connected layer at the end of the learned CNN model. In order to\nverify the proposed methodology, we evaluated the classification accuracy and\ninclusion rate of polarity words using two movie review datasets. Experimental\nresult show that the proposed model can not only correctly classify the\nsentence polarity but also successfully identify the corresponding words with\nhigh polarity scores.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 10:35:41 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 01:56:16 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Lee", "Gichang", ""], ["Jeong", "Jaeyun", ""], ["Seo", "Seungwan", ""], ["Kim", "CzangYeob", ""], ["Kang", "Pilsung", ""]]}, {"id": "1709.09927", "submitter": "Kazutoshi Sasahara", "authors": "Take Yo and Kazutoshi Sasahara", "title": "Inference of Personal Attributes from Tweets Using Machine Learning", "comments": "10 pages, 5 figures, Proceedings of the 2017 IEEE Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using machine learning algorithms, including deep learning, we studied the\nprediction of personal attributes from the text of tweets, such as gender,\noccupation, and age groups. We applied word2vec to construct word vectors,\nwhich were then used to vectorize tweet blocks. The resulting tweet vectors\nwere used as inputs for training models, and the prediction accuracy of those\nmodels was examined as a function of the dimension of the tweet vectors and the\nsize of the tweet blacks. The results showed that the machine learning\nalgorithms could predict the three personal attributes of interest with 60-70%\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 12:58:18 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 05:50:25 GMT"}, {"version": "v3", "created": "Sun, 24 Dec 2017 14:25:34 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Yo", "Take", ""], ["Sasahara", "Kazutoshi", ""]]}, {"id": "1709.10053", "submitter": "Alberto Cetoli", "authors": "A. Cetoli, S. Bragaglia, A.D. O'Harney, M. Sloan", "title": "Graph Convolutional Networks for Named Entity Recognition", "comments": "Accepted at the 16th International Workshop on Treebanks and\n  Linguistic Theories", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the role of the dependency tree in a named\nentity recognizer upon using a set of GCN. We perform a comparison among\ndifferent NER architectures and show that the grammar of a sentence positively\ninfluences the results. Experiments on the ontonotes dataset demonstrate\nconsistent performance improvements, without requiring heavy feature\nengineering nor additional language-specific knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:44:22 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 14:12:40 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Cetoli", "A.", ""], ["Bragaglia", "S.", ""], ["O'Harney", "A. D.", ""], ["Sloan", "M.", ""]]}, {"id": "1709.10159", "submitter": "Haji Mohammad Saleem", "authors": "Haji Mohammad Saleem, Kelly P Dillon, Susan Benesch and Derek Ruths", "title": "A Web of Hate: Tackling Hateful Speech in Online Social Spaces", "comments": "9 pages, presented at the first workshop on Text Analytics for\n  Cybersecurity and Online Safety (TA-COS), collocated with LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social platforms are beset with hateful speech - content that\nexpresses hatred for a person or group of people. Such content can frighten,\nintimidate, or silence platform users, and some of it can inspire other users\nto commit violence. Despite widespread recognition of the problems posed by\nsuch content, reliable solutions even for detecting hateful speech are lacking.\nIn the present work, we establish why keyword-based methods are insufficient\nfor detection. We then propose an approach to detecting hateful speech that\nuses content produced by self-identifying hateful communities as training data.\nOur approach bypasses the expensive annotation process often required to train\nkeyword systems and performs well across several established platforms, making\nsubstantial improvements over current state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 20:31:30 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Saleem", "Haji Mohammad", ""], ["Dillon", "Kelly P", ""], ["Benesch", "Susan", ""], ["Ruths", "Derek", ""]]}, {"id": "1709.10191", "submitter": "Mingbo Ma", "authors": "Mingbo Ma, Kai Zhao, Liang Huang, Bing Xiang and Bowen Zhou", "title": "Jointly Trained Sequential Labeling and Classification by Sparse\n  Attention Neural Networks", "comments": "interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence-level classification and sequential labeling are two fundamental\ntasks in language understanding. While these two tasks are usually modeled\nseparately, in reality, they are often correlated, for example in intent\nclassification and slot filling, or in topic classification and named-entity\nrecognition. In order to utilize the potential benefits from their\ncorrelations, we propose a jointly trained model for learning the two tasks\nsimultaneously via Long Short-Term Memory (LSTM) networks. This model predicts\nthe sentence-level category and the word-level label sequence from the stepwise\noutput hidden representations of LSTM. We also introduce a novel mechanism of\n\"sparse attention\" to weigh words differently based on their semantic relevance\nto sentence-level classification. The proposed method outperforms baseline\nmodels on ATIS and TREC datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 22:40:07 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Ma", "Mingbo", ""], ["Zhao", "Kai", ""], ["Huang", "Liang", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1709.10204", "submitter": "Bin Bi", "authors": "Bin Bi and Hao Ma", "title": "A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering", "comments": "A paper with a similar method has been published earlier at\n  arXiv:1706.04815 The authors believe there is no need for a separate\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel neural machine reading model for open-domain\nquestion answering at scale. Existing machine comprehension models typically\nassume that a short piece of relevant text containing answers is already\nidentified and given to the models, from which the models are designed to\nextract answers. This assumption, however, is not realistic for building a\nlarge-scale open-domain question answering system which requires both deep text\nunderstanding and identifying relevant text from corpus simultaneously.\n  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates\nboth passage ranking and answer extraction in one single framework. A Q&A\nsystem based on this framework allows users to issue an open-domain question\nwithout needing to provide a piece of text that must contain the answer.\nExperiments show that the unified NCR model is able to outperform the\nstates-of-the-art in both retrieval of relevant text and answer extraction.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 00:27:48 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 17:56:02 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Bi", "Bin", ""], ["Ma", "Hao", ""]]}, {"id": "1709.10217", "submitter": "Wei-Nan Zhang", "authors": "Wei-Nan Zhang, Zhigang Chen, Wanxiang Che, Guoping Hu, Ting Liu", "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the first evaluation of Chinese human-computer\ndialogue technology. We detail the evaluation scheme, tasks, metrics and how to\ncollect and annotate the data for training, developing and test. The evaluation\nincludes two tasks, namely user intent classification and online testing of\ntask-oriented dialogue. To consider the different sources of the data for\ntraining and developing, the first task can also be divided into two sub tasks.\nBoth the two tasks are coming from the real problems when using the\napplications developed by industry. The evaluation data is provided by the\niFLYTEK Corporation. Meanwhile, in this paper, we publish the evaluation\nresults to present the current performance of the participants in the two tasks\nof Chinese human-computer dialogue technology. Moreover, we analyze the\nexisting problems of human-computer dialogue as well as the evaluation scheme\nitself.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 02:35:28 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 01:49:40 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Wei-Nan", ""], ["Chen", "Zhigang", ""], ["Che", "Wanxiang", ""], ["Hu", "Guoping", ""], ["Liu", "Ting", ""]]}, {"id": "1709.10367", "submitter": "Maja Rudolph", "authors": "Maja Rudolph, Francisco Ruiz, Susan Athey, David Blei", "title": "Structured Embedding Models for Grouped Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a powerful approach for analyzing language, and\nexponential family embeddings (EFE) extend them to other types of data. Here we\ndevelop structured exponential family embeddings (S-EFE), a method for\ndiscovering embeddings that vary across related groups of data. We study how\nthe word usage of U.S. Congressional speeches varies across states and party\naffiliation, how words are used differently across sections of the ArXiv, and\nhow the co-purchase patterns of groceries can vary across seasons. Key to the\nsuccess of our method is that the groups share statistical information. We\ndevelop two sharing strategies: hierarchical modeling and amortization. We\ndemonstrate the benefits of this approach in empirical studies of speeches,\nabstracts, and shopping baskets. We show how S-EFE enables group-specific\ninterpretation of word usage, and outperforms EFE in predicting held-out data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 14:14:58 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Rudolph", "Maja", ""], ["Ruiz", "Francisco", ""], ["Athey", "Susan", ""], ["Blei", "David", ""]]}, {"id": "1709.10381", "submitter": "Lasha Abzianidze", "authors": "Lasha Abzianidze, Johan Bos", "title": "Towards Universal Semantic Tagging", "comments": "9 pages, International Conference on Computational Semantics (IWCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes the task of universal semantic tagging---tagging word\ntokens with language-neutral, semantically informative tags. We argue that the\ntask, with its independent nature, contributes to better semantic analysis for\nwide-coverage multilingual text. We present the initial version of the semantic\ntagset and show that (a) the tags provide semantically fine-grained\ninformation, and (b) they are suitable for cross-lingual semantic parsing. An\napplication of the semantic tagging in the Parallel Meaning Bank supports both\nof these points as the tags contribute to formal lexical semantics and their\ncross-lingual projection. As a part of the application, we annotate a small\ncorpus with the semantic tags and present new baseline result for universal\nsemantic tagging.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 12:58:05 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Abzianidze", "Lasha", ""], ["Bos", "Johan", ""]]}, {"id": "1709.10423", "submitter": "Yanchao Yu", "authors": "Yanchao Yu, Arash Eshghi, Oliver Lemon", "title": "Learning how to learn: an adaptive dialogue agent for incrementally\n  learning visually grounded word meanings", "comments": "10 pages, RoboNLP Workshop from ACL Conference", "journal-ref": null, "doi": "10.18653/v1/W17-2802", "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimised multi-modal dialogue agent for interactive learning\nof visually grounded word meanings from a human tutor, trained on real\nhuman-human tutoring data. Within a life-long interactive learning period, the\nagent, trained using Reinforcement Learning (RL), must be able to handle\nnatural conversations with human users and achieve good learning performance\n(accuracy) while minimising human effort in the learning process. We train and\nevaluate this system in interaction with a simulated human tutor, which is\nbuilt on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual\nlearning task. The results show that: 1) The learned policy can coherently\ninteract with the simulated user to achieve the goal of the task (i.e. learning\nvisual attributes of objects, e.g. colour and shape); and 2) it finds a better\ntrade-off between classifier accuracy and tutoring costs than hand-crafted\nrule-based policies, including ones with dynamic policies.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:21:31 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Yu", "Yanchao", ""], ["Eshghi", "Arash", ""], ["Lemon", "Oliver", ""]]}, {"id": "1709.10426", "submitter": "Yanchao Yu", "authors": "Yanchao Yu, Arash Eshghi, Oliver Lemon", "title": "Training an adaptive dialogue policy for interactive learning of\n  visually grounded word meanings", "comments": "11 pages, SIGDIAL 2016 Conference", "journal-ref": null, "doi": "10.18653/v1/W16-3643", "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-modal dialogue system for interactive learning of\nperceptually grounded word meanings from a human tutor. The system integrates\nan incremental, semantic parsing/generation framework - Dynamic Syntax and Type\nTheory with Records (DS-TTR) - with a set of visual classifiers that are\nlearned throughout the interaction and which ground the meaning representations\nthat it produces. We use this system in interaction with a simulated human\ntutor to study the effects of different dialogue policies and capabilities on\nthe accuracy of learned meanings, learning rates, and efforts/costs to the\ntutor. We show that the overall performance of the learning agent is affected\nby (1) who takes initiative in the dialogues; (2) the ability to express/use\ntheir confidence level about visual attributes; and (3) the ability to process\nelliptical and incrementally constructed dialogue turns. Ultimately, we train\nan adaptive dialogue policy which optimises the trade-off between classifier\naccuracy and tutoring costs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:28:31 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Yu", "Yanchao", ""], ["Eshghi", "Arash", ""], ["Lemon", "Oliver", ""]]}, {"id": "1709.10431", "submitter": "Yanchao Yu", "authors": "Yanchao Yu, Arash Eshghi, Gregory Mills, Oliver Joseph Lemon", "title": "The BURCHAK corpus: a Challenge Data Set for Interactive Learning of\n  Visually Grounded Word Meanings", "comments": "10 pages, THE 6TH WORKSHOP ON VISION AND LANGUAGE (VL'17)", "journal-ref": null, "doi": "10.18653/v1/W17-2001", "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and describe a new freely available human-human dialogue dataset\nfor interactive learning of visually grounded word meanings through ostensive\ndefinition by a tutor to a learner. The data has been collected using a novel,\ncharacter-by-character variant of the DiET chat tool (Healey et al., 2003;\nMills and Healey, submitted) with a novel task, where a Learner needs to learn\ninvented visual attribute words (such as \" burchak \" for square) from a tutor.\nAs such, the text-based interactions closely resemble face-to-face conversation\nand thus contain many of the linguistic phenomena encountered in natural,\nspontaneous dialogue. These include self-and other-correction, mid-sentence\ncontinuations, interruptions, overlaps, fillers, and hedges. We also present a\ngeneric n-gram framework for building user (i.e. tutor) simulations from this\ntype of incremental data, which is freely available to researchers. We show\nthat the simulations produce outputs that are similar to the original data\n(e.g. 78% turn match similarity). Finally, we train and evaluate a\nReinforcement Learning dialogue control agent for learning visually grounded\nword meanings, trained from the BURCHAK corpus. The learned policy shows\ncomparable performance to a rule-based system built previously.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:43:06 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Yu", "Yanchao", ""], ["Eshghi", "Arash", ""], ["Mills", "Gregory", ""], ["Lemon", "Oliver Joseph", ""]]}, {"id": "1709.10445", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Pablo Estrada, Kyomin Jung", "title": "Synonym Discovery with Etymology-based Word Embeddings", "comments": "6 pages, IEEE Symposium Series on Computational Intelligence (IEEE\n  SSCI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to learn word embeddings based on an extended\nversion of the distributional hypothesis. Our model derives word embedding\nvectors using the etymological composition of words, rather than the context in\nwhich they appear. It has the strength of not requiring a large text corpus,\nbut instead it requires reliable access to etymological roots of words, making\nit specially fit for languages with logographic writing systems. The model\nconsists on three steps: (1) building an etymological graph, which is a\nbipartite network of words and etymological roots, (2) obtaining the\nbiadjacency matrix of the etymological graph and reducing its dimensionality,\n(3) using columns/rows of the resulting matrices as embedding vectors. We test\nour model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by\na set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both\ncases we show that our model performs well in the task of synonym discovery.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 15:10:20 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 12:57:59 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Estrada", "Pablo", ""], ["Jung", "Kyomin", ""]]}, {"id": "1709.10486", "submitter": "Casey Kennington", "authors": "Casey Kennington and Sarah Plane", "title": "Symbol, Conversational, and Societal Grounding with a Toy Robot", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Essential to meaningful interaction is grounding at the symbolic,\nconversational, and societal levels. We present ongoing work with Anki's Cozmo\ntoy robot as a research platform where we leverage the recent\nwords-as-classifiers model of lexical semantics in interactive reference\nresolution tasks for language grounding.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 16:36:53 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Kennington", "Casey", ""], ["Plane", "Sarah", ""]]}]