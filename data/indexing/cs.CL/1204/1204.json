[{"id": "1204.0140", "submitter": "Mario Jarmasz", "authors": "Mario Jarmasz", "title": "Roget's Thesaurus as a Lexical Resource for Natural Language Processing", "comments": "Thesis submitted to the Faculty of Graduate and Postdoctoral Studies\n  in partial fulfillment of the requirements for the degree of Master of\n  Computer Science July, 2003. Ottawa-Carleton Institute for Computer Science,\n  School of Information Technology and Engineering, University of Ottawa,\n  Ottawa, Ontario, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WordNet proved that it is possible to construct a large-scale electronic\nlexical database on the principles of lexical semantics. It has been accepted\nand used extensively by computational linguists ever since it was released.\nInspired by WordNet's success, we propose as an alternative a similar resource,\nbased on the 1987 Penguin edition of Roget's Thesaurus of English Words and\nPhrases.\n  Peter Mark Roget published his first Thesaurus over 150 years ago. Countless\nwriters, orators and students of the English language have used it.\nComputational linguists have employed Roget's for almost 50 years in Natural\nLanguage Processing, however hesitated in accepting Roget's Thesaurus because a\nproper machine tractable version was not available.\n  This dissertation presents an implementation of a machine-tractable version\nof the 1987 Penguin edition of Roget's Thesaurus - the first implementation of\nits kind to use an entire current edition. It explains the steps necessary for\ntaking a machine-readable file and transforming it into a tractable system.\nThis involves converting the lexical material into a format that can be more\neasily exploited, identifying data structures and designing classes to\ncomputerize the Thesaurus. Roget's organization is studied in detail and\ncontrasted with WordNet's.\n  We show two applications of the computerized Thesaurus: computing semantic\nsimilarity between words and phrases, and building lexical chains in a text.\nThe experiments are performed using well-known benchmarks and the results are\ncompared to those of other systems that use Roget's, WordNet and statistical\ntechniques. Roget's has turned out to be an excellent resource for measuring\nsemantic similarity; lexical chains are easily built but more difficult to\nevaluate. We also explain ways in which Roget's Thesaurus and WordNet can be\ncombined.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 21:53:56 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Jarmasz", "Mario", ""]]}, {"id": "1204.0184", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "Parallel Spell-Checking Algorithm Based on Yahoo! N-Grams Dataset", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org/; International Journal of Research and Reviews in\n  Computer Science (IJRRCS), Vol. 3, No. 1, February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spell-checking is the process of detecting and sometimes providing\nsuggestions for incorrectly spelled words in a text. Basically, the larger the\ndictionary of a spell-checker is, the higher is the error detection rate;\notherwise, misspellings would pass undetected. Unfortunately, traditional\ndictionaries suffer from out-of-vocabulary and data sparseness problems as they\ndo not encompass large vocabulary of words indispensable to cover proper names,\ndomain-specific terms, technical jargons, special acronyms, and terminologies.\nAs a result, spell-checkers will incur low error detection and correction rate\nand will fail to flag all errors in the text. This paper proposes a new\nparallel shared-memory spell-checking algorithm that uses rich real-world word\nstatistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors\nin computer text. Essentially, the proposed algorithm can be divided into three\nsub-algorithms that run in a parallel fashion: The error detection algorithm\nthat detects misspellings, the candidates generation algorithm that generates\ncorrection suggestions, and the error correction algorithm that performs\ncontextual error correction. Experiments conducted on a set of text articles\ncontaining misspellings, showed a remarkable spelling error correction rate\nthat resulted in a radical reduction of both non-word and real-word errors in\nelectronic text. In a further study, the proposed algorithm is to be optimized\nfor message-passing systems so as to become more flexible and less costly to\nscale over distributed machines.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 09:28:20 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1204.0188", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Mohammad Alwani", "title": "OCR Context-Sensitive Error Correction Based on Google Web 1T 5-Gram\n  Data Set", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org/; American Journal of Scientific Research, Issue. 50,\n  February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the dawn of the computing era, information has been represented\ndigitally so that it can be processed by electronic computers. Paper books and\ndocuments were abundant and widely being published at that time; and hence,\nthere was a need to convert them into digital format. OCR, short for Optical\nCharacter Recognition was conceived to translate paper-based books into digital\ne-books. Regrettably, OCR systems are still erroneous and inaccurate as they\nproduce misspellings in the recognized text, especially when the source\ndocument is of low printing quality. This paper proposes a post-processing OCR\ncontext-sensitive error correction method for detecting and correcting non-word\nand real-word OCR errors. The cornerstone of this proposed approach is the use\nof Google Web 1T 5-gram data set as a dictionary of words to spell-check OCR\ntext. The Google data set incorporates a very large vocabulary and word\nstatistics entirely reaped from the Internet, making it a reliable source to\nperform dictionary-based error correction. The core of the proposed solution is\na combination of three algorithms: The error detection, candidate spellings\ngenerator, and error correction algorithms, which all exploit information\nextracted from Google Web 1T 5-gram data set. Experiments conducted on scanned\nimages written in different languages showed a substantial improvement in the\nOCR error correction rate. As future developments, the proposed algorithm is to\nbe parallelised so as to support parallel and distributed computing\narchitectures.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 10:06:55 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""], ["Alwani", "Mohammad", ""]]}, {"id": "1204.0191", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Mohammad Alwani", "title": "OCR Post-Processing Error Correction Algorithm using Google Online\n  Spelling Suggestion", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org/; Journal of Emerging Trends in Computing and\n  Information Sciences, Vol. 3, No. 1, January 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of digital optical scanners, a lot of paper-based books,\ntextbooks, magazines, articles, and documents are being transformed into an\nelectronic version that can be manipulated by a computer. For this purpose,\nOCR, short for Optical Character Recognition was developed to translate scanned\ngraphical text into editable computer text. Unfortunately, OCR is still\nimperfect as it occasionally mis-recognizes letters and falsely identifies\nscanned text, leading to misspellings and linguistics errors in the OCR output\ntext. This paper proposes a post-processing context-based error correction\nalgorithm for detecting and correcting OCR non-word and real-word errors. The\nproposed algorithm is based on Google's online spelling suggestion which\nharnesses an internal database containing a huge collection of terms and word\nsequences gathered from all over the web, convenient to suggest possible\nreplacements for words that have been misspelled during the OCR process.\nExperiments carried out revealed a significant improvement in OCR error\ncorrection rate. Future research can improve upon the proposed algorithm so\nmuch so that it can be parallelized and executed over multiprocessing\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 10:34:38 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""], ["Alwani", "Mohammad", ""]]}, {"id": "1204.0245", "submitter": "Mario Jarmasz", "authors": "Mario Jarmasz, and Stan Szpakowicz", "title": "Roget's Thesaurus and Semantic Similarity", "comments": "8 pages", "journal-ref": "Proceedings of Conference on Recent Advances in Natural Language\n  Processing (RANLP 2003), Borovets, Bulgaria, September 2003, 212-219", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have implemented a system that measures semantic similarity using a\ncomputerized 1987 Roget's Thesaurus, and evaluated it by performing a few\ntypical tests. We compare the results of these tests with those produced by\nWordNet-based similarity measures. One of the benchmarks is Miller and Charles'\nlist of 30 noun pairs to which human judges had assigned similarity measures.\nWe correlate these measures with those computed by several NLP systems. The 30\npairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have\nalso studied. Our Roget's-based system gets correlations of .878 for the\nsmaller and .818 for the larger list of noun pairs; this is quite close to the\n.885 that Resnik obtained when he employed humans to replicate the Miller and\nCharles experiment. We further evaluate our measure by using Roget's and\nWordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the\ncorrect synonym must be selected amongst a group of four words. Our system gets\n78.75%, 82.00% and 74.33% of the questions respectively.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 17:04:13 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Jarmasz", "Mario", ""], ["Szpakowicz", "Stan", ""]]}, {"id": "1204.0255", "submitter": "Mario Jarmasz", "authors": "Mario Jarmasz and Caroline Barri\\`ere", "title": "Keyphrase Extraction : Enhancing Lists", "comments": "8 pages; Proceedings of the 2nd Conference on Computational\n  Linguistics in the North-East (CLiNE 2004), Montr\\'eal, Canada, August", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes some modest improvements to Extractor, a state-of-the-art\nkeyphrase extraction system, by using a terabyte-sized corpus to estimate the\ninformativeness and semantic similarity of keyphrases. We present two\ntechniques to improve the organization and remove outliers of lists of\nkeyphrases. The first is a simple ordering according to their occurrences in\nthe corpus; the second is clustering according to semantic similarity.\nEvaluation issues are discussed. We present a novel technique of comparing\nextracted keyphrases to a gold standard which relies on semantic similarity\nrather than string matching or an evaluation involving human judges.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 19:15:58 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Jarmasz", "Mario", ""], ["Barri\u00e8re", "Caroline", ""]]}, {"id": "1204.0257", "submitter": "Mario Jarmasz", "authors": "Mario Jarmasz and Stan Szpakowicz", "title": "Not As Easy As It Seems: Automating the Construction of Lexical Chains\n  Using Roget's Thesaurus", "comments": "5 pages", "journal-ref": "Proceedings of the 16th Canadian Conference on Artificial\n  Intelligence (AI 2003), Halifax, Canada, June 2003. Lecture Notes in Computer\n  Science 2671, Springer-Verlag 2003, 544-549", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morris and Hirst present a method of linking significant words that are about\nthe same topic. The resulting lexical chains are a means of identifying\ncohesive regions in a text, with applications in many natural language\nprocessing tasks, including text summarization. The first lexical chains were\nconstructed manually using Roget's International Thesaurus. Morris and Hirst\nwrote that automation would be straightforward given an electronic thesaurus.\nAll applications so far have used WordNet to produce lexical chains, perhaps\nbecause adequate electronic versions of Roget's were not available until\nrecently. We discuss the building of lexical chains using an electronic version\nof Roget's Thesaurus. We implement a variant of the original algorithm, and\nexplain the necessary design decisions. We include a comparison with other\nimplementations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 19:19:36 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Jarmasz", "Mario", ""], ["Szpakowicz", "Stan", ""]]}, {"id": "1204.0258", "submitter": "Mario Jarmasz", "authors": "Mario Jarmasz and Stan Szpakowicz", "title": "Roget's Thesaurus: a Lexical Resource to Treasure", "comments": "6 pages", "journal-ref": "Proceedings of the NAACL WordNet and Other Lexical Resources\n  workshop. Pittsburgh, June 2001, 186 - 188", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the steps involved in creating an electronic lexical\nknowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic\nrelations are labelled with the help of WordNet. The two resources are compared\nin a qualitative and quantitative manner. Differences in the organization of\nthe lexical material are discussed, as well as the possibility of merging both\nresources.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 19:25:29 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Jarmasz", "Mario", ""], ["Szpakowicz", "Stan", ""]]}, {"id": "1204.1615", "submitter": "Sofiene Haboubi", "authors": "Sofiene Haboubi and Samia Maddouri and Hamid Amiri", "title": "Discrimination between Arabic and Latin from bilingual documents", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/CCCA.2011.6031496", "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  2011 International Conference on Communications, Computing and Control\nApplications (CCCA)\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 09:28:19 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Haboubi", "Sofiene", ""], ["Maddouri", "Samia", ""], ["Amiri", "Hamid", ""]]}, {"id": "1204.2523", "submitter": "Khalid El-Arini", "authors": "Khalid El-Arini, Emily B. Fox, Carlos Guestrin", "title": "Concept Modeling with Superwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In information retrieval, a fundamental goal is to transform a document into\nconcepts that are representative of its content. The term \"representative\" is\nin itself challenging to define, and various tasks require different\ngranularities of concepts. In this paper, we aim to model concepts that are\nsparse over the vocabulary, and that flexibly adapt their content based on\nother relevant semantic information such as textual structure or associated\nimage features. We explore a Bayesian nonparametric model based on nested beta\nprocesses that allows for inferring an unknown number of strictly sparse\nconcepts. The resulting model provides an inherently different representation\nof concepts than a standard LDA (or HDP) based topic model, and allows for\ndirect incorporation of semantic features. We demonstrate the utility of this\nrepresentation on multilingual blog data and the Congressional Record.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 18:53:58 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["El-Arini", "Khalid", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1204.2765", "submitter": "Taha Yasseri", "authors": "Taha Yasseri, Andr\\'as Kornai, and J\\'anos Kert\\'esz", "title": "A practical approach to language complexity: a Wikipedia case study", "comments": "2 new figures, 1 new section, and 2 new supporting texts", "journal-ref": "PLoS ONE 7(11): e48386 (2012)", "doi": "10.1371/journal.pone.0048386", "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present statistical analysis of English texts from\nWikipedia. We try to address the issue of language complexity empirically by\ncomparing the simple English Wikipedia (Simple) to comparable samples of the\nmain English Wikipedia (Main). Simple is supposed to use a more simplified\nlanguage with a limited vocabulary, and editors are explicitly requested to\nfollow this guideline, yet in practice the vocabulary richness of both samples\nare at the same level. Detailed analysis of longer units (n-grams of words and\npart of speech tags) shows that the language of Simple is less complex than\nthat of Main primarily due to the use of shorter sentences, as opposed to\ndrastically simplified syntax or vocabulary. Comparing the two language\nvarieties by the Gunning readability index supports this conclusion. We also\nreport on the topical dependence of language complexity, e.g. that the language\nis more advanced in conceptual articles compared to person-based (biographical)\nand object-based articles. Finally, we investigate the relation between\nconflict and language complexity by analyzing the content of the talk pages\nassociated to controversial and peacefully developing articles, concluding that\ncontroversy has the effect of reducing language complexity.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 16:21:10 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2012 11:49:52 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Yasseri", "Taha", ""], ["Kornai", "Andr\u00e1s", ""], ["Kert\u00e9sz", "J\u00e1nos", ""]]}, {"id": "1204.2804", "submitter": "Myle Ott", "authors": "Myle Ott, Claire Cardie, Jeff Hancock", "title": "Estimating the Prevalence of Deception in Online Review Communities", "comments": "10 pages, 4 figures, 3 tables, to appear at WWW 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers' purchase decisions are increasingly influenced by user-generated\nonline reviews. Accordingly, there has been growing concern about the potential\nfor posting \"deceptive opinion spam\" -- fictitious reviews that have been\ndeliberately written to sound authentic, to deceive the reader. But while this\npractice has received considerable public attention and concern, relatively\nlittle is known about the actual prevalence, or rate, of deception in online\nreview communities, and less still about the factors that influence it.\n  We propose a generative model of deception which, in conjunction with a\ndeception classifier, we use to explore the prevalence of deception in six\npopular online review communities: Expedia, Hotels.com, Orbitz, Priceline,\nTripAdvisor, and Yelp. We additionally propose a theoretical model of online\nreviews based on economic signaling theory, in which consumer reviews diminish\nthe inherent information asymmetry between consumers and producers, by acting\nas a signal to a product's true, unknown quality. We find that deceptive\nopinion spam is a growing problem overall, but with different growth rates\nacross communities. These rates, we argue, are driven by the different\nsignaling costs associated with deception for each review community, e.g.,\nposting requirements. When measures are taken to increase signaling cost, e.g.,\nfiltering reviews written by first-time reviewers, deception prevalence is\neffectively reduced.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 18:25:04 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Ott", "Myle", ""], ["Cardie", "Claire", ""], ["Hancock", "Jeff", ""]]}, {"id": "1204.2847", "submitter": "Chris Fournier", "authors": "Chris Fournier and Diana Inkpen", "title": "Segmentation Similarity and Agreement", "comments": "10 pages, LaTeX, corrected a typo in equation 4", "journal-ref": "Proceedings of the 2012 Conference of the North American Chapter\n  of the Association for Computational Linguistics: Human Language\n  Technologies, pages 152-161, Montr\\'eal, Canada, June 3-8, 2012", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new segmentation evaluation metric, called segmentation\nsimilarity (S), that quantifies the similarity between two segmentations as the\nproportion of boundaries that are not transformed when comparing them using\nedit distance, essentially using edit distance as a penalty function and\nscaling penalties by segmentation size. We propose several adapted\ninter-annotator agreement coefficients which use S that are suitable for\nsegmentation. We show that S is configurable enough to suit a wide variety of\nsegmentation evaluations, and is an improvement upon the state of the art. We\nalso propose using inter-annotator agreement coefficients to evaluate automatic\nsegmenters in terms of human performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 22:01:27 GMT"}, {"version": "v2", "created": "Sat, 19 May 2012 19:15:56 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Fournier", "Chris", ""], ["Inkpen", "Diana", ""]]}, {"id": "1204.3198", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho and Antoni Hern\\'andez-Fern\\'andez", "title": "The failure of the law of brevity in two New World primates. Statistical\n  caveats", "comments": "Little improvements in the statistical arguments", "journal-ref": "Statistical caveats. Glottotheory 4 (1), 45-55 (2013)", "doi": "10.1524/glot.2013.0004", "report-no": null, "categories": "q-bio.NC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallels of Zipf's law of brevity, the tendency of more frequent words to be\nshorter, have been found in bottlenose dolphins and Formosan macaques. Although\nthese findings suggest that behavioral repertoires are shaped by a general\nprinciple of compression, common marmosets and golden-backed uakaris do not\nexhibit the law. However, we argue that the law may be impossible or difficult\nto detect statistically in a given species if the repertoire is too small, a\nproblem that could be affecting golden backed uakaris, and show that the law is\npresent in a subset of the repertoire of common marmosets. We suggest that the\nvisibility of the law will depend on the subset of the repertoire under\nconsideration or the repertoire size.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2012 19:25:54 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 17:26:00 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""], ["Hern\u00e1ndez-Fern\u00e1ndez", "Antoni", ""]]}, {"id": "1204.3458", "submitter": "Bob Coecke", "authors": "Bob Coecke", "title": "The logic of quantum mechanics - Take II", "comments": "23 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CL cs.LO math.CT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forward a new take on the logic of quantum mechanics, following\nSchroedinger's point of view that it is composition which makes quantum theory\nwhat it is, rather than its particular propositional structure due to the\nexistence of superpositions, as proposed by Birkhoff and von Neumann. This\ngives rise to an intrinsically quantitative kind of logic, which truly deserves\nthe name `logic' in that it also models meaning in natural language, the latter\nbeing the origin of logic, that it supports automation, the most prominent\npractical use of logic, and that it supports probabilistic inference.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 12:22:13 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Coecke", "Bob", ""]]}, {"id": "1204.3498", "submitter": "Vahed Qazvinian", "authors": "Vahed Qazvinian and Dragomir R. Radev", "title": "A Computational Analysis of Collective Discourse", "comments": "Presented at Collective Intelligence conference, 2012\n  (arXiv:1204.2991)", "journal-ref": null, "doi": null, "report-no": "CollectiveIntelligence/2012/59", "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on the computational analysis of collective discourse,\na collective behavior seen in non-expert content contributions in online social\nmedia. We collect and analyze a wide range of real-world collective discourse\ndatasets from movie user reviews to microblogs and news headlines to scientific\ncitations. We show that all these datasets exhibit diversity of perspective, a\nproperty seen in other collective systems and a criterion in wise crowds. Our\nexperiments also confirm that the network of different perspective\nco-occurrences exhibits the small-world property with high clustering of\ndifferent perspectives. Finally, we show that non-expert contributions in\ncollective discourse can be used to answer simple questions that are otherwise\nhard to answer.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 14:27:39 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2012 17:17:28 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Qazvinian", "Vahed", ""], ["Radev", "Dragomir R.", ""]]}, {"id": "1204.3731", "submitter": "Damiano Spina", "authors": "Arkaitz Zubiaga, Damiano Spina, Enrique Amig\\'o and Julio Gonzalo", "title": "Towards Real-Time Summarization of Scheduled Events from Twitter Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper explores the real-time summarization of scheduled events such as\nsoccer games from torrential flows of Twitter streams. We propose and evaluate\nan approach that substantially shrinks the stream of tweets in real-time, and\nconsists of two steps: (i) sub-event detection, which determines if something\nnew has occurred, and (ii) tweet selection, which picks a representative tweet\nto describe each sub-event. We compare the summaries generated in three\nlanguages for all the soccer games in \"Copa America 2011\" to reference live\nreports offered by Yahoo! Sports journalists. We show that simple text analysis\nmethods which do not involve external knowledge lead to summaries that cover\n84% of the sub-events on average, and 100% of key types of sub-events (such as\ngoals in soccer). Our approach should be straightforwardly applicable to other\nkinds of scheduled events such as other sports, award ceremonies, keynote\ntalks, TV shows, etc.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 08:58:39 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Spina", "Damiano", ""], ["Amig\u00f3", "Enrique", ""], ["Gonzalo", "Julio", ""]]}, {"id": "1204.3800", "submitter": "Srinivasan Kalyanaraman", "authors": "Srinivasan Kalyanaraman", "title": "Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha)", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jules Bloch's work on formation of the Marathi language has to be expanded\nfurther to provide for a study of evolution and formation of Indian languages\nin the Indian language union (sprachbund). The paper analyses the stages in the\nevolution of early writing systems which began with the evolution of counting\nin the ancient Near East. A stage anterior to the stage of syllabic\nrepresentation of sounds of a language, is identified. Unique geometric shapes\nrequired for tokens to categorize objects became too large to handle to\nabstract hundreds of categories of goods and metallurgical processes during the\nproduction of bronze-age goods. About 3500 BCE, Indus script as a writing\nsystem was developed to use hieroglyphs to represent the 'spoken words'\nidentifying each of the goods and processes. A rebus method of representing\nsimilar sounding words of the lingua franca of the artisans was used in Indus\nscript. This method is recognized and consistently applied for the lingua\nfranca of the Indian sprachbund. That the ancient languages of India,\nconstituted a sprachbund (or language union) is now recognized by many\nlinguists. The sprachbund area is proximate to the area where most of the Indus\nscript inscriptions were discovered, as documented in the corpora. That\nhundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced\nby their use on early punch-marked coins. This explains the combined use of\nsyllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on\nRampurva copper bolt, and Sohgaura copper plate from about 6th century\nBCE.Indian hieroglyphs constitute a writing system for meluhha language and are\nrebus representations of archaeo-metallurgy lexemes. The rebus principle was\nemployed by the early scripts and can legitimately be used to decipher the\nIndus script, after secure pictorial identification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 14:14:26 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Kalyanaraman", "Srinivasan", ""]]}, {"id": "1204.4346", "submitter": "Alex Fabrikant", "authors": "James Cook, Atish Das Sarma, Alex Fabrikant, Andrew Tomkins", "title": "Your Two Weeks of Fame and Your Grandmother's", "comments": "This version supercedes the short version of this paper published in\n  the proceedings of WWW 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Did celebrity last longer in 1929, 1992 or 2009? We investigate the\nphenomenon of fame by mining a collection of news articles that spans the\ntwentieth century, and also perform a side study on a collection of blog posts\nfrom the last 10 years. By analyzing mentions of personal names, we measure\neach person's time in the spotlight, using two simple metrics that evaluate,\nroughly, the duration of a single news story about a person, and the overall\nduration of public interest in a person. We watched the distribution evolve\nfrom 1895 to 2010, expecting to find significantly shortening fame durations,\nper the much popularly bemoaned shortening of society's attention spans and\nquickening of media's news cycles. Instead, we conclusively demonstrate that,\nthrough many decades of rapid technological and societal change, through the\nappearance of Twitter, communication satellites, and the Internet, fame\ndurations did not decrease, neither for the typical case nor for the extremely\nfamous, with the last statistically significant fame duration decreases coming\nin the early 20th century, perhaps from the spread of telegraphy and telephony.\nFurthermore, while median fame durations stayed persistently constant, for the\nmost famous of the famous, as measured by either volume or duration of media\nattention, fame durations have actually trended gently upward since the 1940s,\nwith statistically significant increases on 40-year timescales. Similar studies\nhave been done with much shorter timescales specifically in the context of\ninformation spreading on Twitter and similar social networking sites. To the\nbest of our knowledge, this is the first massive scale study of this nature\nthat spans over a century of archived data, thereby allowing us to track\nchanges across decades.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 13:37:34 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Cook", "James", ""], ["Sarma", "Atish Das", ""], ["Fabrikant", "Alex", ""], ["Tomkins", "Andrew", ""]]}, {"id": "1204.4914", "submitter": "Diederik Aerts", "authors": "Diederik Aerts and Sandro Sozzo", "title": "Quantum Interference in Cognition: Structural Aspects of the Brain", "comments": "15 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1004.2530", "journal-ref": "In V. Ovchinnikov and P. Dini (Eds.), IARIA, Proceedings of the\n  Sixth International Conference on Quantum, Nano and Micro Technologies, pp.\n  33-41, 2012", "doi": null, "report-no": null, "categories": "cs.AI cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify the presence of typically quantum effects, namely 'superposition'\nand 'interference', in what happens when human concepts are combined, and\nprovide a quantum model in complex Hilbert space that represents faithfully\nexperimental data measuring the situation of combining concepts. Our model\nshows how 'interference of concepts' explains the effects of underextension and\noverextension when two concepts combine to the disjunction of these two\nconcepts. This result supports our earlier hypothesis that human thought has a\nsuperposed two-layered structure, one layer consisting of 'classical logical\nthought' and a superposed layer consisting of 'quantum conceptual thought'.\nPossible connections with recent findings of a 'grid-structure' for the brain\nare analyzed, and influences on the mind/brain relation, and consequences on\napplied disciplines, such as artificial intelligence and quantum computation,\nare considered.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2012 17:17:56 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1204.5316", "submitter": "Maxime Lefrancois", "authors": "Maxime Lefran\\c{c}ois (INRIA Sophia Antipolis), Fabien Gandon (INRIA\n  Sophia Antipolis)", "title": "ILexicOn: toward an ECD-compliant interlingual lexical ontology\n  described with semantic web formalisms", "comments": null, "journal-ref": "MTT - 5th International Conference on Meaning-Text Theory - 2011\n  (2011) 155-164", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in bridging the world of natural language and the world of\nthe semantic web in particular to support natural multilingual access to the\nweb of data. In this paper we introduce a new type of lexical ontology called\ninterlingual lexical ontology (ILexicOn), which uses semantic web formalisms to\nmake each interlingual lexical unit class (ILUc) support the projection of its\nsemantic decomposition on itself. After a short overview of existing lexical\nontologies, we briefly introduce the semantic web formalisms we use. We then\npresent the three layered architecture of our approach: i) the interlingual\nlexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formally\ndefined; iii) the data layer. We illustrate our approach with a standalone\nILexicOn, and introduce and explain a concise human-readable notation to\nrepresent ILexicOns. Finally, we show how semantic web formalisms enable the\nprojection of a semantic decomposition on the decomposed ILUc.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 09:13:59 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Lefran\u00e7ois", "Maxime", "", "INRIA Sophia Antipolis"], ["Gandon", "Fabien", "", "INRIA\n  Sophia Antipolis"]]}, {"id": "1204.5345", "submitter": "William Stevens", "authors": "William M. Stevens, Andrew Adamatzky, Ishrat Jahan, Ben de Lacy\n  Costello", "title": "Time-dependent wave selection for information processing in excitable\n  media", "comments": null, "journal-ref": "Phys. Rev. E 85, 066129 (2012)", "doi": "10.1103/PhysRevE.85.066129", "report-no": null, "categories": "nlin.PS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an improved technique for implementing logic circuits in\nlight-sensitive chemical excitable media. The technique makes use of the\nconstant-speed propagation of waves along defined channels in an excitable\nmedium based on the Belousov-Zhabotinsky reaction, along with the mutual\nannihilation of colliding waves. What distinguishes this work from previous\nwork in this area is that regions where channels meet at a junction can\nperiodically alternate between permitting the propagation of waves and blocking\nthem. These valve-like areas are used to select waves based on the length of\ntime that it takes waves to propagate from one valve to another. In an\nexperimental implementation, the channels which make up the circuit layout are\nprojected by a digital projector connected to a computer. Excitable channels\nare projected as dark areas, unexcitable regions as light areas. Valves\nalternate between dark and light: every valve has the same period and phase,\nwith a 50% duty cycle. This scheme can be used to make logic gates based on\ncombinations of OR and AND-NOT operations, with few geometrical constraints.\nBecause there are few geometrical constraints, compact circuits can be\nimplemented. Experimental results from an implementation of a 4-bit input,\n2-bit output integer square root circuit are given. This is the most complex\nlogic circuit that has been implemented in BZ excitable media to date.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 11:46:53 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Stevens", "William M.", ""], ["Adamatzky", "Andrew", ""], ["Jahan", "Ishrat", ""], ["Costello", "Ben de Lacy", ""]]}, {"id": "1204.5369", "submitter": "Marco Guerini", "authors": "Marco Guerini, Carlo Strapparava, Oliviero Stock", "title": "Ecological Evaluation of Persuasive Messages Using Google AdWords", "comments": "To appear at ACL 2012. 9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a growing interest in crowdsourcing\nmethodologies to be used in experimental research for NLP tasks. In particular,\nevaluation of systems and theories about persuasion is difficult to accommodate\nwithin existing frameworks. In this paper we present a new cheap and fast\nmethodology that allows fast experiment building and evaluation with\nfully-automated analysis at a low cost. The central idea is exploiting existing\ncommercial tools for advertising on the web, such as Google AdWords, to measure\nmessage impact in an ecological setting. The paper includes a description of\nthe approach, tips for how to use AdWords for scientific research, and results\nof pilot experiments on the impact of affective text variations which confirm\nthe effectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 13:22:44 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Guerini", "Marco", ""], ["Strapparava", "Carlo", ""], ["Stock", "Oliviero", ""]]}, {"id": "1204.5852", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Mohammad Alwani", "title": "Context-sensitive Spelling Correction Using Google Web 1T 5-Gram\n  Information", "comments": "LACSC - Lebanese Association for Computational Sciences -\n  http://www.lacsc.org", "journal-ref": "Computer and Information Science, Vol. 5, No. 3, May 2012", "doi": "10.5539/cis.v5n3p37", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computing, spell checking is the process of detecting and sometimes\nproviding spelling suggestions for incorrectly spelled words in a text.\nBasically, a spell checker is a computer program that uses a dictionary of\nwords to perform spell checking. The bigger the dictionary is, the higher is\nthe error detection rate. The fact that spell checkers are based on regular\ndictionaries, they suffer from data sparseness problem as they cannot capture\nlarge vocabulary of words including proper names, domain-specific terms,\ntechnical jargons, special acronyms, and terminologies. As a result, they\nexhibit low error detection rate and often fail to catch major errors in the\ntext. This paper proposes a new context-sensitive spelling correction method\nfor detecting and correcting non-word and real-word errors in digital text\ndocuments. The approach hinges around data statistics from Google Web 1T 5-gram\ndata set which consists of a big volume of n-gram word sequences, extracted\nfrom the World Wide Web. Fundamentally, the proposed method comprises an error\ndetector that detects misspellings, a candidate spellings generator based on a\ncharacter 2-gram model that generates correction suggestions, and an error\ncorrector that performs contextual error correction. Experiments conducted on a\nset of text documents from different domains and containing misspellings,\nshowed an outstanding spelling error correction rate and a drastic reduction of\nboth non-word and real-word errors. In a further study, the proposed algorithm\nis to be parallelized so as to lower the computational cost of the error\ndetection and correction processes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 07:44:18 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Bassil", "Youssef", ""], ["Alwani", "Mohammad", ""]]}, {"id": "1204.6362", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams and Adel Elsayed", "title": "A Corpus-based Evaluation of Lexical Components of a Domainspecific Text\n  to Knowledge Mapping Prototype", "comments": "2008 IEEE International Conference on Computer and Information\n  Technology (ICCIT 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The aim of this paper is to evaluate the lexical components of a Text to\nKnowledge Mapping (TKM) prototype. The prototype is domain-specific, the\npurpose of which is to map instructional text onto a knowledge domain. The\ncontext of the knowledge domain of the prototype is physics, specifically DC\nelectrical circuits. During development, the prototype has been tested with a\nlimited data set from the domain. The prototype now reached a stage where it\nneeds to be evaluated with a representative linguistic data set called corpus.\nA corpus is a collection of text drawn from typical sources which can be used\nas a test data set to evaluate NLP systems. As there is no available corpus for\nthe domain, we developed a representative corpus and annotated it with\nlinguistic information. The evaluation of the prototype considers one of its\ntwo main components- lexical knowledge base. With the corpus, the evaluation\nenriches the lexical knowledge resources like vocabulary and grammar structure.\nThis leads the prototype to parse a reasonable amount of sentences in the\ncorpus.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 03:48:16 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Shams", "Rushdi", ""], ["Elsayed", "Adel", ""]]}, {"id": "1204.6364", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams, Adel Elsayed, Quazi Mah-Zereen Akter", "title": "A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping\n  Prototype", "comments": "Journal of Computers, Academy Publishers 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)\nPrototype. The prototype is domain-specific, the purpose of which is to map\ninstructional text onto a knowledge domain. The context of the knowledge domain\nis DC electrical circuit. During development, the prototype has been tested\nwith a limited data set from the domain. The prototype reached a stage where it\nneeds to be evaluated with a representative linguistic data set called corpus.\nA corpus is a collection of text drawn from typical sources which can be used\nas a test data set to evaluate NLP systems. As there is no available corpus for\nthe domain, we developed and annotated a representative corpus. The evaluation\nof the prototype considers two of its major components- lexical components and\nknowledge model. Evaluation on lexical components enriches the lexical\nresources of the prototype like vocabulary and grammar structures. This leads\nthe prototype to parse a reasonable amount of sentences in the corpus. While\ndealing with the lexicon was straight forward, the identification and\nextraction of appropriate semantic relations was much more involved. It was\nnecessary, therefore, to manually develop a conceptual structure for the domain\nto formulate a domain-specific framework of semantic relations. The framework\nof semantic relationsthat has resulted from this study consisted of 55\nrelations, out of which 42 have inverse relations. We also conducted rhetorical\nanalysis on the corpus to prove its representativeness in conveying semantic.\nFinally, we conducted a topical and discourse analysis on the corpus to analyze\nthe coverage of discourse by the prototype.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 03:52:21 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Shams", "Rushdi", ""], ["Elsayed", "Adel", ""], ["Akter", "Quazi Mah-Zereen", ""]]}, {"id": "1204.6441", "submitter": "Daniel Gayo Avello", "authors": "Daniel Gayo-Avello", "title": "\"I Wanted to Predict Elections with Twitter and all I got was this Lousy\n  Paper\" -- A Balanced Survey on Election Prediction using Twitter Data", "comments": "13 pages, no figures. Annotated bibliography of 25 papers regarding\n  electoral prediction from Twitter data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting X from Twitter is a popular fad within the Twitter research\nsubculture. It seems both appealing and relatively easy. Among such kind of\nstudies, electoral prediction is maybe the most attractive, and at this moment\nthere is a growing body of literature on such a topic. This is not only an\ninteresting research problem but, above all, it is extremely difficult.\nHowever, most of the authors seem to be more interested in claiming positive\nresults than in providing sound and reproducible methods. It is also especially\nworrisome that many recent papers seem to only acknowledge those studies\nsupporting the idea of Twitter predicting elections, instead of conducting a\nbalanced literature review showing both sides of the matter. After reading many\nof such papers I have decided to write such a survey myself. Hence, in this\npaper, every study relevant to the matter of electoral prediction using social\nmedia is commented. From this review it can be concluded that the predictive\npower of Twitter regarding elections has been greatly exaggerated, and that\nhard research problems still lie ahead.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 22:18:06 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Gayo-Avello", "Daniel", ""]]}]