[{"id": "2107.00042", "submitter": "Jaume Baixeries", "authors": "Neus Catal\\`a, Jaume Baixeries, Ramon Ferrer-Cancho, Llu\\'is Padr\\'o\n  and Antoni Hern\\'andez-Fern\\'andez", "title": "Zipf's laws of meaning in Catalan", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his pioneering research, G. K. Zipf formulated a couple of statistical\nlaws on the relationship between the frequency of a word with its number of\nmeanings: the law of meaning distribution, relating the frequency of a word and\nits frequency rank, and the meaning-frequency law, relating the frequency of a\nword with its number of meanings. Although these laws were formulated more than\nhalf a century ago, they have been only investigated in a few languages. Here\nwe present the first study of these laws in Catalan.\n  We verify these laws in Catalan via the relationship among their exponents\nand that of the rank-frequency law. We present a new protocol for the analysis\nof these Zipfian laws that can be extended to other languages. We report the\nfirst evidence of two marked regimes for these laws in written language and\nspeech, paralleling the two regimes in Zipf's rank-frequency law in large\nmulti-author corpora discovered in early 2000s. Finally, the implications of\nthese two regimes will be discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 18:06:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Catal\u00e0", "Neus", ""], ["Baixeries", "Jaume", ""], ["Ferrer-Cancho", "Ramon", ""], ["Padr\u00f3", "Llu\u00eds", ""], ["Hern\u00e1ndez-Fern\u00e1ndez", "Antoni", ""]]}, {"id": "2107.00061", "submitter": "Elizabeth Clark", "authors": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin\n  Gururangan, Noah A. Smith", "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated\n  Text", "comments": "references added, corrected typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human evaluations are typically considered the gold standard in natural\nlanguage generation, but as models' fluency improves, how well can evaluators\ndetect and judge machine-generated text? We run a study assessing non-experts'\nability to distinguish between human- and machine-authored text (GPT2 and GPT3)\nin three domains (stories, news articles, and recipes). We find that, without\ntraining, evaluators distinguished between GPT3- and human-authored text at\nrandom chance level. We explore three approaches for quickly training\nevaluators to better identify GPT3-authored text (detailed instructions,\nannotated examples, and paired examples) and find that while evaluators'\naccuracy improved up to 55%, it did not significantly improve across the three\ndomains. Given the inconsistent results across text domains and the often\ncontradictory reasons evaluators gave for their judgments, we examine the role\nuntrained human evaluations play in NLG evaluation and provide recommendations\nto NLG researchers for improving human evaluations of text generated from\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:00:25 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 06:07:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Clark", "Elizabeth", ""], ["August", "Tal", ""], ["Serrano", "Sofia", ""], ["Haduong", "Nikita", ""], ["Gururangan", "Suchin", ""], ["Smith", "Noah A.", ""]]}, {"id": "2107.00077", "submitter": "William McCarthy", "authors": "William P. McCarthy, Robert D. Hawkins, Haoliang Wang, Cameron\n  Holdaway, Judith E. Fan", "title": "Learning to communicate about shared procedural abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world tasks require agents to coordinate their behavior to achieve\nshared goals. Successful collaboration requires not only adopting the same\ncommunicative conventions, but also grounding these conventions in the same\ntask-appropriate conceptual abstractions. We investigate how humans use natural\nlanguage to collaboratively solve physical assembly problems more effectively\nover time. Human participants were paired up in an online environment to\nreconstruct scenes containing two block towers. One participant could see the\ntarget towers, and sent assembly instructions for the other participant to\nreconstruct. Participants provided increasingly concise instructions across\nrepeated attempts on each pair of towers, using higher-level referring\nexpressions that captured each scene's hierarchical structure. To explain these\nfindings, we extend recent probabilistic models of ad-hoc convention formation\nwith an explicit perceptual learning mechanism. These results shed light on the\ninductive biases that enable intelligent agents to coordinate upon shared\nprocedural abstractions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:59:11 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["McCarthy", "William P.", ""], ["Hawkins", "Robert D.", ""], ["Wang", "Haoliang", ""], ["Holdaway", "Cameron", ""], ["Fan", "Judith E.", ""]]}, {"id": "2107.00080", "submitter": "Benajmin Radford J", "authors": "Benjamin J. Radford", "title": "Regressing Location on Text for Probabilistic Geocoding", "comments": "5 pages, 4 figures. Proceedings of the CASE Workshop at ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text data are an important source of detailed information about social and\npolitical events. Automated systems parse large volumes of text data to infer\nor extract structured information that describes actors, actions, dates, times,\nand locations. One of these sub-tasks is geocoding: predicting the geographic\ncoordinates associated with events or locations described by a given text. We\npresent an end-to-end probabilistic model for geocoding text data.\nAdditionally, we collect a novel data set for evaluating the performance of\ngeocoding systems. We compare the model-based solution, called ELECTRo-map, to\nthe current state-of-the-art open source system for geocoding texts for event\ndata. Finally, we discuss the benefits of end-to-end model-based geocoding,\nincluding principled uncertainty estimation and the ability of these models to\nleverage contextual information.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 20:04:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Radford", "Benjamin J.", ""]]}, {"id": "2107.00124", "submitter": "Ashwinkumar Ganesan", "authors": "Ashwinkumar Ganesan, Francis Ferraro, Tim Oates", "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold\n  Alignment", "comments": null, "journal-ref": "Findings of the Association for Computational Linguistics: ACL\n  2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bi-Directional Manifold Alignment (BDMA) that learns a\nnon-linear mapping between two manifolds by explicitly training it to be\nbijective. We demonstrate BDMA by training a model for a pair of languages\nrather than individual, directed source and target combinations, reducing the\nnumber of models by 50%. We show that models trained with BDMA in the \"forward\"\n(source to target) direction can successfully map words in the \"reverse\"\n(target to source) direction, yielding equivalent (or better) performance to\nstandard unidirectional translation models where the source and target language\nis flipped. We also show how BDMA reduces the overall size of the model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:13:42 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ganesan", "Ashwinkumar", ""], ["Ferraro", "Francis", ""], ["Oates", "Tim", ""]]}, {"id": "2107.00152", "submitter": "Shuyang Cao", "authors": "Shuyang Cao and Lu Wang", "title": "Controllable Open-ended Question Generation with A New Question Type\n  Ontology", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the less-explored task of generating open-ended questions that\nare typically answered by multiple sentences. We first define a new question\ntype ontology which differentiates the nuanced nature of questions better than\nwidely used question words. A new dataset with 4,959 questions is labeled based\non the new ontology. We then propose a novel question type-aware question\ngeneration framework, augmented by a semantic graph representation, to jointly\npredict question focuses and produce the question. Based on this framework, we\nfurther use both exemplars and automatically generated templates to improve\ncontrollability and diversity. Experiments on two newly collected large-scale\ndatasets show that our model improves question quality over competitive\ncomparisons based on automatic metrics. Human judges also rate our model\noutputs highly in answerability, coverage of scope, and overall quality.\nFinally, our model variants with templates can produce questions with enhanced\ncontrollability and diversity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 00:02:03 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Cao", "Shuyang", ""], ["Wang", "Lu", ""]]}, {"id": "2107.00175", "submitter": "Keli Xie", "authors": "Keli Xie, Siyuan Lu, Meiqi Wang, Zhongfeng Wang", "title": "Elbert: Fast Albert with Confidence-Window Based Early Exit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success in Natural Language Processing (NLP) area, large\npre-trained language models like BERT are not well-suited for\nresource-constrained or real-time applications owing to the large number of\nparameters and slow inference speed. Recently, compressing and accelerating\nBERT have become important topics. By incorporating a parameter-sharing\nstrategy, ALBERT greatly reduces the number of parameters while achieving\ncompetitive performance. Nevertheless, ALBERT still suffers from a long\ninference time. In this work, we propose the ELBERT, which significantly\nimproves the average inference speed compared to ALBERT due to the proposed\nconfidence-window based early exit mechanism, without introducing additional\nparameters or extra training overhead. Experimental results show that ELBERT\nachieves an adaptive inference speedup varying from 2$\\times$ to 10$\\times$\nwith negligible accuracy degradation compared to ALBERT on various datasets.\nBesides, ELBERT achieves higher accuracy than existing early exit methods used\nfor accelerating BERT under the same computation cost. Furthermore, to\nunderstand the principle of the early exit mechanism, we also visualize the\ndecision-making process of it in ELBERT.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:02:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Xie", "Keli", ""], ["Lu", "Siyuan", ""], ["Wang", "Meiqi", ""], ["Wang", "Zhongfeng", ""]]}, {"id": "2107.00176", "submitter": "Shweta Yadav", "authors": "Shweta Yadav, Deepak Gupta, Asma Ben Abacha and Dina Demner-Fushman", "title": "Reinforcement Learning for Abstractive Question Summarization with\n  Question-aware Semantic Rewards", "comments": "To appear at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growth of online consumer health questions has led to the necessity for\nreliable and accurate question answering systems. A recent study showed that\nmanual summarization of consumer health questions brings significant\nimprovement in retrieving relevant answers. However, the automatic\nsummarization of long questions is a challenging task due to the lack of\ntraining data and the complexity of the related subtasks, such as the question\nfocus and type recognition. In this paper, we introduce a reinforcement\nlearning-based framework for abstractive question summarization. We propose two\nnovel rewards obtained from the downstream tasks of (i) question-type\nidentification and (ii) question-focus recognition to regularize the question\ngeneration model. These rewards ensure the generation of semantically valid\nquestions and encourage the inclusion of key medical entities/foci in the\nquestion summary. We evaluated our proposed method on two benchmark datasets\nand achieved higher performance over state-of-the-art models. The manual\nevaluation of the summaries reveals that the generated questions are more\ndiverse and have fewer factual inconsistencies than the baseline summaries\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:06:46 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yadav", "Shweta", ""], ["Gupta", "Deepak", ""], ["Abacha", "Asma Ben", ""], ["Demner-Fushman", "Dina", ""]]}, {"id": "2107.00186", "submitter": "Akshat Gupta", "authors": "Zhiyuan Guo, Yuexin Li, Guo Chen, Xingyu Chen, Akshat Gupta", "title": "Word-Free Spoken Language Understanding for Mandarin-Chinese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken dialogue systems such as Siri and Alexa provide great convenience to\npeople's everyday life. However, current spoken language understanding (SLU)\npipelines largely depend on automatic speech recognition (ASR) modules, which\nrequire a large amount of language-specific training data. In this paper, we\npropose a Transformer-based SLU system that works directly on phones. This\nacoustic-based SLU system consists of only two blocks and does not require the\npresence of ASR module. The first block is a universal phone recognition\nsystem, and the second block is a Transformer-based language model for phones.\nWe verify the effectiveness of the system on an intent classification dataset\nin Mandarin Chinese.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:31:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Guo", "Zhiyuan", ""], ["Li", "Yuexin", ""], ["Chen", "Guo", ""], ["Chen", "Xingyu", ""], ["Gupta", "Akshat", ""]]}, {"id": "2107.00189", "submitter": "Xiangyu Xi", "authors": "Xiangyu Xi, Wei Ye, Shikun Zhang, Quanxiu Wang, Huixing Jiang, Wei Wu", "title": "Capturing Event Argument Interaction via A Bi-Directional Entity-Level\n  Recurrent Decoder", "comments": null, "journal-ref": "ACL-IJCNLP 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capturing interactions among event arguments is an essential step towards\nrobust event argument extraction (EAE). However, existing efforts in this\ndirection suffer from two limitations: 1) The argument role type information of\ncontextual entities is mainly utilized as training signals, ignoring the\npotential merits of directly adopting it as semantically rich input features;\n2) The argument-level sequential semantics, which implies the overall\ndistribution pattern of argument roles over an event mention, is not well\ncharacterized. To tackle the above two bottlenecks, we formalize EAE as a\nSeq2Seq-like learning problem for the first time, where a sentence with a\nspecific event trigger is mapped to a sequence of event argument roles. A\nneural architecture with a novel Bi-directional Entity-level Recurrent Decoder\n(BERD) is proposed to generate argument roles by incorporating contextual\nentities' argument role predictions, like a word-by-word text generation\nprocess, thereby distinguishing implicit argument distribution patterns within\nan event more accurately.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:55:12 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Xi", "Xiangyu", ""], ["Ye", "Wei", ""], ["Zhang", "Shikun", ""], ["Wang", "Quanxiu", ""], ["Jiang", "Huixing", ""], ["Wu", "Wei", ""]]}, {"id": "2107.00279", "submitter": "Dan Liu", "authors": "Dan Liu, Mengge Du, Xiaoxi Li, Yuchen Hu, Lirong Dai", "title": "The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at\n  IWSLT 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous\nSpeech Translation task. We proposed a novel simultaneous translation model,\nCross Attention Augmented Transducer (CAAT), which extends conventional RNN-T\nto sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous\ntranslation. Experiments on speech-to-text (S2T) and text-to-text (T2T)\nsimultaneous translation tasks shows CAAT achieves better quality-latency\ntrade-offs compared to \\textit{wait-k}, one of the previous state-of-the-art\napproaches. Based on CAAT architecture and data augmentation, we build S2T and\nT2T simultaneous translation systems in this evaluation campaign. Compared to\nlast year's optimal systems, our S2T simultaneous translation system improves\nby an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous\ntranslation system improves by an average of 4.6 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:09:00 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 09:34:51 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Dan", ""], ["Du", "Mengge", ""], ["Li", "Xiaoxi", ""], ["Hu", "Yuchen", ""], ["Dai", "Lirong", ""]]}, {"id": "2107.00281", "submitter": "Anne Lauscher", "authors": "Anne Lauscher, Henning Wachsmuth, Iryna Gurevych, and Goran Glava\\v{s}", "title": "Scientia Potentia Est -- On the Role of Knowledge in Computational\n  Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite extensive research in the past years, the computational modeling of\nargumentation remains challenging. The primary reason lies in the inherent\ncomplexity of the human processes behind, which commonly requires the\nintegration of extensive knowledge far beyond what is needed for many other\nnatural language understanding tasks. Existing work on the mining, assessment,\nreasoning, and generation of arguments acknowledges this issue, calling for\nmore research on the integration of common sense and world knowledge into\ncomputational models. However, a systematic effort to collect and organize the\ntypes of knowledge needed is still missing, hindering targeted progress in the\nfield. In this opinionated survey paper, we address the issue by (1) proposing\na pyramid of types of knowledge required in computational argumentation, (2)\nbriefly discussing the state of the art on the role and integration of these\ntypes in the field, and (3) outlining the main challenges for future work.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:12:41 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lauscher", "Anne", ""], ["Wachsmuth", "Henning", ""], ["Gurevych", "Iryna", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2107.00308", "submitter": "Bence Halpern", "authors": "Bence Mark Halpern, Julian Fritsch, Enno Hermann, Rob van Son, Odette\n  Scharenborg, Mathew Magimai.-Doss", "title": "An Objective Evaluation Framework for Pathological Speech Synthesis", "comments": "4 pages, 4 figures. Accepted to the ITG Conference on Speech\n  Communication | 29.09.2021 - 01.10.2021 | Kiel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of pathological speech systems is currently hindered by the\nlack of a standardised objective evaluation framework. In this work, (1) we\nutilise existing detection and analysis techniques to propose a general\nframework for the consistent evaluation of synthetic pathological speech. This\nframework evaluates the voice quality and the intelligibility aspects of speech\nand is shown to be complementary using our experiments. (2) Using our proposed\nevaluation framework, we develop and test a dysarthric voice conversion system\n(VC) using CycleGAN-VC and a PSOLA-based speech rate modification technique. We\nshow that the developed system is able to synthesise dysarthric speech with\ndifferent levels of speech intelligibility.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:55:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Halpern", "Bence Mark", ""], ["Fritsch", "Julian", ""], ["Hermann", "Enno", ""], ["van Son", "Rob", ""], ["Scharenborg", "Odette", ""], ["-Doss", "Mathew Magimai.", ""]]}, {"id": "2107.00315", "submitter": "Neeraj Varshney", "authors": "Neeraj Varshney, Swaroop Mishra, Chitta Baral", "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP\n  Systems", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:08:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Varshney", "Neeraj", ""], ["Mishra", "Swaroop", ""], ["Baral", "Chitta", ""]]}, {"id": "2107.00318", "submitter": "Ryokan Ri", "authors": "Ryokan Ri, Toshiaki Nakazawa and Yoshimasa Tsuruoka", "title": "Zero-pronoun Data Augmentation for Japanese-to-English Translation", "comments": "WAT2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For Japanese-to-English translation, zero pronouns in Japanese pose a\nchallenge, since the model needs to infer and produce the corresponding pronoun\nin the target side of the English sentence. However, although fully resolving\nzero pronouns often needs discourse context, in some cases, the local context\nwithin a sentence gives clues to the inference of the zero pronoun. In this\nstudy, we propose a data augmentation method that provides additional training\nsignals for the translation model to learn correlations between local context\nand zero pronouns. We show that the proposed method significantly improves the\naccuracy of zero pronoun translation with machine translation experiments in\nthe conversational domain.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:17:59 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ri", "Ryokan", ""], ["Nakazawa", "Toshiaki", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "2107.00323", "submitter": "Pouya Pezeshkpour", "authors": "Pouya Pezeshkpour, Sarthak Jain, Sameer Singh and Byron C. Wallace", "title": "Combining Feature and Instance Attribution to Detect Artifacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training the large deep neural networks that dominate NLP requires large\ndatasets. Many of these are collected automatically or via crowdsourcing, and\nmay exhibit systematic biases or annotation artifacts. By the latter, we mean\ncorrelations between inputs and outputs that are spurious, insofar as they do\nnot represent a generally held causal relationship between features and\nclasses; models that exploit such correlations may appear to perform a given\ntask well, but fail on out of sample data. In this paper we propose methods to\nfacilitate identification of training data artifacts, using new hybrid\napproaches that combine saliency maps (which highlight important input\nfeatures) with instance attribution methods (which retrieve training samples\ninfluential to a given prediction). We show that this proposed training-feature\nattribution approach can be used to uncover artifacts in training data, and use\nit to identify previously unreported artifacts in a few standard NLP datasets.\nWe execute a small user study to evaluate whether these methods are useful to\nNLP researchers in practice, with promising results. We make code for all\nmethods and experiments in this paper available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:26:13 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Pezeshkpour", "Pouya", ""], ["Jain", "Sarthak", ""], ["Singh", "Sameer", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2107.00333", "submitter": "Itziar Gonzalez-Dios", "authors": "Xavier G\\'omez Guinovart, Itziar Gonzalez-Dios, Antoni Oliver, German\n  Rigau", "title": "Multilingual Central Repository: a Cross-lingual Framework for\n  Developing Wordnets", "comments": "11 pages, 1 figure. To appear in Special Issue on Linking,\n  Integrating and Extending Wordnets, Linguistic Issues in Language Technology\n  (LiLT) Volume 10, Issue 4, Sep 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language resources are necessary for language processing,but building them is\ncostly, involves many researches from different areas and needs constant\nupdating. In this paper, we describe the crosslingual framework used for\ndeveloping the Multilingual Central Repository (MCR), a multilingual knowledge\nbase that includes wordnets of Basque, Catalan, English, Galician, Portuguese,\nSpanish and the following ontologies: Base Concepts, Top Ontology, WordNet\nDomains and Suggested Upper Merged Ontology. We present the story of MCR, its\nstate in 2017 and the developed tools.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:50:55 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 12:10:11 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Guinovart", "Xavier G\u00f3mez", ""], ["Gonzalez-Dios", "Itziar", ""], ["Oliver", "Antoni", ""], ["Rigau", "German", ""]]}, {"id": "2107.00334", "submitter": "Ryokan Ri", "authors": "Ryokan Ri, Toshiaki Nakazawa and Yoshimasa Tsuruoka", "title": "Modeling Target-side Inflection in Placeholder Translation", "comments": "MT Summit 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Placeholder translation systems enable the users to specify how a specific\nphrase is translated in the output sentence. The system is trained to output\nspecial placeholder tokens, and the user-specified term is injected into the\noutput through the context-free replacement of the placeholder token. However,\nthis approach could result in ungrammatical sentences because it is often the\ncase that the specified term needs to be inflected according to the context of\nthe output, which is unknown before the translation. To address this problem,\nwe propose a novel method of placeholder translation that can inflect specified\nterms according to the grammatical construction of the output sentence. We\nextend the sequence-to-sequence architecture with a character-level decoder\nthat takes the lemma of a user-specified term and the words generated from the\nword-level decoder to output the correct inflected form of the lemma. We\nevaluate our approach with a Japanese-to-English translation task in the\nscientific writing domain, and show that our model can incorporate specified\nterms in the correct form more successfully than other comparable models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:54:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ri", "Ryokan", ""], ["Nakazawa", "Toshiaki", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "2107.00368", "submitter": "Razieh Baradaran", "authors": "Razieh Baradaran and Hossein Amirkhani", "title": "Ensemble Learning-Based Approach for Improving Generalization Capability\n  of Machine Reading Comprehension Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:11:17 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 17:28:31 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Baradaran", "Razieh", ""], ["Amirkhani", "Hossein", ""]]}, {"id": "2107.00395", "submitter": "Baotian Hu", "authors": "Yunxin Li, Yu Zhao, Baotian Hu, Qingcai Chen, Yang Xiang, Xiaolong\n  Wang, Yuxin Ding, Lin Ma", "title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character\n  with its Glyph", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:14:05 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Yunxin", ""], ["Zhao", "Yu", ""], ["Hu", "Baotian", ""], ["Chen", "Qingcai", ""], ["Xiang", "Yang", ""], ["Wang", "Xiaolong", ""], ["Ding", "Yuxin", ""], ["Ma", "Lin", ""]]}, {"id": "2107.00411", "submitter": "Amit Gajbhiye", "authors": "Amit Gajbhiye, Marina Fomicheva, Fernando Alva-Manchego, Fr\\'ed\\'eric\n  Blain, Abiola Obamuyide, Nikolaos Aletras, Lucia Specia", "title": "Knowledge Distillation for Quality Estimation", "comments": "ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quality Estimation (QE) is the task of automatically predicting Machine\nTranslation quality in the absence of reference translations, making it\napplicable in real-time settings, such as translating online social media\nconversations. Recent success in QE stems from the use of multilingual\npre-trained representations, where very large models lead to impressive\nresults. However, the inference time, disk and memory requirements of such\nmodels do not allow for wide usage in the real world. Models trained on\ndistilled pre-trained representations remain prohibitively large for many usage\nscenarios. We instead propose to directly transfer knowledge from a strong QE\nteacher model to a much smaller model with a different, shallower architecture.\nWe show that this approach, in combination with data augmentation, leads to\nlight-weight QE models that perform competitively with distilled pre-trained\nrepresentations with 8x fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:36:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Gajbhiye", "Amit", ""], ["Fomicheva", "Marina", ""], ["Alva-Manchego", "Fernando", ""], ["Blain", "Fr\u00e9d\u00e9ric", ""], ["Obamuyide", "Abiola", ""], ["Aletras", "Nikolaos", ""], ["Specia", "Lucia", ""]]}, {"id": "2107.00414", "submitter": "Anne Lauscher", "authors": "Anne Lauscher, Brandon Ko, Bailey Kuhl, Sophie Johnson, David Jurgens,\n  Arman Cohan, Kyle Lo", "title": "MultiCite: Modeling realistic citations requires moving beyond the\n  single-sentence single-label setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citation context analysis (CCA) is an important task in natural language\nprocessing that studies how and why scholars discuss each others' work. Despite\nbeing studied for decades, traditional frameworks for CCA have largely relied\non overly-simplistic assumptions of how authors cite, which ignore several\nimportant phenomena. For instance, scholarly papers often contain rich\ndiscussions of cited work that span multiple sentences and express multiple\nintents concurrently. Yet, CCA is typically approached as a single-sentence,\nsingle-label classification task, and thus existing datasets fail to capture\nthis interesting discourse. In our work, we address this research gap by\nproposing a novel framework for CCA as a document-level context extraction and\nlabeling task. We release MultiCite, a new dataset of 12,653 citation contexts\nfrom over 1,200 computational linguistics papers. Not only is it the largest\ncollection of expert-annotated citation contexts to-date, MultiCite contains\nmulti-sentence, multi-label citation contexts within full paper texts. Finally,\nwe demonstrate how our dataset, while still usable for training classic CCA\nmodels, also supports the development of new types of models for CCA beyond\nfixed-width text classification. We release our code and dataset at\nhttps://github.com/allenai/multicite.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:54:23 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lauscher", "Anne", ""], ["Ko", "Brandon", ""], ["Kuhl", "Bailey", ""], ["Johnson", "Sophie", ""], ["Jurgens", "David", ""], ["Cohan", "Arman", ""], ["Lo", "Kyle", ""]]}, {"id": "2107.00439", "submitter": "Shammur Absar Chowdhury", "authors": "Shammur Absar Chowdhury, Nadir Durrani, Ahmed Ali", "title": "What do End-to-End Speech Models Learn about Speaker, Language and\n  Channel Information? A Layer-wise and Neuron-level Analysis", "comments": "Submitted to CSL. Keywords: Speech, Neuron Analysis,\n  Interpretibility, Diagnostic Classifier, AI explainability, End-to-End\n  Architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  End-to-end DNN architectures have pushed the state-of-the-art in speech\ntechnologies, as well as in other spheres of AI, leading researchers to train\nmore complex and deeper models. These improvements came at the cost of\ntransparency. DNNs are innately opaque and difficult to interpret. We no longer\nunderstand what features are learned, where they are preserved, and how they\ninter-operate. Such an analysis is important for better model understanding,\ndebugging and to ensure fairness in ethical decision making. In this work, we\nanalyze the representations trained within deep speech models, towards the task\nof speaker recognition, dialect identification and reconstruction of masked\nsignals. We carry a layer- and neuron-level analysis on the utterance-level\nrepresentations captured within pretrained speech models for speaker, language\nand channel properties. We study: is this information captured in the learned\nrepresentations? where is it preserved? how is it distributed? and can we\nidentify a minimal subset of network that posses this information. Using\ndiagnostic classifiers, we answered these questions. Our results reveal: (i)\nchannel and gender information is omnipresent and is redundantly distributed\n(ii) complex properties such as dialectal information is encoded only in the\ntask-oriented pretrained network and is localised in the upper layers (iii) a\nminimal subset of neurons can be extracted to encode the predefined property\n(iv) salient neurons are sometimes shared between properties and can highlights\npresence of biases in the network. Our cross-architectural comparison indicates\nthat (v) the pretrained models captures speaker-invariant information and (vi)\nthe pretrained CNNs models are competitive to the Transformers for encoding\ninformation for the studied properties. To the best of our knowledge, this is\nthe first study to investigate neuron analysis on the speech models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:32:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chowdhury", "Shammur Absar", ""], ["Durrani", "Nadir", ""], ["Ali", "Ahmed", ""]]}, {"id": "2107.00440", "submitter": "Piji Li", "authors": "Dong Wang, Ning Ding, Piji Li, Hai-Tao Zheng", "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural\n  Language Understanding", "comments": "ACL 2021, Main Conference, Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite pre-trained language models have proven useful for learning\nhigh-quality semantic representations, these models are still vulnerable to\nsimple perturbations. Recent works aimed to improve the robustness of\npre-trained models mainly focus on adversarial training from perturbed examples\nwith similar semantics, neglecting the utilization of different or even\nopposite semantics. Different from the image processing field, the text is\ndiscrete and few word substitutions can cause significant semantic changes. To\nstudy the impact of semantics caused by small perturbations, we conduct a\nseries of pilot experiments and surprisingly find that adversarial training is\nuseless or even harmful for the model to detect these semantic changes. To\naddress this problem, we propose Contrastive Learning with semantIc Negative\nExamples (CLINE), which constructs semantic negative examples unsupervised to\nimprove the robustness under semantically adversarial attacking. By comparing\nwith similar and opposite semantic examples, the model can effectively perceive\nthe semantic changes caused by small perturbations. Empirical results show that\nour approach yields substantial improvements on a range of sentiment analysis,\nreasoning, and reading comprehension tasks. And CLINE also ensures the\ncompactness within the same semantics and separability across different\nsemantics in sentence-level.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:34:12 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Dong", ""], ["Ding", "Ning", ""], ["Li", "Piji", ""], ["Zheng", "Hai-Tao", ""]]}, {"id": "2107.00596", "submitter": "Shweta Yadav", "authors": "Sriram Pingali, Shweta Yadav, Pratik Dutta, and Sriparna Saha", "title": "Multimodal Graph-based Transformer Framework for Biomedical Relation\n  Extraction", "comments": "To appear in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The recent advancement of pre-trained Transformer models has propelled the\ndevelopment of effective text mining models across various biomedical tasks.\nHowever, these models are primarily learned on the textual data and often lack\nthe domain knowledge of the entities to capture the context beyond the\nsentence. In this study, we introduced a novel framework that enables the model\nto learn multi-omnics biological information about entities (proteins) with the\nhelp of additional multi-modal cues like molecular structure. Towards this,\nrather developing modality-specific architectures, we devise a generalized and\noptimized graph based multi-modal learning mechanism that utilizes the\nGraphBERT model to encode the textual and molecular structure information and\nexploit the underlying features of various modalities to enable end-to-end\nlearning. We evaluated our proposed method on ProteinProtein Interaction task\nfrom the biomedical corpus, where our proposed generalized approach is observed\nto be benefited by the additional domain-specific modality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:37:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Pingali", "Sriram", ""], ["Yadav", "Shweta", ""], ["Dutta", "Pratik", ""], ["Saha", "Sriparna", ""]]}, {"id": "2107.00635", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Tatsuya Kawahara", "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency\n  of Streaming Monotonic Attention ASR", "comments": "Accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:49:31 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 17:58:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2107.00636", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Brian Yan, Siddharth Dalmia, Pengcheng Guo, Jiatong\n  Shi, Kevin Duh, Shinji Watanabe", "title": "ESPnet-ST IWSLT 2021 Offline Speech Translation System", "comments": "IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the ESPnet-ST group's IWSLT 2021 submission in the\noffline speech translation track. This year we made various efforts on training\ndata, architecture, and audio segmentation. On the data side, we investigated\nsequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech\ntranslation. Specifically, we used multi-referenced SeqKD from multiple\nteachers trained on different amounts of bitext. On the architecture side, we\nadopted the Conformer encoder and the Multi-Decoder architecture, which equips\ndedicated decoders for speech recognition and translation tasks in a unified\nencoder-decoder model and enables search in both source and target language\nspaces during inference. We also significantly improved audio segmentation by\nusing the pyannote.audio toolkit and merging multiple short segments for long\ncontext modeling. Experimental evaluations showed that each of them contributed\nto large improvements in translation performance. Our best E2E system combined\nall the above techniques with model ensembling and achieved 31.4 BLEU on the\n2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of\ntst2021.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:49:43 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 15:43:01 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Yan", "Brian", ""], ["Dalmia", "Siddharth", ""], ["Guo", "Pengcheng", ""], ["Shi", "Jiatong", ""], ["Duh", "Kevin", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2107.00653", "submitter": "Yu Shi", "authors": "Yu Shi", "title": "Transformer-F: A Transformer network with effective methods for learning\n  universal sentence representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 03:20:11 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Shi", "Yu", ""]]}, {"id": "2107.00676", "submitter": "Mitesh M. Khapra", "authors": "Sumanth Doddapaneni, Gowtham Ramesh, Anoop Kunchukuttan, Pratyush\n  Kumar, Mitesh M. Khapra", "title": "A Primer on Pretrained Multilingual Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \\textit{etc.}\nhave emerged as a viable option for bringing the power of pretraining to a\nlarge number of languages. Given their success in zero shot transfer learning,\nthere has emerged a large body of work in (i) building bigger MLLMs covering a\nlarge number of languages (ii) creating exhaustive benchmarks covering a wider\nvariety of tasks and languages for evaluating MLLMs (iii) analysing the\nperformance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks\n(iv) understanding the universal language patterns (if any) learnt by MLLMs and\n(v) augmenting the (often) limited capacity of MLLMs to improve their\nperformance on seen or even unseen languages. In this survey, we review the\nexisting literature covering the above broad areas of research pertaining to\nMLLMs. Based on our survey, we recommend some promising directions of future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:01:46 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Doddapaneni", "Sumanth", ""], ["Ramesh", "Gowtham", ""], ["Kunchukuttan", "Anoop", ""], ["Kumar", "Pratyush", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "2107.00692", "submitter": "Brendan Shillingford", "authors": "Brendan Shillingford, Yannis Assael, Misha Denil", "title": "Interactive decoding of words from visual speech recognition models", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes an interactive decoding method to improve the performance\nof visual speech recognition systems using user input to compensate for the\ninherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines,\nwhich produce phonemes and feed these through a finite state transducer, our\nmethod instead expands words in lockstep, facilitating the insertion of\ninteraction points at each word position. Interaction points enable us to\nsolicit input during decoding, allowing users to interactively direct the\ndecoding process. We simulate the behavior of user input using an oracle to\ngive an automated evaluation, and show promise for the use of this method for\ntext input.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:38:01 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Shillingford", "Brendan", ""], ["Assael", "Yannis", ""], ["Denil", "Misha", ""]]}, {"id": "2107.00730", "submitter": "Anubhab Ghosh", "authors": "Anubhab Ghosh, Antoine Honor\\'e, Dong Liu, Gustav Eje Henter, Saikat\n  Chatterjee", "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech\n  Phones with Explainability", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 20:10:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ghosh", "Anubhab", ""], ["Honor\u00e9", "Antoine", ""], ["Liu", "Dong", ""], ["Henter", "Gustav Eje", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "2107.00753", "submitter": "Nitish Joshi", "authors": "Nitish Joshi, He He", "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 21:46:43 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Joshi", "Nitish", ""], ["He", "He", ""]]}, {"id": "2107.00789", "submitter": "Komei Sugiura", "authors": "Motonari Kambara and Komei Sugiura", "title": "Case Relation Transformer: A Crossmodal Language Generation Model for\n  Fetching Instructions", "comments": "Accepted for presentation at IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 01:40:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kambara", "Motonari", ""], ["Sugiura", "Komei", ""]]}, {"id": "2107.00807", "submitter": "Nanjiang Jiang", "authors": "Nanjiang Jiang and Marie-Catherine de Marneffe", "title": "He Thinks He Knows Better than the Doctors: BERT for Event Factuality\n  Fails on Pragmatics", "comments": "to be published in TACL, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 02:47:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Jiang", "Nanjiang", ""], ["de Marneffe", "Marie-Catherine", ""]]}, {"id": "2107.00811", "submitter": "Komei Sugiura", "authors": "Shintaro Ishikawa and Komei Sugiura", "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language\n  Comprehension Model for Domestic Service Robots", "comments": "Accepted for presentation at IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 03:11:02 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ishikawa", "Shintaro", ""], ["Sugiura", "Komei", ""]]}, {"id": "2107.00841", "submitter": "Peng Gao", "authors": "Feng Gao, Jian-Cheng Ni, Peng Gao, Zi-Li Zhou, Yan-Yan Li, Hamido\n  Fujita", "title": "Heterogeneous Graph Attention Network for Multi-hop Machine Reading\n  Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-hop machine reading comprehension is a challenging task in natural\nlanguage processing, which requires more reasoning ability and explainability.\nSpectral models based on graph convolutional networks grant the inferring\nabilities and lead to competitive results, however, part of them still face the\nchallenge of analyzing the reasoning in a human-understandable way. Inspired by\nthe concept of the Grandmother Cells in cognitive neuroscience, a spatial graph\nattention framework named crname, imitating the procedure was proposed. This\nmodel is designed to assemble the semantic features in multi-angle\nrepresentations and automatically concentrate or alleviate the information for\nreasoning. The name \"crname\" is a metaphor for the pattern of the model: regard\nthe subjects of queries as the start points of clues, take the reasoning\nentities as bridge points, and consider the latent candidate entities as the\ngrandmother cells, and the clues end up in candidate entities. The proposed\nmodel allows us to visualize the reasoning graph and analyze the importance of\nedges connecting two entities and the selectivity in the mention and candidate\nnodes, which can be easier to be comprehended empirically. The official\nevaluations in open-domain multi-hop reading dataset WikiHop and Drug-drug\nInteractions dataset MedHop prove the validity of our approach and show the\nprobability of the application of the model in the molecular biology domain.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 05:29:39 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gao", "Feng", ""], ["Ni", "Jian-Cheng", ""], ["Gao", "Peng", ""], ["Zhou", "Zi-Li", ""], ["Li", "Yan-Yan", ""], ["Fujita", "Hamido", ""]]}, {"id": "2107.00910", "submitter": "Sehoon Kim", "authors": "Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Joseph Hassoun,\n  Kurt Keutzer", "title": "Learned Token Pruning for Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in deploying transformer models is their prohibitive\ninference cost, which quadratically scales with the input sequence length. This\nmakes it especially difficult to use transformers for processing long\nsequences. To address this, we present a novel Learned Token Pruning (LTP)\nmethod that reduces redundant tokens as the data passes through the different\nlayers of the transformer. In particular, LTP prunes tokens with an attention\nscore below a threshold value, which is learned during training. Importantly,\nour threshold based method avoids algorithmically expensive operations such as\ntop-k token selection which are used in prior token pruning methods, and also\nleads to structured pruning. We extensively test the performance of our\napproach on multiple GLUE tasks and show that our learned threshold based\nmethod consistently outperforms the prior state-of-the-art top-k token based\nmethod by up to ~2% higher accuracy with the same amount of FLOPs. Furthermore,\nour preliminary results show up to 1.4x and 1.9x throughput improvement on\nTesla T4 GPU and Intel Haswell CPU, respectively, with less than 1% of accuracy\ndrop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch\nand has been open-sourced.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:00:13 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kim", "Sehoon", ""], ["Shen", "Sheng", ""], ["Thorsley", "David", ""], ["Gholami", "Amir", ""], ["Hassoun", "Joseph", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2107.00927", "submitter": "Luisa M\\\"arz", "authors": "Luisa M\\\"arz, Stefan Schweter, Nina Poerner, Benjamin Roth and Hinrich\n  Sch\\\"utze", "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors", "comments": "14 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:37:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["M\u00e4rz", "Luisa", ""], ["Schweter", "Stefan", ""], ["Poerner", "Nina", ""], ["Roth", "Benjamin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2107.00941", "submitter": "Rahul Goel", "authors": "Raj Jagtap, Abhinav Kumar, Rahul Goel, Shakshi Sharma, Rajesh Sharma,\n  Clint P. George", "title": "Misinformation Detection on YouTube Using Video Captions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:02:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Jagtap", "Raj", ""], ["Kumar", "Abhinav", ""], ["Goel", "Rahul", ""], ["Sharma", "Shakshi", ""], ["Sharma", "Rajesh", ""], ["George", "Clint P.", ""]]}, {"id": "2107.00955", "submitter": "Anastasia Zhukova", "authors": "Anastasia Zhukova, Felix Hamborg, Karsten Donnay and Bela Gipp", "title": "Concept Identification of Directly and Indirectly Related Mentions\n  Referring to Groups of Persons", "comments": null, "journal-ref": "Diversity, Divergence, Dialogue (2021) 514-526", "doi": "10.1007/978-3-030-71292-1_40", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised concept identification through clustering, i.e., identification\nof semantically related words and phrases, is a common approach to identify\ncontextual primitives employed in various use cases, e.g., text dimension\nreduction, i.e., replace words with the concepts to reduce the vocabulary size,\nsummarization, and named entity resolution. We demonstrate the first results of\nan unsupervised approach for the identification of groups of persons as actors\nextracted from a set of related articles. Specifically, the approach clusters\nmentions of groups of persons that act as non-named entity actors in the texts,\ne.g., \"migrant families\" = \"asylum-seekers.\" Compared to our baseline, the\napproach keeps the mentions of the geopolitical entities separated, e.g., \"Iran\nleaders\" != \"European leaders,\" and clusters (in)directly related mentions with\ndiverse wording, e.g., \"American officials\" = \"Trump Administration.\"\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:38:43 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zhukova", "Anastasia", ""], ["Hamborg", "Felix", ""], ["Donnay", "Karsten", ""], ["Gipp", "Bela", ""]]}, {"id": "2107.00956", "submitter": "R\\'emy Portelas", "authors": "Grgur Kova\\v{c}, R\\'emy Portelas, Katja Hofmann, Pierre-Yves Oudeyer", "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement\n  Learning Agents", "comments": "under review. This paper extends and generalizes work in\n  arXiv:2104.13207", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:39:18 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 06:42:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kova\u010d", "Grgur", ""], ["Portelas", "R\u00e9my", ""], ["Hofmann", "Katja", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2107.00967", "submitter": "Xiang Hu", "authors": "Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, Gerard\n  de Melo", "title": "R2D2: Recursive Transformer based on Differentiable Tree for\n  Interpretable Hierarchical Language Modeling", "comments": "To be published in the proceedings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:00:46 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Hu", "Xiang", ""], ["Mi", "Haitao", ""], ["Wen", "Zujie", ""], ["Wang", "Yafang", ""], ["Su", "Yi", ""], ["Zheng", "Jing", ""], ["de Melo", "Gerard", ""]]}, {"id": "2107.01068", "submitter": "Shahab Jalalvand", "authors": "Shahab Jalalvand and Srinivas Bangalore", "title": "Unsupervised Spoken Utterance Classification", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An intelligent virtual assistant (IVA) enables effortless conversations in\ncall routing through spoken utterance classification (SUC) which is a special\nform of spoken language understanding (SLU). Building a SUC system requires a\nlarge amount of supervised in-domain data that is not always available. In this\npaper, we introduce an unsupervised spoken utterance classification approach\n(USUC) that does not require any in-domain data except for the intent labels\nand a few para-phrases per intent. USUC is consisting of a KNN classifier (K=1)\nand a complex embedding model trained on a large amount of unsupervised\ncustomer service corpus. Among all embedding models, we demonstrate that Elmo\nworks best for USUC. However, an Elmo model is too slow to be used at run-time\nfor call routing. To resolve this issue, first, we compute the uni- and bi-gram\nembedding vectors offline and we build a lookup table of n-grams and their\ncorresponding embedding vector. Then we use this table to compute sentence\nembedding vectors at run-time, along with back-off techniques for unseen\nn-grams. Experiments show that USUC outperforms the traditional utterance\nclassification methods by reducing the classification error rate from 32.9% to\n27.0% without requiring supervised data. Moreover, our lookup and back-off\ntechnique increases the processing speed from 16 utterances per second to 118\nutterances per second.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:22:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Jalalvand", "Shahab", ""], ["Bangalore", "Srinivas", ""]]}, {"id": "2107.01076", "submitter": "Marya Bazzi", "authors": "Adam Tsakalidis, Pierpaolo Basile, Marya Bazzi, Mihai Cucuringu and\n  Barbara McGillivray", "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n  In this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:32:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Tsakalidis", "Adam", ""], ["Basile", "Pierpaolo", ""], ["Bazzi", "Marya", ""], ["Cucuringu", "Mihai", ""], ["McGillivray", "Barbara", ""]]}, {"id": "2107.01183", "submitter": "Saif M. Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Ethics Sheets for AI Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several high-profile events, such as the use of biased recidivism systems and\nmass testing of emotion recognition systems on vulnerable sub-populations, have\nhighlighted how technology will often lead to more adverse outcomes for those\nthat are already marginalized. In this paper, I will make a case for thinking\nabout ethical considerations not just at the level of individual models and\ndatasets, but also at the level of AI tasks. I will present a new form of such\nan effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the\nassumptions and ethical considerations hidden in how a task is commonly framed\nand in the choices we make regarding the data, method, and evaluation. Finally,\nI will provide an example ethics sheet for automatic emotion recognition.\nTogether with Data Sheets for datasets and Model Cards for AI systems, Ethics\nSheets aid in the development and deployment of responsible AI systems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:45:40 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 15:55:39 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "2107.01198", "submitter": "Abheesht Sharma", "authors": "Abheesht Sharma, Gunjan Chhablani, Harshit Pandey, Rajaswa Patil", "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature", "comments": "6 pages, 5 figures, Submitted to EMNLP-2021 Demo Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 17:33:25 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 19:32:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Sharma", "Abheesht", ""], ["Chhablani", "Gunjan", ""], ["Pandey", "Harshit", ""], ["Patil", "Rajaswa", ""]]}, {"id": "2107.01202", "submitter": "Mohd Zeeshan Ansari", "authors": "Mohd Zeeshan Ansari, M M Sufyan Beg, Tanvir Ahmad, Mohd Jazib Khan,\n  Ghazali Wasim", "title": "Language Identification of Hindi-English tweets using code-mixed BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 17:51:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ansari", "Mohd Zeeshan", ""], ["Beg", "M M Sufyan", ""], ["Ahmad", "Tanvir", ""], ["Khan", "Mohd Jazib", ""], ["Wasim", "Ghazali", ""]]}, {"id": "2107.01275", "submitter": "Timo Lohrenz", "authors": "Timo Lohrenz, Patrick Schwarz, Zhengyang Li, Tim Fingscheidt", "title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End\n  Automatic Speech Recognition", "comments": "submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter. Upon acceptance, models will be\npublished on github.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 21:01:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Lohrenz", "Timo", ""], ["Schwarz", "Patrick", ""], ["Li", "Zhengyang", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2107.01294", "submitter": "Yao Dou", "authors": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin\n  Choi", "title": "Scarecrow: A Framework for Scrutinizing Machine Text", "comments": "The project webpage is at https://yao-dou.github.io/scarecrow/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern neural text generation systems can produce remarkably fluent and\ngrammatical texts. While earlier language models suffered from repetition and\nsyntactic errors, the errors made by contemporary models are often semantic,\nnarrative, or discourse failures.\n  To facilitate research of these complex error types, we introduce a new\nstructured, crowdsourced error annotation schema called Scarecrow. The error\ncategories used in Scarecrow -- such as redundancy, commonsense errors, and\nincoherence -- were identified by combining expert analysis with several pilot\nrounds of ontology-free crowd annotation to arrive at a schema which covers the\nerror phenomena found in real machine generated text.\n  We use Scarecrow to collect 13k annotations of 1.3k human and machine\ngenerate paragraphs of English language news text, amounting to over 41k spans\neach labeled with its error category, severity, a natural language explanation,\nand antecedent span (where relevant). We collect annotations for text generated\nby state-of-the-art systems with varying known performance levels, from GPT-2\nSmall through the largest GPT-3. We isolate several factors for detailed\nanalysis, including parameter count, training data, and decoding technique. Our\nresults show both expected and surprising differences across these settings.\nThese findings demonstrate the value of Scarecrow annotations in the assessment\nof current and future text generation systems. We release our complete\nannotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 22:37:03 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 21:40:39 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Dou", "Yao", ""], ["Forbes", "Maxwell", ""], ["Koncel-Kedziorski", "Rik", ""], ["Smith", "Noah A.", ""], ["Choi", "Yejin", ""]]}, {"id": "2107.01366", "submitter": "Eugene Kharitonov", "authors": "Rahma Chaabouni, Roberto Dess\\`i, Eugene Kharitonov", "title": "Can Transformers Jump Around Right in Natural Language? Assessing\n  Performance Transfer from SCAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 07:45:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chaabouni", "Rahma", ""], ["Dess\u00ec", "Roberto", ""], ["Kharitonov", "Eugene", ""]]}, {"id": "2107.01431", "submitter": "JingHui Qin", "authors": "Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng Tang, Liang Lin", "title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous math word problem solvers following the encoder-decoder paradigm\nfail to explicitly incorporate essential math symbolic constraints, leading to\nunexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic\nSolver (NS-Solver) to explicitly and seamlessly incorporate different levels of\nsymbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem\nreader to encode problems, a programmer to generate symbolic equations, and a\nsymbolic executor to obtain answers. Along with target expression supervision,\nour solver is also optimized via 4 new auxiliary objectives to enforce\ndifferent symbolic reasoning: a) self-supervised number prediction task\npredicting both number quantity and number locations; b) commonsense constant\nprediction task predicting what prior knowledge (e.g. how many legs a chicken\nhas) is required; c) program consistency checker computing the semantic loss\nbetween predicted equation and target equation to ensure reasonable equation\nmapping; d) duality exploiting task exploiting the quasi duality between\nsymbolic equation generation and problem's part-of-speech generation to enhance\nthe understanding ability of a solver. Besides, to provide a more realistic and\nchallenging benchmark for developing a universal and scalable solver, we also\nconstruct a new large-scale MWP benchmark CM17K consisting of 4 kinds of MWPs\n(arithmetic, one-unknown linear, one-unknown non-linear, equation set) with\nmore than 17K samples. Extensive experiments on Math23K and our CM17k\ndemonstrate the superiority of our NS-Solver compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 13:14:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Qin", "Jinghui", ""], ["Liang", "Xiaodan", ""], ["Hong", "Yining", ""], ["Tang", "Jianheng", ""], ["Lin", "Liang", ""]]}, {"id": "2107.01540", "submitter": "Hossein Rouhizadeh", "authors": "Hossein Rouhizadeh, Mehrnoush Shamsfard, Vahideh Tajalli, and Masoud\n  Rouhziadeh", "title": "Persian-WSD-Corpus: A Sense Annotated Corpus for Persian All-words Word\n  Sense Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word Sense Disambiguation (WSD) is a long-standing task in Natural Language\nProcessing(NLP) that aims to automatically identify the most relevant meaning\nof the words in a given context. Developing standard WSD test collections can\nbe mentioned as an important prerequisite for developing and evaluating\ndifferent WSD systems in the language of interest. Although many WSD test\ncollections have been developed for a variety of languages, no standard\nAll-words WSD benchmark is available for Persian. In this paper, we address\nthis shortage for the Persian language by introducing SBU-WSD-Corpus, as the\nfirst standard test set for the Persian All-words WSD task. SBU-WSD-Corpus is\nmanually annotated with senses from the Persian WordNet (FarsNet) sense\ninventory. To this end, three annotators used SAMP (a tool for sense annotation\nbased on FarsNet lexical graph) to perform the annotation task. SBU-WSD-Corpus\nconsists of 19 Persian documents in different domains such as Sports, Science,\nArts, etc. It includes 5892 content words of Persian running text and 3371\nmanually sense annotated words (2073 nouns, 566 verbs, 610 adjectives, and 122\nadverbs). Providing baselines for future studies on the Persian All-words WSD\ntask, we evaluate several WSD models on SBU-WSD-Corpus. The corpus is publicly\navailable at https://github.com/hrouhizadeh/SBU-WSD-Corpus.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:09:28 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rouhizadeh", "Hossein", ""], ["Shamsfard", "Mehrnoush", ""], ["Tajalli", "Vahideh", ""], ["Rouhziadeh", "Masoud", ""]]}, {"id": "2107.01545", "submitter": "Shota Horiguchi", "authors": "Shota Horiguchi, Shinji Watanabe, Paola Garcia, Yawen Xue, Yuki\n  Takashima, Yohei Kawaguchi", "title": "Towards Neural Diarization for Unlimited Numbers of Speakers Using\n  Global and Local Attractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attractor-based end-to-end diarization is achieving comparable accuracy to\nthe carefully tuned conventional clustering-based methods on challenging\ndatasets. However, the main drawback is that it cannot deal with the case where\nthe number of speakers is larger than the one observed during training. This is\nbecause its speaker counting relies on supervised learning. In this work, we\nintroduce an unsupervised clustering process embedded in the attractor-based\nend-to-end diarization. We first split a sequence of frame-wise embeddings into\nshort subsequences and then perform attractor-based diarization for each\nsubsequence. Given subsequence-wise diarization results, inter-subsequence\nspeaker correspondence is obtained by unsupervised clustering of the vectors\ncomputed from the attractors from all the subsequences. This makes it possible\nto produce diarization results of a large number of speakers for the whole\nrecording even if the number of output speakers for each subsequence is\nlimited. Experimental results showed that our method could produce accurate\ndiarization results of an unseen number of speakers. Our method achieved 11.84\n%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,\nrespectively, each of which is better than the conventional end-to-end\ndiarization methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:34:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Horiguchi", "Shota", ""], ["Watanabe", "Shinji", ""], ["Garcia", "Paola", ""], ["Xue", "Yawen", ""], ["Takashima", "Yuki", ""], ["Kawaguchi", "Yohei", ""]]}, {"id": "2107.01549", "submitter": "Ryo Masumura", "authors": "Ryo Masumura, Daiki Okamura, Naoki Makishima, Mana Ihori, Akihiko\n  Takashima, Tomohiro Tanaka, Shota Orihashi", "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker\n  Overlapped Speech Recognition and Speaker Attribute Estimation", "comments": "Accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel modeling method for single-channel\nmulti-talker overlapped automatic speech recognition (ASR) systems. Fully\nneural network based end-to-end models have dramatically improved the\nperformance of multi-taker overlapped ASR tasks. One promising approach for\nend-to-end modeling is autoregressive modeling with serialized output training\nin which transcriptions of multiple speakers are recursively generated one\nafter another. This enables us to naturally capture relationships between\nspeakers. However, the conventional modeling method cannot explicitly take into\naccount the speaker attributes of individual utterances such as gender and age\ninformation. In fact, the performance deteriorates when each speaker is the\nsame gender or is close in age. To address this problem, we propose unified\nautoregressive modeling for joint end-to-end multi-talker overlapped ASR and\nspeaker attribute estimation. Our key idea is to handle gender and age\nestimation tasks within the unified autoregressive modeling. In the proposed\nmethod, transformer-based autoregressive model recursively generates not only\ntextual tokens but also attribute tokens of each speaker. This enables us to\neffectively utilize speaker attributes for improving multi-talker overlapped\nASR. Experiments on Japanese multi-talker overlapped ASR tasks demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:47:18 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Masumura", "Ryo", ""], ["Okamura", "Daiki", ""], ["Makishima", "Naoki", ""], ["Ihori", "Mana", ""], ["Takashima", "Akihiko", ""], ["Tanaka", "Tomohiro", ""], ["Orihashi", "Shota", ""]]}, {"id": "2107.01569", "submitter": "Tomohiro Tanaka", "authors": "Tomohiro Tanaka, Ryo Masumura, Mana Ihori, Akihiko Takashima, Takafumi\n  Moriya, Takanori Ashihara, Shota Orihashi, Naoki Makishima", "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic\n  Speech Recognition", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cross-modal transformer-based neural correction models that\nrefines the output of an automatic speech recognition (ASR) system so as to\nexclude ASR errors. Generally, neural correction models are composed of\nencoder-decoder networks, which can directly model sequence-to-sequence mapping\nproblems. The most successful method is to use both input speech and its ASR\noutput text as the input contexts for the encoder-decoder networks. However,\nthe conventional method cannot take into account the relationships between\nthese two different modal inputs because the input contexts are separately\nencoded for each modal. To effectively leverage the correlated information\nbetween the two different modal inputs, our proposed models encode two\ndifferent contexts jointly on the basis of cross-modal self-attention using a\ntransformer. We expect that cross-modal self-attention can effectively capture\nthe relationships between two different modals for refining ASR hypotheses. We\nalso introduce a shallow fusion technique to efficiently integrate the\nfirst-pass ASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed models\nachieve better ASR performance than conventional neural correction models.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 07:58:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tanaka", "Tomohiro", ""], ["Masumura", "Ryo", ""], ["Ihori", "Mana", ""], ["Takashima", "Akihiko", ""], ["Moriya", "Takafumi", ""], ["Ashihara", "Takanori", ""], ["Orihashi", "Shota", ""], ["Makishima", "Naoki", ""]]}, {"id": "2107.01571", "submitter": "Shen Ge", "authors": "Zhiqi Huang, Fenglin Liu, Xian Wu, Shen Ge, Helin Wang, Wei Fan,\n  Yuexian Zou", "title": "Audio-Oriented Multimodal Machine Comprehension: Task, Dataset and Model", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While Machine Comprehension (MC) has attracted extensive research interests\nin recent years, existing approaches mainly belong to the category of Machine\nReading Comprehension task which mines textual inputs (paragraphs and\nquestions) to predict the answers (choices or text spans). However, there are a\nlot of MC tasks that accept audio input in addition to the textual input, e.g.\nEnglish listening comprehension test. In this paper, we target the problem of\nAudio-Oriented Multimodal Machine Comprehension, and its goal is to answer\nquestions based on the given audio and textual information. To solve this\nproblem, we propose a Dynamic Inter- and Intra-modality Attention (DIIA) model\nto effectively fuse the two modalities (audio and textual). DIIA can work as an\nindependent component and thus be easily integrated into existing MC models.\nMoreover, we further develop a Multimodal Knowledge Distillation (MKD) module\nto enable our multimodal MC model to accurately predict the answers based only\non either the text or the audio. As a result, the proposed approach can handle\nvarious tasks including: Audio-Oriented Multimodal Machine Comprehension,\nMachine Reading Comprehension and Machine Listening Comprehension, in a single\nmodel, making fair comparisons possible between our model and the existing\nunimodal MC models. Experimental results and analysis prove the effectiveness\nof the proposed approaches. First, the proposed DIIA boosts the baseline models\nby up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the\nMKD module allows our multimodal MC model to significantly outperform the\nunimodal models by up to 18.87%, which are trained and tested with only audio\nor textual data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 08:35:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Huang", "Zhiqi", ""], ["Liu", "Fenglin", ""], ["Wu", "Xian", ""], ["Ge", "Shen", ""], ["Wang", "Helin", ""], ["Fan", "Wei", ""], ["Zou", "Yuexian", ""]]}, {"id": "2107.01573", "submitter": "Shammur Absar Chowdhury", "authors": "Ahmed Ali, Shammur Chowdhury, Amir Hussein, Yasser Hifny", "title": "Arabic Code-Switching Speech Recognition using Monolingual Data", "comments": "Accepted in Interspeech 2021, speech recognition, code-switching,\n  ASR, transformer, WFST, graph approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Code-switching in automatic speech recognition (ASR) is an important\nchallenge due to globalization. Recent research in multilingual ASR shows\npotential improvement over monolingual systems. We study key issues related to\nmultilingual modeling for ASR through a series of large-scale ASR experiments.\nOur innovative framework deploys a multi-graph approach in the weighted finite\nstate transducers (WFST) framework. We compare our WFST decoding strategies\nwith a transformer sequence to sequence system trained on the same data. Given\na code-switching scenario between Arabic and English languages, our results\nshow that the WFST decoding approaches were more suitable for the\nintersentential code-switching datasets. In addition, the transformer system\nperformed better for intrasentential code-switching task. With this study, we\nrelease an artificially generated development and test sets, along with\necological code-switching test set, to benchmark the ASR performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 08:40:49 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ali", "Ahmed", ""], ["Chowdhury", "Shammur", ""], ["Hussein", "Amir", ""], ["Hifny", "Yasser", ""]]}, {"id": "2107.01583", "submitter": "Jiawei Sheng", "authors": "Jiawei Sheng, Shu Guo, Bowen Yu, Qian Li, Yiming Hei, Lihong Wang,\n  Tingwen Liu and Hongbo Xu", "title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping\n  Event Extraction", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event extraction (EE) is a crucial information extraction task that aims to\nextract event information in texts. Most existing methods assume that events\nappear in sentences without overlaps, which are not applicable to the\ncomplicated overlapping event extraction. This work systematically studies the\nrealistic event overlapping problem, where a word may serve as triggers with\nseveral types or arguments with different roles. To tackle the above problem,\nwe propose a novel joint learning framework with cascade decoding for\noverlapping event extraction, termed as CasEE. Particularly, CasEE sequentially\nperforms type detection, trigger extraction and argument extraction, where the\noverlapped targets are extracted separately conditioned on the specific former\nprediction. All the subtasks are jointly learned in a framework to capture\ndependencies among the subtasks. The evaluation on a public event extraction\nbenchmark FewFC demonstrates that CasEE achieves significant improvements on\noverlapping event extraction over previous competitive methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 10:01:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sheng", "Jiawei", ""], ["Guo", "Shu", ""], ["Yu", "Bowen", ""], ["Li", "Qian", ""], ["Hei", "Yiming", ""], ["Wang", "Lihong", ""], ["Liu", "Tingwen", ""], ["Xu", "Hongbo", ""]]}, {"id": "2107.01592", "submitter": "Luxi Xing", "authors": "Luxi Xing, Yue Hu, Jing Yu, Yuqiang Xie, Wei Peng", "title": "Coarse-to-Careful: Seeking Semantic-related Knowledge for Open-domain\n  Commonsense Question Answering", "comments": "In ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is prevalent to utilize external knowledge to help machine answer\nquestions that need background commonsense, which faces a problem that\nunlimited knowledge will transmit noisy and misleading information. Towards the\nissue of introducing related knowledge, we propose a semantic-driven\nknowledge-aware QA framework, which controls the knowledge injection in a\ncoarse-to-careful fashion. We devise a tailoring strategy to filter extracted\nknowledge under monitoring of the coarse semantic of question on the knowledge\nextraction stage. And we develop a semantic-aware knowledge fetching module\nthat engages structural knowledge information and fuses proper knowledge\naccording to the careful semantic of questions in a hierarchical way.\nExperiments demonstrate that the proposed approach promotes the performance on\nthe CommonsenseQA dataset comparing with strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 10:56:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Xing", "Luxi", ""], ["Hu", "Yue", ""], ["Yu", "Jing", ""], ["Xie", "Yuqiang", ""], ["Peng", "Wei", ""]]}, {"id": "2107.01598", "submitter": "Mohammad Rostami", "authors": "Mohammad Rostami, Aram Galstyan", "title": "Domain Adaptation for Sentiment Analysis Using Increased Intraclass\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 11:39:12 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rostami", "Mohammad", ""], ["Galstyan", "Aram", ""]]}, {"id": "2107.01656", "submitter": "Baban Gain", "authors": "Baban Gain and Dibyanayan Bandyopadhyay and Asif Ekbal", "title": "IITP at WAT 2021: System description for English-Hindi Multimodal\n  Translation Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Machine Translation (NMT) is a predominant machine translation\ntechnology nowadays because of its end-to-end trainable flexibility. However,\nNMT still struggles to translate properly in low-resource settings specifically\non distant language pairs. One way to overcome this is to use the information\nfrom other modalities if available. The idea is that despite differences in\nlanguages, both the source and target language speakers see the same thing and\nthe visual representation of both the source and target is the same, which can\npositively assist the system. Multimodal information can help the NMT system to\nimprove the translation by removing ambiguity on some phrases or words. We\nparticipate in the 8th Workshop on Asian Translation (WAT - 2021) for\nEnglish-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU\npoints for Evaluation and Challenge subset, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 14:56:28 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Gain", "Baban", ""], ["Bandyopadhyay", "Dibyanayan", ""], ["Ekbal", "Asif", ""]]}, {"id": "2107.01700", "submitter": "Tuan Manh Lai", "authors": "Tuan Manh Lai, Trung Bui, Doo Soon Kim", "title": "End-to-end Neural Coreference Resolution Revisited: A Simple yet\n  Effective Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add a lot of extra complexity\nto the original model. Motivated by this observation and the recent advances in\npre-trained Transformer language models, we propose a simple yet effective\nbaseline for coreference resolution. Our model is a simplified version of the\noriginal neural coreference resolution model, however, it achieves impressive\nperformance, outperforming all recent extended works on the public English\nOntoNotes benchmark. Our work provides evidence for the necessity of carefully\njustifying the complexity of existing or newly proposed models, as introducing\na conceptual or practical simplification to an existing model can still yield\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 18:12:24 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 06:32:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lai", "Tuan Manh", ""], ["Bui", "Trung", ""], ["Kim", "Doo Soon", ""]]}, {"id": "2107.01791", "submitter": "Mingyue Han", "authors": "Mingyue Han and Yinglin Wang", "title": "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal\n  Reasoning Models", "comments": "ACL2021, Main Conference, Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models (PLM) achieve surprising performance on the Choice\nof Plausible Alternatives (COPA) task. However, whether PLMs have truly\nacquired the ability of causal reasoning remains a question. In this paper, we\ninvestigate the problem of semantic similarity bias and reveal the\nvulnerability of current COPA models by certain attacks. Previous solutions\nthat tackle the superficial cues of unbalanced token distribution still\nencounter the same problem of semantic bias, even more seriously due to the\nutilization of more training data. We mitigate this problem by simply adding a\nregularization loss and experimental results show that this solution not only\nimproves the model's generalization ability, but also assists the models to\nperform more robustly on a challenging dataset, BCOPA-CE, which has unbiased\ntoken distribution and is more difficult for models to distinguish cause and\neffect.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 05:08:30 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Han", "Mingyue", ""], ["Wang", "Yinglin", ""]]}, {"id": "2107.01875", "submitter": "Lanqing Xue", "authors": "Lanqing Xue, Kaitao Song, Duocai Wu, Xu Tan, Nevin L. Zhang, Tao Qin,\n  Wei-Qiang Zhang, Tie-Yan Liu", "title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling", "comments": "Accepted by ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rap generation, which aims to produce lyrics and corresponding singing beats,\nneeds to model both rhymes and rhythms. Previous works for rap generation\nfocused on rhyming lyrics but ignored rhythmic beats, which are important for\nrap performance. In this paper, we develop DeepRapper, a Transformer-based rap\ngeneration system that can model both rhymes and rhythms. Since there is no\navailable rap dataset with rhythmic beats, we develop a data mining pipeline to\ncollect a large-scale rap dataset, which includes a large number of rap songs\nwith aligned lyrics and rhythmic beats. Second, we design a Transformer-based\nautoregressive language model which carefully models rhymes and rhythms.\nSpecifically, we generate lyrics in the reverse order with rhyme representation\nand constraint for rhyme enhancement and insert a beat symbol into lyrics for\nrhythm/beat modeling. To our knowledge, DeepRapper is the first system to\ngenerate rap with both rhymes and rhythms. Both objective and subjective\nevaluations demonstrate that DeepRapper generates creative and high-quality\nraps with rhymes and rhythms. Code will be released on GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:01:46 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Xue", "Lanqing", ""], ["Song", "Kaitao", ""], ["Wu", "Duocai", ""], ["Tan", "Xu", ""], ["Zhang", "Nevin L.", ""], ["Qin", "Tao", ""], ["Zhang", "Wei-Qiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2107.01982", "submitter": "James Barry", "authors": "James Barry, Alireza Mohammadshahi, Joachim Wagner, Jennifer Foster,\n  James Henderson", "title": "The DCU-EPFL Enhanced Dependency Parser at the IWPT 2021 Shared Task", "comments": "Submitted to the IWPT 2021 Shared Task: From Raw Text to Enhanced\n  Universal Dependencies: the Parsing Shared Task at IWPT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the DCU-EPFL submission to the IWPT 2021 Shared Task on Parsing\ninto Enhanced Universal Dependencies. The task involves parsing Enhanced UD\ngraphs, which are an extension of the basic dependency trees designed to be\nmore facilitative towards representing semantic structure. Evaluation is\ncarried out on 29 treebanks in 17 languages and participants are required to\nparse the data from each language starting from raw strings. Our approach uses\nthe Stanza pipeline to preprocess the text files, XLMRoBERTa to obtain\ncontextualized token representations, and an edge-scoring and labeling model to\npredict the enhanced graph. Finally, we run a post-processing script to ensure\nall of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9\nparticipants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57.\nWe carry out additional post-deadline experiments which include using Trankit\nfor pre-processing, XLM-RoBERTa-LARGE, treebank concatenation, and multitask\nlearning between a basic and an enhanced dependency parser. All of these\nmodifications improve our initial score and our final system has a coarse ELAS\nof 88.04.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:42:59 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Barry", "James", ""], ["Mohammadshahi", "Alireza", ""], ["Wagner", "Joachim", ""], ["Foster", "Jennifer", ""], ["Henderson", "James", ""]]}, {"id": "2107.01987", "submitter": "Mehrnoush ShamsFard", "authors": "Zeinab Rahimi and Mehrnoush ShamsFard", "title": "Contradiction Detection in Persian Text", "comments": "24 pages, 9 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of semantic contradictory sentences is one of the most challenging\nand fundamental issues for NLP applications such as recognition of textual\nentailments. Contradiction in this study includes different types of semantic\nconfrontation, such as conflict and antonymy. Due to lack of sufficient data to\napply precise machine learning and specifically deep learning methods to\nPersian and other low resource languages, rule-based approaches that can\nfunction similarly to these systems will be of a great interest. Also recently,\nemergence of new methods such as transfer learning, has opened up the\npossibility of deep learning for low-resource languages. Considering two above\npoints, in this study, along with a simple rule-base baseline, a novel\nrule-base system for identifying semantic contradiction along with a Bert base\ndeep contradiction detection system for Persian texts have been introduced. The\nrule base system has used frequent rule mining method to extract appropriate\ncontradiction rules using a development set. Extracted rules are tested for\ndifferent categories of contradictory sentences. In this system the maximum\nf-measure among contradiction categories is obtained for negation about 90% and\nthe average F-measure of system for all classes is about 76% which outperforms\nother algorithms on Persian texts. On the other hand, because of medium\nperformance of rule base system for some categories of contradiction, we use a\nBert base deep learning system using our translated dataset; with average\nF-measure of 73. Our hybrid system has f-measure of about 80.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:51:56 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rahimi", "Zeinab", ""], ["ShamsFard", "Mehrnoush", ""]]}, {"id": "2107.02012", "submitter": "Prathmesh Pathwar", "authors": "Prathmesh Pathwar, Simran Gill", "title": "Tackling COVID-19 Infodemic using Deep Learning", "comments": "15 pages, 4 figures, Accepted in 4th International Conference on\n  Computational Intelligence and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humanity is battling one of the most deleterious virus in modern history, the\nCOVID-19 pandemic, but along with the pandemic there's an infodemic permeating\nthe pupil and society with misinformation which exacerbates the current malady.\nWe try to detect and classify fake news on online media to detect fake\ninformation relating to COVID-19 and coronavirus. The dataset contained fake\nposts, articles and news gathered from fact checking websites like politifact\nwhereas real tweets were taken from verified twitter handles. We incorporated\nmultiple conventional classification techniques like Naive Bayes, KNN, Gradient\nBoost and Random Forest along with Deep learning approaches, specifically CNN,\nRNN, DNN and the ensemble model RMDL. We analyzed these approaches with two\nfeature extraction techniques, TF-IDF and GloVe Word Embeddings which would\nprovide deeper insights into the dataset containing COVID-19 info on online\nmedia.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:07:47 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pathwar", "Prathmesh", ""], ["Gill", "Simran", ""]]}, {"id": "2107.02024", "submitter": "Hadi Mansourifar", "authors": "Hadi Mansourifar, Dana Alsagheer, Weidong Shi, Lan Ni, Yan Huang", "title": "Statistical Analysis of Perspective Scores on Hate Speech Detection", "comments": "Accepted paper in International IJCAI Workshop on Artificial\n  Intelligence for Social Good 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech detection has become a hot topic in recent years due to the\nexponential growth of offensive language in social media. It has proven that,\nstate-of-the-art hate speech classifiers are efficient only when tested on the\ndata with the same feature distribution as training data. As a consequence,\nmodel architecture plays the second role to improve the current results. In\nsuch a diverse data distribution relying on low level features is the main\ncause of deficiency due to natural bias in data. That's why we need to use high\nlevel features to avoid a biased judgement. In this paper, we statistically\nanalyze the Perspective Scores and their impact on hate speech detection. We\nshow that, different hate speech datasets are very similar when it comes to\nextract their Perspective Scores. Eventually, we prove that, over-sampling the\nPerspective Scores of a hate speech dataset can significantly improve the\ngeneralization performance when it comes to be tested on other hate speech\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:17:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mansourifar", "Hadi", ""], ["Alsagheer", "Dana", ""], ["Shi", "Weidong", ""], ["Ni", "Lan", ""], ["Huang", "Yan", ""]]}, {"id": "2107.02025", "submitter": "Jim Samuel", "authors": "Jim Samuel, Ratnakar Palle and Eduardo Correa Soares", "title": "Textual Data Distributions: Kullback Leibler Textual Distributions\n  Contrasts on GPT-2 Generated Texts, with Supervised, Unsupervised Learning on\n  Vaccine & Market Topics & Sentiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Efficient textual data distributions (TDD) alignment and generation are open\nresearch problems in textual analytics and NLP. It is presently difficult to\nparsimoniously and methodologically confirm that two or more natural language\ndatasets belong to similar distributions, and to identify the extent to which\ntextual data possess alignment. This study focuses on addressing a segment of\nthe broader problem described above by applying multiple supervised and\nunsupervised machine learning (ML) methods to explore the behavior of TDD by\n(i) topical alignment, and (ii) by sentiment alignment. Furthermore we use\nmultiple text generation methods including fine-tuned GPT-2, to generate text\nby topic and by sentiment. Finally we develop a unique process driven variation\nof Kullback-Leibler divergence (KLD) application to TDD, named KL Textual\nDistributions Contrasts(KL-TDC) to identify the alignment of machine generated\ntextual corpora with naturally occurring textual corpora. This study thus\nidentifies a unique approach for generating and validating TDD by topic and\nsentiment, which can be used to help address sparse data problems and other\nresearch, practice and classroom situations in need of artificially generated\ntopic or sentiment aligned textual data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 21:30:46 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Samuel", "Jim", ""], ["Palle", "Ratnakar", ""], ["Soares", "Eduardo Correa", ""]]}, {"id": "2107.02027", "submitter": "Mario Michael Krell", "authors": "Matej Kosec and Sheng Fu and Mario Michael Krell", "title": "Packing: Towards 2x NLP BERT Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We find that at sequence length 512 padding tokens represent in excess of 50%\nof the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder\nRepresentations from Transformers). Therefore by removing all padding we\nachieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic\nof the dataset, we develop and contrast two deterministic packing algorithms.\nBoth algorithms rely on the assumption that sequences are interchangeable and\ntherefore packing can be performed on the histogram of sequence lengths, rather\nthan per sample. This transformation of the problem leads to algorithms which\nare fast and have linear complexity in dataset size. The shortest-pack-first\nhistogram-packing (SPFHP) algorithm determines the packing order for the\nWikipedia dataset of over 16M sequences in 0.02 seconds. The non-negative\nleast-squares histogram-packing (NNLSHP) algorithm converges in 28.4 seconds\nbut produces solutions which are more depth efficient, managing to get near\noptimal packing by combining a maximum of 3 sequences in one sample. Using the\ndataset with multiple sequences per sample requires additional masking in the\nattention layer and a modification of the MLM loss function. We demonstrate\nthat both of these changes are straightforward to implement and have relatively\nlittle impact on the achievable performance gain on modern hardware. Finally,\nwe pretrain BERT-Large using the packed dataset, demonstrating no loss of\nconvergence and the desired 2x speed-up.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:37:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kosec", "Matej", ""], ["Fu", "Sheng", ""], ["Krell", "Mario Michael", ""]]}, {"id": "2107.02039", "submitter": "Burc Gokden", "authors": "Burc Gokden", "title": "Power Law Graph Transformer for Machine Translation and Representation\n  Learning", "comments": "55 pages, 39 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the Power Law Graph Transformer, a transformer model with well\ndefined deductive and inductive tasks for prediction and representation\nlearning. The deductive task learns the dataset level (global) and instance\nlevel (local) graph structures in terms of learnable power law distribution\nparameters. The inductive task outputs the prediction probabilities using the\ndeductive task output, similar to a transductive model. We trained our model\nwith Turkish-English and Portuguese-English datasets from TED talk transcripts\nfor machine translation and compared the model performance and characteristics\nto a transformer model with scaled dot product attention trained on the same\nexperimental setup. We report BLEU scores of $17.79$ and $28.33$ on the\nTurkish-English and Portuguese-English translation tasks with our model,\nrespectively. We also show how a duality between a quantization set and\nN-dimensional manifold representation can be leveraged to transform between\nlocal and global deductive-inductive outputs using successive application of\nlinear and non-linear transformations end-to-end.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:59:37 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Gokden", "Burc", ""]]}, {"id": "2107.02040", "submitter": "Romina Etezadi", "authors": "Romina Etezadi, Mehrnoush Shamsfard", "title": "A Knowledge-based Approach for Answering Complex Questions in Persian", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research on open-domain question answering (QA) has a long tradition. A\nchallenge in this domain is answering complex questions (CQA) that require\ncomplex inference methods and large amounts of knowledge. In low resource\nlanguages, such as Persian, there are not many datasets for open-domain complex\nquestions and also the language processing toolkits are not very accurate. In\nthis paper, we propose a knowledge-based approach for answering Persian complex\nquestions using Farsbase; the Persian knowledge graph, exploiting PeCoQ; the\nnewly created complex Persian question dataset. In this work, we handle\nmulti-constraint and multi-hop questions by building their set of possible\ncorresponding logical forms. Then Multilingual-BERT is used to select the\nlogical form that best describes the input complex question syntactically and\nsemantically. The answer to the question is built from the answer to the\nlogical form, extracted from the knowledge graph. Experiments show that our\napproach outperforms other approaches in Persian CQA.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:01:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Etezadi", "Romina", ""], ["Shamsfard", "Mehrnoush", ""]]}, {"id": "2107.02102", "submitter": "Yuxiang Wu", "authors": "Yuxiang Wu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel", "title": "Training Adaptive Computation for Open-Domain Question Answering with\n  Computational Constraints", "comments": "7 pages, 1 figure, to be published in ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adaptive Computation (AC) has been shown to be effective in improving the\nefficiency of Open-Domain Question Answering (ODQA) systems. However, current\nAC approaches require tuning of all model parameters, and training\nstate-of-the-art ODQA models requires significant computational resources that\nmay not be available for most researchers. We propose Adaptive Passage Encoder,\nan AC method that can be applied to an existing ODQA model and can be trained\nefficiently on a single GPU. It keeps the parameters of the base ODQA model\nfixed, but it overrides the default layer-by-layer computation of the encoder\nwith an AC policy that is trained to optimise the computational efficiency of\nthe model. Our experimental results show that our method improves upon a\nstate-of-the-art model on two datasets, and is also more accurate than previous\nAC methods due to the stronger base ODQA model. All source code and datasets\nare available at https://github.com/uclnlp/APE.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:48:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wu", "Yuxiang", ""], ["Minervini", "Pasquale", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2107.02126", "submitter": "Qian Li", "authors": "Qian Li, Hao Peng, Jianxin Li, Yiming Hei, Rui Sun, Jiawei Sheng, Shu\n  Guo, Lihong Wang, Philip S. Yu", "title": "Deep Learning Schema-based Event Extraction: Literature Review and\n  Current Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema-based event extraction is a critical technique to apprehend the\nessential content of events promptly. With the rapid development of deep\nlearning technology, event extraction technology based on deep learning has\nbecome a research hotspot. Numerous methods, datasets, and evaluation metrics\nhave been proposed in the literature, raising the need for a comprehensive and\nupdated survey. This paper fills the gap by reviewing the state-of-the-art\napproaches, focusing on deep learning-based models. We summarize the task\ndefinition, paradigm, and models of schema-based event extraction and then\ndiscuss each of these in detail. We introduce benchmark datasets that support\ntests of predictions and evaluation metrics. A comprehensive comparison between\ndifferent techniques is also provided in this survey. Finally, we conclude by\nsummarizing future research directions facing the research area.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:32:45 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 07:25:06 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 06:57:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Li", "Qian", ""], ["Peng", "Hao", ""], ["Li", "Jianxin", ""], ["Hei", "Yiming", ""], ["Sun", "Rui", ""], ["Sheng", "Jiawei", ""], ["Guo", "Shu", ""], ["Wang", "Lihong", ""], ["Yu", "Philip S.", ""]]}, {"id": "2107.02137", "submitter": "Shuohuan Wang", "authors": "Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan\n  Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua\n  Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan\n  Ouyang, Dianhai Yu, Hao Tian, Hua Wu, Haifeng Wang", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language\n  Understanding and Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained models have achieved state-of-the-art results in various Natural\nLanguage Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown\nthat scaling up pre-trained language models can improve their generalization\nabilities. Particularly, the GPT-3 model with 175 billion parameters shows its\nstrong task-agnostic zero-shot/few-shot learning capabilities. Despite their\nsuccess, these large-scale models are trained on plain texts without\nintroducing knowledge such as linguistic knowledge and world knowledge. In\naddition, most large-scale models are trained in an auto-regressive way. As a\nresult, this kind of traditional fine-tuning approach demonstrates relatively\nweak performance when solving downstream language understanding tasks. In order\nto solve the above problems, we propose a unified framework named ERNIE 3.0 for\npre-training large-scale knowledge enhanced models. It fuses auto-regressive\nnetwork and auto-encoding network, so that the trained model can be easily\ntailored for both natural language understanding and generation tasks with\nzero-shot learning, few-shot learning or fine-tuning. We trained the model with\n10 billion parameters on a 4TB corpus consisting of plain texts and a\nlarge-scale knowledge graph. Empirical results show that the model outperforms\nthe state-of-the-art models on 54 Chinese NLP tasks, and its English version\nachieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing\nthe human performance by +0.8% (90.6% vs. 89.8%).\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:54:59 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sun", "Yu", ""], ["Wang", "Shuohuan", ""], ["Feng", "Shikun", ""], ["Ding", "Siyu", ""], ["Pang", "Chao", ""], ["Shang", "Junyuan", ""], ["Liu", "Jiaxiang", ""], ["Chen", "Xuyi", ""], ["Zhao", "Yanbin", ""], ["Lu", "Yuxiang", ""], ["Liu", "Weixin", ""], ["Wu", "Zhihua", ""], ["Gong", "Weibao", ""], ["Liang", "Jianzhong", ""], ["Shang", "Zhizhou", ""], ["Sun", "Peng", ""], ["Liu", "Wei", ""], ["Ouyang", "Xuan", ""], ["Yu", "Dianhai", ""], ["Tian", "Hao", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2107.02153", "submitter": "Sewon Min", "authors": "Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, Hannaneh\n  Hajishirzi", "title": "FaVIQ: FAct Verification from Information-seeking Questions", "comments": "12 pages, 3 figures; Data & Code available at https://faviq.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite significant interest in developing general purpose fact checking\nmodels, it is challenging to construct a large-scale fact verification dataset\nwith realistic claims that would occur in the real world. Existing claims are\neither authored by crowdworkers, thereby introducing subtle biases that are\ndifficult to control for, or manually verified by professional fact checkers,\ncausing them to be expensive and limited in scale. In this paper, we construct\na challenging, realistic, and large-scale fact verification dataset called\nFaVIQ, using information-seeking questions posed by real users who do not know\nhow to answer. The ambiguity in information-seeking questions enables\nautomatically constructing true and false claims that reflect confusions arisen\nfrom users (e.g., the year of the movie being filmed vs. being released). Our\nclaims are verified to be natural, contain little lexical bias, and require a\ncomplete understanding of the evidence for verification. Our experiments show\nthat the state-of-the-art models are far from solving our new task. Moreover,\ntraining on our data helps in professional fact-checking, outperforming models\ntrained on the most widely used dataset FEVER or in-domain data by up to 17%\nabsolute. Altogether, our data will serve as a challenging benchmark for\nnatural language understanding and support future progress in professional fact\nchecking.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:31:44 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Park", "Jungsoo", ""], ["Min", "Sewon", ""], ["Kang", "Jaewoo", ""], ["Zettlemoyer", "Luke", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2107.02173", "submitter": "Alexander Hoyle", "authors": "Alexander Hoyle, Pranav Goel, Denis Peskov, Andrew Hian-Cheong, Jordan\n  Boyd-Graber, Philip Resnik", "title": "Is Automated Topic Model Evaluation Broken?: The Incoherence of\n  Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic model evaluation, like evaluation of other unsupervised methods, can be\ncontentious. However, the field has coalesced around automated estimates of\ntopic coherence, which rely on the frequency of word co-occurrences in a\nreference corpus. Recent models relying on neural components surpass classical\ntopic models according to these metrics. At the same time, unlike classical\nmodels, the practice of neural topic model evaluation suffers from a validation\ngap: automatic coherence for neural models has not been validated using human\nexperimentation. In addition, as we show via a meta-analysis of topic modeling\nliterature, there is a substantial standardization gap in the use of automated\ntopic modeling benchmarks. We address both the standardization gap and the\nvalidation gap. Using two of the most widely used topic model evaluation\ndatasets, we assess a dominant classical model and two state-of-the-art neural\nmodels in a systematic, clearly documented, reproducible way. We use automatic\ncoherence along with the two most widely accepted human judgment tasks, namely,\ntopic rating and word intrusion. Automated evaluation will declare one model\nsignificantly different from another when corresponding human evaluations do\nnot, calling into question the validity of fully automatic evaluations\nindependent of human judgments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:58:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hoyle", "Alexander", ""], ["Goel", "Pranav", ""], ["Peskov", "Denis", ""], ["Hian-Cheong", "Andrew", ""], ["Boyd-Graber", "Jordan", ""], ["Resnik", "Philip", ""]]}, {"id": "2107.02175", "submitter": "Junaid Baber", "authors": "Mohammad Aimal, Maheen Bakhtyar, Junaid Baber, Sadia Lakho, Umar\n  Mohammad, Warda Ahmed, Jahanvash Karim", "title": "Identifying negativity factors from social media text corpus using\n  sentiment analysis method", "comments": "Paper is accepted in the SOFA 2020 conference", "journal-ref": "SOFA 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic sentiment analysis play vital role in decision making. Many\norganizations spend a lot of budget to understand their customer satisfaction\nby manually going over their feedback/comments or tweets. Automatic sentiment\nanalysis can give overall picture of the comments received against any event,\nproduct, or activity. Usually, the comments/tweets are classified into two main\nclasses that are negative or positive. However, the negative comments are too\nabstract to understand the basic reason or the context. organizations are\ninterested to identify the exact reason for the negativity. In this research\nstudy, we hierarchically goes down into negative comments, and link them with\nmore classes. Tweets are extracted from social media sites such as Twitter and\nFacebook. If the sentiment analysis classifies any tweet into negative class,\nthen we further try to associates that negative comments with more possible\nnegative classes. Based on expert opinions, the negative comments/tweets are\nfurther classified into 8 classes. Different machine learning algorithms are\nevaluated and their accuracy are reported.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:15:31 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Aimal", "Mohammad", ""], ["Bakhtyar", "Maheen", ""], ["Baber", "Junaid", ""], ["Lakho", "Sadia", ""], ["Mohammad", "Umar", ""], ["Ahmed", "Warda", ""], ["Karim", "Jahanvash", ""]]}, {"id": "2107.02192", "submitter": "Wei Ping", "authors": "Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein,\n  Anima Anandkumar, Bryan Catanzaro", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:00:14 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:34:30 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhu", "Chen", ""], ["Ping", "Wei", ""], ["Xiao", "Chaowei", ""], ["Shoeybi", "Mohammad", ""], ["Goldstein", "Tom", ""], ["Anandkumar", "Anima", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2107.02246", "submitter": "Serge Sharoff", "authors": "Mikhail Lepekhin, Serge Sharoff", "title": "Experiments with adversarial attacks on text genres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural models based on pre-trained transformers, such as BERT or XLM-RoBERTa,\ndemonstrate SOTA results in many NLP tasks, including non-topical\nclassification, such as genre identification. However, often these approaches\nexhibit low reliability to minor alterations of the test texts. A related\nprobelm concerns topical biases in the training corpus, for example, the\nprevalence of words on a specific topic in a specific genre can trick the genre\nclassifier to recognise any text on this topic in this genre. In order to\nmitigate the reliability problem, this paper investigates techniques for\nattacking genre classifiers to understand the limitations of the transformer\nmodels and to improve their performance. While simple text attacks, such as\nthose based on word replacement using keywords extracted by tf-idf, are not\ncapable of deceiving powerful models like XLM-RoBERTa, we show that\nembedding-based algorithms which can replace some of the most ``significant''\nwords with words similar to them, for example, TextFooler, have the ability to\ninfluence model predictions in a significant proportion of cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 19:37:59 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lepekhin", "Mikhail", ""], ["Sharoff", "Serge", ""]]}, {"id": "2107.02268", "submitter": "Christian Huber", "authors": "Christian Huber, Juan Hussain, Sebastian St\\\"uker, Alexander Waibel", "title": "Instant One-Shot Word-Learning for Context-Specific Neural\n  Sequence-to-Sequence Speech Recognition", "comments": "7 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition (ASR). When using appropriate modeling units,\ne.g., byte-pair encoded characters, these systems are in principal open\nvocabulary systems. In practice, however, they often fail to recognize words\nnot seen during training, e.g., named entities, numbers or technical terms. To\nalleviate this problem we supplement an end-to-end ASR system with a\nword/phrase memory and a mechanism to access this memory to recognize the words\nand phrases correctly. After the training of the ASR system, and when it has\nalready been deployed, a relevant word can be added or subtracted instantly\nwithout the need for further training. In this paper we demonstrate that\nthrough this mechanism our system is able to recognize more than 85% of newly\nadded words that it previously failed to recognize compared to a strong\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:08:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Huber", "Christian", ""], ["Hussain", "Juan", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alexander", ""]]}, {"id": "2107.02276", "submitter": "Hamed Yaghoobian", "authors": "Hamed Yaghoobian, Hamid R. Arabnia, Khaled Rasheed", "title": "Sarcasm Detection: A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sarcasm detection is the task of identifying irony containing utterances in\nsentiment-bearing text. However, the figurative and creative nature of sarcasm\nposes a great challenge for affective computing systems performing sentiment\nanalysis. This article compiles and reviews the salient work in the literature\nof automatic sarcasm detection. Thus far, three main paradigm shifts have\noccurred in the way researchers have approached this task: 1) semi-supervised\npattern extraction to identify implicit sentiment, 2) use of hashtag-based\nsupervision, and 3) incorporation of context beyond target text. In this\narticle, we provide a comprehensive review of the datasets, approaches, trends,\nand issues in sarcasm and irony detection.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:20:29 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 02:07:19 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yaghoobian", "Hamed", ""], ["Arabnia", "Hamid R.", ""], ["Rasheed", "Khaled", ""]]}, {"id": "2107.02282", "submitter": "Jiacheng Li", "authors": "Jiacheng Li, Haibo Ding, Jingbo Shang, Julian McAuley, Zhe Feng", "title": "Weakly Supervised Named Entity Tagging with Learnable Logical Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of building entity tagging systems by using a few rules\nas weak supervision. Previous methods mostly focus on disambiguation entity\ntypes based on contexts and expert-provided rules, while assuming entity spans\nare given. In this work, we propose a novel method TALLOR that bootstraps\nhigh-quality logical rules to train a neural tagger in a fully automated\nmanner. Specifically, we introduce compound rules that are composed from simple\nrules to increase the precision of boundary detection and generate more diverse\npseudo labels. We further design a dynamic label selection strategy to ensure\npseudo label quality and therefore avoid overfitting the neural tagger.\nExperiments on three datasets demonstrate that our method outperforms other\nweakly supervised methods and even rivals a state-of-the-art distantly\nsupervised tagger with a lexicon of over 2,000 terms when starting from only 20\nsimple rules. Our method can serve as a tool for rapidly building taggers in\nemerging domains and tasks. Case studies show that learned rules can\npotentially explain the predicted entities.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:32:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Li", "Jiacheng", ""], ["Ding", "Haibo", ""], ["Shang", "Jingbo", ""], ["McAuley", "Julian", ""], ["Feng", "Zhe", ""]]}, {"id": "2107.02286", "submitter": "Klim Zaporojets", "authors": "Severine Verlinden, Klim Zaporojets, Johannes Deleu, Thomas Demeester,\n  Chris Develder", "title": "Injecting Knowledge Base Information into End-to-End Joint Entity and\n  Relation Extraction and Coreference Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a joint information extraction (IE) model, solving named entity\nrecognition, coreference resolution and relation extraction jointly over the\nwhole document. In particular, we study how to inject information from a\nknowledge base (KB) in such IE model, based on unsupervised entity linking. The\nused KB entity representations are learned from either (i) hyperlinked text\ndocuments (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear\ncomplementary in raising IE performance. Representations of corresponding\nentity linking (EL) candidates are added to text span representations of the\ninput document, and we experiment with (i) taking a weighted average of the EL\ncandidate representations based on their prior (in Wikipedia), and (ii) using\nan attention scheme over the EL candidate list. Results demonstrate an increase\nof up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a\nstrong performance of the prior-based model, our quantitative and qualitative\nanalysis reveals the advantage of using the attention-based approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:49:02 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Verlinden", "Severine", ""], ["Zaporojets", "Klim", ""], ["Deleu", "Johannes", ""], ["Demeester", "Thomas", ""], ["Develder", "Chris", ""]]}, {"id": "2107.02294", "submitter": "Piotr \\.Zelasko", "authors": "Piotr \\.Zelasko, Raghavendra Pappagari, Najim Dehak", "title": "What Helps Transformers Recognize Conversational Structure? Importance\n  of Context, Punctuation, and Labels in Dialog Act Recognition", "comments": "Accepted for publication in Transactions of the Association of\n  Computational Linguistics. This is a pre-MIT Press publication version and it\n  is subject to change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog acts can be interpreted as the atomic units of a conversation, more\nfine-grained than utterances, characterized by a specific communicative\nfunction. The ability to structure a conversational transcript as a sequence of\ndialog acts -- dialog act recognition, including the segmentation -- is\ncritical for understanding dialog. We apply two pre-trained transformer models,\nXLNet and Longformer, to this task in English and achieve strong results on\nSwitchboard Dialog Act and Meeting Recorder Dialog Act corpora with dialog act\nsegmentation error rates (DSER) of 8.4% and 14.2%. To understand the key\nfactors affecting dialog act recognition, we perform a comparative analysis of\nmodels trained under different conditions. We find that the inclusion of a\nbroader conversational context helps disambiguate many dialog act classes,\nespecially those infrequent in the training data. The presence of punctuation\nin the transcripts has a massive effect on the models' performance, and a\ndetailed analysis reveals specific segmentation patterns observed in its\nabsence. Finally, we find that the label set specificity does not affect dialog\nact segmentation performance. These findings have significant practical\nimplications for spoken language understanding applications that depend heavily\non a good-quality segmentation being available.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:56:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["\u017belasko", "Piotr", ""], ["Pappagari", "Raghavendra", ""], ["Dehak", "Najim", ""]]}, {"id": "2107.02331", "submitter": "Siddharth Karamcheti", "authors": "Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher D.\n  Manning", "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on\n  Active Learning for Visual Question Answering", "comments": "Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning promises to alleviate the massive data needs of supervised\nmachine learning: it has successfully improved sample efficiency by an order of\nmagnitude on traditional tasks like topic classification and object\nrecognition. However, we uncover a striking contrast to this promise: across 5\nmodels and 4 datasets on the task of visual question answering, a wide variety\nof active learning approaches fail to outperform random selection. To\nunderstand this discrepancy, we profile 8 active learning methods on a\nper-example basis, and identify the problem as collective outliers -- groups of\nexamples that active learning methods prefer to acquire but models fail to\nlearn (e.g., questions that ask about text in images or require external\nknowledge). Through systematic ablation experiments and qualitative\nvisualizations, we verify that collective outliers are a general phenomenon\nresponsible for degrading pool-based active learning. Notably, we show that\nactive learning sample efficiency increases significantly as the number of\ncollective outliers in the active learning pool decreases. We conclude with a\ndiscussion and prescriptive recommendations for mitigating the effects of these\noutliers in future work.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 00:52:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Karamcheti", "Siddharth", ""], ["Krishna", "Ranjay", ""], ["Fei-Fei", "Li", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2107.02416", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Zixia Jia, Yong Jiang, Kewei Tu", "title": "Enhanced Universal Dependency Parsing with Automated Concatenation of\n  Embeddings", "comments": "Second Place in IWPT 2021 shared task, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes the system used in submission from SHANGHAITECH team to\nthe IWPT 2021 Shared Task. Our system is a graph-based parser with the\ntechnique of Automated Concatenation of Embeddings (ACE). Because recent work\nfound that better word representations can be obtained by concatenating\ndifferent types of embeddings, we use ACE to automatically find the better\nconcatenation of embeddings for the task of enhanced universal dependencies.\nAccording to official results averaged on 17 languages, our system ranks 2nd\nover 9 teams.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:33:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Xinyu", ""], ["Jia", "Zixia", ""], ["Jiang", "Yong", ""], ["Tu", "Kewei", ""]]}, {"id": "2107.02418", "submitter": "Changzhi Sun", "authors": "Changzhi Sun, Xinbo Zhang, Jiangjie Chen, Chun Gan, Yuanbin Wu, Jiaze\n  Chen, Hao Zhou, Lei Li", "title": "Probabilistic Graph Reasoning for Natural Proof Generation", "comments": "Accepted by Findings of ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of reasoning over natural language\nstatements. Prior neural based approaches do not explicitly consider the\ninter-dependency among answers and their proofs. In this paper, we propose\nPRobr, a novel approach for joint answer prediction and proof generation. PRobr\ndefines a joint probabilistic distribution over all possible proof graphs and\nanswers via an induced graphical model. We then optimize the model using\nvariational approximation on top of neural textual representation. Experiments\non multiple datasets under diverse settings (fully supervised, few-shot and\nzero-shot evaluation) verify the effectiveness of PRobr, e.g., achieving\n10%-30% improvement on QA accuracy in few/zero-shot evaluation. Our codes and\nmodels can be found at https://github.com/changzhisun/PRobr/.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:34:41 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Sun", "Changzhi", ""], ["Zhang", "Xinbo", ""], ["Chen", "Jiangjie", ""], ["Gan", "Chun", ""], ["Wu", "Yuanbin", ""], ["Chen", "Jiaze", ""], ["Zhou", "Hao", ""], ["Li", "Lei", ""]]}, {"id": "2107.02421", "submitter": "Inari Listenmaa", "authors": "Inari Listenmaa, Jason Morris, Alfred Ang, Maryam Hanafiah, Regina\n  Cheong", "title": "An NLG pipeline for a legal expert system: a work in progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present the NLG component for L4, a prototype domain-specific language\n(DSL) for drafting laws and contracts. As a concrete use case, we describe a\npipeline for a legal expert system created from L4 code. The NLG component is\nused in two steps. The first step is to create an interview, whose answers are\nprocessed into a query for an automated reasoner. The second step is to render\nthe answers of the reasoner in natural language.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:42:38 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Listenmaa", "Inari", ""], ["Morris", "Jason", ""], ["Ang", "Alfred", ""], ["Hanafiah", "Maryam", ""], ["Cheong", "Regina", ""]]}, {"id": "2107.02444", "submitter": "Chen Xu", "authors": "Chen Xu, Xiaoqian Liu, Xiaowen Liu, Laohu Wang, Canan Huang, Tong\n  Xiao, Jingbo Zhu", "title": "The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline\n  Task", "comments": "IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the submission of the NiuTrans end-to-end speech\ntranslation system for the IWSLT 2021 offline task, which translates from the\nEnglish audio to German text directly without intermediate transcription. We\nuse the Transformer-based model architecture and enhance it by Conformer,\nrelative position encoding, and stacked acoustic and textual encoding. To\naugment the training data, the English transcriptions are translated to German\ntranslations. Finally, we employ ensemble decoding to integrate the predictions\nfrom several models trained with the different datasets. Combining these\ntechniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which\nshows the enormous potential of the end-to-end model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:45:23 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:21:18 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Xu", "Chen", ""], ["Liu", "Xiaoqian", ""], ["Liu", "Xiaowen", ""], ["Wang", "Laohu", ""], ["Huang", "Canan", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""]]}, {"id": "2107.02472", "submitter": "Marco Guerini", "authors": "Yi-Ling Chung, Serra Sinem Tekiroglu, Sara Tonelli, Marco Guerini", "title": "Empowering NGOs in Countering Online Hate Messages", "comments": "Preprint of the paper published in Online Social Networks and Media\n  Journal (OSNEM)", "journal-ref": null, "doi": "10.1016/j.osnem.2021.100150", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on online hate speech have mostly focused on the automated detection\nof harmful messages. Little attention has been devoted so far to the\ndevelopment of effective strategies to fight hate speech, in particular through\nthe creation of counter-messages. While existing manual scrutiny and\nintervention strategies are time-consuming and not scalable, advances in\nnatural language processing have the potential to provide a systematic approach\nto hatred management. In this paper, we introduce a novel ICT platform that NGO\noperators can use to monitor and analyze social media data, along with a\ncounter-narrative suggestion tool. Our platform aims at increasing the\nefficiency and effectiveness of operators' activities against islamophobia. We\ntest the platform with more than one hundred NGO operators in three countries\nthrough qualitative and quantitative evaluation. Results show that NGOs favor\nthe platform solution with the suggestion tool, and that the time required to\nproduce counter-narratives significantly decreases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:36:24 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chung", "Yi-Ling", ""], ["Tekiroglu", "Serra Sinem", ""], ["Tonelli", "Sara", ""], ["Guerini", "Marco", ""]]}, {"id": "2107.02499", "submitter": "Anton Golubev", "authors": "Anton Golubev and Natalia Loukachevitch", "title": "Transfer Learning for Improving Results on Russian Sentiment Datasets", "comments": "Dialogue 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we test transfer learning approach on Russian sentiment\nbenchmark datasets using additional train sample created with distant\nsupervision technique. We compare several variants of combining additional data\nwith benchmark train samples. The best results were achieved using three-step\napproach of sequential training on general, thematic and original train\nsamples. For most datasets, the results were improved by more than 3% to the\ncurrent state-of-the-art methods. The BERT-NLI model treating sentiment\nclassification problem as a natural language inference task reached the human\nlevel of sentiment analysis on one of the datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:31:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Golubev", "Anton", ""], ["Loukachevitch", "Natalia", ""]]}, {"id": "2107.02527", "submitter": "Catherine Lai", "authors": "Elijah Gutierrez, Pilar Oplustil-Gallegos, Catherine Lai", "title": "Location, Location: Enhancing the Evaluation of Text-to-Speech Synthesis\n  Using the Rapid Prosody Transcription Paradigm", "comments": "Accepted to Speech Synthesis Workshop 2019: https://ssw11.hte.hu/en/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-Speech synthesis systems are generally evaluated using Mean Opinion\nScore (MOS) tests, where listeners score samples of synthetic speech on a\nLikert scale. A major drawback of MOS tests is that they only offer a general\nmeasure of overall quality-i.e., the naturalness of an utterance-and so cannot\ntell us where exactly synthesis errors occur. This can make evaluation of the\nappropriateness of prosodic variation within utterances inconclusive. To\naddress this, we propose a novel evaluation method based on the Rapid Prosody\nTranscription paradigm. This allows listeners to mark the locations of errors\nin an utterance in real-time, providing a probabilistic representation of the\nperceptual errors that occur in the synthetic signal. We conduct experiments\nthat confirm that the fine-grained evaluation can be mapped to system rankings\nof standard MOS tests, but the error marking gives a much more comprehensive\nassessment of synthesized prosody. In particular, for standard audiobook test\nset samples, we see that error marks consistently cluster around words at major\nprosodic boundaries indicated by punctuation. However, for question-answer\nbased stimuli, where we control information structure, we see differences\nemerge in the ability of neural TTS systems to generate context-appropriate\nprosodic prominence.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:36:40 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gutierrez", "Elijah", ""], ["Oplustil-Gallegos", "Pilar", ""], ["Lai", "Catherine", ""]]}, {"id": "2107.02530", "submitter": "Yuzi Yan", "authors": "Yuzi Yan, Xu Tan, Bohan Li, Guangyan Zhang, Tao Qin, Sheng Zhao, Yuan\n  Shen, Wei-Qiang Zhang, Tie-Yan Liu", "title": "AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style", "comments": "Accepted by INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While recent text to speech (TTS) models perform very well in synthesizing\nreading-style (e.g., audiobook) speech, it is still challenging to synthesize\nspontaneous-style speech (e.g., podcast or conversation), mainly because of two\nreasons: 1) the lack of training data for spontaneous speech; 2) the difficulty\nin modeling the filled pauses (um and uh) and diverse rhythms in spontaneous\nspeech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that\nfine-tunes a well-trained reading-style TTS model for spontaneous-style speech.\nSpecifically, 1) to insert filled pauses (FP) in the text sequence\nappropriately, we introduce an FP predictor to the TTS model; 2) to model the\nvarying rhythms, we introduce a duration predictor based on mixture of experts\n(MoE), which contains three experts responsible for the generation of fast,\nmedium and slow speech respectively, and fine-tune it as well as the pitch\npredictor for rhythm adaptation; 3) to adapt to other speaker timbre, we\nfine-tune some parameters in the decoder with few speech data. To address the\nchallenge of lack of training data, we mine a spontaneous speech dataset to\nsupport our research this work and facilitate future research on spontaneous\nTTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and\nrhythms in spontaneous styles, and achieves much better MOS and SMOS scores\nthan previous adaptive TTS systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:40:45 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yan", "Yuzi", ""], ["Tan", "Xu", ""], ["Li", "Bohan", ""], ["Zhang", "Guangyan", ""], ["Qin", "Tao", ""], ["Zhao", "Sheng", ""], ["Shen", "Yuan", ""], ["Zhang", "Wei-Qiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2107.02681", "submitter": "Jaemin Cho", "authors": "Zineng Tang, Jaemin Cho, Hao Tan, Mohit Bansal", "title": "VidLanKD: Improving Language Understanding via Video-Distilled Knowledge\n  Transfer", "comments": "18 pages (5 figures, 10 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization has attracted\nattention by using the predictions of a text-to-image retrieval model as labels\nfor language model supervision. Despite its success, the method suffers from\napproximation error of using finite image labels and the lack of vocabulary\ndiversity of a small image-text dataset. To overcome these limitations, we\npresent VidLanKD, a video-language knowledge distillation method for improving\nlanguage understanding. We train a multi-modal teacher model on a video-text\ndataset, and then transfer its knowledge to a student language model with a\ntext dataset. To avoid approximation error, we propose to use different\nknowledge distillation objectives. In addition, the use of a large-scale\nvideo-text dataset helps learn diverse and richer vocabularies. In our\nexperiments, VidLanKD achieves consistent improvements over text-only language\nmodels and vokenization models, on several downstream language understanding\ntasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world\nknowledge, physical reasoning, and temporal reasoning capabilities of our model\nby evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we\npresent comprehensive ablation studies as well as visualizations of the learned\ntext-to-video grounding results of our teacher and student language models. Our\ncode and models are available at: https://github.com/zinengtang/VidLanKD\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:41:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tang", "Zineng", ""], ["Cho", "Jaemin", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2107.02720", "submitter": "Luca De Nardis", "authors": "Maria-Gabriella Di Benedetto, Stefanie Shattuck-Hufnagel, Jeung-Yoon\n  Choi, Luca De Nardis, Javier Arango, Ian Chan, Alec DeCaprio", "title": "Lexical Access Model for Italian -- Modeling human speech processing:\n  identification of words in running speech toward lexical access based on the\n  detection of landmarks and other acoustic cues to features", "comments": "Submitted to Language and Speech, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modelling the process that a listener actuates in deriving the words intended\nby a speaker requires setting a hypothesis on how lexical items are stored in\nmemory. This work aims at developing a system that imitates humans when\nidentifying words in running speech and, in this way, provide a framework to\nbetter understand human speech processing. We build a speech recognizer for\nItalian based on the principles of Stevens' model of Lexical Access in which\nwords are stored as hierarchical arrangements of distinctive features (Stevens,\nK. N. (2002). \"Toward a model for lexical access based on acoustic landmarks\nand distinctive features,\" J. Acoust. Soc. Am., 111(4):1872-1891). Over the\npast few decades, the Speech Communication Group at the Massachusetts Institute\nof Technology (MIT) developed a speech recognition system for English based on\nthis approach. Italian will be the first language beyond English to be\nexplored; the extension to another language provides the opportunity to test\nthe hypothesis that words are represented in memory as a set of\nhierarchically-arranged distinctive features, and reveal which of the\nunderlying mechanisms may have a language-independent nature. This paper also\nintroduces a new Lexical Access corpus, the LaMIT database, created and labeled\nspecifically for this work, that will be provided freely to the speech research\ncommunity. Future developments will test the hypothesis that specific acoustic\ndiscontinuities - called landmarks - that serve as cues to features, are\nlanguage independent, while other cues may be language-dependent, with powerful\nimplications for understanding how the human brain recognizes speech.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:54:56 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Di Benedetto", "Maria-Gabriella", ""], ["Shattuck-Hufnagel", "Stefanie", ""], ["Choi", "Jeung-Yoon", ""], ["De Nardis", "Luca", ""], ["Arango", "Javier", ""], ["Chan", "Ian", ""], ["DeCaprio", "Alec", ""]]}, {"id": "2107.02757", "submitter": "Zhibin Duan", "authors": "Zhibin Duan, Dongsheng Wang, Bo Chen, Chaojie Wang, Wenchao Chen,\n  Yewen Li, Jie Ren, Mingyuan Zhou", "title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical topic models such as the gamma belief network (GBN) have\ndelivered promising results in mining multi-layer document representations and\ndiscovering interpretable topic taxonomies. However, they often assume in the\nprior that the topics at each layer are independently drawn from the Dirichlet\ndistribution, ignoring the dependencies between the topics both at the same\nlayer and across different layers. To relax this assumption, we propose\nsawtooth factorial topic embedding guided GBN, a deep generative model of\ndocuments that captures the dependencies and semantic similarities between the\ntopics in the embedding space. Specifically, both the words and topics are\nrepresented as embedding vectors of the same dimension. The topic matrix at a\nlayer is factorized into the product of a factor loading matrix and a topic\nembedding matrix, the transpose of which is set as the factor loading matrix of\nthe layer above. Repeating this particular type of factorization, which shares\ncomponents between adjacent layers, leads to a structure referred to as\nsawtooth factorization. An auto-encoding variational inference network is\nconstructed to optimize the model parameter via stochastic gradient descent.\nExperiments on big corpora show that our models outperform other neural topic\nmodels on extracting deeper interpretable topics and deriving better document\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 10:14:57 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Duan", "Zhibin", ""], ["Wang", "Dongsheng", ""], ["Chen", "Bo", ""], ["Wang", "Chaojie", ""], ["Chen", "Wenchao", ""], ["Li", "Yewen", ""], ["Ren", "Jie", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2107.02759", "submitter": "Juan Camilo V\\'asquez-Correa", "authors": "Daniel Escobar-Grisales, Juan Camilo Vasquez-Correa, Juan Rafael\n  Orozco-Arroyave", "title": "Gender Recognition in Informal and Formal Language Scenarios via\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The interest in demographic information retrieval based on text data has\nincreased in the research community because applications have shown success in\ndifferent sectors such as security, marketing, heath-care, and others.\nRecognition and identification of demographic traits such as gender, age,\nlocation, or personality based on text data can help to improve different\nmarketing strategies. For instance it makes it possible to segment and to\npersonalize offers, thus products and services are exposed to the group of\ngreatest interest. This type of technology has been discussed widely in\ndocuments from social media. However, the methods have been poorly studied in\ndata with a more formal structure, where there is no access to emoticons,\nmentions, and other linguistic phenomena that are only present in social media.\nThis paper proposes the use of recurrent and convolutional neural networks, and\na transfer learning strategy for gender recognition in documents that are\nwritten in informal and formal languages. Models are tested in two different\ndatabases consisting of Tweets and call-center conversations. Accuracies of up\nto 75\\% are achieved for both databases. The results also indicate that it is\npossible to transfer the knowledge from a system trained on a specific type of\nexpressions or idioms such as those typically used in social media into a more\nformal type of text data, where the amount of data is more scarce and its\nstructure is completely different.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:32:50 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Escobar-Grisales", "Daniel", ""], ["Vasquez-Correa", "Juan Camilo", ""], ["Orozco-Arroyave", "Juan Rafael", ""]]}, {"id": "2107.02782", "submitter": "Arnab Bhattacharya", "authors": "Hrishikesh Terdalkar, Arnab Bhattacharya", "title": "Sangrahaka: A Tool for Annotating and Querying Knowledge Graphs", "comments": null, "journal-ref": "FSE 2021", "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a web-based annotation and querying tool Sangrahaka.\nIt annotates entities and relationships from text corpora and constructs a\nknowledge graph (KG). The KG is queried using templatized natural language\nqueries. The application is language and corpus agnostic, but can be tuned for\nspecial needs of a specific language or a corpus. A customized version of the\nframework has been used in two annotation tasks. The application is available\nfor download and installation. Besides having a user-friendly interface, it is\nfast, supports customization, and is fault tolerant on both client and server\nside. The code is available at https://github.com/hrishikeshrt/sangrahaka and\nthe presentation with a demo is available at https://youtu.be/nw9GFLVZMMo.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:44:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Terdalkar", "Hrishikesh", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "2107.02794", "submitter": "Maxwell Nye", "authors": "Maxwell Nye, Michael Henry Tessler, Joshua B. Tenenbaum, Brenden M.\n  Lake", "title": "Improving Coherence and Consistency in Neural Sequence Models with\n  Dual-System, Neuro-Symbolic Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human reasoning can often be understood as an interplay between two systems:\nthe intuitive and associative (\"System 1\") and the deliberative and logical\n(\"System 2\"). Neural sequence models -- which have been increasingly successful\nat performing complex, structured tasks -- exhibit the advantages and failure\nmodes of System 1: they are fast and learn patterns from data, but are often\ninconsistent and incoherent. In this work, we seek a lightweight, training-free\nmeans of improving existing System 1-like sequence models by adding System\n2-inspired logical reasoning. We explore several variations on this theme in\nwhich candidate generations from a neural sequence model are examined for\nlogical consistency by a symbolic reasoning module, which can either accept or\nreject the generations. Our approach uses neural inference to mediate between\nthe neural System 1 and the logical System 2. Results in robust story\ngeneration and grounded instruction-following show that this approach can\nincrease the coherence and accuracy of neurally-based generations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:59:49 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nye", "Maxwell", ""], ["Tessler", "Michael Henry", ""], ["Tenenbaum", "Joshua B.", ""], ["Lake", "Brenden M.", ""]]}, {"id": "2107.02852", "submitter": "Naoyuki Kanda", "authors": "Naoyuki Kanda, Xiong Xiao, Jian Wu, Tianyan Zhou, Yashesh Gaur,\n  Xiaofei Wang, Zhong Meng, Zhuo Chen, Takuya Yoshioka", "title": "A Comparative Study of Modular and Joint Approaches for\n  Speaker-Attributed ASR on Monaural Long-Form Audio", "comments": "Submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker-attributed automatic speech recognition (SA-ASR) is a task to\nrecognize \"who spoke what\" from multi-talker recordings. An SA-ASR system\nusually consists of multiple modules such as speech separation, speaker\ndiarization and ASR. On the other hand, considering the joint optimization, an\nend-to-end (E2E) SA-ASR model has recently been proposed with promising results\non simulation data. In this paper, we present our recent study on the\ncomparison of such modular and joint approaches towards SA-ASR on real monaural\nrecordings. We develop state-of-the-art SA-ASR systems for both modular and\njoint approaches by leveraging large-scale training data, including 75 thousand\nhours of ASR training data and the VoxCeleb corpus for speaker representation\nlearning. We also propose a new pipeline that performs the E2E SA-ASR model\nafter speaker clustering. Our evaluation on the AMI meeting corpus reveals that\nafter fine-tuning with a small real data, the joint system performs 9.2--29.4%\nbetter in accuracy compared to the best modular system while the modular system\nperforms better before such fine-tuning. We also conduct various error analyses\nto show the remaining issues for the monaural SA-ASR.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 19:36:48 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kanda", "Naoyuki", ""], ["Xiao", "Xiong", ""], ["Wu", "Jian", ""], ["Zhou", "Tianyan", ""], ["Gaur", "Yashesh", ""], ["Wang", "Xiaofei", ""], ["Meng", "Zhong", ""], ["Chen", "Zhuo", ""], ["Yoshioka", "Takuya", ""]]}, {"id": "2107.02858", "submitter": "Claire Bowern", "authors": "Rachel Sterneck, Annie Polish, Claire Bowern", "title": "Topic Modeling in the Voynich Manuscript", "comments": "See https://lingbuzz.net/lingbuzz/006068 for a version that has the\n  Voynich font (and better figure placement), since arxiv does not allow\n  xelatex compilation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article presents the results of investigations using topic modeling of\nthe Voynich Manuscript (Beinecke MS408). Topic modeling is a set of\ncomputational methods which are used to identify clusters of subjects within\ntext. We use latent dirichlet allocation, latent semantic analysis, and\nnonnegative matrix factorization to cluster Voynich pages into `topics'. We\nthen compare the topics derived from the computational models to clusters\nderived from the Voynich illustrations and from paleographic analysis. We find\nthat computationally derived clusters match closely to a conjunction of scribe\nand subject matter (as per the illustrations), providing further evidence that\nthe Voynich Manuscript contains meaningful text.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 19:50:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Sterneck", "Rachel", ""], ["Polish", "Annie", ""], ["Bowern", "Claire", ""]]}, {"id": "2107.02865", "submitter": "Aidan Hogan", "authors": "Daniel Diomedi, Aidan Hogan", "title": "Question Answering over Knowledge Graphs with Neural Machine Translation\n  and Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of Question Answering over Knowledge Graphs (KGQA) is to find\nanswers for natural language questions over a knowledge graph. Recent KGQA\napproaches adopt a neural machine translation (NMT) approach, where the natural\nlanguage question is translated into a structured query language. However, NMT\nsuffers from the out-of-vocabulary problem, where terms in a question may not\nhave been seen during training, impeding their translation. This issue is\nparticularly problematic for the millions of entities that large knowledge\ngraphs describe. We rather propose a KGQA approach that delegates the\nprocessing of entities to entity linking (EL) systems. NMT is then used to\ncreate a query template with placeholders that are filled by entities\nidentified in an EL phase. Slot filling is used to decide which entity fills\nwhich placeholder. Experiments for QA over Wikidata show that our approach\noutperforms pure NMT: while there remains a strong dependence on having seen\nsimilar query templates during training, errors relating to entities are\ngreatly reduced.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 19:57:02 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Diomedi", "Daniel", ""], ["Hogan", "Aidan", ""]]}, {"id": "2107.02875", "submitter": "Won Ik Cho", "authors": "Won Ik Cho, Seok Min Kim, Hyunchang Cho, Nam Soo Kim", "title": "Kosp2e: Korean Speech to English Translation Corpus", "comments": "Interspeech 2021 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most speech-to-text (S2T) translation studies use English speech as a source,\nwhich makes it difficult for non-English speakers to take advantage of the S2T\ntechnologies. For some languages, this problem was tackled through corpus\nconstruction, but the farther linguistically from English or the more\nunder-resourced, this deficiency and underrepresentedness becomes more\nsignificant. In this paper, we introduce kosp2e (read as `kospi'), a corpus\nthat allows Korean speech to be translated into English text in an end-to-end\nmanner. We adopt open license speech recognition corpus, translation corpus,\nand spoken language corpora to make our dataset freely available to the public,\nand check the performance through the pipeline and training-based approaches.\nUsing pipeline and various end-to-end schemes, we obtain the highest BLEU of\n21.3 and 18.0 for each based on the English hypothesis, validating the\nfeasibility of our data. We plan to supplement annotations for other target\nlanguages through community contributions in the future.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:34:06 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cho", "Won Ik", ""], ["Kim", "Seok Min", ""], ["Cho", "Hyunchang", ""], ["Kim", "Nam Soo", ""]]}, {"id": "2107.02893", "submitter": "Chao-Chun Liang", "authors": "Daniel Lee, Chao-Chun Liang, Keh-Yih Su", "title": "Answering Chinese Elementary School Social Study Multiple Choice\n  Questions", "comments": "TAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to answer the Chinese elementary school Social\nStudy Multiple Choice questions. Although BERT has demonstrated excellent\nperformance on Reading Comprehension tasks, it is found not good at handling\nsome specific types of questions, such as Negation, All-of-the-above, and\nNone-of-the-above. We thus propose a novel framework to cascade BERT with a\nPre-Processor and an Answer-Selector modules to tackle the above challenges.\nExperimental results show the proposed approach effectively improves the\nperformance of BERT, and thus demonstrate the feasibility of supplementing BERT\nwith additional modules.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 14:15:21 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lee", "Daniel", ""], ["Liang", "Chao-Chun", ""], ["Su", "Keh-Yih", ""]]}, {"id": "2107.02968", "submitter": "Ali Madani", "authors": "Alvin Chan, Ali Madani, Ben Krause, Nikhil Naik", "title": "Deep Extrapolation for Attribute-Enhanced Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute extrapolation in sample generation is challenging for deep neural\nnetworks operating beyond the training distribution. We formulate a new task\nfor extrapolation in sequence generation, focusing on natural language and\nproteins, and propose GENhance, a generative framework that enhances attributes\nthrough a learned latent space. Trained on movie reviews and a computed protein\nstability dataset, GENhance can generate strongly-positive text reviews and\nhighly stable protein sequences without being exposed to similar data during\ntraining. We release our benchmark tasks and models to contribute to the study\nof generative modeling extrapolation and data-driven design in biology and\nchemistry.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:30:36 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chan", "Alvin", ""], ["Madani", "Ali", ""], ["Krause", "Ben", ""], ["Naik", "Nikhil", ""]]}, {"id": "2107.02975", "submitter": "Irene Li", "authors": "Irene Li, Jessica Pan, Jeremy Goldwasser, Neha Verma, Wai Pan Wong,\n  Muhammed Yavuz Nuzumlal{\\i}, Benjamin Rosand, Yixin Li, Matthew Zhang, David\n  Chang, R. Andrew Taylor, Harlan M. Krumholz and Dragomir Radev", "title": "Neural Natural Language Processing for Unstructured Data in Electronic\n  Health Records: a Review", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHRs), digital collections of patient healthcare\nevents and observations, are ubiquitous in medicine and critical to healthcare\ndelivery, operations, and research. Despite this central role, EHRs are\nnotoriously difficult to process automatically. Well over half of the\ninformation stored within EHRs is in the form of unstructured text (e.g.\nprovider notes, operation reports) and remains largely untapped for secondary\nuse. Recently, however, newer neural network and deep learning approaches to\nNatural Language Processing (NLP) have made considerable advances,\noutperforming traditional statistical and rule-based systems on a variety of\ntasks. In this survey paper, we summarize current neural NLP methods for EHR\napplications. We focus on a broad scope of tasks, namely, classification and\nprediction, word embeddings, extraction, generation, and other topics such as\nquestion answering, phenotyping, knowledge graphs, medical dialogue,\nmultilinguality, interpretability, etc.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:50:02 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Li", "Irene", ""], ["Pan", "Jessica", ""], ["Goldwasser", "Jeremy", ""], ["Verma", "Neha", ""], ["Wong", "Wai Pan", ""], ["Nuzumlal\u0131", "Muhammed Yavuz", ""], ["Rosand", "Benjamin", ""], ["Li", "Yixin", ""], ["Zhang", "Matthew", ""], ["Chang", "David", ""], ["Taylor", "R. Andrew", ""], ["Krumholz", "Harlan M.", ""], ["Radev", "Dragomir", ""]]}, {"id": "2107.02983", "submitter": "Gihan Dias", "authors": "Upuli Liyanapathirana, Kaumini Gunasinghe, Gihan Dias", "title": "SinSpell: A Comprehensive Spelling Checker for Sinhala", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have built SinSpell, a comprehensive spelling checker for the Sinhala\nlanguage which is spoken by over 16 million people, mainly in Sri Lanka.\nHowever, until recently, Sinhala had no spelling checker with acceptable\ncoverage. Sinspell is still the only open source Sinhala spelling checker.\nSinSpell identifies possible spelling errors and suggests corrections. It also\ncontains a module which auto-corrects evident errors. To maintain accuracy,\nSinSpell was designed as a rule-based system based on Hunspell. A set of words\nwas compiled from several sources and verified. These were divided into\nmorphological classes, and the valid roots, suffixes and prefixes for each\nclass were identified, together with lists of irregular words and exceptions.\nThe errors in a corpus of Sinhala documents were analysed and commonly\nmisspelled words and types of common errors were identified. We found that the\nmost common errors were in vowel length and similar sounding letters. Errors\ndue to incorrect typing and encoding were also found. This analysis was used to\ndevelop the suggestion generator and auto-corrector.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 02:36:43 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liyanapathirana", "Upuli", ""], ["Gunasinghe", "Kaumini", ""], ["Dias", "Gihan", ""]]}, {"id": "2107.03006", "submitter": "Jacob Austin", "authors": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow and Rianne\n  van den Berg", "title": "Structured Denoising Diffusion Models in Discrete State-Spaces", "comments": "10 pages plus references and appendices. First two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown\nimpressive results on image and waveform generation in continuous state spaces.\nHere, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),\ndiffusion-like generative models for discrete data that generalize the\nmultinomial diffusion model of Hoogeboom et al. 2021, by going beyond\ncorruption processes with uniform transition probabilities. This includes\ncorruption with transition matrices that mimic Gaussian kernels in continuous\nspace, matrices based on nearest neighbors in embedding space, and matrices\nthat introduce absorbing states. The third allows us to draw a connection\nbetween diffusion models and autoregressive and mask-based generative models.\nWe show that the choice of transition matrix is an important design decision\nthat leads to improved results in image and text domains. We also introduce a\nnew loss function that combines the variational lower bound with an auxiliary\ncross entropy loss. For text, this model class achieves strong results on\ncharacter-level text generation while scaling to large vocabularies on LM1B. On\nthe image dataset CIFAR-10, our models approach the sample quality and exceed\nthe log-likelihood of the continuous-space DDPM model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:11:00 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 17:09:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Austin", "Jacob", ""], ["Johnson", "Daniel D.", ""], ["Ho", "Jonathan", ""], ["Tarlow", "Daniel", ""], ["Berg", "Rianne van den", ""]]}, {"id": "2107.03007", "submitter": "Wenjie Peng", "authors": "Huahuan Zheng, Wenjie Peng, Zhijian Ou and Jinsong Zhang", "title": "Advancing CTC-CRF Based End-to-End Speech Recognition with Wordpieces\n  and Conformers", "comments": "Submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition systems have been largely improved in the past\nfew decades and current systems are mainly hybrid-based and end-to-end-based.\nThe recently proposed CTC-CRF framework inherits the data-efficiency of the\nhybrid approach and the simplicity of the end-to-end approach. In this paper,\nwe further advance CTC-CRF based ASR technique with explorations on modeling\nunits and neural architectures. Specifically, we investigate techniques to\nenable the recently developed wordpiece modeling units and Conformer neural\nnetworks to be succesfully applied in CTC-CRFs. Experiments are conducted on\ntwo English datasets (Switchboard, Librispeech) and a German dataset from\nCommonVoice. Experimental results suggest that (i) Conformer can improve the\nrecognition performance significantly; (ii) Wordpiece-based systems perform\nslightly worse compared with phone-based systems for the target language with a\nlow degree of grapheme-phoneme correspondence (e.g. English), while the two\nsystems can perform equally strong when such degree of correspondence is high\nfor the target language (e.g. German).\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:12:06 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 14:04:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zheng", "Huahuan", ""], ["Peng", "Wenjie", ""], ["Ou", "Zhijian", ""], ["Zhang", "Jinsong", ""]]}, {"id": "2107.03054", "submitter": "Xueyuan Lin", "authors": "Xueyuan Lin, Haihong E, Wenyu Song, Haoran Luo", "title": "EchoEA: Echo Information between Entities and Relations for Entity\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) is to discover entities referring to the same object in\nthe real world from different knowledge graphs (KGs). It plays an important\nrole in automatically integrating KGs from multiple sources.\n  Existing knowledge graph embedding (KGE) methods based on Graph Neural\nNetworks (GNNs) have achieved promising results, which enhance entity\nrepresentation with relation information unidirectionally. Besides, more and\nmore methods introduce semi-supervision to ask for more labeled training data.\n  However, two challenges still exist in these methods: (1) Insufficient\ninteraction: The interaction between entities and relations is insufficiently\nutilized. (2) Low-quality bootstrapping: The generated semi-supervised data is\nof low quality.\n  In this paper, we propose a novel framework, Echo Entity Alignment (EchoEA),\nwhich leverages self-attention mechanism to spread entity information to\nrelations and echo back to entities. The relation representation is dynamically\ncomputed from entity representation. Symmetrically, the next entity\nrepresentation is dynamically calculated from relation representation, which\nshows sufficient interaction.\n  Furthermore, we propose attribute-combined bi-directional global-filtered\nstrategy (ABGS) to improve bootstrapping, reduce false samples and generate\nhigh-quality training data.\n  The experimental results on three real-world cross-lingual datasets are\nstable at around 96\\% at hits@1 on average, showing that our approach not only\nsignificantly outperforms the state-of-the-art methods, but also is universal\nand transferable for existing KGE methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 07:34:21 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lin", "Xueyuan", ""], ["E", "Haihong", ""], ["Song", "Wenyu", ""], ["Luo", "Haoran", ""]]}, {"id": "2107.03069", "submitter": "Gerard I. G\\'allego", "authors": "Belen Alastruey and Gerard I. G\\'allego and Marta R. Costa-juss\\`a", "title": "Efficient Transformer for Direct Speech Translation", "comments": "(c) 2021 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Transformer-based models has surpassed the barriers of text.\nWhen working with speech, we must face a problem: the sequence length of an\naudio input is not suitable for the Transformer. To bypass this problem, a\nusual approach is adding strided convolutional layers, to reduce the sequence\nlength before using the Transformer. In this paper, we propose a new approach\nfor direct Speech Translation, where thanks to an efficient Transformer we can\nwork with a spectrogram without having to use convolutional layers before the\nTransformer. This allows the encoder to learn directly from the spectrogram and\nno information is lost. We have created an encoder-decoder model, where the\nencoder is an efficient Transformer -- the Longformer -- and the decoder is a\ntraditional Transformer decoder. Our results, which are close to the ones\nobtained with the standard approach, show that this is a promising research\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:13:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Alastruey", "Belen", ""], ["G\u00e1llego", "Gerard I.", ""], ["Costa-juss\u00e0", "Marta R.", ""]]}, {"id": "2107.03072", "submitter": "Sevil Sen", "authors": "Sevil Sen and Burcu Can", "title": "Android Security using NLP Techniques: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Android is among the most targeted platform by attackers. While attackers are\nimproving their techniques, traditional solutions based on static and dynamic\nanalysis have been also evolving. In addition to the application code, Android\napplications have some metadata that could be useful for security analysis of\napplications. Unlike traditional application distribution mechanisms, Android\napplications are distributed centrally in mobile markets. Therefore, beside\napplication packages, such markets contain app information provided by app\ndevelopers and app users. The availability of such useful textual data together\nwith the advancement in Natural Language Processing (NLP) that is used to\nprocess and understand textual data has encouraged researchers to investigate\nthe use of NLP techniques in Android security. Especially, security solutions\nbased on NLP have accelerated in the last 5 years and proven to be useful. This\nstudy reviews these proposals and aim to explore possible research directions\nfor future studies by presenting state-of-the-art in this domain. We mainly\nfocus on NLP-based solutions under four categories: description-to-behaviour\nfidelity, description generation, privacy and malware detection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:33:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Sen", "Sevil", ""], ["Can", "Burcu", ""]]}, {"id": "2107.03104", "submitter": "Fangyuan Wang", "authors": "Fangyuan Wang, Zhigang Song, Hongchen Jiang, Bo Xu", "title": "MACCIF-TDNN: Multi aspect aggregation of channel and context\n  interdependence features in TDNN-based speaker verification", "comments": "6 pages. arXiv admin note: text overlap with arXiv:2005.07143 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the recent state-of-the-art results for speaker verification are\nachieved by X-vector and its subsequent variants. In this paper, we propose a\nnew network architecture which aggregates the channel and context\ninterdependence features from multi aspect based on Time Delay Neural Network\n(TDNN). Firstly, we use the SE-Res2Blocks as in ECAPA-TDNN to explicitly model\nthe channel interdependence to realize adaptive calibration of channel\nfeatures, and process local context features in a multi-scale way at a more\ngranular level compared with conventional TDNN-based methods. Secondly, we\nexplore to use the encoder structure of Transformer to model the global context\ninterdependence features at an utterance level which can capture better long\nterm temporal characteristics. Before the pooling layer, we aggregate the\noutputs of SE-Res2Blocks and Transformer encoder to leverage the complementary\nchannel and context interdependence features learned by themself respectively.\nFinally, instead of performing a single attentive statistics pooling, we also\nfind it beneficial to extend the pooling method in a multi-head way which can\ndiscriminate features from multiple aspect. The proposed MACCIF-TDNN\narchitecture can outperform most of the state-of-the-art TDNN-based systems on\nVoxCeleb1 test sets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:43:42 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Fangyuan", ""], ["Song", "Zhigang", ""], ["Jiang", "Hongchen", ""], ["Xu", "Bo", ""]]}, {"id": "2107.03134", "submitter": "Zeljko Kraljevic", "authors": "Zeljko Kraljevic, Anthony Shek, Daniel Bean, Rebecca Bendayan, James\n  Teo, Richard Dobson", "title": "MedGPT: Medical Concept Prediction from Clinical Narratives", "comments": "6 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The data available in Electronic Health Records (EHRs) provides the\nopportunity to transform care, and the best way to provide better care for one\npatient is through learning from the data available on all other patients.\nTemporal modelling of a patient's medical history, which takes into account the\nsequence of past events, can be used to predict future events such as a\ndiagnosis of a new disorder or complication of a previous or existing disorder.\nWhile most prediction approaches use mostly the structured data in EHRs or a\nsubset of single-domain predictions and outcomes, we present MedGPT a novel\ntransformer-based pipeline that uses Named Entity Recognition and Linking tools\n(i.e. MedCAT) to structure and organize the free text portion of EHRs and\nanticipate a range of future medical events (initially disorders). Since a\nlarge portion of EHR data is in text form, such an approach benefits from a\ngranular and detailed view of a patient while introducing modest additional\nnoise. MedGPT effectively deals with the noise and the added granularity, and\nachieves a precision of 0.344, 0.552 and 0.640 (vs LSTM 0.329, 0.538 and 0.633)\nwhen predicting the top 1, 3 and 5 candidate future disorders on real world\nhospital data from King's College Hospital, London, UK (\\textasciitilde600k\npatients). We also show that our model captures medical knowledge by testing it\non an experimental medical multiple choice question answering task, and by\nexamining the attentional focus of the model using gradient-based saliency\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:36:28 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kraljevic", "Zeljko", ""], ["Shek", "Anthony", ""], ["Bean", "Daniel", ""], ["Bendayan", "Rebecca", ""], ["Teo", "James", ""], ["Dobson", "Richard", ""]]}, {"id": "2107.03141", "submitter": "Taimoor Ahmed Javed", "authors": "Taimoor Ahmed Javed, Waseem Shahzad, Umair Arshad", "title": "Hierarchical Text Classification of Urdu News using Deep Neural Network", "comments": "22 pages with 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital text is increasing day by day on the internet. It is very challenging\nto classify a large and heterogeneous collection of data, which require\nimproved information processing methods to organize text. To classify large\nsize of corpus, one common approach is to use hierarchical text classification,\nwhich aims to classify textual data in a hierarchical structure. Several\napproaches have been proposed to tackle classification of text but most of the\nresearch has been done on English language. This paper proposes a deep learning\nmodel for hierarchical text classification of news in Urdu language -\nconsisting of 51,325 sentences from 8 online news websites belonging to the\nfollowing genres: Sports; Technology; and Entertainment. The objectives of this\npaper are twofold: (1) to develop a large human-annotated dataset of news in\nUrdu language for hierarchical text classification; and (2) to classify Urdu\nnews hierarchically using our proposed model based on LSTM mechanism named as\nHierarchical Multi-layer LSTMs (HMLSTM). Our model consists of two modules:\nText Representing Layer, for obtaining text representation in which we use\nWord2vec embedding to transform the words to vector and Urdu Hierarchical LSTM\nLayer (UHLSTML) an end-to-end fully connected deep LSTMs network to perform\nautomatic feature learning, we train one LSTM layer for each level of the class\nhierarchy. We have performed extensive experiments on our self created dataset\nnamed as Urdu News Dataset for Hierarchical Text Classification (UNDHTC). The\nresult shows that our proposed method is very effective for hierarchical text\nclassification and it outperforms baseline methods significantly and also\nachieved good results as compare to deep neural model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:06:11 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Javed", "Taimoor Ahmed", ""], ["Shahzad", "Waseem", ""], ["Arshad", "Umair", ""]]}, {"id": "2107.03158", "submitter": "Markus Bayer", "authors": "Markus Bayer, Marc-Andr\\'e Kaufhold, Christian Reuter", "title": "A Survey on Data Augmentation for Text Classification", "comments": "35 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:37:03 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 12:46:29 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bayer", "Markus", ""], ["Kaufhold", "Marc-Andr\u00e9", ""], ["Reuter", "Christian", ""]]}, {"id": "2107.03175", "submitter": "Xiachong Feng", "authors": "Xiachong Feng, Xiaocheng Feng, Bing Qin", "title": "A Survey on Dialogue Summarization: Recent Advances and New Frontiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of dialogue systems and natural language generation\ntechniques, the resurgence of dialogue summarization has attracted significant\nresearch attentions, which aims to condense the original dialogue into a\nshorter version covering salient information. However, there remains a lack of\ncomprehensive survey for this task. To this end, we take the first step and\npresent a thorough review of this research field. In detail, we provide an\noverview of publicly available research datasets, summarize existing works\naccording to the domain of input dialogue as well as organize leaderboards\nunder unified metrics. Furthermore, we discuss some future directions and give\nour thoughts. We hope that this first survey of dialogue summarization can\nprovide the community with a quick access and a general picture to this task\nand motivate future researches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:11:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Feng", "Xiachong", ""], ["Feng", "Xiaocheng", ""], ["Qin", "Bing", ""]]}, {"id": "2107.03176", "submitter": "Ernie Chang", "authors": "Ernie Chang, Xiaoyu Shen, Hui-Syuan Yeh, Vera Demberg", "title": "On Training Instance Selection for Few-Shot Neural Text Generation", "comments": "Accepted at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale pretrained language models have led to dramatic improvements in\ntext generation. Impressive performance can be achieved by finetuning only on a\nsmall number of instances (few-shot setting). Nonetheless, almost all previous\nwork simply applies random sampling to select the few-shot training instances.\nLittle to no attention has been paid to the selection strategies and how they\nwould affect model performance. In this work, we present a study on training\ninstance selection in few-shot neural text generation. The selection decision\nis made based only on the unlabeled data so as to identify the most worthwhile\ndata points that should be annotated under some budget of labeling cost. Based\non the intuition that the few-shot training instances should be diverse and\nrepresentative of the entire data distribution, we propose a simple selection\nstrategy with K-means clustering. We show that even with the naive\nclustering-based approach, the generation models consistently outperform random\nsampling on three text generation tasks: data-to-text generation, document\nsummarization and question generation. We hope that this work will call for\nmore attention on this largely unexplored area.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:16:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chang", "Ernie", ""], ["Shen", "Xiaoyu", ""], ["Yeh", "Hui-Syuan", ""], ["Demberg", "Vera", ""]]}, {"id": "2107.03179", "submitter": "Ernie Chang", "authors": "Ernie Chang, Yow-Ting Shiue, Hui-Syuan Yeh, Vera Demberg", "title": "Time-Aware Ancient Chinese Text Translation and Inference", "comments": "Accepted at LChange at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we aim to address the challenges surrounding the translation\nof ancient Chinese text: (1) The linguistic gap due to the difference in eras\nresults in translations that are poor in quality, and (2) most translations are\nmissing the contextual information that is often very crucial to understanding\nthe text. To this end, we improve upon past translation techniques by proposing\nthe following: We reframe the task as a multi-label prediction task where the\nmodel predicts both the translation and its particular era. We observe that\nthis helps to bridge the linguistic gap as chronological context is also used\nas auxiliary information. % As a natural step of generalization, we pivot on\nthe modern Chinese translations to generate multilingual outputs. %We show\nexperimentally the efficacy of our framework in producing quality translation\noutputs and also validate our framework on a collected task-specific parallel\ncorpus. We validate our framework on a parallel corpus annotated with\nchronology information and show experimentally its efficacy in producing\nquality translation outputs. We release both the code and the data\nhttps://github.com/orina1123/time-aware-ancient-text-translation for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:23:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chang", "Ernie", ""], ["Shiue", "Yow-Ting", ""], ["Yeh", "Hui-Syuan", ""], ["Demberg", "Vera", ""]]}, {"id": "2107.03242", "submitter": "Kyungjae Lee", "authors": "Kyungjae Lee, Seung-won Hwang, Sang-eun Han and Dohyeon Lee", "title": "Robustifying Multi-hop QA through Pseudo-Evidentiality Training", "comments": "Accepted to ACL2021 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the bias problem of multi-hop question answering models,\nof answering correctly without correct reasoning. One way to robustify these\nmodels is by supervising to not only answer right, but also with right\nreasoning chains. An existing direction is to annotate reasoning chains to\ntrain models, requiring expensive additional annotations. In contrast, we\npropose a new approach to learn evidentiality, deciding whether the answer\nprediction is supported by correct evidences, without such annotations.\nInstead, we compare counterfactual changes in answer confidence with and\nwithout evidence sentences, to generate \"pseudo-evidentiality\" annotations. We\nvalidate our proposed model on an original set and challenge set in HotpotQA,\nshowing that our method is accurate and robust in multi-hop reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:15:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lee", "Kyungjae", ""], ["Hwang", "Seung-won", ""], ["Han", "Sang-eun", ""], ["Lee", "Dohyeon", ""]]}, {"id": "2107.03266", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen, Niko Partanen, Khalid Alnajjar", "title": "Lemmatization of Historical Old Literary Finnish Texts in Modern\n  Orthography", "comments": "la 28e Conf\\'erence sur le Traitement Automatique des Langues\n  Naturelles (TALN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Texts written in Old Literary Finnish represent the first literary work ever\nwritten in Finnish starting from the 16th century. There have been several\nprojects in Finland that have digitized old publications and made them\navailable for research use. However, using modern NLP methods in such data\nposes great challenges. In this paper we propose an approach for simultaneously\nnormalizing and lemmatizing Old Literary Finnish into modern spelling. Our best\nmodel reaches to 96.3\\% accuracy in texts written by Agricola and 87.7\\%\naccuracy in other contemporary out-of-domain text. Our method has been made\nfreely available on Zenodo and Github.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:01:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Partanen", "Niko", ""], ["Alnajjar", "Khalid", ""]]}, {"id": "2107.03277", "submitter": "Llu\\'is Alemany-Puig", "authors": "Llu\\'is Alemany-Puig and Ramon Ferrer-i-Cancho", "title": "Linear-time calculation of the expected sum of edge lengths in random\n  projective linearizations of trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The syntactic structure of a sentence is often represented using syntactic\ndependency trees. The sum of the distances between syntactically related words\nhas been in the limelight for the past decades. Research on dependency\ndistances led to the formulation of the principle of dependency distance\nminimization whereby words in sentences are ordered so as to minimize that sum.\nNumerous random baselines have been defined to carry out related quantitative\nstudies on languages. The simplest random baseline is the expected value of the\nsum in unconstrained random permutations of the words in the sentence, namely\nwhen all the shufflings of the words of a sentence are allowed and equally\nlikely. Here we focus on a popular baseline: random projective permutations of\nthe words of the sentence, that is, permutations where the syntactic dependency\nstructure is projective, a formal constraint that sentences satisfy often in\nlanguages. Thus far, the expectation of the sum of dependency distances in\nrandom projective shufflings of a sentence has been estimated approximately\nwith a Monte Carlo procedure whose cost is of the order of $Zn$, where $n$ is\nthe number of words of the sentence and $Z$ is the number of samples; the\nlarger $Z$, the lower the error of the estimation but the larger the time cost.\nHere we present formulae to compute that expectation without error in time of\nthe order of $n$. Furthermore, we show that star trees maximize it, and devise\na dynamic programming algorithm to retrieve the trees that minimize it.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:11:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Alemany-Puig", "Llu\u00eds", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "2107.03286", "submitter": "Hyunmin Jeon", "authors": "Hyunmin Jeon, Gary Geunbae Lee", "title": "DORA: Toward Policy Optimization for Task-oriented Dialogue System with\n  Efficient Context", "comments": "23 pages, 9 figures, submitted to Computer Speech ans Language\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, reinforcement learning (RL) has been applied to task-oriented\ndialogue systems by using latent actions to solve shortcomings of supervised\nlearning (SL). In this paper, we propose a multi-domain task-oriented dialogue\nsystem, called Dialogue System with Optimizing a Recurrent Action Policy using\nEfficient Context (DORA), that uses SL, with subsequently applied RL to\noptimize dialogue systems using a recurrent dialogue policy. This dialogue\npolicy recurrently generates explicit system actions as a both word-level and\nhigh-level policy. As a result, DORA is clearly optimized during both SL and RL\nsteps by using an explicit system action policy that considers an efficient\ncontext instead of the entire dialogue history. The system actions are both\ninterpretable and controllable, whereas the latent actions are not. DORA\nimproved the success rate by 6.6 points on MultiWOZ 2.0 and by 10.9 points on\nMultiWOZ 2.1.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:24:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Jeon", "Hyunmin", ""], ["Lee", "Gary Geunbae", ""]]}, {"id": "2107.03297", "submitter": "Angelo Salatino Dr", "authors": "Mojtaba Nayyeri, Gokce Muge Cil, Sahar Vahdati, Francesco Osborne,\n  Mahfuzur Rahman, Simone Angioni, Angelo Salatino, Diego Reforgiato Recupero,\n  Nadezhda Vassilyeva, Enrico Motta, Jens Lehmann", "title": "Trans4E: Link Prediction on Scholarly Knowledge Graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2021.02.100", "report-no": null, "categories": "cs.AI cs.CL cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the\nquality of AI-based services. In the scholarly domain, KGs describing research\npublications typically lack important information, hindering our ability to\nanalyse and predict research dynamics. In recent years, link prediction\napproaches based on Knowledge Graph Embedding models became the first aid for\nthis issue. In this work, we present Trans4E, a novel embedding model that is\nparticularly fit for KGs which include N to M relations with N$\\gg$M. This is\ntypical for KGs that categorize a large number of entities (e.g., research\narticles, patents, persons) according to a relatively small set of categories.\nTrans4E was applied on two large-scale knowledge graphs, the Academia/Industry\nDynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the\ninformation about Fields of Study (e.g., 'neural networks', 'machine learning',\n'artificial intelligence'), and affiliation types (e.g., 'education',\n'company', 'government'), improving the scope and accuracy of the resulting\ndata. We evaluated our approach against alternative solutions on AIDA, MAG, and\nfour other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms\nthe other models when using low embedding dimensions and obtains competitive\nresults in high dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 09:34:44 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nayyeri", "Mojtaba", ""], ["Cil", "Gokce Muge", ""], ["Vahdati", "Sahar", ""], ["Osborne", "Francesco", ""], ["Rahman", "Mahfuzur", ""], ["Angioni", "Simone", ""], ["Salatino", "Angelo", ""], ["Recupero", "Diego Reforgiato", ""], ["Vassilyeva", "Nadezhda", ""], ["Motta", "Enrico", ""], ["Lehmann", "Jens", ""]]}, {"id": "2107.03438", "submitter": "Junha Roh", "authors": "Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox", "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 18:55:03 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 04:55:50 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Roh", "Junha", ""], ["Desingh", "Karthik", ""], ["Farhadi", "Ali", ""], ["Fox", "Dieter", ""]]}, {"id": "2107.03444", "submitter": "Philippe Laban", "authors": "Philippe Laban and Tobias Schnabel and Paul Bennett and Marti A.\n  Hearst", "title": "Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text", "comments": "Accepted at ACL-IJCNLP 2021, 14 pages, 7 figures", "journal-ref": "Association for Computational Linguistics (2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Keep it Simple (KiS), a new approach to unsupervised text\nsimplification which learns to balance a reward across three properties:\nfluency, salience and simplicity. We train the model with a novel algorithm to\noptimize the reward (k-SCST), in which the model proposes several candidate\nsimplifications, computes each candidate's reward, and encourages candidates\nthat outperform the mean reward. Finally, we propose a realistic text\ncomprehension task as an evaluation method for text simplification. When tested\non the English news domain, the KiS model outperforms strong supervised\nbaselines by more than 4 SARI points, and can help people complete a\ncomprehension task an average of 18% faster while retaining accuracy, when\ncompared to the original text. Code available:\nhttps://github.com/tingofurro/keep_it_simple\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:12:44 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Laban", "Philippe", ""], ["Schnabel", "Tobias", ""], ["Bennett", "Paul", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2107.03448", "submitter": "Philippe Laban", "authors": "Philippe Laban and Luke Dai and Lucas Bandarkar and Marti A. Hearst", "title": "Can Transformer Models Measure Coherence In Text? Re-Thinking the\n  Shuffle Test", "comments": "Accepted at ACL-IJCNLP 2021 (short paper), 7 pages, 4 figures", "journal-ref": "Association for Computational Linguistics (2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shuffle Test is the most common task to evaluate whether NLP models can\nmeasure coherence in text. Most recent work uses direct supervision on the\ntask; we show that by simply finetuning a RoBERTa model, we can achieve a near\nperfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding\nperformance is unlikely to lead to a good model of text coherence, and suggest\nthat the Shuffle Test should be approached in a Zero-Shot setting: models\nshould be evaluated without being trained on the task itself. We evaluate\ncommon models in this setting, such as Generative and Bi-directional\nTransformers, and find that larger architectures achieve high-performance\nout-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of\nthe original by increasing the size of blocks shuffled. Even though human\nreader performance remains high (around 95% accuracy), model performance drops\nfrom 94% to 78% as block size increases, creating a conceptually simple\nchallenge to benchmark NLP models. Code available:\nhttps://github.com/tingofurro/shuffle_test/\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:19:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Laban", "Philippe", ""], ["Dai", "Luke", ""], ["Bandarkar", "Lucas", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2107.03450", "submitter": "Jean-Baptiste Camps", "authors": "Jean-Baptiste Camps and Chahan Vidal-Gor\\`ene and Marguerite Vernet", "title": "Handling Heavily Abbreviated Manuscripts: HTR engines vs text\n  normalisation approaches", "comments": "Accompanying data available at: https://zenodo.org/record/5071964", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Although abbreviations are fairly common in handwritten sources, particularly\nin medieval and modern Western manuscripts, previous research dealing with\ncomputational approaches to their expansion is scarce. Yet abbreviations\npresent particular challenges to computational approaches such as handwritten\ntext recognition and natural language processing tasks. Often, pre-processing\nultimately aims to lead from a digitised image of the source to a normalised\ntext, which includes expansion of the abbreviations. We explore different\nsetups to obtain such a normalised text, either directly, by training HTR\nengines on normalised (i.e., expanded, disabbreviated) text, or by decomposing\nthe process into discrete steps, each making use of specialist models for\nrecognition, word segmentation and normalisation. The case studies considered\nhere are drawn from the medieval Latin tradition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:23:22 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Camps", "Jean-Baptiste", ""], ["Vidal-Gor\u00e8ne", "Chahan", ""], ["Vernet", "Marguerite", ""]]}, {"id": "2107.03451", "submitter": "Emily Dinan", "authors": "Emily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon Spruit,\n  Dirk Hovy, Y-Lan Boureau, Verena Rieser", "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and\n  Tooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last several years, end-to-end neural conversational agents have\nvastly improved in their ability to carry a chit-chat conversation with humans.\nHowever, these models are often trained on large datasets from the internet,\nand as a result, may learn undesirable behaviors from this data, such as toxic\nor otherwise harmful language. Researchers must thus wrestle with the issue of\nhow and when to release these models. In this paper, we survey the problem\nlandscape for safety for end-to-end conversational AI and discuss recent and\nrelated work. We highlight tensions between values, potential positive impact\nand potential harms, and provide a framework for making decisions about whether\nand how to release these models, following the tenets of value-sensitive\ndesign. We additionally provide a suite of tools to enable researchers to make\nbetter-informed decisions about training and releasing end-to-end\nconversational AI models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:25:57 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 15:37:52 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 15:05:22 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Dinan", "Emily", ""], ["Abercrombie", "Gavin", ""], ["Bergman", "A. Stevie", ""], ["Spruit", "Shannon", ""], ["Hovy", "Dirk", ""], ["Boureau", "Y-Lan", ""], ["Rieser", "Verena", ""]]}, {"id": "2107.03466", "submitter": "Bennett Kleinberg", "authors": "Maximilian Mozes, Isabelle van der Vegt, Bennett Kleinberg", "title": "Worry, coping and resignation -- A repeated-measures study on emotional\n  responses after a year in the pandemic", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of COVID-19 lockdown measures and an outlook on return to\nnormality are demanding societal changes. Among the most pressing questions is\nhow individuals adjust to the pandemic. This paper examines the emotional\nresponses to the pandemic in a repeated-measures design. Data (n=1698) were\ncollected in April 2020 (during strict lockdown measures) and in April 2021\n(when vaccination programmes gained traction). We asked participants to report\ntheir emotions and express these in text data. Statistical tests revealed an\naverage trend towards better adjustment to the pandemic. However, clustering\nanalyses suggested a more complex heterogeneous pattern with a well-coping and\na resigning subgroup of participants. Linguistic computational analyses\nuncovered that topics and n-gram frequencies shifted towards attention to the\nvaccination programme and away from general worrying. Implications for public\nmental health efforts in identifying people at heightened risk are discussed.\nThe dataset is made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 20:20:10 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Mozes", "Maximilian", ""], ["van der Vegt", "Isabelle", ""], ["Kleinberg", "Bennett", ""]]}, {"id": "2107.03529", "submitter": "Dhara Shah", "authors": "Bhashithe Abeysinghe, Dhara Shah, Chris Freas, Robert Harrison,\n  Rajshekhar Sunderraman", "title": "POSLAN: Disentangling Chat with Positional and Language encoded Post\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most online message threads inherently will be cluttered and any new user or\nan existing user visiting after a hiatus will have a difficult time\nunderstanding whats being discussed in the thread. Similarly cluttered\nresponses in a message thread makes analyzing the messages a difficult problem.\nThe need for disentangling the clutter is much higher when the platform where\nthe discussion is taking place does not provide functions to retrieve reply\nrelations of the messages. This introduces an interesting problem to which\n\\cite{wang2011learning} phrases as a structural learning problem. We create\nvector embeddings for posts in a thread so that it captures both linguistic and\npositional features in relation to a context of where a given message is in.\nUsing these embeddings for posts we compute a similarity based connectivity\nmatrix which then converted into a graph. After employing a pruning mechanisms\nthe resultant graph can be used to discover the reply relation for the posts in\nthe thread. The process of discovering or disentangling chat is kept as an\nunsupervised mechanism. We present our experimental results on a data set\nobtained from Telegram with limited meta data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 23:32:17 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Abeysinghe", "Bhashithe", ""], ["Shah", "Dhara", ""], ["Freas", "Chris", ""], ["Harrison", "Robert", ""], ["Sunderraman", "Rajshekhar", ""]]}, {"id": "2107.03625", "submitter": "Yves Bestgen", "authors": "Yves Bestgen", "title": "Using CollGram to Compare Formulaic Language in Human and Neural Machine\n  Translation", "comments": "Accepted at Translation and Interpreting Technology Online - TRITON\n  2021, two figures added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 06:30:35 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 09:59:18 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Bestgen", "Yves", ""]]}, {"id": "2107.03675", "submitter": "Ke Shi", "authors": "Huayun Zhang, Ke Shi, Nancy F. Chen", "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil", "comments": "Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech evaluation is an essential component in computer-assisted language\nlearning (CALL). While speech evaluation on English has been popular, automatic\nspeech scoring on low resource languages remains challenging. Work in this area\nhas focused on monolingual specific designs and handcrafted features stemming\nfrom resource-rich languages like English. Such approaches are often difficult\nto generalize to other languages, especially if we also want to consider\nsuprasegmental qualities such as rhythm. In this work, we examine three\ndifferent languages that possess distinct rhythm patterns: English\n(stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit\nrobust feature representations inspired by music processing and vector\nrepresentation learning. Empirical validations show consistent gains for all\nthree languages when predicting pronunciation, rhythm and intonation\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 08:36:51 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhang", "Huayun", ""], ["Shi", "Ke", ""], ["Chen", "Nancy F.", ""]]}, {"id": "2107.03760", "submitter": "Vivek Srivastava", "authors": "Vivek Srivastava, Mayank Singh", "title": "HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish\n  Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text generation is a highly active area of research in the computational\nlinguistic community. The evaluation of the generated text is a challenging\ntask and multiple theories and metrics have been proposed over the years.\nUnfortunately, text generation and evaluation are relatively understudied due\nto the scarcity of high-quality resources in code-mixed languages where the\nwords and phrases from multiple languages are mixed in a single utterance of\ntext and speech. To address this challenge, we present a corpus (HinGE) for a\nwidely popular code-mixed language Hinglish (code-mixing of Hindi and English\nlanguages). HinGE has Hinglish sentences generated by humans as well as two\nrule-based algorithms corresponding to the parallel Hindi-English sentences. In\naddition, we demonstrate the inefficacy of widely-used evaluation metrics on\nthe code-mixed data. The HinGE dataset will facilitate the progress of natural\nlanguage generation research in code-mixed languages.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:11:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Srivastava", "Vivek", ""], ["Singh", "Mayank", ""]]}, {"id": "2107.03809", "submitter": "Mateusz Klimaszewski", "authors": "Mateusz Klimaszewski, Alina Wr\\'oblewska", "title": "COMBO: a new module for EUD parsing", "comments": "Accepted at IWPT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the COMBO-based approach for EUD parsing and its implementation,\nwhich took part in the IWPT 2021 EUD shared task. The goal of this task is to\nparse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The\nproposed approach uses COMBO to predict UD trees and EUD graphs. These\nstructures are then merged into the final EUD graphs. Some EUD edge labels are\nextended with case information using a single language-independent expansion\nrule. In the official evaluation, the solution ranked fourth, achieving an\naverage ELAS of 83.79%. The source code is available at\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:34:25 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Klimaszewski", "Mateusz", ""], ["Wr\u00f3blewska", "Alina", ""]]}, {"id": "2107.03844", "submitter": "Firoj Alam", "authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin,\n  Naira Khan, Shammur Absar Chowdhury", "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models", "comments": "Under Review, Bangla language processing, text classification,\n  sequence tagging, datasets, benchmarks, transformer models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:49:46 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 06:43:33 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 05:41:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Alam", "Firoj", ""], ["Hasan", "Arid", ""], ["Alam", "Tanvirul", ""], ["Khan", "Akib", ""], ["Tajrin", "Janntatul", ""], ["Khan", "Naira", ""], ["Chowdhury", "Shammur Absar", ""]]}, {"id": "2107.03884", "submitter": "Aadesh Gupta", "authors": "Aadesh Gupta, Kaustubh D.Dhole, Rahul Tarway, Swetha Prabhakar, Ashish\n  Shrivastava", "title": "CANDLE: Decomposing Conditional and Conjunctive Queries for\n  Task-Oriented Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain-specific dialogue systems generally determine user intents by relying\non sentence-level classifiers which mainly focus on single action sentences.\nSuch classifiers are not designed to effectively handle complex queries\ncomposed of conditional and sequential clauses that represent multiple actions.\nWe attempt to decompose such queries into smaller single-action sub-queries\nthat are reasonable for intent classifiers to understand in a dialogue\npipeline. We release CANDLE (Conditional & AND type Expressions), a dataset\nconsisting of 3124 utterances manually tagged with conditional and sequential\nlabels and demonstrates this decomposition by training two baseline taggers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:07:11 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Gupta", "Aadesh", ""], ["Dhole", "Kaustubh D.", ""], ["Tarway", "Rahul", ""], ["Prabhakar", "Swetha", ""], ["Shrivastava", "Ashish", ""]]}, {"id": "2107.03950", "submitter": "Yu-Ying Chuang", "authors": "Yu-Ying Chuang, Mihi Kang, Xuefeng Luo, R. Harald Baayen", "title": "Vector Space Morphology with Linear Discriminative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents three case studies of modeling aspects of lexical\nprocessing with Linear Discriminative Learning (LDL), the computational engine\nof the Discriminative Lexicon model (Baayen et al., 2019). With numeric\nrepresentations of word forms and meanings, LDL learns to map one vector space\nonto the other, without being informed about any morphological structure or\ninflectional classes. The modeling results demonstrated that LDL not only\nperforms well for understanding and producing morphologically complex words,\nbut also generates quantitative measures that are predictive for human\nbehavioral data. LDL models are straightforward to implement with the JudiLing\npackage (Luo et al., 2021). Worked examples are provided for three modeling\nchallenges: producing and understanding Korean verb inflection, predicting\nprimed Dutch lexical decision latencies, and predicting the acoustic duration\nof Mandarin words.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:22:50 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chuang", "Yu-Ying", ""], ["Kang", "Mihi", ""], ["Luo", "Xuefeng", ""], ["Baayen", "R. Harald", ""]]}, {"id": "2107.03959", "submitter": "Jason R.C. Nurse Dr", "authors": "Rahime Belen Saglam and Jason R.C. Nurse and Duncan Hodges", "title": "Privacy Concerns in Chatbot Interactions: When to Trust and When to\n  Worry", "comments": null, "journal-ref": "23rd International Conference on Human-Computer Interaction (HCII\n  2021)", "doi": "10.1007/978-3-030-78642-7_53", "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through advances in their conversational abilities, chatbots have started to\nrequest and process an increasing variety of sensitive personal information.\nThe accurate disclosure of sensitive information is essential where it is used\nto provide advice and support to users in the healthcare and finance sectors.\nIn this study, we explore users' concerns regarding factors associated with the\nuse of sensitive data by chatbot providers. We surveyed a representative sample\nof 491 British citizens. Our results show that the user concerns focus on\ndeleting personal information and concerns about their data's inappropriate\nuse. We also identified that individuals were concerned about losing control\nover their data after a conversation with conversational agents. We found no\neffect from a user's gender or education but did find an effect from the user's\nage, with those over 45 being more concerned than those under 45. We also\nconsidered the factors that engender trust in a chatbot. Our respondents'\nprimary focus was on the chatbot's technical elements, with factors such as the\nresponse quality being identified as the most critical factor. We again found\nno effect from the user's gender or education level; however, when we\nconsidered some social factors (e.g. avatars or perceived 'friendliness'), we\nfound those under 45 years old rated these as more important than those over\n45. The paper concludes with a discussion of these results within the context\nof designing inclusive, digital systems that support a wide range of users.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:31:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Saglam", "Rahime Belen", ""], ["Nurse", "Jason R. C.", ""], ["Hodges", "Duncan", ""]]}, {"id": "2107.04007", "submitter": "Melissa Roemmele", "authors": "Melissa Roemmele", "title": "Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing", "comments": "Accepted at ICCC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Getting machines to generate text perceived as creative is a long-pursued\ngoal. A growing body of research directs this goal towards augmenting the\ncreative writing abilities of human authors. In this paper, we pursue this\nobjective by analyzing how observing examples of automatically generated text\ninfluences writing. In particular, we examine a task referred to as sentence\ninfilling, which involves transforming a list of words into a complete\nsentence. We emphasize \"storiability\" as a desirable feature of the resulting\nsentences, where \"storiable\" sentences are those that suggest a story a reader\nwould be curious to hear about. Both humans and an automated system (based on a\nneural language model) performed this sentence infilling task. In one setting,\npeople wrote sentences on their own; in a different setting, people observed\nthe sentences produced by the model while writing their own sentences. Readers\nthen assigned storiability preferences to the resulting sentences in a\nsubsequent evaluation. We find that human-authored sentences were judged as\nmore storiable when authors observed the generated examples, and that\nstoriability increased as authors derived more semantic content from the\nexamples. This result gives evidence of an \"inspiration through observation\"\nparadigm for human-computer collaborative writing, through which human writing\ncan be enhanced by text generation models without directly copying their\noutput.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:53:22 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Roemmele", "Melissa", ""]]}, {"id": "2107.04011", "submitter": "Jawad Haqbeen", "authors": "J. Haqbeen, T. Ito, S. Sahab, R. Hadfi, T. Sato, S. Okuhara", "title": "Meeting the SDGs : Enabling the Goals by Cooperation with Crowd using a\n  Conversational AI Platform", "comments": "7 pages, 6 figures, 1 table, To appear as a conference paper at KICSS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we report about a large-scale online discussion with 1099\ncitizens on the Afghanistan Sustainable Development Goals.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 04:14:19 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Haqbeen", "J.", ""], ["Ito", "T.", ""], ["Sahab", "S.", ""], ["Hadfi", "R.", ""], ["Sato", "T.", ""], ["Okuhara", "S.", ""]]}, {"id": "2107.04082", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Diptanu Gon Choudhury, Frank Zhang, Kritika Singh,\n  Alexei Baevski, Assaf Sela, Yatharth Saraf, Michael Auli", "title": "Improved Language Identification Through Cross-Lingual Self-Supervised\n  Learning", "comments": "Submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 25 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n93% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 19:37:06 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 03:24:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tjandra", "Andros", ""], ["Choudhury", "Diptanu Gon", ""], ["Zhang", "Frank", ""], ["Singh", "Kritika", ""], ["Baevski", "Alexei", ""], ["Sela", "Assaf", ""], ["Saraf", "Yatharth", ""], ["Auli", "Michael", ""]]}, {"id": "2107.04132", "submitter": "Peter Jansen", "authors": "Peter A Jansen", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language\n  Environments", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text Worlds are virtual environments for embodied agents that, unlike 2D or\n3D environments, are rendered exclusively using textual descriptions. These\nenvironments offer an alternative to higher-fidelity 3D environments due to\ntheir low barrier to entry, providing the ability to study semantics,\ncompositional inference, and other high-level tasks with rich high-level action\nspaces while controlling for perceptual input. This systematic survey outlines\nrecent developments in tooling, environments, and agent modeling for Text\nWorlds, while examining recent trends in knowledge graphs, common sense\nreasoning, transfer learning of Text World performance to higher-fidelity\nenvironments, as well as near-term development targets that, once achieved,\nmake Text Worlds an attractive general research paradigm for natural language\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 22:15:16 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Jansen", "Peter A", ""]]}, {"id": "2107.04152", "submitter": "Han He", "authors": "Han He, Jinho D. Choi", "title": "Levi Graph AMR Parser using Heterogeneous Attention", "comments": "Accepted in IWPT 2021: The 17th International Conference on Parsing\n  Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coupled with biaffine decoders, transformers have been effectively adapted to\ntext-to-graph transduction and achieved state-of-the-art performance on AMR\nparsing. Many prior works, however, rely on the biaffine decoder for either or\nboth arc and label predictions although most features used by the decoder may\nbe learned by the transformer already. This paper presents a novel approach to\nAMR parsing by combining heterogeneous data (tokens, concepts, labels) as one\ninput to a transformer to learn attention, and use only attention matrices from\nthe transformer to predict all elements in AMR graphs (concepts, arcs, labels).\nAlthough our models use significantly fewer parameters than the previous\nstate-of-the-art graph parser, they show similar or better accuracy on AMR 2.0\nand 3.0.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 00:06:17 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["He", "Han", ""], ["Choi", "Jinho D.", ""]]}, {"id": "2107.04212", "submitter": "Zi Lin", "authors": "Ian D. Kivlichan, Zi Lin, Jeremiah Liu, Lucy Vasserman", "title": "Measuring and Improving Model-Moderator Collaboration using Uncertainty\n  Estimation", "comments": "WOAH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content moderation is often performed by a collaboration between humans and\nmachine learning models. However, it is not well understood how to design the\ncollaborative process so as to maximize the combined moderator-model system\nperformance. This work presents a rigorous study of this problem, focusing on\nan approach that incorporates model uncertainty into the collaborative process.\nFirst, we introduce principled metrics to describe the performance of the\ncollaborative system under capacity constraints on the human moderator,\nquantifying how efficiently the combined system utilizes human decisions. Using\nthese metrics, we conduct a large benchmark study evaluating the performance of\nstate-of-the-art uncertainty models under different collaborative review\nstrategies. We find that an uncertainty-based strategy consistently outperforms\nthe widely used strategy based on toxicity scores, and moreover that the choice\nof review strategy drastically changes the overall system performance. Our\nresults demonstrate the importance of rigorous metrics for understanding and\ndeveloping effective moderator-model systems for content moderation, as well as\nthe utility of uncertainty estimation in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 05:07:25 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kivlichan", "Ian D.", ""], ["Lin", "Zi", ""], ["Liu", "Jeremiah", ""], ["Vasserman", "Lucy", ""]]}, {"id": "2107.04217", "submitter": "Zeyu Zhang", "authors": "Zeyu Zhang, Thuy Vu, and Alessandro Moschitti", "title": "Joint Models for Answer Verification in Question Answering Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 05:34:36 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Zeyu", ""], ["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2107.04239", "submitter": "Rui Wang", "authors": "Rui Wang and Xu Tan and Renqian Luo and Tao Qin and Tie-Yan Liu", "title": "A Survey on Low-Resource Neural Machine Translation", "comments": "A short version has been submitted to IJCAI2021 Survey Track on Feb.\n  26th, 2021, accepted on Apr. 16th, 2021. 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural approaches have achieved state-of-the-art accuracy on machine\ntranslation but suffer from the high cost of collecting large scale parallel\ndata. Thus, a lot of research has been conducted for neural machine translation\n(NMT) with very limited parallel data, i.e., the low-resource setting. In this\npaper, we provide a survey for low-resource NMT and classify related works into\nthree categories according to the auxiliary data they used: (1) exploiting\nmonolingual data of source and/or target languages, (2) exploiting data from\nauxiliary languages, and (3) exploiting multi-modal data. We hope that our\nsurvey can help researchers to better understand this field and inspire them to\ndesign better algorithms, and help industry practitioners to choose appropriate\nalgorithms for their applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:26:38 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Rui", ""], ["Tan", "Xu", ""], ["Luo", "Renqian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2107.04268", "submitter": "Xinying Qiu", "authors": "Xinying Qiu, Yuan Chen, Hanwu Chen, Jian-Yun Nie, Yuming Shen, Dawei\n  Lu", "title": "Learning Syntactic Dense Embedding with Correlation Graph for Automatic\n  Readability Assessment", "comments": "Accepted to the 59th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models for automatic readability assessment generally discard\nlinguistic features traditionally used in machine learning models for the task.\nWe propose to incorporate linguistic features into neural network models by\nlearning syntactic dense embeddings based on linguistic features. To cope with\nthe relationships between the features, we form a correlation graph among\nfeatures and use it to learn their embeddings so that similar features will be\nrepresented by similar embeddings. Experiments with six data sets of two\nproficiency levels demonstrate that our proposed methodology can complement\nBERT-only model to achieve significantly better performances for automatic\nreadability assessment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:26:17 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Qiu", "Xinying", ""], ["Chen", "Yuan", ""], ["Chen", "Hanwu", ""], ["Nie", "Jian-Yun", ""], ["Shen", "Yuming", ""], ["Lu", "Dawei", ""]]}, {"id": "2107.04292", "submitter": "Yijun Wang", "authors": "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li, and Junchi Yan", "title": "UniRE: A Unified Label Space for Entity Relation Extraction", "comments": "ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many joint entity relation extraction models setup two separated label spaces\nfor the two sub-tasks (i.e., entity detection and relation classification). We\nargue that this setting may hinder the information interaction between entities\nand relations. In this work, we propose to eliminate the different treatment on\nthe two sub-tasks' label spaces. The input of our model is a table containing\nall word pairs from a sentence. Entities and relations are represented by\nsquares and rectangles in the table. We apply a unified classifier to predict\neach cell's label, which unifies the learning of two sub-tasks. For testing, an\neffective (yet fast) approximate decoder is proposed for finding squares and\nrectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)\nshow that, using only half the number of parameters, our model achieves\ncompetitive accuracy with the best extractor, and is faster.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 08:09:37 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Yijun", ""], ["Sun", "Changzhi", ""], ["Wu", "Yuanbin", ""], ["Zhou", "Hao", ""], ["Li", "Lei", ""], ["Yan", "Junchi", ""]]}, {"id": "2107.04372", "submitter": "Rolandos Alexandros Potamias", "authors": "Rolandos Alexandros Potamias and Georgios Siolas and Andreas -\n  Georgios Stafylopatis", "title": "A Robust Deep Ensemble Classifier for Figurative Language Detection", "comments": "Published in Engineering Applications of Neural Networks (EANN)-2019", "journal-ref": null, "doi": "10.1007/978-3-030-20257-6_14", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition and classification of Figurative Language (FL) is an open problem\nof Sentiment Analysis in the broader field of Natural Language Processing (NLP)\ndue to the contradictory meaning contained in phrases with metaphorical\ncontent. The problem itself contains three interrelated FL recognition tasks:\nsarcasm, irony and metaphor which, in the present paper, are dealt with\nadvanced Deep Learning (DL) techniques. First, we introduce a data\nprepossessing framework towards efficient data representation formats so that\nto optimize the respective inputs to the DL models. In addition, special\nfeatures are extracted in order to characterize the syntactic, expressive,\nemotional and temper content reflected in the respective social media text\nreferences. These features aim to capture aspects of the social network user's\nwriting method. Finally, features are fed to a robust, Deep Ensemble Soft\nClassifier (DESC) which is based on the combination of different DL techniques.\nUsing three different benchmark datasets (one of them containing various FL\nforms) we conclude that the DESC model achieves a very good performance, worthy\nof comparison with relevant methodologies and state-of-the-art technologies in\nthe challenging field of FL recognition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 11:26:37 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Potamias", "Rolandos Alexandros", ""], ["Siolas", "Georgios", ""], ["Stafylopatis", "Andreas - Georgios", ""]]}, {"id": "2107.04374", "submitter": "Usman Naseem", "authors": "Usman Naseem, Adam G. Dunn, Matloob Khushi, Jinman Kim", "title": "Benchmarking for Biomedical Natural Language Processing Tasks with a\n  Domain Specific ALBERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of biomedical text data and advances in natural language\nprocessing (NLP) have made new applications in biomedical NLP possible.\nLanguage models trained or fine tuned using domain specific corpora can\noutperform general models, but work to date in biomedical NLP has been limited\nin terms of corpora and tasks. We present BioALBERT, a domain-specific\nadaptation of A Lite Bidirectional Encoder Representations from Transformers\n(ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical\n(MIMIC-III) corpora and fine tuned for 6 different tasks across 20 benchmark\ndatasets. Experiments show that BioALBERT outperforms the state of the art on\nnamed entity recognition (+11.09% BLURB score improvement), relation extraction\n(+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document\nclassification (+0.62% F1-score), and question answering (+2.83% BLURB score).\nIt represents a new state of the art in 17 out of 20 benchmark datasets. By\nmaking BioALBERT models and data available, our aim is to help the biomedical\nNLP community avoid computational costs of training and establish a new set of\nbaselines for future efforts across a broad range of biomedical NLP tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 11:47:13 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Naseem", "Usman", ""], ["Dunn", "Adam G.", ""], ["Khushi", "Matloob", ""], ["Kim", "Jinman", ""]]}, {"id": "2107.04512", "submitter": "Scott Roy", "authors": "Scott Roy, Cliff Brunk, Kyu-Young Kim, Justin Zhao, Markus Freitag,\n  Mihir Kale, Gagan Bansal, Sidharth Mudgal, Chris Varano", "title": "Using Machine Translation to Localize Task Oriented NLG Output", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in a task oriented natural language application like\nthe Google Assistant, Siri, or Alexa is to localize the output to many\nlanguages. This paper explores doing this by applying machine translation to\nthe English output. Using machine translation is very scalable, as it can work\nwith any English output and can handle dynamic text, but otherwise the problem\nis a poor fit. The required quality bar is close to perfection, the range of\nsentences is extremely narrow, and the sentences are often very different than\nthe ones in the machine translation training data. This combination of\nrequirements is novel in the field of domain adaptation for machine\ntranslation. We are able to reach the required quality bar by building on\nexisting ideas and adding new ones: finetuning on in-domain translations,\nadding sentences from the Web, adding semantic annotations, and using automatic\nerror detection. The paper shares our approach and results, together with a\ndistillation model to serve the translation models at scale.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:56:45 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Roy", "Scott", ""], ["Brunk", "Cliff", ""], ["Kim", "Kyu-Young", ""], ["Zhao", "Justin", ""], ["Freitag", "Markus", ""], ["Kale", "Mihir", ""], ["Bansal", "Gagan", ""], ["Mudgal", "Sidharth", ""], ["Varano", "Chris", ""]]}, {"id": "2107.04553", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer", "title": "Can Deep Neural Networks Predict Data Correlations from Column Names?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For humans, it is often possible to predict data correlations from column\nnames. We conduct experiments to find out whether deep neural networks can\nlearn to do the same. If so, e.g., it would open up the possibility of tuning\ntools that use NLP analysis on schema elements to prioritize their efforts for\ncorrelation detection.\n  We analyze correlations for around 120,000 column pairs, taken from around\n4,000 data sets. We try to predict correlations, based on column names alone.\nFor predictions, we exploit pre-trained language models, based on the recently\nproposed Transformer architecture. We consider different types of correlations,\nmultiple prediction methods, and various prediction scenarios. We study the\nimpact of factors such as column name length or the amount of training data on\nprediction accuracy. Altogether, we find that deep neural networks can predict\ncorrelations with a relatively high accuracy in many scenarios (e.g., with an\naccuracy of 95% for long column names).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:11:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Trummer", "Immanuel", ""]]}, {"id": "2107.04677", "submitter": "Dilin Wang", "authors": "Dilin Wang, Yuan Shangguan, Haichuan Yang, Pierce Chuang, Jiatong\n  Zhou, Meng Li, Ganesh Venkatesh, Ozlem Kalinli, Vikas Chandra", "title": "Noisy Training Improves E2E ASR for the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic speech recognition (ASR) has become increasingly ubiquitous on\nmodern edge devices. Past work developed streaming End-to-End (E2E) all-neural\nspeech recognizers that can run compactly on edge devices. However, E2E ASR\nmodels are prone to overfitting and have difficulties in generalizing to unseen\ntesting data. Various techniques have been proposed to regularize the training\nof ASR models, including layer normalization, dropout, spectrum data\naugmentation and speed distortions in the inputs. In this work, we present a\nsimple yet effective noisy training strategy to further improve the E2E ASR\nmodel training. By introducing random noise to the parameter space during\ntraining, our method can produce smoother models at convergence that generalize\nbetter. We apply noisy training to improve both dense and sparse\nstate-of-the-art Emformer models and observe consistent WER reduction.\nSpecifically, when training Emformers with 90% sparsity, we achieve 12% and 14%\nWER improvements on the LibriSpeech Test-other and Test-clean data set,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:56:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Dilin", ""], ["Shangguan", "Yuan", ""], ["Yang", "Haichuan", ""], ["Chuang", "Pierce", ""], ["Zhou", "Jiatong", ""], ["Li", "Meng", ""], ["Venkatesh", "Ganesh", ""], ["Kalinli", "Ozlem", ""], ["Chandra", "Vikas", ""]]}, {"id": "2107.04691", "submitter": "Vatsal Raina", "authors": "Vatsal Raina, Mark J.F. Gales", "title": "An Initial Investigation of Non-Native Spoken Question-Answering", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-based machine comprehension (MC) systems have a wide-range of\napplications, and standard corpora exist for developing and evaluating\napproaches. There has been far less research on spoken question answering (SQA)\nsystems. The SQA task considered in this paper is to extract the answer from a\ncandidate$\\text{'}$s spoken response to a question in a prompt-response style\nlanguage assessment test. Applying these MC approaches to this SQA task rather\nthan, for example, off-topic response detection provides far more detailed\ninformation that can be used for further downstream processing. One significant\nchallenge is the lack of appropriately annotated speech corpora to train\nsystems for this task. Hence, a transfer-learning style approach is adopted\nwhere a system trained on text-based MC is evaluated on an SQA task with\nnon-native speakers. Mismatches must be considered between text documents and\nspoken responses; non-native spoken grammar and written grammar. In practical\nSQA, ASR systems are used, necessitating an investigation of the impact of ASR\nerrors. We show that a simple text-based ELECTRA MC model trained on SQuAD2.0\ntransfers well for SQA. It is found that there is an approximately linear\nrelationship between ASR errors and the SQA assessment scores but grammar\nmismatches have minimal impact.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 21:59:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Raina", "Vatsal", ""], ["Gales", "Mark J. F.", ""]]}, {"id": "2107.04734", "submitter": "Ankita Pasad", "authors": "Ankita Pasad, Ju-Chieh Chou, Karen Livescu", "title": "Layer-wise Analysis of a Self-supervised Speech Representation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 02:13:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pasad", "Ankita", ""], ["Chou", "Ju-Chieh", ""], ["Livescu", "Karen", ""]]}, {"id": "2107.04736", "submitter": "Shrey Desai", "authors": "Shrey Desai, Akshat Shrivastava, Justin Rill, Brian Moran, Safiyyah\n  Saleem, Alexander Zotov, Ahmed Aly", "title": "Assessing Data Efficiency in Task-Oriented Semantic Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data efficiency, despite being an attractive characteristic, is often\nchallenging to measure and optimize for in task-oriented semantic parsing;\nunlike exact match, it can require both model- and domain-specific setups,\nwhich have, historically, varied widely across experiments. In our work, as a\nstep towards providing a unified solution to data-efficiency-related questions,\nwe introduce a four-stage protocol which gives an approximate measure of how\nmuch in-domain, \"target\" data a parser requires to achieve a certain quality\nbar. Specifically, our protocol consists of (1) sampling target subsets of\ndifferent cardinalities, (2) fine-tuning parsers on each subset, (3) obtaining\na smooth curve relating target subset (%) vs. exact match (%), and (4)\nreferencing the curve to mine ad-hoc (target subset, exact match) points. We\napply our protocol in two real-world case studies -- model generalizability and\nintent complexity -- illustrating its flexibility and applicability to\npractitioners in task-oriented semantic parsing.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 02:43:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Desai", "Shrey", ""], ["Shrivastava", "Akshat", ""], ["Rill", "Justin", ""], ["Moran", "Brian", ""], ["Saleem", "Safiyyah", ""], ["Zotov", "Alexander", ""], ["Aly", "Ahmed", ""]]}, {"id": "2107.04781", "submitter": "Bryar Hassan Dr.", "authors": "Bryar A. Hassan, Tarik A. Rashid and Seyedali Mirjalili", "title": "Formal context reduction in deriving concept hierarchies from corpora\n  using adaptive evolutionary clustering algorithm star", "comments": "Complex Intell. Syst. (2021)", "journal-ref": null, "doi": "10.1007/s40747-021-00422-w", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is beneficial to automate the process of deriving concept hierarchies from\ncorpora since a manual construction of concept hierarchies is typically a\ntime-consuming and resource-intensive process. As such, the overall process of\nlearning concept hierarchies from corpora encompasses a set of steps: parsing\nthe text into sentences, splitting the sentences and then tokenising it. After\nthe lemmatisation step, the pairs are extracted using FCA. However, there might\nbe some uninteresting and erroneous pairs in the formal context. Generating\nformal context may lead to a time-consuming process, so formal context size\nreduction is required to remove uninterested and erroneous pairs, taking less\ntime to extract the concept lattice and concept hierarchies accordingly. In\nthis premise, this study aims to propose two frameworks: (1) A framework to\nreview the current process of deriving concept hierarchies from corpus\nutilising FCA; (2) A framework to decrease the formal contexts ambiguity of the\nfirst framework using an adaptive version of ECA*. Experiments are conducted by\napplying 385 sample corpora from Wikipedia on the two frameworks to examine the\nreducing size of formal context, which leads to yield concept lattice and\nconcept hierarchy. The resulting lattice of formal context is evaluated to the\nstandard one using concept lattice-invariants. Accordingly, the homomorphic\nbetween the two lattices preserves the quality of resulting concept hierarchies\nby 89% in contrast to the basic ones, and the reduced concept lattice inherits\nthe structural relation of the standard one. The adaptive ECA* is examined\nagainst its four counterpart baseline algorithms to measure the execution time\non random datasets with different densities (fill ratios). The results show\nthat adaptive ECA* performs concept lattice faster than other mentioned\ncompetitive techniques in different fill ratios.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 07:18:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hassan", "Bryar A.", ""], ["Rashid", "Tarik A.", ""], ["Mirjalili", "Seyedali", ""]]}, {"id": "2107.04835", "submitter": "Hang Hua", "authors": "Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, Jiebo Luo", "title": "Noise Stability Regularization for Improving BERT Fine-tuning", "comments": "Proceedings of the 2021 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-tuning pre-trained language models such as BERT has become a common\npractice dominating leaderboards across various NLP tasks. Despite its recent\nsuccess and wide adoption, this process is unstable when there are only a small\nnumber of training samples available. The brittleness of this process is often\nreflected by the sensitivity to random seeds. In this paper, we propose to\ntackle this problem based on the noise stability property of deep nets, which\nis investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020).\nSpecifically, we introduce a novel and effective regularization method to\nimprove fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability\nRegularization (LNSR). We extend the theories about adding noise to the input\nand prove that our method gives a stabler regularization effect. We provide\nsupportive evidence by experimentally confirming that well-performing models\nshow a low sensitivity to noise and fine-tuning with LNSR exhibits clearly\nhigher generalizability and stability. Furthermore, our method also\ndemonstrates advantages over other state-of-the-art algorithms including L2-SP\n(Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 13:19:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hua", "Hang", ""], ["Li", "Xingjian", ""], ["Dou", "Dejing", ""], ["Xu", "Cheng-Zhong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2107.04880", "submitter": "Gaochen Wu", "authors": "Gaochen Wu, Bin Xu, Yuxin Qin, Fei Kong, Bangchang Liu, Hongwen Zhao,\n  Dejie Chang", "title": "PatentMiner: Patent Vacancy Mining via Context-enhanced and\n  Knowledge-guided Graph Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although there are a small number of work to conduct patent research by\nbuilding knowledge graph, but without constructing patent knowledge graph using\npatent documents and combining latest natural language processing methods to\nmine hidden rich semantic relationships in existing patents and predict new\npossible patents. In this paper, we propose a new patent vacancy prediction\napproach named PatentMiner to mine rich semantic knowledge and predict new\npotential patents based on knowledge graph (KG) and graph attention mechanism.\nFirstly, patent knowledge graph over time (e.g. year) is constructed by\ncarrying out named entity recognition and relation extrac-tion from patent\ndocuments. Secondly, Common Neighbor Method (CNM), Graph Attention Networks\n(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to\nperform link prediction in the constructed knowledge graph to dig out the\npotential triples. Finally, patents are defined on the knowledge graph by means\nof co-occurrence relationship, that is, each patent is represented as a fully\nconnected subgraph containing all its entities and co-occurrence relationships\nof the patent in the knowledge graph; Furthermore, we propose a new patent\nprediction task which predicts a fully connected subgraph with newly added\nprediction links as a new pa-tent. The experimental results demonstrate that\nour proposed patent predic-tion approach can correctly predict new patents and\nContext-enhanced Graph Attention Networks is much better than the baseline.\nMeanwhile, our proposed patent vacancy prediction task still has significant\nroom to im-prove.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 17:34:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wu", "Gaochen", ""], ["Xu", "Bin", ""], ["Qin", "Yuxin", ""], ["Kong", "Fei", ""], ["Liu", "Bangchang", ""], ["Zhao", "Hongwen", ""], ["Chang", "Dejie", ""]]}, {"id": "2107.04929", "submitter": "Peter Sheridan Dodds", "authors": "E. Davis, C. M. Danforth, W. Mieder, and P. S. Dodds", "title": "Computational Paremiology: Charting the temporal, ecological dynamics of\n  proverb use in books, news articles, and tweets", "comments": "Main paper: 16 pages, 9 figures, 1 table; Supplementary: 5 pages, 4\n  tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proverbs are an essential component of language and culture, and though much\nattention has been paid to their history and currency, there has been\ncomparatively little quantitative work on changes in the frequency with which\nthey are used over time. With wider availability of large corpora reflecting\nmany diverse genres of documents, it is now possible to take a broad and\ndynamic view of the importance of the proverb. Here, we measure temporal\nchanges in the relevance of proverbs within three corpora, differing in kind,\nscale, and time frame: Millions of books over centuries; hundreds of millions\nof news articles over twenty years; and billions of tweets over a decade. We\nfind that proverbs present heavy-tailed frequency-of-usage rank distributions\nin each venue; exhibit trends reflecting the cultural dynamics of the eras\ncovered; and have evolved into contemporary forms on social media.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 23:41:01 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Davis", "E.", ""], ["Danforth", "C. M.", ""], ["Mieder", "W.", ""], ["Dodds", "P. S.", ""]]}, {"id": "2107.05002", "submitter": "Gaochen Wu", "authors": "Gaochen Wu, Bin Xu1, Yuxin Qin, Fei Kong, Bangchang Liu, Hongwen Zhao,\n  Dejie Chang", "title": "Improving Low-resource Reading Comprehension via Cross-lingual\n  Transposition Rethinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extractive Reading Comprehension (ERC) has made tremendous advances enabled\nby the availability of large-scale high-quality ERC training data. Despite of\nsuch rapid progress and widespread application, the datasets in languages other\nthan high-resource languages such as English remain scarce. To address this\nissue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by\nmodelling existing high-quality extractive reading comprehension datasets in a\nmultilingual environment. To be specific, we present multilingual adaptive\nattention (MAA) to combine intra-attention and inter-attention to learn more\ngeneral generalizable semantic and lexical knowledge from each pair of language\nfamilies. Furthermore, to make full use of existing datasets, we adopt a new\ntraining framework to train our model by calculating task-level similarities\nbetween each existing dataset and target dataset. The experimental results show\nthat our XLTT model surpasses six baselines on two multilingual ERC benchmarks,\nespecially more effective for low-resource languages with 3.9 and 4.1 average\nimprovement in F1 and EM, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 09:35:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wu", "Gaochen", ""], ["Xu1", "Bin", ""], ["Qin", "Yuxin", ""], ["Kong", "Fei", ""], ["Liu", "Bangchang", ""], ["Zhao", "Hongwen", ""], ["Chang", "Dejie", ""]]}, {"id": "2107.05038", "submitter": "Chengrui Zhu", "authors": "Chengrui Zhu, Keyu An, Huahuan Zheng, Zhijian Ou", "title": "Multilingual and crosslingual speech recognition using\n  phonological-vector based phone embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of phonological features (PFs) potentially allows language-specific\nphones to remain linked in training, which is highly desirable for information\nsharing for multilingual and crosslingual speech recognition methods for\nlow-resourced languages. A drawback suffered by previous methods in using\nphonological features is that the acoustic-to-PF extraction in a bottom-up way\nis itself difficult. In this paper, we propose to join phonology driven phone\nembedding (top-down) and deep neural network (DNN) based acoustic feature\nextraction (bottom-up) to calculate phone probabilities. The new method is\ncalled JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion\nfrom acoustics to phonological features is required for speech recognition. For\neach phone in the IPA (International Phonetic Alphabet) table, we encode its\nphonological features to a phonological-vector, and then apply linear or\nnonlinear transformation of the phonological-vector to obtain the phone\nembedding. A series of multilingual and crosslingual (both zero-shot and\nfew-shot) speech recognition experiments are conducted on the CommonVoice\ndataset (German, French, Spanish and Italian) and the AISHLL-1 dataset\n(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone\nembeddings over both JoinAP with linear phone embeddings and the traditional\nmethod with flat phone embeddings.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 12:56:47 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhu", "Chengrui", ""], ["An", "Keyu", ""], ["Zheng", "Huahuan", ""], ["Ou", "Zhijian", ""]]}, {"id": "2107.05133", "submitter": "Gus Hahn-Powell", "authors": "Seethalakshmi Gopalakrishnan, Victor Chen, Gus Hahn-Powell, Bharadwaj\n  Tirunagar", "title": "Computer-assisted construct classification of organizational performance\n  concerning different stakeholder groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The number of research articles in business and management has dramatically\nincreased along with terminology, constructs, and measures. Proper\nclassification of organizational performance constructs from research articles\nplays an important role in categorizing the literature and understanding to\nwhom its research implications may be relevant. In this work, we classify\nconstructs (i.e., concepts and terminology used to capture different aspects of\norganizational performance) in research articles into a three-level\ncategorization: (a) performance and non-performance categories (Level 0); (b)\nfor performance constructs, stakeholder group-level of performance concerning\ninvestors, customers, employees, and the society (community and natural\nenvironment) (Level 1); and (c) for each stakeholder group-level, subcategories\nof different ways of measurement (Level 2). We observed that increasing\ncontextual information with features extracted from surrounding sentences and\nexternal references improves classification of disaggregate-level labels, given\nlimited training data. Our research has implications for computer-assisted\nconstruct identification and classification - an essential step for research\nsynthesis.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 21:39:37 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gopalakrishnan", "Seethalakshmi", ""], ["Chen", "Victor", ""], ["Hahn-Powell", "Gus", ""], ["Tirunagar", "Bharadwaj", ""]]}, {"id": "2107.05168", "submitter": "Haipang Wu", "authors": "Jingyao Zhou, Haipang Wu, Zehao Lin, Guodun Li, Yin Zhang", "title": "Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue\n  States and Conversations", "comments": "SIGDIAL 2021 Accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recently proposed approaches in dialogue state tracking (DST) leverage\nthe context and the last dialogue states to track current dialogue states,\nwhich are often slot-value pairs. Although the context contains the complete\ndialogue information, the information is usually indirect and even requires\nreasoning to obtain. The information in the lastly predicted dialogue states is\ndirect, but when there is a prediction error, the dialogue information from\nthis source will be incomplete or erroneous. In this paper, we propose the\nDialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States\nand Conversations network (FPDSC). This model extracts information of each\ndialogue turn by modeling interactions among each turn utterance, the\ncorresponding last dialogue states, and dialogue slots. Then the representation\nof each dialogue turn is aggregated by a hierarchical structure to form the\npassage information, which is utilized in the current turn of DST. Experimental\nresults validate the effectiveness of the fusion network with 55.03% and 59.07%\njoint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, which reaches the\nstate-of-the-art performance. Furthermore, we conduct the deleted-value and\nrelated-slot experiments on MultiWOZ 2.1 to evaluate our model.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 02:30:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhou", "Jingyao", ""], ["Wu", "Haipang", ""], ["Lin", "Zehao", ""], ["Li", "Guodun", ""], ["Zhang", "Yin", ""]]}, {"id": "2107.05176", "submitter": "Guangyue Xu", "authors": "Guangyue Xu, Parisa Kordjamshidi, Joyce Y. Chai", "title": "Zero-Shot Compositional Concept Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 03:31:56 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xu", "Guangyue", ""], ["Kordjamshidi", "Parisa", ""], ["Chai", "Joyce Y.", ""]]}, {"id": "2107.05192", "submitter": "Luyao Ma", "authors": "Luyao Ma, Yating Zhang, Tianyi Wang, Xiaozhong Liu, Wei Ye, Changlong\n  Sun, Shikun Zhang", "title": "Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning\n  in the Real Court Setting", "comments": "Accepted at SIGIR2021", "journal-ref": null, "doi": "10.1145/3404835.3462945", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal judgment prediction(LJP) is an essential task for legal AI. While prior\nmethods studied on this topic in a pseudo setting by employing the\njudge-summarized case narrative as the input to predict the judgment,\nneglecting critical case life-cycle information in real court setting could\nthreaten the case logic representation quality and prediction correctness. In\nthis paper, we introduce a novel challenging dataset from real courtrooms to\npredict the legal judgment in a reasonably encyclopedic manner by leveraging\nthe genuine input of the case -- plaintiff's claims and court debate data, from\nwhich the case's facts are automatically recognized by comprehensively\nunderstanding the multi-role dialogues of the court debate, and then learnt to\ndiscriminate the claims so as to reach the final judgment through multi-task\nlearning. An extensive set of experiments with a large civil trial data set\nshows that the proposed model can more accurately characterize the interactions\namong claims, fact and debate for legal judgment prediction, achieving\nsignificant improvements over strong state-of-the-art baselines. Moreover, the\nuser study conducted with real judges and law school students shows the neural\npredictions can also be interpretable and easily observed, and thus enhancing\nthe trial efficiency and judgment quality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 04:27:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ma", "Luyao", ""], ["Zhang", "Yating", ""], ["Wang", "Tianyi", ""], ["Liu", "Xiaozhong", ""], ["Ye", "Wei", ""], ["Sun", "Changlong", ""], ["Zhang", "Shikun", ""]]}, {"id": "2107.05219", "submitter": "Pengsen Cheng", "authors": "Pengsen Cheng, Jiayong Liu, Jinqiao Dai", "title": "CatVRNN: Generating Category Texts via Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controlling the model to generate texts of different categories is a\nchallenging task that is getting more and more attention. Recently, generative\nadversarial net (GAN) has shown promising results in category text generation.\nHowever, the texts generated by GANs usually suffer from the problems of mode\ncollapse and training instability. To avoid the above problems, we propose a\nnovel model named category-aware variational recurrent neural network\n(CatVRNN), which is inspired by multi-task learning. In our model, generation\nand classification are trained simultaneously, aiming at generating texts of\ndifferent categories. Moreover, the use of multi-task learning can improve the\nquality of generated texts, when the classification task is appropriate. And we\npropose a function to initialize the hidden state of CatVRNN to force model to\ngenerate texts of a specific category. Experimental results on three datasets\ndemonstrate that our model can do better than several state-of-the-art text\ngeneration methods based GAN in the category accuracy and quality of generated\ntexts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 06:42:37 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cheng", "Pengsen", ""], ["Liu", "Jiayong", ""], ["Dai", "Jinqiao", ""]]}, {"id": "2107.05243", "submitter": "Jun Wang", "authors": "Jun Wang, Chang Xu, Francisco Guzman, Ahmed El-Kishky, Yuqing Tang,\n  Benjamin I. P. Rubinstein, Trevor Cohn", "title": "Putting words into the system's mouth: A targeted attack on neural\n  machine translation using monolingual data poisoning", "comments": "Findings of ACL, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation systems are known to be vulnerable to adversarial\ntest inputs, however, as we show in this paper, these systems are also\nvulnerable to training attacks. Specifically, we propose a poisoning attack in\nwhich a malicious adversary inserts a small poisoned sample of monolingual text\ninto the training set of a system trained using back-translation. This sample\nis designed to induce a specific, targeted translation behaviour, such as\npeddling misinformation. We present two methods for crafting poisoned examples,\nand show that only a tiny handful of instances, amounting to only 0.02% of the\ntraining set, is sufficient to enact a successful attack. We outline a defence\nmethod against said attacks, which partly ameliorates the problem. However, we\nstress that this is a blind-spot in modern NMT, demanding immediate attention.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 08:07:09 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Jun", ""], ["Xu", "Chang", ""], ["Guzman", "Francisco", ""], ["El-Kishky", "Ahmed", ""], ["Tang", "Yuqing", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Cohn", "Trevor", ""]]}, {"id": "2107.05295", "submitter": "Lasse Hansen", "authors": "Kenneth Enevoldsen, Lasse Hansen, Kristoffer Nielbo", "title": "DaCy: A Unified Framework for Danish NLP", "comments": "8 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 10:14:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Enevoldsen", "Kenneth", ""], ["Hansen", "Lasse", ""], ["Nielbo", "Kristoffer", ""]]}, {"id": "2107.05357", "submitter": "Fabio Celli PhD", "authors": "Armend Duzha, Cristiano Casadei, Michael Tosi, Fabio Celli", "title": "Hate versus Politics: Detection of Hate against Policy makers in Italian\n  tweets", "comments": "to appear in SN social sciences - special issue on hate speech", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate detection of hate speech against politicians, policy making and\npolitical ideas is crucial to maintain democracy and free speech.\nUnfortunately, the amount of labelled data necessary for training models to\ndetect hate speech are limited and domain-dependent. In this paper, we address\nthe issue of classification of hate speech against policy makers from Twitter\nin Italian, producing the first resource of this type in this language. We\ncollected and annotated 1264 tweets, examined the cases of disagreements\nbetween annotators, and performed in-domain and cross-domain hate speech\nclassifications with different features and algorithms. We achieved a\nperformance of ROC AUC 0.83 and analyzed the most predictive attributes, also\nfinding the different language features in the anti-policymakers and\nanti-immigration domains. Finally, we visualized networks of hashtags to\ncapture the topics used in hateful and normal tweets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:24:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Duzha", "Armend", ""], ["Casadei", "Cristiano", ""], ["Tosi", "Michael", ""], ["Celli", "Fabio", ""]]}, {"id": "2107.05365", "submitter": "Jing Li", "authors": "Jing Li, Binling Wang, Yiming Zhi, Zheng Li, Lin Li, Qingyang Hong,\n  Dong Wang", "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fifth Oriental Language Recognition (OLR) Challenge focuses on language\nrecognition in a variety of complex environments to promote its development.\nThe OLR 2020 Challenge includes three tasks: (1) cross-channel language\nidentification, (2) dialect identification, and (3) noisy language\nidentification. We choose Cavg as the principle evaluation metric, and the\nEqual Error Rate (EER) as the secondary metric. There were 58 teams\nparticipating in this challenge and one third of the teams submitted valid\nresults. Compared with the best baseline, the Cavg values of Top 1 system for\nthe three tasks were relatively reduced by 82%, 62% and 48%, respectively. This\npaper describes the three tasks, the database profile, and the final results.\nWe also outline the novel approaches that improve the performance of language\nrecognition systems most significantly, such as the utilization of auxiliary\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:42:40 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Li", "Jing", ""], ["Wang", "Binling", ""], ["Zhi", "Yiming", ""], ["Li", "Zheng", ""], ["Li", "Lin", ""], ["Hong", "Qingyang", ""], ["Wang", "Dong", ""]]}, {"id": "2107.05377", "submitter": "Tianwen Wei", "authors": "Tianwen Wei, Jianwei Qi, Shenghuan He", "title": "A Flexible Multi-Task Model for BERT Serving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:42:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wei", "Tianwen", ""], ["Qi", "Jianwei", ""], ["He", "Shenghuan", ""]]}, {"id": "2107.05380", "submitter": "Anish Acharya", "authors": "Anish Acharya, Rudrajit Das", "title": "DISCO : efficient unsupervised decoding for discrete natural language\n  problems via convex relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we study test time decoding; an ubiquitous step in almost all\nsequential text generation task spanning across a wide array of natural\nlanguage processing (NLP) problems. Our main contribution is to develop a\ncontinuous relaxation framework for the combinatorial NP-hard decoding problem\nand propose Disco - an efficient algorithm based on standard first order\ngradient based. We provide tight analysis and show that our proposed algorithm\nlinearly converges to within $\\epsilon$ neighborhood of the optima. Finally, we\nperform preliminary experiments on the task of adversarial text generation and\nshow superior performance of Disco over several popular decoding approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 00:40:25 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 20:34:37 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Acharya", "Anish", ""], ["Das", "Rudrajit", ""]]}, {"id": "2107.05381", "submitter": "Daniel Devatman Hromada", "authors": "Daniel Devatman Hromada", "title": "Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?", "comments": "16 pages, 3 figures, 2 source code snippets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Departing from the postulate that Voynich Manuscript is not a hoax but rather\nencodes authentic contents, our article presents an evolutionary algorithm\nwhich aims to find the most optimal mapping between voynichian glyphs and\ncandidate phonemic values. Core component of the decoding algorithm is a\nprocess of maximization of a fitness function which aims to find most optimal\nset of substitution rules allowing to transcribe the part of the manuscript --\nwhich we call the Calendar -- into lists of feminine names. This leads to sets\nof character subsitution rules which allow us to consistently transcribe dozens\namong three hundred calendar tokens into feminine names: a result far\nsurpassing both ``popular'' as well as \"state of the art\" tentatives to crack\nthe manuscript. What's more, by using name lists stemming from different\nlanguages as potential cribs, our ``adaptive'' method can also be useful in\nidentification of the language in which the manuscript is written.\n  As far as we can currently tell, results of our experiments indicate that the\nCalendar part of the manuscript contains names from baltoslavic, balkanic or\nhebrew language strata. Two further indications are also given: primo, highest\nfitness values were obtained when the crib list contains names with specific\ninfixes at token's penultimate position as is the case, for example, for slavic\n\\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the\nmost successful scenario, 240 characters contained in 35 distinct Voynichese\ntokens were successfully transcribed. Secundo, in case of crib stemming from\nHebrew language, whole adaptation process converges to significantly better\nfitness values when transcribing voynichian tokens whose order of individual\ncharacters have been reversed, and when lists feminine and not masculine names\nare used as the crib.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 23:39:59 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hromada", "Daniel Devatman", ""]]}, {"id": "2107.05382", "submitter": "Tomohiro Tanaka", "authors": "Tomohiro Tanaka, Ryo Masumura, Mana Ihori, Akihiko Takashima, Shota\n  Orihashi, Naoki Makishima", "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with\n  Semi-Supervised Learning", "comments": "Accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semi-supervised learning method for building end-to-end rich\ntranscription-style automatic speech recognition (RT-ASR) systems from\nsmall-scale rich transcription-style and large-scale common transcription-style\ndatasets. In spontaneous speech tasks, various speech phenomena such as\nfillers, word fragments, laughter and coughs, etc. are often included. While\ncommon transcriptions do not give special awareness to these phenomena, rich\ntranscriptions explicitly convert them into special phenomenon tokens as well\nas textual tokens. In previous studies, the textual and phenomenon tokens were\nsimultaneously estimated in an end-to-end manner. However, it is difficult to\nbuild accurate RT-ASR systems because large-scale rich transcription-style\ndatasets are often unavailable. To solve this problem, our training method uses\na limited rich transcription-style dataset and common transcription-style\ndataset simultaneously. The Key process in our semi-supervised learning is to\nconvert the common transcription-style dataset into a pseudo-rich\ntranscription-style dataset. To this end, we introduce style tokens which\ncontrol phenomenon tokens are generated or not into transformer-based\nautoregressive modeling. We use this modeling for generating the pseudo-rich\ntranscription-style datasets and for building RT-ASR system from the pseudo and\noriginal datasets. Our experiments on spontaneous ASR tasks showed the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:52:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tanaka", "Tomohiro", ""], ["Masumura", "Ryo", ""], ["Ihori", "Mana", ""], ["Takashima", "Akihiko", ""], ["Orihashi", "Shota", ""], ["Makishima", "Naoki", ""]]}, {"id": "2107.05385", "submitter": "Monika Daryani", "authors": "Monika Daryani and James Caverlee", "title": "Identifying Hijacked Reviews", "comments": "To be published in ACL-IJCNLP 2021 Workshop on e-Commerce and NLP\n  (ECNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake reviews and review manipulation are growing problems on online\nmarketplaces globally. Review Hijacking is a new review manipulation tactic in\nwhich unethical sellers \"hijack\" an existing product page (usually one with\nmany positive reviews), then update the product details like title, photo, and\ndescription with those of an entirely different product. With the earlier\nreviews still attached, the new item appears well-reviewed. However, there are\nno public datasets of review hijacking and little is known in the literature\nabout this tactic. Hence, this paper proposes a three-part study: (i) we\npropose a framework to generate synthetically labeled data for review hijacking\nby swapping products and reviews; (ii) then, we evaluate the potential of both\na Twin LSTM network and BERT sequence pair classifier to distinguish legitimate\nreviews from hijacked ones using this data; and (iii) we then deploy the best\nperforming model on a collection of 31K products (with 6.5 M reviews) in the\noriginal data, where we find 100s of previously unknown examples of review\nhijacking.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 20:43:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Daryani", "Monika", ""], ["Caverlee", "James", ""]]}, {"id": "2107.05392", "submitter": "Olha Kaminska", "authors": "Olha Kaminska, Chris Cornelis, Veronique Hoste", "title": "Fuzzy-Rough Nearest Neighbour Approaches for Emotion Detection in Tweets", "comments": "The paper submitted to the IJCRS 2021 conference, organized jointly\n  with IFSA-EUSFLAT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media are an essential source of meaningful data that can be used in\ndifferent tasks such as sentiment analysis and emotion recognition. Mostly,\nthese tasks are solved with deep learning methods. Due to the fuzzy nature of\ntextual data, we consider using classification methods based on fuzzy rough\nsets. Specifically, we develop an approach for the SemEval-2018 emotion\ndetection task, based on the fuzzy rough nearest neighbour (FRNN) classifier\nenhanced with ordered weighted average (OWA) operators. We use tuned ensembles\nof FRNN--OWA models based on different text embedding methods. Our results are\ncompetitive with the best SemEval solutions based on more complicated deep\nlearning methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:52:47 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kaminska", "Olha", ""], ["Cornelis", "Chris", ""], ["Hoste", "Veronique", ""]]}, {"id": "2107.05393", "submitter": "Si-An Chen", "authors": "Jie-Jyun Liu, Tsung-Han Yang, Si-An Chen, Chih-Jen Lin", "title": "Parameter Selection: Why We Should Pay More Attention to It", "comments": "Accepted by ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The importance of parameter selection in supervised learning is well known.\nHowever, due to the many parameter combinations, an incomplete or an\ninsufficient procedure is often applied. This situation may cause misleading or\nconfusing conclusions. In this opinion paper, through an intriguing example we\npoint out that the seriousness goes beyond what is generally recognized. In the\ntopic of multi-label classification for medical code prediction, one\ninfluential paper conducted a proper parameter selection on a set, but when\nmoving to a subset of frequently occurring labels, the authors used the same\nparameters without a separate tuning. The set of frequent labels became a\npopular benchmark in subsequent studies, which kept pushing the state of the\nart. However, we discovered that most of the results in these studies cannot\nsurpass the approach in the original paper if a parameter tuning had been\nconducted at the time. Thus it is unclear how much progress the subsequent\ndevelopments have actually brought. The lesson clearly indicates that without\nenough attention on parameter selection, the research progress in our field can\nbe uncertain or even illusive.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:55:34 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liu", "Jie-Jyun", ""], ["Yang", "Tsung-Han", ""], ["Chen", "Si-An", ""], ["Lin", "Chih-Jen", ""]]}, {"id": "2107.05394", "submitter": "Olha Kaminska", "authors": "Olha Kaminska, Chris Cornelis, Veronique Hoste", "title": "Nearest neighbour approaches for Emotion Detection in Tweets", "comments": "The paper was presented at EACL 2021 during the WASSA workshop as a\n  poster and published at ACL Anthology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion detection is an important task that can be applied to social media\ndata to discover new knowledge. While the use of deep learning methods for this\ntask has been prevalent, they are black-box models, making their decisions hard\nto interpret for a human operator. Therefore, in this paper, we propose an\napproach using weighted $k$ Nearest Neighbours (kNN), a simple, easy to\nimplement, and explainable machine learning model. These qualities can help to\nenhance results' reliability and guide error analysis. In particular, we apply\nthe weighted kNN model to the shared emotion detection task in tweets from\nSemEval-2018. Tweets are represented using different text embedding methods and\nemotion lexicon vocabulary scores, and classification is done by an ensemble of\nweighted kNN models. Our best approaches obtain results competitive with\nstate-of-the-art solutions and open up a promising alternative path to neural\nnetwork methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:00:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kaminska", "Olha", ""], ["Cornelis", "Chris", ""], ["Hoste", "Veronique", ""]]}, {"id": "2107.05418", "submitter": "Zhenhua Feng", "authors": "Shuang Wu, Xiaoning Song and Zhenhua Feng", "title": "MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named\n  Entity Recognition", "comments": "Accepted to ACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, word enhancement has become very popular for Chinese Named Entity\nRecognition (NER), reducing segmentation errors and increasing the semantic and\nboundary information of Chinese words. However, these methods tend to ignore\nthe information of the Chinese character structure after integrating the\nlexical information. Chinese characters have evolved from pictographs since\nancient times, and their structure often reflects more information about the\ncharacters. This paper presents a novel Multi-metadata Embedding based\nCross-Transformer (MECT) to improve the performance of Chinese NER by fusing\nthe structural information of Chinese characters. Specifically, we use\nmulti-metadata embedding in a two-stream Transformer to integrate Chinese\ncharacter features with the radical-level embedding. With the structural\ncharacteristics of Chinese characters, MECT can better capture the semantic\ninformation of Chinese characters for NER. The experimental results obtained on\nseveral well-known benchmarking datasets demonstrate the merits and superiority\nof the proposed MECT method.\\footnote{The source code of the proposed method is\npublicly available at https://github.com/CoderMusou/MECT4CNER.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 13:39:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wu", "Shuang", ""], ["Song", "Xiaoning", ""], ["Feng", "Zhenhua", ""]]}, {"id": "2107.05465", "submitter": "Yixin Liu", "authors": "Yixin Liu, Jiaxin Guo, Jieyang Dong, Luoqian Jiang and Haoyuan Ouyang", "title": "Priority prediction of Asian Hornet sighting report using machine\n  learning methods", "comments": "2021 IEEE International Conference on Software Engineering and\n  Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As infamous invaders to the North American ecosystem, the Asian giant hornet\n(Vespa mandarinia) is devastating not only to native bee colonies, but also to\nlocal apiculture. One of the most effective way to combat the harmful species\nis to locate and destroy their nests. By mobilizing the public to actively\nreport possible sightings of the Asian giant hornet, the governmentcould timely\nsend inspectors to confirm and possibly destroy the nests. However, such\nconfirmation requires lab expertise, where manually checking the reports one by\none is extremely consuming of human resources. Further given the limited\nknowledge of the public about the Asian giant hornet and the randomness of\nreport submission, only few of the numerous reports proved positive, i.e.\nexisting nests. How to classify or prioritize the reports efficiently and\nautomatically, so as to determine the dispatch of personnel, is of great\nsignificance to the control of the Asian giant hornet. In this paper, we\npropose a method to predict the priority of sighting reports based on machine\nlearning. We model the problem of optimal prioritization of sighting reports as\na problem of classification and prediction. We extracted a variety of rich\nfeatures in the report: location, time, image(s), and textual description.\nBased on these characteristics, we propose a classification model based on\nlogistic regression to predict the credibility of a certain report.\nFurthermore, our model quantifies the impact between reports to get the\npriority ranking of the reports. Extensive experiments on the public dataset\nfrom the WSDA (the Washington State Department of Agriculture) have proved the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:33:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liu", "Yixin", ""], ["Guo", "Jiaxin", ""], ["Dong", "Jieyang", ""], ["Jiang", "Luoqian", ""], ["Ouyang", "Haoyuan", ""]]}, {"id": "2107.05467", "submitter": "Cees Roele", "authors": "Cees Roele", "title": "WVOQ at SemEval-2021 Task 6: BART for Span Detection and Classification", "comments": "5 pages, 1 figure, accepted at SemEval-2021 co-located with\n  ACL-IJCNLP 2021", "journal-ref": "SemEval-2021", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel solution to span detection and classification is presented in which a\nBART EncoderDecoder model is used to transform textual input into a version\nwith XML-like marked up spans. This markup is subsequently translated to an\nidentification of the beginning and end of fragments and of their classes.\nDiscussed is how pre-training methodology both explains the relative success of\nthis method and its limitations. This paper reports on participation in task 6\nof SemEval-2021: Detection of Persuasion Techniques in Texts and Images.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 07:59:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Roele", "Cees", ""]]}, {"id": "2107.05476", "submitter": "Jie Wang", "authors": "Jianyu Cai, Jiajun Chen, Taoxing Pan, Zhanqiu Zhang, Jie Wang", "title": "Technical Report of Team GraphMIRAcles in the WikiKG90M-LSC Track of\n  OGB-LSC @ KDD Cup 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Link prediction in large-scale knowledge graphs has gained increasing\nattention recently. The OGB-LSC team presented OGB Large-Scale Challenge\n(OGB-LSC), a collection of three real-world datasets for advancing the\nstate-of-the-art in large-scale graph machine learning. In this paper, we\nintroduce the solution of our team GraphMIRAcles in the WikiKG90M-LSC track of\nOGB-LSC @ KDD Cup 2021. In the WikiKG90M-LSC track, the goal is to\nautomatically predict missing links in WikiKG90M, a large scale knowledge graph\nextracted from Wikidata. To address this challenge, we propose a framework that\nintegrates three components -- a basic model ComplEx-CMRC, a rule miner AMIE 3,\nand an inference model to predict missing links. Experiments demonstrate that\nour solution achieves an MRR of 0.9707 on the test dataset. Moreover, as the\nknowledge distillation in the inference model uses test tail candidates --\nwhich are unavailable in practice -- we conduct ablation studies on knowledge\ndistillation. Experiments demonstrate that our model without knowledge\ndistillation achieves an MRR of 0.9533 on the full validation dataset.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:44:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cai", "Jianyu", ""], ["Chen", "Jiajun", ""], ["Pan", "Taoxing", ""], ["Zhang", "Zhanqiu", ""], ["Wang", "Jie", ""]]}, {"id": "2107.05541", "submitter": "Mohammad Sabik Irbaz", "authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD\n  Abdullah Al Nasim", "title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02%, precision: 80.82%,\nrecall: 83.02%, F1-score: 80%).\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:09:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 01:52:58 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 19:23:43 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Khan", "Fahim Shahriar", ""], ["Mushabbir", "Mueeze Al", ""], ["Irbaz", "Mohammad Sabik", ""], ["Nasim", "MD Abdullah Al", ""]]}, {"id": "2107.05604", "submitter": "Wei-Ning Hsu", "authors": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Xutai Ma, Adam\n  Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu", "title": "Direct speech-to-speech translation with discrete units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a direct speech-to-speech translation (S2ST) model that translates\nspeech from one language to speech in another language without relying on\nintermediate text generation. Previous work addresses the problem by training\nan attention-based sequence-to-sequence model that maps source speech\nspectrograms into target spectrograms. To tackle the challenge of modeling\ncontinuous spectrogram features of the target speech, we propose to predict the\nself-supervised discrete representations learned from an unlabeled speech\ncorpus instead. When target text transcripts are available, we design a\nmultitask learning framework with joint speech and text training that enables\nthe model to generate dual mode output (speech and text) simultaneously in the\nsame inference pass. Experiments on the Fisher Spanish-English dataset show\nthat predicting discrete units and joint speech and text training improve model\nperformance by 11 BLEU compared with a baseline that predicts spectrograms and\nbridges 83% of the performance gap towards a cascaded system. When trained\nwithout any text transcripts, our model achieves similar performance as a\nbaseline that predicts spectrograms and is trained with text data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:40:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lee", "Ann", ""], ["Chen", "Peng-Jen", ""], ["Wang", "Changhan", ""], ["Gu", "Jiatao", ""], ["Ma", "Xutai", ""], ["Polyak", "Adam", ""], ["Adi", "Yossi", ""], ["He", "Qing", ""], ["Tang", "Yun", ""], ["Pino", "Juan", ""], ["Hsu", "Wei-Ning", ""]]}, {"id": "2107.05612", "submitter": "Valts Blukis", "authors": "Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi", "title": "A Persistent Spatial Semantic Representation for High-level Natural\n  Language Instruction Execution", "comments": "Submitted to CoRL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language provides an accessible and expressive interface to specify\nlong-term tasks for robotic agents. However, non-experts are likely to specify\nsuch tasks with high-level instructions, which abstract over specific robot\nactions through several layers of abstraction. We propose that key to bridging\nthis gap between language and robot actions over long execution horizons are\npersistent representations. We propose a persistent spatial semantic\nrepresentation method, and show how it enables building an agent that performs\nhierarchical reasoning to effectively execute long-term tasks. We evaluate our\napproach on the ALFRED benchmark and achieve state-of-the-art results, despite\ncompletely avoiding the commonly used step-by-step instructions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:47:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Blukis", "Valts", ""], ["Paxton", "Chris", ""], ["Fox", "Dieter", ""], ["Garg", "Animesh", ""], ["Artzi", "Yoav", ""]]}, {"id": "2107.05684", "submitter": "Paul Rodrigues", "authors": "Evan Williams, Paul Rodrigues, Sieu Tran", "title": "Accenture at CheckThat! 2021: Interesting claim identification and\n  ranking with contextually sensitive lexical training data augmentation", "comments": "To Appear As: Evan Williams, Paul Rodrigues, Sieu Tran. Accenture at\n  CheckThat! 2021: Interesting claim identification and ranking with\n  contextually sensitive lexical training data augmentation. In: Faggioli et\n  al. Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum.\n  Bucharest, Romania. 21-24 September 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses the approach used by the Accenture Team for CLEF2021\nCheckThat! Lab, Task 1, to identify whether a claim made in social media would\nbe interesting to a wide audience and should be fact-checked. Twitter training\nand test data were provided in English, Arabic, Spanish, Turkish, and\nBulgarian. Claims were to be classified (check-worthy/not check-worthy) and\nranked in priority order for the fact-checker. Our method used deep neural\nnetwork transformer models with contextually sensitive lexical augmentation\napplied on the supplied training datasets to create additional training\nsamples. This augmentation approach improved the performance for all languages.\nOverall, our architecture and data augmentation pipeline produced the best\nsubmitted system for Arabic, and performance scales according to the quantity\nof provided training data for English, Spanish, Turkish, and Bulgarian. This\npaper investigates the deep neural network architectures for each language as\nwell as the provided data to examine why the approach worked so effectively for\nArabic, and discusses additional data augmentation measures that should could\nbe useful to this problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:46:47 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Williams", "Evan", ""], ["Rodrigues", "Paul", ""], ["Tran", "Sieu", ""]]}, {"id": "2107.05687", "submitter": "Christopher Schr\\\"oder", "authors": "Christopher Schr\\\"oder, Andreas Niekler, Martin Potthast", "title": "Uncertainty-based Query Strategies for Active Learning with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively cancels out, or even outweighs\naforementioned cost savings. In this paper, we revisit uncertainty-based query\nstrategies, which had been largely outperformed before, but are particularly\nsuited in the context of fine-tuning transformers. In an extensive evaluation\non five widely used text classification benchmarks, we show that considerable\nimprovements of up to 14.4 percentage points in area under the learning curve\nare achieved, as well as a final accuracy close to the state of the art for all\nbut one benchmark, using only between 0.4% and 15% of the training data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:56:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Schr\u00f6der", "Christopher", ""], ["Niekler", "Andreas", ""], ["Potthast", "Martin", ""]]}, {"id": "2107.05693", "submitter": "Mitchell Naylor", "authors": "Mitchell Naylor, Christi French, Samantha Terker, Uday Kamath", "title": "Quantifying Explainability in NLP and Analyzing Algorithms for\n  Performance-Explainability Tradeoff", "comments": "To appear at Interpretable ML in Healthcare workshop at ICML 2021. 9\n  pages (excluding references), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The healthcare domain is one of the most exciting application areas for\nmachine learning, but a lack of model transparency contributes to a lag in\nadoption within the industry. In this work, we explore the current art of\nexplainability and interpretability within a case study in clinical text\nclassification, using a task of mortality prediction within MIMIC-III clinical\nnotes. We demonstrate various visualization techniques for fully interpretable\nmethods as well as model-agnostic post hoc attributions, and we provide a\ngeneralized method for evaluating the quality of explanations using infidelity\nand local Lipschitz across model types from logistic regression to BERT\nvariants. With these metrics, we introduce a framework through which\npractitioners and researchers can assess the frontier between a model's\npredictive performance and the quality of its available explanations. We make\nour code available to encourage continued refinement of these methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 19:07:24 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Naylor", "Mitchell", ""], ["French", "Christi", ""], ["Terker", "Samantha", ""], ["Kamath", "Uday", ""]]}, {"id": "2107.05697", "submitter": "Hao Zhu", "authors": "Hao Zhu, Graham Neubig, Yonatan Bisk", "title": "Few-shot Language Coordination by Modeling Theory of Mind", "comments": "Thirty-eighth International Conference on Machine Learning (ICML\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $\\textit{No man is an island.}$ Humans communicate with a large community by\ncoordinating with different interlocutors within short conversations. This\nability has been understudied by the research on building neural communicative\nagents. We study the task of few-shot $\\textit{language coordination}$: agents\nquickly adapting to their conversational partners' language abilities.\nDifferent from current communicative agents trained with self-play, we require\nthe lead agent to coordinate with a $\\textit{population}$ of agents with\ndifferent linguistic abilities, quickly adapting to communicate with unseen\nagents in the population. This requires the ability to model the partner's\nbeliefs, a vital component of human communication. Drawing inspiration from\ntheory-of-mind (ToM; Premack& Woodruff (1978)), we study the effect of the\nspeaker explicitly modeling the listeners' mental states. The speakers, as\nshown in our experiments, acquire the ability to predict the reactions of their\npartner, which helps it generate instructions that concisely express its\ncommunicative goal. We examine our hypothesis that the instructions generated\nwith ToM modeling yield better communication performance in both a referential\ngame and a language navigation task. Positive results from our experiments hint\nat the importance of explicitly modeling communication as a socio-pragmatic\nprogress.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 19:26:11 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhu", "Hao", ""], ["Neubig", "Graham", ""], ["Bisk", "Yonatan", ""]]}, {"id": "2107.05768", "submitter": "Hanjun Dai", "authors": "Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale\n  Schuurmans, Bo Dai", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 22:43:11 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ren", "Hongyu", ""], ["Dai", "Hanjun", ""], ["Dai", "Zihang", ""], ["Yang", "Mengjiao", ""], ["Leskovec", "Jure", ""], ["Schuurmans", "Dale", ""], ["Dai", "Bo", ""]]}, {"id": "2107.05782", "submitter": "Yun Tang", "authors": "Yun Tang, Juan Pino, Xian Li, Changhan Wang, Dmitriy Genzel", "title": "Improving Speech Translation by Understanding and Learning from the\n  Auxiliary Text Translation Task", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining and multitask learning are widely used to improve the speech to\ntext translation performance. In this study, we are interested in training a\nspeech to text translation model along with an auxiliary text to text\ntranslation task. We conduct a detailed analysis to understand the impact of\nthe auxiliary task on the primary task within the multitask learning framework.\nOur analysis confirms that multitask learning tends to generate similar decoder\nrepresentations from different modalities and preserve more information from\nthe pretrained text translation modules. We observe minimal negative transfer\neffect between the two tasks and sharing more parameters is helpful to transfer\nknowledge from the text task to the speech task. The analysis also reveals that\nthe modality representation difference at the top decoder layers is still not\nnegligible, and those layers are critical for the translation quality. Inspired\nby these findings, we propose three methods to improve translation quality.\nFirst, a parameter sharing and initialization strategy is proposed to enhance\ninformation sharing between the tasks. Second, a novel attention-based\nregularization is proposed for the encoders and pulls the representations from\ndifferent modalities closer. Third, an online knowledge distillation is\nproposed to enhance the knowledge transfer from the text to the speech task.\nOur experiments show that the proposed approach improves translation\nperformance by more than 2 BLEU over a strong baseline and achieves\nstate-of-the-art results on the \\textsc{MuST-C} English-German, English-French\nand English-Spanish language pairs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 23:53:40 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Tang", "Yun", ""], ["Pino", "Juan", ""], ["Li", "Xian", ""], ["Wang", "Changhan", ""], ["Genzel", "Dmitriy", ""]]}, {"id": "2107.05799", "submitter": "Jiajie Zou", "authors": "Jiajie Zou and Nai Ding", "title": "Deep Neural Networks Evolve Human-like Attention Distribution during\n  Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention is a key mechanism for information selection in both biological\nbrains and many state-of-the-art deep neural networks (DNNs). Here, we\ninvestigate whether humans and DNNs allocate attention in comparable ways when\nreading a text passage to subsequently answer a specific question. We analyze 3\ntransformer-based DNNs that reach human-level performance when trained to\nperform the reading comprehension task. We find that the DNN attention\ndistribution quantitatively resembles human attention distribution measured by\nfixation times. Human readers fixate longer on words that are more relevant to\nthe question-answering task, demonstrating that attention is modulated by\ntop-down reading goals, on top of lower-level visual and text features of the\nstimulus. Further analyses reveal that the attention weights in DNNs are also\ninfluenced by both top-down reading goals and lower-level stimulus features,\nwith the shallow layers more strongly influenced by lower-level text features\nand the deep layers attending more to task-relevant words. Additionally, deep\nlayers' attention to task-relevant words gradually emerges when pre-trained DNN\nmodels are fine-tuned to perform the reading comprehension task, which\ncoincides with the improvement in task performance. These results demonstrate\nthat DNNs can evolve human-like attention distribution through task\noptimization, which suggests that human attention during goal-directed reading\ncomprehension is a consequence of task optimization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 01:07:22 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zou", "Jiajie", ""], ["Ding", "Nai", ""]]}, {"id": "2107.05833", "submitter": "Nitish Gupta", "authors": "Nitish Gupta, Sameer Singh, Matt Gardner", "title": "Enforcing Consistency in Weakly Supervised Semantic Parsing", "comments": "Published in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The predominant challenge in weakly supervised semantic parsing is that of\nspurious programs that evaluate to correct answers for the wrong reasons. Prior\nwork uses elaborate search strategies to mitigate the prevalence of spurious\nprograms; however, they typically consider only one input at a time. In this\nwork we explore the use of consistency between the output programs for related\ninputs to reduce the impact of spurious programs. We bias the program search\n(and thus the model's training signal) towards programs that map the same\nphrase in related inputs to the same sub-parts in their respective programs.\nAdditionally, we study the importance of designing logical formalisms that\nfacilitate this kind of consAistency-based training. We find that a more\nconsistent formalism leads to improved model performance even without\nconsistency-based training. When combined together, these two insights lead to\na 10% absolute improvement over the best prior result on the Natural Language\nVisual Reasoning dataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 03:48:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gupta", "Nitish", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""]]}, {"id": "2107.05866", "submitter": "Shuang Peng", "authors": "Shuang Peng, Mengdi Zhou, Minghui Yang, Haitao Mi, Shaosheng Cao,\n  Zujie Wen, Teng Xu, Hongbin Wang, Lei Liu", "title": "A Dialogue-based Information Extraction System for Medical Insurance\n  Assessment", "comments": "To be published in the Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Chinese medical insurance industry, the assessor's role is essential\nand requires significant efforts to converse with the claimant. This is a\nhighly professional job that involves many parts, such as identifying personal\ninformation, collecting related evidence, and making a final insurance report.\nDue to the coronavirus (COVID-19) pandemic, the previous offline insurance\nassessment has to be conducted online. However, for the junior assessor often\nlacking practical experience, it is not easy to quickly handle such a complex\nonline procedure, yet this is important as the insurance company needs to\ndecide how much compensation the claimant should receive based on the\nassessor's feedback. In order to promote assessors' work efficiency and speed\nup the overall procedure, in this paper, we propose a dialogue-based\ninformation extraction system that integrates advanced NLP technologies for\nmedical insurance assessment. With the assistance of our system, the average\ntime cost of the procedure is reduced from 55 minutes to 35 minutes, and the\ntotal human resources cost is saved 30% compared with the previous offline\nprocedure. Until now, the system has already served thousands of online claim\ncases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 06:14:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Peng", "Shuang", ""], ["Zhou", "Mengdi", ""], ["Yang", "Minghui", ""], ["Mi", "Haitao", ""], ["Cao", "Shaosheng", ""], ["Wen", "Zujie", ""], ["Xu", "Teng", ""], ["Wang", "Hongbin", ""], ["Liu", "Lei", ""]]}, {"id": "2107.05876", "submitter": "Long Zhou", "authors": "Long Zhou, Jinyu Li, Eric Sun, Shujie Liu", "title": "A Configurable Multilingual Model is All You Need to Recognize All\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual automatic speech recognition (ASR) models have shown great\npromise in recent years because of the simplified model training and deployment\nprocess. Conventional methods either train a universal multilingual model\nwithout taking any language information or with a 1-hot language ID (LID)\nvector to guide the recognition of the target language. In practice, the user\ncan be prompted to pre-select several languages he/she can speak. The\nmultilingual model without LID cannot well utilize the language information set\nby the user while the multilingual model with LID can only handle one\npre-selected language. In this paper, we propose a novel configurable\nmultilingual model (CMM) which is trained only once but can be configured as\ndifferent models based on users' choices by extracting language-specific\nmodules together with a universal model from the trained CMM. Particularly, a\nsingle CMM can be deployed to any user scenario where the users can pre-select\nany combination of languages. Trained with 75K hours of transcribed anonymized\nMicrosoft multilingual data and evaluated with 10-language test sets, the\nproposed CMM improves from the universal multilingual model by 26.0%, 16.9%,\nand 10.4% relative word error reduction when the user selects 1, 2, or 3\nlanguages, respectively. CMM also performs significantly better on\ncode-switching test sets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 06:52:41 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhou", "Long", ""], ["Li", "Jinyu", ""], ["Sun", "Eric", ""], ["Liu", "Shujie", ""]]}, {"id": "2107.05885", "submitter": "Chao Feng", "authors": "Chao Feng, Shi-jie We", "title": "Exploiting Network Structures to Improve Semantic Representation for the\n  Financial Domain", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the participation of the MiniTrue team in the FinSim-3\nshared task on learning semantic similarities for the financial domain in\nEnglish language. Our approach combines contextual embeddings learned by\ntransformer-based language models with network structures embeddings extracted\non external knowledge sources, to create more meaningful representations of\nfinancial domain entities and terms. For this, two BERT based language models\nand a knowledge graph embedding model are used. Besides, we propose a voting\nfunction to joint three basic models for the final inference. Experimental\nresults show that the model with the knowledge graph embeddings has achieved a\nsuperior result than these models with only contextual embeddings.\nNevertheless, we also observe that our voting function brings an extra benefit\nto the final system.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:32:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Feng", "Chao", ""], ["We", "Shi-jie", ""]]}, {"id": "2107.05907", "submitter": "Shengqiang Li", "authors": "Shengqiang Li, Menglong Xu, Xiao-Lei Zhang", "title": "Conformer-based End-to-end Speech Recognition With Rotary Position\n  Embedding", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based end-to-end speech recognition models have received\nconsiderable attention in recent years due to their high training speed and\nability to model a long-range global context. Position embedding in the\ntransformer architecture is indispensable because it provides supervision for\ndependency modeling between elements at different positions in the input\nsequence. To make use of the time order of the input sequence, many works\ninject some information about the relative or absolute position of the element\ninto the input sequence. In this work, we investigate various position\nembedding methods in the convolution-augmented transformer (conformer) and\nadopt a novel implementation named rotary position embedding (RoPE). RoPE\nencodes absolute positional information into the input sequence by a rotation\nmatrix, and then naturally incorporates explicit relative position information\ninto a self-attention module. To evaluate the effectiveness of the RoPE method,\nwe conducted experiments on AISHELL-1 and LibriSpeech corpora. Results show\nthat the conformer enhanced with RoPE achieves superior performance in the\nspeech recognition task. Specifically, our model achieves a relative word error\nrate reduction of 8.70% and 7.27% over the conformer on test-clean and\ntest-other sets of the LibriSpeech corpus respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 08:07:22 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Shengqiang", ""], ["Xu", "Menglong", ""], ["Zhang", "Xiao-Lei", ""]]}, {"id": "2107.05987", "submitter": "Eva Vanmassenhove", "authors": "Nishtha Jain, Maja Popovic, Declan Groves, Eva Vanmassenhove", "title": "Generating Gender Augmented Data for NLP", "comments": "10 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gender bias is a frequent occurrence in NLP-based applications, especially\npronounced in gender-inflected languages. Bias can appear through associations\nof certain adjectives and animate nouns with the natural gender of referents,\nbut also due to unbalanced grammatical gender frequencies of inflected words.\nThis type of bias becomes more evident in generating conversational utterances\nwhere gender is not specified within the sentence, because most current NLP\napplications still work on a sentence-level context. As a step towards more\ninclusive NLP, this paper proposes an automatic and generalisable rewriting\napproach for short conversational sentences. The rewriting method can be\napplied to sentences that, without extra-sentential context, have multiple\nequivalent alternatives in terms of gender. The method can be applied both for\ncreating gender balanced outputs as well as for creating gender balanced\ntraining data. The proposed approach is based on a neural machine translation\n(NMT) system trained to 'translate' from one gender alternative to another.\nBoth the automatic and manual analysis of the approach show promising results\nfor automatic generation of gender alternatives for conversational sentences in\nSpanish.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 11:13:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Jain", "Nishtha", ""], ["Popovic", "Maja", ""], ["Groves", "Declan", ""], ["Vanmassenhove", "Eva", ""]]}, {"id": "2107.06010", "submitter": "Tu Anh Dinh", "authors": "Tu Anh Dinh", "title": "Zero-shot Speech Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Speech Translation (ST) is the task of translating speech in one language\ninto text in another language. Traditional cascaded approaches for ST, using\nAutomatic Speech Recognition (ASR) and Machine Translation (MT) systems, are\nprone to error propagation. End-to-end approaches use only one system to avoid\npropagating error, yet are difficult to employ due to data scarcity. We explore\nzero-shot translation, which enables translating a pair of languages that is\nunseen during training, thus avoid the use of end-to-end ST data. Zero-shot\ntranslation has been shown to work for multilingual machine translation, yet\nhas not been studied for speech translation. We attempt to build zero-shot ST\nmodels that are trained only on ASR and MT tasks but can do ST task during\ninference. The challenge is that the representation of text and audio is\nsignificantly different, thus the models learn ASR and MT tasks in different\nways, making it non-trivial to perform zero-shot. These models tend to output\nthe wrong language when performing zero-shot ST. We tackle the issues by\nincluding additional training data and an auxiliary loss function that\nminimizes the text-audio difference. Our experiment results and analysis show\nthat the methods are promising for zero-shot ST. Moreover, our methods are\nparticularly useful in the few-shot settings where a limited amount of ST data\nis available, with improvements of up to +11.8 BLEU points compared to direct\nend-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from\npre-trained ASR model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 12:00:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Dinh", "Tu Anh", ""]]}, {"id": "2107.06051", "submitter": "Guojun Wu", "authors": "Guojun Wu", "title": "Rating Facts under Coarse-to-fine Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of manipulating fake news as a political weapon has become a global\nconcern and highlighted the incapability of manually fact checking against\nrapidly produced fake news. Thus, statistical approaches are required if we are\nto address this problem efficiently. The shortage of publicly available\ndatasets is one major bottleneck of automated fact checking. To remedy this, we\ncollected 24K manually rated statements from PolitiFact. The class values\nexhibit a natural order with respect to truthfulness as shown in Table 1. Thus,\nour task represents a twist from standard classification, due to the various\ndegrees of similarity between classes. To investigate this, we defined\ncoarse-to-fine classification regimes, which presents new challenge for\nclassification. To address this, we propose BERT-based models. After training,\nclass similarity is sensible over the multi-class datasets, especially in the\nfine-grained one. Under all the regimes, BERT achieves state of the art, while\nthe additional layers provide insignificant improvement.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:05:11 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 10:51:41 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wu", "Guojun", ""]]}, {"id": "2107.06055", "submitter": "Ahmet \\\"Ust\\\"un", "authors": "Arianna Bisazza, Ahmet \\\"Ust\\\"un, Stephan Sportel", "title": "On the Difficulty of Translating Free-Order Case-Marking Languages", "comments": "Accepted to TACL, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying factors that make certain languages harder to model than others\nis essential to reach language equality in future Natural Language Processing\ntechnologies. Free-order case-marking languages, such as Russian, Latin or\nTamil, have proved more challenging than fixed-order languages for the tasks of\nsyntactic parsing and subject-verb agreement prediction. In this work, we\ninvestigate whether this class of languages is also more difficult to translate\nby state-of-the-art Neural Machine Translation models (NMT). Using a variety of\nsynthetic languages and a newly introduced translation challenge set, we find\nthat word order flexibility in the source language only leads to a very small\nloss of NMT quality, even though the core verb arguments become impossible to\ndisambiguate in sentences without semantic cues. The latter issue is indeed\nsolved by the addition of case marking. However, in medium- and low-resource\nsettings, the overall NMT quality of fixed-order languages remains unmatched.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:09:58 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bisazza", "Arianna", ""], ["\u00dcst\u00fcn", "Ahmet", ""], ["Sportel", "Stephan", ""]]}, {"id": "2107.06056", "submitter": "Prathamesh Kalamkar", "authors": "Prathamesh Kalamkar, Janani Venugopalan Ph.D., Vivek Raghavan Ph.D", "title": "Indian Legal NLP Benchmarks : A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:10:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kalamkar", "Prathamesh", ""], ["D.", "Janani Venugopalan Ph.", ""], ["D", "Vivek Raghavan Ph.", ""]]}, {"id": "2107.06155", "submitter": "Hari Krishna Vydana Mr", "authors": "Hari Krishna Vydana, Martin Karafi'at, Luk'as Burget, \"Honza\"\n  Cernock'y", "title": "The IWSLT 2021 BUT Speech Translation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper describes BUT's English to German offline speech translation(ST)\nsystems developed for IWSLT2021. They are based on jointly trained Automatic\nSpeech Recognition-Machine Translation models. Their performances is evaluated\non MustC-Common test set. In this work, we study their efficiency from the\nperspective of having a large amount of separate ASR training data and MT\ntraining data, and a smaller amount of speech-translation training data. Large\namounts of ASR and MT training data are utilized for pre-training the ASR and\nMT models. Speech-translation data is used to jointly optimize ASR-MT models by\ndefining an end-to-end differentiable path from speech to translations. For\nthis purpose, we use the internal continuous representations from the\nASR-decoder as the input to MT module. We show that speech translation can be\nfurther improved by training the ASR-decoder jointly with the MT-module using\nlarge amount of text-only MT training data. We also show significant\nimprovements by training an ASR module capable of generating punctuated text,\nrather than leaving the punctuation task to the MT module.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:11:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Vydana", "Hari Krishna", ""], ["Karafi'at", "Martin", ""], ["Burget", "Luk'as", ""], ["Cernock'y", "\"Honza\"", ""]]}, {"id": "2107.06243", "submitter": "Moniba Keymanesh", "authors": "Moniba Keymanesh, Tanya Berger-Wolf, Micha Elsner, Srinivasan\n  Parthasarathy", "title": "Fairness-aware Summarization for Justified Decision-Making", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many applications such as recidivism prediction, facility inspection, and\nbenefit assignment, it's important for individuals to know the\ndecision-relevant information for the model's prediction. In addition, the\nmodel's predictions should be fairly justified. Essentially, decision-relevant\nfeatures should provide sufficient information for the predicted outcome and\nshould be independent of the membership of individuals in protected groups such\nas race and gender. In this work, we focus on the problem of (un)fairness in\nthe justification of the text-based neural models. We tie the explanatory power\nof the model to fairness in the outcome and propose a fairness-aware\nsummarization mechanism to detect and counteract the bias in such models. Given\na potentially biased natural language explanation for a decision, we use a\nmulti-task neural model and an attribution mechanism based on integrated\ngradients to extract the high-utility and discrimination-free justifications in\nthe form of a summary. The extracted summary is then used for training a model\nto make decisions for individuals. Results on several real-world datasets\nsuggests that our method: (i) assists users to understand what information is\nused for the model's decision and (ii) enhances the fairness in outcomes while\nsignificantly reducing the demographic leakage.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:04:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Keymanesh", "Moniba", ""], ["Berger-Wolf", "Tanya", ""], ["Elsner", "Micha", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "2107.06246", "submitter": "Alina Karakanta", "authors": "Alina Karakanta, Marco Gaido, Matteo Negri, Marco Turchi", "title": "Between Flexibility and Consistency: Joint Generation of Captions and\n  Subtitles", "comments": "Accepted at IWSLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Speech translation (ST) has lately received growing interest for the\ngeneration of subtitles without the need for an intermediate source language\ntranscription and timing (i.e. captions). However, the joint generation of\nsource captions and target subtitles does not only bring potential output\nquality advantages when the two decoding processes inform each other, but it is\nalso often required in multilingual scenarios. In this work, we focus on ST\nmodels which generate consistent captions-subtitles in terms of structure and\nlexical content. We further introduce new metrics for evaluating subtitling\nconsistency. Our findings show that joint decoding leads to increased\nperformance and consistency between the generated captions and subtitles while\nstill allowing for sufficient flexibility to produce subtitles conforming to\nlanguage-specific needs and norms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:06:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Karakanta", "Alina", ""], ["Gaido", "Marco", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2107.06310", "submitter": "Zining Zhu", "authors": "Zining Zhu, Bai Li, Yang Xu, Frank Rudzicz", "title": "What do writing features tell us about AI papers?", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the numbers of submissions to conferences grow quickly, the task of\nassessing the quality of academic papers automatically, convincingly, and with\nhigh accuracy attracts increasing attention. We argue that studying\ninterpretable dimensions of these submissions could lead to scalable solutions.\nWe extract a collection of writing features, and construct a suite of\nprediction tasks to assess the usefulness of these features in predicting\ncitation counts and the publication of AI-related papers. Depending on the\nvenues, the writing features can predict the conference vs. workshop appearance\nwith F1 scores up to 60-90, sometimes even outperforming the content-based\ntf-idf features and RoBERTa. We show that the features describe writing style\nmore than content. To further understand the results, we estimate the causal\nimpact of the most indicative features. Our analysis on writing features\nprovides a perspective to assessing and refining the writing of academic\narticles at scale.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 18:12:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhu", "Zining", ""], ["Li", "Bai", ""], ["Xu", "Yang", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2107.06383", "submitter": "Liunian Harold Li", "authors": "Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach,\n  Kai-Wei Chang, Zhewei Yao, Kurt Keutzer", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 20:48:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shen", "Sheng", ""], ["Li", "Liunian Harold", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""], ["Rohrbach", "Anna", ""], ["Chang", "Kai-Wei", ""], ["Yao", "Zhewei", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2107.06400", "submitter": "Sergio Rojas-Galeano", "authors": "Sergio Rojas-Galeano", "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 21:17:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Rojas-Galeano", "Sergio", ""]]}, {"id": "2107.06426", "submitter": "Apurba Nath Mr", "authors": "Apurba Nath, Aayush Kubba", "title": "TSCAN : Dialog Structure discovery using SCAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we discover dialog structure by dividing utterances into labelled\nclusters. Can these labels be generated from the data. Typically for dialogs we\nneed an ontology and use that to discover structure, however by using\nunsupervised classification and self-labelling we are able to intuit this\nstructure without any labels or ontology. In this paper we apply SCAN (Semantic\nClustering using Nearest Neighbors) to dialog data. We used BERT for pretext\ntask and an adaptation of SCAN for clustering and self labeling. These clusters\nare used to identify transition probabilities and create the dialog structure.\nThe self-labelling method used for SCAN makes these structures interpretable as\nevery cluster has a label. As the approach is unsupervised, evaluation metrics\nis a challenge, we use statistical measures as proxies for structure quality\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 22:55:07 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 14:30:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nath", "Apurba", ""], ["Kubba", "Aayush", ""]]}, {"id": "2107.06472", "submitter": "Bei Yu", "authors": "Jun Wang, Bei Yu", "title": "Linking Health News to Research Literature", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 03:50:51 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Bei", ""]]}, {"id": "2107.06483", "submitter": "Ishan Tarunesh", "authors": "Ishan Tarunesh, Syamantak Kumar, Preethi Jyothi", "title": "From Machine Translation to Code-Switching: Generating High-Quality\n  Code-Switched Text", "comments": "In Proceedings of The Joint Conference of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International\n  Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating code-switched text is a problem of growing interest, especially\ngiven the scarcity of corpora containing large volumes of real code-switched\ntext. In this work, we adapt a state-of-the-art neural machine translation\nmodel to generate Hindi-English code-switched sentences starting from\nmonolingual Hindi sentences. We outline a carefully designed curriculum of\npretraining steps, including the use of synthetic code-switched text, that\nenable the model to generate high-quality code-switched text. Using text\ngenerated from our model as data augmentation, we show significant reductions\nin perplexity on a language modeling task, compared to using text from other\ngenerative models of CS text. We also show improvements using our text for a\ndownstream code-switched natural language inference task. Our generated text is\nfurther subjected to a rigorous evaluation using a human evaluation study and a\nrange of objective metrics, where we show performance comparable (and sometimes\neven superior) to code-switched text obtained via crowd workers who are native\nHindi speakers.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 04:46:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tarunesh", "Ishan", ""], ["Kumar", "Syamantak", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2107.06493", "submitter": "Hongning Zhu", "authors": "Hongning Zhu, Kong Aik Lee, Haizhou Li", "title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding", "comments": "Accepted by Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a serialized multi-layer multi-head attention for neural\nspeaker embedding in text-independent speaker verification. In prior works,\nframe-level features from one layer are aggregated to form an utterance-level\nrepresentation. Inspired by the Transformer network, our proposed method\nutilizes the hierarchical architecture of stacked self-attention mechanisms to\nderive refined features that are more correlated with speakers. Serialized\nattention mechanism contains a stack of self-attention modules to create\nfixed-dimensional representations of speakers. Instead of utilizing multi-head\nattention in parallel, the proposed serialized multi-layer multi-head attention\nis designed to aggregate and propagate attentive statistics from one layer to\nthe next in a serialized manner. In addition, we employ an input-aware query\nfor each utterance with the statistics pooling. With more layers stacked, the\nneural network can learn more discriminative speaker embeddings. Experiment\nresults on VoxCeleb1 dataset and SITW dataset show that our proposed method\noutperforms other baseline methods, including x-vectors and other x-vectors +\nconventional attentive pooling approaches by 9.7% in EER and 8.1% in DCF0.01.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:38:48 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhu", "Hongning", ""], ["Lee", "Kong Aik", ""], ["Li", "Haizhou", ""]]}, {"id": "2107.06499", "submitter": "Daphne Ippolito", "authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas\n  Eck, Chris Callison-Burch, Nicholas Carlini", "title": "Deduplicating Training Data Makes Language Models Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 06:06:52 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lee", "Katherine", ""], ["Ippolito", "Daphne", ""], ["Nystrom", "Andrew", ""], ["Zhang", "Chiyuan", ""], ["Eck", "Douglas", ""], ["Callison-Burch", "Chris", ""], ["Carlini", "Nicholas", ""]]}, {"id": "2107.06516", "submitter": "Shengnan An", "authors": "Chenyao Liu, Shengnan An, Zeqi Lin, Qian Liu, Bei Chen, Jian-Guang\n  Lou, Lijie Wen, Nanning Zheng and Dongmei Zhang", "title": "Learning Algebraic Recombination for Compositional Generalization", "comments": "ACL Findings 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence models exhibit limited compositional generalization ability\nin semantic parsing tasks. Compositional generalization requires algebraic\nrecombination, i.e., dynamically recombining structured expressions in a\nrecursive manner. However, most previous studies mainly concentrate on\nrecombining lexical units, which is an important but not sufficient part of\nalgebraic recombination. In this paper, we propose LeAR, an end-to-end neural\nmodel to learn algebraic recombination for compositional generalization. The\nkey insight is to model the semantic parsing task as a homomorphism between a\nlatent syntactic algebra and a semantic algebra, thus encouraging algebraic\nrecombination. Specifically, we learn two modules jointly: a Composer for\nproducing latent syntax, and an Interpreter for assigning semantic operations.\nExperiments on two realistic and comprehensive compositional generalization\nbenchmarks demonstrate the effectiveness of our model. The source code is\npublicly available at https://github.com/microsoft/ContextualSP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 07:23:46 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Chenyao", ""], ["An", "Shengnan", ""], ["Lin", "Zeqi", ""], ["Liu", "Qian", ""], ["Chen", "Bei", ""], ["Lou", "Jian-Guang", ""], ["Wen", "Lijie", ""], ["Zheng", "Nanning", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2107.06546", "submitter": "Bertrand Higy", "authors": "Afra Alishahi, Grzegorz Chrupa{\\l}a, Alejandrina Cristia, Emmanuel\n  Dupoux, Bertrand Higy, Marvin Lavechin, Okko R\\\"as\\\"anen and Chen Yu", "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language\n  Modelling track, 2021 edition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the visually-grounded language modelling track that was introduced\nin the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the\nnew track and discuss participation rules in detail. We also present the two\nbaseline systems that were developed for this track.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:29:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Alishahi", "Afra", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Cristia", "Alejandrina", ""], ["Dupoux", "Emmanuel", ""], ["Higy", "Bertrand", ""], ["Lavechin", "Marvin", ""], ["R\u00e4s\u00e4nen", "Okko", ""], ["Yu", "Chen", ""]]}, {"id": "2107.06569", "submitter": "Wanying Xie", "authors": "Wanying Xie, Yang Feng, Shuhao Gu, Dong Yu", "title": "Importance-based Neuron Allocation for Multilingual Neural Machine\n  Translation", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual neural machine translation with a single model has drawn much\nattention due to its capability to deal with multiple languages. However, the\ncurrent multilingual translation paradigm often makes the model tend to\npreserve the general knowledge, but ignore the language-specific knowledge.\nSome previous works try to solve this problem by adding various kinds of\nlanguage-specific modules to the model, but they suffer from the parameter\nexplosion problem and require specialized manual design. To solve these\nproblems, we propose to divide the model neurons into general and\nlanguage-specific parts based on their importance across languages. The general\npart is responsible for preserving the general knowledge and participating in\nthe translation of all the languages, while the language-specific part is\nresponsible for preserving the language-specific knowledge and participating in\nthe translation of some specific languages. Experimental results on several\nlanguage pairs, covering IWSLT and Europarl corpus datasets, demonstrate the\neffectiveness and universality of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 09:15:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Xie", "Wanying", ""], ["Feng", "Yang", ""], ["Gu", "Shuhao", ""], ["Yu", "Dong", ""]]}, {"id": "2107.06632", "submitter": "Ayyoob Imani Googhari", "authors": "Ayyoob Imani, Masoud Jalili Sabet, Philipp Dufter, Michael Cysouw,\n  Hinrich Sch\\\"utze", "title": "ParCourE: A Parallel Corpus Explorer for a Massively Multilingual Corpus", "comments": "The Joint Conference of the 59th Annual Meeting of the Association\n  for Computational Linguistics and the 10th International Joint Conference on\n  Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more than 7000 languages worldwide, multilingual natural language\nprocessing (NLP) is essential both from an academic and commercial perspective.\nResearching typological properties of languages is fundamental for progress in\nmultilingual NLP. Examples include assessing language similarity for effective\ntransfer learning, injecting inductive biases into machine learning models or\ncreating resources such as dictionaries and inflection tables. We provide\nParCourE, an online tool that allows to browse a word-aligned parallel corpus,\ncovering 1334 languages. We give evidence that this is useful for typological\nresearch. ParCourE can be set up for any parallel corpus and can thus be used\nfor typological research on other corpora as well as for exploring their\nquality and properties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:16:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 08:23:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Imani", "Ayyoob", ""], ["Sabet", "Masoud Jalili", ""], ["Dufter", "Philipp", ""], ["Cysouw", "Michael", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2107.06751", "submitter": "Guillaume Cabanac", "authors": "Guillaume Cabanac and Cyril Labb\\'e and Alexander Magazinov", "title": "Tortured phrases: A dubious writing style emerging in science. Evidence\n  of critical issues affecting established journals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 20:47:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Cabanac", "Guillaume", ""], ["Labb\u00e9", "Cyril", ""], ["Magazinov", "Alexander", ""]]}, {"id": "2107.06776", "submitter": "Alexis Toumi", "authors": "Bob Coecke, Giovanni de Felice, Konstantinos Meichanetzidis, Alexis\n  Toumi", "title": "How to make qubits speak", "comments": "Invited contribution to \"Quantum Computing in the Arts and\n  Humanities\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a story about making quantum computers speak, and doing so in a\nquantum-native, compositional and meaning-aware manner. Recently we did\nquestion-answering with an actual quantum computer. We explain what we did,\nstress that this was all done in terms of pictures, and provide many pointers\nto the related literature. In fact, besides natural language, many other things\ncan be implemented in a quantum-native, compositional and meaning-aware manner,\nand we provide the reader with some indications of that broader pictorial\nlandscape, including our account on the notion of compositionality. We also\nprovide some guidance for the actual execution, so that the reader can give it\na go as well.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:34:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Coecke", "Bob", ""], ["de Felice", "Giovanni", ""], ["Meichanetzidis", "Konstantinos", ""], ["Toumi", "Alexis", ""]]}, {"id": "2107.06779", "submitter": "Jingwen Hu", "authors": "Jingwen Hu, Yuchen Liu, Jinming Zhao, Qin Jin", "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion\n  Recognition in Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition in conversation (ERC) is a crucial component in affective\ndialogue systems, which helps the system understand users' emotions and\ngenerate empathetic responses. However, most works focus on modeling speaker\nand contextual information primarily on the textual modality or simply\nleveraging multimodal information through feature concatenation. In order to\nexplore a more effective way of utilizing both multimodal and long-distance\ncontextual information, we propose a new model based on multimodal fused graph\nconvolutional network, MMGCN, in this work. MMGCN can not only make use of\nmultimodal dependencies effectively, but also leverage speaker information to\nmodel inter-speaker and intra-speaker dependency. We evaluate our proposed\nmodel on two public benchmark datasets, IEMOCAP and MELD, and the results prove\nthe effectiveness of MMGCN, which outperforms other SOTA methods by a\nsignificant margin under the multimodal conversation setting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:37:02 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hu", "Jingwen", ""], ["Liu", "Yuchen", ""], ["Zhao", "Jinming", ""], ["Jin", "Qin", ""]]}, {"id": "2107.06785", "submitter": "Novanto Yudistira", "authors": "Kuncahyo Setyo Nugroho, Anantha Yullian Sukmadewa, Novanto Yudistira", "title": "Large-Scale News Classification using BERT Language Model: Spark NLP\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:42:15 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 02:04:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Nugroho", "Kuncahyo Setyo", ""], ["Sukmadewa", "Anantha Yullian", ""], ["Yudistira", "Novanto", ""]]}, {"id": "2107.06796", "submitter": "Novanto Yudistira", "authors": "Aisyah Awalina, Jibran Fawaid, Rifky Yunus Krisnabayu, Novanto\n  Yudistira", "title": "Indonesia's Fake News Detection using Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news is a problem faced by society in this era. It is not rare for fake\nnews to cause provocation and problem for the people. Indonesia, as a country\nwith the 4th largest population, has a problem in dealing with fake news. More\nthan 30% of rural and urban population are deceived by this fake news problem.\nAs we have been studying, there is only few literatures on preventing the\nspread of fake news in Bahasa Indonesia. So, this research is conducted to\nprevent these problems. The dataset used in this research was obtained from a\nnews portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on\nthis page, we got 1116 data consisting of valid news and fake news. The dataset\ncan be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This\ndataset will be combined with other available datasets. The methods used are\nCNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This\nresearch shows that the BERT method with Transformer Network has the best\nresults with an accuracy of up to 90%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:52:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Awalina", "Aisyah", ""], ["Fawaid", "Jibran", ""], ["Krisnabayu", "Rifky Yunus", ""], ["Yudistira", "Novanto", ""]]}, {"id": "2107.06802", "submitter": "Novanto Yudistira", "authors": "Kuncahyo Setyo Nugroho, Anantha Yullian Sukmadewa, Haftittah\n  Wuswilahaken DW, Fitra Abdurrachman Bachtiar, Novanto Yudistira", "title": "BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps\n  Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User reviews have an essential role in the success of the developed mobile\napps. User reviews in the textual form are unstructured data, creating a very\nhigh complexity when processed for sentiment analysis. Previous approaches that\nhave been used often ignore the context of reviews. In addition, the relatively\nsmall data makes the model overfitting. A new approach, BERT, has been\nintroduced as a transfer learning model with a pre-trained model that has\npreviously been trained to have a better context representation. This study\nexamines the effectiveness of fine-tuning BERT for sentiment analysis using two\ndifferent pre-trained models. Besides the multilingual pre-trained model, we\nuse the pre-trained model that only has been trained in Indonesian. The dataset\nused is Indonesian user reviews of the ten best apps in 2020 in Google Play\nsites. We also perform hyper-parameter tuning to find the optimum trained\nmodel. Two training data labeling approaches were also tested to determine the\neffectiveness of the model, which is score-based and lexicon-based. The\nexperimental results show that pre-trained models trained in Indonesian have\nbetter average accuracy on lexicon-based data. The pre-trained Indonesian model\nhighest accuracy is 84%, with 25 epochs and a training time of 24 minutes.\nThese results are better than all of the machine learning and multilingual\npre-trained models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:00:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Nugroho", "Kuncahyo Setyo", ""], ["Sukmadewa", "Anantha Yullian", ""], ["DW", "Haftittah Wuswilahaken", ""], ["Bachtiar", "Fitra Abdurrachman", ""], ["Yudistira", "Novanto", ""]]}, {"id": "2107.06820", "submitter": "Razin A. Shaikh", "authors": "Razin A. Shaikh and Lia Yeh and Benjamin Rodatz and Bob Coecke", "title": "Composing Conversational Negation", "comments": "14 pages, many figures, In Proceedings ACT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negation in natural language does not follow Boolean logic and is therefore\ninherently difficult to model. In particular, it takes into account the broader\nunderstanding of what is being negated. In previous work, we proposed a\nframework for negation of words that accounts for `worldly context'. In this\npaper, we extend that proposal now accounting for the compositional structure\ninherent in language, within the DisCoCirc framework. We compose the negations\nof single words to capture the negation of sentences. We also describe how to\nmodel the negation of words whose meanings evolve in the text.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:24:41 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shaikh", "Razin A.", ""], ["Yeh", "Lia", ""], ["Rodatz", "Benjamin", ""], ["Coecke", "Bob", ""]]}, {"id": "2107.06876", "submitter": "Johannes Klicpera", "authors": "Johannes Klicpera, Marten Lienen, Stephan G\\\"unnemann", "title": "Scalable Optimal Transport in High Dimensions for Graph Distances,\n  Embedding Alignment, and More", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DS cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:40:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Klicpera", "Johannes", ""], ["Lienen", "Marten", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2107.06886", "submitter": "Sujeong Kim", "authors": "Sujeong Kim, Amir Tamrakar", "title": "\"How to best say it?\" : Translating Directives in Machine Language into\n  Natural Language in the Blocks World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to generate optimal natural language for block placement\ndirectives generated by a machine's planner during human-agent interactions in\nthe blocks world. A non user-friendly machine directive, e.g., move(ObjId,\ntoPos), is transformed into visually and contextually grounded referring\nexpressions that are much easier for the user to comprehend. We describe an\nalgorithm that progressively and generatively transforms the machine's\ndirective in ECI (Elementary Composable Ideas)-space, generating many\nalternative versions of the directive. We then define a cost function to\nevaluate the ease of comprehension of these alternatives and select the best\noption. The parameters for this cost function were derived empirically from a\nuser study that measured utterance-to-action timings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:59:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kim", "Sujeong", ""], ["Tamrakar", "Amir", ""]]}, {"id": "2107.06905", "submitter": "Tianze Shi", "authors": "Tianze Shi, Lillian Lee", "title": "Transition-based Bubble Parsing: Improvements on Coordination Structure\n  Prediction", "comments": "ACL 2021", "journal-ref": "In Proceedings of ACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transition-based bubble parser to perform coordination structure\nidentification and dependency-based syntactic analysis simultaneously. Bubble\nrepresentations were proposed in the formal linguistics literature decades ago;\nthey enhance dependency trees by encoding coordination boundaries and internal\nrelationships within coordination structures explicitly. In this paper, we\nintroduce a transition system and neural models for parsing these\nbubble-enhanced structures. Experimental results on the English Penn Treebank\nand the English GENIA corpus show that our parsers beat previous\nstate-of-the-art approaches on the task of coordination structure prediction,\nespecially for the subset of sentences with complex coordination structures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:00:05 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Shi", "Tianze", ""], ["Lee", "Lillian", ""]]}, {"id": "2107.06907", "submitter": "Tianze Shi", "authors": "Tianze Shi, Lillian Lee", "title": "TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage\n  Generic- to Individual-Language Finetuning", "comments": "IWPT 2021 Shared Task", "journal-ref": "Proceedings of IWPT 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our contribution to the IWPT 2021 shared task on parsing into\nenhanced Universal Dependencies. Our main system component is a hybrid\ntree-graph parser that integrates (a) predictions of spanning trees for the\nenhanced graphs with (b) additional graph edges not present in the spanning\ntrees. We also adopt a finetuning strategy where we first train a\nlanguage-generic parser on the concatenation of data from all available\nlanguages, and then, in a second step, finetune on each individual language\nseparately. Additionally, we develop our own complete set of pre-processing\nmodules relevant to the shared task, including tokenization, sentence\nsegmentation, and multiword token expansion, based on pre-trained XLM-R models\nand our own pre-training of character-level language models. Our submission\nreaches a macro-average ELAS of 89.24 on the test set. It ranks top among all\nteams, with a margin of more than 2 absolute ELAS over the next best-performing\nsubmission, and best score on 16 out of 17 languages.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:00:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Shi", "Tianze", ""], ["Lee", "Lillian", ""]]}, {"id": "2107.06912", "submitter": "Marcella Cornia", "authors": "Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia\n  Cascianelli, Giuseppe Fiameni, Rita Cucchiara", "title": "From Show to Tell: A Survey on Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:00:54 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Stefanini", "Matteo", ""], ["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cascianelli", "Silvia", ""], ["Fiameni", "Giuseppe", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2107.06955", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu,\n  Gargi Ghosh, Luke Zettlemoyer", "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:39:31 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Aghajanyan", "Armen", ""], ["Okhonko", "Dmytro", ""], ["Lewis", "Mike", ""], ["Joshi", "Mandar", ""], ["Xu", "Hu", ""], ["Ghosh", "Gargi", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2107.06959", "submitter": "Yun Tang", "authors": "Yun Tang, Hongyu Gong, Xian Li, Changhan Wang, Juan Pino, Holger\n  Schwenk, Naman Goyal", "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual\n  Shared Task", "comments": "Accepted by IWSLT 2021 as a system paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n  In some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:43:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tang", "Yun", ""], ["Gong", "Hongyu", ""], ["Li", "Xian", ""], ["Wang", "Changhan", ""], ["Pino", "Juan", ""], ["Schwenk", "Holger", ""], ["Goyal", "Naman", ""]]}, {"id": "2107.06963", "submitter": "Hannah Rashkin", "authors": "Hannah Rashkin, David Reitter, Gaurav Singh Tomar, Dipanjan Das", "title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable\n  Features", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge-grounded dialogue systems are intended to convey information that\nis based on evidence provided in a given source text. We discuss the challenges\nof training a generative neural dialogue model for such systems that is\ncontrolled to stay faithful to the evidence. Existing datasets contain a mix of\nconversational responses that are faithful to selected evidence as well as more\nsubjective or chit-chat style responses. We propose different evaluation\nmeasures to disentangle these different styles of responses by quantifying the\ninformativeness and objectivity. At training time, additional inputs based on\nthese evaluation measures are given to the dialogue model. At generation time,\nthese additional inputs act as stylistic controls that encourage the model to\ngenerate responses that are faithful to the provided evidence. We also\ninvestigate the usage of additional controls at decoding time using resampling\ntechniques. In addition to automatic metrics, we perform a human evaluation\nstudy where raters judge the output of these controlled generation models to be\ngenerally more objective and faithful to the evidence compared to baseline\ndialogue systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:52:12 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Rashkin", "Hannah", ""], ["Reitter", "David", ""], ["Tomar", "Gaurav Singh", ""], ["Das", "Dipanjan", ""]]}, {"id": "2107.06990", "submitter": "Tazin Afrin", "authors": "Tazin Afrin, Elaine Wang, Diane Litman, Lindsay C. Matsumura, Richard\n  Correnti", "title": "Annotation and Classification of Evidence and Reasoning Revisions in\n  Argumentative Writing", "comments": "10 pages, 11 tables, 15th Workshop on Innovative Use of NLP for\n  Building Educational Applications", "journal-ref": null, "doi": "10.18653/v1/2020.bea-1.7", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 20:58:26 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Afrin", "Tazin", ""], ["Wang", "Elaine", ""], ["Litman", "Diane", ""], ["Matsumura", "Lindsay C.", ""], ["Correnti", "Richard", ""]]}, {"id": "2107.07002", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, Zhe Zhao, Neil Houlsby,\n  Fernando Diaz, Donald Metzler, Oriol Vinyals", "title": "The Benchmark Lottery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 21:08:30 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Tay", "Yi", ""], ["Gritsenko", "Alexey A.", ""], ["Zhao", "Zhe", ""], ["Houlsby", "Neil", ""], ["Diaz", "Fernando", ""], ["Metzler", "Donald", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2107.07113", "submitter": "Zitao Liu", "authors": "Guowei Xu, Wenbiao Ding, Weiping Fu, Zhongqin Wu, Zitao Liu", "title": "Robust Learning for Text Classification with Multi-source Noise\n  Simulation and Hard Example Mining", "comments": "ECML-PKDD'21: The European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications involve the use of Optical Character Recognition\n(OCR) engines to transform handwritten images into transcripts on which\ndownstream Natural Language Processing (NLP) models are applied. In this\nprocess, OCR engines may introduce errors and inputs to downstream NLP models\nbecome noisy. Despite that pre-trained models achieve state-of-the-art\nperformance in many NLP benchmarks, we prove that they are not robust to noisy\ntexts generated by real OCR engines. This greatly limits the application of NLP\nmodels in real-world scenarios. In order to improve model performance on noisy\nOCR transcripts, it is natural to train the NLP model on labelled noisy texts.\nHowever, in most cases there are only labelled clean texts. Since there is no\nhandwritten pictures corresponding to the text, it is impossible to directly\nuse the recognition model to obtain noisy labelled data. Human resources can be\nemployed to copy texts and take pictures, but it is extremely expensive\nconsidering the size of data for model training. Consequently, we are\ninterested in making NLP models intrinsically robust to OCR errors in a low\nresource manner. We propose a novel robust training framework which 1) employs\nsimple but effective methods to directly simulate natural OCR noises from clean\ntexts and 2) iteratively mines the hard examples from a large number of\nsimulated samples for optimal performance. 3) To make our model learn\nnoise-invariant representations, a stability loss is employed. Experiments on\nthree real-world datasets show that the proposed framework boosts the\nrobustness of pre-trained models by a large margin. We believe that this work\ncan greatly promote the application of NLP models in actual scenarios, although\nthe algorithm we use is simple and straightforward. We make our codes and three\ndatasets publicly\navailable\\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 04:39:22 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Guowei", ""], ["Ding", "Wenbiao", ""], ["Fu", "Weiping", ""], ["Wu", "Zhongqin", ""], ["Liu", "Zitao", ""]]}, {"id": "2107.07119", "submitter": "Zitao Liu", "authors": "Yang Hao, Hang Li, Wenbiao Ding, Zhongqin Wu, Jiliang Tang, Rose\n  Luckin, Zitao Liu", "title": "Multi-Task Learning based Online Dialogic Instruction Detection with\n  Pre-trained Language Models", "comments": "AIED'21: The 22nd International Conference on Artificial Intelligence\n  in Education, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study computational approaches to detect online dialogic\ninstructions, which are widely used to help students understand learning\nmaterials, and build effective study habits. This task is rather challenging\ndue to the widely-varying quality and pedagogical styles of dialogic\ninstructions. To address these challenges, we utilize pre-trained language\nmodels, and propose a multi-task paradigm which enhances the ability to\ndistinguish instances of different classes by enlarging the margin between\ncategories via contrastive loss. Furthermore, we design a strategy to fully\nexploit the misclassified examples during the training stage. Extensive\nexperiments on a real-world online educational data set demonstrate that our\napproach achieves superior performance compared to representative baselines. To\nencourage reproducible results, we make our implementation online available at\n\\url{https://github.com/AIED2021/multitask-dialogic-instruction}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 04:57:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Hao", "Yang", ""], ["Li", "Hang", ""], ["Ding", "Wenbiao", ""], ["Wu", "Zhongqin", ""], ["Tang", "Jiliang", ""], ["Luckin", "Rose", ""], ["Liu", "Zitao", ""]]}, {"id": "2107.07122", "submitter": "Zitao Liu", "authors": "Qiongqiong Liu, Tianqiao Liu, Jiafu Zhao, Qiang Fang, Wenbiao Ding,\n  Zhongqin Wu, Feng Xia, Jiliang Tang, Zitao Liu", "title": "Solving ESL Sentence Completion Questions via Pre-trained Neural\n  Language Models", "comments": "AIED'21: The 22nd International Conference on Artificial Intelligence\n  in Education, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL) and building computational approaches to automatically solve such\nquestions is beneficial to language learners. In this work, we propose a neural\nframework to solve SC questions in English examinations by utilizing\npre-trained language models. We conduct extensive experiments on a real-world\nK-12 ESL SC question dataset and the results demonstrate the superiority of our\nmodel in terms of prediction accuracy. Furthermore, we run precision-recall\ntrade-off analysis to discuss the practical issues when deploying it in\nreal-life scenarios. To encourage reproducible results, we make our code\npublicly available at \\url{https://github.com/AIED2021/ESL-SentenceCompletion}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:01:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Liu", "Qiongqiong", ""], ["Liu", "Tianqiao", ""], ["Zhao", "Jiafu", ""], ["Fang", "Qiang", ""], ["Ding", "Wenbiao", ""], ["Wu", "Zhongqin", ""], ["Xia", "Feng", ""], ["Tang", "Jiliang", ""], ["Liu", "Zitao", ""]]}, {"id": "2107.07150", "submitter": "Tongshuang Wu", "authors": "Alexis Ross, Tongshuang Wu, Hao Peng, Matthew E. Peters, Matt Gardner", "title": "Tailor: Generating and Perturbing Text with Semantic Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Making controlled perturbations is essential for various tasks (e.g., data\naugmentation), but building task-specific generators can be expensive. We\nintroduce Tailor, a task-agnostic generation system that perturbs text in a\nsemantically-controlled way. With unlikelihood training, we design Tailor's\ngenerator to follow a series of control codes derived from semantic roles.\nThrough modifications of these control codes, Tailor can produce fine-grained\nperturbations. We implement a set of operations on control codes that can be\ncomposed into complex perturbation strategies, and demonstrate their\neffectiveness in three distinct applications: First, Tailor facilitates the\nconstruction of high-quality contrast sets that are lexically diverse, and less\nbiased than original task test data. Second, paired with automated labeling\nheuristics, Tailor helps improve model generalization through data\naugmentation: We obtain an average gain of 1.73 on an NLI challenge set by\nperturbing just 5% of training data. Third, without any finetuning overhead,\nTailor's perturbations effectively improve compositionality in fine-grained\nstyle transfer, outperforming fine-tuned baselines on 6 transfers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 06:38:59 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ross", "Alexis", ""], ["Wu", "Tongshuang", ""], ["Peng", "Hao", ""], ["Peters", "Matthew E.", ""], ["Gardner", "Matt", ""]]}, {"id": "2107.07170", "submitter": "Jonathan Bragg", "authors": "Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy", "title": "FLEX: Unifying Evaluation for Few-Shot NLP", "comments": "First two authors contributed equally. Code and leaderboard available\n  at: https://github.com/allenai/flex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:37:06 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bragg", "Jonathan", ""], ["Cohan", "Arman", ""], ["Lo", "Kyle", ""], ["Beltagy", "Iz", ""]]}, {"id": "2107.07223", "submitter": "Candy Olivia Mawalim", "authors": "Candy Olivia Mawalim and Masashi Unoki", "title": "Improving Security in McAdams Coefficient-Based Speaker Anonymization by\n  Watermarking Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speaker anonymization aims to suppress speaker individuality to protect\nprivacy in speech while preserving the other aspects, such as speech content.\nOne effective solution for anonymization is to modify the McAdams coefficient.\nIn this work, we propose a method to improve the security for speaker\nanonymization based on the McAdams coefficient by using a speech watermarking\napproach. The proposed method consists of two main processes: one for embedding\nand one for detection. In embedding process, two different McAdams coefficients\nrepresent binary bits ``0\" and ``1\". The watermarked speech is then obtained by\nframe-by-frame bit inverse switching. Subsequently, the detection process is\ncarried out by a power spectrum comparison. We conducted objective evaluations\nwith reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech\nwatermarking with reference to the Information Hiding Challenge (IHC) and found\nthat our method could satisfy the blind detection, inaudibility, and robustness\nrequirements in watermarking. It also significantly improved the anonymization\nperformance in comparison to the secondary baseline system in VP2020.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 09:56:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Mawalim", "Candy Olivia", ""], ["Unoki", "Masashi", ""]]}, {"id": "2107.07253", "submitter": "Asier Guti\\'errez-Fandi\\~no", "authors": "Asier Guti\\'errez-Fandi\\~no, Jordi Armengol-Estap\\'e, Marc P\\`amies,\n  Joan Llop-Palao, Joaqu\\'in Silveira-Ocampo, Casimiro Pio Carrino, Aitor\n  Gonzalez-Agirre, Carme Armentano-Oller, Carlos Rodriguez-Penagos, Marta\n  Villegas", "title": "Spanish Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:23:05 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Guti\u00e9rrez-Fandi\u00f1o", "Asier", ""], ["Armengol-Estap\u00e9", "Jordi", ""], ["P\u00e0mies", "Marc", ""], ["Llop-Palao", "Joan", ""], ["Silveira-Ocampo", "Joaqu\u00edn", ""], ["Carrino", "Casimiro Pio", ""], ["Gonzalez-Agirre", "Aitor", ""], ["Armentano-Oller", "Carme", ""], ["Rodriguez-Penagos", "Carlos", ""], ["Villegas", "Marta", ""]]}, {"id": "2107.07261", "submitter": "Ori Yoran", "authors": "Ori Yoran, Alon Talmor, Jonathan Berant", "title": "Turning Tables: Generating Examples from Semi-structured Tables for\n  Endowing Language Models with Reasoning Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:37:14 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yoran", "Ori", ""], ["Talmor", "Alon", ""], ["Berant", "Jonathan", ""]]}, {"id": "2107.07402", "submitter": "Anirudh Gupta", "authors": "Anirudh Gupta, Harveen Singh Chadha, Priyanshi Shah, Neeraj Chimmwal,\n  Ankur Dhuriya, Rishabh Gaur, Vivek Raghavan", "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 15:42:43 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Gupta", "Anirudh", ""], ["Chadha", "Harveen Singh", ""], ["Shah", "Priyanshi", ""], ["Chimmwal", "Neeraj", ""], ["Dhuriya", "Ankur", ""], ["Gaur", "Rishabh", ""], ["Raghavan", "Vivek", ""]]}, {"id": "2107.07430", "submitter": "Daphne Ippolito", "authors": "Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, Ann Yuan", "title": "Wordcraft: a Human-AI Collaborative Editor for Story Writing", "comments": null, "journal-ref": "First Workshop on Bridging Human-Computer Interaction and Natural\n  Language Processing at EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural language models grow in effectiveness, they are increasingly being\napplied in real-world settings. However these applications tend to be limited\nin the modes of interaction they support. In this extended abstract, we propose\nWordcraft, an AI-assisted editor for story writing in which a writer and a\ndialog system collaborate to write a story. Our novel interface uses few-shot\nlearning and the natural affordances of conversation to support a variety of\ninteractions. Our editor provides a sandbox for writers to probe the boundaries\nof transformer-based language models and paves the way for future\nhuman-in-the-loop training pipelines and novel evaluation methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:18:27 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Coenen", "Andy", ""], ["Davis", "Luke", ""], ["Ippolito", "Daphne", ""], ["Reif", "Emily", ""], ["Yuan", "Ann", ""]]}, {"id": "2107.07445", "submitter": "Jiahui Gao", "authors": "Jiahui Gao, Hang Xu, Han shi, Xiaozhe Ren, Philip L.H. Yu, Xiaodan\n  Liang, Xin Jiang, Zhenguo Li", "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:46:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Gao", "Jiahui", ""], ["Xu", "Hang", ""], ["shi", "Han", ""], ["Ren", "Xiaozhe", ""], ["Yu", "Philip L. H.", ""], ["Liang", "Xiaodan", ""], ["Jiang", "Xin", ""], ["Li", "Zhenguo", ""]]}, {"id": "2107.07498", "submitter": "Liang  Xu", "authors": "Liang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei Zhang, Hu Yuan, Huilin\n  Xu, Guoao Wei, Xiang Pan, Hai Hu", "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark", "comments": "Work in Progress; 8 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods. This work first\nintroduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first\ncomprehensive small sample evaluation benchmark in Chinese. It includes nine\ntasks, ranging from single-sentence and sentence-pair classification tasks to\nmachine reading comprehension tasks. Given the high variance of the few-shot\nlearning performance, we provide multiple training/validation sets to\nfacilitate a more accurate and stable evaluation of few-shot modeling. An\nunlabeled training set with up to 20,000 additional samples per task is\nprovided, allowing researchers to explore better ways of using unlabeled\nsamples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning\nmethods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark.Our results show that: 1) all five few-shot\nlearning methods exhibit better performance than fine-tuning or zero-shot\nlearning; 2) among the five methods, PET is the best performing few-shot\nmethod; 3) few-shot learning performance is highly dependent on the specific\ntask. Our benchmark and code are available at\nhttps://github.com/CLUEbenchmark/FewCLUE\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:51:25 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Liang", ""], ["Lu", "Xiaojing", ""], ["Yuan", "Chenyang", ""], ["Zhang", "Xuanwei", ""], ["Yuan", "Hu", ""], ["Xu", "Huilin", ""], ["Wei", "Guoao", ""], ["Pan", "Xiang", ""], ["Hu", "Hai", ""]]}, {"id": "2107.07502", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu,\n  Leslie Chen, Peter Wu, Michelle A. Lee, Yuke Zhu, Ruslan Salakhutdinov,\n  Louis-Philippe Morency", "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning", "comments": "Code: https://github.com/pliang279/MultiBench and Website:\n  https://cmu-multicomp-lab.github.io/multibench/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:54:36 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Liang", "Paul Pu", ""], ["Lyu", "Yiwei", ""], ["Fan", "Xiang", ""], ["Wu", "Zetian", ""], ["Cheng", "Yun", ""], ["Wu", "Jason", ""], ["Chen", "Leslie", ""], ["Wu", "Peter", ""], ["Lee", "Michelle A.", ""], ["Zhu", "Yuke", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2107.07509", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Tatsuya Kawahara", "title": "VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording", "comments": "Accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose novel decoding algorithms to enable streaming\nautomatic speech recognition (ASR) on unsegmented long-form recordings without\nvoice activity detection (VAD), based on monotonic chunkwise attention (MoChA)\nwith an auxiliary connectionist temporal classification (CTC) objective. We\npropose a block-synchronous beam search decoding to take advantage of efficient\nbatched output-synchronous and low-latency input-synchronous searches. We also\npropose a VAD-free inference algorithm that leverages CTC probabilities to\ndetermine a suitable timing to reset the model states to tackle the\nvulnerability to long-form data. Experimental evaluations demonstrate that the\nblock-synchronous decoding achieves comparable accuracy to the\nlabel-synchronous one. Moreover, the VAD-free inference can recognize long-form\nspeech robustly for up to a few hours.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:59:10 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2107.07566", "submitter": "Jason  Weston", "authors": "Mojtaba Komeili, Kurt Shuster, Jason Weston", "title": "Internet-Augmented Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest store of continually updating knowledge on our planet can be\naccessed via internet search. In this work we study giving access to this\ninformation to conversational agents. Large language models, even though they\nstore an impressive amount of knowledge within their weights, are known to\nhallucinate facts when generating dialogue (Shuster et al., 2021); moreover,\nthose facts are frozen in time at the point of model training. In contrast, we\npropose an approach that learns to generate an internet search query based on\nthe context, and then conditions on the search results to finally generate a\nresponse, a method that can employ up-to-the-minute relevant information. We\ntrain and evaluate such models on a newly collected dataset of human-human\nconversations whereby one of the speakers is given access to internet search\nduring knowledgedriven discussions in order to ground their responses. We find\nthat search-query based access of the internet in conversation provides\nsuperior performance compared to existing approaches that either use no\naugmentation or FAISS-based retrieval (Lewis et al., 2020).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 19:00:35 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Komeili", "Mojtaba", ""], ["Shuster", "Kurt", ""], ["Weston", "Jason", ""]]}, {"id": "2107.07567", "submitter": "Jason  Weston", "authors": "Jing Xu, Arthur Szlam, Jason Weston", "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent improvements in open-domain dialogue models, state of the art\nmodels are trained and evaluated on short conversations with little context. In\ncontrast, the long-term conversation setting has hardly been studied. In this\nwork we collect and release a human-human dataset consisting of multiple chat\nsessions whereby the speaking partners learn about each other's interests and\ndiscuss the things they have learnt from past sessions. We show how existing\nmodels trained on existing datasets perform poorly in this long-term\nconversation setting in both automatic and human evaluations, and we study\nlong-context models that can perform much better. In particular, we find\nretrieval-augmented methods and methods with an ability to summarize and recall\nprevious conversations outperform the standard encoder-decoder architectures\ncurrently considered state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 19:01:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Xu", "Jing", ""], ["Szlam", "Arthur", ""], ["Weston", "Jason", ""]]}, {"id": "2107.07610", "submitter": "Zhao Meng", "authors": "Zhao Meng, Yihan Dong, Mrinmaya Sachan, Roger Wattenhofer", "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for\n  Robust Pretrained Language Models", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper improves the robustness of the pretrained language model BERT\nagainst word substitution-based adversarial attacks by leveraging\nself-supervised contrastive learning with adversarial perturbations. One\nadvantage of our method compared to previous works is that it is capable of\nimproving model robustness without using any labels. Additionally, we also\ncreate an adversarial attack for word-level adversarial training on BERT. The\nattack is efficient, allowing adversarial training for BERT on adversarial\nexamples generated on the fly during training. Experimental results on four\ndatasets show that our method improves the robustness of BERT against four\ndifferent word substitution-based adversarial attacks. Furthermore, to\nunderstand why our method can improve the model robustness against adversarial\nattacks, we study vector representations of clean examples and their\ncorresponding adversarial examples before and after applying our method. As our\nmethod improves model robustness with unlabeled raw data, it opens up the\npossibility of using large text datasets to train robust language models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 21:03:34 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Meng", "Zhao", ""], ["Dong", "Yihan", ""], ["Sachan", "Mrinmaya", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2107.07634", "submitter": "Takuya Higuchi", "authors": "Takuya Higuchi, Anmol Gupta, Chandra Dhir", "title": "Multi-task Learning with Cross Attention for Keyword Spotting", "comments": "Submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 22:38:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Higuchi", "Takuya", ""], ["Gupta", "Anmol", ""], ["Dhir", "Chandra", ""]]}, {"id": "2107.07653", "submitter": "Qian Liu", "authors": "Qian Liu and Bei Chen and Jiaqi Guo and Zeqi Lin and Jian-guang Lou", "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor", "comments": "Work in progress, the project homepage is at\n  https://table-pretraining.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent years pre-trained language models hit a success on modeling natural\nlanguage sentences and (semi-)structured tables. However, existing table\npre-training techniques always suffer from low data quality and low\npre-training efficiency. In this paper, we show that table pre-training can be\nrealized by learning a neural SQL executor over a synthetic corpus, which is\nobtained by automatically synthesizing executable SQL queries. By pre-training\non the synthetic corpus, our approach TAPEX dramatically improves the\nperformance on downstream tasks, boosting existing language models by at most\n19.5%. Meanwhile, TAPEX has remarkably high pre-training efficiency and yields\nstrong results when using a small pre-trained corpus. Experimental results\ndemonstrate that TAPEX outperforms previous table pre-training approaches by a\nlarge margin, and our model achieves new state-of-the-art results on four\nwell-known datasets, including improving the WikiSQL denotation accuracy to\n89.6% (+4.9%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the\nSQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.6%\n(+3.6%). Our work opens the way to reason over structured data by pre-training\non synthetic executable programs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 00:40:11 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liu", "Qian", ""], ["Chen", "Bei", ""], ["Guo", "Jiaqi", ""], ["Lin", "Zeqi", ""], ["Lou", "Jian-guang", ""]]}, {"id": "2107.07682", "submitter": "Yukun Jiang", "authors": "Yukun Jiang", "title": "The Application of Active Query K-Means in Text Classification", "comments": "6 pages, 3 algorithms, 4 tables, 8 figures For source code and\n  questions, please email Yukun Jiang at jy2363@nyu.edu Reply would follow\n  shortly", "journal-ref": "In proceedings of 3rd International Conference on Natural Language\n  Processing 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 03:06:35 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jiang", "Yukun", ""]]}, {"id": "2107.07691", "submitter": "Liam Magee", "authors": "Liam Magee, Lida Ghahremanlou, Karen Soldatic, and Shanthi Robertson", "title": "Intersectional Bias in Causal Language Models", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 03:46:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Magee", "Liam", ""], ["Ghahremanlou", "Lida", ""], ["Soldatic", "Karen", ""], ["Robertson", "Shanthi", ""]]}, {"id": "2107.07705", "submitter": "Qin Ruan", "authors": "Qin Ruan, Brian Mac Namee, Ruihai Dong", "title": "Pseudo-labelling Enhanced Media Bias Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:47:50 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ruan", "Qin", ""], ["Mac Namee", "Brian", ""], ["Dong", "Ruihai", ""]]}, {"id": "2107.07771", "submitter": "Yajing Sun", "authors": "Yajing Sun, Yue Hu, Luxi Xing, Yuqiang Xie, Xiangpeng Wei", "title": "Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for\n  Open-domain Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-End intelligent neural dialogue systems suffer from the problems of\ngenerating inconsistent and repetitive responses. Existing dialogue models pay\nattention to unilaterally incorporating personal knowledge into the dialog\nwhile ignoring the fact that incorporating the personality-related conversation\ninformation into personal knowledge taken as the bilateral information flow\nboosts the quality of the subsequent conversation. Besides, it is indispensable\nto control personal knowledge utilization over the conversation level. In this\npaper, we propose a conversation-adaption multi-view persona aware response\ngeneration model that aims at enhancing conversation consistency and\nalleviating the repetition from two folds. First, we consider conversation\nconsistency from multiple views. From the view of the persona profile, we\ndesign a novel interaction module that not only iteratively incorporates\npersonalized knowledge into each turn conversation but also captures the\npersonality-related information from conversation to enhance personalized\nknowledge semantic representation. From the view of speaking style, we\nintroduce the speaking style vector and feed it into the decoder to keep the\nspeaking style consistency. To avoid conversation repetition, we devise a\ncoverage mechanism to keep track of the activation of personal knowledge\nutilization. Experiments on both automatic and human evaluation verify the\nsuperiority of our model over previous models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 08:59:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Sun", "Yajing", ""], ["Hu", "Yue", ""], ["Xing", "Luxi", ""], ["Xie", "Yuqiang", ""], ["Wei", "Xiangpeng", ""]]}, {"id": "2107.07903", "submitter": "Jordi Armengol-Estap\\'e", "authors": "Jordi Armengol-Estap\\'e, Casimiro Pio Carrino, Carlos\n  Rodriguez-Penagos, Ona de Gibert Bonet, Carme Armentano-Oller, Aitor\n  Gonzalez-Agirre, Maite Melero and Marta Villegas", "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced\n  Languages? A Comprehensive Assessment for Catalan", "comments": "Accepted into Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual language models have been a crucial breakthrough as they\nconsiderably reduce the need of data for under-resourced languages.\nNevertheless, the superiority of language-specific models has already been\nproven for languages having access to large amounts of data. In this work, we\nfocus on Catalan with the aim to explore to what extent a medium-sized\nmonolingual language model is competitive with state-of-the-art large\nmultilingual models. For this, we: (1) build a clean, high-quality textual\nCatalan corpus (CaText), the largest to date (but only a fraction of the usual\nsize of the previous work in monolingual language models), (2) train a\nTransformer-based language model for Catalan (BERTa), and (3) devise a thorough\nevaluation in a diversity of settings, comprising a complete array of\ndownstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and\nClassification, Text Classification, Question Answering, and Semantic Textual\nSimilarity, with most of the corresponding datasets being created ex novo. The\nresult is a new benchmark, the Catalan Language Understanding Benchmark (CLUB),\nwhich we publish as an open resource, together with the clean textual corpus,\nthe language model, and the cleaning pipeline. Using state-of-the-art\nmultilingual models and a monolingual model trained only on Wikipedia as\nbaselines, we consistently observe the superiority of our model across tasks\nand settings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 13:52:01 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Armengol-Estap\u00e9", "Jordi", ""], ["Carrino", "Casimiro Pio", ""], ["Rodriguez-Penagos", "Carlos", ""], ["Bonet", "Ona de Gibert", ""], ["Armentano-Oller", "Carme", ""], ["Gonzalez-Agirre", "Aitor", ""], ["Melero", "Maite", ""], ["Villegas", "Marta", ""]]}, {"id": "2107.07940", "submitter": "Wenliang Chen", "authors": "Pengju Zhang, Yonghui Jia, Muhua Zhu, Wenliang Chen, Min Zhang", "title": "Exploiting Rich Syntax for Better Knowledge Base Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on Knowledge Base Question Answering (KBQA) have shown great\nprogress on this task via better question understanding. Previous works for\nencoding questions mainly focus on the word sequences, but seldom consider the\ninformation from syntactic trees.In this paper, we propose an approach to learn\nsyntax-based representations for KBQA. First, we encode path-based syntax by\nconsidering the shortest dependency paths between keywords. Then, we propose\ntwo encoding strategies to mode the information of whole syntactic trees to\nobtain tree-based syntax. Finally, we combine both path-based and tree-based\nsyntax representations for KBQA. We conduct extensive experiments on a widely\nused benchmark dataset and the experimental results show that our syntax-aware\nsystems can make full use of syntax information in different settings and\nachieve state-of-the-art performance of KBQA.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 14:59:05 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Zhang", "Pengju", ""], ["Jia", "Yonghui", ""], ["Zhu", "Muhua", ""], ["Chen", "Wenliang", ""], ["Zhang", "Min", ""]]}, {"id": "2107.07956", "submitter": "Zitao Liu", "authors": "Hang Li, Yu Kang, Yang Hao, Wenbiao Ding, Zhongqin Wu, Zitao Liu", "title": "A Multimodal Machine Learning Framework for Teacher Vocal Delivery\n  Evaluation", "comments": "AIED'21: The 22nd International Conference on Artificial Intelligence\n  in Education, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of vocal delivery is one of the key indicators for evaluating\nteacher enthusiasm, which has been widely accepted to be connected to the\noverall course qualities. However, existing evaluation for vocal delivery is\nmainly conducted with manual ratings, which faces two core challenges:\nsubjectivity and time-consuming. In this paper, we present a novel machine\nlearning approach that utilizes pairwise comparisons and a multimodal\northogonal fusing algorithm to generate large-scale objective evaluation\nresults of the teacher vocal delivery in terms of fluency and passion. We\ncollect two datasets from real-world education scenarios and the experiment\nresults demonstrate the effectiveness of our algorithm. To encourage\nreproducible results, we make our code public available at\n\\url{https://github.com/tal-ai/ML4VocalDelivery.git}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:09:39 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Hang", ""], ["Kang", "Yu", ""], ["Hao", "Yang", ""], ["Ding", "Wenbiao", ""], ["Wu", "Zhongqin", ""], ["Liu", "Zitao", ""]]}, {"id": "2107.07957", "submitter": "Zitao Liu", "authors": "Shiting Xu, Guowei Xu, Peilei Jia, Wenbiao Ding, Zhongqin Wu, Zitao\n  Liu", "title": "Automatic Task Requirements Writing Evaluation via Machine Reading\n  Comprehension", "comments": "AIED'21: The 22nd International Conference on Artificial Intelligence\n  in Education, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task requirements (TRs) writing is an important question type in Key English\nTest and Preliminary English Test. A TR writing question may include multiple\nrequirements and a high-quality essay must respond to each requirement\nthoroughly and accurately. However, the limited teacher resources prevent\nstudents from getting detailed grading instantly. The majority of existing\nautomatic essay scoring systems focus on giving a holistic score but rarely\nprovide reasons to support it. In this paper, we proposed an end-to-end\nframework based on machine reading comprehension (MRC) to address this problem\nto some extent. The framework not only detects whether an essay responds to a\nrequirement question, but clearly marks where the essay answers the question.\nOur framework consists of three modules: question normalization module, ELECTRA\nbased MRC module and response locating module. We extensively explore\nstate-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and\n0.85 F1 score on a real-world educational dataset. To encourage reproducible\nresults, we make our code publicly available at\n\\url{https://github.com/aied2021TRMRC/AIED_2021_TRMRC_code}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:12:52 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Xu", "Shiting", ""], ["Xu", "Guowei", ""], ["Jia", "Peilei", ""], ["Ding", "Wenbiao", ""], ["Wu", "Zhongqin", ""], ["Liu", "Zitao", ""]]}, {"id": "2107.07958", "submitter": "Zitao Liu", "authors": "Yang Hao, Xiao Zhai, Wenbiao Ding, Zitao Liu", "title": "Temporal-aware Language Representation Learning From Crowdsourced Labels", "comments": "The 59th Annual Meeting of the Association for Computational\n  Linguistics Workshop on Representation Learning for NLP (ACL RepL4NLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective language representations from crowdsourced labels is\ncrucial for many real-world machine learning tasks. A challenging aspect of\nthis problem is that the quality of crowdsourced labels suffer high intra- and\ninter-observer variability. Since the high-capacity deep neural networks can\neasily memorize all disagreements among crowdsourced labels, directly applying\nexisting supervised language representation learning algorithms may yield\nsuboptimal solutions. In this paper, we propose \\emph{TACMA}, a\n\\underline{t}emporal-\\underline{a}ware language representation learning\nheuristic for \\underline{c}rowdsourced labels with \\underline{m}ultiple\n\\underline{a}nnotators. The proposed approach (1) explicitly models the\nintra-observer variability with attention mechanism; (2) computes and\naggregates per-sample confidence scores from multiple workers to address the\ninter-observer disagreements. The proposed heuristic is extremely easy to\nimplement in around 5 lines of code. The proposed heuristic is evaluated on\nfour synthetic and four real-world data sets. The results show that our\napproach outperforms a wide range of state-of-the-art baselines in terms of\nprediction accuracy and AUC. To encourage the reproducible results, we make our\ncode publicly available at \\url{https://github.com/CrowdsourcingMining/TACMA}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:25:56 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Hao", "Yang", ""], ["Zhai", "Xiao", ""], ["Ding", "Wenbiao", ""], ["Liu", "Zitao", ""]]}, {"id": "2107.07970", "submitter": "Camille Koenders", "authors": "Camille Koenders, Johannes Filla, Nicolai Schneider, Vinicius Woloszyn", "title": "How Vulnerable Are Automatic Fake News Detection Methods to Adversarial\n  Attacks?", "comments": "9 pages, Github:\n  https://github.com/nicolaischneider/FakeNewsDetectionVulnerability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the spread of false information on the internet has increased dramatically\nin recent years, more and more attention is being paid to automated fake news\ndetection. Some fake news detection methods are already quite successful.\nNevertheless, there are still many vulnerabilities in the detection algorithms.\nThe reason for this is that fake news publishers can structure and formulate\ntheir texts in such a way that a detection algorithm does not expose this text\nas fake news. This paper shows that it is possible to automatically attack\nstate-of-the-art models that have been trained to detect Fake News, making\nthese vulnerable. For this purpose, corresponding models were first trained\nbased on a dataset. Then, using Text-Attack, an attempt was made to manipulate\nthe trained models in such a way that previously correctly identified fake news\nwas classified as true news. The results show that it is possible to\nautomatically bypass Fake News detection mechanisms, leading to implications\nconcerning existing policy initiatives.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:36:03 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Koenders", "Camille", ""], ["Filla", "Johannes", ""], ["Schneider", "Nicolai", ""], ["Woloszyn", "Vinicius", ""]]}, {"id": "2107.07974", "submitter": "Wilbert Heeringa", "authors": "Wilbert Heeringa, Gosse Bouma, Martha Hofman, Eduard Drenth, Jan\n  Wijffels, Hans Van de Velde", "title": "POS tagging, lemmatization and dependency parsing of West Frisian", "comments": "6 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:41:37 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 12:38:35 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Heeringa", "Wilbert", ""], ["Bouma", "Gosse", ""], ["Hofman", "Martha", ""], ["Drenth", "Eduard", ""], ["Wijffels", "Jan", ""], ["Van de Velde", "Hans", ""]]}, {"id": "2107.08091", "submitter": "Rudolf Arseni Braun", "authors": "Rudolf A. Braun, Srikanth Madikeri, Petr Motlicek", "title": "A Comparison of Methods for OOV-word Recognition on a New Public Dataset", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9415124", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A common problem for automatic speech recognition systems is how to recognize\nwords that they did not see during training. Currently there is no established\nmethod of evaluating different techniques for tackling this problem. We propose\nusing the CommonVoice dataset to create test sets for multiple languages which\nhave a high out-of-vocabulary (OOV) ratio relative to a training set and\nrelease a new tool for calculating relevant performance metrics. We then\nevaluate, within the context of a hybrid ASR system, how much better subword\nmodels are at recognizing OOVs, and how much benefit one can get from\nincorporating OOV-word information into an existing system by modifying WFSTs.\nAdditionally, we propose a new method for modifying a subword-based language\nmodel so as to better recognize OOV-words. We showcase very large improvements\nin OOV-word recognition and make both the data and code available.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 19:39:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Braun", "Rudolf A.", ""], ["Madikeri", "Srikanth", ""], ["Motlicek", "Petr", ""]]}, {"id": "2107.08124", "submitter": "Oskar Wysocki", "authors": "Oskar Wysocki, Malina Florea, Donal Landers and Andre Freitas", "title": "Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems", "comments": "20 pages, 6 figures, 9 supplementary figures, Lexicon.txt in the\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel statistical corpus analysis framework targeted\ntowards the interpretation of Natural Language Processing (NLP) architectural\npatterns at scale. The proposed approach combines saturation-based lexicon\nconstruction, statistical corpus analysis methods and graph collocations to\ninduce a synthesis representation of NLP architectural patterns from corpora.\nThe framework is validated in the full corpus of Semeval tasks and demonstrated\ncoherent architectural patterns which can be used to answer architectural\nquestions on a data-driven fashion, providing a systematic mechanism to\ninterpret a largely dynamic and exponentially growing field.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 21:10:43 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wysocki", "Oskar", ""], ["Florea", "Malina", ""], ["Landers", "Donal", ""], ["Freitas", "Andre", ""]]}, {"id": "2107.08128", "submitter": "Emad Elwany", "authors": "Allison Hegel, Marina Shah, Genevieve Peaslee, Brendan Roof, Emad\n  Elwany", "title": "The Law of Large Documents: Understanding the Structure of Legal\n  Contracts Using Visual Cues", "comments": null, "journal-ref": "Document Intelligence Workshop at KDD, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Large, pre-trained transformer models like BERT have achieved\nstate-of-the-art results on document understanding tasks, but most\nimplementations can only consider 512 tokens at a time. For many real-world\napplications, documents can be much longer, and the segmentation strategies\ntypically used on longer documents miss out on document structure and\ncontextual information, hurting their results on downstream tasks. In our work\non legal agreements, we find that visual cues such as layout, style, and\nplacement of text in a document are strong features that are crucial to\nachieving an acceptable level of accuracy on long documents. We measure the\nimpact of incorporating such visual cues, obtained via computer vision methods,\non the accuracy of document understanding tasks including document\nsegmentation, entity extraction, and attribute classification. Our method of\nsegmenting documents based on structural metadata out-performs existing methods\non four long-document understanding tasks as measured on the Contract\nUnderstanding Atticus Dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 21:21:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hegel", "Allison", ""], ["Shah", "Marina", ""], ["Peaslee", "Genevieve", ""], ["Roof", "Brendan", ""], ["Elwany", "Emad", ""]]}, {"id": "2107.08146", "submitter": "Peter Jansen", "authors": "Peter Jansen", "title": "Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tamarian, a fictional language introduced in the Star Trek episode Darmok,\ncommunicates meaning through utterances of metaphorical references, such as\n\"Darmok and Jalad at Tanagra\" instead of \"We should work together.\" This work\nassembles a Tamarian-English dictionary of utterances from the original episode\nand several follow-on novels, and uses this to construct a parallel corpus of\n456 English-Tamarian utterances. A machine translation system based on a large\nlanguage model (T5) is trained using this parallel corpus, and is shown to\nproduce an accuracy of 76% when translating from English to Tamarian on known\nutterances.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 23:35:45 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jansen", "Peter", ""]]}, {"id": "2107.08173", "submitter": "Binzong Geng", "authors": "Binzong Geng, Fajie Yuan, Qiancheng Xu, Ying Shen, Ruifeng Xu, Min\n  Yang", "title": "Continual Learning for Task-oriented Dialogue System with Iterative\n  Network Pruning, Expanding and Masking", "comments": "Accepted by The Annual Meeting of the Association for Computational\n  Linguistics (ACL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 03:20:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Geng", "Binzong", ""], ["Yuan", "Fajie", ""], ["Xu", "Qiancheng", ""], ["Shen", "Ying", ""], ["Xu", "Ruifeng", ""], ["Yang", "Min", ""]]}, {"id": "2107.08188", "submitter": "David Wadden", "authors": "David Wadden, Kyle Lo", "title": "Overview and Insights from the SciVer Shared Task on Scientific Claim\n  Verification", "comments": "SciVer shared task, presented at the 2nd Scholarly Document\n  Processing (SDP) workshop at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an overview of the SciVer shared task, presented at the 2nd\nScholarly Document Processing (SDP) workshop at NAACL 2021. In this shared\ntask, systems were provided a scientific claim and a corpus of research\nabstracts, and asked to identify which articles SUPPORT or REFUTE the claim as\nwell as provide evidentiary sentences justifying those labels. 11 teams made a\ntotal of 14 submissions to the shared task leaderboard, leading to an\nimprovement of more than +23 F1 on the primary task evaluation metric. In\naddition to surveying the participating systems, we provide several insights\ninto modeling approaches to support continued progress and future research on\nthe important and challenging task of scientific claim verification.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 05:47:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wadden", "David", ""], ["Lo", "Kyle", ""]]}, {"id": "2107.08199", "submitter": "Lei Xun", "authors": "Hishan Parry, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, Geoff V.\n  Merrett", "title": "Dynamic Transformer for Efficient Machine Translation on Embedded\n  Devices", "comments": "Accepted at MLCAD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 07:36:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Parry", "Hishan", ""], ["Xun", "Lei", ""], ["Sabet", "Amin", ""], ["Bi", "Jia", ""], ["Hare", "Jonathon", ""], ["Merrett", "Geoff V.", ""]]}, {"id": "2107.08212", "submitter": "Xuebo Liu", "authors": "Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao,\n  Shuming Shi, Zhaopeng Tu", "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation", "comments": "Accepted to Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 10:02:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Xuebo", ""], ["Wang", "Longyue", ""], ["Wong", "Derek F.", ""], ["Ding", "Liang", ""], ["Chao", "Lidia S.", ""], ["Shi", "Shuming", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2107.08248", "submitter": "Jack Weston", "authors": "Jack Weston, Raphael Lenain, Udeepa Meepegama and Emil Fristed", "title": "Learning De-identified Representations of Prosody from Raw Audio", "comments": "ICML 2021", "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, ICML 2021, 18-24 July 2021, Virtual Event. Proceedings of Machine\n  Learning Research 139, PMLR 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 14:37:25 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Weston", "Jack", ""], ["Lenain", "Raphael", ""], ["Meepegama", "Udeepa", ""], ["Fristed", "Emil", ""]]}, {"id": "2107.08251", "submitter": "Jack Weston", "authors": "Jack Weston, Raphael Lenain, Udeepa Meepegama and Emil Fristed", "title": "Generative Pretraining for Paraphrase Evaluation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 14:48:48 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 10:48:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Weston", "Jack", ""], ["Lenain", "Raphael", ""], ["Meepegama", "Udeepa", ""], ["Fristed", "Emil", ""]]}, {"id": "2107.08264", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Jianben He, Zhihua Jin, Muqiao Yang, Yong Wang, Huamin Qu", "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment\n  Analysis", "comments": "11 pages, 7 figures. This paper is accepted by IEEE VIS, 2021. To\n  appear in IEEE Transactions on Visualization and Computer Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 15:54:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 02:20:19 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Xingbo", ""], ["He", "Jianben", ""], ["Jin", "Zhihua", ""], ["Yang", "Muqiao", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.08329", "submitter": "Yutao Zhu", "authors": "Yutao Zhu, Jian-Yun Nie, Kun Zhou, Pan Du, Hao Jiang, Zhicheng Dou", "title": "Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463011", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proactive dialogue system has the ability to proactively lead the\nconversation. Different from the general chatbots which only react to the user,\nproactive dialogue systems can be used to achieve some goals, e.g., to\nrecommend some items to the user. Background knowledge is essential to enable\nsmooth and natural transitions in dialogue. In this paper, we propose a new\nmulti-task learning framework for retrieval-based knowledge-grounded proactive\ndialogue. To determine the relevant knowledge to be used, we frame knowledge\nprediction as a complementary task and use explicit signals to supervise its\nlearning. The final response is selected according to the predicted knowledge,\nthe goal to achieve, and the context. Experimental results show that explicit\nmodeling of knowledge prediction and goal selection can greatly improve the\nfinal response selection. Our code is available at\nhttps://github.com/DaoD/KPN/.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 00:27:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhu", "Yutao", ""], ["Nie", "Jian-Yun", ""], ["Zhou", "Kun", ""], ["Du", "Pan", ""], ["Jiang", "Hao", ""], ["Dou", "Zhicheng", ""]]}, {"id": "2107.08337", "submitter": "Anupama Chingacham", "authors": "Anupama Chingacham, Vera Demberg, Dietrich Klakow", "title": "Exploring the Potential of Lexical Paraphrases for Mitigating\n  Noise-Induced Comprehension Errors", "comments": "Accepted in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Listening in noisy environments can be difficult even for individuals with a\nnormal hearing thresholds. The speech signal can be masked by noise, which may\nlead to word misperceptions on the side of the listener, and overall difficulty\nto understand the message. To mitigate hearing difficulties on listeners, a\nco-operative speaker utilizes voice modulation strategies like Lombard speech\nto generate noise-robust utterances, and similar solutions have been developed\nfor speech synthesis systems. In this work, we propose an alternate solution of\nchoosing noise-robust lexical paraphrases to represent an intended meaning. Our\nresults show that lexical paraphrases differ in their intelligibility in noise.\nWe evaluate the intelligibility of synonyms in context and find that choosing a\nlexical unit that is less risky to be misheard than its synonym introduced an\naverage gain in comprehension of 37% at SNR -5 dB and 21% at SNR 0 dB for\nbabble noise.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 01:16:33 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chingacham", "Anupama", ""], ["Demberg", "Vera", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2107.08347", "submitter": "Deval Mehta", "authors": "Xin Pei, Deval Mehta", "title": "Beyond a binary of (non)racist tweets: A four-dimensional categorical\n  detection and analysis of racist and xenophobic opinions on Twitter in early\n  Covid-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcending the binary categorization of racist and xenophobic texts, this\nresearch takes cues from social science theories to develop a four dimensional\ncategory for racism and xenophobia detection, namely stigmatization,\noffensiveness, blame, and exclusion. With the aid of deep learning techniques,\nthis categorical detection enables insights into the nuances of emergent topics\nreflected in racist and xenophobic expression on Twitter. Moreover, a stage\nwise analysis is applied to capture the dynamic changes of the topics across\nthe stages of early development of Covid-19 from a domestic epidemic to an\ninternational public health emergency, and later to a global pandemic. The main\ncontributions of this research include, first the methodological advancement.\nBy bridging the state-of-the-art computational methods with social science\nperspective, this research provides a meaningful approach for future research\nto gain insight into the underlying subtlety of racist and xenophobic\ndiscussion on digital platforms. Second, by enabling a more accurate\ncomprehension and even prediction of public opinions and actions, this research\npaves the way for the enactment of effective intervention policies to combat\nracist crimes and social exclusion under Covid-19.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 02:37:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pei", "Xin", ""], ["Mehta", "Deval", ""]]}, {"id": "2107.08356", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Yao Ming, Tongshuang Wu, Haipeng Zeng, Yong Wang, Huamin\n  Qu", "title": "DeHumor: Visual Analytics for Decomposing Humor", "comments": "15 pages. A preprint version of a publication at IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2021", "journal-ref": null, "doi": "10.1109/TVCG.2021.3097709", "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 04:01:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Xingbo", ""], ["Ming", "Yao", ""], ["Wu", "Tongshuang", ""], ["Zeng", "Haipeng", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.08357", "submitter": "Jun Wang", "authors": "Jun Wang, Chang Xu, Francisco Guzman, Ahmed El-Kishky, Benjamin I. P.\n  Rubinstein, Trevor Cohn", "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical\n  Translation", "comments": "Findings of ACL, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mistranslated numbers have the potential to cause serious effects, such as\nfinancial loss or medical misinformation. In this work we develop comprehensive\nassessments of the robustness of neural machine translation systems to\nnumerical text via behavioural testing. We explore a variety of numerical\ntranslation capabilities a system is expected to exhibit and design effective\ntest examples to expose system underperformance. We find that numerical\nmistranslation is a general issue: major commercial systems and\nstate-of-the-art research models fail on many of our test examples, for high-\nand low-resource languages. Our tests reveal novel errors that have not\npreviously been reported in NMT systems, to the best of our knowledge. Lastly,\nwe discuss strategies to mitigate numerical mistranslation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 04:09:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Jun", ""], ["Xu", "Chang", ""], ["Guzman", "Francisco", ""], ["El-Kishky", "Ahmed", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Cohn", "Trevor", ""]]}, {"id": "2107.08408", "submitter": "Ashutosh Modi", "authors": "Ishika Singh and Gargi Singh and Ashutosh Modi", "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based\n  Games", "comments": "55 Pages (8 Pages main content + 2 Pages references + 45 Pages\n  Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires understanding and interaction\nusing natural language in a partially observable environment. In this paper, we\nimprove the semantic understanding of the agent by proposing a simple RL with\nLM framework where we use transformer-based language models with Deep RL\nmodels. We perform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1, to achieve a\nscore of 44.7, which is 1.6 higher than the state-of-the-art model. Our\nproposed approach also performs comparably to the state-of-the-art models on\nthe other set of text games.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 10:28:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Singh", "Ishika", ""], ["Singh", "Gargi", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2107.08512", "submitter": "Diego Amancio", "authors": "Henrique F. de Arruda, Sandro M. Reia, Filipi N. Silva, Diego R.\n  Amancio and Luciano da F. Costa", "title": "A pattern recognition approach for distinguishing between prose and\n  poetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poetry and prose are written artistic expressions that help us to appreciate\nthe reality we live. Each of these styles has its own set of subjective\nproperties, such as rhyme and rhythm, which are easily caught by a human\nreader's eye and ear. With the recent advances in artificial intelligence, the\ngap between humans and machines may have decreased, and today we observe\nalgorithms mastering tasks that were once exclusively performed by humans. In\nthis paper, we propose an automated method to distinguish between poetry and\nprose based solely on aural and rhythmic properties. In other to compare prose\nand poetry rhythms, we represent the rhymes and phones as temporal sequences\nand thus we propose a procedure for extracting rhythmic features from these\nsequences. The classification of the considered texts using the set of features\nextracted resulted in a best accuracy of 0.78, obtained with a neural network.\nInterestingly, by using an approach based on complex networks to visualize the\nsimilarities between the different texts considered, we found that the patterns\nof poetry vary much more than prose. Consequently, a much richer and complex\nset of rhythmic possibilities tends to be found in that modality.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 18:44:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["de Arruda", "Henrique F.", ""], ["Reia", "Sandro M.", ""], ["Silva", "Filipi N.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "2107.08523", "submitter": "William Gantt", "authors": "William Gantt", "title": "Argument Linking: A Survey and Forecast", "comments": "An unpublished survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic role labeling (SRL) -- identifying the semantic relationships\nbetween a predicate and other constituents in the same sentence -- is a\nwell-studied task in natural language understanding (NLU). However, many of\nthese relationships are evident only at the level of the document, as a role\nfor a predicate in one sentence may often be filled by an argument in a\ndifferent one. This more general task, known as implicit semantic role labeling\nor argument linking, has received increased attention in recent years, as\nresearchers have recognized its centrality to information extraction and NLU.\nThis paper surveys the literature on argument linking and identifies several\nnotable shortcomings of existing approaches that indicate the paths along which\nfuture research effort could most profitably be spent.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 19:28:20 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gantt", "William", ""]]}, {"id": "2107.08582", "submitter": "Ning Bian", "authors": "Ning Bian, Xianpei Han, Bo Chen, Hongyu Lin, Ben He, Le Sun", "title": "Bridging the Gap between Language Model and Reading Comprehension:\n  Unsupervised MRC via Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent success in machine reading comprehension (MRC), learning\nhigh-quality MRC models still requires large-scale labeled training data, even\nusing strong pre-trained language models (PLMs). The pre-training tasks for\nPLMs are not question-answering or MRC-based tasks, making existing PLMs unable\nto be directly used for unsupervised MRC. Specifically, MRC aims to spot an\naccurate answer span from the given document, but PLMs focus on token filling\nin sentences. In this paper, we propose a new framework for unsupervised MRC.\nFirstly, we propose to learn to spot answer spans in documents via\nself-supervised learning, by designing a self-supervision pretext task for MRC\n- Spotting-MLM. Solving this task requires capturing deep interactions between\nsentences in documents. Secondly, we apply a simple sentence rewriting strategy\nin the inference stage to alleviate the expression mismatch between questions\nand documents. Experiments show that our method achieves a new state-of-the-art\nperformance for unsupervised MRC.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:14:36 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bian", "Ning", ""], ["Han", "Xianpei", ""], ["Chen", "Bo", ""], ["Lin", "Hongyu", ""], ["He", "Ben", ""], ["Sun", "Le", ""]]}, {"id": "2107.08661", "submitter": "Ye Jia", "authors": "Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz", "title": "Translatotron 2: Robust direct speech-to-speech translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 07:43:49 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 06:03:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Jia", "Ye", ""], ["Ramanovich", "Michelle Tadmor", ""], ["Remez", "Tal", ""], ["Pomerantz", "Roi", ""]]}, {"id": "2107.08685", "submitter": "Nyoungwoo Lee", "authors": "Nyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin Choi, Sung-Hyun Myaeng", "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with\n  Semantically Relevant Images", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-modal dialogue systems, it is important to allow the use of images\nas part of a multi-turn conversation. Training such dialogue systems generally\nrequires a large-scale dataset consisting of multi-turn dialogues that involve\nimages, but such datasets rarely exist. In response, this paper proposes a 45k\nmulti-modal dialogue dataset created with minimal human intervention. Our\nmethod to create such a dataset consists of (1) preparing and pre-processing\ntext dialogue datasets, (2) creating image-mixed dialogues by using a\ntext-to-image replacement technique, and (3) employing a\ncontextual-similarity-based filtering step to ensure the contextual coherence\nof the dataset. To evaluate the validity of our dataset, we devise a simple\nretrieval model for dialogue sentence prediction tasks. Automatic metrics and\nhuman evaluation results on such tasks show that our dataset can be effectively\nused as training data for multi-modal dialogue systems which require an\nunderstanding of images and text in a context-aware manner. Our dataset and\ngeneration code is available at\nhttps://github.com/shh1574/multi-modal-dialogue-dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:44:11 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lee", "Nyoungwoo", ""], ["Shin", "Suwon", ""], ["Choo", "Jaegul", ""], ["Choi", "Ho-Jin", ""], ["Myaeng", "Sung-Hyun", ""]]}, {"id": "2107.08720", "submitter": "Marco Guerini", "authors": "Margherita Fanton, Helena Bonaldi, Serra Sinem Tekiroglu, Marco\n  Guerini", "title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative\n  Dataset to Fight Online Hate Speech", "comments": "To appear at ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undermining the impact of hateful content with informed and non-aggressive\nresponses, called counter narratives, has emerged as a possible solution for\nhaving healthier online communities. Thus, some NLP studies have started\naddressing the task of counter narrative generation. Although such studies have\nmade an effort to build hate speech / counter narrative (HS/CN) datasets for\nneural generation, they fall short in reaching either high-quality and/or\nhigh-quantity. In this paper, we propose a novel human-in-the-loop data\ncollection methodology in which a generative language model is refined\niteratively by using its own data from the previous loops to generate new\ntraining samples that experts review and/or post-edit. Our experiments\ncomprised several loops including dynamic variations. Results show that the\nmethodology is scalable and facilitates diverse, novel, and cost-effective data\ncollection. To our knowledge, the resulting dataset is the only expert-based\nmulti-target HS/CN dataset available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:45:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fanton", "Margherita", ""], ["Bonaldi", "Helena", ""], ["Tekiroglu", "Serra Sinem", ""], ["Guerini", "Marco", ""]]}, {"id": "2107.08721", "submitter": "Qinkai Chen", "authors": "Qinkai Chen", "title": "Stock Movement Prediction with Financial News using Contextualized\n  Embedding from BERT", "comments": "22 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CL cs.LG q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:47:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Qinkai", ""]]}, {"id": "2107.08728", "submitter": "Sergey A. Slavnov", "authors": "Sergey Slavnov", "title": "Cobordisms and commutative categorial grammars", "comments": "This is the final version of the previously posted series of drafts\n  on cobordimns and categorial grammars. Concise and, hopefully, much improved\n  presentation, but no new mathematical content compared to preceding versions.\n  arXiv admin note: text overlap with arXiv:1911.03962", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a concrete surface representation of abstract categorial grammars\nin the category of word cobordisms or cowordisms for short, which are certain\nbipartite graphs decorated with words in a given alphabet, generalizing linear\nlogic proof-nets. We also introduce and study linear logic grammars, directly\nbased on cobordisms and using classical multiplicative linear logic as a typing\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:55:21 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Slavnov", "Sergey", ""]]}, {"id": "2107.08739", "submitter": "Francesco Fabiano", "authors": "Francesco Fabiano, Biplav Srivastava, Jonathan Lenchner, Lior Horesh,\n  Francesca Rossi, Marianna Bergamaschi Ganapini", "title": "E-PDDL: A Standardized Way of Defining Epistemic Planning Problems", "comments": "9 pages, Knowledge Engineering for Planning and Scheduling - ICAPS\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epistemic Planning (EP) refers to an automated planning setting where the\nagent reasons in the space of knowledge states and tries to find a plan to\nreach a desirable state from the current state. Its general form, the\nMulti-agent Epistemic Planning (MEP) problem involves multiple agents who need\nto reason about both the state of the world and the information flow between\nagents. In a MEP problem, multiple approaches have been developed recently with\nvarying restrictions, such as considering only the concept of knowledge while\nnot allowing the idea of belief, or not allowing for ``complex\" modal operators\nsuch as those needed to handle dynamic common knowledge. While the diversity of\napproaches has led to a deeper understanding of the problem space, the lack of\na standardized way to specify MEP problems independently of solution approaches\nhas created difficulties in comparing performance of planners, identifying\npromising techniques, exploring new strategies like ensemble methods, and\nmaking it easy for new researchers to contribute to this research area. To\naddress the situation, we propose a unified way of specifying EP problems - the\nEpistemic Planning Domain Definition Language, E-PDDL. We show that E-PPDL can\nbe supported by leading MEP planners and provide corresponding parser code that\ntranslates EP problems specified in E-PDDL into (M)EP problems that can be\nhandled by several planners. This work is also useful in building more general\nepistemic planning environments where we envision a meta-cognitive module that\ntakes a planning problem in E-PDDL, identifies and assesses some of its\nfeatures, and autonomously decides which planner is the best one to solve it.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 10:20:20 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fabiano", "Francesco", ""], ["Srivastava", "Biplav", ""], ["Lenchner", "Jonathan", ""], ["Horesh", "Lior", ""], ["Rossi", "Francesca", ""], ["Ganapini", "Marianna Bergamaschi", ""]]}, {"id": "2107.08772", "submitter": "Dana Ruiter", "authors": "Dana Ruiter, Dietrich Klakow, Josef van Genabith, Cristina\n  Espa\\~na-Bonet", "title": "Integrating Unsupervised Data Generation into Self-Supervised Neural\n  Machine Translation for Low-Resource Languages", "comments": "11 pages, 8 figures, accepted at MT-Summit 2021 (Research Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most language combinations, parallel data is either scarce or simply\nunavailable. To address this, unsupervised machine translation (UMT) exploits\nlarge amounts of monolingual data by using synthetic data generation techniques\nsuch as back-translation and noising, while self-supervised NMT (SSNMT)\nidentifies parallel sentences in smaller comparable data and trains on them. To\ndate, the inclusion of UMT data generation techniques in SSNMT has not been\ninvestigated. We show that including UMT techniques into SSNMT significantly\noutperforms SSNMT and UMT on all tested language pairs, with improvements of up\nto +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,\nrespectively, on Afrikaans to English. We further show that the combination of\nmultilingual denoising autoencoding, SSNMT with backtranslation and bilingual\nfinetuning enables us to learn machine translation even for distant language\npairs for which only small amounts of monolingual data are available, e.g.\nyielding BLEU scores of 11.6 (English to Swahili).\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 11:56:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ruiter", "Dana", ""], ["Klakow", "Dietrich", ""], ["van Genabith", "Josef", ""], ["Espa\u00f1a-Bonet", "Cristina", ""]]}, {"id": "2107.08807", "submitter": "Sara Papi", "authors": "Alina Karakanta, Sara Papi, Matteo Negri, Marco Turchi", "title": "Simultaneous Speech Translation for Live Subtitling: from Delay to\n  Display", "comments": null, "journal-ref": "Proceedings of MT Summit 2021 at Automatic Spoken Language\n  Translation in Real-World Settings", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the increased audiovisualisation of communication, the need for live\nsubtitles in multilingual events is more relevant than ever. In an attempt to\nautomatise the process, we aim at exploring the feasibility of simultaneous\nspeech translation (SimulST) for live subtitling. However, the word-for-word\nrate of generation of SimulST systems is not optimal for displaying the\nsubtitles in a comprehensible and readable way. In this work, we adapt SimulST\nsystems to predict subtitle breaks along with the translation. We then propose\na display mode that exploits the predicted break structure by presenting the\nsubtitles in scrolling lines. We compare our proposed mode with a display 1)\nword-for-word and 2) in blocks, in terms of reading speed and delay.\nExperiments on three language pairs (en$\\rightarrow$it, de, fr) show that\nscrolling lines is the only mode achieving an acceptable reading speed while\nkeeping delay close to a 4-second threshold. We argue that simultaneous\ntranslation for readable live subtitles still faces challenges, the main one\nbeing poor translation quality, and propose directions for steering future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 12:35:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:27:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Karakanta", "Alina", ""], ["Papi", "Sara", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2107.08902", "submitter": "Anjum Anjum", "authors": "Bhumika Bhatia, Anuj Verma, Anjum, Rahul Katarya", "title": "Analysing Cyberbullying using Natural Language Processing by\n  Understanding Jargon in Social Media", "comments": "10 pages", "journal-ref": "Sustainable Advanced Computing - Select Proceedings of ICSAC 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 04:20:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bhatia", "Bhumika", ""], ["Verma", "Anuj", ""], ["Anjum", "", ""], ["Katarya", "Rahul", ""]]}, {"id": "2107.08929", "submitter": "Nianlong Gu", "authors": "Nianlong Gu, Elliott Ash, Richard H.R. Hahnloser", "title": "MemSum: Extractive Summarization of Long Documents using Multi-step\n  Episodic Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MemSum (Multi-step Episodic Markov decision process extractive\nSUMmarizer), a reinforcement-learning-based extractive summarizer enriched at\nany given time step with information on the current extraction history. Similar\nto previous models in this vein, MemSum iteratively selects sentences into the\nsummary. Our innovation is in considering a broader information set when\nsummarizing that would intuitively also be used by humans in this task: 1) the\ntext content of the sentence, 2) the global text context of the rest of the\ndocument, and 3) the extraction history consisting of the set of sentences that\nhave already been extracted. With a lightweight architecture, MemSum\nnonetheless obtains state-of-the-art test-set performance (ROUGE score) on long\ndocument datasets (PubMed, arXiv, and GovReport). Supporting analysis\ndemonstrates that the added awareness of extraction history gives MemSum\nrobustness against redundancy in the source document.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:41:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gu", "Nianlong", ""], ["Ash", "Elliott", ""], ["Hahnloser", "Richard H. R.", ""]]}, {"id": "2107.08957", "submitter": "Xi Yang", "authors": "Xi Yang, Zehao Yu, Yi Guo, Jiang Bian and Yonghui Wu", "title": "Clinical Relation Extraction Using Transformer-based Models", "comments": "1 Figure; 36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:15:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yang", "Xi", ""], ["Yu", "Zehao", ""], ["Guo", "Yi", ""], ["Bian", "Jiang", ""], ["Wu", "Yonghui", ""]]}, {"id": "2107.08973", "submitter": "Shivangi Bithel", "authors": "Shivangi Bithel, Sumitra S Malagi", "title": "Unsupervised Identification of Relevant Prior Cases", "comments": "Code: https://github.com/shivangibithel/Information-Retrieval-CS6370", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document retrieval has taken its role in almost all domains of knowledge\nunderstanding, including the legal domain. Precedent refers to a court decision\nthat is considered as authority for deciding subsequent cases involving\nidentical or similar facts or similar legal issues. In this work, we propose\ndifferent unsupervised approaches to solve the task of identifying relevant\nprecedents to a given query case. Our proposed approaches are using word\nembeddings like word2vec, doc2vec, and sent2vec, finding cosine similarity\nusing TF-IDF, retrieving relevant documents using BM25 scores, using the\npre-trained model and SBERT to find the most similar document, and using the\nproduct of BM25 and TF-IDF scores to find the most relevant document for a\ngiven query. We compared all the methods based on precision@10, recall@10, and\nMRR. Based on the comparative analysis, we found that the TF-IDF score\nmultiplied by the BM25 score gives the best result. In this paper, we have also\npresented the analysis that we did to improve the BM25 score.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:41:49 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bithel", "Shivangi", ""], ["Malagi", "Sumitra S", ""]]}, {"id": "2107.09055", "submitter": "Vikas Bajpai", "authors": "Priyank Sonkiya, Vikas Bajpai and Anukriti Bansal", "title": "Stock price prediction using BERT and GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 18:31:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Sonkiya", "Priyank", ""], ["Bajpai", "Vikas", ""], ["Bansal", "Anukriti", ""]]}, {"id": "2107.09099", "submitter": "Qiushi Huang", "authors": "Qiushi Huang, Tom Ko, H Lilian Tang, Xubo Liu, Bo Wu", "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 18:24:33 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Huang", "Qiushi", ""], ["Ko", "Tom", ""], ["Tang", "H Lilian", ""], ["Liu", "Xubo", ""], ["Wu", "Bo", ""]]}, {"id": "2107.09106", "submitter": "Spencer Whitehead", "authors": "Spencer Whitehead, Hui Wu, Heng Ji, Rogerio Feris, Kate Saenko", "title": "Separating Skills and Concepts for Novel Visual Question Answering", "comments": "Paper at CVPR 2021. 14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 18:55:10 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Whitehead", "Spencer", ""], ["Wu", "Hui", ""], ["Ji", "Heng", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""]]}, {"id": "2107.09186", "submitter": "Haoran Xu", "authors": "Haoran Xu and Philipp Koehn", "title": "Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and\n  Isometric Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, a linearly orthogonal transformation mapping is learned by\naligning static type-level embeddings to build a shared semantic space. In view\nof the analysis that contextual embeddings contain richer semantic features, we\ninvestigate a context-aware and dictionary-free mapping approach by leveraging\nparallel corpora. We illustrate that our contextual embedding space mapping\nsignificantly outperforms previous multilingual word embedding methods on the\nbilingual dictionary induction (BDI) task by providing a higher degree of\nisomorphism. To improve the quality of mapping, we also explore sense-level\nembeddings that are split from type-level representations, which can align\nspaces in a finer resolution and yield more precise mapping. Moreover, we\nreveal that contextual embedding spaces suffer from their natural properties --\nanisotropy and anisometry. To mitigate these two problems, we introduce the\niterative normalization algorithm as an imperative preprocessing step. Our\nfindings unfold the tight relationship between isotropy, isometry, and\nisomorphism in normalized contextual embedding spaces.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 22:57:36 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Xu", "Haoran", ""], ["Koehn", "Philipp", ""]]}, {"id": "2107.09240", "submitter": "Yi-Fu Wu", "authors": "Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn", "title": "Generative Video Transformer: Can Objects be the Words?", "comments": "Published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:08:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wu", "Yi-Fu", ""], ["Yoon", "Jaesik", ""], ["Ahn", "Sungjin", ""]]}, {"id": "2107.09274", "submitter": "Joosung Lee", "authors": "Joosung Lee", "title": "Paraphrasing via Ranking Many Candidates", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple and effective way to generate a variety of paraphrases\nand find a good quality paraphrase among them. As in previous studies, it is\ndifficult to ensure that one generation method always generates the best\nparaphrase in various domains. Therefore, we focus on finding the best\ncandidate from multiple candidates, rather than assuming that there is only one\ncombination of generative models and decoding options. Our approach shows that\nit is easy to apply in various domains and has sufficiently good performance\ncompared to previous methods. In addition, our approach can be used for data\nagumentation that extends the downstream corpus, showing that it can help\nimprove performance in English and Korean datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 06:24:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lee", "Joosung", ""]]}, {"id": "2107.09278", "submitter": "Qinglin Zhang", "authors": "Qinglin Zhang, Qian Chen, Yali Li, Jiaqing Liu, Wen Wang", "title": "Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken\n  Document Segmentation", "comments": "submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcripts generated by automatic speech recognition (ASR) systems for\nspoken documents lack structural annotations such as paragraphs, significantly\nreducing their readability. Automatically predicting paragraph segmentation for\nspoken documents may both improve readability and downstream NLP performance\nsuch as summarization and machine reading comprehension. We propose a sequence\nmodel with self-adaptive sliding window for accurate and efficient paragraph\nsegmentation. We also propose an approach to exploit phonetic information,\nwhich significantly improves robustness of spoken document segmentation to ASR\nerrors. Evaluations are conducted on the English Wiki-727K document\nsegmentation benchmark, a Chinese Wikipedia-based document segmentation dataset\nwe created, and an in-house Chinese spoken document dataset. Our proposed model\noutperforms the state-of-the-art (SOTA) model based on the same BERT-Base,\nincreasing segmentation F1 on the English benchmark by 4.2 points and on\nChinese datasets by 4.3-10.1 points, while reducing inference time to less than\n1/6 of inference time of the current SOTA.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 06:44:13 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhang", "Qinglin", ""], ["Chen", "Qian", ""], ["Li", "Yali", ""], ["Liu", "Jiaqing", ""], ["Wang", "Wen", ""]]}, {"id": "2107.09285", "submitter": "Kaylee Burns", "authors": "Kaylee Burns, Christopher D. Manning, Li Fei-Fei", "title": "Neural Abstructions: Abstractions that Support Construction for Grounded\n  Language Learning", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although virtual agents are increasingly situated in environments where\nnatural language is the most effective mode of interaction with humans, these\nexchanges are rarely used as an opportunity for learning. Leveraging language\ninteractions effectively requires addressing limitations in the two most common\napproaches to language grounding: semantic parsers built on top of fixed object\ncategories are precise but inflexible and end-to-end models are maximally\nexpressive, but fickle and opaque. Our goal is to develop a system that\nbalances the strengths of each approach so that users can teach agents new\ninstructions that generalize broadly from a single example. We introduce the\nidea of neural abstructions: a set of constraints on the inference procedure of\na label-conditioned generative model that can affect the meaning of the label\nin context. Starting from a core programming language that operates over\nabstructions, users can define increasingly complex mappings from natural\nlanguage to actions. We show that with this method a user population is able to\nbuild a semantic parser for an open-ended house modification task in Minecraft.\nThe semantic parser that results is both flexible and expressive: the\npercentage of utterances sourced from redefinitions increases steadily over the\ncourse of 191 total exchanges, achieving a final value of 28%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:01:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Burns", "Kaylee", ""], ["Manning", "Christopher D.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "2107.09293", "submitter": "Suzhen Wang", "authors": "Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, Xin Yu", "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural\n  Head Motion", "comments": null, "journal-ref": "IJCAI 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:22:42 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Suzhen", ""], ["Li", "Lincheng", ""], ["Ding", "Yu", ""], ["Fan", "Changjie", ""], ["Yu", "Xin", ""]]}, {"id": "2107.09332", "submitter": "Seongsik Park", "authors": "Seongsik Park, Harksoo Kim", "title": "Improving Sentence-Level Relation Extraction through Curriculum Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sentence-level relation extraction mainly aims to classify the relation\nbetween two entities in a sentence. The sentence-level relation extraction\ncorpus is often containing data of difficulty for the model to infer or noise\ndata. In this paper, we propose a curriculum learning-based relation extraction\nmodel that split data by difficulty and utilize it for learning. In the\nexperiments with the representative sentence-level relation extraction\ndatasets, TACRED and Re-TACRED, the proposed method showed good performances.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:44:40 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Park", "Seongsik", ""], ["Kim", "Harksoo", ""]]}, {"id": "2107.09333", "submitter": "Mahyar Emami", "authors": "Endri Bezati, Mahyar Emami, J\\\"orn Janneck, James Larus", "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical\n  report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To increase performance and efficiency, systems use FPGAs as reconfigurable\naccelerators. A key challenge in designing these systems is partitioning\ncomputation between processors and an FPGA. An appropriate division of labor\nmay be difficult to predict in advance and require experiments and\nmeasurements. When an investigation requires rewriting part of the system in a\nnew language or with a new programming model, its high cost can retard the\nstudy of different configurations. A single-language system with an appropriate\nprogramming model and compiler that targets both platforms simplifies this\nexploration to a simple recompile with new compiler directives.\n  This work introduces StreamBlocks, an open-source compiler and runtime that\nuses the CAL dataflow programming language to partition computations across\nheterogeneous (CPU/accelerator) platforms. Because of the dataflow model's\nsemantics and the CAL language, StreamBlocks can exploit both thread\nparallelism in multi-core CPUs and the inherent parallelism of FPGAs.\nStreamBlocks supports exploring the design space with a profile-guided tool\nthat helps identify the best hardware-software partitions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:46:47 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bezati", "Endri", ""], ["Emami", "Mahyar", ""], ["Janneck", "J\u00f6rn", ""], ["Larus", "James", ""]]}, {"id": "2107.09356", "submitter": "Kevin Eloff", "authors": "Kevin Eloff, Herman A. Engelbrecht", "title": "Toward Collaborative Reinforcement Learning Agents that Communicate\n  Through Text-Based Natural Language", "comments": "5 pages, 6 figures, 3 tables; published in 2021 Southern African\n  Universities Power Engineering Conference/Robotics and Mechatronics/Pattern\n  Recognition Association of South Africa (SAUPEC/RobMech/PRASA), 2021, pp.\n  1-6, (c) 2021 IEEE", "journal-ref": "In: 2021 Southern African Universities Power Engineering\n  Conference/Robotics and Mechatronics/Pattern Recognition Association of South\n  Africa (SAUPEC/RobMech/PRASA), 2021, pp. 1-6", "doi": "10.1109/SAUPEC/RobMech/PRASA52254.2021.9377018", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication between agents in collaborative multi-agent settings is in\ngeneral implicit or a direct data stream. This paper considers text-based\nnatural language as a novel form of communication between multiple agents\ntrained with reinforcement learning. This could be considered first steps\ntoward a truly autonomous communication without the need to define a limited\nset of instructions, and natural collaboration between humans and robots.\nInspired by the game of Blind Leads, we propose an environment where one agent\nuses natural language instructions to guide another through a maze. We test the\nability of reinforcement learning agents to effectively communicate through\ndiscrete word-level symbols and show that the agents are able to sufficiently\ncommunicate through natural language with a limited vocabulary. Although the\ncommunication is not always perfect English, the agents are still able to\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\n0.61 over randomly generated sequences while maintaining a 100% maze completion\nrate. This is a 3.5 times the performance of the random baseline using our\nreference set.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:19:29 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 13:28:31 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Eloff", "Kevin", ""], ["Engelbrecht", "Herman A.", ""]]}, {"id": "2107.09429", "submitter": "Huiqiang Jiang", "authors": "Huiqiang Jiang, Guoxin Wang, Weile Chen, Chengxi Zhang, B\\\"orje F.\n  Karlsson", "title": "BoningKnife: Joint Entity Mention Detection and Typing for Nested NER\n  via prior Boundary Knowledge", "comments": "Work performed at Microsoft Research Asia between 2019/2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While named entity recognition (NER) is a key task in natural language\nprocessing, most approaches only target flat entities, ignoring nested\nstructures which are common in many scenarios. Most existing nested NER methods\ntraverse all sub-sequences which is both expensive and inefficient, and also\ndon't well consider boundary knowledge which is significant for nested\nentities. In this paper, we propose a joint entity mention detection and typing\nmodel via prior boundary knowledge (BoningKnife) to better handle nested NER\nextraction and recognition tasks. BoningKnife consists of two modules,\nMentionTagger and TypeClassifier. MentionTagger better leverages boundary\nknowledge beyond just entity start/end to improve the handling of nesting\nlevels and longer spans, while generating high quality mention candidates.\nTypeClassifier utilizes a two-level attention mechanism to decouple different\nnested level representations and better distinguish entity types. We jointly\ntrain both modules sharing a common representation and a new dual-info\nattention layer, which leads to improved representation focus on entity-related\ninformation. Experiments over different datasets show that our approach\noutperforms previous state of the art methods and achieves 86.41, 85.46, and\n94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:44:36 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Jiang", "Huiqiang", ""], ["Wang", "Guoxin", ""], ["Chen", "Weile", ""], ["Zhang", "Chengxi", ""], ["Karlsson", "B\u00f6rje F.", ""]]}, {"id": "2107.09433", "submitter": "Daniele Falavigna", "authors": "Roberto Gretter, Marco Matassoni, Daniele Falavigna", "title": "Seed Words Based Data Selection for Language Model Adaptation", "comments": "11 pages", "journal-ref": "Proceedings of MT Summit 2021 - August 16-20, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the problem of language model customization in applications where\nthe ASR component needs to manage domain-specific terminology; although current\nstate-of-the-art speech recognition technology provides excellent results for\ngeneric domains, the adaptation to specialized dictionaries or glossaries is\nstill an open issue. In this work we present an approach for automatically\nselecting sentences, from a text corpus, that match, both semantically and\nmorphologically, a glossary of terms (words or composite words) furnished by\nthe user. The final goal is to rapidly adapt the language model of an hybrid\nASR system with a limited amount of in-domain text data in order to\nsuccessfully cope with the linguistic domain at hand; the vocabulary of the\nbaseline model is expanded and tailored, reducing the resulting OOV rate. Data\nselection strategies based on shallow morphological seeds and semantic\nsimilarity viaword2vec are introduced and discussed; the experimental setting\nconsists in a simultaneous interpreting scenario, where ASRs in three languages\nare designed to recognize the domain-specific terms (i.e. dentistry). Results\nusing different metrics (OOV rate, WER, precision and recall) show the\neffectiveness of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 12:08:27 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Gretter", "Roberto", ""], ["Matassoni", "Marco", ""], ["Falavigna", "Daniele", ""]]}, {"id": "2107.09477", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Tomoki Hayashi, Xinjian Li, Shinji Watanabe, Tomoki\n  Toda", "title": "On Prosody Modeling for ASR+TTS based Voice Conversion", "comments": "Submitted to ASRU2021. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In voice conversion (VC), an approach showing promising results in the latest\nvoice conversion challenge (VCC) 2020 is to first use an automatic speech\nrecognition (ASR) model to transcribe the source speech into the underlying\nlinguistic contents; these are then used as input by a text-to-speech (TTS)\nsystem to generate the converted speech. Such a paradigm, referred to as\nASR+TTS, overlooks the modeling of prosody, which plays an important role in\nspeech naturalness and conversion similarity. Although some researchers have\nconsidered transferring prosodic clues from the source speech, there arises a\nspeaker mismatch during training and conversion. To address this issue, in this\nwork, we propose to directly predict prosody from the linguistic representation\nin a target-speaker-dependent manner, referred to as target text prediction\n(TTP). We evaluate both methods on the VCC2020 benchmark and consider different\nlinguistic representations. The results demonstrate the effectiveness of TTP in\nboth objective and subjective evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:30:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Hayashi", "Tomoki", ""], ["Li", "Xinjian", ""], ["Watanabe", "Shinji", ""], ["Toda", "Tomoki", ""]]}, {"id": "2107.09556", "submitter": "Luyu Wang", "authors": "Luyu Wang, Yujia Li, Ozlem Aslan, Oriol Vinyals", "title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new dataset of Wikipedia articles each paired with a knowledge\ngraph, to facilitate the research in conditional text generation, graph\ngeneration and graph representation learning. Existing graph-text paired\ndatasets typically contain small graphs and short text (1 or few sentences),\nthus limiting the capabilities of the models that can be learned on the data.\nOur new dataset WikiGraphs is collected by pairing each Wikipedia article from\nthe established WikiText-103 benchmark (Merity et al., 2016) with a subgraph\nfrom the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy\nto benchmark against other state-of-the-art text generative models that are\ncapable of generating long paragraphs of coherent text. Both the graphs and the\ntext data are of significantly larger scale compared to prior graph-text paired\ndatasets. We present baseline graph neural network and transformer model\nresults on our dataset for 3 tasks: graph -> text generation, graph -> text\nretrieval and text -> graph retrieval. We show that better conditioning on the\ngraph provides gains in generation and retrieval quality but there is still\nlarge room for improvement.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:18:30 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Luyu", ""], ["Li", "Yujia", ""], ["Aslan", "Ozlem", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2107.09609", "submitter": "Jie Lei", "authors": "Jie Lei, Tamara L. Berg, Mohit Bansal", "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural\n  Language Queries", "comments": "17 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHighlights) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, Moment-DETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:42:58 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lei", "Jie", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2107.09622", "submitter": "Zeeshan Khan", "authors": "Zeeshan Khan, Kartheek Akella, Vinay P. Namboodiri, C V Jawahar", "title": "More Parameters? No Thanks!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work studies the long-standing problems of model capacity and negative\ninterference in multilingual neural machine translation MNMT. We use network\npruning techniques and observe that pruning 50-70% of the parameters from a\ntrained MNMT model results only in a 0.29-1.98 drop in the BLEU score.\nSuggesting that there exist large redundancies even in MNMT models. These\nobservations motivate us to use the redundant parameters and counter the\ninterference problem efficiently. We propose a novel adaptation strategy, where\nwe iteratively prune and retrain the redundant parameters of an MNMT to improve\nbilingual representations while retaining the multilinguality. Negative\ninterference severely affects high resource languages, and our method\nalleviates it without any additional adapter modules. Hence, we call it\nparameter-free adaptation strategy, paving way for the efficient adaptation of\nMNMT. We demonstrate the effectiveness of our method on a 9 language MNMT\ntrained on TED talks, and report an average improvement of +1.36 BLEU on high\nresource pairs. Code will be released here.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:04:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Khan", "Zeeshan", ""], ["Akella", "Kartheek", ""], ["Namboodiri", "Vinay P.", ""], ["Jawahar", "C V", ""]]}, {"id": "2107.09625", "submitter": "Shuang Ao", "authors": "Shuang Ao, Xeno Acharya", "title": "Learning ULMFiT and Self-Distillation with Calibration for Medical\n  Dialogue System", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A medical dialogue system is essential for healthcare service as providing\nprimary clinical advice and diagnoses. It has been gradually adopted and\npracticed in medical organizations in the form of a conversational bot, largely\ndue to the advancement of NLP. In recent years, the introduction of\nstate-of-the-art deep learning models and transfer learning techniques like\nUniversal Language Model Fine Tuning (ULMFiT) and Knowledge Distillation (KD)\nlargely contributes to the performance of NLP tasks. However, some deep neural\nnetworks are poorly calibrated and wrongly estimate the uncertainty. Hence the\nmodel is not trustworthy, especially in sensitive medical decision-making\nsystems and safety tasks. In this paper, we investigate the well-calibrated\nmodel for ULMFiT and self-distillation (SD) in a medical dialogue system. The\ncalibrated ULMFiT (CULMFiT) is obtained by incorporating label smoothing (LS),\na commonly used regularization technique to achieve a well-calibrated model.\nMoreover, we apply the technique to recalibrate the confidence score called\ntemperature scaling (TS) with KD to observe its correlation with network\ncalibration. To further understand the relation between SD and calibration, we\nuse both fixed and optimal temperatures to fine-tune the whole model. All\nexperiments are conducted on the consultation backpain dataset collected by\nexperts then further validated using a large publicly medial dialogue corpus.\nWe empirically show that our proposed methodologies outperform conventional\nmethods in terms of accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:11:24 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ao", "Shuang", ""], ["Acharya", "Xeno", ""]]}, {"id": "2107.09648", "submitter": "James Michaelov", "authors": "James A. Michaelov, Megan D. Bardolph, Seana Coulson, Benjamin K.\n  Bergen", "title": "Different kinds of cognitive plausibility: why are transformers better\n  than RNNs at predicting N400 amplitude?", "comments": null, "journal-ref": "Proceedings of the 43rd Annual Meeting of the Cognitive Science\n  Society (2021) 300-306", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:33:13 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Michaelov", "James A.", ""], ["Bardolph", "Megan D.", ""], ["Coulson", "Seana", ""], ["Bergen", "Benjamin K.", ""]]}, {"id": "2107.09710", "submitter": "Tushar Sarkar", "authors": "Tushar Sarkar, Nishant Rajadhyaksha", "title": "TLA: Twitter Linguistic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linguistics has been instrumental in developing a deeper understanding of\nhuman nature. Words are indispensable to bequeath the thoughts, emotions, and\npurpose of any human interaction, and critically analyzing these words can\nelucidate the social and psychological behavior and characteristics of these\nsocial animals. Social media has become a platform for human interaction on a\nlarge scale and thus gives us scope for collecting and using that data for our\nstudy. However, this entire process of collecting, labeling, and analyzing this\ndata iteratively makes the entire procedure cumbersome. To make this entire\nprocess easier and structured, we would like to introduce TLA(Twitter\nLinguistic Analysis). In this paper, we describe TLA and provide a basic\nunderstanding of the framework and discuss the process of collecting, labeling,\nand analyzing data from Twitter for a corpus of languages while providing\ndetailed labeled datasets for all the languages and the models are trained on\nthese datasets. The analysis provided by TLA will also go a long way in\nunderstanding the sentiments of different linguistic communities and come up\nwith new and innovative solutions for their problems based on the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:25:48 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Sarkar", "Tushar", ""], ["Rajadhyaksha", "Nishant", ""]]}, {"id": "2107.09729", "submitter": "Uri Shaham", "authors": "Uri Shaham and Omer Levy", "title": "What Do You Get When You Cross Beam Search with Nucleus Sampling?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine beam search with the probabilistic pruning technique of nucleus\nsampling to create two deterministic nucleus search algorithms for natural\nlanguage generation. The first algorithm, p-exact search, locally prunes the\nnext-token distribution and performs an exact search over the remaining space.\nThe second algorithm, dynamic beam search, shrinks and expands the beam size\naccording to the entropy of the candidate's probability distribution. Despite\nthe probabilistic intuition behind nucleus search, experiments on machine\ntranslation and summarization benchmarks show that both algorithms reach the\nsame performance levels as standard beam search.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:59:14 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 11:49:36 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Shaham", "Uri", ""], ["Levy", "Omer", ""]]}, {"id": "2107.09768", "submitter": "Mehdi Ghatee Dr.", "authors": "Sajad Dadgar, Mehdi Ghatee", "title": "Checkovid: A COVID-19 misinformation detection system on Twitter using\n  network and content mining perspectives", "comments": "20 Pages, 18 Figures, 7 Tables, Submitted for Review Process in a\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the COVID-19 pandemic, social media platforms were ideal for\ncommunicating due to social isolation and quarantine. Also, it was the primary\nsource of misinformation dissemination on a large scale, referred to as the\ninfodemic. Therefore, automatic debunking misinformation is a crucial problem.\nTo tackle this problem, we present two COVID-19 related misinformation datasets\non Twitter and propose a misinformation detection system comprising\nnetwork-based and content-based processes based on machine learning algorithms\nand NLP techniques. In the network-based process, we focus on social\nproperties, network characteristics, and users. On the other hand, we classify\nmisinformation using the content of the tweets directly in the content-based\nprocess, which contains text classification models (paragraph-level and\nsentence-level) and similarity models. The evaluation results on the\nnetwork-based process show the best results for the artificial neural network\nmodel with an F1 score of 88.68%. In the content-based process, our novel\nsimilarity models, which obtained an F1 score of 90.26%, show an improvement in\nthe misinformation classification results compared to the network-based models.\nIn addition, in the text classification models, the best result was achieved\nusing the stacking ensemble-learning model by obtaining an F1 score of 95.18%.\nFurthermore, we test our content-based models on the Constraint@AAAI2021\ndataset, and by getting an F1 score of 94.38%, we improve the baseline results.\nFinally, we develop a fact-checking website called Checkovid that uses each\nprocess to detect misinformative and informative claims in the domain of\nCOVID-19 from different perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 20:58:23 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Dadgar", "Sajad", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "2107.09840", "submitter": "Weijia Xu", "authors": "Weijia Xu, Batool Haider, Jason Krone and Saab Mansour", "title": "Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual\n  Transfer", "comments": "MetaNLP at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:16:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Xu", "Weijia", ""], ["Haider", "Batool", ""], ["Krone", "Jason", ""], ["Mansour", "Saab", ""]]}, {"id": "2107.09846", "submitter": "Zhongyang Li", "authors": "Zhongyang Li, Xiao Ding, Ting Liu, J. Edward Hu, Benjamin Van Durme", "title": "Guided Generation of Cause and Effect", "comments": "accepted in IJCAI 2020 main track", "journal-ref": null, "doi": "10.24963/ijcai.2020/502", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a conditional text generation framework that posits sentential\nexpressions of possible causes and effects. This framework depends on two novel\nresources we develop in the course of this work: a very large-scale collection\nof English sentences expressing causal patterns CausalBank; and a refinement\nover previous work on constructing large lexical causal knowledge graphs Cause\nEffect Graph. Further, we extend prior work in lexically-constrained decoding\nto support disjunctive positive constraints. Human assessment confirms that our\napproach gives high-quality and diverse outputs. Finally, we use CausalBank to\nperform continued training of an encoder supporting a recent state-of-the-art\nmodel for causal reasoning, leading to a 3-point improvement on the COPA\nchallenge set, with no change in model architecture.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:32:47 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Zhongyang", ""], ["Ding", "Xiao", ""], ["Liu", "Ting", ""], ["Hu", "J. Edward", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2107.09852", "submitter": "Zhongyang Li", "authors": "Zhongyang Li, Xiao Ding, Kuo Liao, Ting Liu, Bing Qin", "title": "CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with\n  Minimal Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown success in incorporating pre-trained models like BERT\nto improve NLP systems. However, existing pre-trained models lack of causal\nknowledge which prevents today's NLP systems from thinking like humans. In this\npaper, we investigate the problem of injecting causal knowledge into\npre-trained models. There are two fundamental problems: 1) how to collect a\nlarge-scale causal resource from unstructured texts; 2) how to effectively\ninject causal knowledge into pre-trained models. To address these issues, we\npropose CausalBERT, which collects the largest scale of causal resource using\nprecise causal patterns and causal embedding techniques. In addition, we adopt\na regularization-based method to preserve the already learned knowledge with an\nextra regularization term while injecting causal knowledge. Extensive\nexperiments on 7 datasets, including four causal pair classification tasks, two\ncausal QA tasks and a causal inference task, demonstrate that CausalBERT\ncaptures rich causal knowledge and outperforms all pre-trained models-based\nstate-of-the-art methods, achieving a new causal inference benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:49:46 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Zhongyang", ""], ["Ding", "Xiao", ""], ["Liao", "Kuo", ""], ["Liu", "Ting", ""], ["Qin", "Bing", ""]]}, {"id": "2107.09881", "submitter": "Joseph Marvin Imperial", "authors": "Joseph Marvin Imperial", "title": "How Do Pedophiles Tweet? Investigating the Writing Styles and Online\n  Personas of Child Cybersex Traffickers in the Philippines", "comments": "Submitted as a short paper for a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most important humanitarian responsibility of every individual is\nto protect the future of our children. This entails not only protection of\nphysical welfare but also from ill events that can potentially affect the\nmental well-being of a child such as sexual coercion and abuse which, in\nworst-case scenarios, can result to lifelong trauma. In this study, we perform\na preliminary investigation of how child sex peddlers spread illegal\npornographic content and target minors for sexual activities on Twitter in the\nPhilippines using Natural Language Processing techniques. Results of our\nstudies show frequently used and co-occurring words that traffickers use to\nspread content as well as four main roles played by these entities that\ncontribute to the proliferation of child pornography in the country.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 05:26:52 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Imperial", "Joseph Marvin", ""]]}, {"id": "2107.09931", "submitter": "Shreya Pathak", "authors": "Archiki Prasad, Mohammad Ali Rehan, Shreya Pathak, Preethi Jyothi", "title": "The Effectiveness of Intermediate-Task Training for Code-Switched\n  Natural Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While recent benchmarks have spurred a lot of new work on improving the\ngeneralization of pretrained multilingual language models on multilingual\ntasks, techniques to improve code-switched natural language understanding tasks\nhave been far less explored. In this work, we propose the use of bilingual\nintermediate pretraining as a reliable technique to derive large and consistent\nperformance gains on three different NLP tasks using code-switched text. We\nachieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the\nmean accuracies and F1 scores over previous state-of-the-art systems for\nHindi-English Natural Language Inference (NLI), Question Answering (QA) tasks,\nand Spanish-English Sentiment Analysis (SA) respectively. We show consistent\nperformance gains on four different code-switched language-pairs\n(Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA.\nWe also present a code-switched masked language modelling (MLM) pretraining\ntechnique that consistently benefits SA compared to standard MLM pretraining\nusing real code-switched text.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 08:10:59 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Prasad", "Archiki", ""], ["Rehan", "Mohammad Ali", ""], ["Pathak", "Shreya", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2107.09948", "submitter": "Alex John Quijano", "authors": "Alex John Quijano, Rick Dale, and Suzanne Sindi", "title": "A Statistical Model of Word Rank Evolution", "comments": "This manuscript - with 53 pages and 28 figures - is a draft for a\n  journal research article submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of large linguistic data sets enables data-driven approaches\nto study linguistic change. This work explores the word rank dynamics of eight\nlanguages by investigating the Google Books corpus unigram frequency data set.\nWe observed the rank changes of the unigrams from 1900 to 2008 and compared it\nto a Wright-Fisher inspired model that we developed for our analysis. The model\nsimulates a neutral evolutionary process with the restriction of having no\ndisappearing words. This work explains the mathematical framework of the model\n- written as a Markov Chain with multinomial transition probabilities - to show\nhow frequencies of words change in time. From our observations in the data and\nour model, word rank stability shows two types of characteristics: (1) the\nincrease/decrease in ranks are monotonic, or (2) the average rank stays the\nsame. Based on our model, high-ranked words tend to be more stable while\nlow-ranked words tend to be more volatile. Some words change in ranks in two\nways: (a) by an accumulation of small increasing/decreasing rank changes in\ntime and (b) by shocks of increase/decrease in ranks. Most of the stopwords and\nSwadesh words are observed to be stable in ranks across eight languages. These\nsignatures suggest unigram frequencies in all languages have changed in a\nmanner inconsistent with a purely neutral evolutionary process.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 08:57:32 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 06:41:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Quijano", "Alex John", ""], ["Dale", "Rick", ""], ["Sindi", "Suzanne", ""]]}, {"id": "2107.09980", "submitter": "Jannik Fischbach", "authors": "Jannik Fischbach, Tobias Springer, Julian Frattini, Henning Femmer,\n  Andreas Vogelsang, and Daniel Mendez", "title": "Fine-Grained Causality Extraction From Natural Language Requirements\n  Using Recursive Neural Tensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Context:] Causal relations (e.g., If A, then B) are prevalent in functional\nrequirements. For various applications of AI4RE, e.g., the automatic derivation\nof suitable test cases from requirements, automatically extracting such causal\nstatements are a basic necessity. [Problem:] We lack an approach that is able\nto extract causal relations from natural language requirements in fine-grained\nform. Specifically, existing approaches do not consider the combinatorics\nbetween causes and effects. They also do not allow to split causes and effects\ninto more granular text fragments (e.g., variable and condition), making the\nextracted relations unsuitable for automatic test case derivation. [Objective &\nContributions:] We address this research gap and make the following\ncontributions: First, we present the Causality Treebank, which is the first\ncorpus of fully labeled binary parse trees representing the composition of\n1,571 causal requirements. Second, we propose a fine-grained causality\nextractor based on Recursive Neural Tensor Networks. Our approach is capable of\nrecovering the composition of causal statements written in natural language and\nachieves a F1 score of 74 % in the evaluation on the Causality Treebank. Third,\nwe disclose our open data sets as well as our code to foster the discourse on\nthe automatic extraction of causality in the RE community.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:52:10 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:43:30 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fischbach", "Jannik", ""], ["Springer", "Tobias", ""], ["Frattini", "Julian", ""], ["Femmer", "Henning", ""], ["Vogelsang", "Andreas", ""], ["Mendez", "Daniel", ""]]}, {"id": "2107.10021", "submitter": "Henry Watkins", "authors": "Henry Watkins, Robert Gray, Ashwani Jha, Parashkev Nachev", "title": "An artificial intelligence natural language processing pipeline for\n  information extraction in neuroradiology", "comments": "20 pages, 2 png image figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of electronic health records in medical research is difficult because\nof the unstructured format. Extracting information within reports and\nsummarising patient presentations in a way amenable to downstream analysis\nwould be enormously beneficial for operational and clinical research. In this\nwork we present a natural language processing pipeline for information\nextraction of radiological reports in neurology. Our pipeline uses a hybrid\nsequence of rule-based and artificial intelligence models to accurately extract\nand summarise neurological reports. We train and evaluate a custom language\nmodel on a corpus of 150000 radiological reports from National Hospital for\nNeurology and Neurosurgery, London MRI imaging. We also present results for\nstandard NLP tasks on domain-specific neuroradiology datasets. We show our\npipeline, called `neuroNLP', can reliably extract clinically relevant\ninformation from these reports, enabling downstream modelling of reports and\nassociated imaging on a heretofore unprecedented scale.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:31:57 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Watkins", "Henry", ""], ["Gray", "Robert", ""], ["Jha", "Ashwani", ""], ["Nachev", "Parashkev", ""]]}, {"id": "2107.10023", "submitter": "Jannik Fischbach", "authors": "Noah Jadallah, Jannik Fischbach, Julian Frattini, and Andreas\n  Vogelsang", "title": "CATE: CAusality Tree Extractor from Natural Language Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal relations (If A, then B) are prevalent in requirements artifacts.\nAutomatically extracting causal relations from requirements holds great\npotential for various RE activities (e.g., automatic derivation of suitable\ntest cases). However, we lack an approach capable of extracting causal\nrelations from natural language with reasonable performance. In this paper, we\npresent our tool CATE (CAusality Tree Extractor), which is able to parse the\ncomposition of a causal relation as a tree structure. CATE does not only\nprovide an overview of causes and effects in a sentence, but also reveals their\nsemantic coherence by translating the causal relation into a binary tree. We\nencourage fellow researchers and practitioners to use CATE at\nhttps://causalitytreeextractor.com/\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:37:31 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:36:56 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jadallah", "Noah", ""], ["Fischbach", "Jannik", ""], ["Frattini", "Julian", ""], ["Vogelsang", "Andreas", ""]]}, {"id": "2107.10042", "submitter": "Jan Lehe\\v{c}ka", "authors": "Jan Lehe\\v{c}ka, Jan \\v{S}vec", "title": "Comparison of Czech Transformers on Text Classification Tasks", "comments": "https://huggingface.co/fav-kky", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present our progress in pre-training monolingual\nTransformers for Czech and contribute to the research community by releasing\nour models for public. The need for such models emerged from our effort to\nemploy Transformers in our language-specific tasks, but we found the\nperformance of the published multilingual models to be very limited. Since the\nmultilingual models are usually pre-trained from 100+ languages, most of\nlow-resourced languages (including Czech) are under-represented in these\nmodels. At the same time, there is a huge amount of monolingual training data\navailable in web archives like Common Crawl. We have pre-trained and publicly\nreleased two monolingual Czech Transformers and compared them with relevant\npublic models, trained (at least partially) for Czech. The paper presents the\nTransformers pre-training procedure as well as a comparison of pre-trained\nmodels on text classification task from various domains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:22:34 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lehe\u010dka", "Jan", ""], ["\u0160vec", "Jan", ""]]}, {"id": "2107.10137", "submitter": "Lin Pan", "authors": "Lin Pan, Chung-Wei Hang, Avirup Sil, Saloni Potdar, Mo Yu", "title": "Improved Text Classification via Contrastive Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple and general method to regularize the fine-tuning of\nTransformer-based encoders for text classification tasks. Specifically, during\nfine-tuning we generate adversarial examples by perturbing the word embeddings\nof the model and perform contrastive learning on clean and adversarial examples\nin order to teach the model to learn noise-invariant representations. By\ntraining on both clean and adversarial examples along with the additional\ncontrastive objective, we observe consistent improvement over standard\nfine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned\nBERT Large model outperforms BERT Large baseline by 1.7% on average, and our\nfine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We\nadditionally validate our method in different domains using three intent\nclassification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa\nLarge baseline by 1-2% on average.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:14:15 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Pan", "Lin", ""], ["Hang", "Chung-Wei", ""], ["Sil", "Avirup", ""], ["Potdar", "Saloni", ""], ["Yu", "Mo", ""]]}, {"id": "2107.10181", "submitter": "Ayush Suhane", "authors": "Srijan Bansal, Vishal Garimella, Ayush Suhane, Animesh Mukherjee", "title": "Debiasing Multilingual Word Embeddings: A Case Study of Three Indian\n  Languages", "comments": "This work is accepted as a long paper in the proceedings of ACM\n  HyperText 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advance the current state-of-the-art method for debiasing\nmonolingual word embeddings so as to generalize well in a multilingual setting.\nWe consider different methods to quantify bias and different debiasing\napproaches for monolingual as well as multilingual settings. We demonstrate the\nsignificance of our bias-mitigation approach on downstream NLP applications.\nOur proposed methods establish the state-of-the-art performance for debiasing\nmultilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu\nin addition to English. We believe that our work will open up new opportunities\nin building unbiased downstream NLP applications that are inherently dependent\non the quality of the word embeddings used.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:12:51 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 16:57:31 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bansal", "Srijan", ""], ["Garimella", "Vishal", ""], ["Suhane", "Ayush", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2107.10188", "submitter": "Qingxiang Wang", "authors": "Qingxiang Wang, Cezary Kaliszyk", "title": "JEFL: Joint Embedding of Formal Proof Libraries", "comments": "Submission to FroCoS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The heterogeneous nature of the logical foundations used in different\ninteractive proof assistant libraries has rendered discovery of similar\nmathematical concepts among them difficult. In this paper, we compare a\npreviously proposed algorithm for matching concepts across libraries with our\nunsupervised embedding approach that can help us retrieve similar concepts. Our\napproach is based on the fasttext implementation of Word2Vec, on top of which a\ntree traversal module is added to adapt its algorithm to the representation\nformat of our data export pipeline. We compare the explainability,\ncustomizability, and online-servability of the approaches and argue that the\nneural embedding approach has more potential to be integrated into an\ninteractive proof assistant.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:31:33 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Qingxiang", ""], ["Kaliszyk", "Cezary", ""]]}, {"id": "2107.10204", "submitter": "Shruti Phadke", "authors": "Shruti Phadke, Mattia Samory, Tanushree Mitra", "title": "Characterizing Social Imaginaries and Self-Disclosures of Dissonance in\n  Online Conspiracy Discussion Communities", "comments": "Accepted at CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Online discussion platforms offer a forum to strengthen and propagate belief\nin misinformed conspiracy theories. Yet, they also offer avenues for conspiracy\ntheorists to express their doubts and experiences of cognitive dissonance. Such\nexpressions of dissonance may shed light on who abandons misguided beliefs and\nunder which circumstances. This paper characterizes self-disclosures of\ndissonance about QAnon, a conspiracy theory initiated by a mysterious leader Q\nand popularized by their followers, anons in conspiracy theory subreddits. To\nunderstand what dissonance and disbelief mean within conspiracy communities, we\nfirst characterize their social imaginaries, a broad understanding of how\npeople collectively imagine their social existence. Focusing on 2K posts from\ntwo image boards, 4chan and 8chan, and 1.2 M comments and posts from 12\nsubreddits dedicated to QAnon, we adopt a mixed methods approach to uncover the\nsymbolic language representing the movement, expectations, practices, heroes\nand foes of the QAnon community. We use these social imaginaries to create a\ncomputational framework for distinguishing belief and dissonance from general\ndiscussion about QAnon. Further, analyzing user engagement with QAnon\nconspiracy subreddits, we find that self-disclosures of dissonance correlate\nwith a significant decrease in user contributions and ultimately with their\ndeparture from the community. We contribute a computational framework for\nidentifying dissonance self-disclosures and measuring the changes in user\nengagement surrounding dissonance. Our work can provide insights into designing\ndissonance-based interventions that can potentially dissuade conspiracists from\nonline conspiracy discussion communities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:49:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Phadke", "Shruti", ""], ["Samory", "Mattia", ""], ["Mitra", "Tanushree", ""]]}, {"id": "2107.10251", "submitter": "Dana Kenna", "authors": "Dana Kenna", "title": "Using Adversarial Debiasing to Remove Bias from Word Embeddings", "comments": "7 pages, 9 Figures - Adapted from my thesis project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Word Embeddings have been shown to contain the societal biases present in the\noriginal corpora. Existing methods to deal with this problem have been shown to\nonly remove superficial biases. The method of Adversarial Debiasing was\npresumed to be similarly superficial, but this is was not verified in previous\nworks. Using the experiments that demonstrated the shallow removal in other\nmethods, I show results that suggest Adversarial Debiasing is more effective at\nremoving bias and thus motivate further investigation on the utility of\nAdversarial Debiasing.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:56:55 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 01:31:28 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kenna", "Dana", ""]]}, {"id": "2107.10300", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha and Vinija Jain", "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural\n  Language with Interpretability", "comments": "12 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:56:34 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chadha", "Aman", ""], ["Jain", "Vinija", ""]]}, {"id": "2107.10314", "submitter": "Christopher Schr\\\"oder", "authors": "Christopher Schr\\\"oder, Lydia M\\\"uller, Andreas Niekler, Martin\n  Potthast", "title": "Small-text: Active Learning for Text Classification in Python", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present small-text, a simple modular active learning library, which offers\npool-based active learning for text classification in Python. It comes with\nvarious pre-implemented state-of-the-art query strategies, including some which\ncan leverage the GPU. Clearly defined interfaces allow to combine a multitude\nof such query strategies with different classifiers, thereby facilitating a\nquick mix and match, and enabling a rapid development of both active learning\nexperiments and applications. To make various classifiers accessible in a\nconsistent way, it integrates several well-known machine learning libraries,\nnamely, scikit-learn, PyTorch, and huggingface transformers -- for which the\nlatter integrations are available as optionally installable extensions. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 19:23:56 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Schr\u00f6der", "Christopher", ""], ["M\u00fcller", "Lydia", ""], ["Niekler", "Andreas", ""], ["Potthast", "Martin", ""]]}, {"id": "2107.10326", "submitter": "Ali Balali", "authors": "Ali Balali, Masoud Asadpour, Seyed Hossein Jafari", "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an\n  online annotation tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 19:43:22 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Balali", "Ali", ""], ["Asadpour", "Masoud", ""], ["Jafari", "Seyed Hossein", ""]]}, {"id": "2107.10328", "submitter": "Vladimir Vargas-Calder\\'on", "authors": "Vladimir Vargas-Calder\\'on, Andreina Moros Ochoa, Gilmer Yovani Castro\n  Nieto and Jorge E. Camargo", "title": "Machine learning for assessing quality of service in the hospitality\n  sector based on customer reviews", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s40558-021-00207-4", "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing use of online hospitality platforms provides firsthand\ninformation about clients preferences, which are essential to improve hotel\nservices and increase the quality of service perception. Customer reviews can\nbe used to automatically extract the most relevant aspects of the quality of\nservice for hospitality clientele. This paper proposes a framework for the\nassessment of the quality of service in the hospitality sector based on the\nexploitation of customer reviews through natural language processing and\nmachine learning methods. The proposed framework automatically discovers the\nquality of service aspects relevant to hotel customers. Hotel reviews from\nBogot\\'a and Madrid are automatically scrapped from Booking.com. Semantic\ninformation is inferred through Latent Dirichlet Allocation and FastText, which\nallow representing text reviews as vectors. A dimensionality reduction\ntechnique is applied to visualise and interpret large amounts of customer\nreviews. Visualisations of the most important quality of service aspects are\ngenerated, allowing to qualitatively and quantitatively assess the quality of\nservice. Results show that it is possible to automatically extract the main\nquality of service aspects perceived by customers from large customer review\ndatasets. These findings could be used by hospitality managers to understand\nclients better and to improve the quality of service.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 19:45:40 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Vargas-Calder\u00f3n", "Vladimir", ""], ["Ochoa", "Andreina Moros", ""], ["Nieto", "Gilmer Yovani Castro", ""], ["Camargo", "Jorge E.", ""]]}, {"id": "2107.10342", "submitter": "Mikhail Burtsev", "authors": "Mikhail Burtsev and Anna Rumshisky", "title": "Multi-Stream Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer-based encoder-decoder models produce a fused token-wise\nrepresentation after every encoder layer. We investigate the effects of\nallowing the encoder to preserve and explore alternative hypotheses, combined\nat the end of the encoding process. To that end, we design and examine a\n$\\textit{Multi-stream Transformer}$ architecture and find that splitting the\nTransformer encoder into multiple encoder streams and allowing the model to\nmerge multiple representational hypotheses improves performance, with further\nimprovement obtained by adding a skip connection between the first and the\nfinal encoder layer.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 20:16:57 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Burtsev", "Mikhail", ""], ["Rumshisky", "Anna", ""]]}, {"id": "2107.10410", "submitter": "Kai-Hui Liang", "authors": "Kai-Hui Liang, Patrick Lange, Yoo Jung Oh, Jingwen Zhang, Yoshimi\n  Fukuoka, Zhou Yu", "title": "Evaluation of In-Person Counseling Strategies To Develop Physical\n  Activity Chatbot for Women", "comments": "Accepted by SIGDIAL 2021 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence chatbots are the vanguard in technology-based\nintervention to change people's behavior. To develop intervention chatbots, the\nfirst step is to understand natural language conversation strategies in human\nconversation. This work introduces an intervention conversation dataset\ncollected from a real-world physical activity intervention program for women.\nWe designed comprehensive annotation schemes in four dimensions (domain,\nstrategy, social exchange, and task-focused exchange) and annotated a subset of\ndialogs. We built a strategy classifier with context information to detect\nstrategies from both trainers and participants based on the annotation. To\nunderstand how human intervention induces effective behavior changes, we\nanalyzed the relationships between the intervention strategies and the\nparticipants' changes in the barrier and social support for physical activity.\nWe also analyzed how participant's baseline weight correlates to the amount of\noccurrence of the corresponding strategy. This work lays the foundation for\ndeveloping a personalized physical activity intervention bot. The dataset and\ncode are available at\nhttps://github.com/KaihuiLiang/physical-activity-counseling\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 00:39:21 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Liang", "Kai-Hui", ""], ["Lange", "Patrick", ""], ["Oh", "Yoo Jung", ""], ["Zhang", "Jingwen", ""], ["Fukuoka", "Yoshimi", ""], ["Yu", "Zhou", ""]]}, {"id": "2107.10413", "submitter": "Alina Arseniev-Koehler", "authors": "Alina Arseniev-Koehler", "title": "Theoretical foundations and limits of word embeddings: what types of\n  meaning can they capture?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring meaning is a central problem in cultural sociology and word\nembeddings may offer powerful new tools to do so. But like any tool, they build\non and exert theoretical assumptions. In this paper I theorize the ways in\nwhich word embeddings model three core premises of a structural linguistic\ntheory of meaning: that meaning is relational, coherent, and may be analyzed as\na static system. In certain ways, word embedding methods are vulnerable to the\nsame, enduring critiques of these premises. In other ways, they offer novel\nsolutions to these critiques. More broadly, formalizing the study of meaning\nwith word embeddings offers theoretical opportunities to clarify core concepts\nand debates in cultural sociology, such as the coherence of meaning. Just as\nnetwork analysis specified the once vague notion of social relations (Borgatti\net al. 2009), formalizing meaning with embedding methods can push us to specify\nand reimagine meaning itself.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 00:40:33 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Arseniev-Koehler", "Alina", ""]]}, {"id": "2107.10427", "submitter": "Yijin Liu", "authors": "Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu and Jie Zhou", "title": "Confidence-Aware Scheduled Sampling for Neural Machine Translation", "comments": "Findings of ACL-2021, code at\n  https://github.com/Adaxry/conf_aware_ss4nmt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scheduled sampling is an effective method to alleviate the exposure bias\nproblem of neural machine translation. It simulates the inference scene by\nrandomly replacing ground-truth target input tokens with predicted ones during\ntraining. Despite its success, its critical schedule strategies are merely\nbased on training steps, ignoring the real-time model competence, which limits\nits potential performance and convergence speed. To address this issue, we\npropose confidence-aware scheduled sampling. Specifically, we quantify\nreal-time model competence by the confidence of model predictions, based on\nwhich we design fine-grained schedule strategies. In this way, the model is\nexactly exposed to predicted tokens for high-confidence positions and still\nground-truth tokens for low-confidence positions. Moreover, we observe vanilla\nscheduled sampling suffers from degenerating into the original teacher forcing\nmode since most predicted tokens are the same as ground-truth tokens.\nTherefore, under the above confidence-aware strategy, we further expose more\nnoisy tokens (e.g., wordy and incorrect word order) instead of predicted ones\nfor high-confidence token positions. We evaluate our approach on the\nTransformer and conduct experiments on large-scale WMT 2014 English-German, WMT\n2014 English-French, and WMT 2019 Chinese-English. Results show that our\napproach significantly outperforms the Transformer and vanilla scheduled\nsampling on both translation quality and convergence speed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 02:49:04 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Liu", "Yijin", ""], ["Meng", "Fandong", ""], ["Chen", "Yufeng", ""], ["Xu", "Jinan", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.10434", "submitter": "Chengzhi Zhang", "authors": "Qingqing Zhou, Chengzhi Zhang", "title": "Impacts Towards a comprehensive assessment of the book impact by\n  integrating multiple evaluation sources", "comments": null, "journal-ref": "Journal of Informetrics, 2021. 15(3): 101162", "doi": "10.1016/j.joi.2021.101195", "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The surge in the number of books published makes the manual evaluation\nmethods difficult to efficiently evaluate books. The use of books' citations\nand alternative evaluation metrics can assist manual evaluation and reduce the\ncost of evaluation. However, most existing evaluation research was based on a\nsingle evaluation source with coarse-grained analysis, which may obtain\nincomprehensive or one-sided evaluation results of book impact. Meanwhile,\nrelying on a single resource for book assessment may lead to the risk that the\nevaluation results cannot be obtained due to the lack of the evaluation data,\nespecially for newly published books. Hence, this paper measured book impact\nbased on an evaluation system constructed by integrating multiple evaluation\nsources. Specifically, we conducted finer-grained mining on the multiple\nevaluation sources, including books' internal evaluation resources and external\nevaluation resources. Various technologies (e.g. topic extraction, sentiment\nanalysis, text classification) were used to extract corresponding evaluation\nmetrics from the internal and external evaluation resources. Then, Expert\nevaluation combined with analytic hierarchy process was used to integrate the\nevaluation metrics and construct a book impact evaluation system. Finally, the\nreliability of the evaluation system was verified by comparing with the results\nof expert evaluation, detailed and diversified evaluation results were then\nobtained. The experimental results reveal that differential evaluation\nresources can measure the books' impacts from different dimensions, and the\nintegration of multiple evaluation data can assess books more comprehensively.\nMeanwhile, the book impact evaluation system can provide personalized\nevaluation results according to the users' evaluation purposes. In addition,\nthe disciplinary differences should be considered for assessing books' impacts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 03:11:10 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhou", "Qingqing", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2107.10443", "submitter": "Eugene Bagdasaryan", "authors": "Eugene Bagdasaryan and Vitaly Shmatikov", "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization.\n  We introduce the concept of a \"meta-backdoor\" to explain model-spinning\nattacks. These attacks produce models whose output is valid and preserves\ncontext, yet also satisfies a meta-task chosen by the adversary (e.g., positive\nsentiment). Previously studied backdoors in language models simply flip\nsentiment labels or replace words without regard to context. Their outputs are\nincorrect on inputs with the trigger. Meta-backdoors, on the other hand, are\nthe first class of backdoors that can be deployed against seq2seq models to (a)\nintroduce adversary-chosen spin into the output, while (b) maintaining standard\naccuracy metrics.\n  To demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment.\n  We explain why model spinning can be a dangerous technique in AI-powered\ndisinformation and discuss how to mitigate these attacks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 03:41:52 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bagdasaryan", "Eugene", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "2107.10474", "submitter": "Jung Hoon Lee", "authors": "Junghoon Lee, Jounghee Kim, Pilsung Kang", "title": "Back-Translated Task Adaptive Pretraining: Improving Accuracy and\n  Robustness on Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) pretrained on a large text corpus and fine-tuned on a\ndownstream text corpus and fine-tuned on a downstream task becomes a de facto\ntraining strategy for several natural language processing (NLP) tasks.\nRecently, an adaptive pretraining method retraining the pretrained language\nmodel with task-relevant data has shown significant performance improvements.\nHowever, current adaptive pretraining methods suffer from underfitting on the\ntask distribution owing to a relatively small amount of data to re-pretrain the\nLM. To completely use the concept of adaptive pretraining, we propose a\nback-translated task-adaptive pretraining (BT-TAPT) method that increases the\namount of task-specific data for LM re-pretraining by augmenting the task data\nusing back-translation to generalize the LM to the target task domain. The\nexperimental results show that the proposed BT-TAPT yields improved\nclassification accuracy on both low- and high-resource data and better\nrobustness to noise than the conventional adaptive pretraining method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 06:27:35 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Lee", "Junghoon", ""], ["Kim", "Jounghee", ""], ["Kang", "Pilsung", ""]]}, {"id": "2107.10523", "submitter": "Ying Zhang", "authors": "Ying Zhang, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou", "title": "Target-Oriented Fine-tuning for Zero-Resource Named Entity Recognition", "comments": "9 pages, ACL21 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-resource named entity recognition (NER) severely suffers from data\nscarcity in a specific domain or language. Most studies on zero-resource NER\ntransfer knowledge from various data by fine-tuning on different auxiliary\ntasks. However, how to properly select training data and fine-tuning tasks is\nstill an open problem. In this paper, we tackle the problem by transferring\nknowledge from three aspects, i.e., domain, language and task, and\nstrengthening connections among them. Specifically, we propose four practical\nguidelines to guide knowledge transfer and task fine-tuning. Based on these\nguidelines, we design a target-oriented fine-tuning (TOF) framework to exploit\nvarious data from three aspects in a unified training manner. Experimental\nresults on six benchmarks show that our method yields consistent improvements\nover baselines in both cross-domain and cross-lingual scenarios. Particularly,\nwe achieve new state-of-the-art performance on five benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 08:48:34 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhang", "Ying", ""], ["Meng", "Fandong", ""], ["Chen", "Yufeng", ""], ["Xu", "Jinan", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.10614", "submitter": "Matej Ul\\v{c}ar", "authors": "Matej Ul\\v{c}ar and Ale\\v{s} \\v{Z}agar and Carlos S. Armendariz and\n  Andra\\v{z} Repar and Senja Pollak and Matthew Purver and Marko\n  Robnik-\\v{S}ikonja", "title": "Evaluation of contextual embeddings on less-resourced languages", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The current dominance of deep neural networks in natural language processing\nis based on contextual embeddings such as ELMo, BERT, and BERT derivatives.\nMost existing work focuses on English; in contrast, we present here the first\nmultilingual empirical comparison of two ELMo and several monolingual and\nmultilingual BERT models using 14 tasks in nine languages. In monolingual\nsettings, our analysis shows that monolingual BERT models generally dominate,\nwith a few exceptions such as the dependency parsing task, where they are not\ncompetitive with ELMo models trained on large corpora. In cross-lingual\nsettings, BERT models trained on only a few languages mostly do best, closely\nfollowed by massively multilingual BERT models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 12:32:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ul\u010dar", "Matej", ""], ["\u017dagar", "Ale\u0161", ""], ["Armendariz", "Carlos S.", ""], ["Repar", "Andra\u017e", ""], ["Pollak", "Senja", ""], ["Purver", "Matthew", ""], ["Robnik-\u0160ikonja", "Marko", ""]]}, {"id": "2107.10637", "submitter": "Ilnar Salimzianov", "authors": "Ilnar Salimzianov", "title": "A baseline model for computationally inexpensive speech recognition for\n  Kazakh using the Coqui STT framework", "comments": "4 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mobile devices are transforming the way people interact with computers, and\nspeech interfaces to applications are ever more important. Automatic Speech\nRecognition systems recently published are very accurate, but often require\npowerful machinery (specialised Graphical Processing Units) for inference,\nwhich makes them impractical to run on commodity devices, especially in\nstreaming mode. Impressed by the accuracy of, but dissatisfied with the\ninference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)\nwhen not using a GPU, we trained a new baseline acoustic model (on the same\ndataset as the aforementioned paper) and three language models for use with the\nCoqui STT framework. Results look promising, but further epochs of training and\nparameter sweeping or, alternatively, limiting the vocabulary that the ASR\nsystem must support, is needed to reach a production-level accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:17:42 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Salimzianov", "Ilnar", ""]]}, {"id": "2107.10648", "submitter": "Shakshi Sharma", "authors": "Mohit Mayank, Shakshi Sharma, Rajesh Sharma", "title": "DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection", "comments": "8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 07:09:59 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mayank", "Mohit", ""], ["Sharma", "Shakshi", ""], ["Sharma", "Rajesh", ""]]}, {"id": "2107.10649", "submitter": "Venktesh Viswanathan", "authors": "Venktesh V, Mukesh Mohania, Vikram Goyal", "title": "TagRec: Automated Tagging of Questions with Hierarchical Learning\n  Taxonomy", "comments": "16 pages, accepted at ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online educational platforms organize academic questions based on a\nhierarchical learning taxonomy (subject-chapter-topic). Automatically tagging\nnew questions with existing taxonomy will help organize these questions into\ndifferent classes of hierarchical taxonomy so that they can be searched based\non the facets like chapter. This task can be formulated as a flat multi-class\nclassification problem. Usually, flat classification based methods ignore the\nsemantic relatedness between the terms in the hierarchical taxonomy and the\nquestions. Some traditional methods also suffer from the class imbalance issues\nas they consider only the leaf nodes ignoring the hierarchy. Hence, we\nformulate the problem as a similarity-based retrieval task where we optimize\nthe semantic relatedness between the taxonomy and the questions. We demonstrate\nthat our method helps to handle the unseen labels and hence can be used for\ntaxonomy tagging in the wild. In this method, we augment the question with its\ncorresponding answer to capture more semantic information and then align the\nquestion-answer pair's contextualized embedding with the corresponding label\n(taxonomy) vector representations. The representations are aligned by\nfine-tuning a transformer based model with a loss function that is a\ncombination of the cosine similarity and hinge rank loss. The loss function\nmaximizes the similarity between the question-answer pair and the correct label\nrepresentations and minimizes the similarity to unrelated labels. Finally, we\nperform experiments on two real-world datasets. We show that the proposed\nlearning method outperforms representations learned using the multi-class\nclassification method and other state of the art methods by 6% as measured by\nRecall@k. We also demonstrate the performance of the proposed method on unseen\nbut related learning content like the learning objectives without re-training\nthe network.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 11:50:55 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["V", "Venktesh", ""], ["Mohania", "Mukesh", ""], ["Goyal", "Vikram", ""]]}, {"id": "2107.10650", "submitter": "Byung-Hak Kim", "authors": "Byung-Hak Kim and Varun Ganapathi", "title": "Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction\n  from Clinical Notes by Machines", "comments": "To appear in Proceedings of Machine Learning Research, Volume 149:\n  Machine Learning for Healthcare Conference (MLHC), Virtual, August 6-7, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of medical codes from clinical notes is both a practical and\nessential need for every healthcare delivery organization within current\nmedical systems. Automating annotation will save significant time and excessive\neffort spent by human coders today. However, the biggest challenge is directly\nidentifying appropriate medical codes out of several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. In the past\nthree years, with Convolutional Neural Networks (CNN) and Long Short-Term\nMemory (LTSM) networks, there have been vast improvements in tackling the most\nchallenging benchmark of the MIMIC-III-full-label inpatient clinical notes\ndataset. This progress raises the fundamental question of how far automated\nmachine learning (ML) systems are from human coders' working performance. We\nassessed the baseline of human coders' performance on the same subsampled\ntesting set. We also present our Read, Attend, and Code (RAC) model for\nlearning the medical code assignment mappings. By connecting convolved\nembeddings with self-attention and code-title guided attention modules,\ncombined with sentence permutation-based data augmentations and stochastic\nweight averaging training, RAC establishes a new state of the art (SOTA),\nconsiderably outperforming the current best Macro-F1 by 18.7%, and reaches past\nthe human-level coding baseline. This new milestone marks a meaningful step\ntoward fully autonomous medical coding (AMC) in machines reaching parity with\nhuman coders' performance in medical code prediction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 06:01:58 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kim", "Byung-Hak", ""], ["Ganapathi", "Varun", ""]]}, {"id": "2107.10651", "submitter": "Erniel Barrios", "authors": "Dominic B. Dayta and Erniel B. Barrios", "title": "Semiparametric Latent Topic Modeling on Consumer-Generated Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Legacy procedures for topic modelling have generally suffered problems of\noverfitting and a weakness towards reconstructing sparse topic structures. With\nmotivation from a consumer-generated corpora, this paper proposes\nsemiparametric topic model, a two-step approach utilizing nonnegative matrix\nfactorization and semiparametric regression in topic modeling. The model\nenables the reconstruction of sparse topic structures in the corpus and\nprovides a generative model for predicting topics in new documents entering the\ncorpus. Assuming the presence of auxiliary information related to the topics,\nthis approach exhibits better performance in discovering underlying topic\nstructures in cases where the corpora are small and limited in vocabulary. In\nan actual consumer feedback corpus, the model also demonstrably provides\ninterpretable and useful topic definitions comparable with those produced by\nother methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 00:22:02 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dayta", "Dominic B.", ""], ["Barrios", "Erniel B.", ""]]}, {"id": "2107.10652", "submitter": "Rajvir Kaur", "authors": "Rajvir Kaur, Jeewani Anupama Ginige and Oliver Obst", "title": "A Systematic Literature Review of Automated ICD Coding and\n  Classification Systems using Discharge Summaries", "comments": "33 pages, 1 figure. Under review in the Journal of Artificial\n  Intelligence in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Codification of free-text clinical narratives have long been recognised to be\nbeneficial for secondary uses such as funding, insurance claim processing and\nresearch. The current scenario of assigning codes is a manual process which is\nvery expensive, time-consuming and error prone. In recent years, many\nresearchers have studied the use of Natural Language Processing (NLP), related\nMachine Learning (ML) and Deep Learning (DL) methods and techniques to resolve\nthe problem of manual coding of clinical narratives and to assist human coders\nto assign clinical codes more accurately and efficiently. This systematic\nliterature review provides a comprehensive overview of automated clinical\ncoding systems that utilises appropriate NLP, ML and DL methods and techniques\nto assign ICD codes to discharge summaries. We have followed the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and\nconducted a comprehensive search of publications from January, 2010 to December\n2020 in four academic databases- PubMed, ScienceDirect, Association for\nComputing Machinery(ACM) Digital Library, and the Association for Computational\nLinguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the\ninclusion criteria. This review identified: datasets having discharge\nsummaries; NLP techniques along with some other data extraction processes,\ndifferent feature extraction and embedding techniques. To measure the\nperformance of classification methods, different evaluation metrics are used.\nLastly, future research directions are provided to scholars who are interested\nin automated ICD code assignment. Efforts are still required to improve ICD\ncode prediction accuracy, availability of large-scale de-identified clinical\ncorpora with the latest version of the classification system. This can be a\nplatform to guide and share knowledge with the less experienced coders and\nresearchers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 03:55:17 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kaur", "Rajvir", ""], ["Ginige", "Jeewani Anupama", ""], ["Obst", "Oliver", ""]]}, {"id": "2107.10655", "submitter": "Mirela Silva", "authors": "Hanyu Shi, Mirela Silva, Daniel Capecci, Luiz Giovanini, Lauren Czech,\n  Juliana Fernandes, Daniela Oliveira", "title": "Lumen: A Machine Learning Framework to Expose Influence Cues in Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phishing and disinformation are popular social engineering attacks with\nattackers invariably applying influence cues in texts to make them more\nappealing to users. We introduce Lumen, a learning-based framework that exposes\ninfluence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)\nobjectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was\ntrained with a newly developed dataset of 3K texts comprised of disinformation,\nphishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in\ncomparison to other learning models showed that Lumen and LSTM presented the\nbest F1-micro score, but Lumen yielded better interpretability. Our results\nhighlight the promise of ML to expose influence cues in text, towards the goal\nof application in automatic labeling tools to improve the accuracy of\nhuman-based detection and reduce the likelihood of users falling for deceptive\nonline content.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:53:13 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Shi", "Hanyu", ""], ["Silva", "Mirela", ""], ["Capecci", "Daniel", ""], ["Giovanini", "Luiz", ""], ["Czech", "Lauren", ""], ["Fernandes", "Juliana", ""], ["Oliveira", "Daniela", ""]]}, {"id": "2107.10658", "submitter": "Joanna Rownicka", "authors": "Joanna Rownicka, Kilian Sprenkamp, Antonio Tripiana, Volodymyr\n  Gromoglasov, Timo P Kunz", "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI", "comments": "accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for a Digital\nEinstein character, to enable human-computer interaction within the digital\nconversation experience. To create the voice which fits the context well, we\nfirst design a voice character and we produce the recordings which correspond\nto the desired speech attributes. We then model the voice. Our solution\nutilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes\nand Parallel WaveGAN to generate the waveforms. The system supports a character\ninput and gives a speech waveform at the output. We use a custom dictionary for\nselected words to ensure their proper pronunciation. Our proposed cloud\narchitecture enables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:03:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Rownicka", "Joanna", ""], ["Sprenkamp", "Kilian", ""], ["Tripiana", "Antonio", ""], ["Gromoglasov", "Volodymyr", ""], ["Kunz", "Timo P", ""]]}, {"id": "2107.10821", "submitter": "Tom Kocmi", "authors": "Tom Kocmi and Christian Federmann and Roman Grundkiewicz and Marcin\n  Junczys-Dowmunt and Hitokazu Matsushita and Arul Menezes", "title": "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic metrics are commonly used as the exclusive tool for declaring the\nsuperiority of one machine translation system's quality over another. The\ncommunity choice of automatic metric guides research directions and industrial\ndevelopments by deciding which models are deemed better. Evaluating metrics\ncorrelations has been limited to a small collection of human judgements. In\nthis paper, we corroborate how reliable metrics are in contrast to human\njudgements on - to the best of our knowledge - the largest collection of human\njudgements. We investigate which metrics have the highest accuracy to make\nsystem-level quality rankings for pairs of systems, taking human judgement as a\ngold standard, which is the closest scenario to the real metric usage.\nFurthermore, we evaluate the performance of various metrics across different\nlanguage pairs and domains. Lastly, we show that the sole use of BLEU\nnegatively affected the past development of improved models. We release the\ncollection of human judgements of 4380 systems, and 2.3 M annotated sentences\nfor further analysis and replication of our work.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:22:22 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kocmi", "Tom", ""], ["Federmann", "Christian", ""], ["Grundkiewicz", "Roman", ""], ["Junczys-Dowmunt", "Marcin", ""], ["Matsushita", "Hitokazu", ""], ["Menezes", "Arul", ""]]}, {"id": "2107.10922", "submitter": "Giulia Rambelli", "authors": "Paolo Pedinotti, Giulia Rambelli, Emmanuele Chersoni, Enrico Santus,\n  Alessandro Lenci, Philippe Blache", "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized\n  Event Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prior research has explored the ability of computational models to predict a\nword semantic fit with a given predicate. While much work has been devoted to\nmodeling the typicality relation between verbs and arguments in isolation, in\nthis paper we take a broader perspective by assessing whether and to what\nextent computational approaches have access to the information about the\ntypicality of entire events and situations described in language (Generalized\nEvent Knowledge). Given the recent success of Transformers Language Models\n(TLMs), we decided to test them on a benchmark for the \\textit{dynamic\nestimation of thematic fit}. The evaluation of these models was performed in\ncomparison with SDM, a framework specifically designed to integrate events in\nsentence meaning representations, and we conducted a detailed error analysis to\ninvestigate which factors affect their behavior. Our results show that TLMs can\nreach performances that are comparable to those achieved by SDM. However,\nadditional analysis consistently suggests that TLMs do not capture important\naspects of event knowledge, and their predictions often depend on surface\nlinguistic features, such as frequent words, collocations and syntactic\npatterns, thereby showing sub-optimal generalization abilities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:52:26 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Pedinotti", "Paolo", ""], ["Rambelli", "Giulia", ""], ["Chersoni", "Emmanuele", ""], ["Santus", "Enrico", ""], ["Lenci", "Alessandro", ""], ["Blache", "Philippe", ""]]}, {"id": "2107.10932", "submitter": "Mohammad Ramezanali", "authors": "Tim Lou, Michael Park, Mohammad Ramezanali, Vincent Tang", "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms", "comments": "final experimental results forthcoming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 21:24:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lou", "Tim", ""], ["Park", "Michael", ""], ["Ramezanali", "Mohammad", ""], ["Tang", "Vincent", ""]]}, {"id": "2107.10941", "submitter": "Qinkai Chen", "authors": "Qinkai Chen and Christian-Yann Robert", "title": "Graph-Based Learning for Stock Movement Prediction with Textual and\n  Relational Data", "comments": "10 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting stock prices from textual information is a challenging task due to\nthe uncertainty of the market and the difficulty understanding the natural\nlanguage from a machine's perspective. Previous researches focus mostly on\nsentiment extraction based on single news. However, the stocks on the financial\nmarket can be highly correlated, one news regarding one stock can quickly\nimpact the prices of other stocks. To take this effect into account, we propose\na new stock movement prediction framework: Multi-Graph Recurrent Network for\nStock Forecasting (MGRN). This architecture allows to combine the textual\nsentiment from financial news and multiple relational information extracted\nfrom other financial data. Through an accuracy test and a trading simulation on\nthe stocks in the STOXX Europe 600 index, we demonstrate a better performance\nfrom our model than other benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 21:57:18 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Chen", "Qinkai", ""], ["Robert", "Christian-Yann", ""]]}, {"id": "2107.11020", "submitter": "Junyi Jessy Li", "authors": "Alexander Tekle, Chau Pham, Cornelia Caragea, Junyi Jessy Li", "title": "When a crisis strikes: Emotion analysis and detection during COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~1K tweets labeled with\nemotions. We examine how well large pre-trained language models generalize\nacross domains and crises in the task of perceived emotion prediction in the\ncontext of COVID-19. Our results show that existing models do not directly\ntransfer from one disaster type to another but using labeled emotional corpora\nfor domain adaptation is beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 04:07:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tekle", "Alexander", ""], ["Pham", "Chau", ""], ["Caragea", "Cornelia", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2107.11094", "submitter": "Vivek Madan", "authors": "Fred Qin, Vivek Madan, Ujjwal Ratan, Zohar Karnin, Vishaal Kapoor,\n  Parminder Bhatia, and Taha Kass-Hout", "title": "Improving Early Sepsis Prediction with Multi Modal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sepsis is a life-threatening disease with high morbidity, mortality and\nhealthcare costs. The early prediction and administration of antibiotics and\nintravenous fluids is considered crucial for the treatment of sepsis and can\nsave potentially millions of lives and billions in health care costs.\nProfessional clinical care practitioners have proposed clinical criterion which\naid in early detection of sepsis; however, performance of these criterion is\noften limited. Clinical text provides essential information to estimate the\nseverity of the sepsis in addition to structured clinical data. In this study,\nwe explore how clinical text can complement structured data towards early\nsepsis prediction task. In this paper, we propose multi modal model which\nincorporates both structured data in the form of patient measurements as well\nas textual notes on the patient. We employ state-of-the-art NLP models such as\nBERT and a highly specialized NLP model in Amazon Comprehend Medical to\nrepresent the text. On the MIMIC-III dataset containing records of ICU\nadmissions, we show that by using these notes, one achieves an improvement of\n6.07 points in a standard utility score for Sepsis prediction and 2.89% in\nAUROC score. Our methods significantly outperforms a clinical criteria\nsuggested by experts, qSOFA, as well as the winning model of the PhysioNet\nComputing in Cardiology Challenge for predicting Sepsis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 09:25:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Qin", "Fred", ""], ["Madan", "Vivek", ""], ["Ratan", "Ujjwal", ""], ["Karnin", "Zohar", ""], ["Kapoor", "Vishaal", ""], ["Bhatia", "Parminder", ""], ["Kass-Hout", "Taha", ""]]}, {"id": "2107.11113", "submitter": "Wenxuan Hu", "authors": "Binling Wang, Wenxuan Hu, Jing Li, Yiming Zhi, Zheng Li, Qingyang\n  Hong, Lin Li, Dong Wang, Liming Song and Cheng Yang", "title": "OLR 2021 Challenge: Datasets, Rules and Baselines", "comments": "arXiv admin note: text overlap with arXiv:2006.03473,\n  arXiv:1907.07626, arXiv:1806.00616, arXiv:1706.09742", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the sixth Oriental Language Recognition (OLR) 2021\nChallenge, which intends to improve the performance of language recognition\nsystems and speech recognition systems within multilingual scenarios. The data\nprofile, four tasks, two baselines, and the evaluation principles are\nintroduced in this paper. In addition to the Language Identification (LID)\ntasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to\nOLR 2021 Challenge for the first time. The challenge this year focuses on more\npractical and challenging problems, with four tasks: (1) constrained LID, (2)\nunconstrained LID, (3) constrained multilingual ASR, (4) unconstrained\nmultilingual ASR. Baselines for LID tasks and multilingual ASR tasks are\nprovided, respectively. The LID baseline system is an extended TDNN x-vector\nmodel constructed with Pytorch. A transformer-based end-to-end model is\nprovided as the multilingual ASR baseline system. These recipes will be online\npublished, and available for participants to construct their own LID or ASR\nsystems. The baseline results demonstrate that those tasks are rather\nchallenging and deserve more effort to achieve better performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 09:57:29 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wang", "Binling", ""], ["Hu", "Wenxuan", ""], ["Li", "Jing", ""], ["Zhi", "Yiming", ""], ["Li", "Zheng", ""], ["Hong", "Qingyang", ""], ["Li", "Lin", ""], ["Wang", "Dong", ""], ["Song", "Liming", ""], ["Yang", "Cheng", ""]]}, {"id": "2107.11164", "submitter": "Yunlong Liang", "authors": "Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu and Jie Zhou", "title": "Modeling Bilingual Conversational Characteristics for Neural Chat\n  Translation", "comments": "Accepted as a long paper at ACL 2021. Code and data are available at\n  https://github.com/XL2248/CPCC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural chat translation aims to translate bilingual conversational text,\nwhich has a broad application in international exchanges and cooperation.\nDespite the impressive performance of sentence-level and context-aware Neural\nMachine Translation (NMT), there still remain challenges to translate bilingual\nconversational text due to its inherent characteristics such as role\npreference, dialogue coherence, and translation consistency. In this paper, we\naim to promote the translation quality of conversational text by modeling the\nabove properties. Specifically, we design three latent variational modules to\nlearn the distributions of bilingual conversational characteristics. Through\nsampling from these learned distributions, the latent variables, tailored for\nrole preference, dialogue coherence, and translation consistency, are\nincorporated into the NMT model for better translation. We evaluate our\napproach on the benchmark dataset BConTrasT (English-German) and a\nself-collected bilingual dialogue corpus, named BMELD (English-Chinese).\nExtensive experiments show that our approach notably boosts the performance\nover strong baselines by a large margin and significantly surpasses some\nstate-of-the-art context-aware NMT models in terms of BLEU and TER.\nAdditionally, we make the BMELD dataset publicly available for the research\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:23:34 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Liang", "Yunlong", ""], ["Meng", "Fandong", ""], ["Chen", "Yufeng", ""], ["Xu", "Jinan", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.11252", "submitter": "Bingqian Lin", "authors": "Bingqian Lin, Yi Zhu, Yanxin Long, Xiaodan Liang, Qixiang Ye, Liang\n  Lin", "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language\n  Navigation", "comments": "Accepted by TPAMI 2021", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3097435", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:11:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lin", "Bingqian", ""], ["Zhu", "Yi", ""], ["Long", "Yanxin", ""], ["Liang", "Xiaodan", ""], ["Ye", "Qixiang", ""], ["Lin", "Liang", ""]]}, {"id": "2107.11275", "submitter": "Ekaterina Artemova", "authors": "Ivan Fursov, Alexey Zaytsev, Pavel Burnyshev, Ekaterina Dmitrieva,\n  Nikita Klyuchnikov, Andrey Kravchenko, Ekaterina Artemova, Evgeny Burnaev", "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.11078", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robustness of huge Transformer-based models for natural language processing\nis an important issue due to their capabilities and wide adoption. One way to\nunderstand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input can fool\na model.\n  Due to the discrete nature of textual data, gradient-based adversarial\nmethods, widely used in computer vision, are not applicable per~se. The\nstandard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account.\n  In this paper, we propose a new black-box sentence-level attack. Our method\nfine-tunes a pre-trained language model to generate adversarial examples. A\nproposed differentiable loss function depends on a substitute classifier score\nand an approximate edit distance computed via a deep learning model.\n  We show that the proposed attack outperforms competitors on a diverse set of\nNLP problems for both computed metrics and human evaluation. Moreover, due to\nthe usage of the fine-tuned language model, the generated adversarial examples\nare hard to detect, thus current models are not robust. Hence, it is difficult\nto defend from the proposed attack, which is not the case for other attacks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:43:13 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Fursov", "Ivan", ""], ["Zaytsev", "Alexey", ""], ["Burnyshev", "Pavel", ""], ["Dmitrieva", "Ekaterina", ""], ["Klyuchnikov", "Nikita", ""], ["Kravchenko", "Andrey", ""], ["Artemova", "Ekaterina", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2107.11351", "submitter": "Nikita Bhutani", "authors": "Kameron B. Rodrigues, Shweta Khushu, Mukut Mukherjee, Andrew Banister,\n  Anthony Hevia, Sampath Duddu, Nikita Bhutani", "title": "Powering Effective Climate Communication with a Climate Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many accept climate change and its growing impacts, few converse about\nit well, limiting the adoption speed of societal changes necessary to address\nit. In order to make effective climate communication easier, we aim to build a\nsystem that presents to any individual the climate information predicted to\nbest motivate and inspire them to take action given their unique set of\npersonal values. To alleviate the cold-start problem, the system relies on a\nknowledge base (ClimateKB) of causes and effects of climate change, and their\nassociations to personal values. Since no such comprehensive ClimateKB exists,\nwe revisit knowledge base construction techniques and build a ClimateKB from\nfree text. We plan to open source the ClimateKB and associated code to\nencourage future research and applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 17:02:06 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Rodrigues", "Kameron B.", ""], ["Khushu", "Shweta", ""], ["Mukherjee", "Mukut", ""], ["Banister", "Andrew", ""], ["Hevia", "Anthony", ""], ["Duddu", "Sampath", ""], ["Bhutani", "Nikita", ""]]}, {"id": "2107.11353", "submitter": "Edoardo Maria Ponti", "authors": "Edoardo Maria Ponti, Julia Kreutzer, Ivan Vuli\\'c, and Siva Reddy", "title": "Modelling Latent Translations for Cross-Lingual Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While achieving state-of-the-art results in multiple tasks and languages,\ntranslation-based cross-lingual transfer is often overlooked in favour of\nmassively multilingual pre-trained encoders. Arguably, this is due to its main\nlimitations: 1) translation errors percolating to the classification phase and\n2) the insufficient expressiveness of the maximum-likelihood translation. To\nremedy this, we propose a new technique that integrates both steps of the\ntraditional pipeline (translation and classification) into a single model, by\ntreating the intermediate translations as a latent random variable. As a\nresult, 1) the neural machine translation system can be fine-tuned with a\nvariant of Minimum Risk Training where the reward is the accuracy of the\ndownstream task classifier. Moreover, 2) multiple samples can be drawn to\napproximate the expected loss across all possible translations during\ninference. We evaluate our novel latent translation-based model on a series of\nmultilingual NLU tasks, including commonsense reasoning, paraphrase\nidentification, and natural language inference. We report gains for both\nzero-shot and few-shot learning setups, up to 2.7 accuracy points on average,\nwhich are even more prominent for low-resource languages (e.g., Haitian\nCreole). Finally, we carry out in-depth analyses comparing different underlying\nNMT models and assessing the impact of alternative translations on the\ndownstream performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 17:11:27 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ponti", "Edoardo Maria", ""], ["Kreutzer", "Julia", ""], ["Vuli\u0107", "Ivan", ""], ["Reddy", "Siva", ""]]}, {"id": "2107.11414", "submitter": "Lucas Gris", "authors": "Lucas Rafael Stefanel Gris, Edresson Casanova, Frederico Santos de\n  Oliveira, Anderson da Silva Soares, Arnaldo Candido Junior", "title": "Brazilian Portuguese Speech Recognition Using Wav2vec 2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been shown to be efficient in various tasks,\nespecially in the development of speech recognition systems, that is, systems\nthat aim to transcribe a sentence in audio in a sequence of words. Despite the\nprogress in the area, speech recognition can still be considered difficult,\nespecially for languages lacking available data, as Brazilian Portuguese. In\nthis sense, this work presents the development of an public Automatic Speech\nRecognition system using only open available audio data, from the fine-tuning\nof the Wav2vec 2.0 XLSR-53 model pre-trained in many languages over Brazilian\nPortuguese data. The final model presents a Word Error Rate of 11.95% (Common\nVoice Dataset). This corresponds to 13% less than the best open Automatic\nSpeech Recognition model for Brazilian Portuguese available according to our\nbest knowledge, which is a promising result for the language. In general, this\nwork validates the use of self-supervising learning techniques, in special, the\nuse of the Wav2vec 2.0 architecture in the development of robust systems, even\nfor languages having few available data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:54:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gris", "Lucas Rafael Stefanel", ""], ["Casanova", "Edresson", ""], ["de Oliveira", "Frederico Santos", ""], ["Soares", "Anderson da Silva", ""], ["Junior", "Arnaldo Candido", ""]]}, {"id": "2107.11481", "submitter": "Sougata Saha", "authors": "Sougata Saha, Souvik Das, Rohini Srihari", "title": "Similarity Based Label Smoothing For Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative neural conversational systems are generally trained with the\nobjective of minimizing the entropy loss between the training \"hard\" targets\nand the predicted logits. Often, performance gains and improved generalization\ncan be achieved by using regularization techniques like label smoothing, which\nconverts the training \"hard\" targets to \"soft\" targets. However, label\nsmoothing enforces a data independent uniform distribution on the incorrect\ntraining targets, which leads to an incorrect assumption of equi-probable\nincorrect targets for each correct target. In this paper we propose and\nexperiment with incorporating data dependent word similarity based weighing\nmethods to transforms the uniform distribution of the incorrect target\nprobabilities in label smoothing, to a more natural distribution based on\nsemantics. We introduce hyperparameters to control the incorrect target\ndistribution, and report significant performance gains over networks trained\nusing standard label smoothing based loss, on two standard open domain dialogue\ncorpora.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 23:25:19 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Saha", "Sougata", ""], ["Das", "Souvik", ""], ["Srihari", "Rohini", ""]]}, {"id": "2107.11534", "submitter": "Vivek Srivastava", "authors": "Ayush Garg, Sammed S Kagi, Vivek Srivastava, Mayank Singh", "title": "MIPE: A Metric Independent Pipeline for Effective Code-Mixed NLG\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-mixing is a phenomenon of mixing words and phrases from two or more\nlanguages in a single utterance of speech and text. Due to the high linguistic\ndiversity, code-mixing presents several challenges in evaluating standard\nnatural language generation (NLG) tasks. Various widely popular metrics perform\npoorly with the code-mixed NLG tasks. To address this challenge, we present a\nmetric independent evaluation pipeline MIPE that significantly improves the\ncorrelation between evaluation metrics and human judgments on the generated\ncode-mixed text. As a use case, we demonstrate the performance of MIPE on the\nmachine-generated Hinglish (code-mixing of Hindi and English languages)\nsentences from the HinGE corpus. We can extend the proposed evaluation strategy\nto other code-mixed language pairs, NLG tasks, and evaluation metrics with\nminimal to no effort.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 05:24:26 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Garg", "Ayush", ""], ["Kagi", "Sammed S", ""], ["Srivastava", "Vivek", ""], ["Singh", "Mayank", ""]]}, {"id": "2107.11572", "submitter": "Liang Ding", "authors": "Liang Ding, Di Wu, Dacheng Tao", "title": "The USYD-JD Speech Translation System for IWSLT 2021", "comments": "IWSLT 2021 winning system of the low-resource speech translation\n  track", "journal-ref": null, "doi": null, "report-no": "usyd-jd-01", "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper describes the University of Sydney& JD's joint submission of the\nIWSLT 2021 low resource speech translation task. We participated in the\nSwahili-English direction and got the best scareBLEU (25.3) score among all the\nparticipants. Our constrained system is based on a pipeline framework, i.e. ASR\nand NMT. We trained our models with the officially provided ASR and MT\ndatasets. The ASR system is based on the open-sourced tool Kaldi and this work\nmainly explores how to make the most of the NMT models. To reduce the\npunctuation errors generated by the ASR model, we employ our previous work\nSlotRefine to train a punctuation correction model. To achieve better\ntranslation performance, we explored the most recent effective strategies,\nincluding back translation, knowledge distillation, multi-feature reranking and\ntransductive finetuning. For model structure, we tried auto-regressive and\nnon-autoregressive models, respectively. In addition, we proposed two novel\npre-train approaches, i.e. \\textit{de-noising training} and\n\\textit{bidirectional training} to fully exploit the data. Extensive\nexperiments show that adding the above techniques consistently improves the\nBLEU scores, and the final submission system outperforms the baseline\n(Transformer ensemble model trained with the original parallel data) by\napproximately 10.8 BLEU score, achieving the SOTA performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 09:53:34 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ding", "Liang", ""], ["Wu", "Di", ""], ["Tao", "Dacheng", ""]]}, {"id": "2107.11584", "submitter": "Jonas-Dario Troles", "authors": "Jonas-Dario Troles, Ute Schmid", "title": "Extending Challenge Sets to Uncover Gender Bias in Machine Translation:\n  Impact of Stereotypical Verbs and Adjectives", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human gender bias is reflected in language and text production. Because\nstate-of-the-art machine translation (MT) systems are trained on large corpora\nof text, mostly generated by humans, gender bias can also be found in MT. For\ninstance when occupations are translated from a language like English, which\nmostly uses gender neutral words, to a language like German, which mostly uses\na feminine and a masculine version for an occupation, a decision must be made\nby the MT System. Recent research showed that MT systems are biased towards\nstereotypical translation of occupations. In 2019 the first, and so far only,\nchallenge set, explicitly designed to measure the extent of gender bias in MT\nsystems has been published. In this set measurement of gender bias is solely\nbased on the translation of occupations. In this paper we present an extension\nof this challenge set, called WiBeMT, with gender-biased adjectives and adds\nsentences with gender-biased verbs. The resulting challenge set consists of\nover 70, 000 sentences and has been translated with three commercial MT\nsystems: DeepL Translator, Microsoft Translator, and Google Translate. Results\nshow a gender bias for all three MT systems. This gender bias is to a great\nextent significantly influenced by adjectives and to a lesser extent by verbs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 11:22:10 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Troles", "Jonas-Dario", ""], ["Schmid", "Ute", ""]]}, {"id": "2107.11597", "submitter": "Omar Al-Harbi Mohammad", "authors": "Omar Al-Harbi", "title": "Negation Handling in Machine Learning-Based Sentiment Classification for\n  Colloquial Arabic", "comments": "13 pages", "journal-ref": "International Journal of Operations Research and Information\n  Systems (2020) 33-45", "doi": "10.4018/IJORIS.2020100102", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One crucial aspect of sentiment analysis is negation handling, where the\noccurrence of negation can flip the sentiment of a sentence and negatively\naffects the machine learning-based sentiment classification. The role of\nnegation in Arabic sentiment analysis has been explored only to a limited\nextent, especially for colloquial Arabic. In this paper, the author addresses\nthe negation problem of machine learning-based sentiment classification for a\ncolloquial Arabic language. To this end, we propose a simple rule-based\nalgorithm for handling the problem; the rules were crafted based on observing\nmany cases of negation. Additionally, simple linguistic knowledge and sentiment\nlexicon are used for this purpose. The author also examines the impact of the\nproposed algorithm on the performance of different machine learning algorithms.\nThe results given by the proposed algorithm are compared with three baseline\nmodels. The experimental results show that there is a positive impact on the\nclassifiers accuracy, precision and recall when the proposed algorithm is used\ncompared to the baselines.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 13:12:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Al-Harbi", "Omar", ""]]}, {"id": "2107.11610", "submitter": "Abbas Ghaddar", "authors": "Abbas Ghaddar, Philippe Langlais, Ahmad Rashid, Mehdi Rezagholizadeh", "title": "Context-aware Adversarial Training for Name Regularity Bias in Named\n  Entity Recognition", "comments": "MIT Press\\TACL 2021\\Presented at ACL 2021 This is the exact same\n  content of the TACL version, except the figures and tables are better aligned", "journal-ref": "journal={Transactions of the Association for Computational\n  Linguistics}, volume={9}, pages={586--604}, year={2021},", "doi": "10.1162/tacl_a_00386", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we examine the ability of NER models to use contextual\ninformation when predicting the type of an ambiguous entity. We introduce NRB,\na new testbed carefully designed to diagnose Name Regularity Bias of NER\nmodels. Our results indicate that all state-of-the-art models we tested show\nsuch a bias; BERT fine-tuned models significantly outperforming feature-based\n(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance\non standard benchmarks.\n  To mitigate this bias, we propose a novel model-agnostic training method that\nadds learnable adversarial noise to some entity mentions, thus enforcing models\nto focus more strongly on the contextual signal, leading to significant gains\non NRB. Combining it with two other training strategies, data augmentation and\nparameter freezing, leads to further gains.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 13:55:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ghaddar", "Abbas", ""], ["Langlais", "Philippe", ""], ["Rashid", "Ahmad", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "2107.11628", "submitter": "Brian Yan", "authors": "Brian Yan, Siddharth Dalmia, David R. Mortensen, Florian Metze, Shinji\n  Watanabe", "title": "Differentiable Allophone Graphs for Language-Universal Speech\n  Recognition", "comments": "INTERSPEECH 2021. Contains additional studies on phone recognition\n  for unseen languages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building language-universal speech recognition systems entails producing\nphonological units of spoken sound that can be shared across languages. While\nspeech annotations at the language-specific phoneme or surface levels are\nreadily available, annotations at a universal phone level are relatively rare\nand difficult to produce. In this work, we present a general framework to\nderive phone-level supervision from only phonemic transcriptions and\nphone-to-phoneme mappings with learnable weights represented using weighted\nfinite-state transducers, which we call differentiable allophone graphs. By\ntraining multilingually, we build a universal phone-based speech recognition\nmodel with interpretable probabilistic phone-to-phoneme mappings for each\nlanguage. These phone-based systems with learned allophone graphs can be used\nby linguists to document new languages, build phone-based lexicons that capture\nrich pronunciation variations, and re-evaluate the allophone mappings of seen\nlanguage. We demonstrate the aforementioned benefits of our proposed framework\nwith a system trained on 7 diverse languages.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:09:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yan", "Brian", ""], ["Dalmia", "Siddharth", ""], ["Mortensen", "David R.", ""], ["Metze", "Florian", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2107.11652", "submitter": "Vladimir Araujo", "authors": "Vladimir Araujo, Andr\\'es Carvallo, Carlos Aspillaga, Camilo Thorne,\n  Denis Parra", "title": "Stress Test Evaluation of Biomedical Word Embeddings", "comments": "Accepted paper BioNLP2021", "journal-ref": null, "doi": "10.18653/v1/2021.bionlp-1.13", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of pretrained word embeddings has motivated their use in the\nbiomedical domain, with contextualized embeddings yielding remarkable results\nin several biomedical NLP tasks. However, there is a lack of research on\nquantifying their behavior under severe \"stress\" scenarios. In this work, we\nsystematically evaluate three language models with adversarial examples --\nautomatically constructed tests that allow us to examine how robust the models\nare. We propose two types of stress scenarios focused on the biomedical named\nentity recognition (NER) task, one inspired by spelling errors and another\nbased on the use of synonyms for medical terms. Our experiments with three\nbenchmarks show that the performance of the original models decreases\nconsiderably, in addition to revealing their weaknesses and strengths. Finally,\nwe show that adversarial training causes the models to improve their robustness\nand even to exceed the original performance in some cases.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 16:45:03 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Araujo", "Vladimir", ""], ["Carvallo", "Andr\u00e9s", ""], ["Aspillaga", "Carlos", ""], ["Thorne", "Camilo", ""], ["Parra", "Denis", ""]]}, {"id": "2107.11665", "submitter": "Jingqing Zhang", "authors": "Jingqing Zhang, Luis Bolanos, Ashwani Tanwar, Albert Sokol, Julia Ive,\n  Vibhor Gupta, Yike Guo", "title": "Clinical Utility of the Automatic Phenotype Annotation in Unstructured\n  Clinical Notes: ICU Use Cases", "comments": "Manuscript under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical notes contain information not present elsewhere, including drug\nresponse and symptoms, all of which are highly important when predicting key\noutcomes in acute care patients. We propose the automatic annotation of\nphenotypes from clinical notes as a method to capture essential information to\npredict outcomes in the Intensive Care Unit (ICU). This information is\ncomplementary to typically used vital signs and laboratory test results. We\ndemonstrate and validate our approach conducting experiments on the prediction\nof in-hospital mortality, physiological decompensation and length of stay in\nthe ICU setting for over 24,000 patients. The prediction models incorporating\nphenotypic information consistently outperform the baseline models leveraging\nonly vital signs and laboratory test results. Moreover, we conduct a thorough\ninterpretability study, showing that phenotypes provide valuable insights at\nthe patient and cohort levels. Our approach illustrates the viability of using\nphenotypes to determine outcomes in the ICU.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 17:55:55 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Jingqing", ""], ["Bolanos", "Luis", ""], ["Tanwar", "Ashwani", ""], ["Sokol", "Albert", ""], ["Ive", "Julia", ""], ["Gupta", "Vibhor", ""], ["Guo", "Yike", ""]]}, {"id": "2107.11666", "submitter": "Piotr Koniusz", "authors": "Hao Zhu, Piotr Koniusz", "title": "Graph Convolutional Network with Generalized Factorized Bilinear\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Graph Convolutional Networks (GCNs) have demonstrated their power in\nvarious applications, the graph convolutional layers, as the most important\ncomponent of GCN, are still using linear transformations and a simple pooling\nstep. In this paper, we propose a novel generalization of Factorized Bilinear\n(FB) layer to model the feature interactions in GCNs. FB performs two\nmatrix-vector multiplications, that is, the weight matrix is multiplied with\nthe outer product of the vector of hidden features from both sides. However,\nthe FB layer suffers from the quadratic number of coefficients, overfitting and\nthe spurious correlations due to correlations between channels of hidden\nrepresentations that violate the i.i.d. assumption. Thus, we propose a compact\nFB layer by defining a family of summarizing operators applied over the\nquadratic term. We analyze proposed pooling operators and motivate their use.\nOur experimental results on multiple datasets demonstrate that the GFB-GCN is\ncompetitive with other methods for text classification.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 17:57:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhu", "Hao", ""], ["Koniusz", "Piotr", ""]]}, {"id": "2107.11757", "submitter": "Lukas Stappen", "authors": "Lukas Stappen and Lea Schumann and Benjamin Sertolli and Alice Baird\n  and Benjamin Weigel and Erik Cambria and Bj\\\"orn W. Schuller", "title": "MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation\n  Fusion and Discrete Class Transformation Toolbox", "comments": "(1) https://github.com/lstappen/MuSe-Toolbox (2) docker pull\n  musetoolbox/musetoolbox", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the MuSe-Toolbox - a Python-based open-source toolkit for\ncreating a variety of continuous and discrete emotion gold standards. In a\nsingle framework, we unify a wide range of fusion methods and propose the novel\nRater Aligned Annotation Weighting (RAAW), which aligns the annotations in a\ntranslation-invariant way before weighting and fusing them based on the\ninter-rater agreements between the annotations. Furthermore, discrete\ncategories tend to be easier for humans to interpret than continuous signals.\nWith this in mind, the MuSe-Toolbox provides the functionality to run\nexhaustive searches for meaningful class clusters in the continuous gold\nstandards. To our knowledge, this is the first toolkit that provides a wide\nselection of state-of-the-art emotional gold standard methods and their\ntransformation to discrete classes. Experimental results indicate that\nMuSe-Toolbox can provide promising and novel class formations which can be\nbetter predicted than hard-coded classes boundaries with minimal human\nintervention. The implementation (1) is out-of-the-box available with all\ndependencies using a Docker container (2).\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 08:46:18 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Stappen", "Lukas", ""], ["Schumann", "Lea", ""], ["Sertolli", "Benjamin", ""], ["Baird", "Alice", ""], ["Weigel", "Benjamin", ""], ["Cambria", "Erik", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2107.11768", "submitter": "Linhao Zhang", "authors": "Linhao Zhang, Yu Shi, Linjun Shou, Ming Gong, Houfeng Wang, Michael\n  Zeng", "title": "A Joint and Domain-Adaptive Approach to Spoken Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spoken Language Understanding (SLU) is composed of two subtasks: intent\ndetection (ID) and slot filling (SF). There are two lines of research on SLU.\nOne jointly tackles these two subtasks to improve their prediction accuracy,\nand the other focuses on the domain-adaptation ability of one of the subtasks.\nIn this paper, we attempt to bridge these two lines of research and propose a\njoint and domain adaptive approach to SLU. We formulate SLU as a constrained\ngeneration task and utilize a dynamic vocabulary based on domain-specific\nontology. We conduct experiments on the ASMixed and MTOD datasets and achieve\ncompetitive performance with previous state-of-the-art joint models. Besides,\nresults show that our joint model can be effectively adapted to a new domain.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 09:38:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Linhao", ""], ["Shi", "Yu", ""], ["Shou", "Linjun", ""], ["Gong", "Ming", ""], ["Wang", "Houfeng", ""], ["Zeng", "Michael", ""]]}, {"id": "2107.11778", "submitter": "Linhao Zhang", "authors": "Linhao Zhang, Houfeng Wang", "title": "Learn to Focus: Hierarchical Dynamic Copy Network for Dialogue State\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, researchers have explored using the encoder-decoder framework to\ntackle dialogue state tracking (DST), which is a key component of task-oriented\ndialogue systems. However, they regard a multi-turn dialogue as a flat\nsequence, failing to focus on useful information when the sequence is long. In\nthis paper, we propose a Hierarchical Dynamic Copy Network (HDCN) to facilitate\nfocusing on the most informative turn, making it easier to extract slot values\nfrom the dialogue context. Based on the encoder-decoder framework, we adopt a\nhierarchical copy approach that calculates two levels of attention at the word-\nand turn-level, which are then renormalized to obtain the final copy\ndistribution. A focus loss term is employed to encourage the model to assign\nthe highest turn-level attention weight to the most informative turn.\nExperimental results show that our model achieves 46.76% joint accuracy on the\nMultiWOZ 2.1 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 10:43:28 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Linhao", ""], ["Wang", "Houfeng", ""]]}, {"id": "2107.11781", "submitter": "Linhao Zhang", "authors": "Linhao Zhang, Houfeng Wang", "title": "Towards Controlled and Diverse Generation of Article Comments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much research in recent years has focused on automatic article commenting.\nHowever, few of previous studies focus on the controllable generation of\ncomments. Besides, they tend to generate dull and commonplace comments, which\nfurther limits their practical application. In this paper, we make the first\nstep towards controllable generation of comments, by building a system that can\nexplicitly control the emotion of the generated comments. To achieve this, we\nassociate each kind of emotion category with an embedding and adopt a dynamic\nfusion mechanism to fuse this embedding into the decoder. A sentence-level\nemotion classifier is further employed to better guide the model to generate\ncomments expressing the desired emotion. To increase the diversity of the\ngenerated comments, we propose a hierarchical copy mechanism that allows our\nmodel to directly copy words from the input articles. We also propose a\nrestricted beam search (RBS) algorithm to increase intra-sentence diversity.\nExperimental results show that our model can generate informative and diverse\ncomments that express the desired emotions with high accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 11:06:22 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Linhao", ""], ["Wang", "Houfeng", ""]]}, {"id": "2107.11823", "submitter": "Bohong Wu", "authors": "Bohong Wu, Zhuosheng Zhang, Hai Zhao", "title": "Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop reading comprehension (MHRC) requires not only to predict the\ncorrect answer span in the given passage, but also to provide a chain of\nsupporting evidences for reasoning interpretability. It is natural to model\nsuch a process into graph structure by understanding multi-hop reasoning as\njumping over entity nodes, which has made graph modelling dominant on this\ntask. Recently, there have been dissenting voices about whether graph modelling\nis indispensable due to the inconvenience of the graph building, however\nexisting state-of-the-art graph-free attempts suffer from huge performance gap\ncompared to graph-based ones. This work presents a novel graph-free alternative\nwhich firstly outperform all graph models on MHRC. In detail, we exploit a\nselect-to-guide (S2G) strategy to accurately retrieve evidence paragraphs in a\ncoarse-to-fine manner, incorporated with two novel attention mechanisms, which\nsurprisingly shows conforming to the nature of multi-hop reasoning. Our\ngraph-free model achieves significant and consistent performance gain over\nstrong baselines and the current new state-of-the-art on the MHRC benchmark,\nHotpotQA, among all the published works.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 15:07:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Bohong", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2107.11879", "submitter": "Marco Valentino", "authors": "Marco Valentino, Mokanarangan Thayaparan, Deborah Ferreira, Andr\\'e\n  Freitas", "title": "Hybrid Autoregressive Solver for Scalable Abductive Natural Language\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regenerating natural language explanations for science questions is a\nchallenging task for evaluating complex multi-hop and abductive inference\ncapabilities. In this setting, Transformers trained on human-annotated\nexplanations achieve state-of-the-art performance when adopted as cross-encoder\narchitectures. However, while much attention has been devoted to the quality of\nthe constructed explanations, the problem of performing abductive inference at\nscale is still under-studied. As intrinsically not scalable, the cross-encoder\narchitectural paradigm is not suitable for efficient multi-hop inference on\nmassive facts banks. To maximise both accuracy and inference time, we propose a\nhybrid abductive solver that autoregressively combines a dense bi-encoder with\na sparse model of explanatory power, computed leveraging explicit patterns in\nthe explanations. Our experiments demonstrate that the proposed framework can\nachieve performance comparable with the state-of-the-art cross-encoder while\nbeing $\\approx 50$ times faster and scalable to corpora of millions of facts.\nMoreover, we study the impact of the hybridisation on semantic drift and\nscience question answering without additional training, showing that it boosts\nthe quality of the explanations and contributes to improved downstream\ninference performance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 19:29:53 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Valentino", "Marco", ""], ["Thayaparan", "Mokanarangan", ""], ["Ferreira", "Deborah", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2107.11904", "submitter": "Bo-Hsiang (Andy) Tseng", "authors": "Bo-Hsiang Tseng, Yinpei Dai, Florian Kreyssig, Bill Byrne", "title": "Transferable Dialogue Systems and User Simulators", "comments": "Accepted by ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the difficulties in training dialogue systems is the lack of training\ndata. We explore the possibility of creating dialogue data through the\ninteraction between a dialogue system and a user simulator. Our goal is to\ndevelop a modelling framework that can incorporate new dialogue scenarios\nthrough self-play between the two agents. In this framework, we first pre-train\nthe two agents on a collection of source domain dialogues, which equips the\nagents to converse with each other via natural language. With further\nfine-tuning on a small amount of target domain data, the agents continue to\ninteract with the aim of improving their behaviors using reinforcement learning\nwith structured reward functions. In experiments on the MultiWOZ dataset, two\npractical transfer learning problems are investigated: 1) domain adaptation and\n2) single-to-multiple domain transfer. We demonstrate that the proposed\nframework is highly effective in bootstrapping the performance of the two\nagents in transfer learning. We also show that our method leads to improvements\nin dialogue system performance on complete datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 22:59:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tseng", "Bo-Hsiang", ""], ["Dai", "Yinpei", ""], ["Kreyssig", "Florian", ""], ["Byrne", "Bill", ""]]}, {"id": "2107.11906", "submitter": "Zhenhai Zhu", "authors": "Zhenhai Zhu and Radu Soricut", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for\n  Sequences", "comments": "ACL2021 long paper oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 23:07:03 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhu", "Zhenhai", ""], ["Soricut", "Radu", ""]]}, {"id": "2107.11956", "submitter": "Xin-Chun Li", "authors": "Xin-Chun Li, De-Chuan Zhan, Yunfeng Shao, Bingshuai Li, Shaoming Song", "title": "Preliminary Steps Towards Federated Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 04:57:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Xin-Chun", ""], ["Zhan", "De-Chuan", ""], ["Shao", "Yunfeng", ""], ["Li", "Bingshuai", ""], ["Song", "Shaoming", ""]]}, {"id": "2107.11976", "submitter": "Akari Asai", "authors": "Akari Asai, Xinyan Yu, Jungo Kasai, Hannaneh Hajishirzi", "title": "One Question Answering Model for Many Languages with Cross-lingual Dense\n  Passage Retrieval", "comments": "Our code and trained model are publicly available at\n  https://github.com/AkariAsai/CORA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present CORA, a Cross-lingual Open-Retrieval Answer Generation model that\ncan answer questions across many languages even when language-specific\nannotated data or knowledge sources are unavailable. We introduce a new dense\npassage retrieval algorithm that is trained to retrieve documents across\nlanguages for a question. Combined with a multilingual autoregressive\ngeneration model, CORA answers directly in the target language without any\ntranslation or in-language retrieval modules as used in prior work. We propose\nan iterative training method that automatically extends annotated data\navailable only in high-resource languages to low-resource ones. Our results\nshow that CORA substantially outperforms the previous state of the art on\nmultilingual open question answering benchmarks across 26 languages, 9 of which\nare unseen during training. Our analyses show the significance of cross-lingual\nretrieval and generation in many languages, particularly under low-resource\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:02:54 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Asai", "Akari", ""], ["Yu", "Xinyan", ""], ["Kasai", "Jungo", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2107.12064", "submitter": "Zikun Hu", "authors": "Zikun Hu, Yixin Cao, Lifu Huang, Tat-Seng Chua", "title": "How Knowledge Graph and Attention Help? A Quantitative Analysis into\n  Bag-level Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph (KG) and attention mechanism have been demonstrated effective\nin introducing and selecting useful information for weakly supervised methods.\nHowever, only qualitative analysis and ablation study are provided as evidence.\nIn this paper, we contribute a dataset and propose a paradigm to quantitatively\nevaluate the effect of attention and KG on bag-level relation extraction (RE).\nWe find that (1) higher attention accuracy may lead to worse performance as it\nmay harm the model's ability to extract entity mention features; (2) the\nperformance of attention is largely influenced by various noise distribution\npatterns, which is closely related to real-world datasets; (3) KG-enhanced\nattention indeed improves RE performance, while not through enhanced attention\nbut by incorporating entity prior; and (4) attention mechanism may exacerbate\nthe issue of insufficient training data. Based on these findings, we show that\na straightforward variant of RE model can achieve significant improvements (6%\nAUC on average) on two real-world datasets as compared with three\nstate-of-the-art baselines. Our codes and datasets are available at\nhttps://github.com/zig-kwin-hu/how-KG-ATT-help.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:38:28 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hu", "Zikun", ""], ["Cao", "Yixin", ""], ["Huang", "Lifu", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2107.12079", "submitter": "Andrea Galassi", "authors": "Bettina Fazzinga, Andrea Galassi, Paolo Torroni", "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information", "comments": "20 pages, 2 figures, currently under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:58:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fazzinga", "Bettina", ""], ["Galassi", "Andrea", ""], ["Torroni", "Paolo", ""]]}, {"id": "2107.12088", "submitter": "Ond\\v{r}ej Pra\\v{z}\\'ak", "authors": "Ond\\v{r}ej Pra\\v{z}\\'ak, Miloslav Konop\\'ik, Jakub Sido", "title": "Multilingual Coreference Resolution with Harmonized Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present coreference resolution experiments with a newly\ncreated multilingual corpus CorefUD. We focus on the following languages:\nCzech, Russian, Polish, German, Spanish, and Catalan. In addition to\nmonolingual experiments, we combine the training data in multilingual\nexperiments and train two joined models -- for Slavic languages and for all the\nlanguages together. We rely on an end-to-end deep learning model that we\nslightly adapted for the CorefUD corpus. Our results show that we can profit\nfrom harmonized annotations, and using joined models helps significantly for\nthe languages with smaller training data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:11:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pra\u017e\u00e1k", "Ond\u0159ej", ""], ["Konop\u00edk", "Miloslav", ""], ["Sido", "Jakub", ""]]}, {"id": "2107.12135", "submitter": "Ashutosh Modi", "authors": "Gargi Singh and Dhanajit Brahma and Piyush Rai and Ashutosh Modi", "title": "Fine-Grained Emotion Prediction by Modeling Emotion Definitions", "comments": "8 Pages, accepted at ACII 2021 for Orals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a new framework for fine-grained emotion prediction\nin the text through emotion definition modeling. Our approach involves a\nmulti-task learning framework that models definitions of emotions as an\nauxiliary task while being trained on the primary task of emotion prediction.\nWe model definitions using masked language modeling and class definition\nprediction tasks. Our models outperform existing state-of-the-art for\nfine-grained emotion dataset GoEmotions. We further show that this trained\nmodel can be used for transfer learning on other benchmark datasets in emotion\nprediction with varying emotion label sets, domains, and sizes. The proposed\nmodels outperform the baselines on transfer learning experiments demonstrating\nthe generalization capability of the models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:11:18 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Singh", "Gargi", ""], ["Brahma", "Dhanajit", ""], ["Rai", "Piyush", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2107.12168", "submitter": "Hanzhou Wu", "authors": "Biao Yi, Hanzhou Wu, Guorui Feng and Xinpeng Zhang", "title": "Exploiting Language Model for Efficient Linguistic Steganalysis: An\n  Empirical Study", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in linguistic steganalysis have successively applied CNNs,\nRNNs, GNNs and other deep learning models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated steganographic texts and carrier texts in terms of the conditional\nprobability distribution of individual words. Such kind of statistical\ndifference can be naturally captured by the language model used for generating\nsteganographic texts, which drives us to give the classifier a priori knowledge\nof the language model to enhance the steganalysis ability. To this end, we\npresent two methods to efficient linguistic steganalysis in this paper. One is\nto pre-train a language model based on RNN, and the other is to pre-train a\nsequence autoencoder. Experimental results show that the two methods have\ndifferent degrees of performance improvement when compared to the randomly\ninitialized RNN classifier, and the convergence speed is significantly\naccelerated. Moreover, our methods have achieved the best detection results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:37:18 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yi", "Biao", ""], ["Wu", "Hanzhou", ""], ["Feng", "Guorui", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2107.12203", "submitter": "Gongbo Tang", "authors": "Gongbo Tang, Philipp R\\\"onchen, Rico Sennrich, Joakim Nivre", "title": "Revisiting Negation in Neural Machine Translation", "comments": "To appear at TACL and to be presented at ACL 2021. Authors' final\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we evaluate the translation of negation both automatically and\nmanually, in English--German (EN--DE) and English--Chinese (EN--ZH). We show\nthat the ability of neural machine translation (NMT) models to translate\nnegation has improved with deeper and more advanced networks, although the\nperformance varies between language pairs and translation directions. The\naccuracy of manual evaluation in EN-DE, DE-EN, EN-ZH, and ZH-EN is 95.7%,\n94.8%, 93.4%, and 91.7%, respectively. In addition, we show that\nunder-translation is the most significant error type in NMT, which contrasts\nwith the more diverse error profile previously observed for statistical machine\ntranslation. To better understand the root of the under-translation of\nnegation, we study the model's information flow and training data. While our\ninformation flow analysis does not reveal any deficiencies that could be used\nto detect or fix the under-translation of negation, we find that negation is\noften rephrased during training, which could make it more difficult for the\nmodel to learn a reliable link between source and target negation. We finally\nconduct intrinsic analysis and extrinsic probing tasks on negation, showing\nthat NMT models can distinguish negation and non-negation tokens very well and\nencode a lot of information about negation in hidden states but nevertheless\nleave room for improvement.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:19:57 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tang", "Gongbo", ""], ["R\u00f6nchen", "Philipp", ""], ["Sennrich", "Rico", ""], ["Nivre", "Joakim", ""]]}, {"id": "2107.12214", "submitter": "Lu Xu", "authors": "Lu Xu, Yew Ken Chia, Lidong Bing", "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction", "comments": "ACL 2021, long paper, main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA\nwhich outputs triplets of an aspect target, its associated sentiment, and the\ncorresponding opinion term. Recent models perform the triplet extraction in an\nend-to-end manner but heavily rely on the interactions between each target word\nand opinion word. Thereby, they cannot perform well on targets and opinions\nwhich contain multiple words. Our proposed span-level approach explicitly\nconsiders the interaction between the whole spans of targets and opinions when\npredicting their sentiment relation. Thus, it can make predictions with the\nsemantics of whole spans, ensuring better sentiment consistency. To ease the\nhigh computational cost caused by span enumeration, we propose a dual-channel\nspan pruning strategy by incorporating supervision from the Aspect Term\nExtraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not\nonly improves computational efficiency but also distinguishes the opinion and\ntarget spans more properly. Our framework simultaneously achieves strong\nperformance for the ASTE as well as ATE and OTE tasks. In particular, our\nanalysis shows that our span-level approach achieves more significant\nimprovements over the baselines on triplets with multi-word targets or\nopinions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:47:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Lu", ""], ["Chia", "Yew Ken", ""], ["Bing", "Lidong", ""]]}, {"id": "2107.12220", "submitter": "Hendrik Schuff", "authors": "Hendrik Schuff, Heike Adel, Ngoc Thang Vu", "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:56:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Schuff", "Hendrik", ""], ["Adel", "Heike", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2107.12226", "submitter": "Ivan P Yamshchikov", "authors": "Anastasia Malysheva, Alexey Tikhonov, Ivan P. Yamshchikov", "title": "DYPLODOC: Dynamic Plots for Document Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narrative generation and analysis are still on the fringe of modern natural\nlanguage processing yet are crucial in a variety of applications. This paper\nproposes a feature extraction method for plot dynamics. We present a dataset\nthat consists of the plot descriptions for thirteen thousand TV shows alongside\nmeta-information on their genres and dynamic plots extracted from them. We\nvalidate the proposed tool for plot dynamics extraction and discuss possible\napplications of this method to the tasks of narrative analysis and generation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:12:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Malysheva", "Anastasia", ""], ["Tikhonov", "Alexey", ""], ["Yamshchikov", "Ivan P.", ""]]}, {"id": "2107.12262", "submitter": "Chengcheng Han", "authors": "ChengCheng Han, Zeqiu Fan, Dongxiang Zhang, Minghui Qiu, Ming Gao,\n  Aoying Zhou", "title": "Meta-Learning Adversarial Domain Adaptation Network for Few-Shot Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning has emerged as a trending technique to tackle few-shot text\nclassification and achieved state-of-the-art performance. However, existing\nsolutions heavily rely on the exploitation of lexical features and their\ndistributional signatures on training data, while neglecting to strengthen the\nmodel's ability to adapt to new tasks. In this paper, we propose a novel\nmeta-learning framework integrated with an adversarial domain adaptation\nnetwork, aiming to improve the adaptive ability of the model and generate\nhigh-quality text embedding for new classes. Extensive experiments are\nconducted on four benchmark datasets and our method demonstrates clear\nsuperiority over the state-of-the-art models in all the datasets. In\nparticular, the accuracy of 1-shot and 5-shot classification on the dataset of\n20 Newsgroups is boosted from 52.1% to 59.6%, and from 68.3% to 77.8%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:09:40 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Han", "ChengCheng", ""], ["Fan", "Zeqiu", ""], ["Zhang", "Dongxiang", ""], ["Qiu", "Minghui", ""], ["Gao", "Ming", ""], ["Zhou", "Aoying", ""]]}, {"id": "2107.12428", "submitter": "Seba Susan", "authors": "Sunakshi Mehra, Seba Susan", "title": "Improving Word Recognition in Speech Transcriptions by Decision-level\n  Fusion of Stemming and Two-way Phoneme Pruning", "comments": "Accepted in International Advanced Computing Conference (2020)", "journal-ref": null, "doi": "10.1007/978-981-16-0401-0_19", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce an unsupervised approach for correcting highly imperfect speech\ntranscriptions based on a decision-level fusion of stemming and two-way phoneme\npruning. Transcripts are acquired from videos by extracting audio using Ffmpeg\nframework and further converting audio to text transcript using Google API. In\nthe benchmark LRW dataset, there are 500 word categories, and 50 videos per\nclass in mp4 format. All videos consist of 29 frames (each 1.16 s long) and the\nword appears in the middle of the video. In our approach we tried to improve\nthe baseline accuracy from 9.34% by using stemming, phoneme extraction,\nfiltering and pruning. After applying the stemming algorithm to the text\ntranscript and evaluating the results, we achieved 23.34% accuracy in word\nrecognition. To convert words to phonemes we used the Carnegie Mellon\nUniversity (CMU) pronouncing dictionary that provides a phonetic mapping of\nEnglish words to their pronunciations. A two-way phoneme pruning is proposed\nthat comprises of the two non-sequential steps: 1) filtering and pruning the\nphonemes containing vowels and plosives 2) filtering and pruning the phonemes\ncontaining vowels and fricatives. After obtaining results of stemming and\ntwo-way phoneme pruning, we applied decision-level fusion and that led to an\nimprovement of word recognition rate upto 32.96%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:44:24 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Mehra", "Sunakshi", ""], ["Susan", "Seba", ""]]}, {"id": "2107.12514", "submitter": "Jesse Thomason", "authors": "Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, Luke\n  Zettlemoyer", "title": "Language Grounding with 3D Objects", "comments": "https://github.com/snaredataset/snare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 23:35:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Thomason", "Jesse", ""], ["Shridhar", "Mohit", ""], ["Bisk", "Yonatan", ""], ["Paxton", "Chris", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2107.12542", "submitter": "Yawen Ouyang", "authors": "Yawen Ouyang, Jiasheng Ye, Yu Chen, Xinyu Dai, Shujian Huang, Jiajun\n  Chen", "title": "Energy-based Unknown Intent Detection with Data Manipulation", "comments": "10 pages, 4 figures, accepted by Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unknown intent detection aims to identify the out-of-distribution (OOD)\nutterance whose intent has never appeared in the training set. In this paper,\nwe propose using energy scores for this task as the energy score is\ntheoretically aligned with the density of the input and can be derived from any\nclassifier. However, high-quality OOD utterances are required during the\ntraining stage in order to shape the energy gap between OOD and in-distribution\n(IND), and these utterances are difficult to collect in practice. To tackle\nthis problem, we propose a data manipulation framework to Generate high-quality\nOOD utterances with importance weighTs (GOT). Experimental results show that\nthe energy-based detector fine-tuned by GOT can achieve state-of-the-art\nresults on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 01:32:23 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ouyang", "Yawen", ""], ["Ye", "Jiasheng", ""], ["Chen", "Yu", ""], ["Dai", "Xinyu", ""], ["Huang", "Shujian", ""], ["Chen", "Jiajun", ""]]}, {"id": "2107.12578", "submitter": "Jinyu Guo", "authors": "Jinyu Guo, Kai Shuang, Jijie Li and Zihan Wang", "title": "Dual Slot Selector via Local Reliability Verification for Dialogue State\n  Tracking", "comments": "Accepted by ACL-IJCNLP 2021 main conference (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of dialogue state tracking (DST) is to predict the current dialogue\nstate given all previous dialogue contexts. Existing approaches generally\npredict the dialogue state at every turn from scratch. However, the\noverwhelming majority of the slots in each turn should simply inherit the slot\nvalues from the previous turn. Therefore, the mechanism of treating slots\nequally in each turn not only is inefficient but also may lead to additional\nerrors because of the redundant slot value generation. To address this problem,\nwe devise the two-stage DSS-DST which consists of the Dual Slot Selector based\non the current turn dialogue, and the Slot Value Generator based on the\ndialogue history. The Dual Slot Selector determines each slot whether to update\nslot value or to inherit the slot value from the previous turn from two\naspects: (1) if there is a strong relationship between it and the current turn\ndialogue utterances; (2) if a slot value with high reliability can be obtained\nfor it through the current turn dialogue. The slots selected to be updated are\npermitted to enter the Slot Value Generator to update values by a hybrid\nmethod, while the other slots directly inherit the values from the previous\nturn. Empirical results show that our method achieves 56.93%, 60.73%, and\n58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets\nrespectively and achieves a new state-of-the-art performance with significant\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:40:05 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Guo", "Jinyu", ""], ["Shuang", "Kai", ""], ["Li", "Jijie", ""], ["Wang", "Zihan", ""]]}, {"id": "2107.12603", "submitter": "Ming Liu Dr", "authors": "Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang", "title": "Federated Learning Meets Natural Language Processing: A Survey", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 05:07:48 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Liu", "Ming", ""], ["Ho", "Stella", ""], ["Wang", "Mengqi", ""], ["Gao", "Longxiang", ""], ["Jin", "Yuan", ""], ["Zhang", "He", ""]]}, {"id": "2107.12606", "submitter": "Yuchen Chai Mr", "authors": "Yuchen Chai (1), Juan Palacios (1), Jianghao Wang (2), Yichun Fan (1)\n  and Siqi Zheng (1) ((1) Massachusetts Institute of Technology, (2) Chinese\n  Academy of Science)", "title": "Measuring daily-life fear perception change: a computational study in\n  the context of COVID-19", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19, as a global health crisis, has triggered the fear emotion with\nunprecedented intensity. Besides the fear of getting infected, the outbreak of\nCOVID-19 also created significant disruptions in people's daily life and thus\nevoked intensive psychological responses indirect to COVID-19 infections. Here,\nwe construct an expressed fear database using 16 million social media posts\ngenerated by 536 thousand users between January 1st, 2019 and August 31st, 2020\nin China. We employ deep learning techniques to detect the fear emotion within\neach post and apply topic models to extract the central fear topics. Based on\nthis database, we find that sleep disorders (\"nightmare\" and \"insomnia\") take\nup the largest share of fear-labeled posts in the pre-pandemic period (January\n2019-December 2019), and significantly increase during the COVID-19. We\nidentify health and work-related concerns are the two major sources of fear\ninduced by the COVID-19. We also detect gender differences, with females\ngenerating more posts containing the daily-life fear sources during the\nCOVID-19 period. This research adopts a data-driven approach to trace back\npublic emotion, which can be used to complement traditional surveys to achieve\nreal-time emotion monitoring to discern societal concerns and support policy\ndecision-making.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 05:17:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Chai", "Yuchen", ""], ["Palacios", "Juan", ""], ["Wang", "Jianghao", ""], ["Fan", "Yichun", ""], ["Zheng", "Siqi", ""]]}, {"id": "2107.12627", "submitter": "Zuchao Li", "authors": "Zuchao Li, Kevin Parnow, Hai Zhao, Zhuosheng Zhang, Rui Wang, Masao\n  Utiyama, Eiichiro Sumita", "title": "Cross-lingual Transferring of Pre-trained Contextualized Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the pre-trained contextualized language model (PrLM) has made a\nsignificant impact on NLP, training PrLMs in languages other than English can\nbe impractical for two reasons: other languages often lack corpora sufficient\nfor training powerful PrLMs, and because of the commonalities among human\nlanguages, computationally expensive PrLM training for different languages is\nsomewhat redundant. In this work, building upon the recent works connecting\ncross-lingual model transferring and neural machine translation, we thus\npropose a novel cross-lingual model transferring framework for PrLMs: TreLM. To\nhandle the symbol order and sequence length differences between languages, we\npropose an intermediate ``TRILayer\" structure that learns from these\ndifferences and creates a better transfer in our primary translation direction,\nas well as a new cross-lingual language modeling objective for transfer\ntraining. Additionally, we showcase an embedding aligning that adversarially\nadapts a PrLM's non-contextualized embedding space and the TRILayer structure\nto learn a text transformation network across languages, which addresses the\nvocabulary difference between languages. Experiments on both language\nunderstanding and structure parsing tasks show the proposed framework\nsignificantly outperforms language models trained from scratch with limited\ndata in both performance and efficiency. Moreover, despite an insignificant\nperformance loss compared to pre-training from scratch in resource-rich\nscenarios, our cross-lingual model transferring framework is significantly more\neconomical.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 06:51:13 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Zuchao", ""], ["Parnow", "Kevin", ""], ["Zhao", "Hai", ""], ["Zhang", "Zhuosheng", ""], ["Wang", "Rui", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "2107.12651", "submitter": "Xinzhe Han", "authors": "Xinzhe Han, Shuhui Wang, Chi Su, Qingming Huang, Qi Tian", "title": "Greedy Gradient Ensemble for Robust Visual Question Answering", "comments": "Accepted by ICCV 2021. Code: https://github.com/GeraldHan/GGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:02:49 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Han", "Xinzhe", ""], ["Wang", "Shuhui", ""], ["Su", "Chi", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2107.12708", "submitter": "Anna Rogers", "authors": "Anna Rogers, Matt Gardner, and Isabelle Augenstein", "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering\n  and Reading Comprehension", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of ``reasoning types\" in question answering and\npropose a new taxonomy. We also discuss the implications of over-focusing on\nEnglish, and survey the current monolingual resources for other languages and\nmultilingual resources. The study is aimed at both practitioners looking for\npointers to the wealth of existing data, and at researchers working on new\nresources.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:09:13 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Rogers", "Anna", ""], ["Gardner", "Matt", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2107.12866", "submitter": "Sheikh Muhammad Sarwar", "authors": "Sheikh Muhammad Sarwar and Vanessa Murdock", "title": "Unsupervised Domain Adaptation for Hate Speech Detection Using a Data\n  Augmentation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online harassment in the form of hate speech has been on the rise in recent\nyears. Addressing the issue requires a combination of content moderation by\npeople, aided by automatic detection methods. As content moderation is itself\nharmful to the people doing it, we desire to reduce the burden by improving the\nautomatic detection of hate speech. Hate speech presents a challenge as it is\ndirected at different target groups using a completely different vocabulary.\nFurther the authors of the hate speech are incentivized to disguise their\nbehavior to avoid being removed from a platform. This makes it difficult to\ndevelop a comprehensive data set for training and evaluating hate speech\ndetection models because the examples that represent one hate speech domain do\nnot typically represent others, even within the same language or culture. We\npropose an unsupervised domain adaptation approach to augment labeled data for\nhate speech detection. We evaluate the approach with three different models\n(character CNNs, BiLSTMs and BERT) on three different collections. We show our\napproach improves Area under the Precision/Recall curve by as much as 42% and\nrecall by as much as 278%, with no loss (and in some cases a significant gain)\nin precision.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:01:22 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Murdock", "Vanessa", ""]]}, {"id": "2107.12895", "submitter": "Roman Klinger", "authors": "Felix Casel and Amelie Heindl and Roman Klinger", "title": "Emotion Recognition under Consideration of the Emotion Component Process\n  Model", "comments": "accepted at KONVENS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:53:25 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Casel", "Felix", ""], ["Heindl", "Amelie", ""], ["Klinger", "Roman", ""]]}, {"id": "2107.12920", "submitter": "Roman Klinger", "authors": "Bao Minh Doan Dang and Laura Oberl\\\"ander and Roman Klinger", "title": "Emotion Stimulus Detection in German News Headlines", "comments": "accepted at KONVENS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:22:04 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 07:21:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Dang", "Bao Minh Doan", ""], ["Oberl\u00e4nder", "Laura", ""], ["Klinger", "Roman", ""]]}, {"id": "2107.12930", "submitter": "James Barry", "authors": "James Barry, Joachim Wagner, Lauren Cassidy, Alan Cowap, Teresa Lynn,\n  Abigail Walsh, M\\'iche\\'al J. \\'O Meachair, Jennifer Foster", "title": "gaBERT -- an Irish Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many Natural Language Processing\ntasks. Over 120 monolingual BERT models covering over 50 languages have been\nreleased, as well as a multilingual model trained on 104 languages. We\nintroduce, gaBERT, a monolingual BERT model for the Irish language. We compare\nour gaBERT model to multilingual BERT and show that gaBERT provides better\nrepresentations for a downstream parsing task. We also show how different\nfiltering criteria, vocabulary size and the choice of subword tokenisation\nmodel affect downstream performance. We release gaBERT and related code to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:38:53 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 08:20:27 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Barry", "James", ""], ["Wagner", "Joachim", ""], ["Cassidy", "Lauren", ""], ["Cowap", "Alan", ""], ["Lynn", "Teresa", ""], ["Walsh", "Abigail", ""], ["Meachair", "M\u00edche\u00e1l J. \u00d3", ""], ["Foster", "Jennifer", ""]]}, {"id": "2107.13031", "submitter": "Martin Andrews", "authors": "Vivek Kalyan and Sam Witteveen and Martin Andrews", "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference\n  Explanation Regeneration by Matching Expert Ratings", "comments": "Accepted paper for TextGraphs-15 workshop at NAACL 2021. (5 pages\n  including references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 18:29:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kalyan", "Vivek", ""], ["Witteveen", "Sam", ""], ["Andrews", "Martin", ""]]}, {"id": "2107.13054", "submitter": "Cameron R. Wolfe", "authors": "Cameron R. Wolfe and Keld T. Lundgaard", "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:42:14 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wolfe", "Cameron R.", ""], ["Lundgaard", "Keld T.", ""]]}, {"id": "2107.13077", "submitter": "Yufei Wang", "authors": "Yufei Wang, Can Xu, Huang Hu, Chongyang Tao, Stephen Wan, Mark Dras,\n  Mark Johnson, Daxin Jiang", "title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-Sequence (S2S) neural text generation models, especially the\npre-trained ones (e.g., BART and T5), have exhibited compelling performance on\nvarious natural language generation tasks. However, the black-box nature of\nthese models limits their application in tasks where specific rules (e.g.,\ncontrollable constraints, prior knowledge) need to be executed. Previous works\neither design specific model structure (e.g., Copy Mechanism corresponding to\nthe rule \"the generated output should include certain words in the source\ninput\") or implement specialized inference algorithm (e.g., Constrained Beam\nSearch) to execute particular rules through the text generation. These methods\nrequire careful design case-by-case and are difficult to support multiple rules\nconcurrently. In this paper, we propose a novel module named Neural\nRule-Execution Tracking Machine that can be equipped into various\ntransformer-based generators to leverage multiple rules simultaneously to guide\nthe neural generation model for superior generation performance in a unified\nand scalable way. Extensive experimental results on several benchmarks verify\nthe effectiveness of our proposed model in both controllable and general text\ngeneration.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:41:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wang", "Yufei", ""], ["Xu", "Can", ""], ["Hu", "Huang", ""], ["Tao", "Chongyang", ""], ["Wan", "Stephen", ""], ["Dras", "Mark", ""], ["Johnson", "Mark", ""], ["Jiang", "Daxin", ""]]}, {"id": "2107.13165", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Rene Clever, Jaysa Ramirez, Gale Lucas, Jonathan Gratch", "title": "Towards Emotion-Aware Agents For Negotiation Dialogues", "comments": "Accepted at 9th International Conference on Affective Computing &\n  Intelligent Interaction (ACII 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 04:42:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chawla", "Kushal", ""], ["Clever", "Rene", ""], ["Ramirez", "Jaysa", ""], ["Lucas", "Gale", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2107.13189", "submitter": "Qing Lyu", "authors": "Qing Lyu, Li Zhang, Chris Callison-Burch", "title": "Goal-Oriented Script Construction", "comments": "To be published in INLG2021 (14th International Conference on Natural\n  Language Generation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 06:39:31 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lyu", "Qing", ""], ["Zhang", "Li", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2107.13290", "submitter": "Mohammed Mustafa Abdelgwad", "authors": "Mohammed M.Abdelgwad", "title": "Arabic aspect based sentiment analysis using BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:34:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Abdelgwad", "Mohammed M.", ""]]}, {"id": "2107.13377", "submitter": "Michael Tessler", "authors": "Michael Henry Tessler, Pedro A. Tsividis, Jason Madeano, Brin Harper,\n  and Joshua B. Tenenbaum", "title": "Growing knowledge culturally across generations to solve novel, complex\n  tasks", "comments": "Poster presentation at the 43rd Annual Meeting of the Cognitive\n  Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge built culturally across generations allows humans to learn far more\nthan an individual could glean from their own experience in a lifetime.\nCultural knowledge in turn rests on language: language is the richest record of\nwhat previous generations believed, valued, and practiced. The power and\nmechanisms of language as a means of cultural learning, however, are not well\nunderstood. We take a first step towards reverse-engineering cultural learning\nthrough language. We developed a suite of complex high-stakes tasks in the form\nof minimalist-style video games, which we deployed in an iterated learning\nparadigm. Game participants were limited to only two attempts (two lives) to\nbeat each game and were allowed to write a message to a future participant who\nread the message before playing. Knowledge accumulated gradually across\ngenerations, allowing later generations to advance further in the games and\nperform more efficient actions. Multigenerational learning followed a\nstrikingly similar trajectory to individuals learning alone with an unlimited\nnumber of lives. These results suggest that language provides a sufficient\nmedium to express and accumulate the knowledge people acquire in these diverse\ntasks: the dynamics of the environment, valuable goals, dangerous risks, and\nstrategies for success. The video game paradigm we pioneer here is thus a rich\ntest bed for theories of cultural transmission and learning from language.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:09:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Tessler", "Michael Henry", ""], ["Tsividis", "Pedro A.", ""], ["Madeano", "Jason", ""], ["Harper", "Brin", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2107.13425", "submitter": "Mi Zhang", "authors": "Mi Zhang, Tieyun Qian", "title": "Multi-Scale Feature and Metric Learning for Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods in relation extraction have leveraged the lexical features\nin the word sequence and the syntactic features in the parse tree. Though\neffective, the lexical features extracted from the successive word sequence may\nintroduce some noise that has little or no meaningful content. Meanwhile, the\nsyntactic features are usually encoded via graph convolutional networks which\nhave restricted receptive field. To address the above limitations, we propose a\nmulti-scale feature and metric learning framework for relation extraction.\nSpecifically, we first develop a multi-scale convolutional neural network to\naggregate the non-successive mainstays in the lexical sequence. We also design\na multi-scale graph convolutional network which can increase the receptive\nfield towards specific syntactic roles. Moreover, we present a multi-scale\nmetric learning paradigm to exploit both the feature-level relation between\nlexical and syntactic features and the sample-level relation between instances\nwith the same or different classes. We conduct extensive experiments on three\nreal world datasets for various types of relation extraction tasks. The results\ndemonstrate that our model significantly outperforms the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:14:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Mi", ""], ["Qian", "Tieyun", ""]]}, {"id": "2107.13530", "submitter": "Bethan Thomas", "authors": "Samuel Kessler, Bethan Thomas, Salah Karout", "title": "Continual-wav2vec2: an Application of Continual Learning for\n  Self-Supervised Automatic Speech Recognition", "comments": "11 pages, 9 figures including references and appendix. Accepted at\n  ICML 2021 Workshop: Self-Supervised Learning for Reasoning and Perception", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:39:03 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kessler", "Samuel", ""], ["Thomas", "Bethan", ""], ["Karout", "Salah", ""]]}, {"id": "2107.13541", "submitter": "Xinshuai Dong", "authors": "Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, Hong Liu", "title": "Towards Robustness Against Natural Language Word Substitutions", "comments": "Conference paper ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness against word substitutions has a well-defined and widely\nacceptable form, i.e., using semantically similar words as substitutions, and\nthus it is considered as a fundamental stepping-stone towards broader\nrobustness in natural language processing. Previous defense methods capture\nword substitutions in vector space by using either $l_2$-ball or\nhyper-rectangle, which results in perturbation sets that are not inclusive\nenough or unnecessarily large, and thus impedes mimicry of worst cases for\nrobust training. In this paper, we introduce a novel \\textit{Adversarial Sparse\nConvex Combination} (ASCC) method. We model the word substitution attack space\nas a convex hull and leverages a regularization term to enforce perturbation\ntowards an actual substitution, thus aligning our modeling better with the\ndiscrete textual space. Based on the ASCC method, we further propose\nASCC-defense, which leverages ASCC to generate worst-case perturbations and\nincorporates adversarial training towards robustness. Experiments show that\nASCC-defense outperforms the current state-of-the-arts in terms of robustness\non two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural\nlanguage inference, concerning several attacks across multiple model\narchitectures. Besides, we also envision a new class of defense towards\nrobustness in NLP, where our robustly trained word vectors can be plugged into\na normally trained model and enforce its robustness without applying any other\ndefense techniques.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:55:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Dong", "Xinshuai", ""], ["Luu", "Anh Tuan", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""]]}, {"id": "2107.13586", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi,\n  Graham Neubig", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "comments": "Website: http://pretrain.nlpedia.ai/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website http://pretrain.nlpedia.ai/ including\nconstantly-updated survey, and paperlist.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 18:09:46 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liu", "Pengfei", ""], ["Yuan", "Weizhe", ""], ["Fu", "Jinlan", ""], ["Jiang", "Zhengbao", ""], ["Hayashi", "Hiroaki", ""], ["Neubig", "Graham", ""]]}, {"id": "2107.13592", "submitter": "Leon Derczynski", "authors": "Erida Nurce, Jorgel Keci, Leon Derczynski", "title": "Detecting Abusive Albanian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ever growing usage of social media in the recent years has had a direct\nimpact on the increased presence of hate speech and offensive speech in online\nplatforms. Research on effective detection of such content has mainly focused\non English and a few other widespread languages, while the leftover majority\nfail to have the same work put into them and thus cannot benefit from the\nsteady advancements made in the field. In this paper we present \\textsc{Shaj},\nan annotated Albanian dataset for hate speech and offensive speech that has\nbeen constructed from user-generated content on various social media platforms.\nIts annotation follows the hierarchical schema introduced in OffensEval. The\ndataset is tested using three different classification models, the best of\nwhich achieves an F1 score of 0.77 for the identification of offensive\nlanguage, 0.64 F1 score for the automatic categorization of offensive types and\nlastly, 0.52 F1 score for the offensive language target identification.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 18:47:32 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Nurce", "Erida", ""], ["Keci", "Jorgel", ""], ["Derczynski", "Leon", ""]]}, {"id": "2107.13602", "submitter": "Barlas Oguz", "authors": "Barlas O\\u{g}uz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis,\n  Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Wen-tau\n  Yih, Sonal Gupta, Yashar Mehdad", "title": "Domain-matched Pre-training Tasks for Dense Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 19:13:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["O\u011fuz", "Barlas", ""], ["Lakhotia", "Kushal", ""], ["Gupta", "Anchit", ""], ["Lewis", "Patrick", ""], ["Karpukhin", "Vladimir", ""], ["Piktus", "Aleksandra", ""], ["Chen", "Xilun", ""], ["Riedel", "Sebastian", ""], ["Yih", "Wen-tau", ""], ["Gupta", "Sonal", ""], ["Mehdad", "Yashar", ""]]}, {"id": "2107.13662", "submitter": "Laura V\\'asquez-Rodr\\'iguez", "authors": "Laura V\\'asquez-Rodr\\'iguez, Matthew Shardlow, Piotr Przyby{\\l}a,\n  Sophia Ananiadou", "title": "Investigating Text Simplification Evaluation", "comments": "7 pages, 3 figures, 1 table", "journal-ref": "Findings of the Association for Computational Linguistics:\n  ACL-IJCNLP 2021, pages 876-882", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern text simplification (TS) heavily relies on the availability of gold\nstandard data to build machine learning models. However, existing studies show\nthat parallel TS corpora contain inaccurate simplifications and incorrect\nalignments. Additionally, evaluation is usually performed by using metrics such\nas BLEU or SARI to compare system output to the gold standard. A major\nlimitation is that these metrics do not match human judgements and the\nperformance on different datasets and linguistic phenomena vary greatly.\nFurthermore, our research shows that the test and training subsets of parallel\ndatasets differ significantly. In this work, we investigate existing TS\ncorpora, providing new insights that will motivate the improvement of existing\nstate-of-the-art TS evaluation methods. Our contributions include the analysis\nof TS corpora based on existing modifications used for simplification and an\nempirical study on TS models performance by using better-distributed datasets.\nWe demonstrate that by improving the distribution of TS datasets, we can build\nmore robust TS models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 22:49:32 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["V\u00e1squez-Rodr\u00edguez", "Laura", ""], ["Shardlow", "Matthew", ""], ["Przyby\u0142a", "Piotr", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "2107.13686", "submitter": "Yichun Yin", "authors": "Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu", "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient\n  Pre-trained Language Models", "comments": "ACL 2021. The code and models are released at\n  https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/AutoTinyBERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 00:47:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yin", "Yichun", ""], ["Chen", "Cheng", ""], ["Shang", "Lifeng", ""], ["Jiang", "Xin", ""], ["Chen", "Xiao", ""], ["Liu", "Qun", ""]]}, {"id": "2107.13689", "submitter": "Yui Oka", "authors": "Yui Oka, Katsuhito Sudoh, and Satoshi Nakamura", "title": "Using Perturbed Length-aware Positional Encoding for Non-autoregressive\n  Neural Machine Translation", "comments": "5 pages, 1 figures. Will be presented at ACL SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-autoregressive neural machine translation (NAT) usually employs\nsequence-level knowledge distillation using autoregressive neural machine\ntranslation (AT) as its teacher model. However, a NAT model often outputs\nshorter sentences than an AT model. In this work, we propose sequence-level\nknowledge distillation (SKD) using perturbed length-aware positional encoding\nand apply it to a student model, the Levenshtein Transformer. Our method\noutperformed a standard Levenshtein Transformer by 2.5 points in bilingual\nevaluation understudy (BLEU) at maximum in a WMT14 German to English\ntranslation. The NAT model output longer sentences than the baseline NAT\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 00:51:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Oka", "Yui", ""], ["Sudoh", "Katsuhito", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "2107.13764", "submitter": "Sohom Ghosh", "authors": "Ankush Chopra and Sohom Ghosh", "title": "Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking\n  of Financial Terms", "comments": "Accepted at 3rd Workshop on Financial Technology and Natural Language\n  Processing (FinNLP) in conjunction with 30th International Joint Conference\n  on Artificial Intelligence (IJCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hypernym and synonym matching are one of the mainstream Natural Language\nProcessing (NLP) tasks. In this paper, we present systems that attempt to solve\nthis problem. We designed these systems to participate in the FinSim-3, a\nshared task of FinNLP workshop at IJCAI-2021. The shared task is focused on\nsolving this problem for the financial domain. We experimented with various\ntransformer based pre-trained embeddings by fine-tuning these for either\nclassification or phrase similarity tasks. We also augmented the provided\ndataset with abbreviations derived from prospectus provided by the organizers\nand definitions of the financial terms from DBpedia [Auer et al., 2007],\nInvestopedia, and the Financial Industry Business Ontology (FIBO). Our best\nperforming system uses both FinBERT [Araci, 2019] and data augmentation from\nthe afore-mentioned sources. We observed that term expansion using data\naugmentation in conjunction with semantic similarity is beneficial for this\ntask and could be useful for the other tasks that deal with short phrases. Our\nbest performing model (Accuracy: 0.917, Rank: 1.156) was developed by\nfine-tuning SentenceBERT [Reimers et al., 2019] (with FinBERT at the backend)\nover an extended labelled set created using the hierarchy of labels present in\nFIBO.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 06:17:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chopra", "Ankush", ""], ["Ghosh", "Sohom", ""]]}, {"id": "2107.13935", "submitter": "Mor Geva", "authors": "Mor Geva, Tomer Wolfson, Jonathan Berant", "title": "Break, Perturb, Build: Automatic Perturbation of Reasoning Paths through\n  Question Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts to create challenge benchmarks that test the abilities of\nnatural language understanding models have largely depended on human\nannotations. In this work, we introduce the \"Break, Perturb, Build\" (BPB)\nframework for automatic reasoning-oriented perturbation of question-answer\npairs. BPB represents a question by decomposing it into the reasoning steps\nthat are required to answer it, symbolically perturbs the decomposition, and\nthen generates new question-answer pairs. We demonstrate the effectiveness of\nBPB by creating evaluation sets for three reading comprehension (RC)\nbenchmarks, generating thousands of high-quality examples without human\nintervention. We evaluate a range of RC models on our evaluation sets, which\nreveals large performance gaps on generated examples compared to the original\ndata. Moreover, symbolic perturbations enable fine-grained analysis of the\nstrengths and limitations of models. Last, augmenting the training data with\nexamples generated by BPB helps close performance gaps, without any drop on the\noriginal data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 12:49:03 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Geva", "Mor", ""], ["Wolfson", "Tomer", ""], ["Berant", "Jonathan", ""]]}, {"id": "2107.13955", "submitter": "Louis Clouatre", "authors": "Louis Clouatre, Prasanna Parthasarathi, Amal Zouaq, Sarath Chandar", "title": "Demystifying Neural Language Models' Insensitivity to Word-Order", "comments": "11 pages, 13 figure + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research analyzing the sensitivity of natural language understanding\nmodels to word-order perturbations have shown that the state-of-the-art models\nin several language tasks may have a unique way to understand the text that\ncould seldom be explained with conventional syntax and semantics. In this\npaper, we investigate the insensitivity of natural language models to\nword-order by quantifying perturbations and analysing their effect on neural\nmodels' performance on language understanding tasks in GLUE benchmark. Towards\nthat end, we propose two metrics - the Direct Neighbour Displacement (DND) and\nthe Index Displacement Count (IDC) - that score the local and global ordering\nof tokens in the perturbed texts and observe that perturbation functions found\nin prior literature affect only the global ordering while the local ordering\nremains relatively unperturbed. We propose perturbations at the granularity of\nsub-words and characters to study the correlation between DND, IDC and the\nperformance of neural language models on natural language tasks. We find that\nneural language models - pretrained and non-pretrained Transformers, LSTMs, and\nConvolutional architectures - require local ordering more so than the global\nordering of tokens. The proposed metrics and the suite of perturbations allow a\nsystematic way to study the (in)sensitivity of neural language understanding\nmodels to varying degree of perturbations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 13:34:20 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Clouatre", "Louis", ""], ["Parthasarathi", "Prasanna", ""], ["Zouaq", "Amal", ""], ["Chandar", "Sarath", ""]]}, {"id": "2107.14154", "submitter": "Chester Palen-Michel", "authors": "Chester Palen-Michel, Nolan Holley, Constantine Lignos", "title": "Addressing Barriers to Reproducible Named Entity Recognition Evaluation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To address what we believe is a looming crisis of unreproducible evaluation\nfor named entity recognition tasks, we present guidelines for reproducible\nevaluation. The guidelines we propose are extremely simple, focusing on\ntransparency regarding how chunks are encoded and scored, but very few papers\ncurrently being published fully comply with them. We demonstrate that despite\nthe apparent simplicity of NER evaluation, unreported differences in the\nscoring procedure can result in changes to scores that are both of noticeable\nmagnitude and are statistically significant. We provide SeqScore, an open\nsource toolkit that addresses many of the issues that cause replication\nfailures and makes following our guidelines easy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:26:04 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Palen-Michel", "Chester", ""], ["Holley", "Nolan", ""], ["Lignos", "Constantine", ""]]}]