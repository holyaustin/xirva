[{"id": "1504.00325", "submitter": "C. Lawrence Zitnick", "authors": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh\n  Gupta, Piotr Dollar, C. Lawrence Zitnick", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "comments": "arXiv admin note: text overlap with arXiv:1411.4952", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the Microsoft COCO Caption dataset and evaluation\nserver. When completed, the dataset will contain over one and a half million\ncaptions describing over 330,000 images. For the training and validation\nimages, five independent human generated captions will be provided. To ensure\nconsistency in evaluation of automatic caption generation algorithms, an\nevaluation server is used. The evaluation server receives candidate captions\nand scores them using several popular metrics, including BLEU, METEOR, ROUGE\nand CIDEr. Instructions for using the evaluation server are provided.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 18:13:43 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 20:21:16 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Chen", "Xinlei", ""], ["Fang", "Hao", ""], ["Lin", "Tsung-Yi", ""], ["Vedantam", "Ramakrishna", ""], ["Gupta", "Saurabh", ""], ["Dollar", "Piotr", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1504.00548", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen and Yoshua Bengio", "title": "Learning to Understand Phrases by Embedding the Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional models that learn rich semantic word representations are a\nsuccess story of recent NLP research. However, developing models that learn\nuseful representations of phrases and sentences has proved far harder. We\npropose using the definitions found in everyday dictionaries as a means of\nbridging this gap between lexical and phrasal semantics. Neural language\nembedding models can be effectively trained to map dictionary definitions\n(phrases) to (lexical) representations of the words defined by those\ndefinitions. We present two applications of these architectures: \"reverse\ndictionaries\" that return the name of a concept given a definition or\ndescription and general-knowledge crossword question answerers. On both tasks,\nneural language embedding models trained on definitions from a handful of\nfreely-available lexical resources perform as well or better than existing\ncommercial systems that rely on significant task-specific engineering. The\nresults highlight the effectiveness of both neural embedding architectures and\ndefinition-based training for developing models that understand phrases and\nsentences.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 13:30:27 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 19:34:07 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2015 21:34:26 GMT"}, {"version": "v4", "created": "Tue, 22 Mar 2016 16:30:17 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Hill", "Felix", ""], ["Cho", "Kyunghyun", ""], ["Korhonen", "Anna", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1504.00657", "submitter": "Geoffrey Fairchild", "authors": "Geoffrey Fairchild (1 and 3), Lalindra De Silva (2), Sara Y. Del Valle\n  (1), Alberto M. Segre (3) ((1) Los Alamos National Laboratory, Los Alamos,\n  NM, USA, (2) The University of Utah, Salt Lake City, UT, USA, (3) The\n  University of Iowa, Iowa City, IA, USA)", "title": "Eliciting Disease Data from Wikipedia Articles", "comments": "9 pages, 3 figures, 4 tables, accepted to 2015 ICWSM Wikipedia\n  workshop; v2 includes author formatting fixes and a few sentences removed to\n  make it 8 pages (although arXiv renders it as 9); v3 uses embedded type 1\n  fonts in the figures and title-cases the title (required by AAAI); v4 fixes\n  typo in abstract", "journal-ref": null, "doi": null, "report-no": "LA-UR-15-22528", "categories": "cs.IR cs.CL cs.SI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional disease surveillance systems suffer from several disadvantages,\nincluding reporting lags and antiquated technology, that have caused a movement\ntowards internet-based disease surveillance systems. Internet systems are\nparticularly attractive for disease outbreaks because they can provide data in\nnear real-time and can be verified by individuals around the globe. However,\nmost existing systems have focused on disease monitoring and do not provide a\ndata repository for policy makers or researchers. In order to fill this gap, we\nanalyzed Wikipedia article content.\n  We demonstrate how a named-entity recognizer can be trained to tag case\ncounts, death counts, and hospitalization counts in the article narrative that\nachieves an F1 score of 0.753. We also show, using the 2014 West African Ebola\nvirus disease epidemic article as a case study, that there are detailed time\nseries data that are consistently updated that closely align with ground truth\ndata.\n  We argue that Wikipedia can be used to create the first community-driven\nopen-source emerging disease detection, monitoring, and repository system.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 19:34:01 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 17:53:32 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 00:42:23 GMT"}, {"version": "v4", "created": "Mon, 24 Aug 2015 22:14:55 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Fairchild", "Geoffrey", "", "1 and 3"], ["De Silva", "Lalindra", ""], ["Del Valle", "Sara Y.", ""], ["Segre", "Alberto M.", ""]]}, {"id": "1504.00854", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Evaluation Evaluation a Monte Carlo study", "comments": "5 pages, 14 Equations, 2 Figures, 1 Table, as submitted to European\n  Conference on Artificial Intelligence (shorter version published with 2\n  pages, 4 Equations, 0 Figures, 1 Table)", "journal-ref": "ECAI 2008, pp.843-844", "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade there has been increasing concern about the biases\nembodied in traditional evaluation methods for Natural Language\nProcessing/Learning, particularly methods borrowed from Information Retrieval.\nWithout knowledge of the Bias and Prevalence of the contingency being tested,\nor equivalently the expectation due to chance, the simple conditional\nprobabilities Recall, Precision and Accuracy are not meaningful as evaluation\nmeasures, either individually or in combinations such as F-factor. The\nexistence of bias in NLP measures leads to the 'improvement' of systems by\nincreasing their bias, such as the practice of improving tagging and parsing\nscores by using most common value (e.g. water is always a Noun) rather than the\nattempting to discover the correct one. The measures Cohen Kappa and Powers\nInformedness are discussed as unbiased alternative to Recall and related to the\npsychologically significant measure DeltaP. In this paper we will analyze both\nbiased and unbiased measures theoretically, characterizing the precise\nrelationship between all these measures as well as evaluating the evaluation\nmeasures themselves empirically using a Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 14:46:29 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1504.00923", "submitter": "Fred Richardson", "authors": "Fred Richardson, Douglas Reynolds, Najim Dehak", "title": "A Unified Deep Neural Network for Speaker and Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned feature representations and sub-phoneme posteriors from Deep Neural\nNetworks (DNNs) have been used separately to produce significant performance\ngains for speaker and language recognition tasks. In this work we show how\nthese gains are possible using a single DNN for both speaker and language\nrecognition. The unified DNN approach is shown to yield substantial performance\nimprovements on the the 2013 Domain Adaptation Challenge speaker recognition\ntask (55% reduction in EER for the out-of-domain condition) and on the NIST\n2011 Language Recognition Evaluation (48% reduction in EER for the 30s test\ncondition).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:57:06 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Richardson", "Fred", ""], ["Reynolds", "Douglas", ""], ["Dehak", "Najim", ""]]}, {"id": "1504.01106", "submitter": "Lili Mou", "authors": "Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin", "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a tree-based convolutional neural network (TBCNN) for\ndiscriminative sentence modeling. Our models leverage either constituency trees\nor dependency trees of sentences. The tree-based convolution process extracts\nsentences' structural features, and these features are aggregated by max\npooling. Such architecture allows short propagation paths between the output\nlayer and underlying feature detectors, which enables effective structural\nfeature learning and extraction. We evaluate our models on two tasks: sentiment\nanalysis and question classification. In both experiments, TBCNN outperforms\nprevious state-of-the-art results, including existing neural networks and\ndedicated feature/rule engineering. We also make efforts to visualize the\ntree-based convolution process, shedding light on how our models work.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 10:18:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 07:30:08 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 17:16:32 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 12:23:16 GMT"}, {"version": "v5", "created": "Tue, 2 Jun 2015 05:56:06 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Mou", "Lili", ""], ["Peng", "Hao", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1504.01182", "submitter": "Nayan Jyoti Kalita", "authors": "Nayan Jyoti Kalita, Baharul Islam", "title": "Bengali to Assamese Statistical Machine Translation using Moses (Corpus\n  Based)", "comments": "6 pages, International Conference on Cognitive Computing and\n  Information Processing (CCIP-15), 3-4 March 2015, Noida (India)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine dialect interpretation assumes a real part in encouraging man-machine\ncorrespondence and in addition men-men correspondence in Natural Language\nProcessing (NLP). Machine Translation (MT) alludes to utilizing machine to\nchange one dialect to an alternate. Statistical Machine Translation is a type\nof MT consisting of Language Model (LM), Translation Model (TM) and decoder. In\nthis paper, Bengali to Assamese Statistical Machine Translation Model has been\ncreated by utilizing Moses. Other translation tools like IRSTLM for Language\nModel and GIZA-PP-V1.0.7 for Translation model are utilized within this\nframework which is accessible in Linux situations. The purpose of the LM is to\nencourage fluent output and the purpose of TM is to encourage similarity\nbetween input and output, the decoder increases the probability of translated\ntext in target language. A parallel corpus of 17100 sentences in Bengali and\nAssamese has been utilized for preparing within this framework. Measurable MT\nprocedures have not so far been generally investigated for Indian dialects. It\nmight be intriguing to discover to what degree these models can help the\nimmense continuous MT deliberations in the nation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 01:18:24 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Kalita", "Nayan Jyoti", ""], ["Islam", "Baharul", ""]]}, {"id": "1504.01255", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Semi-supervised Convolutional Neural Networks for Text Categorization\n  via Region Embedding", "comments": "v1 has a different title, and the results there are obsolete. The\n  current version is to appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new semi-supervised framework with convolutional neural\nnetworks (CNNs) for text categorization. Unlike the previous approaches that\nrely on word embeddings, our method learns embeddings of small text regions\nfrom unlabeled data for integration into a supervised CNN. The proposed scheme\nfor embedding learning is based on the idea of two-view semi-supervised\nlearning, which is intended to be useful for the task of interest even though\nthe training is done on unlabeled data. Our models achieve better results than\nprevious approaches on sentiment classification and topic classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 10:42:07 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 11:32:44 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2015 15:26:16 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1504.01383", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Vlad Niculae, Caroline Suen, Justine Zhang, Cristian\n  Danescu-Niculescu-Mizil, Jure Leskovec", "title": "QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting\n  Patterns", "comments": "To appear in the Proceedings of WWW 2015. 11pp, 10 fig. Interactive\n  visualization, data, and other info available at\n  http://snap.stanford.edu/quotus/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the extremely large pool of events and stories available, media outlets\nneed to focus on a subset of issues and aspects to convey to their audience.\nOutlets are often accused of exhibiting a systematic bias in this selection\nprocess, with different outlets portraying different versions of reality.\nHowever, in the absence of objective measures and empirical evidence, the\ndirection and extent of systematicity remains widely disputed.\n  In this paper we propose a framework based on quoting patterns for\nquantifying and characterizing the degree to which media outlets exhibit\nsystematic bias. We apply this framework to a massive dataset of news articles\nspanning the six years of Obama's presidency and all of his speeches, and\nreveal that a systematic pattern does indeed emerge from the outlet's quoting\nbehavior. Moreover, we show that this pattern can be successfully exploited in\nan unsupervised prediction setting, to determine which new quotes an outlet\nwill select to broadcast. By encoding bias patterns in a low-rank space we\nprovide an analysis of the structure of political media coverage. This reveals\na latent media bias space that aligns surprisingly well with political ideology\nand outlet type. A linguistic analysis exposes striking differences across\nthese latent dimensions, showing how the different types of media outlets\nportray different realities even when reporting on the same events. For\nexample, outlets mapped to the mainstream conservative side of the latent space\nfocus on quotes that portray a presidential persona disproportionately\ncharacterized by negativity.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 20:00:28 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Niculae", "Vlad", ""], ["Suen", "Caroline", ""], ["Zhang", "Justine", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Leskovec", "Jure", ""]]}, {"id": "1504.01427", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sunil Kopparapu, Saurabh Bhatnagar, K. Sahana, Sathyanarayana,\n  Akhilesh Srivastava, P.V.S. Rao", "title": "A Metric to Classify Style of Spoken Speech", "comments": "5 pages; OCOCOSDA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to classify spoken speech based on the style of speaking is an\nimportant problem. With the advent of BPO's in recent times, specifically those\nthat cater to a population other than the local population, it has become\nnecessary for BPO's to identify people with certain style of speaking\n(American, British etc). Today BPO's employ accent analysts to identify people\nhaving the required style of speaking. This process while involving human bias,\nit is becoming increasingly infeasible because of the high attrition rate in\nthe BPO industry. In this paper, we propose a new metric, which robustly and\naccurately helps classify spoken speech based on the style of speaking. The\nrole of the proposed metric is substantiated by using it to classify real\nspeech data collected from over seventy different people working in a BPO. We\ncompare the performance of the metric against human experts who independently\ncarried out the classification process. Experimental results show that the\nperformance of the system using the novel metric performs better than two\ndifferent human expert.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 22:00:12 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Kopparapu", "Sunil", ""], ["Bhatnagar", "Saurabh", ""], ["Sahana", "K.", ""], ["Sathyanarayana", "", ""], ["Srivastava", "Akhilesh", ""], ["Rao", "P. V. S.", ""]]}, {"id": "1504.01482", "submitter": "William Chan", "authors": "William Chan, Ian Lane", "title": "Deep Recurrent Neural Networks for Acoustic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep Recurrent Neural Network (RNN) model for acoustic\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\nand the final DNN takes the context and models the posterior probabilities of\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\neval92 task or more than 8% relative improvement over the baseline DNN models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:12:14 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01483", "submitter": "William Chan", "authors": "William Chan and Nan Rosemary Ke and Ian Lane", "title": "Transferring Knowledge from a RNN to a DNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\non embedded systems with limited computational capacity. Traditionally, the\napproach for embedded platforms is to either train a small DNN directly, or to\ntrain a small DNN that learns the output distribution of a large DNN. In this\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\ndivergence against the small DNN. The small DNN trained on the soft RNN\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\ncompared to a baseline 4.54 WER or more than 13% relative improvement.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:15:44 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Ke", "Nan Rosemary", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01496", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sunil Kumar Kopparapu", "title": "Voice based self help System: User Experience Vs Accuracy", "comments": "5 pages; 1 figure", "journal-ref": null, "doi": "10.1007/978-90-481-3658-2_18", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, self help systems are being increasingly deployed by service\nbased industries because they are capable of delivering better customer service\nand increasingly the switch is to voice based self help systems because they\nprovide a natural interface for a human to interact with a machine. A speech\nbased self help system ideally needs a speech recognition engine to convert\nspoken speech to text and in addition a language processing engine to take care\nof any misrecognitions by the speech recognition engine. Any off-the-shelf\nspeech recognition engine is generally a combination of acoustic processing and\nspeech grammar. While this is the norm, we believe that ideally a speech\nrecognition application should have in addition to a speech recognition engine\na separate language processing engine to give the system better performance. In\nthis paper, we discuss ways in which the speech recognition engine and the\nlanguage processing engine can be combined to give a better user experience.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 07:02:38 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1504.01683", "submitter": "Miao Fan", "authors": "Miao Fan, Kai Cao, Yifan He and Ralph Grishman", "title": "Jointly Embedding Relations and Mentions for Knowledge Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a joint embedding model for predicting relations\nbetween a pair of entities in the scenario of relation inference. It differs\nfrom most stand-alone approaches which separately operate on either knowledge\nbases or free texts. The proposed model simultaneously learns low-dimensional\nvector representations for both triplets in knowledge repositories and the\nmentions of relations in free texts, so that we can leverage the evidence both\nresources to make more accurate predictions. We use NELL to evaluate the\nperformance of our approach, compared with cutting-edge methods. Results of\nextensive experiments show that our model achieves significant improvement on\nrelation extraction.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 17:44:30 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 16:42:31 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2015 18:18:29 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 01:59:29 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Fan", "Miao", ""], ["Cao", "Kai", ""], ["He", "Yifan", ""], ["Grishman", "Ralph", ""]]}, {"id": "1504.01684", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou, Thomas Fang Zheng and Ralph Grishman", "title": "Large Margin Nearest Neighbor Embedding for Knowledge Representation", "comments": "arXiv admin note: text overlap with arXiv:1503.08155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional way of storing facts in triplets ({\\it head\\_entity, relation,\ntail\\_entity}), abbreviated as ({\\it h, r, t}), makes the knowledge intuitively\ndisplayed and easily acquired by mankind, but hardly computed or even reasoned\nby AI machines. Inspired by the success in applying {\\it Distributed\nRepresentations} to AI-related fields, recent studies expect to represent each\nentity and relation with a unique low-dimensional embedding, which is different\nfrom the symbolic and atomic framework of displaying knowledge in triplets. In\nthis way, the knowledge computing and reasoning can be essentially facilitated\nby means of a simple {\\it vector calculation}, i.e. ${\\bf h} + {\\bf r} \\approx\n{\\bf t}$. We thus contribute an effective model to learn better embeddings\nsatisfying the formula by pulling the positive tail entities ${\\bf t^{+}}$ to\nget together and close to {\\bf h} + {\\bf r} ({\\it Nearest Neighbor}), and\nsimultaneously pushing the negatives ${\\bf t^{-}}$ away from the positives\n${\\bf t^{+}}$ via keeping a {\\it Large Margin}. We also design a corresponding\nlearning algorithm to efficiently find the optimal solution based on {\\it\nStochastic Gradient Descent} in iterative fashion. Quantitative experiments\nillustrate that our approach can achieve the state-of-the-art performance,\ncompared with several latest methods on some benchmark datasets for two\nclassical applications, i.e. {\\it Link prediction} and {\\it Triplet\nclassification}. Moreover, we analyze the parameter complexities among all the\nevaluated models, and analytical results indicate that our model needs fewer\ncomputational resources on outperforming the other methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 17:50:31 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""], ["Grishman", "Ralph", ""]]}, {"id": "1504.02059", "submitter": "Hayat Alrefaie", "authors": "Hayat Alrefaie and Allan Ramsay", "title": "Supporting Language Learners with the Meanings Of Closed Class Items", "comments": "12 pages include references, 5 figures and AIAPP 2015 conference in\n  Geneva", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of language learning involves the mastery of countless tasks:\nmaking the constituent sounds of the language being learned, learning the\ngrammatical patterns, and acquiring the requisite vocabulary for reception and\nproduction. While a plethora of computational tools exist to facilitate the\nfirst and second of these tasks, a number of challenges arise with respect to\nenabling the third. This paper describes a tool that has been designed to\nsupport language learners with the challenge of understanding the use of\nclosed-class lexical items. The process of learning the Arabic for office is\n(mktb) is relatively simple and should be possible by means of simple\nrepetition of the word. However, it is much more difficult to learn and\ncorrectly use the Arabic equivalent of the word on. The current paper describes\na mechanism for the delivery of diagnostic information regarding specific\nlexical examples, with the aim of clearly demonstrating why a particular\ntranslation of a given closed-class item may be appropriate in certain\nsituations but not others, thereby helping learners to understand and use the\nterm correctly.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 18:12:07 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Alrefaie", "Hayat", ""], ["Ramsay", "Allan", ""]]}, {"id": "1504.02148", "submitter": "Chao-Lin Liu", "authors": "Peter K. Bol and Chao-Lin Liu and Hongsu Wang", "title": "Mining and discovering biographical information in Difangzhi with a\n  language-model-based approach", "comments": "6 pages, 4 figures, 1 table, 2015 International Conference on Digital\n  Humanities. in Proceedings of the 2015 International Conference on Digital\n  Humanities (DH 2015). July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results of expanding the contents of the China Biographical\nDatabase by text mining historical local gazetteers, difangzhi. The goal of the\ndatabase is to see how people are connected together, through kinship, social\nconnections, and the places and offices in which they served. The gazetteers\nare the single most important collection of names and offices covering the Song\nthrough Qing periods. Although we begin with local officials we shall\neventually include lists of local examination candidates, people from the\nlocality who served in government, and notable local figures with biographies.\nThe more data we collect the more connections emerge. The value of doing\nsystematic text mining work is that we can identify relevant connections that\nare either directly informative or can become useful without deep historical\nresearch. Academia Sinica is developing a name database for officials in the\ncentral governments of the Ming and Qing dynasties.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:38:35 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Bol", "Peter K.", ""], ["Liu", "Chao-Lin", ""], ["Wang", "Hongsu", ""]]}, {"id": "1504.02150", "submitter": "Chao-Lin Liu", "authors": "Wei-Jie Huang and Chao-Lin Liu", "title": "Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual\n  Entailment in NTCIR RITE Evaluation Tasks", "comments": "20 pages, 1 figure, 26 tables, Journal article in Soft Computing\n  (Spinger). Soft Computing, online. Springer, Germany, 2015", "journal-ref": null, "doi": "10.1007/s00500-015-1629-1", "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We computed linguistic information at the lexical, syntactic, and semantic\nlevels for Recognizing Inference in Text (RITE) tasks for both traditional and\nsimplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,\nnamed-entity recognition, and near synonym recognition were employed, and\nfeatures like counts of common words, statement lengths, negation words, and\nantonyms were considered to judge the entailment relationships of two\nstatements, while we explored both heuristics-based functions and\nmachine-learning approaches. The reported systems showed robustness by\nsimultaneously achieving second positions in the binary-classification subtasks\nfor both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted\nmore experiments with the test data of NTCIR-9 RITE, with good results. We also\nextended our work to search for better configurations of our classifiers and\ninvestigated contributions of individual features. This extended work showed\ninteresting results and should encourage further discussion.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:47:59 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Huang", "Wei-Jie", ""], ["Liu", "Chao-Lin", ""]]}, {"id": "1504.02162", "submitter": "Diego Amancio", "authors": "Diego R. Amancio, Filipi N. Silva and Luciano da F. Costa", "title": "Concentric network symmetry grasps authors' styles in word adjacency\n  networks", "comments": "Accepted for publication in Europhys. Lett. (EPL). The supplementary\n  information is available from\n  https://dl.dropboxusercontent.com/u/2740286/symmetry.pdf", "journal-ref": "Europhys. Lett. 110 68001 (2015)", "doi": "10.1209/0295-5075/110/68001", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several characteristics of written texts have been inferred from statistical\nanalysis derived from networked models. Even though many network measurements\nhave been adapted to study textual properties at several levels of complexity,\nsome textual aspects have been disregarded. In this paper, we study the\nsymmetry of word adjacency networks, a well-known representation of text as a\ngraph. A statistical analysis of the symmetry distribution performed in several\nnovels showed that most of the words do not display symmetric patterns of\nconnectivity. More specifically, the merged symmetry displayed a distribution\nsimilar to the ubiquitous power-law distribution. Our experiments also revealed\nthat the studied metrics do not correlate with other traditional network\nmeasurements, such as the degree or betweenness centrality. The effectiveness\nof the symmetry measurements was verified in the authorship attribution task.\nInterestingly, we found that specific authors prefer particular types of\nsymmetric motifs. As a consequence, the authorship of books could be accurately\nidentified in 82.5% of the cases, in a dataset comprising books written by 8\nauthors. Because the proposed measurements for text analysis are complementary\nto the traditional approach, they can be used to improve the characterization\nof text networks, which might be useful for related applications, such as those\nrelying on the identification of topical words and information retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 00:49:36 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 13:19:39 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Amancio", "Diego R.", ""], ["Silva", "Filipi N.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1504.02490", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Mari Ostendorf", "title": "Leveraging Twitter for Low-Resource Conversational Speech Language\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications involving conversational speech, data sparsity is a limiting\nfactor in building a better language model. We propose a simple,\nlanguage-independent method to quickly harvest large amounts of data from\nTwitter to supplement a smaller training set that is more closely matched to\nthe domain. The techniques lead to a significant reduction in perplexity on\nfour low-resource languages even though the presence on Twitter of these\nlanguages is relatively small. We also find that the Twitter text is more\nuseful for learning word classes than the in-domain text and that use of these\nword classes leads to further reductions in perplexity. Additionally, we\nintroduce a method of using social and textual information to prioritize the\ndownload queue during the Twitter crawling. This maximizes the amount of useful\ndata that can be collected, impacting both perplexity and vocabulary coverage.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 20:21:32 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Jaech", "Aaron", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1504.03068", "submitter": "Ahmad Kamal", "authors": "Ahmad Kamal", "title": "Review Mining for Feature Based Opinion Summarization and Visualization", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": "International Journal of Computer Applications, 119(17), 2015, pp.\n  6-13", "doi": "10.5120/21157-4183", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application and usage of opinion mining, especially for business\nintelligence, product recommendation, targeted marketing etc. have fascinated\nmany research attentions around the globe. Various research efforts attempted\nto mine opinions from customer reviews at different levels of granularity,\nincluding word-, sentence-, and document-level. However, development of a fully\nautomatic opinion mining and sentiment analysis system is still elusive. Though\nthe development of opinion mining and sentiment analysis systems are getting\nmomentum, most of them attempt to perform document-level sentiment analysis,\nclassifying a review document as positive, negative, or neutral. Such\ndocument-level opinion mining approaches fail to provide insight about users\nsentiment on individual features of a product or service. Therefore, it seems\nto be a great help for both customers and manufacturers, if the reviews could\nbe processed at a finer-grained level and presented in a summarized form\nthrough some visual means, highlighting individual features of a product and\nusers sentiment expressed over them. In this paper, the design of a unified\nopinion mining and sentiment analysis framework is presented at the\nintersection of both machine learning and natural language processing\napproaches. Also, design of a novel feature-level review summarization scheme\nis proposed to visualize mined features, opinions and their polarity values in\na comprehendible way.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 05:53:59 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 03:02:18 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Kamal", "Ahmad", ""]]}, {"id": "1504.03425", "submitter": "Iftekhar Naim", "authors": "Iftekhar Naim, M. Iftekhar Tanveer, Daniel Gildea, Mohammed (Ehsan)\n  Hoque", "title": "Automated Analysis and Prediction of Job Interview Performance", "comments": "14 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational framework for automatically quantifying verbal and\nnonverbal behaviors in the context of job interviews. The proposed framework is\ntrained by analyzing the videos of 138 interview sessions with 69\ninternship-seeking undergraduates at the Massachusetts Institute of Technology\n(MIT). Our automated analysis includes facial expressions (e.g., smiles, head\ngestures, facial tracking points), language (e.g., word counts, topic\nmodeling), and prosodic information (e.g., pitch, intonation, and pauses) of\nthe interviewees. The ground truth labels are derived by taking a weighted\naverage over the ratings of 9 independent judges. Our framework can\nautomatically predict the ratings for interview traits such as excitement,\nfriendliness, and engagement with correlation coefficients of 0.75 or higher,\nand can quantify the relative importance of prosody, language, and facial\nexpressions. By analyzing the relative feature weights learned by the\nregression models, our framework recommends to speak more fluently, use less\nfiller words, speak as \"we\" (vs. \"I\"), use more unique words, and smile more.\nWe also find that the students who were rated highly while answering the first\ninterview question were also rated highly overall (i.e., first impression\nmatters). Finally, our MIT Interview dataset will be made available to other\nresearchers to further validate and expand our findings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 05:49:26 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Naim", "Iftekhar", "", "Ehsan"], ["Tanveer", "M. Iftekhar", "", "Ehsan"], ["Gildea", "Daniel", "", "Ehsan"], ["Mohammed", "", "", "Ehsan"], ["Hoque", "", ""]]}, {"id": "1504.03608", "submitter": "Jan Macutek", "authors": "Michaela Koscov\\'a, J\\'an Macutek, Emmerich Kelih", "title": "A data-based classification of Slavic languages: Indices of qualitative\n  variation applied to grapheme frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ord's graph is a simple graphical method for displaying frequency\ndistributions of data or theoretical distributions in the two-dimensional\nplane. Its coordinates are proportions of the first three moments, either\nempirical or theoretical ones. A modification of the Ord's graph based on\nproportions of indices of qualitative variation is presented. Such a\nmodification makes the graph applicable also to data of categorical character.\nIn addition, the indices are normalized with values between 0 and 1, which\nenables comparing data files divided into different numbers of categories. Both\nthe original and the new graph are used to display grapheme frequencies in\neleven Slavic languages. As the original Ord's graph requires an assignment of\nnumbers to the categories, graphemes were ordered decreasingly according to\ntheir frequencies. Data were taken from parallel corpora, i.e., we work with\ngrapheme frequencies from a Russian novel and its translations to ten other\nSlavic languages. Then, cluster analysis is applied to the graph coordinates.\nWhile the original graph yields results which are not linguistically\ninterpretable, the modification reveals meaningful relations among the\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 16:23:30 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Koscov\u00e1", "Michaela", ""], ["Macutek", "J\u00e1n", ""], ["Kelih", "Emmerich", ""]]}, {"id": "1504.03659", "submitter": "Azad Dehghan Mr", "authors": "Azad Dehghan", "title": "Temporal ordering of clinical events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes a minimalistic set of methods engineered to anchor\nclinical events onto a temporal space. Specifically, we describe methods to\nextract clinical events (e.g., Problems, Treatments and Tests), temporal\nexpressions (i.e., time, date, duration, and frequency), and temporal links\n(e.g., Before, After, Overlap) between events and temporal entities. These\nmethods are developed and validated using high quality datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 18:48:58 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Dehghan", "Azad", ""]]}, {"id": "1504.04317", "submitter": "Robert Bridges", "authors": "Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall", "title": "Towards a relation extraction framework for cyber-security concepts", "comments": "4 pages in Cyber & Information Security Research Conference 2015, ACM", "journal-ref": null, "doi": "10.1145/2746266.2746277", "report-no": null, "categories": "cs.IR cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to assist security analysts in obtaining information pertaining to\ntheir network, such as novel vulnerabilities, exploits, or patches, information\nretrieval methods tailored to the security domain are needed. As labeled text\ndata is scarce and expensive, we follow developments in semi-supervised Natural\nLanguage Processing and implement a bootstrapping algorithm for extracting\nsecurity entities and their relationships from text. The algorithm requires\nlittle input data, specifically, a few relations or patterns (heuristics for\nidentifying relations), and incorporates an active learning component which\nqueries the user on the most important decisions to prevent drifting from the\ndesired relations. Preliminary testing on a small corpus shows promising\nresults, obtaining precision of .82.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 17:26:24 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Jones", "Corinne L.", ""], ["Bridges", "Robert A.", ""], ["Huffer", "Kelly", ""], ["Goodall", "John", ""]]}, {"id": "1504.04666", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Unsupervised Dependency Parsing: Let's Use Supervised Parsers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-training approach to unsupervised dependency parsing that\nreuses existing supervised and unsupervised parsing algorithms. Our approach,\ncalled `iterated reranking' (IR), starts with dependency trees generated by an\nunsupervised parser, and iteratively improves these trees using the richer\nprobability models used in supervised parsing that are in turn trained on these\ntrees. Our system achieves 1.8% accuracy higher than the state-of-the-part\nparser of Spitkovsky et al. (2013) on the WSJ corpus.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 00:23:16 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1504.04716", "submitter": "Vishal Shukla", "authors": "Vishal Shukla", "title": "Gap Analysis of Natural Language Processing Systems with respect to\n  Linguistic Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modality is one of the important components of grammar in linguistics. It\nlets speaker to express attitude towards, or give assessment or potentiality of\nstate of affairs. It implies different senses and thus has different\nperceptions as per the context. This paper presents an account showing the gap\nin the functionality of the current state of art Natural Language Processing\n(NLP) systems. The contextual nature of linguistic modality is studied. In this\npaper, the works and logical approaches employed by Natural Language Processing\nsystems dealing with modality are reviewed. It sees human cognition and\nintelligence as multi-layered approach that can be implemented by intelligent\nsystems for learning. Lastly, current flow of research going on within this\nfield is talked providing futurology.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 13:28:59 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Shukla", "Vishal", ""]]}, {"id": "1504.04751", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk and Meltem Turhan Y\\\"ondem", "title": "A Knowledge-poor Pronoun Resolution System for Turkish", "comments": "Appears in Proceedings of the 6th Discourse Anaphora and Anaphora\n  Resolution Colloquium (DAARC), 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pronoun resolution system which requires limited syntactic knowledge to\nidentify the antecedents of personal and reflexive pronouns in Turkish is\npresented. As in its counterparts for languages like English, Spanish and\nFrench, the core of the system is the constraints and preferences determined\nempirically. In the evaluation phase, it performed considerably better than the\nbaseline algorithm used for comparison. The system is significant for its being\nthe first fully specified knowledge-poor computational framework for pronoun\nresolution in Turkish where Turkish possesses different structural properties\nfrom the languages for which knowledge-poor systems had been developed.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 18:34:19 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""], ["Y\u00f6ndem", "Meltem Turhan", ""]]}, {"id": "1504.04770", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, C\\'edric Archambeau", "title": "Online Inference for Relation Extraction with a Reduced Feature Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to web-scale corpora is gradually bringing robust automatic knowledge\nbase creation and extension within reach. To exploit these large\nunannotated---and extremely difficult to annotate---corpora, unsupervised\nmachine learning methods are required. Probabilistic models of text have\nrecently found some success as such a tool, but scalability remains an obstacle\nin their application, with standard approaches relying on sampling schemes that\nare known to be difficult to scale. In this report, we therefore present an\nempirical assessment of the sublinear time sparse stochastic variational\ninference (SSVI) scheme applied to RelLDA. We demonstrate that online inference\nleads to relatively strong qualitative results but also identify some of its\npathologies---and those of the model---which will need to be overcome if SSVI\nis to be used for large-scale relation extraction.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 22:08:50 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "1504.04802", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka", "title": "Gradual Classical Logic for Attributed Objects - Extended in\n  Re-Presentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1404.6036", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding about things is conceptual. By stating that we reason about\nobjects, it is in fact not the objects but concepts referring to them that we\nmanipulate. Now, so long just as we acknowledge infinitely extending notions\nsuch as space, time, size, colour, etc, - in short, any reasonable quality -\ninto which an object is subjected, it becomes infeasible to affirm atomicity in\nthe concept referring to the object. However, formal/symbolic logics typically\npresume atomic entities upon which other expressions are built. Can we reflect\nour intuition about the concept onto formal/symbolic logics at all? I assure\nthat we can, but the usual perspective about the atomicity needs inspected. In\nthis work, I present gradual logic which materialises the observation that we\ncannot tell apart whether a so-regarded atomic entity is atomic or is just\natomic enough not to be considered non-atomic. The motivation is to capture\ncertain phenomena that naturally occur around concepts with attributes,\nincluding presupposition and contraries. I present logical particulars of the\nlogic, which is then mapped onto formal semantics. Two linguistically\ninteresting semantics will be considered. Decidability is shown.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 07:03:17 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Arisaka", "Ryuta", ""]]}, {"id": "1504.04884", "submitter": "Ramon Ferrer i Cancho", "authors": "R. Ferrer-i-Cancho, C. Bentz and C. Seguin", "title": "Compression and the origins of Zipf's law of abbreviation", "comments": "New results for optimal non-singular coding have been added; some\n  sections have been reorganized", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CL cs.SI math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages across the world exhibit Zipf's law of abbreviation, namely more\nfrequent words tend to be shorter. The generalized version of the law - an\ninverse relationship between the frequency of a unit and its magnitude - holds\nalso for the behaviours of other species and the genetic code. The apparent\nuniversality of this pattern in human language and its ubiquity in other\ndomains calls for a theoretical understanding of its origins. To this end, we\ngeneralize the information theoretic concept of mean code length as a mean\nenergetic cost function over the probability and the magnitude of the types of\nthe repertoire. We show that the minimization of that cost function and a\nnegative correlation between probability and the magnitude of types are\nintimately related.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 20:57:11 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2015 07:33:50 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 15:56:03 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Ferrer-i-Cancho", "R.", ""], ["Bentz", "C.", ""], ["Seguin", "C.", ""]]}, {"id": "1504.05070", "submitter": "Han Zhao", "authors": "Han Zhao, Zhengdong Lu, Pascal Poupart", "title": "Self-Adaptive Hierarchical Sentence Model", "comments": "8 pages, 7 figures, accepted as a full paper at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately model a sentence at varying stages (e.g.,\nword-phrase-sentence) plays a central role in natural language processing. As\nan effort towards this goal we propose a self-adaptive hierarchical sentence\nmodel (AdaSent). AdaSent effectively forms a hierarchy of representations from\nwords to phrases and then to sentences through recursive gated local\ncomposition of adjacent segments. We design a competitive mechanism (through\ngating networks) to allow the representations of the same sentence to be\nengaged in a particular learning task (e.g., classification), therefore\neffectively mitigating the gradient vanishing problem persistent in other\nrecursive models. Both qualitative and quantitative analysis shows that AdaSent\ncan automatically form and select the representations suitable for the task at\nhand during training, yielding superior classification performance over\ncompetitor models on 5 benchmark data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 14:26:41 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 17:12:56 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Zhao", "Han", ""], ["Lu", "Zhengdong", ""], ["Poupart", "Pascal", ""]]}, {"id": "1504.05319", "submitter": "Gabriela Ferraro", "authors": "Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, Nathan Schneider\n  and Timothy Baldwin", "title": "Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word:\n  The Impact of Word Representation on Sequence Labelling Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Word embeddings -- distributed word representations that can be learned from\nunlabelled data -- have been shown to have high utility in many natural\nlanguage processing applications. In this paper, we perform an extrinsic\nevaluation of five popular word embedding methods in the context of four\nsequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE\nidentification. A particular focus of the paper is analysing the effects of\ntask-based updating of word representations. We show that when using word\nembeddings as features, as few as several hundred training instances are\nsufficient to achieve competitive results, and that word embeddings lead to\nimprovements over OOV words and out of domain. Perhaps more surprisingly, our\nresults indicate there is little difference between the different word\nembedding methods, and that simple Brown clusters are often competitive with\nword embeddings across all tasks we consider.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 06:58:26 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:50:17 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Qu", "Lizhen", ""], ["Ferraro", "Gabriela", ""], ["Zhou", "Liyuan", ""], ["Hou", "Weiwei", ""], ["Schneider", "Nathan", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1504.05929", "submitter": "Bishan Yang", "authors": "Bishan Yang and Claire Cardie and Peter Frazier", "title": "A Hierarchical Distance-dependent Bayesian Model for Event Coreference\n  Resolution", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical distance-dependent Bayesian model for event\ncoreference resolution. While existing generative models for event coreference\nresolution are completely unsupervised, our model allows for the incorporation\nof pairwise distances between event mentions -- information that is widely used\nin supervised coreference models to guide the generative clustering processing\nfor better event clustering both within and across documents. We model the\ndistances between event mentions using a feature-rich learnable distance\nfunction and encode them as Bayesian priors for nonparametric clustering.\nExperiments on the ECB+ corpus show that our model outperforms state-of-the-art\nmethods for both within- and cross-document event coreference resolution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 19:13:49 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 05:41:59 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Yang", "Bishan", ""], ["Cardie", "Claire", ""], ["Frazier", "Peter", ""]]}, {"id": "1504.06063", "submitter": "Lin Ma", "authors": "Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li", "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence", "comments": "Accepted by ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose multimodal convolutional neural networks (m-CNNs)\nfor matching image and sentence. Our m-CNN provides an end-to-end framework\nwith convolutional architectures to exploit image representation, word\ncomposition, and the matching relations between the two modalities. More\nspecifically, it consists of one image CNN encoding the image content, and one\nmatching CNN learning the joint representation of image and sentence. The\nmatching CNN composes words to different semantic fragments and learns the\ninter-modal relations between image and the composed fragments at different\nlevels, thus fully exploit the matching relations between image and sentence.\nExperimental results on benchmark databases of bidirectional image and sentence\nretrieval demonstrate that the proposed m-CNNs can effectively capture the\ninformation necessary for image and sentence matching. Specifically, our\nproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and\nMicrosoft COCO databases achieve the state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 07:10:13 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 01:47:05 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 08:09:54 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2015 07:30:53 GMT"}, {"version": "v5", "created": "Sat, 29 Aug 2015 09:35:09 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ma", "Lin", ""], ["Lu", "Zhengdong", ""], ["Shang", "Lifeng", ""], ["Li", "Hang", ""]]}, {"id": "1504.06077", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne, Mathieu Andro, Roselyne Corbi\\`ere, Tien T. Phan", "title": "Open Data Platform for Knowledge Access in Plant Health Domain : VESPA\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important data are locked in ancient literature. It would be uneconomic to\nproduce these data again and today or to extract them without the help of text\nmining technologies. Vespa is a text mining project whose aim is to extract\ndata on pest and crops interactions, to model and predict attacks on crops, and\nto reduce the use of pesticides. A few attempts proposed an agricultural\ninformation access. Another originality of our work is to parse documents with\na dependency of the document architecture.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:27:29 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Turenne", "Nicolas", ""], ["Andro", "Mathieu", ""], ["Corbi\u00e8re", "Roselyne", ""], ["Phan", "Tien T.", ""]]}, {"id": "1504.06078", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne, Tien Phan", "title": "x.ent: R Package for Entities and Relations Extraction based on\n  Unsupervised Learning and Document Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction with accurate precision is still a challenge when\nprocessing full text databases. We propose an approach based on cooccurrence\nanalysis in each document for which we used document organization to improve\naccuracy of relation extraction. This approach is implemented in a R package\ncalled \\emph{x.ent}. Another facet of extraction relies on use of extracted\nrelation into a querying system for expert end-users. Two datasets had been\nused. One of them gets interest from specialists of epidemiology in plant\nhealth. For this dataset usage is dedicated to plant-disease exploration\nthrough agricultural information news. An open-data platform exploits exports\nfrom \\emph{x.ent} and is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:28:01 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Turenne", "Nicolas", ""], ["Phan", "Tien", ""]]}, {"id": "1504.06080", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "svcR: An R Package for Support Vector Clustering improved with Geometric\n  Hashing applied to Lexical Pattern Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new R package which takes a numerical matrix format as data\ninput, and computes clusters using a support vector clustering method (SVC). We\nhave implemented an original 2D-grid labeling approach to speed up cluster\nextraction. In this sense, SVC can be seen as an efficient cluster extraction\nif clusters are separable in a 2-D map. Secondly we showed that this SVC\napproach using a Jaccard-Radial base kernel can help to classify well enough a\nset of terms into ontological classes and help to define regular expression\nrules for information extraction in documents; our case study concerns a set of\nterms and documents about developmental and molecular biology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:29:11 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1504.06329", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and John Grothendieck", "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions", "comments": "10 pages, 8 tables; appeared in Proceedings of the Seventeenth\n  Conference on Computational Natural Language Learning, August 2013", "journal-ref": "In Proceedings of the Seventeenth Conference on Computational\n  Natural Language Learning, pages 10-19, Sofia, Bulgaria, August 2013.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the natural language processing (NLP) community, active learning has\nbeen widely investigated and applied in order to alleviate the annotation\nbottleneck faced by developers of new NLP systems and technologies. This paper\npresents the first theoretical analysis of stopping active learning based on\nstabilizing predictions (SP). The analysis has revealed three elements that are\ncentral to the success of the SP method: (1) bounds on Cohen's Kappa agreement\nbetween successively trained models impose bounds on differences in F-measure\nperformance of the models; (2) since the stop set does not have to be labeled,\nit can be made large in practice, helping to guarantee that the results\ntransfer to previously unseen streams of examples at test/application time; and\n(3) good (low variance) sample estimates of Kappa between successive models can\nbe obtained. Proofs of relationships between the level of Kappa agreement and\nthe difference in performance between consecutive models are presented.\nSpecifically, if the Kappa agreement between two models exceeds a threshold T\n(where $T>0$), then the difference in F-measure performance between those\nmodels is bounded above by $\\frac{4(1-T)}{T}$ in all cases. If precision of the\npositive conjunction of the models is assumed to be $p$, then the bound can be\ntightened to $\\frac{4(1-T)}{(p+1)T}$.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 20:07:01 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Bloodgood", "Michael", ""], ["Grothendieck", "John", ""]]}, {"id": "1504.06391", "submitter": "Eben Haber", "authors": "Eben M. Haber", "title": "On the Stability of Online Language Features: How Much Text do you Need\n  to know a Person?", "comments": "4 pages, 4 figures, not published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, numerous studies have inferred personality and other traits\nfrom people's online writing. While these studies are encouraging, more\ninformation is needed in order to use these techniques with confidence. How do\nlinguistic features vary across different online media, and how much text is\nrequired to have a representative sample for a person? In this paper, we\nexamine several large sets of online, user-generated text, drawn from Twitter,\nemail, blogs, and online discussion forums. We examine and compare\npopulation-wide results for the linguistic measure LIWC, and the inferred\ntraits of Big5 Personality and Basic Human Values. We also empirically measure\nthe stability of these traits across different sized samples for each\nindividual. Our results highlight the importance of tuning models to each\nonline medium, and include guidelines for the minimum amount of text required\nfor a representative result.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 04:45:55 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Haber", "Eben M.", ""]]}, {"id": "1504.06580", "submitter": "Cicero dos Santos", "authors": "Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou", "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "comments": "Accepted as a long paper in the 53rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification is an important semantic processing task for which\nstate-ofthe-art systems still rely on costly handcrafted features. In this work\nwe tackle the relation classification task using a convolutional neural network\nthat performs classification by ranking (CR-CNN). We propose a new pairwise\nranking loss function that makes it easy to reduce the impact of artificial\nclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,\nwhich is designed for the task of classifying the relationship between two\nnominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art\nfor this dataset and achieve a F1 of 84.1 without using any costly handcrafted\nfeatures. Additionally, our experimental results show that: (1) our approach is\nmore effective than CNN followed by a softmax classifier; (2) omitting the\nrepresentation of the artificial class Other improves both precision and\nrecall; and (3) using only word embeddings as input features is enough to\nachieve state-of-the-art results if we consider only the text between the two\ntarget nominals.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 17:50:33 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 13:58:05 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1504.06650", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan and Michael Collins", "title": "Learning Dictionaries for Named Entity Recognition using Minimal\n  Supervision", "comments": "In 14th Conference of the European Chapter of the Association for\n  Computational Linguistic, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach for automatic construction of dictionaries\nfor Named Entity Recognition (NER) using large amounts of unlabeled data and a\nfew seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower\ndimensional embeddings (representations) for candidate phrases and classify\nthese phrases using a small number of labeled examples. Our method achieves\n16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER\nrespectively. We also show that by adding candidate phrase embeddings as\nfeatures in a sequence tagger gives better performance compared to using word\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 21:43:55 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Collins", "Michael", ""]]}, {"id": "1504.06654", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Jeevan Shankar, Alexandre Passos and Andrew\n  McCallum", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in\n  Vector Space", "comments": "In Conference on Empirical Methods in Natural Language Processing,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is rising interest in vector-space word embeddings and their use in\nNLP, especially given recent methods for their fast estimation at very large\nscale. Nearly all this work, however, assumes a single vector per word type\nignoring polysemy and thus jeopardizing their usefulness for downstream tasks.\nWe present an extension to the Skip-gram model that efficiently learns multiple\nembeddings per word type. It differs from recent related work by jointly\nperforming word sense discrimination and embedding learning, by\nnon-parametrically estimating the number of senses per word type, and by its\nefficiency and scalability. We present new state-of-the-art results in the word\nsimilarity in context task and demonstrate its scalability by training with one\nmachine on a corpus of nearly 1 billion tokens in less than 6 hours.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 22:12:14 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Shankar", "Jeevan", ""], ["Passos", "Alexandre", ""], ["McCallum", "Andrew", ""]]}, {"id": "1504.06658", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan and Ming-Wei Chang", "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion:\n  New Dataset and Methods", "comments": "North American Chapter of the Association for Computational\n  Linguistics- Human Language Technologies, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of previous work in knowledge base (KB) completion has focused on the\nproblem of relation extraction. In this work, we focus on the task of inferring\nmissing entity type instances in a KB, a fundamental task for KB competition\nyet receives little attention. Due to the novelty of this task, we construct a\nlarge-scale dataset and design an automatic evaluation methodology. Our\nknowledge base completion method uses information within the existing KB and\nexternal information from Wikipedia. We show that individual methods trained\nwith a global objective that considers unobserved cells from both the entity\nand the type side gives consistently higher quality predictions compared to\nbaseline methods. We also perform manual evaluation on a small subset of the\ndata to verify the effectiveness of our knowledge base completion methods and\nthe correctness of our proposed automatic evaluation method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 22:32:40 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Chang", "Ming-Wei", ""]]}, {"id": "1504.06662", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Benjamin Roth and Andrew McCallum", "title": "Compositional Vector Space Models for Knowledge Base Completion", "comments": "The 53rd Annual Meeting of the Association for Computational\n  Linguistics and The 7th International Joint Conference of the Asian\n  Federation of Natural Language Processing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base (KB) completion adds new facts to a KB by making inferences\nfrom existing facts, for example by inferring with high likelihood\nnationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop\nrelational synonyms like this, or use as evidence a multi-hop relational path\ntreated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paper\npresents an approach that reasons about conjunctions of multi-hop relations\nnon-atomically, composing the implications of a path using a recursive neural\nnetwork (RNN) that takes as inputs vector embeddings of the binary relation in\nthe path. Not only does this allow us to generalize to paths unseen at training\ntime, but also, with a single high-capacity RNN, to predict new relation types\nnot seen when the compositional model was trained (zero-shot learning). We\nassemble a new dataset of over 52M relational triples, and show that our method\nimproves over a traditional classifier by 11%, and a method leveraging\npre-trained embeddings by 7%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 23:06:10 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 21:23:45 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Roth", "Benjamin", ""], ["McCallum", "Andrew", ""]]}, {"id": "1504.06665", "submitter": "Jonathan May", "authors": "Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May", "title": "Using Syntax-Based Machine Translation to Parse English into Abstract\n  Meaning Representation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parser for Abstract Meaning Representation (AMR). We treat\nEnglish-to-AMR conversion within the framework of string-to-tree, syntax-based\nmachine translation (SBMT). To make this work, we transform the AMR structure\ninto a form suitable for the mechanics of SBMT and useful for modeling. We\nintroduce an AMR-specific language model and add data and features drawn from\nsemantic resources. Our resulting AMR parser improves upon state-of-the-art\nresults by 7 Smatch points.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 23:24:10 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 16:36:13 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Pust", "Michael", ""], ["Hermjakob", "Ulf", ""], ["Knight", "Kevin", ""], ["Marcu", "Daniel", ""], ["May", "Jonathan", ""]]}, {"id": "1504.06692", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille", "title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence\n  Descriptions of Images", "comments": "ICCV 2015 camera ready version. We add much more novel visual\n  concepts in the NVC dataset and have released it, see\n  http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of learning novel visual concepts, and\ntheir interactions with other concepts, from a few images with sentence\ndescriptions. Using linguistic context and visual features, our method is able\nto efficiently hypothesize the semantic meaning of new words and add them to\nits word dictionary so that they can be used to describe images which contain\nthese novel concepts. Our method has an image captioning module based on m-RNN\nwith several improvements. In particular, we propose a transposed weight\nsharing scheme, which not only improves performance on image captioning, but\nalso makes the model more suitable for the novel concept learning task. We\npropose methods to prevent overfitting the new concepts. In addition, three\nnovel concept datasets are constructed for this new task. In the experiments,\nwe show that our method effectively learns novel visual concepts from a few\nexamples without disturbing the previously learned concepts. The project page\nis http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 06:45:35 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 02:36:05 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yuille", "Alan", ""]]}, {"id": "1504.06936", "submitter": "Alejandro Metke Jimenez", "authors": "Alejandro Metke-Jimenez, Sarvnaz Karimi", "title": "Concept Extraction to Identify Adverse Drug Reactions in Medical Forums:\n  A Comparison of Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is becoming an increasingly important source of information to\ncomplement traditional pharmacovigilance methods. In order to identify signals\nof potential adverse drug reactions, it is necessary to first identify medical\nconcepts in the social media text. Most of the existing studies use\ndictionary-based methods which are not evaluated independently from the overall\nsignal detection task.\n  We compare different approaches to automatically identify and normalise\nmedical concepts in consumer reviews in medical forums. Specifically, we\nimplement several dictionary-based methods popular in the relevant literature,\nas well as a method we suggest based on a state-of-the-art machine learning\nmethod for entity recognition. MetaMap, a popular biomedical concept extraction\ntool, is used as a baseline. Our evaluations were performed in a controlled\nsetting on a common corpus which is a collection of medical forum posts\nannotated with concepts and linked to controlled vocabularies such as MedDRA\nand SNOMED CT.\n  To our knowledge, our study is the first to systematically examine the effect\nof popular concept extraction methods in the area of signal detection for\nadverse reactions. We show that the choice of algorithm or controlled\nvocabulary has a significant impact on concept extraction, which will impact\nthe overall signal detection process. We also show that our proposed machine\nlearning approach significantly outperforms all the other methods in\nidentification of both adverse reactions and drugs, even when trained with a\nrelatively small set of annotated text.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 05:56:13 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Metke-Jimenez", "Alejandro", ""], ["Karimi", "Sarvnaz", ""]]}, {"id": "1504.07071", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Dennis Wegener, Siegfried Schomisch", "title": "Exploring semantically-related concepts from Wikipedia: the case of SeRE", "comments": "In Classification & visualization : interfaces to knowledge ;\n  proceedings of the International UDC Seminar 24 - 25 October 2013, The Hague,\n  The Netherlands, edited by Aida Slavic, Almila Akdag Salah, and Sylvie\n  Davies, 153-165. W\\\"urzburg: Ergon-Verl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our web application SeRE designed to explore\nsemantically related concepts. Wikipedia and DBpedia are rich data sources to\nextract related entities for a given topic, like in- and out-links, broader and\nnarrower terms, categorisation information etc. We use the Wikipedia full text\nbody to compute the semantic relatedness for extracted terms, which results in\na list of entities that are most relevant for a topic. For any given query, the\nuser interface of SeRE visualizes these related concepts, ordered by semantic\nrelatedness; with snippets from Wikipedia articles that explain the connection\nbetween those two entities. In a user study we examine how SeRE can be used to\nfind important entities and their relationships for a given topic and to answer\nthe question of how the classification system can be used for filtering.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:08:36 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hienert", "Daniel", ""], ["Wegener", "Dennis", ""], ["Schomisch", "Siegfried", ""]]}, {"id": "1504.07225", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran", "title": "Correlational Neural Networks", "comments": "27 pages. To Appear in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common Representation Learning (CRL), wherein different descriptions (or\nviews) of the data are embedded in a common subspace, is receiving a lot of\nattention recently. Two popular paradigms here are Canonical Correlation\nAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA\nbased approaches learn a joint representation by maximizing correlation of the\nviews when projected to the common subspace. AE based methods learn a common\nrepresentation by minimizing the error of reconstructing the two views. Each of\nthese approaches has its own advantages and disadvantages. For example, while\nCCA based approaches outperform AE based approaches for the task of transfer\nlearning, they are not as scalable as the latter. In this work we propose an AE\nbased approach called Correlational Neural Network (CorrNet), that explicitly\nmaximizes correlation among the views when projected to the common subspace.\nThrough a series of experiments, we demonstrate that the proposed CorrNet is\nbetter than the above mentioned approaches with respect to its ability to learn\ncorrelated common representations. Further, we employ CorrNet for several cross\nlanguage tasks and show that the representations learned using CorrNet perform\nbetter than the ones learned using other state of the art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:51:34 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 20:34:28 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 19:14:05 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Chandar", "Sarath", ""], ["Khapra", "Mitesh M.", ""], ["Larochelle", "Hugo", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1504.07295", "submitter": "Matt Taddy", "authors": "Matt Taddy", "title": "Document Classification by Inversion of Distributed Language\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many recent advances in the structure and measurement of\ndistributed language models: those that map from words to a vector-space that\nis rich in information about word choice and composition. This vector-space is\nthe distributed language representation. The goal of this note is to point out\nthat any distributed representation can be turned into a classifier through\ninversion via Bayes rule. The approach is simple and modular, in that it will\nwork with any language representation whose training can be formulated as\noptimizing a probability model. In our application to 2 million sentences from\nYelp reviews, we also find that it performs as well as or better than complex\npurpose-built algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 22:32:40 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 19:46:35 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 15:27:20 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Taddy", "Matt", ""]]}, {"id": "1504.07324", "submitter": "Piji Li", "authors": "Piji Li, Lidong Bing, Wai Lam, Hang Li and Yi Liao", "title": "Reader-Aware Multi-Document Summarization via Sparse Coding", "comments": "7 pages, 2 figures, accepted as a full paper at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new MDS paradigm called reader-aware multi-document\nsummarization (RA-MDS). Specifically, a set of reader comments associated with\nthe news reports are also collected. The generated summaries from the reports\nfor the event should be salient according to not only the reports but also the\nreader comments. To tackle this RA-MDS problem, we propose a\nsparse-coding-based method that is able to calculate the salience of the text\nunits by jointly considering news reports and reader comments. Another\nreader-aware characteristic of our framework is to improve linguistic quality\nvia entity rewriting. The rewriting consideration is jointly assessed together\nwith other summarization requirements under a unified optimization model. To\nsupport the generation of compressive summaries via optimization, we explore a\nfiner syntactic unit, namely, noun/verb phrase. In this work, we also generate\na data set for conducting RA-MDS. Extensive experiments on this data set and\nsome classical data sets demonstrate the effectiveness of our proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 01:34:33 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Li", "Piji", ""], ["Bing", "Lidong", ""], ["Lam", "Wai", ""], ["Li", "Hang", ""], ["Liao", "Yi", ""]]}, {"id": "1504.07395", "submitter": "Thanh-Le Ha", "authors": "Thanh-Le Ha, Jan Niehues, Alex Waibel", "title": "Lexical Translation Model Using a Deep Neural Network Architecture", "comments": null, "journal-ref": "Proceedings of the 11th International Workshop on Spoken Language\n  Translation (IWSLT 2014), page 223-229, Lake Tahoe - US, December 4th and\n  5th, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine the advantages of a model using global source\nsentence contexts, the Discriminative Word Lexicon, and neural networks. By\nusing deep neural networks instead of the linear maximum entropy model in the\nDiscriminative Word Lexicon models, we are able to leverage dependencies\nbetween different source words due to the non-linearity. Furthermore, the\nmodels for different target words can share parameters and therefore data\nsparsity problems are effectively reduced.\n  By using this approach in a state-of-the-art translation system, we can\nimprove the performance by up to 0.5 BLEU points for three different language\npairs on the TED translation task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 09:43:40 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Ha", "Thanh-Le", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1504.07459", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Adrien Guille and Julien Velcin", "title": "CommentWatcher: An Open Source Web-based platform for analyzing\n  discussions on web forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CommentWatcher, an open source tool aimed at analyzing discussions\non web forums. Constructed as a web platform, CommentWatcher features automatic\nmass fetching of user posts from forum on multiple sites, extracting topics,\nvisualizing the topics as an expression cloud and exploring their temporal\nevolution. The underlying social network of users is simultaneously constructed\nusing the citation relations between users and visualized as a graph structure.\nOur platform addresses the issues of the diversity and dynamics of structures\nof webpages hosting the forums by implementing a parser architecture that is\nindependent of the HTML structure of webpages. This allows easy on-the-fly\nadding of new websites. Two types of users are targeted: end users who seek to\nstudy the discussed topics and their temporal evolution, and researchers in\nneed of establishing a forum benchmark dataset and comparing the performances\nof analysis tools.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 13:18:00 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Guille", "Adrien", ""], ["Velcin", "Julien", ""]]}, {"id": "1504.07678", "submitter": "Hongzhao Huang", "authors": "Hongzhao Huang and Larry Heck and Heng Ji", "title": "Leveraging Deep Neural Networks and Knowledge Graphs for Entity\n  Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Disambiguation aims to link mentions of ambiguous entities to a\nknowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for\nthis task based on the assumption that information from the same semantic\ncontext tends to belong to the same topic. This paper presents a novel deep\nsemantic relatedness model (DSRM) based on deep neural networks (DNN) and\nsemantic knowledge graphs (KGs) to measure entity semantic relatedness for\ntopical coherence modeling. The DSRM is directly trained on large-scale KGs and\nit maps heterogeneous types of knowledge of an entity from KGs to numerical\nfeature vectors in a latent space such that the distance between two\nsemantically-related entities is minimized. Compared with the state-of-the-art\nrelatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains\n19.4% and 24.5% reductions in entity disambiguation errors on two publicly\navailable datasets respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 22:47:25 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Huang", "Hongzhao", ""], ["Heck", "Larry", ""], ["Ji", "Heng", ""]]}, {"id": "1504.07843", "submitter": "HyeJin Youn", "authors": "Hyejin Youn, Logan Sutton, Eric Smith, Cristopher Moore, Jon F.\n  Wilkins, Ian Maddieson, William Croft, Tanmoy Bhattacharya", "title": "On the universal structure of human lexical semantics", "comments": "Press embargo in place until publication", "journal-ref": "PNAS 113 7 1766-1771 (2016)", "doi": "10.1073/pnas.1520752113", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How universal is human conceptual structure? The way concepts are organized\nin the human brain may reflect distinct features of cultural, historical, and\nenvironmental background in addition to properties universal to human\ncognition. Semantics, or meaning expressed through language, provides direct\naccess to the underlying conceptual structure, but meaning is notoriously\ndifficult to measure, let alone parameterize. Here we provide an empirical\nmeasure of semantic proximity between concepts using cross-linguistic\ndictionaries. Across languages carefully selected from a phylogenetically and\ngeographically stratified sample of genera, translations of words reveal cases\nwhere a particular language uses a single polysemous word to express concepts\nrepresented by distinct words in another. We use the frequency of polysemies\nlinking two concepts as a measure of their semantic proximity, and represent\nthe pattern of such linkages by a weighted network. This network is highly\nuneven and fragmented: certain concepts are far more prone to polysemy than\nothers, and there emerge naturally interpretable clusters loosely connected to\neach other. Statistical analysis shows such structural properties are\nconsistent across different language groups, largely independent of geography,\nenvironment, and literacy. It is therefore possible to conclude the conceptual\nstructure connecting basic vocabulary studied is primarily due to universal\nfeatures of human cognition and language use.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:06:20 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Youn", "Hyejin", ""], ["Sutton", "Logan", ""], ["Smith", "Eric", ""], ["Moore", "Cristopher", ""], ["Wilkins", "Jon F.", ""], ["Maddieson", "Ian", ""], ["Croft", "William", ""], ["Bhattacharya", "Tanmoy", ""]]}, {"id": "1504.08050", "submitter": "Shuangyong Song", "authors": "Shuangyong Song and Yao Meng", "title": "Detecting Concept-level Emotion Cause in Microblogging", "comments": "2 pages, 2 figures, to appear on WWW 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead\nof the mere word-level models, to discover causes of microblogging users'\ndiversified emotions on specific hot event. A modified topic-supervised biterm\ntopic model is utilized in CECM to detect emotion topics' in event-related\ntweets, and then context-sensitive topical PageRank is utilized to detect\nmeaningful multiword expressions as emotion causes. Experimental results on a\ndataset from Sina Weibo, one of the largest microblogging websites in China,\nshow CECM can better detect emotion causes than baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 00:35:32 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Song", "Shuangyong", ""], ["Meng", "Yao", ""]]}, {"id": "1504.08102", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg", "title": "Detecting and ordering adjectival scalemates", "comments": "Paper presented at MAPLEX 2015, February 9-10, Yamagata, Japan\n  (http://lang.cs.tut.ac.jp/maplex2015/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a pattern-based method that can be used to infer\nadjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically,\nthe proposed method uses lexical patterns to automatically identify and order\npairs of scalemates, followed by a filtering phase in which unrelated pairs are\ndiscarded. For the filtering phase, several different similarity measures are\nimplemented and compared. The model presented in this paper is evaluated using\nthe current standard, along with a novel evaluation set, and shown to be at\nleast as good as the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 07:27:56 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["van Miltenburg", "Emiel", ""]]}, {"id": "1504.08183", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov and Igor Andreev", "title": "Texts in, meaning out: neural language models in semantic similarity\n  task for Russian", "comments": "Proceedings of the Dialog 2015 Conference. Moscow, Russia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Distributed vector representations for natural language vocabulary get a lot\nof attention in contemporary computational linguistics. This paper summarizes\nthe experience of applying neural network language models to the task of\ncalculating semantic similarity for Russian. The experiments were performed in\nthe course of Russian Semantic Similarity Evaluation track, where our models\ntook from the 2nd to the 5th position, depending on the task.\n  We introduce the tools and corpora used, comment on the nature of the shared\ntask and describe the achieved results. It was found out that Continuous\nSkip-gram and Continuous Bag-of-words models, previously successfully applied\nto English material, can be used for semantic modeling of Russian as well.\nMoreover, we show that texts in Russian National Corpus (RNC) provide an\nexcellent training material for such models, outperforming other, much larger\ncorpora. It is especially true for semantic relatedness tasks (although\nstacking models trained on larger corpora on top of RNC models improves\nperformance even more).\n  High-quality semantic vectors learned in such a way can be used in a variety\nof linguistic tasks and promise an exciting field for further study.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 12:03:10 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Andreev", "Igor", ""]]}, {"id": "1504.08342", "submitter": "Shay Cohen", "authors": "Shay B. Cohen and Daniel Gildea", "title": "Parsing Linear Context-Free Rewriting Systems with Fast Matrix\n  Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a matrix multiplication recognition algorithm for a subset of\nbinary linear context-free rewriting systems (LCFRS) with running time\n$O(n^{\\omega d})$ where $M(m) = O(m^{\\omega})$ is the running time for $m\n\\times m$ matrix multiplication and $d$ is the \"contact rank\" of the LCFRS --\nthe maximal number of combination and non-combination points that appear in the\ngrammar rules. We also show that this algorithm can be used as a subroutine to\nget a recognition algorithm for general binary LCFRS with running time\n$O(n^{\\omega d + 1})$. The currently best known $\\omega$ is smaller than\n$2.38$. Our result provides another proof for the best known result for parsing\nmildly context sensitive formalisms such as combinatory categorial grammars,\nhead grammars, linear indexed grammars, and tree adjoining grammars, which can\nbe parsed in time $O(n^{4.76})$. It also shows that inversion transduction\ngrammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRS\nsubsumes many other formalisms and types of grammars, for some of which we also\nimprove the asymptotic complexity of parsing.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 18:53:06 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 20:02:41 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 20:29:08 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Cohen", "Shay B.", ""], ["Gildea", "Daniel", ""]]}]