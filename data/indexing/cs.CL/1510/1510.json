[{"id": "1510.00001", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k", "title": "Polish to English Statistical Machine Translation", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.09097,\n  arXiv:1509.08909, arXiv:1509.08874", "journal-ref": "Polish to English Statistical Machine Translation., XV\n  International Phd Workshop OWD 2013, Wis{\\l}a, p.108-115, 2013", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research explores the effects of various training settings on a Polish\nto English Statistical Machine Translation system for spoken language. Various\nelements of the TED, Europarl, and OPUS parallel text corpora were used as the\nbasis for training of language models, for development, tuning and testing of\nthe translation system. The BLEU, NIST, METEOR and TER metrics were used to\nevaluate the effects of the data preparations on the translation results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:41:00 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""]]}, {"id": "1510.00240", "submitter": "Denis Gordeev", "authors": "Rodmonga Potapova and Denis Gordeev", "title": "Determination of the Internet Anonymity Influence on the Level of\n  Aggression and Usage of Obscene Lexis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the analysis of the semantic content of the anonymous\nRussian-speaking forum 2ch.hk, different verbal means of expressing of the\nemotional state of aggression are revealed for this site, and aggression is\nclassified by its directions. The lexis of different Russian-and English-\nspeaking anonymous forums (2ch.hk and iichan.hk, 4chan.org) and public\ncommunity \"MDK\" of the Russian-speaking social network VK is analyzed and\ncompared with the Open Corpus of the Russian language (Opencorpora.org and\nBrown corpus). The analysis shows that anonymity has no influence on the amount\nof invective items usage. The effectiveness of moderation was shown for\nanonymous forums. It was established that Russian obscene lexis was used to\nexpress the emotional state of aggression only in 60.4% of cases for 2ch.hk.\nThese preliminary results show that the Russian obscene lexis on the Internet\ndoes not have direct dependence on the emotional state of aggression.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 14:01:32 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Potapova", "Rodmonga", ""], ["Gordeev", "Denis", ""]]}, {"id": "1510.00244", "submitter": "Olivier Cur\\'e", "authors": "Fadhela Kerdjoudj and Olivier Cur\\'e", "title": "RDF Knowledge Graph Visualization From a Knowledge Extraction System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a system to visualize RDF knowledge graphs. These\ngraphs are obtained from a knowledge extraction system designed by\nGEOLSemantics. This extraction is performed using natural language processing\nand trigger detection. The user can visualize subgraphs by selecting some\nontology features like concepts or individuals. The system is also\nmultilingual, with the use of the annotated ontology in English, French, Arabic\nand Chinese.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 14:10:25 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Kerdjoudj", "Fadhela", ""], ["Cur\u00e9", "Olivier", ""]]}, {"id": "1510.00259", "submitter": "Stephanie L. Hyland", "authors": "Stephanie L. Hyland, Theofanis Karaletsos, Gunnar R\\\"atsch", "title": "A Generative Model of Words and Relationships from Multiple Sources", "comments": "8 pages, 5 figures; incorporated feedback from reviewers; to appear\n  in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models are a powerful tool to embed words into semantic\nvector spaces. However, learning such models generally relies on the\navailability of abundant and diverse training examples. In highly specialised\ndomains this requirement may not be met due to difficulties in obtaining a\nlarge corpus, or the limited range of expression in average use. Such domains\nmay encode prior knowledge about entities in a knowledge base or ontology. We\npropose a generative model which integrates evidence from diverse data sources,\nenabling the sharing of semantic information. We achieve this by generalising\nthe concept of co-occurrence from distributional semantics to include other\nrelationships between entities or words, which we model as affine\ntransformations on the embedding space. We demonstrate the effectiveness of\nthis approach by outperforming recent models on a link prediction task and\ndemonstrating its ability to profit from partially or fully unobserved data\ntraining labels. We further demonstrate the usefulness of learning from\ndifferent data sources with overlapping vocabularies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 14:42:19 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 17:08:28 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["Karaletsos", "Theofanis", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1510.00277", "submitter": "Martin Gerlach", "authors": "Martin Gerlach, Francesc Font-Clos, Eduardo G. Altmann", "title": "Similarity of symbol frequency distributions with heavy tails", "comments": "13 pages, 7 figures", "journal-ref": "Phys. Rev. X 6, 021009 (2016)", "doi": "10.1103/PhysRevX.6.021009", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the similarity between symbolic sequences is a traditional\nproblem in Information Theory which requires comparing the frequencies of\nsymbols in different sequences. In numerous modern applications, ranging from\nDNA over music to texts, the distribution of symbol frequencies is\ncharacterized by heavy-tailed distributions (e.g., Zipf's law). The large\nnumber of low-frequency symbols in these distributions poses major difficulties\nto the estimation of the similarity between sequences, e.g., they hinder an\naccurate finite-size estimation of entropies. Here we show analytically how the\nsystematic (bias) and statistical (fluctuations) errors in these estimations\ndepend on the sample size~$N$ and on the exponent~$\\gamma$ of the heavy-tailed\ndistribution. Our results are valid for the Shannon entropy $(\\alpha=1)$, its\ncorresponding similarity measures (e.g., the Jensen-Shanon divergence), and\nalso for measures based on the generalized entropy of order $\\alpha$. For small\n$\\alpha$'s, including $\\alpha=1$, the errors decay slower than the $1/N$-decay\nobserved in short-tailed distributions. For $\\alpha$ larger than a critical\nvalue $\\alpha^* = 1+1/\\gamma \\leq 2$, the $1/N$-decay is recovered. We show the\npractical significance of our results by quantifying the evolution of the\nEnglish language over the last two centuries using a complete $\\alpha$-spectrum\nof measures. We find that frequent words change more slowly than less frequent\nwords and that $\\alpha=2$ provides the most robust measure to quantify language\nchange.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 15:10:09 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 19:30:11 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Gerlach", "Martin", ""], ["Font-Clos", "Francesc", ""], ["Altmann", "Eduardo G.", ""]]}, {"id": "1510.00436", "submitter": "Richard Futrell", "authors": "Richard Futrell and Kyle Mahowald and Edward Gibson", "title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and\n  G\\'omez-Rodr\\'iguez (2015) on Dependency Length Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and\nG\\'omez-Rodr\\'iguez, 2015) of our work on empirical evidence of dependency\nlength minimization across languages (Futrell et al., 2015). First, we\nacknowledge error in failing to acknowledge Liu (2008)'s previous work on\ncorpora of 20 languages with similar aims. A correction will appear in PNAS.\nNevertheless, we argue that our work provides novel, strong evidence for\ndependency length minimization as a universal quantitative property of\nlanguages, beyond this previous work, because it provides baselines which focus\non word order preferences. Second, we argue that our choices of baselines were\nappropriate because they control for alternative theories.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 22:09:34 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Futrell", "Richard", ""], ["Mahowald", "Kyle", ""], ["Gibson", "Edward", ""]]}, {"id": "1510.00618", "submitter": "Daniel Gayo Avello", "authors": "Miguel Fernandez-Fernandez and Daniel Gayo-Avello", "title": "Automatic Taxonomy Extraction from Query Logs with no Additional Sources\n  of Information", "comments": "21 pages, 4 figures, 5 tables. Old (2012) unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine logs store detailed information on Web users interactions.\nThus, as more and more people use search engines on a daily basis, important\ntrails of users common knowledge are being recorded in those files. Previous\nresearch has shown that it is possible to extract concept taxonomies from full\ntext documents, while other scholars have proposed methods to obtain similar\nqueries from query logs. We propose a mixture of both lines of research, that\nis, mining query logs not to find related queries nor query hierarchies, but\nactual term taxonomies that could be used to improve search engine\neffectiveness and efficiency. As a result, in this study we have developed a\nmethod that combines lexical heuristics with a supervised classification model\nto successfully extract hyponymy relations from specialization search patterns\nrevealed from log missions, with no additional sources of information, and in a\nlanguage independent way.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 15:02:38 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 08:11:07 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Fernandez-Fernandez", "Miguel", ""], ["Gayo-Avello", "Daniel", ""]]}, {"id": "1510.00726", "submitter": "Yoav Goldberg", "authors": "Yoav Goldberg", "title": "A Primer on Neural Network Models for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, neural networks have re-emerged as powerful\nmachine-learning models, yielding state-of-the-art results in fields such as\nimage recognition and speech processing. More recently, neural network models\nstarted to be applied also to textual natural language signals, again with very\npromising results. This tutorial surveys neural network models from the\nperspective of natural language processing research, in an attempt to bring\nnatural-language researchers up to speed with the neural techniques. The\ntutorial covers input encoding for natural language tasks, feed-forward\nnetworks, convolutional networks, recurrent networks and recursive networks, as\nwell as the computation graph abstraction for automatic gradient computation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 20:17:33 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Goldberg", "Yoav", ""]]}, {"id": "1510.00759", "submitter": "Afshin Rahimi", "authors": "Afshin Rahimi, Moharram Eslami, Bahram Vazirnezhad", "title": "It is not all downhill from here: Syllable Contact Law in Persian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syllable contact pairs crosslinguistically tend to have a falling sonority\nslope a constraint which is called the Syllable Contact Law SCL In this study\nthe phonotactics of syllable contacts in 4202 CVCCVC words of Persian lexicon\nis investigated The consonants of Persian were divided into five sonority\ncategories and the frequency of all possible sonority slopes is computed both\nin lexicon type frequency and in corpus token frequency Since an unmarked\nphonological structure has been shown to diachronically become more frequent we\nexpect to see the same pattern for syllable contact pairs with falling sonority\nslope The correlation of sonority categories of the two consonants in a\nsyllable contact pair is measured using Pointwise Mutual Information\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 00:08:18 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Rahimi", "Afshin", ""], ["Eslami", "Moharram", ""], ["Vazirnezhad", "Bahram", ""]]}, {"id": "1510.00760", "submitter": "Afshin Rahimi", "authors": "Afshin Rahimi, Bahram Vazirnezhad, Moharram Eslami", "title": "P-trac Procedure: The Dispersion and Neutralization of Contrasts in\n  Lexicon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive acoustic cues have an important role in shaping the phonological\nstructure of language as a means to optimal communication. In this paper we\nintroduced P-trac procedure in order to track dispersion of contrasts in\ndifferent contexts in lexicon. The results of applying P-trac procedure to the\ncase of dispersion of contrasts in pre- consonantal contexts and in consonantal\npositions of CVCC sequences in Persian provide Evidence in favor of phonetic\nbasis of dispersion argued by Licensing by Cue hypothesis and the Dispersion\nTheory of Contrast. The P- trac procedure is proved to be very effective in\nrevealing the dispersion of contrasts in lexicon especially when comparing the\ndispersion of contrasts in different contexts.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 00:13:29 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Rahimi", "Afshin", ""], ["Vazirnezhad", "Bahram", ""], ["Eslami", "Moharram", ""]]}, {"id": "1510.01026", "submitter": "Gerardo Febres", "authors": "Gerardo Febres, Klaus Jaffe", "title": "Calculating entropy at different scales among diverse communication\n  systems", "comments": "27 pages, 6 Figures, 6 Tables", "journal-ref": "Complexity, Vol 21, Iss 51 (2016)", "doi": "10.1002/cplx.21746", "report-no": null, "categories": "cs.IT cs.CL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the impact of changing the observation scale over the entropy\nmeasures for text descriptions. MIDI coded Music, computer code and two human\nnatural languages were studied at the scale of characters, words, and at the\nFundamental Scale resulting from adjusting the symbols length used to interpret\neach text-description until it produced minimum entropy. The results show that\nthe Fundamental Scale method is comparable with the use of words when measuring\nentropy levels in written texts. However, this method can also be used in\ncommunication systems lacking words such as music. Measuring symbolic entropy\nat the fundamental scale allows to calculate quantitatively, relative levels of\ncomplexity for different communication systems. The results open novel vision\non differences among the structure of the communication systems studied.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 04:16:34 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Febres", "Gerardo", ""], ["Jaffe", "Klaus", ""]]}, {"id": "1510.01032", "submitter": "Herman Kamper", "authors": "Herman Kamper, Weiran Wang, and Karen Livescu", "title": "Deep convolutional acoustic word embeddings using word-pair side\n  information", "comments": "5 pages, 3 figures; added reference, acknowledgement and link to code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have been revisiting whole words as the basic modelling unit\nin speech recognition and query applications, instead of phonetic units. Such\nwhole-word segmental systems rely on a function that maps a variable-length\nspeech segment to a vector in a fixed-dimensional space; the resulting acoustic\nword embeddings need to allow for accurate discrimination between different\nword types, directly in the embedding space. We compare several old and new\napproaches in a word discrimination task. Our best approach uses side\ninformation in the form of known word pairs to train a Siamese convolutional\nneural network (CNN): a pair of tied networks that take two speech segments as\ninput and produce their embeddings, trained with a hinge loss that separates\nsame-word pairs and different-word pairs by some margin. A word classifier CNN\nperforms similarly, but requires much stronger supervision. Both types of CNNs\nyield large improvements over the best previously published results on the word\ndiscrimination task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 05:25:32 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 14:54:44 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Kamper", "Herman", ""], ["Wang", "Weiran", ""], ["Livescu", "Karen", ""]]}, {"id": "1510.01315", "submitter": "Weibing Deng", "authors": "Weibing Deng and Armen E. Allahverdyan", "title": "Stochastic model for phonemes uncovers an author-dependency of their\n  usage", "comments": "16 pages, 4 figures", "journal-ref": "PLoS ONE 11(4): e0152561 (2016)", "doi": "10.1371/journal.pone.0152561", "report-no": null, "categories": "cs.CL nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study rank-frequency relations for phonemes, the minimal units that still\nrelate to linguistic meaning. We show that these relations can be described by\nthe Dirichlet distribution, a direct analogue of the ideal-gas model in\nstatistical mechanics. This description allows us to demonstrate that the\nrank-frequency relations for phonemes of a text do depend on its author. The\nauthor-dependency effect is not caused by the author's vocabulary (common words\nused in different texts), and is confirmed by several alternative means. This\nsuggests that it can be directly related to phonemes. These features contrast\nto rank-frequency relations for words, which are both author and text\nindependent and are governed by the Zipf's law.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 09:43:25 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2016 07:34:54 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Deng", "Weibing", ""], ["Allahverdyan", "Armen E.", ""]]}, {"id": "1510.01431", "submitter": "Alexander Mathews", "authors": "Alexander Mathews, Lexing Xie, Xuming He", "title": "SentiCap: Generating Image Descriptions with Sentiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress on image recognition and language modeling is making\nautomatic description of image content a reality. However, stylized,\nnon-factual aspects of the written description are missing from the current\nsystems. One such style is descriptions with emotions, which is commonplace in\neveryday communication, and influences decision-making and interpersonal\nrelationships. We design a system to describe an image with emotions, and\npresent a model that automatically generates captions with positive or negative\nsentiments. We propose a novel switching recurrent neural network with\nword-level regularization, which is able to produce emotional image captions\nusing only 2000+ training sentences containing sentiments. We evaluate the\ncaptions with different automatic and crowd-sourcing metrics. Our model\ncompares favourably in common quality metrics for image captioning. In 84.6% of\ncases the generated positive captions were judged as being at least as\ndescriptive as the factual captions. Of these positive captions 88% were\nconfirmed by the crowd-sourced workers as having the appropriate sentiment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 04:57:47 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 23:03:23 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Mathews", "Alexander", ""], ["Xie", "Lexing", ""], ["He", "Xuming", ""]]}, {"id": "1510.01562", "submitter": "Benjamin Piwowarski", "authors": "Benjamin Piwowarski and Sylvain Lamprier and Nicolas Despres", "title": "Parameterized Neural Network Language Models for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval (IR) models need to deal with two difficult issues,\nvocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to\nthe difficulty of retrieving relevant documents that do not contain exact query\nterms but semantically related terms. Term dependencies refers to the need of\nconsidering the relationship between the words of the query when estimating the\nrelevance of a document. A multitude of solutions has been proposed to solve\neach of these two problems, but no principled model solve both. In parallel, in\nthe last few years, language models based on neural networks have been used to\ncope with complex natural language processing tasks like emotion and paraphrase\ndetection. Although they present good abilities to cope with both term\ndependencies and vocabulary mismatch problems, thanks to the distributed\nrepresentation of words they are based upon, such models could not be used\nreadily in IR, where the estimation of one language model per document (or\nquery) is required. This is both computationally unfeasible and prone to\nover-fitting. Based on a recent work that proposed to learn a generic language\nmodel that can be modified through a set of document-specific parameters, we\nexplore use of new neural network models that are adapted to ad-hoc IR tasks.\nWithin the language model IR framework, we propose and study the use of a\ngeneric language model as well as a document-specific language model. Both can\nbe used as a smoothing component, but the latter is more adapted to the\ndocument at hand and has the potential of being used as a full document\nlanguage model. We experiment with such models and analyze their results on\nTREC-1 to 8 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 13:07:31 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Piwowarski", "Benjamin", ""], ["Lamprier", "Sylvain", ""], ["Despres", "Nicolas", ""]]}, {"id": "1510.01570", "submitter": "David Alfter", "authors": "David Alfter", "title": "Analyzer and generator for Pali", "comments": "Bachelor Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work describes a system that performs morphological analysis and\ngeneration of Pali words. The system works with regular inflectional paradigms\nand a lexical database. The generator is used to build a collection of\ninflected and derived words, which in turn is used by the analyzer. Generating\nand storing morphological forms along with the corresponding morphological\ninformation allows for efficient and simple look up by the analyzer. Indeed, by\nlooking up a word and extracting the attached morphological information, the\nanalyzer does not have to compute this information. As we must, however, assume\nthe lexical database to be incomplete, the system can also work without the\ndictionary component, using a rule-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 13:33:46 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Alfter", "David", ""]]}, {"id": "1510.01717", "submitter": "David Alfter", "authors": "David Alfter", "title": "Language Segmentation", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Language segmentation consists in finding the boundaries where one language\nends and another language begins in a text written in more than one language.\nThis is important for all natural language processing tasks. The problem can be\nsolved by training language models on language data. However, in the case of\nlow- or no-resource languages, this is problematic. I therefore investigate\nwhether unsupervised methods perform better than supervised methods when it is\ndifficult or impossible to train supervised approaches. A special focus is\ngiven to difficult texts, i.e. texts that are rather short (one sentence),\ncontaining abbreviations, low-resource languages and non-standard language. I\ncompare three approaches: supervised n-gram language models, unsupervised\nclustering and weakly supervised n-gram language model induction. I devised the\nweakly supervised approach in order to deal with difficult text specifically.\nIn order to test the approach, I compiled a small corpus of different text\ntypes, ranging from one-sentence texts to texts of about 300 words. The weakly\nsupervised language model induction approach works well on short and difficult\ntexts, outperforming the clustering algorithm and reaching scores in the\nvicinity of the supervised approach. The results look promising, but there is\nroom for improvement and a more thorough investigation should be undertaken.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 19:35:23 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Alfter", "David", ""]]}, {"id": "1510.01886", "submitter": "Diego Moussallem", "authors": "Diego Moussallem and Ricardo Choren", "title": "Using Ontology-Based Context in the Portuguese-English Translation of\n  Homographs in Textual Dialogues", "comments": "17 pages, 7 figures, 2 tables in International journal of Artificial\n  Intelligence & Applications 2015", "journal-ref": null, "doi": "10.5121/ijaia.2015.6502", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to tackle the existing gap on message\ntranslations in dialogue systems. Currently, submitted messages to the dialogue\nsystems are considered as isolated sentences. Thus, missing context information\nimpede the disambiguation of homographs words in ambiguous sentences. Our\napproach solves this disambiguation problem by using concepts over existing\nontologies.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 10:50:31 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Moussallem", "Diego", ""], ["Choren", "Ricardo", ""]]}, {"id": "1510.01942", "submitter": "Manny Rayner", "authors": "Manny Rayner and Alejandro Armando and Pierrette Bouillon and Sarah\n  Ebling and Johanna Gerlach and Sonia Halimi and Irene Strasly and Nikos\n  Tsourakis", "title": "Helping Domain Experts Build Speech Translation Systems", "comments": "12 pages, 1 figure, to appear in Proc. Future and Emerging Trends in\n  Language Technology 2015, Seville, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new platform, \"Regulus Lite\", which supports rapid development\nand web deployment of several types of phrasal speech translation systems using\na minimal formalism. A distinguishing feature is that most development work can\nbe performed directly by domain experts. We motivate the need for platforms of\nthis type and discuss three specific cases: medical speech translation,\nspeech-to-sign-language translation and voice questionnaires. We briefly\ndescribe initial experiences in developing practical systems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 13:47:12 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Rayner", "Manny", ""], ["Armando", "Alejandro", ""], ["Bouillon", "Pierrette", ""], ["Ebling", "Sarah", ""], ["Gerlach", "Johanna", ""], ["Halimi", "Sonia", ""], ["Strasly", "Irene", ""], ["Tsourakis", "Nikos", ""]]}, {"id": "1510.01949", "submitter": "Martti Vainio", "authors": "Antti Suni and Daniel Aalto and Martti Vainio", "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prominences and boundaries are the essential constituents of prosodic\nstructure in speech. They provide for means to chunk the speech stream into\nlinguistically relevant units by providing them with relative saliences and\ndemarcating them within coherent utterance structures. Prominences and\nboundaries have both been widely used in both basic research on prosody as well\nas in text-to-speech synthesis. However, there are no representation schemes\nthat would provide for both estimating and modelling them in a unified fashion.\nHere we present an unsupervised unified account for estimating and representing\nprosodic prominences and boundaries using a scale-space analysis based on\ncontinuous wavelet transform. The methods are evaluated and compared to earlier\nwork using the Boston University Radio News corpus. The results show that the\nproposed method is comparable with the best published supervised annotation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 14:08:13 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Suni", "Antti", ""], ["Aalto", "Daniel", ""], ["Vainio", "Martti", ""]]}, {"id": "1510.02049", "submitter": "Spandana Gella", "authors": "Spandana Gella, Marc Dymetman, Jean Michel Renders, Sriram\n  Venkatapathy", "title": "Assisting Composition of Email Responses: a Topic Prediction Approach", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an approach for helping agents compose email replies to customer\nrequests. To enable that, we use LDA to extract latent topics from a collection\nof email exchanges. We then use these latent topics to label our data,\nobtaining a so-called \"silver standard\" topic labelling. We exploit this\nlabelled set to train a classifier to: (i) predict the topic distribution of\nthe entire agent's email response, based on features of the customer's email;\nand (ii) predict the topic distribution of the next sentence in the agent's\nreply, based on the customer's email features and on features of the agent's\ncurrent sentence. The experimental results on a large email collection from a\ncontact center in the tele- com domain show that the proposed ap- proach is\neffective in predicting the best topic of the agent's next sentence. In 80% of\nthe cases, the correct topic is present among the top five recommended topics\n(out of fifty possible ones). This shows the potential of this method to be\napplied in an interactive setting, where the agent is presented a small list of\nlikely topics to choose from for the next sentence.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 18:08:45 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Gella", "Spandana", ""], ["Dymetman", "Marc", ""], ["Renders", "Jean Michel", ""], ["Venkatapathy", "Sriram", ""]]}, {"id": "1510.02125", "submitter": "David Schlangen", "authors": "David Schlangen and Sina Zarriess and Casey Kennington", "title": "Resolving References to Objects in Photographs using the\n  Words-As-Classifiers Model", "comments": "11 pages; as in Proceedings of ACL 2016, Berlin, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common use of language is to refer to visually present objects. Modelling\nit in computers requires modelling the link between language and perception.\nThe \"words as classifiers\" model of grounded semantics views words as\nclassifiers of perceptual contexts, and composes the meaning of a phrase\nthrough composition of the denotations of its component words. It was recently\nshown to perform well in a game-playing scenario with a small number of object\ntypes. We apply it to two large sets of real-world photographs that contain a\nmuch larger variety of types and for which referring expressions are available.\nUsing a pre-trained convolutional neural network to extract image features, and\naugmenting these with in-picture positional information, we show that the model\nachieves performance competitive with the state of the art in a reference\nresolution task (given expression, find bounding box of its referent), while,\nas we argue, being conceptually simpler and more flexible.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 20:52:22 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 00:33:40 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 11:53:31 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Schlangen", "David", ""], ["Zarriess", "Sina", ""], ["Kennington", "Casey", ""]]}, {"id": "1510.02358", "submitter": "Javier Vera Z\\'u\\~niga", "authors": "Javier Vera, Pedro Montealegre, Eric Goles", "title": "Automata networks for multi-party communication in the Naming Game", "comments": "Conflict between recently published and arxiv versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Naming Game has been studied to explore the role of self-organization in\nthe development and negotiation of linguistic conventions. In this paper, we\ndefine an automata networks approach to the Naming Game. Two problems are\nfaced: (1) the definition of an automata networks for multi-party communicative\ninteractions; and (2) the proof of convergence for three different orders in\nwhich the individuals are updated (updating schemes). Finally, computer\nsimulations are explored in two-dimensional lattices with the purpose to\nrecover the main features of the Naming Game and to describe the dynamics under\ndifferent updating schemes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 15:15:09 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:27:33 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Vera", "Javier", ""], ["Montealegre", "Pedro", ""], ["Goles", "Eric", ""]]}, {"id": "1510.02387", "submitter": "Pranava Swaroop Madhyastha", "authors": "Pranava Swaroop Madhyastha, Mohit Bansal, Kevin Gimpel and Karen\n  Livescu", "title": "Mapping Unseen Words to Task-Trained Embedding Spaces", "comments": "8 + 3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the supervised training setting in which we learn task-specific\nword embeddings. We assume that we start with initial embeddings learned from\nunlabelled data and update them to learn task-specific embeddings for words in\nthe supervised training data. However, for new words in the test set, we must\nuse either their initial embeddings or a single unknown embedding, which often\nleads to errors. We address this by learning a neural network to map from\ninitial embeddings to the task-specific embedding space, via a multi-loss\nobjective function. The technique is general, but here we demonstrate its use\nfor improved dependency parsing (especially for sentences with\nout-of-vocabulary words), as well as for downstream improvements on sentiment\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 16:17:47 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 06:24:18 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Madhyastha", "Pranava Swaroop", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1510.02675", "submitter": "Benjamin Wilson", "authors": "Benjamin J. Wilson and Adriaan M. J. Schakel", "title": "Controlled Experiments for Word Embeddings", "comments": "Chagelog: Rerun experiment with subsampling turned off;\n  re-interpreted results in light of Schnabel et al. (2015). 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An experimental approach to studying the properties of word embeddings is\nproposed. Controlled experiments, achieved through modifications of the\ntraining corpus, permit the demonstration of direct relations between word\nproperties and word vector direction and length. The approach is demonstrated\nusing the word2vec CBOW model with experiments that independently vary word\nfrequency and word co-occurrence noise. The experiments reveal that word vector\nlength depends more or less linearly on both word frequency and the level of\nnoise in the co-occurrence distribution of the word. The coefficients of\nlinearity depend upon the word. The special point in feature space, defined by\nthe (artificial) word with pure noise in its co-occurrence distribution, is\nfound to be small but non-zero.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 14:03:33 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 05:10:29 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Wilson", "Benjamin J.", ""], ["Schakel", "Adriaan M. J.", ""]]}, {"id": "1510.02693", "submitter": "ShiLiang Zhang", "authors": "ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "comments": "4 pages, 1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new structure for memory neural networks, called feedforward\nsequential memory networks (FSMN), which can learn long-term dependency without\nusing recurrent feedback. The proposed FSMN is a standard feedforward neural\nnetworks equipped with learnable sequential memory blocks in the hidden layers.\nIn this work, we have applied FSMN to several language modeling (LM) tasks.\nExperimental results have shown that the memory blocks in FSMN can learn\neffective representations of long history. Experiments have shown that FSMN\nbased language models can significantly outperform not only feedforward neural\nnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)\nLMs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:04:11 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Zhang", "ShiLiang", ""], ["Jiang", "Hui", ""], ["Wei", "Si", ""], ["Dai", "LiRong", ""]]}, {"id": "1510.02755", "submitter": "Koushiki Sarkar", "authors": "Koushiki Sarkar, Ritwika Law", "title": "A Novel Approach to Document Classification using WordNet", "comments": "(Working Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content based Document Classification is one of the biggest challenges in the\ncontext of free text mining. Current algorithms on document classifications\nmostly rely on cluster analysis based on bag-of-words approach. However that\nmethod is still being applied to many modern scientific dilemmas. It has\nestablished a strong presence in fields like economics and social science to\nmerit serious attention from the researchers. In this paper we would like to\npropose and explore an alternative grounded more securely on the dictionary\nclassification and correlatedness of words and phrases. It is expected that\napplication of our existing knowledge about the underlying classification\nstructure may lead to improvement of the classifier's performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 09:24:27 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 17:56:52 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Sarkar", "Koushiki", ""], ["Law", "Ritwika", ""]]}, {"id": "1510.02823", "submitter": "Daniel Gildea", "authors": "Daniel Gildea and T. Florian Jaeger", "title": "Human languages order information efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most languages use the relative order between words to encode meaning\nrelations. Languages differ, however, in what orders they use and how these\norders are mapped onto different meanings. We test the hypothesis that, despite\nthese differences, human languages might constitute different `solutions' to\ncommon pressures of language use. Using Monte Carlo simulations over data from\nfive languages, we find that their word orders are efficient for processing in\nterms of both dependency length and local lexical probability. This suggests\nthat biases originating in how the brain understands language strongly\nconstrain how human languages change over generations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:05:02 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Gildea", "Daniel", ""], ["Jaeger", "T. Florian", ""]]}, {"id": "1510.02983", "submitter": "Becky Passonneau", "authors": "Boyi Xie and Rebecca J. Passonneau", "title": "OmniGraph: Rich Representation and Graph Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OmniGraph, a novel representation to support a range of NLP classification\ntasks, integrates lexical items, syntactic dependencies and frame semantic\nparses into graphs. Feature engineering is folded into the learning through\nconvolution graph kernel learning to explore different extents of the graph. A\nhigh-dimensional space of features includes individual nodes as well as complex\nsubgraphs. In experiments on a text-forecasting problem that predicts stock\nprice change from news for company mentions, OmniGraph beats several benchmarks\nbased on bag-of-words, syntactic dependencies, and semantic trees. The highly\nexpressive features OmniGraph discovers provide insights into the semantics\nacross distinct market sectors. To demonstrate the method's generality, we also\nreport its high performance results on a fine-grained sentiment corpus.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 21:22:00 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Xie", "Boyi", ""], ["Passonneau", "Rebecca J.", ""]]}, {"id": "1510.03021", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Guan-Tao Jin, Hongsu Wang, Qing-Feng Liu, Wen-Huei\n  Cheng, Wei-Yun Chiu, Richard Tzong-Han Tsai, Yu-Chun Wang", "title": "Textual Analysis for Studying Chinese Historical Documents and Literary\n  Novels", "comments": "11 pages, 7 figures, 2 tables, The Fourth ASE International\n  Conference on Social Informatics", "journal-ref": null, "doi": "10.1145/2818869.2818912", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyzed historical and literary documents in Chinese to gain insights\ninto research issues, and overview our studies which utilized four different\nsources of text materials in this paper. We investigated the history of\nconcepts and transliterated words in China with the Database for the Study of\nModern China Thought and Literature, which contains historical documents about\nChina between 1830 and 1930. We also attempted to disambiguate names that were\nshared by multiple government officers who served between 618 and 1912 and were\nrecorded in Chinese local gazetteers. To showcase the potentials and challenges\nof computer-assisted analysis of Chinese literatures, we explored some\ninteresting yet non-trivial questions about two of the Four Great Classical\nNovels of China: (1) Which monsters attempted to consume the Buddhist monk\nXuanzang in the Journey to the West (JTTW), which was published in the 16th\ncentury, (2) Which was the most powerful monster in JTTW, and (3) Which major\nrole smiled the most in the Dream of the Red Chamber, which was published in\nthe 18th century. Similar approaches can be applied to the analysis and study\nof modern documents, such as the newspaper articles published about the 228\nincident that occurred in 1947 in Taiwan.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 07:45:01 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Jin", "Guan-Tao", ""], ["Wang", "Hongsu", ""], ["Liu", "Qing-Feng", ""], ["Cheng", "Wen-Huei", ""], ["Chiu", "Wei-Yun", ""], ["Tsai", "Richard Tzong-Han", ""], ["Wang", "Yu-Chun", ""]]}, {"id": "1510.03055", "submitter": "Michel Galley", "authors": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "comments": "In. Proc of NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 14:04:57 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 06:59:19 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 22:03:28 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Li", "Jiwei", ""], ["Galley", "Michel", ""], ["Brockett", "Chris", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "1510.03421", "submitter": "Micha{\\l} {\\L}opuszy\\'nski", "authors": "Michal Jungiewicz, Micha{\\l} {\\L}opuszy\\'nski", "title": "Towards Meaningful Maps of Polish Case Law", "comments": null, "journal-ref": null, "doi": "10.3233/978-1-61499-609-5-185", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze the utility of two dimensional document maps for\nexploratory analysis of Polish case law. We start by comparing two methods of\ngenerating such visualizations. First is based on linear principal component\nanalysis (PCA). Second makes use of the modern nonlinear t-Distributed\nStochastic Neighbor Embedding method (t-SNE). We apply both PCA and t-SNE to a\ncorpus of judgments from different courts in Poland. It emerges that t-SNE\nprovides better, more interpretable results than PCA. As a next test, we apply\nt-SNE to randomly selected sample of common court judgments corresponding to\ndifferent keywords. We show that t-SNE, in this case, reveals hidden topical\nstructure of the documents related to keyword,,pension\". In conclusion, we find\nthat the t-SNE method could be a promising tool to facilitate the exploitative\nanalysis of legal texts, e.g., by complementing search or browse functionality\nin legal databases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 13:52:40 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 09:05:49 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Jungiewicz", "Michal", ""], ["\u0141opuszy\u0144ski", "Micha\u0142", ""]]}, {"id": "1510.03519", "submitter": "Janarthanan Rajendran", "authors": "Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman\n  Ravindran", "title": "Bridge Correlational Neural Networks for Multilingual Multimodal\n  Representation Learning", "comments": "Published at NAACL-HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a lot of interest in learning common representations\nfor multiple views of data. Typically, such common representations are learned\nusing a parallel corpus between the two views (say, 1M images and their English\ncaptions). In this work, we address a real-world scenario where no direct\nparallel data is available between two views of interest (say, $V_1$ and $V_2$)\nbut parallel data is available between each of these views and a pivot view\n($V_3$). We propose a model for learning a common representation for $V_1$,\n$V_2$ and $V_3$ using only the parallel data available between $V_1V_3$ and\n$V_2V_3$. The proposed model is generic and even works when there are $n$ views\nof interest and only one pivot view which acts as a bridge between them. There\nare two specific downstream applications that we focus on (i) transfer learning\nbetween languages $L_1$,$L_2$,...,$L_n$ using a pivot language $L$ and (ii)\ncross modal access between images and a language $L_1$ using a pivot language\n$L_2$. Our model achieves state-of-the-art performance in multilingual document\nclassification on the publicly available multilingual TED corpus and promising\nresults in multilingual multimodal retrieval on a new dataset created and\nreleased as a part of this work.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 03:25:18 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 07:44:01 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 09:01:19 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Rajendran", "Janarthanan", ""], ["Khapra", "Mitesh M.", ""], ["Chandar", "Sarath", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1510.03602", "submitter": "Brij Mohan Lal Srivastava", "authors": "Brij Mohan Lal Srivastava, Hari Krishna Vydana, Anil Kumar Vuppala,\n  Manish Shrivastava", "title": "A language model based approach towards large scale and lightweight\n  language identification systems", "comments": "Under review at ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual spoken dialogue systems have gained prominence in the recent\npast necessitating the requirement for a front-end Language Identification\n(LID) system. Most of the existing LID systems rely on modeling the language\ndiscriminative information from low-level acoustic features. Due to the\nvariabilities of speech (speaker and emotional variabilities, etc.),\nlarge-scale LID systems developed using low-level acoustic features suffer from\na degradation in the performance. In this approach, we have attempted to model\nthe higher level language discriminative phonotactic information for developing\nan LID system. In this paper, the input speech signal is tokenized to phone\nsequences by using a language independent phone recognizer. The language\ndiscriminative phonotactic information in the obtained phone sequences are\nmodeled using statistical and recurrent neural network based language modeling\napproaches. As this approach, relies on higher level phonotactical information\nit is more robust to variabilities of speech. Proposed approach is\ncomputationally light weight, highly scalable and it can be used in complement\nwith the existing LID systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 09:51:23 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Srivastava", "Brij Mohan Lal", ""], ["Vydana", "Hari Krishna", ""], ["Vuppala", "Anil Kumar", ""], ["Shrivastava", "Manish", ""]]}, {"id": "1510.03710", "submitter": "Miroslav Vodol\\'an", "authors": "Miroslav Vodol\\'an and Rudolf Kadlec and Jan Kleindienst", "title": "Hybrid Dialog State Tracker", "comments": "Accepted to Machine Learning for SLU & Interaction NIPS 2015\n  Workshop. Model description in Section 2.1 simplified compared to the\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hybrid dialog state tracker that combines a rule based\nand a machine learning based approach to belief state tracking. Therefore, we\ncall it a hybrid tracker. The machine learning in our tracker is realized by a\nLong Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker\nsets a new state-of-the-art result for the Dialog State Tracking Challenge\n(DSTC) 2 dataset when the system uses only live SLU as its input.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 14:44:01 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 08:38:14 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 10:40:31 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Vodol\u00e1n", "Miroslav", ""], ["Kadlec", "Rudolf", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1510.03753", "submitter": "Rudolf Kadlec", "authors": "Rudolf Kadlec, Martin Schmid, Jan Kleindienst", "title": "Improved Deep Learning Baselines for Ubuntu Corpus Dialogs", "comments": "Accepted to Machine Learning for SLU & Interaction NIPS 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of our experiments for the next utterance ranking\non the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog\ncorpus. First, we use an in-house implementation of previously reported models\nto do an independent evaluation using the same data. Second, we evaluate the\nperformances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we\ncreate an ensemble by averaging predictions of multiple models. The ensemble\nfurther improves the performance and it achieves a state-of-the-art result for\nthe next utterance ranking on this dataset. Finally, we discuss our future\nplans using this corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 15:56:26 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 08:23:50 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Kadlec", "Rudolf", ""], ["Schmid", "Martin", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1510.03797", "submitter": "Stefano Gurciullo", "authors": "Stefano Gurciullo, Michael Smallegan, Mar\\'ia Pereda, Federico\n  Battiston, Alice Patania, Sebastian Poledna, Daniel Hedblom, Bahattin Tolga\n  Oztan, Alexander Herzog, Peter John, Slava Mikhaylov", "title": "Complex Politics: A Quantitative Semantic and Topological Analysis of UK\n  House of Commons Debates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This study is a first, exploratory attempt to use quantitative semantics\ntechniques and topological analysis to analyze systemic patterns arising in a\ncomplex political system. In particular, we use a rich data set covering all\nspeeches and debates in the UK House of Commons between 1975 and 2014. By the\nuse of dynamic topic modeling (DTM) and topological data analysis (TDA) we show\nthat both members and parties feature specific roles within the system,\nconsistent over time, and extract global patterns indicating levels of\npolitical cohesion. Our results provide a wide array of novel hypotheses about\nthe complex dynamics of political systems, with valuable policy applications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 17:49:09 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Gurciullo", "Stefano", ""], ["Smallegan", "Michael", ""], ["Pereda", "Mar\u00eda", ""], ["Battiston", "Federico", ""], ["Patania", "Alice", ""], ["Poledna", "Sebastian", ""], ["Hedblom", "Daniel", ""], ["Oztan", "Bahattin Tolga", ""], ["Herzog", "Alexander", ""], ["John", "Peter", ""], ["Mikhaylov", "Slava", ""]]}, {"id": "1510.03820", "submitter": "Ye Zhang", "authors": "Ye Zhang and Byron Wallace", "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional\n  Neural Networks for Sentence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently achieved remarkably strong\nperformance on the practically important task of sentence classification (kim\n2014, kalchbrenner 2014, johnson 2014). However, these models require\npractitioners to specify an exact model architecture and set accompanying\nhyperparameters, including the filter region size, regularization parameters,\nand so on. It is currently unknown how sensitive model performance is to\nchanges in these configurations for the task of sentence classification. We\nthus conduct a sensitivity analysis of one-layer CNNs to explore the effect of\narchitecture components on model performance; our aim is to distinguish between\nimportant and comparatively inconsequential design decisions for sentence\nclassification. We focus on one-layer CNNs (to the exclusion of more complex\nmodels) due to their comparative simplicity and strong empirical performance,\nwhich makes it a modern standard baseline method akin to Support Vector Machine\n(SVMs) and logistic regression. We derive practical advice from our extensive\nempirical results for those interested in getting the most out of CNNs for\nsentence classification in real world settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 19:00:57 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 19:26:44 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2016 07:01:52 GMT"}, {"version": "v4", "created": "Wed, 6 Apr 2016 23:20:27 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Zhang", "Ye", ""], ["Wallace", "Byron", ""]]}, {"id": "1510.04104", "submitter": "Nicholas H. Kirk", "authors": "Simon Kaltenbacher, Nicholas H. Kirk, Dongheui Lee", "title": "A Preliminary Study on the Learning Informativeness of Data Subsets", "comments": "The 8th International Workshop on Human-Friendly Robotics (HFR 2015),\n  Munich, Germany", "journal-ref": null, "doi": "10.13140/RG.2.1.2213.9361", "report-no": null, "categories": "cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the internal state of a robotic system is complex: this is\nperformed from multiple heterogeneous sensor inputs and knowledge sources.\nDiscretization of such inputs is done to capture saliences, represented as\nsymbolic information, which often presents structure and recurrence. As these\nsequences are used to reason over complex scenarios, a more compact\nrepresentation would aid exactness of technical cognitive reasoning\ncapabilities, which are today constrained by computational complexity issues\nand fallback to representational heuristics or human intervention. Such\nproblems need to be addressed to ensure timely and meaningful human-robot\ninteraction. Our work is towards understanding the variability of learning\ninformativeness when training on subsets of a given input dataset. This is in\nview of reducing the training size while retaining the majority of the symbolic\nlearning potential. We prove the concept on human-written texts, and conjecture\nthis work will reduce training data size of sequential instructions, while\npreserving semantic relations, when gathering information from large remote\nsources.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 15:21:00 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Kaltenbacher", "Simon", ""], ["Kirk", "Nicholas H.", ""], ["Lee", "Dongheui", ""]]}, {"id": "1510.04500", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k", "title": "Noisy-parallel and comparable corpora filtering methodology for the\n  extraction of bi-lingual equivalent data at sentence level", "comments": "arXiv admin note: text overlap with arXiv:1509.09093,\n  arXiv:1509.08881", "journal-ref": "Computer Science 16.2 (2015): 169-184", "doi": "10.7494/csci.2015.16.2.169", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text alignment and text quality are critical to the accuracy of Machine\nTranslation (MT) systems, some NLP tools, and any other text processing tasks\nrequiring bilingual data. This research proposes a language independent\nbi-sentence filtering approach based on Polish (not a position-sensitive\nlanguage) to English experiments. This cleaning approach was developed on the\nTED Talks corpus and also initially tested on the Wikipedia comparable corpus,\nbut it can be used for any text domain or language pair. The proposed approach\nimplements various heuristics for sentence comparison. Some of them leverage\nsynonyms and semantic and structural analysis of text as additional\ninformation. Minimization of data loss was ensured. An improvement in MT system\nscore with text processed using the tool is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 12:29:14 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""]]}, {"id": "1510.04600", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek, Wojciech Glinkowski", "title": "Telemedicine as a special case of Machine Translation", "comments": null, "journal-ref": null, "doi": "10.1016/j.compmedimag.2015.09.005", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is evolving quite rapidly in terms of quality. Nowadays,\nwe have several machine translation systems available in the web, which provide\nreasonable translations. However, these systems are not perfect, and their\nquality may decrease in some specific domains. This paper examines the effects\nof different training methods when it comes to Polish - English Statistical\nMachine Translation system used for the medical data. Numerous elements of the\nEMEA parallel text corpora and not related OPUS Open Subtitles project were\nused as the ground for creation of phrase tables and different language models\nincluding the development, tuning and testing of these translation systems. The\nBLEU, NIST, METEOR, and TER metrics have been used in order to evaluate the\nresults of various systems. Our experiments deal with the systems that include\nPOS tagging, factored phrase models, hierarchical models, syntactic taggers,\nand other alignment methods. We also executed a deep analysis of Polish data as\npreparatory work before automatized data processing such as true casing or\npunctuation normalization phase. Normalized metrics was used to compare\nresults. Scores lower than 15% mean that Machine Translation engine is unable\nto provide satisfying quality, scores greater than 30% mean that translations\nshould be understandable without problems and scores over 50 reflect adequate\ntranslations. The average results of Polish to English translations scores for\nBLEU, NIST, METEOR, and TER were relatively high and ranged from 70,58 to\n82,72. The lowest score was 64,38. The average results ranges for English to\nPolish translations were little lower (67,58 - 78,97). The real-life\nimplementations of presented high quality Machine Translation Systems are\nanticipated in general medical practice and telemedicine.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:02:50 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""], ["Glinkowski", "Wojciech", ""]]}, {"id": "1510.04709", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Eva Hasler", "title": "Multilingual Image Description with Neural Sequence Models", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to multi-language image description\nbringing together insights from neural machine translation and neural image\ndescription. To create a description of an image for a given target language,\nour sequence generation models condition on feature vectors from the image, the\ndescription from the source language, and/or a multimodal vector computed over\nthe image and a description in the source language. In image description\nexperiments on the IAPR-TC12 dataset of images aligned with English and German\nsentences, we find significant and substantial improvements in BLEU4 and Meteor\nscores for models trained over multiple languages, compared to a monolingual\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 20:29:21 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:04:35 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Hasler", "Eva", ""]]}, {"id": "1510.04734", "submitter": "Michael Subotin", "authors": "Michael Subotin, Anthony R. Davis", "title": "A Method for Modeling Co-Occurrence Propensity of Clinical Codes with\n  Application to ICD-10-PCS Auto-Coding", "comments": "Submitted to Journal of the American Medical Informatics Association,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Natural language processing methods for medical auto-coding, or\nautomatic generation of medical billing codes from electronic health records,\ngenerally assign each code independently of the others. They may thus assign\ncodes for closely related procedures or diagnoses to the same document, even\nwhen they do not tend to occur together in practice, simply because the right\nchoice can be difficult to infer from the clinical narrative.\n  Materials and Methods. We propose a method that injects awareness of the\npropensities for code co-occurrence into this process. First, a model is\ntrained to estimate the conditional probability that one code is assigned by a\nhuman coder, given than another code is known to have been assigned to the same\ndocument. Then, at runtime, an iterative algorithm is used to apply this model\nto the output of an existing statistical auto-coder to modify the confidence\nscores of the codes.\n  Results. We tested this method in combination with a primary auto-coder for\nICD-10 procedure codes, achieving a 12% relative improvement in F-score over\nthe primary auto-coder baseline.\n  Discussion. The proposed method can be used, with appropriate features, in\ncombination with any auto-coder that generates codes with different levels of\nconfidence.\n  Conclusion. The promising results obtained for ICD-10 procedure codes suggest\nthat the proposed method may have wider applications in auto-coding.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 22:36:08 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Subotin", "Michael", ""], ["Davis", "Anthony R.", ""]]}, {"id": "1510.04780", "submitter": "Kan Ren", "authors": "Chenhao Zhu, Kan Ren, Xuan Liu, Haofen Wang, Yiding Tian and Yong Yu", "title": "A Graph Traversal Based Approach to Answer Non-Aggregation Questions\n  Over DBpedia", "comments": "In the proceedings of the 5th Joint International Semantic Technology\n  (JIST2015): https://link.springer.com/chapter/10.1007/978-3-319-31676-5_16", "journal-ref": null, "doi": "10.1007/978-3-319-31676-5_16", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a question answering system over DBpedia, filling the gap between\nuser information needs expressed in natural language and a structured query\ninterface expressed in SPARQL over the underlying knowledge base (KB). Given\nthe KB, our goal is to comprehend a natural language query and provide\ncorresponding accurate answers. Focusing on solving the non-aggregation\nquestions, in this paper, we construct a subgraph of the knowledge base from\nthe detected entities and propose a graph traversal method to solve both the\nsemantic item mapping problem and the disambiguation problem in a joint way.\nCompared with existing work, we simplify the process of query intention\nunderstanding and pay more attention to the answer path ranking. We evaluate\nour method on a non-aggregation question dataset and further on a complete\ndataset. Experimental results show that our method achieves best performance\ncompared with several state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 05:35:59 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 03:08:54 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zhu", "Chenhao", ""], ["Ren", "Kan", ""], ["Liu", "Xuan", ""], ["Wang", "Haofen", ""], ["Tian", "Yiding", ""], ["Yu", "Yong", ""]]}, {"id": "1510.04972", "submitter": "Weiyi Sun", "authors": "Weiyi Sun, Anna Rumshisky, Ozlem Uzuner", "title": "Normalization of Relative and Incomplete Temporal Expressions in\n  Clinical Narratives", "comments": "Draft version", "journal-ref": "Journal of the American Medical Informatics Association (2015)", "doi": "10.1093/jamia/ocu004", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the RI-TIMEXes in temporally annotated corpora and propose two\nhypotheses regarding the normalization of RI-TIMEXes in the clinical narrative\ndomain: the anchor point hypothesis and the anchor relation hypothesis. We\nannotate the RI-TIMEXes in three corpora to study the characteristics of\nRI-TMEXes in different domains. This informed the design of our RI-TIMEX\nnormalization system for the clinical domain, which consists of an anchor point\nclassifier, an anchor relation classifier and a rule-based RI-TIMEX text span\nparser. We experiment with different feature sets and perform error analysis\nfor each system component. The annotation confirmed the hypotheses that we can\nsimplify the RI-TIMEXes normalization task using two multi-label classifiers.\nOur system achieves anchor point classification, anchor relation classification\nand rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under\nrelaxed matching criteria) respectively on the held-out test set of the 2012\ni2b2 temporal relation challenge. Experiments with feature sets reveals some\ninteresting findings such as the verbal tense feature does not inform the\nanchor relation classification in clinical narratives as much as the tokens\nnear the RI-TIMEX. Error analysis shows that underrepresented anchor point and\nanchor relation classes are difficult to detect. We formulate the RI-TIMEX\nnormalization problem as a pair of multi-label classification problems.\nConsidering only the RI-TIMEX extraction and normalization, the system achieves\nstatistically significant improvement over the RI-TIMEX results of the best\nsystems in the 2012 i2b2 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 18:35:13 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Sun", "Weiyi", ""], ["Rumshisky", "Anna", ""], ["Uzuner", "Ozlem", ""]]}, {"id": "1510.05198", "submitter": "Jiwei Li", "authors": "Jiwei Li, Alan Ritter and Dan Jurafsky", "title": "Learning multi-faceted representations of individuals from heterogeneous\n  evidence using neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring latent attributes of people online is an important social computing\ntask, but requires integrating the many heterogeneous sources of information\navailable on the web. We propose learning individual representations of people\nusing neural nets to integrate rich linguistic and network evidence gathered\nfrom social media. The algorithm is able to combine diverse cues, such as the\ntext a person writes, their attributes (e.g. gender, employer, education,\nlocation) and social relations to other people. We show that by integrating\nboth textual and network evidence, these representations offer improved\nperformance at four important tasks in social media inference on Twitter:\npredicting (1) gender, (2) occupation, (3) location, and (4) friendships for\nusers. Our approach scales to large datasets and the learned representations\ncan be used as general features in and have the potential to benefit a large\nnumber of downstream tasks including link prediction, community detection, or\nprobabilistic reasoning over social networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 04:26:08 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 22:45:41 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 06:20:53 GMT"}, {"version": "v4", "created": "Thu, 11 May 2017 20:47:13 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Li", "Jiwei", ""], ["Ritter", "Alan", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1510.05203", "submitter": "Graham Neubig", "authors": "Graham Neubig, Makoto Morishita, Satoshi Nakamura", "title": "Neural Reranking Improves Subjective Quality of Machine Translation:\n  NAIST at WAT2015", "comments": "7 pages, 1 figure", "journal-ref": "Proceedings of the 2nd Workshop on Asian Translation (WAT), pp.\n  35-41, 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This year, the Nara Institute of Science and Technology (NAIST)'s submission\nto the 2015 Workshop on Asian Translation was based on syntax-based statistical\nmachine translation, with the addition of a reranking component using neural\nattentional machine translation models. Experiments re-confirmed results from\nprevious work stating that neural MT reranking provides a large gain in\nobjective evaluation measures such as BLEU, and also confirmed for the first\ntime that these results also carry over to manual evaluation. We further\nperform a detailed analysis of reasons for this increase, finding that the main\ncontributions of the neural models lie in improvement of the grammatical\ncorrectness of the output, as opposed to improvements in lexical choice of\ncontent words.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 06:26:42 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Neubig", "Graham", ""], ["Morishita", "Makoto", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1510.06168", "submitter": "Peilu Wang", "authors": "Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao", "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory\n  Recurrent Neural Network", "comments": "rejected by ACL 2015 short, score: 4,3,2 (full is 5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has\nbeen shown to be very effective for tagging sequential data, e.g. speech\nutterances or handwritten documents. While word embedding has been demoed as a\npowerful representation for characterizing the statistical properties of\nnatural language. In this study, we propose to use BLSTM-RNN with word\nembedding for part-of-speech (POS) tagging task. When tested on Penn Treebank\nWSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is\nachieved. Without using morphological features, this approach can also achieve\na good performance comparable with the Stanford POS tagger.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 08:57:30 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Wang", "Peilu", ""], ["Qian", "Yao", ""], ["Soong", "Frank K.", ""], ["He", "Lei", ""], ["Zhao", "Hai", ""]]}, {"id": "1510.06342", "submitter": "Matilde Marcolli", "authors": "Jeong Joon Park, Ronnel Boettcher, Andrew Zhao, Alex Mun, Kevin Yuh,\n  Vibhor Kumar, Matilde Marcolli", "title": "Prevalence and recoverability of syntactic parameters in sparse\n  distributed memories", "comments": "13 pages, LaTeX, 4 jpeg figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method, based on Sparse Distributed Memory (Kanerva\nNetworks), for studying dependency relations between different syntactic\nparameters in the Principles and Parameters model of Syntax. We store data of\nsyntactic parameters of world languages in a Kanerva Network and we check the\nrecoverability of corrupted parameter data from the network. We find that\ndifferent syntactic parameters have different degrees of recoverability. We\nidentify two different effects: an overall underlying relation between the\nprevalence of parameters across languages and their degree of recoverability,\nand a finer effect that makes some parameters more easily recoverable beyond\nwhat their prevalence would indicate. We interpret a higher recoverability for\na syntactic parameter as an indication of the existence of a dependency\nrelation, through which the given parameter can be determined using the\nremaining uncorrupted data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 17:11:13 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Park", "Jeong Joon", ""], ["Boettcher", "Ronnel", ""], ["Zhao", "Andrew", ""], ["Mun", "Alex", ""], ["Yuh", "Kevin", ""], ["Kumar", "Vibhor", ""], ["Marcolli", "Matilde", ""]]}, {"id": "1510.06549", "submitter": "Aaron Li", "authors": "Aaron Q Li", "title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an explosion of data, documents, and other content, and people\nrequire tools to analyze and interpret these, tools to turn the content into\ninformation and knowledge. Topic modeling have been developed to solve these\nproblems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns\nin data to be extracted automatically. When analyzing texts, these patterns are\ncalled topics. Among numerous extensions of LDA, few of them can reliably\nanalyze multiple groups of documents and extract topic similarities. Recently,\nthe introduction of differential topic modeling (SPDP) [Chen et. al. 2012]\nperforms uniformly better than many topic models in a discriminative setting.\n  There is also a need to improve the sampling speed for topic models. While\nsome effort has been made for distributed algorithms, there is no work\ncurrently done using graphical processing units (GPU). Note the GPU framework\nhas already become the most cost-efficient platform for many problems.\n  In this thesis, I propose and implement a scalable multi-GPU distributed\nparallel framework which approximates SPDP. Through experiments, I have shown\nmy algorithms have a gain in speed of about 50 times while being almost as\naccurate, with only one single cheap laptop GPU. Furthermore, I have shown the\nspeed improvement is sublinearly scalable when multiple GPUs are used, while\nfairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, the\nspeed improvement could potentially reach a factor of a thousand.\n  Note SPDP is just a representative of other extensions of LDA. Although my\nalgorithm is implemented to work with SPDP, it is designed to be a general\nenough to work with other topic models. The speed-up on smaller collections\n(i.e., 1000s of documents), means that these more complex LDA extensions could\nnow be done in real-time, thus opening up a new way of using these LDA models\nin industry.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 09:40:54 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Li", "Aaron Q", ""]]}, {"id": "1510.06646", "submitter": "Osama Khalifa", "authors": "Osama Khalifa, David Wolfe Corne, Mike Chantler", "title": "A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya\n  Parameters and Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-parameters play a major role in the learning and inference process of\nlatent Dirichlet allocation (LDA). In order to begin the LDA latent variables\nlearning process, these hyper-parameters values need to be pre-determined. We\npropose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs\nNewton' (LDA-GN), which places non-informative priors over these\nhyper-parameters and uses Gibbs sampling to learn appropriate values for them.\nAt the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new\ntechnique for learning the parameters of multivariate Polya distributions. We\nreport Gibbs-Newton performance results compared with two prominent existing\napproaches to the latter task: Minka's fixed-point iteration method and the\nMoments method. We then evaluate LDA-GN in two ways: (i) by comparing it with\nstandard LDA in terms of the ability of the resulting topic models to\ngeneralize to unseen documents; (ii) by comparing it with standard LDA in its\nperformance on a binary classification task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 14:39:58 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 12:29:43 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Khalifa", "Osama", ""], ["Corne", "David Wolfe", ""], ["Chantler", "Mike", ""]]}, {"id": "1510.06786", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Bryan Perozzi, Steven Skiena", "title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet\n  Language", "comments": "11 pages (updated submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new computational technique to detect and analyze statistically\nsignificant geographic variation in language. Our meta-analysis approach\ncaptures statistical properties of word usage across geographical regions and\nuses statistical methods to identify significant changes specific to regions.\nWhile previous approaches have primarily focused on lexical variation between\nregions, our method identifies words that demonstrate semantic and syntactic\nvariation as well.\n  We extend recently developed techniques for neural language models to learn\nword representations which capture differing semantics across geographical\nregions. In order to quantify this variation and ensure robust detection of\ntrue regional differences, we formulate a null model to determine whether\nobserved changes are statistically significant. Our method is the first such\napproach to explicitly account for random variation due to chance while\ndetecting regional variation in word meaning.\n  To validate our model, we study and analyze two different massive online data\nsets: millions of tweets from Twitter spanning not only four different\ncountries but also fifty states, as well as millions of phrases contained in\nthe Google Book Ngrams. Our analysis reveals interesting facets of language\nchange at multiple scales of geographic resolution -- from neighboring states\nto distant continents.\n  Finally, using our model, we propose a measure of semantic distance between\nlanguages. Our analysis of British and American English over a period of 100\nyears reveals that semantic variation between these dialects is shrinking.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 22:53:10 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 14:40:36 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1510.06807", "submitter": "Will Monroe", "authors": "Will Monroe, Christopher Potts", "title": "Learning in the Rational Speech Acts Model", "comments": "12 pages, 3 figures, 1 table. Preprint for Amsterdam Colloquium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rational Speech Acts (RSA) model treats language use as a recursive\nprocess in which probabilistic speaker and listener agents reason about each\nother's intentions to enrich the literal semantics of their language along\nbroadly Gricean lines. RSA has been shown to capture many kinds of\nconversational implicature, but it has been criticized as an unrealistic model\nof speakers, and it has so far required the manual specification of a semantic\nlexicon, preventing its use in natural language processing applications that\nlearn lexical knowledge from data. We address these concerns by showing how to\ndefine and optimize a trained statistical classifier that uses the intermediate\nagents of RSA as hidden layers of representation forming a non-linear\nactivation function. This treatment opens up new application domains and new\npossibilities for learning effectively from data. We validate the model on a\nreferential expression generation task, showing that the best performance is\nachieved by incorporating features approximating well-established insights\nabout natural language generation into RSA.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 02:24:23 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Monroe", "Will", ""], ["Potts", "Christopher", ""]]}, {"id": "1510.07035", "submitter": "Aaron Li", "authors": "Joseph W Robinson, Aaron Q Li", "title": "Fast Latent Variable Models for Inference and Visualization on Mobile\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we outline Vedalia, a high performance distributed network\nfor performing inference on latent variable models in the context of Amazon\nreview visualization. We introduce a new model, RLDA, which extends Latent\nDirichlet Allocation (LDA) [Blei et al., 2003] for the review space by\nincorporating auxiliary data available in online reviews to improve modeling\nwhile simultaneously remaining compatible with pre-existing fast sampling\ntechniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high\nperformance. The network is designed such that computation is efficiently\noffloaded to the client devices using the Chital system [Robinson & Li, 2015],\nimproving response times and reducing server costs. The resulting system is\nable to rapidly compute a large number of specialized latent variable models\nwhile requiring minimal server resources.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 05:26:09 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Robinson", "Joseph W", ""], ["Li", "Aaron Q", ""]]}, {"id": "1510.07099", "submitter": "Yushi Yao", "authors": "Yao Yushi and Huang Zheng", "title": "Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media", "comments": "5 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a joint algorithm for the word segmentation on\nChinese social media. Previous work mainly focus on word segmentation for plain\nChinese text, in order to develop a Chinese social media processing tool, we\nneed to take the main features of social media into account, whose grammatical\nstructure is not rigorous, and the tendency of using colloquial and Internet\nterms makes the existing Chinese-processing tools inefficient to obtain good\nperformance on social media.\n  In our approach, we combine CRF and MMSEG algorithm and extend features of\ntraditional CRF algorithm to train the model for word segmentation, We use\nInternet lexicon in order to improve the performance of our model on Chinese\nsocial media. Our experimental result on Sina Weibo shows that our approach\noutperforms the state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 02:24:54 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Yushi", "Yao", ""], ["Zheng", "Huang", ""]]}, {"id": "1510.07193", "submitter": "Kais Dukes", "authors": "Kais Dukes", "title": "Statistical Parsing by Machine Learning from a Classical Arabic Treebank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into statistical parsing for English has enjoyed over a decade of\nsuccessful results. However, adapting these models to other languages has met\nwith difficulties. Previous comparative work has shown that Modern Arabic is\none of the most difficult languages to parse due to rich morphology and free\nword order. Classical Arabic is the ancient form of Arabic, and is understudied\nin computational linguistics, relative to its worldwide reach as the language\nof the Quran. The thesis is based on seven publications that make significant\ncontributions to knowledge relating to annotating and parsing Classical Arabic.\n  A central argument of this thesis is that using a hybrid representation\nclosely aligned to traditional grammar leads to improved parsing for Arabic. To\ntest this hypothesis, two approaches are compared. As a reference, a pure\ndependency parser is adapted using graph transformations, resulting in an\n87.47% F1-score. This is compared to an integrated parsing model with an\nF1-score of 89.03%, demonstrating that joint dependency-constituency parsing is\nbetter suited to Classical Arabic.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 00:53:03 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Dukes", "Kais", ""]]}, {"id": "1510.07385", "submitter": "Jean-Valere Cossu", "authors": "Jean-Val\\`ere Cossu (LIA), Ludovic Bonnefoy (LIA), Xavier Bost (LIA),\n  Marc El B\\`eze (LIA)", "title": "How to merge three different methods for information filtering ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is now a gold marketing tool for entities concerned with online\nreputation. To automatically monitor online reputation of entities , systems\nhave to deal with ambiguous entity names, polarity detection and topic\ndetection. We propose three approaches to tackle the first issue: monitoring\nTwitter in order to find relevant tweets about a given entity. Evaluated within\nthe framework of the RepLab-2013 Filtering task, each of them has been shown\ncompetitive with state-of-the-art approaches. Mainly we investigate on how much\nmerging strategies may impact performances on a filtering task according to the\nevaluation measure.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 07:17:36 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Cossu", "Jean-Val\u00e8re", "", "LIA"], ["Bonnefoy", "Ludovic", "", "LIA"], ["Bost", "Xavier", "", "LIA"], ["B\u00e8ze", "Marc El", "", "LIA"]]}, {"id": "1510.07439", "submitter": "Abinash Tripathy", "authors": "Abinash Tripathy, Santanu Kumar Rath", "title": "Object Oriented Analysis using Natural Language Processing concepts: A\n  Review", "comments": "12 pages, International Journal of Information Processing, 9(3),\n  38-50, 2015, ISSN : 0973-8215, IK International Publishing House Pvt. Ltd.,\n  New Delhi, India", "journal-ref": "International Journal of Information Processing, vol. 9, no. 3,\n  pp. 38-50, 2015", "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Software Development Life Cycle (SDLC) starts with eliciting requirements\nof the customers in the form of Software Requirement Specification (SRS). SRS\ndocument needed for software development is mostly written in Natural\nLanguage(NL) convenient for the client. From the SRS document only, the class\nname, its attributes and the functions incorporated in the body of the class\nare traced based on pre-knowledge of analyst. The paper intends to present a\nreview on Object Oriented (OO) analysis using Natural Language Processing (NLP)\ntechniques. This analysis can be manual where domain expert helps to generate\nthe required diagram or automated system, where the system generates the\nrequired diagram, from the input in the form of SRS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 11:12:59 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Tripathy", "Abinash", ""], ["Rath", "Santanu Kumar", ""]]}, {"id": "1510.07482", "submitter": "Effi Levi", "authors": "Effi Levi, Roi Reichart and Ari Rappoport", "title": "Edge-Linear First-Order Dependency Parsing with Undirected Minimum\n  Spanning Tree Inference", "comments": "Accepted to ACL 2016, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The run time complexity of state-of-the-art inference algorithms in\ngraph-based dependency parsing is super-linear in the number of input words\n(n). Recently, pruning algorithms for these models have shown to cut a large\nportion of the graph edges, with minimal damage to the resulting parse trees.\nSolving the inference problem in run time complexity determined solely by the\nnumber of edges (m) is hence of obvious importance.\n  We propose such an inference algorithm for first-order models, which encodes\nthe problem as a minimum spanning tree (MST) problem in an undirected graph.\nThis allows us to utilize state-of-the-art undirected MST algorithms whose run\ntime is O(m) at expectation and with a very high probability. A directed parse\ntree is then inferred from the undirected MST and is subsequently improved with\nrespect to the directed parsing model through local greedy updates, both steps\nrunning in O(n) time. In experiments with 18 languages, a variant of the\nfirst-order MSTParser (McDonald et al., 2005b) that employs our algorithm\nperforms very similarly to the original parser that runs an O(n^2) directed MST\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 13:56:25 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 07:15:19 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 14:48:26 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 22:07:37 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Levi", "Effi", ""], ["Reichart", "Roi", ""], ["Rappoport", "Ari", ""]]}, {"id": "1510.07526", "submitter": "Yang Yu", "authors": "Yang Yu, Wei Zhang, Chung-Wei Hang, Bing Xiang and Bowen Zhou", "title": "Empirical Study on Deep Learning Models for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore deep learning models with memory component or\nattention mechanism for question answering task. We combine and compare three\nmodels, Neural Machine Translation, Neural Turing Machine, and Memory Networks\nfor a simulated QA data set. This paper is the first one that uses Neural\nMachine Translation and Neural Turing Machines for solving QA tasks. Our\nresults suggest that the combination of attention and memory have potential to\nsolve certain QA problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 16:03:27 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 16:56:48 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 15:36:56 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Yu", "Yang", ""], ["Zhang", "Wei", ""], ["Hang", "Chung-Wei", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1510.07586", "submitter": "Sudha Rao", "authors": "Sudha Rao, Yogarshi Vyas, Hal Daume III, Philip Resnik", "title": "Parser for Abstract Meaning Representation using Learning to Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel technique to parse English sentences into Abstract Meaning\nRepresentation (AMR) using SEARN, a Learning to Search approach, by modeling\nthe concept and the relation learning in a unified framework. We evaluate our\nparser on multiple datasets from varied domains and show an absolute\nimprovement of 2% to 6% over the state-of-the-art. Additionally we show that\nusing the most frequent concept gives us a baseline that is stronger than the\nstate-of-the-art for concept prediction. We plan to release our parser for\npublic use.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 18:34:16 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Rao", "Sudha", ""], ["Vyas", "Yogarshi", ""], ["Daume", "Hal", "III"], ["Resnik", "Philip", ""]]}, {"id": "1510.07851", "submitter": "Laurent Romary", "authors": "Laurent Romary (ALPAGE, CMB)", "title": "Standards for language resources in ISO -- Looking back at 13 fruitful\n  years", "comments": "edition - die Terminologiefachzeitschrift, Deutscher Terminologie-Tag\n  e.V. (DTT), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of the various projects carried out within\nISO committee TC 37/SC 4 dealing with the management of language (digital)\nresources. On the basis of the technical experience gained in the committee and\nthe wider standardization landscape the paper identifies some possible trends\nfor the future.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 10:43:50 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Romary", "Laurent", "", "ALPAGE, CMB"]]}, {"id": "1510.08418", "submitter": "Enrique Alfonseca", "authors": "Katja Filippova and Enrique Alfonseca", "title": "Fast k-best Sentence Compression", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to sentence compression is to formulate the task as a\nconstrained optimization problem and solve it with integer linear programming\n(ILP) tools. Unfortunately, dependence on ILP may make the compressor\nprohibitively slow, and thus approximation techniques have been proposed which\nare often complex and offer a moderate gain in speed. As an alternative\nsolution, we introduce a novel compression algorithm which generates k-best\ncompressions relying on local deletion decisions. Our algorithm is two orders\nof magnitude faster than a recent ILP-based method while producing better\ncompressions. Moreover, an extensive evaluation demonstrates that the quality\nof compressions does not degrade much as we move from single best to top-five\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 19:04:30 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Filippova", "Katja", ""], ["Alfonseca", "Enrique", ""]]}, {"id": "1510.08480", "submitter": "Umashanthi Pavalanathan", "authors": "Umashanthi Pavalanathan and Jacob Eisenstein", "title": "Emoticons vs. Emojis on Twitter: A Causal Inference Approach", "comments": "In review at AAAI Spring Symposium 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online writing lacks the non-verbal cues present in face-to-face\ncommunication, which provide additional contextual information about the\nutterance, such as the speaker's intention or affective state. To fill this\nvoid, a number of orthographic features, such as emoticons, expressive\nlengthening, and non-standard punctuation, have become popular in social media\nservices including Twitter and Instagram. Recently, emojis have been introduced\nto social media, and are increasingly popular. This raises the question of\nwhether these predefined pictographic characters will come to replace earlier\northographic methods of paralinguistic communication. In this abstract, we\nattempt to shed light on this question, using a matching approach from causal\ninference to test whether the adoption of emojis causes individual users to\nemploy fewer emoticons in their text on Twitter.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 20:41:30 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Pavalanathan", "Umashanthi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1510.08983", "submitter": "Yu Zhang", "authors": "Yu Zhang and Guoguo Chen and Dong Yu and Kaisheng Yao and Sanjeev\n  Khudanpur and James Glass", "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:40:14 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 09:48:01 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chen", "Guoguo", ""], ["Yu", "Dong", ""], ["Yao", "Kaisheng", ""], ["Khudanpur", "Sanjeev", ""], ["Glass", "James", ""]]}, {"id": "1510.08985", "submitter": "Yu Zhang", "authors": "Yu Zhang, Ekapol Chuangsuwanich, James Glass, Dong Yu", "title": "Prediction-Adaptation-Correction Recurrent Neural Networks for\n  Low-Resource Language Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of prediction-adaptation-correction\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\nPAC-RNN is comprised of a pair of neural networks in which a {\\it correction}\nnetwork uses auxiliary information given by a {\\it prediction} network to help\nestimate the state probability. The information from the correction network is\nalso used by the prediction network in a recurrent loop. Our model outperforms\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\nMoreover, transfer learning from a language that is similar to the target\nlanguage can help improve performance further.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:42:03 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chuangsuwanich", "Ekapol", ""], ["Glass", "James", ""], ["Yu", "Dong", ""]]}, {"id": "1510.09079", "submitter": "Marco Guerini", "authors": "Lorenzo Gatti, Marco Guerini, Marco Turchi", "title": "SentiWords: Deriving a High Precision and High Coverage Lexicon for\n  Sentiment Analysis", "comments": "in Affective Computing, IEEE Transactions on (2015)", "journal-ref": null, "doi": "10.1109/TAFFC.2015.2476456", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving prior polarity lexica for sentiment analysis - where positive or\nnegative scores are associated with words out of context - is a challenging\ntask. Usually, a trade-off between precision and coverage is hard to find, and\nit depends on the methodology used to build the lexicon. Manually annotated\nlexica provide a high precision but lack in coverage, whereas automatic\nderivation from pre-existing knowledge guarantees high coverage at the cost of\na lower precision. Since the automatic derivation of prior polarities is less\ntime consuming than manual annotation, there has been a great bloom of these\napproaches, in particular based on the SentiWordNet resource. In this paper, we\ncompare the most frequently used techniques based on SentiWordNet with newer\nones and blend them in a learning framework (a so called 'ensemble method'). By\ntaking advantage of manually built prior polarity lexica, our ensemble method\nis better able to predict the prior value of unseen words and to outperform all\nthe other SentiWordNet approaches. Using this technique we have built\nSentiWords, a prior polarity lexicon of approximately 155,000 words, that has\nboth a high precision and a high coverage. We finally show that in sentiment\nanalysis tasks, using our lexicon allows us to outperform both the single\nmetrics derived from SentiWordNet and popular manually annotated sentiment\nlexica.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 13:19:47 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Gatti", "Lorenzo", ""], ["Guerini", "Marco", ""], ["Turchi", "Marco", ""]]}, {"id": "1510.09202", "submitter": "Hongyu Guo Ph.D", "authors": "Hongyu Guo", "title": "Generating Text with Deep Reinforcement Learning", "comments": "Accepted to the NIPS2015 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel schema for sequence to sequence learning with a Deep\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\nto enable the decoder to first tackle easier portions of the sequences, and\nthen turn to cope with difficult parts. Specifically, in each iteration, an\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\ninput sequence, automatically create features to represent the internal states\nof and formulate a list of potential actions for the DQN. Take rephrasing a\nnatural sentence as an example. This list can contain ranked potential words.\nNext, the DQN learns to make decision on which action (e.g., word) will be\nselected from the list to modify the current decoded sequence. The newly\nmodified output sequence is subsequently used as the input to the DQN for the\nnext decoding iteration. In each iteration, we also bias the reinforcement\nlearning's attention to explore sequence portions which are previously\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\ndecode ten thousands natural sentences. Our experiments indicate that, when\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\nmethod performed competitively well when decoding sentences from the training\nset, but significantly outperformed the baseline when decoding unseen\nsentences, in terms of BLEU score obtained.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 19:02:53 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Guo", "Hongyu", ""]]}]