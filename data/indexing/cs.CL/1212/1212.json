[{"id": "1212.0074", "submitter": "Kyumars Sheykh Esmaili", "authors": "Kyumars Sheykh Esmaili", "title": "Challenges in Kurdish Text Processing", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having a large number of speakers, the Kurdish language is among the\nless-resourced languages. In this work we highlight the challenges and problems\nin providing the required tools and techniques for processing texts written in\nKurdish. From a high-level perspective, the main challenges are: the inherent\ndiversity of the language, standardization and segmentation issues, and the\nlack of language resources.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 07:01:27 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Esmaili", "Kyumars Sheykh", ""]]}, {"id": "1212.0229", "submitter": "J. G. Wolff", "authors": "James Gerard Wolff", "title": "Simplification and integration in computing and cognition: the SP theory\n  and the multiple alignment concept", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this article is to describe potential benefits and\napplications of the SP theory, a unique attempt to simplify and integrate ideas\nacross artificial intelligence, mainstream computing and human cognition, with\ninformation compression as a unifying theme. The theory, including a concept of\nmultiple alignment, combines conceptual simplicity with descriptive and\nexplanatory power in several areas including representation of knowledge,\nnatural language processing, pattern recognition, several kinds of reasoning,\nthe storage and retrieval of information, planning and problem solving,\nunsupervised learning, information compression, and human perception and\ncognition. In the SP machine -- an expression of the SP theory which is\ncurrently realised in the form of computer models -- there is potential for an\noverall simplification of computing systems, including software. As a theory\nwith a broad base of support, the SP theory promises useful insights in many\nareas and the integration of structures and functions, both within a given area\nand amongst different areas. There are potential benefits in natural language\nprocessing (with potential for the understanding and translation of natural\nlanguages), the need for a versatile intelligence in autonomous robots,\ncomputer vision, intelligent databases, maintaining multiple versions of\ndocuments or web pages, software engineering, criminal investigations, the\nmanagement of big data and gaining benefits from it, the semantic web, medical\ndiagnosis, the detection of computer viruses, the economical transmission of\ndata, and data fusion. Further development of these ideas would be facilitated\nby the creation of a high-parallel, web-based, open-source version of the SP\nmachine, with a good user interface. This would provide a means for researchers\nto explore what can be done with the system and to refine it.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 18:06:57 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Wolff", "James Gerard", ""]]}, {"id": "1212.0927", "submitter": "Ke Wu", "authors": "Ke Wu, Philip Resnik", "title": "Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown\n  Automaton", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce efficient algorithms for finding the $k$ shortest paths of a\nweighted pushdown automaton (WPDA), a compact representation of a weighted set\nof strings with potential applications in parsing and machine translation. Both\nof our algorithms are derived from the same weighted deductive logic\ndescription of the execution of a WPDA using different search strategies.\nExperimental results show our Algorithm 2 adds very little overhead vs. the\nsingle shortest path algorithm, even with a large $k$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 03:50:46 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2012 05:39:15 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2013 16:35:34 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Wu", "Ke", ""], ["Resnik", "Philip", ""]]}, {"id": "1212.1192", "submitter": "Mikel Forcada Dr.", "authors": "Miquel Espl\\`a-Gomis, Felipe S\\'anchez-Mart\\'inez, Mikel L. Forcada", "title": "Using external sources of bilingual information for on-the-fly word\n  alignment", "comments": "4 figures, 3 tables, 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we present a new and simple language-independent method for\nword-alignment based on the use of external sources of bilingual information\nsuch as machine translation systems. We show that the few parameters of the\naligner can be trained on a very small corpus, which leads to results\ncomparable to those obtained by the state-of-the-art tool GIZA++ in terms of\nprecision. Regarding other metrics, such as alignment error rate or F-measure,\nthe parametric aligner, when trained on a very small gold-standard (450 pairs\nof sentences), provides results comparable to those produced by GIZA++ when\ntrained on an in-domain corpus of around 10,000 pairs of sentences.\nFurthermore, the results obtained indicate that the training is\ndomain-independent, which enables the use of the trained aligner 'on the fly'\non any new pair of sentences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 22:10:04 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 07:13:02 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Espl\u00e0-Gomis", "Miquel", ""], ["S\u00e1nchez-Mart\u00ednez", "Felipe", ""], ["Forcada", "Mikel L.", ""]]}, {"id": "1212.1362", "submitter": "Martin Gerlach", "authors": "Martin Gerlach and Eduardo G. Altmann", "title": "Stochastic model for the vocabulary growth in natural languages", "comments": "corrected typos and errors in reference list; 10 pages text, 15 pages\n  supplemental material; to appear in Physical Review X", "journal-ref": "Phys. Rev. X 3, 021006 (2013)", "doi": "10.1103/PhysRevX.3.021006", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic model for the number of different words in a given\ndatabase which incorporates the dependence on the database size and historical\nchanges. The main feature of our model is the existence of two different\nclasses of words: (i) a finite number of core-words which have higher frequency\nand do not affect the probability of a new word to be used; and (ii) the\nremaining virtually infinite number of noncore-words which have lower frequency\nand once used reduce the probability of a new word to be used in the future.\nOur model relies on a careful analysis of the google-ngram database of books\npublished in the last centuries and its main consequence is the generalization\nof Zipf's and Heaps' law to two scaling regimes. We confirm that these\ngeneralizations yield the best simple description of the data among generic\ndescriptive models and that the two free parameters depend only on the language\nbut not on the database. From the point of view of our model the main change on\nhistorical time scales is the composition of the specific words included in the\nfinite list of core-words, which we observe to decay exponentially in time with\na rate of approximately 30 words per year for English.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 16:06:21 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 11:58:02 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2013 13:13:53 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Gerlach", "Martin", ""], ["Altmann", "Eduardo G.", ""]]}, {"id": "1212.1478", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "The Clustering of Author's Texts of English Fiction in the Vector Space\n  of Semantic Fields", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering of text documents in the vector space of semantic fields and\nin the semantic space with orthogonal basis has been analysed. It is shown that\nusing the vector space model with the basis of semantic fields is effective in\nthe cluster analysis algorithms of author's texts in English fiction. The\nanalysis of the author's texts distribution in cluster structure showed the\npresence of the areas of semantic space that represent the author's ideolects\nof individual authors. SVD factorization of the semantic fields matrix makes it\npossible to reduce significantly the dimension of the semantic space in the\ncluster analysis of author's texts.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 21:28:19 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1212.1709", "submitter": "Matjaz Perc", "authors": "Matjaz Perc", "title": "Evolution of the most common English words and phrases over the\n  centuries", "comments": "6 two-column pages, 4 figures; accepted for publication in Journal of\n  the Royal Society Interface [tables available at\n  http://www.matjazperc.com/ngrams]", "journal-ref": "J. R. Soc. Interface 9 (2012) 3323-3328", "doi": "10.1098/rsif.2012.0491", "report-no": null, "categories": "physics.soc-ph cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By determining which were the most common English words and phrases since the\nbeginning of the 16th century, we obtain a unique large-scale view of the\nevolution of written text. We find that the most common words and phrases in\nany given year had a much shorter popularity lifespan in the 16th than they had\nin the 20th century. By measuring how their usage propagated across the years,\nwe show that for the past two centuries the process has been governed by linear\npreferential attachment. Along with the steady growth of the English lexicon,\nthis provides an empirical explanation for the ubiquity of the Zipf's law in\nlanguage statistics and confirms that writing, although undoubtedly an\nexpression of art and skill, is not immune to the same influences of\nself-organization that are known to regulate processes as diverse as the making\nof new friends and World Wide Web growth.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 20:57:11 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Perc", "Matjaz", ""]]}, {"id": "1212.1918", "submitter": "Juan Manuel Torres Moreno", "authors": "Juan-Manuel Torres-Moreno, Patricia Vel\\'azquez-Morales, Jean-Guy\n  Meunier", "title": "Condens\\'es de textes par des m\\'ethodes num\\'eriques", "comments": "Conf\\'erence JADT 2002, Saint-Malo/France. 12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Since information in electronic form is already a standard, and that the\nvariety and the quantity of information become increasingly large, the methods\nof summarizing or automatic condensation of texts is a critical phase of the\nanalysis of texts. This article describes CORTEX a system based on numerical\nmethods, which allows obtaining a condensation of a text, which is independent\nof the topic and of the length of the text. The structure of the system enables\nit to find the abstracts in French or Spanish in very short times.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 20:55:52 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Vel\u00e1zquez-Morales", "Patricia", ""], ["Meunier", "Jean-Guy", ""]]}, {"id": "1212.2006", "submitter": "Jiwei Li", "authors": "Jiwei Li and Sujian Li", "title": "A Novel Feature-based Bayesian Model for Query Focused Multi-document\n  Summarization", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both supervised learning methods and LDA based topic model have been\nsuccessfully applied in the field of query focused multi-document\nsummarization. In this paper, we propose a novel supervised approach that can\nincorporate rich sentence features into Bayesian topic models in a principled\nway, thus taking advantages of both topic model and feature based supervised\nlearning methods. Experiments on TAC2008 and TAC2009 demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 09:41:12 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 17:28:14 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Li", "Jiwei", ""], ["Li", "Sujian", ""]]}, {"id": "1212.2036", "submitter": "Jiwei Li", "authors": "Jiwei Li and Sujian Li", "title": "Query-focused Multi-document Summarization: Combining a Novel Topic\n  Model with Graph-based Semi-supervised Learning", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised learning has proven to be an effective approach\nfor query-focused multi-document summarization. The problem of previous\nsemi-supervised learning is that sentences are ranked without considering the\nhigher level information beyond sentence level. Researches on general\nsummarization illustrated that the addition of topic level can effectively\nimprove the summary quality. Inspired by previous researches, we propose a\ntwo-layer (i.e. sentence layer and topic layer) graph-based semi-supervised\nlearning approach. At the same time, we propose a novel topic model which makes\nfull use of the dependence between sentences and words. Experimental results on\nDUC and TAC data sets demonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 11:35:29 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 17:24:00 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 17:13:33 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Li", "Jiwei", ""], ["Li", "Sujian", ""]]}, {"id": "1212.2145", "submitter": "Shuanghong Yang", "authors": "Shuang-Hong Yang", "title": "A Scale-Space Theory for Text", "comments": "9 pages, 6 figures; Nature language processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale-space theory has been established primarily by the computer vision and\nsignal processing communities as a well-founded and promising framework for\nmulti-scale processing of signals (e.g., images). By embedding an original\nsignal into a family of gradually coarsen signals parameterized with a\ncontinuous scale parameter, it provides a formal framework to capture the\nstructure of a signal at different scales in a consistent way. In this paper,\nwe present a scale space theory for text by integrating semantic and spatial\nfilters, and demonstrate how natural language documents can be understood,\nprocessed and analyzed at multiple resolutions, and how this scale-space\nrepresentation can be used to facilitate a variety of NLP and text analysis\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 17:39:44 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Yang", "Shuang-Hong", ""]]}, {"id": "1212.2390", "submitter": "Eric  Werner", "authors": "Eric Werner", "title": "On the complexity of learning a language: An improvement of Block's\n  algorithm", "comments": "7 pages. Key Words: Language learning, rules of language, complexity,\n  learning algorithms, evolution of language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language learning is thought to be a highly complex process. One of the\nhurdles in learning a language is to learn the rules of syntax of the language.\nRules of syntax are often ordered in that before one rule can applied one must\napply another. It has been thought that to learn the order of n rules one must\ngo through all n! permutations. Thus to learn the order of 27 rules would\nrequire 27! steps or 1.08889x10^{28} steps. This number is much greater than\nthe number of seconds since the beginning of the universe! In an insightful\nanalysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the\nassumption of transitivity this vast number of learning steps reduces to a mere\n377 steps. We present a mathematical analysis of the complexity of Block's\nalgorithm. The algorithm has a complexity of order n^2 given n rules. In\naddition, we improve Block's results exponentially, by introducing an algorithm\nthat has complexity of order less than n log n.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 11:35:30 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Werner", "Eric", ""]]}, {"id": "1212.2453", "submitter": "David Azari", "authors": "David Azari, Eric J. Horvitz, Susan Dumais, Eric Brill", "title": "Web-Based Question Answering: A Decision-Making Perspective", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-11-19", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an investigation of the use of probabilistic models and\ncost-benefit analyses to guide resource-intensive procedures used by a\nWeb-based question answering system. We first provide an overview of research\non question-answering systems. Then, we present details on AskMSR, a prototype\nweb-based question answering system. We discuss Bayesian analyses of the\nquality of answers generated by the system and show how we can endow the system\nwith the ability to make decisions about the number of queries issued to a\nsearch engine, given the cost of queries and the expected value of query\nresults in refining an ultimate answer. Finally, we review the results of a set\nof experiments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:03:27 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Azari", "David", ""], ["Horvitz", "Eric J.", ""], ["Dumais", "Susan", ""], ["Brill", "Eric", ""]]}, {"id": "1212.2477", "submitter": "Shyong (Tony) K. Lam", "authors": "Shyong (Tony) K. Lam, David M Pennock, Dan Cosley, Steve Lawrence", "title": "1 Billion Pages = 1 Million Dollars? Mining the Web to Play \"Who Wants\n  to be a Millionaire?\"", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-337-345", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the redundancy and volume of information on the web to build a\ncomputerized player for the ABC TV game show 'Who Wants To Be A Millionaire?'\nThe player consists of a question-answering module and a decision-making\nmodule. The question-answering module utilizes question transformation\ntechniques, natural language parsing, multiple information retrieval\nalgorithms, and multiple search engines; results are combined in the spirit of\nensemble learning using an adaptive weighting scheme. Empirically, the system\ncorrectly answers about 75% of questions from the Millionaire CD-ROM, 3rd\nedition - general-interest trivia questions often about popular culture and\ncommon knowledge. The decision-making module chooses from allowable actions in\nthe game in order to maximize expected risk-adjusted winnings, where the\nestimated probability of answering correctly is a function of past performance\nand confidence in in correctly answering the current question. When given a six\nquestion head start (i.e., when starting from the $2,000 level), we find that\nthe system performs about as well on average as humans starting at the\nbeginning. Our system demonstrates the potential of simple but well-chosen\ntechniques for mining answers from unstructured information such as the web.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:15 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Shyong", "", "", "Tony"], ["Lam", "K.", ""], ["Pennock", "David M", ""], ["Cosley", "Dan", ""], ["Lawrence", "Steve", ""]]}, {"id": "1212.2616", "submitter": "Matjaz Perc", "authors": "Alexander M. Petersen, Joel N. Tenenbaum, Shlomo Havlin, H. Eugene\n  Stanley, Matjaz Perc", "title": "Languages cool as they expand: Allometric scaling and the decreasing\n  need for new words", "comments": "9 two-column pages, 7 figures; accepted for publication in Scientific\n  Reports", "journal-ref": "Sci. Rep. 2 (2012) 943", "doi": "10.1038/srep00943", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the occurrence frequencies of over 15 million words recorded in\nmillions of books published during the past two centuries in seven different\nlanguages. For all languages and chronological subsets of the data we confirm\nthat two scaling regimes characterize the word frequency distributions, with\nonly the more common words obeying the classic Zipf law. Using corpora of\nunprecedented size, we test the allometric scaling relation between the corpus\nsize and the vocabulary size of growing languages to demonstrate a decreasing\nmarginal need for new words, a feature that is likely related to the underlying\ncorrelations between words. We calculate the annual growth fluctuations of word\nuse which has a decreasing trend as the corpus size increases, indicating a\nslowdown in linguistic evolution following language expansion. This \"cooling\npattern\" forms the basis of a third statistical regularity, which unlike the\nZipf and the Heaps law, is dynamical in nature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 20:32:05 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Petersen", "Alexander M.", ""], ["Tenenbaum", "Joel N.", ""], ["Havlin", "Shlomo", ""], ["Stanley", "H. Eugene", ""], ["Perc", "Matjaz", ""]]}, {"id": "1212.2676", "submitter": "Aaron Gerow", "authors": "Aaron Gerow and Mark Keane", "title": "Mining the Web for the Voice of the Herd to Track Stock Market Bubbles", "comments": "Proceedings of the 22nd International Joint Conference on Artificial\n  Intelligence (IJCAI '11), Barcelona, Spain, 16-22 July, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR physics.soc-ph q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that power-law analyses of financial commentaries from newspaper\nweb-sites can be used to identify stock market bubbles, supplementing\ntraditional volatility analyses. Using a four-year corpus of 17,713 online,\nfinance-related articles (10M+ words) from the Financial Times, the New York\nTimes, and the BBC, we show that week-to-week changes in power-law\ndistributions reflect market movements of the Dow Jones Industrial Average\n(DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularities\nin language track the 2007 stock market bubble, showing emerging structure in\nthe language of commentators, as progressively greater agreement arose in their\npositive perceptions of the market. Furthermore, during the bubble period, a\nmarked divergence in positive language occurs as revealed by a Kullback-Leibler\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 23:47:56 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Gerow", "Aaron", ""], ["Keane", "Mark", ""]]}, {"id": "1212.3023", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution, Shahrul Azman Mohd Noah", "title": "Keyword Extraction for Identifying Social Actors", "comments": "7 pages, nothing, draft to ICOCSIM 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the social actor has become one of tasks in Artificial\nIntelligence, whereby extracting keyword from Web snippets depend on the use of\nweb is steadily gaining ground in this research. We develop therefore an\napproach based on overlap principle for utilizing a collection of features in\nweb snippets, where use of keyword will eliminate the un-relevant web pages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 00:34:23 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""], ["Noah", "Shahrul Azman Mohd", ""]]}, {"id": "1212.3138", "submitter": "Aaron Gerow", "authors": "Aaron Georw and Mark Keane", "title": "Identifying Metaphor Hierarchies in a Corpus Analysis of Finance\n  Articles", "comments": null, "journal-ref": "Proceedings of the 33rd Annual Meeting of the Cognitive Science\n  Society (CogSci '11), Boston, MA, USA, 20-23 July, 2011", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a corpus of over 17,000 financial news reports (involving over 10M\nwords), we perform an analysis of the argument-distributions of the UP- and\nDOWN-verbs used to describe movements of indices, stocks, and shares. Using\nmeasures of the overlap in the argument distributions of these verbs and\nk-means clustering of their distributions, we advance evidence for the proposal\nthat the metaphors referred to by these verbs are organised into hierarchical\nstructures of superordinate and subordinate groups.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 11:47:09 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Georw", "Aaron", ""], ["Keane", "Mark", ""]]}, {"id": "1212.3139", "submitter": "Aaron Gerow", "authors": "Aaron Gerow and Mark Keane", "title": "Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles", "comments": "arXiv admin note: text overlap with arXiv:1212.3138", "journal-ref": "Proceedings of the 33rd Annual Meeting of the Cognitive Science\n  Society (CogSci '11), Boston, MA, USA, 20-23 July, 2011", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a corpus of 17,000+ financial news reports (involving over 10M words),\nwe perform an analysis of the argument-distributions of the UP and DOWN verbs\nused to describe movements of indices, stocks and shares. In Study 1\nparticipants identified antonyms of these verbs in a free-response task and a\nmatching task from which the most commonly identified antonyms were compiled.\nIn Study 2, we determined whether the argument-distributions for the verbs in\nthese antonym-pairs were sufficiently similar to predict the most\nfrequently-identified antonym. Cosine similarity correlates moderately with the\nproportions of antonym-pairs identified by people (r = 0.31). More\nimpressively, 87% of the time the most frequently-identified antonym is either\nthe first- or second-most similar pair in the set of alternatives. The\nimplications of these results for distributional approaches to determining\nmetaphoric knowledge are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 11:53:25 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2013 17:11:29 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Gerow", "Aaron", ""], ["Keane", "Mark", ""]]}, {"id": "1212.3162", "submitter": "Aaron Gerow", "authors": "Aaron Gerow and Khurshid Ahmad", "title": "Diachronic Variation in Grammatical Relations", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Computational\n  Linguistics (COLING 2012), Mumbai, India", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of finding and analyzing shifts in grammatical relations\nfound in diachronic corpora. Inspired by the econometric technique of measuring\nreturn and volatility instead of relative frequencies, we propose them as a way\nto better characterize changes in grammatical patterns like nominalization,\nmodification and comparison. To exemplify the use of these techniques, we\nexamine a corpus of NIPS papers and report trends which manifest at the token,\npart-of-speech and grammatical levels. Building up from frequency observations\nto a second-order analysis, we show that shifts in frequencies overlook deeper\ntrends in language, even when part-of-speech information is included. Examining\ntoken, POS and grammatical levels of variation enables a summary view of\ndiachronic text as a whole. We conclude with a discussion about how these\nmethods can inform intuitions about specialist domains as well as changes in\nlanguage use as a whole.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 13:00:55 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Gerow", "Aaron", ""], ["Ahmad", "Khurshid", ""]]}, {"id": "1212.3171", "submitter": "Andrzej Kulig", "authors": "Iwona Grabska-Gradzi\\'nska, Andrzej Kulig, Jaros{\\l}aw Kwapie\\'n,\n  Pawe{\\l} O\\'swi\\k{e}cimka, Stanis{\\l}aw Dro\\.zd\\.z", "title": "Multifractal analysis of sentence lengths in English literary texts", "comments": "5 pages, 5 figures, WCIT 2012 conference", "journal-ref": "AWERProcedia Information Technology & Computer Science 03, 1700,\n  2013", "doi": null, "report-no": null, "categories": "physics.data-an cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents analysis of 30 literary texts written in English by\ndifferent authors. For each text, there were created time series representing\nlength of sentences in words and analyzed its fractal properties using two\nmethods of multifractal analysis: MFDFA and WTMM. Both methods showed that\nthere are texts which can be considered multifractal in this representation but\na majority of texts are not multifractal or even not fractal at all. Out of 30\nbooks, only a few have so-correlated lengths of consecutive sentences that the\nanalyzed signals can be interpreted as real multifractals. An interesting\ndirection for future investigations would be identifying what are the specific\nfeatures which cause certain texts to be multifractal and other to be\nmonofractal or even not fractal at all.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 13:41:03 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Grabska-Gradzi\u0144ska", "Iwona", ""], ["Kulig", "Andrzej", ""], ["Kwapie\u0144", "Jaros\u0142aw", ""], ["O\u015bwi\u0119cimka", "Pawe\u0142", ""], ["Dro\u017cd\u017c", "Stanis\u0142aw", ""]]}, {"id": "1212.3228", "submitter": "Peiyou Song", "authors": "Peiyou Song, Anhei Shu, David Phipps, Dan Wallach, Mohit Tiwari,\n  Jedidiah Crandall, George Luger", "title": "Language Without Words: A Pointillist Model for Natural Language\n  Processing", "comments": "5 pages, 2 figures", "journal-ref": "The 6th International Conference on Soft Computing and Intelligent\n  Systems (SCIS-ISIS 2012) Kobe, Japan", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores two separate questions: Can we perform natural language\nprocessing tasks without a lexicon?; and, Should we? Existing natural language\nprocessing techniques are either based on words as units or use units such as\ngrams only for basic classification tasks. How close can a machine come to\nreasoning about the meanings of words and phrases in a corpus without using any\nlexicon, based only on grams?\n  Our own motivation for posing this question is based on our efforts to find\npopular trends in words and phrases from online Chinese social media. This form\nof written Chinese uses so many neologisms, creative character placements, and\ncombinations of writing systems that it has been dubbed the \"Martian Language.\"\nReaders must often use visual queues, audible queues from reading out loud, and\ntheir knowledge and understanding of current events to understand a post. For\nanalysis of popular trends, the specific problem is that it is difficult to\nbuild a lexicon when the invention of new ways to refer to a word or concept is\neasy and common. For natural language processing in general, we argue in this\npaper that new uses of language in social media will challenge machines'\nabilities to operate with words as the basic unit of understanding, not only in\nChinese but potentially in other languages.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 20:19:58 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Song", "Peiyou", ""], ["Shu", "Anhei", ""], ["Phipps", "David", ""], ["Wallach", "Dan", ""], ["Tiwari", "Mohit", ""], ["Crandall", "Jedidiah", ""], ["Luger", "George", ""]]}, {"id": "1212.3493", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Alejandro Molina, Juan-Manuel Torres-Moreno, Iria da Cunha, Eric\n  SanJuan, Gerardo Sierra", "title": "Sentence Compression in Spanish driven by Discourse Segmentation and\n  Language Models", "comments": "7 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works demonstrated that Automatic Text Summarization (ATS) by\nsentences extraction may be improved using sentence compression. In this work\nwe present a sentence compressions approach guided by level-sentence discourse\nsegmentation and probabilistic language models (LM). The results presented here\nshow that the proposed solution is able to generate coherent summaries with\ngrammatical compressed sentences. The approach is simple enough to be\ntransposed into other languages.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 14:51:22 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 13:10:47 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Molina", "Alejandro", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["da Cunha", "Iria", ""], ["SanJuan", "Eric", ""], ["Sierra", "Gerardo", ""]]}, {"id": "1212.3634", "submitter": "Hanane Froud", "authors": "Hanane Froud, Abdelmonaim Lachkar, and Said Alaoui Ouatik", "title": "A comparative study of root-based and stem-based approaches for\n  measuring the similarity between arabic words for arabic text mining\n  applications", "comments": null, "journal-ref": "Advanced Computing An International Journal (ACIJ), November 2012,\n  Volume 3, Number 6", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation of semantic information contained in the words is needed for\nany Arabic Text Mining applications. More precisely, the purpose is to better\ntake into account the semantic dependencies between words expressed by the\nco-occurrence frequencies of these words. There have been many proposals to\ncompute similarities between words based on their distributions in contexts. In\nthis paper, we compare and contrast the effect of two preprocessing techniques\napplied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)\napproaches for measuring the similarity between Arabic words with the well\nknown abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of\ndistance functions and similarity measures, such as the Euclidean Distance,\nCosine Similarity, Jaccard Coefficient, and the Pearson Correlation\nCoefficient. The obtained results show that, on the one hand, the variety of\nthe corpus produces more accurate results; on the other hand, the Stem-based\napproach outperformed the Root-based one because this latter affects the words\nmeanings.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 23:34:07 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Froud", "Hanane", ""], ["Lachkar", "Abdelmonaim", ""], ["Ouatik", "Said Alaoui", ""]]}, {"id": "1212.4315", "submitter": "Marco Guerini", "authors": "Lorenzo Gatti and Marco Guerini", "title": "Assessing Sentiment Strength in Words Prior Polarities", "comments": "To appear at Coling 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches to sentiment analysis rely on lexica where words are tagged\nwith their prior polarity - i.e. if a word out of context evokes something\npositive or something negative. In particular, broad-coverage resources like\nSentiWordNet provide polarities for (almost) every word. Since words can have\nmultiple senses, we address the problem of how to compute the prior polarity of\na word starting from the polarity of each sense and returning its polarity\nstrength as an index between -1 and 1. We compare 14 such formulae that appear\nin the literature, and assess which one best approximates the human judgement\nof prior polarities, with both regression and classification models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 11:33:50 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Gatti", "Lorenzo", ""], ["Guerini", "Marco", ""]]}, {"id": "1212.4674", "submitter": "Hyonil Kim", "authors": "Hyeok Kong", "title": "Natural Language Understanding Based on Semantic Relations between\n  Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define event expression over sentences of natural language\nand semantic relations between events. Based on this definition, we formally\nconsider text understanding process having events as basic unit.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 14:40:38 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Kong", "Hyeok", ""]]}, {"id": "1212.5238", "submitter": "Andrea Baronchelli", "authors": "Delia Mocanu, Andrea Baronchelli, Bruno Gon\\c{c}alves, Nicola Perra,\n  Alessandro Vespignani", "title": "The Twitter of Babel: Mapping World Languages through Microblogging\n  Platforms", "comments": null, "journal-ref": "PLoS One 8, E61981 (2013)", "doi": "10.1371/journal.pone.0061981", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale analysis and statistics of socio-technical systems that just a\nfew short years ago would have required the use of consistent economic and\nhuman resources can nowadays be conveniently performed by mining the enormous\namount of digital data produced by human activities. Although a\ncharacterization of several aspects of our societies is emerging from the data\nrevolution, a number of questions concerning the reliability and the biases\ninherent to the big data \"proxies\" of social life are still open. Here, we\nsurvey worldwide linguistic indicators and trends through the analysis of a\nlarge-scale dataset of microblogging posts. We show that available data allow\nfor the study of language geography at scales ranging from country-level\naggregation to specific city neighborhoods. The high resolution and coverage of\nthe data allows us to investigate different indicators such as the linguistic\nhomogeneity of different countries, the touristic seasonal patterns within\ncountries and the geographical distribution of different languages in\nmultilingual regions. This work highlights the potential of geolocalized\nstudies of open data sources to improve current analysis and develop indicators\nfor major social phenomena in specific communities.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 20:43:12 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Mocanu", "Delia", ""], ["Baronchelli", "Andrea", ""], ["Gon\u00e7alves", "Bruno", ""], ["Perra", "Nicola", ""], ["Vespignani", "Alessandro", ""]]}, {"id": "1212.6527", "submitter": "Eugene Yuta Bann", "authors": "Eugene Yuta Bann", "title": "Discovering Basic Emotion Sets via Semantic Clustering on a Twitter\n  Corpus", "comments": "University of Bath BSc(Hons) Computer Science Dissertation, 105 Pages", "journal-ref": null, "doi": null, "report-no": "CSBU-2013-01", "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of words are used to describe the spectrum of human emotions, but\nhow many emotions are there really, and how do they interact? Over the past few\ndecades, several theories of emotion have been proposed, each based around the\nexistence of a set of 'basic emotions', and each supported by an extensive\nvariety of research including studies in facial expression, ethology, neurology\nand physiology. Here we present research based on a theory that people transmit\ntheir understanding of emotions through the language they use surrounding\nemotion keywords. Using a labelled corpus of over 21,000 tweets, six of the\nbasic emotion sets proposed in existing literature were analysed using Latent\nSemantic Clustering (LSC), evaluating the distinctiveness of the semantic\nmeaning attached to the emotional label. We hypothesise that the more distinct\nthe language is used to express a certain emotion, then the more distinct the\nperception (including proprioception) of that emotion is, and thus more\n'basic'. This allows us to select the dimensions best representing the entire\nspectrum of emotion. We find that Ekman's set, arguably the most frequently\nused for classifying emotions, is in fact the most semantically distinct\noverall. Next, taking all analysed (that is, previously proposed) emotion terms\ninto account, we determine the optimal semantically irreducible basic emotion\nset using an iterative LSC algorithm. Our newly-derived set (Accepting,\nAshamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a\n6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful,\nSad, Scared). We also demonstrate how using LSC data can help visualise\nemotions. We introduce the concept of an Emotion Profile and briefly analyse\ncompound emotions both visually and mathematically.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2012 15:39:46 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Bann", "Eugene Yuta", ""]]}]