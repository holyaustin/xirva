[{"id": "1605.00090", "submitter": "Yu Wu", "authors": "Yu Wu, Wei Wu, Zhoujun Li, Ming Zhou", "title": "Response Selection with Topic Clues for Retrieval-based Chatbots", "comments": "under reviewed of AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider incorporating topic information into message-response matching to\nboost responses with rich content in retrieval-based chatbots. To this end, we\npropose a topic-aware convolutional neural tensor network (TACNTN). In TACNTN,\nmatching between a message and a response is not only conducted between a\nmessage vector and a response vector generated by convolutional neural\nnetworks, but also leverages extra topic information encoded in two topic\nvectors. The two topic vectors are linear combinations of topic words of the\nmessage and the response respectively, where the topic words are obtained from\na pre-trained LDA model and their weights are determined by themselves as well\nas the message vector and the response vector. The message vector, the response\nvector, and the two topic vectors are fed to neural tensors to calculate a\nmatching score. Empirical study on a public data set and a human annotated data\nset shows that TACNTN can significantly outperform state-of-the-art methods for\nmessage-response matching.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 10:48:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 05:37:26 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 03:02:40 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Wu", "Yu", ""], ["Wu", "Wei", ""], ["Li", "Zhoujun", ""], ["Zhou", "Ming", ""]]}, {"id": "1605.00122", "submitter": "Xinyu Fu", "authors": "Xinyu Fu, Eugene Ch'ng, Uwe Aickelin, Lanyun Zhang", "title": "An Improved System for Sentence-level Novelty Detection in Textual\n  Streams", "comments": null, "journal-ref": null, "doi": "10.1049/cp.2015.0250", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection in news events has long been a difficult problem. A number\nof models performed well on specific data streams but certain issues are far\nfrom being solved, particularly in large data streams from the WWW where\nunpredictability of new terms requires adaptation in the vector space model. We\npresent a novel event detection system based on the Incremental Term\nFrequency-Inverse Document Frequency (TF-IDF) weighting incorporated with\nLocality Sensitive Hashing (LSH). Our system could efficiently and effectively\nadapt to the changes within the data streams of any new terms with continual\nupdates to the vector space model. Regarding miss probability, our proposed\nnovelty detection framework outperforms a recognised baseline system by\napproximately 16% when evaluating a benchmark dataset from Google News.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 15:04:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Fu", "Xinyu", ""], ["Ch'ng", "Eugene", ""], ["Aickelin", "Uwe", ""], ["Zhang", "Lanyun", ""]]}, {"id": "1605.00223", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos\n  Anagnostopoulos and Giovanni Montana", "title": "Text-mining the NeuroSynth corpus using Deep Boltzmann Machines", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale automated meta-analysis of neuroimaging data has recently\nestablished itself as an important tool in advancing our understanding of human\nbrain function. This research has been pioneered by NeuroSynth, a database\ncollecting both brain activation coordinates and associated text across a large\ncohort of neuroimaging research papers. One of the fundamental aspects of such\nmeta-analysis is text-mining. To date, word counts and more sophisticated\nmethods such as Latent Dirichlet Allocation have been proposed. In this work we\npresent an unsupervised study of the NeuroSynth text corpus using Deep\nBoltzmann Machines (DBMs). The use of DBMs yields several advantages over the\naforementioned methods, principal among which is the fact that it yields both\nword and document embeddings in a high-dimensional vector space. Such\nembeddings serve to facilitate the use of traditional machine learning\ntechniques on the text corpus. The proposed DBM model is shown to learn\nembeddings with a clear semantic structure.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 09:01:13 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Lorenz", "Romy", ""], ["Leech", "Robert", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1605.00459", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia", "title": "Multi30K: Multilingual English-German Image Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Multi30K dataset to stimulate multilingual multimodal\nresearch. Recent advances in image description have been demonstrated on\nEnglish-language datasets almost exclusively, but image description should not\nbe limited to English. This dataset extends the Flickr30K dataset with i)\nGerman translations created by professional translators over a subset of the\nEnglish descriptions, and ii) descriptions crowdsourced independently of the\noriginal English descriptions. We outline how the data can be used for\nmultilingual image description and multimodal machine translation, but we\nanticipate the data will be useful for a broader range of tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 12:38:03 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Sima'an", "Khalil", ""], ["Specia", "Lucia", ""]]}, {"id": "1605.00482", "submitter": "Geonmin Kim", "authors": "Geonmin Kim, Hwaran Lee, Jisu Choi, Soo-young Lee", "title": "Compositional Sentence Representation from Character within Large\n  Context Text", "comments": "13pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a Hierarchical Composition Recurrent Network (HCRN)\nconsisting of a 3-level hierarchy of compositional models: character, word and\nsentence. This model is designed to overcome two problems of representing a\nsentence on the basis of a constituent word sequence. The first is a\ndata-sparsity problem in word embedding, and the other is a no usage of\ninter-sentence dependency. In the HCRN, word representations are built from\ncharacters, thus resolving the data-sparsity problem, and inter-sentence\ndependency is embedded into sentence representation at the level of sentence\ncomposition. We adopt a hierarchy-wise learning scheme in order to alleviate\nthe optimization difficulties of learning deep hierarchical recurrent network\nin end-to-end fashion. The HCRN was quantitatively and qualitatively evaluated\non a dialogue act classification task. Especially, sentence representations\nwith an inter-sentence dependency are able to capture both implicit and\nexplicit semantics of sentence, significantly improving performance. In the\nend, the HCRN achieved state-of-the-art performance with a test error rate of\n22.7% for dialogue act classification on the SWBD-DAMSL database.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 13:41:38 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 05:57:09 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 13:35:17 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Kim", "Geonmin", ""], ["Lee", "Hwaran", ""], ["Choi", "Jisu", ""], ["Lee", "Soo-young", ""]]}, {"id": "1605.00855", "submitter": "Xirong Li", "authors": "Xirong Li and Qin Jin", "title": "Improving Image Captioning by Concept-based Sentence Reranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our winning entry in the ImageCLEF 2015 image sentence\ngeneration task. We improve Google's CNN-LSTM model by introducing\nconcept-based sentence reranking, a data-driven approach which exploits the\nlarge amounts of concept-level annotations on Flickr. Different from previous\nusage of concept detection that is tailored to specific image captioning\nmodels, the propose approach reranks predicted sentences in terms of their\nmatches with detected concepts, essentially treating the underlying model as a\nblack box. This property makes the approach applicable to a number of existing\nsolutions. We also experiment with fine tuning on the deep language model,\nwhich improves the performance further. Scoring METEOR of 0.1875 on the\nImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of\n0.1687) with a clear margin.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 12:13:26 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Li", "Xirong", ""], ["Jin", "Qin", ""]]}, {"id": "1605.00942", "submitter": "Seppo Enarvi", "authors": "Seppo Enarvi, Mikko Kurimo", "title": "TheanoLM - An Extensible Toolkit for Neural Network Language Modeling", "comments": null, "journal-ref": "Proc. Interspeech 2016, pp. 3052-3056", "doi": "10.21437/Interspeech.2016-618", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new tool for training neural network language models (NNLMs),\nscoring sentences, and generating text. The tool has been written using Python\nlibrary Theano, which allows researcher to easily extend it and tune any aspect\nof the training process. Regardless of the flexibility, Theano is able to\ngenerate extremely fast native code that can utilize a GPU or multiple CPU\ncores in order to parallelize the heavy numerical computations. The tool has\nbeen evaluated in difficult Finnish and English conversational speech\nrecognition tasks, and significant improvement was obtained over our best\nback-off n-gram models. The results that we obtained in the Finnish task were\ncompared to those from existing RNNLM and RWTHLM toolkits, and found to be as\ngood or better, while training times were an order of magnitude shorter.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:20:31 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 08:04:04 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Enarvi", "Seppo", ""], ["Kurimo", "Mikko", ""]]}, {"id": "1605.01194", "submitter": "Sharmistha Jat", "authors": "Lavanya Sita Tekumalla and Sharmistha", "title": "IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based\n  Multiple Chunk Aligner", "comments": "SEMEVAL Workshop @ NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable semantic textual similarity (iSTS) task adds a crucial\nexplanatory layer to pairwise sentence similarity. We address various\ncomponents of this task: chunk level semantic alignment along with assignment\nof similarity type and score for aligned chunks with a novel system presented\nin this paper. We propose an algorithm, iMATCH, for the alignment of multiple\nnon-contiguous chunks based on Integer Linear Programming (ILP). Similarity\ntype and score assignment for pairs of chunks is done using a supervised\nmulticlass classification technique based on Random Forrest Classifier. Results\nshow that our algorithm iMATCH has low execution time and outperforms most\nother participating systems in terms of alignment score. Of the three datasets,\nwe are top ranked for answer- students dataset in terms of overall score and\nhave top alignment score for headlines dataset in the gold chunks track.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 09:36:49 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Tekumalla", "Lavanya Sita", ""], ["Sharmistha", "", ""]]}, {"id": "1605.01326", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Compression and the origins of Zipf's law for word frequencies", "comments": "arguments have been improved; in press in Complexity (Wiley)", "journal-ref": "Complexity 21, 409-411 (2016)", "doi": "10.1002/cplx.21820", "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we sketch a new derivation of Zipf's law for word frequencies based on\noptimal coding. The structure of the derivation is reminiscent of Mandelbrot's\nrandom typing model but it has multiple advantages over random typing: (1) it\nstarts from realistic cognitive pressures (2) it does not require fine tuning\nof parameters and (3) it sheds light on the origins of other statistical laws\nof language and thus can lead to a compact theory of linguistic laws. Our\nfindings suggest that the recurrence of Zipf's law in human languages could\noriginate from pressure for easy and fast communication.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 16:00:59 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 15:14:10 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1605.01478", "submitter": "Yujie Cao", "authors": "Minlie Huang, Yujie Cao, Chao Dong", "title": "Modeling Rich Contexts for Sentiment Classification with LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis on social media data such as tweets and weibo has become a\nvery important and challenging task. Due to the intrinsic properties of such\ndata, tweets are short, noisy, and of divergent topics, and sentiment\nclassification on these data requires to modeling various contexts such as the\nretweet/reply history of a tweet, and the social context about authors and\nrelationships. While few prior study has approached the issue of modeling\ncontexts in tweet, this paper proposes to use a hierarchical LSTM to model rich\ncontexts in tweet, particularly long-range context. Experimental results show\nthat contexts can help us to perform sentiment classification remarkably\nbetter.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 03:06:47 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Huang", "Minlie", ""], ["Cao", "Yujie", ""], ["Dong", "Chao", ""]]}, {"id": "1605.01635", "submitter": "Omid Sadjadi", "authors": "Seyed Omid Sadjadi, Jason Pelecanos, Sriram Ganapathy", "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis", "comments": "submitted to INTERSPEECH 2016. arXiv admin note: substantial text\n  overlap with arXiv:1602.07291", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the recent advances along with an error analysis of the IBM\nspeaker recognition system for conversational speech. Some of the key\nadvancements that contribute to our system include: a nearest-neighbor\ndiscriminant analysis (NDA) approach (as opposed to LDA) for intersession\nvariability compensation in the i-vector space, the application of speaker and\nchannel-adapted features derived from an automatic speech recognition (ASR)\nsystem for speaker recognition, and the use of a DNN acoustic model with a very\nlarge number of output units (~10k senones) to compute the frame-level soft\nalignments required in the i-vector estimation process. We evaluate these\ntechniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as\nthe 10sec-10sec condition. To our knowledge, results achieved by our system\nrepresent the best performances published to date on these conditions. For\nexample, on the extended tel-tel condition (C5) the system achieves an EER of\n0.59%. To garner further understanding of the remaining errors (on C5), we\nexamine the recordings associated with the low scoring target trials, where\nvarious issues are identified for the problematic recordings/trials.\nInterestingly, it is observed that correcting the pathological recordings not\nonly improves the scores for the target trials but also for the nontarget\ntrials.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 15:57:21 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Sadjadi", "Seyed Omid", ""], ["Pelecanos", "Jason", ""], ["Ganapathy", "Sriram", ""]]}, {"id": "1605.01652", "submitter": "Phong Le", "authors": "Phong Le, Marc Dymetman, Jean-Michel Renders", "title": "LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an LSTM-based method for dynamically integrating several\nword-prediction experts to obtain a conditional language model which can be\ngood simultaneously at several subtasks. We illustrate this general approach\nwith an application to dialogue where we integrate a neural chat model, good at\nconversational aspects, with a neural question-answering model, good at\nretrieving precise information from a knowledge-base, and show how the\nintegration combines the strengths of the independent components. We hope that\nthis focused contribution will attract attention on the benefits of using such\nmixtures of experts in NLP.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 17:00:44 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Le", "Phong", ""], ["Dymetman", "Marc", ""], ["Renders", "Jean-Michel", ""]]}, {"id": "1605.01655", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko", "title": "Stance and Sentiment in Tweets", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can often detect from a person's utterances whether he/she is in favor of\nor against a given target entity -- their stance towards the target. However, a\nperson may express the same stance towards a target by using negative or\npositive language. Here for the first time we present a dataset of\ntweet--target pairs annotated for both stance and sentiment. The targets may or\nmay not be referred to in the tweets, and they may or may not be the target of\nopinion in the tweets. Partitions of this dataset were used as training and\ntest sets in a SemEval-2016 shared task competition. We propose a simple stance\ndetection system that outperforms submissions from all 19 teams that\nparticipated in the shared task. Additionally, access to both stance and\nsentiment annotations allows us to explore several research questions. We show\nthat while knowing the sentiment expressed by a tweet is beneficial for stance\nclassification, it alone is not sufficient. Finally, we use additional\nunlabeled data through distant supervision techniques and word embeddings to\nfurther improve stance classification.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 17:07:54 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Sobhani", "Parinaz", ""], ["Kiritchenko", "Svetlana", ""]]}, {"id": "1605.01661", "submitter": "Ramon Ferrer i Cancho", "authors": "R. Ferrer-i-Cancho, D. Lusseau and B. McCowan", "title": "Parallels of human language in the behavior of bottlenose dolphins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A short review of similarities between dolphins and humans with the help of\nquantitative linguistics and information theory.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 17:38:42 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Ferrer-i-Cancho", "R.", ""], ["Lusseau", "D.", ""], ["McCowan", "B.", ""]]}, {"id": "1605.01744", "submitter": "David Cinciruk", "authors": "Mengke Hu, David Cinciruk, and John MacLaren Walsh", "title": "Improving Automated Patent Claim Parsing: Dataset, System, and\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-the-shelf natural language processing software performs poorly when\nparsing patent claims owing to their use of irregular language relative to the\ncorpora built from news articles and the web typically utilized to train this\nsoftware. Stopping short of the extensive and expensive process of accumulating\na large enough dataset to completely retrain parsers for patent claims, a\nmethod of adapting existing natural language processing software towards patent\nclaims via forced part of speech tag correction is proposed. An Amazon\nMechanical Turk collection campaign organized to generate a public corpus to\ntrain such an improved claim parsing system is discussed, identifying lessons\nlearned during the campaign that can be of use in future NLP dataset collection\ncampaigns with AMT. Experiments utilizing this corpus and other patent claim\nsets measure the parsing performance improvement garnered via the claim parsing\nsystem. Finally, the utility of the improved claim parsing system within other\npatent processing applications is demonstrated via experiments showing improved\nautomated patent subject classification when the new claim parsing system is\nutilized to generate the features.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 20:11:57 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Hu", "Mengke", ""], ["Cinciruk", "David", ""], ["Walsh", "John MacLaren", ""]]}, {"id": "1605.01845", "submitter": "Ildik\\'o Pil\\'an", "authors": "Ildik\\'o Pil\\'an", "title": "Detecting Context Dependence in Exercise Item Candidates Selected from\n  Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the factors influencing the dependence of single sentences on\ntheir larger textual context in order to automatically identify candidate\nsentences for language learning exercises from corpora which are presentable in\nisolation. An in-depth investigation of this question has not been previously\ncarried out. Understanding this aspect can contribute to a more efficient\nselection of candidate sentences which, besides reducing the time required for\nitem writing, can also ensure a higher degree of variability and authenticity.\nWe present a set of relevant aspects collected based on the qualitative\nanalysis of a smaller set of context-dependent corpus example sentences.\nFurthermore, we implemented a rule-based algorithm using these criteria which\nachieved an average precision of 0.76 for the identification of different\nissues related to context dependence. The method has also been evaluated\nempirically where 80% of the sentences in which our system did not detect\ncontext-dependent elements were also considered context-independent by human\nraters.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 07:30:53 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Pil\u00e1n", "Ildik\u00f3", ""]]}, {"id": "1605.01919", "submitter": "Scott A. Hale", "authors": "Scott A. Hale", "title": "User Reviews and Language: How Language Influences Ratings", "comments": null, "journal-ref": "Proceedings of the 2016 CHI Conference on Human Factors in\n  Computing Systems, CHI'16 Extended Abstracts", "doi": "10.1145/2851581.2892466", "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of user reviews of tourist attractions, restaurants, mobile apps,\netc. is increasing for all languages; yet, research is lacking on how reviews\nin multiple languages should be aggregated and displayed. Speakers of different\nlanguages may have consistently different experiences, e.g., different\ninformation available in different languages at tourist attractions or\ndifferent user experiences with software due to\ninternationalization/localization choices. This paper assesses the similarity\nin the ratings given by speakers of different languages to London tourist\nattractions on TripAdvisor. The correlations between different languages are\ngenerally high, but some language pairs are more correlated than others. The\nresults question the common practice of computing average ratings from reviews\nin many languages.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 12:52:09 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Hale", "Scott A.", ""]]}, {"id": "1605.02019", "submitter": "Christopher Moody", "authors": "Christopher E Moody", "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "comments": "Submitted to CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed dense word vectors have been shown to be effective at capturing\ntoken-level semantic and syntactic regularities in language, while topic models\ncan form interpretable representations over documents. In this work, we\ndescribe lda2vec, a model that learns dense word vectors jointly with\nDirichlet-distributed latent document-level mixtures of topic vectors. In\ncontrast to continuous dense document representations, this formulation\nproduces sparse, interpretable document mixtures through a non-negative simplex\nconstraint. Our method is simple to incorporate into existing automatic\ndifferentiation frameworks and allows for unsupervised document representations\ngeared for use by scientists while simultaneously learning word vectors and the\nlinear relationships between them.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 18:13:18 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Moody", "Christopher E", ""]]}, {"id": "1605.02129", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Trung H. Bui, and Hung H. Bui", "title": "Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot\n  task", "comments": "Paper accepted at IWSDS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks.\nIn this paper, we focus on the spoken language understanding pilot task, which\nconsists of tagging a given utterance with speech acts and semantic slots. We\ncompare different classifiers: the best system obtains 0.52 and 0.67 F1-scores\non the test set for speech act recognition for the tourist and the guide\nrespectively, and 0.52 F1-score for semantic tagging for both the guide and the\ntourist.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 01:55:51 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Bui", "Trung H.", ""], ["Bui", "Hung H.", ""]]}, {"id": "1605.02130", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui", "title": "Robust Dialog State Tracking for Large Ontologies", "comments": "Paper accepted at IWSDS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the\nprevious three editions as follows: the number of slot-value pairs present in\nthe ontology is much larger, no spoken language understanding output is given,\nand utterances are labeled at the subdialog level. This paper describes a novel\ndialog state tracking method designed to work robustly under these conditions,\nusing elaborate string matching, coreference resolution tailored for dialogs\nand a few other improvements. The method can correctly identify many values\nthat are not explicitly present in the utterance. On the final evaluation, our\nmethod came in first among 7 competing teams and 24 entries. The F1-score\nachieved by our method was 9 and 7 percentage points higher than that of the\nrunner-up for the utterance-level evaluation and for the subdialog-level\nevaluation, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 02:00:30 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Bui", "Trung H.", ""], ["Bui", "Hung H.", ""]]}, {"id": "1605.02134", "submitter": "Wei-Nan Zhang", "authors": "Wei-Nan Zhang, Ting Liu, Qingyu Yin, Yu Zhang", "title": "Neural Recovery Machine for Chinese Dropped Pronoun", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese,\nJapanese etc. Previous work mainly focused on painstakingly exploring the\nempirical features for DPs recovery. In this paper, we propose a neural\nrecovery machine (NRM) to model and recover DPs in Chinese, so that to avoid\nthe non-trivial feature engineering process. The experimental results show that\nthe proposed NRM significantly outperforms the state-of-the-art approaches on\nboth two heterogeneous datasets. Further experiment results of Chinese zero\npronoun (ZP) resolution show that the performance of ZP resolution can also be\nimproved by recovering the ZPs to DPs.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 02:41:54 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 01:26:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Wei-Nan", ""], ["Liu", "Ting", ""], ["Yin", "Qingyu", ""], ["Zhang", "Yu", ""]]}, {"id": "1605.02150", "submitter": "Elaheh ShafieiBavani", "authors": "Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen", "title": "On Improving Informativity and Grammaticality for Multi-Sentence\n  Compression", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "UNSW-CSE-TR-201517", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi Sentence Compression (MSC) is of great value to many real world\napplications, such as guided microblog summarization, opinion summarization and\nnewswire summarization. Recently, word graph-based approaches have been\nproposed and become popular in MSC. Their key assumption is that redundancy\namong a set of related sentences provides a reliable way to generate\ninformative and grammatical sentences. In this paper, we propose an effective\napproach to enhance the word graph-based MSC and tackle the issue that most of\nthe state-of-the-art MSC approaches are confronted with: i.e., improving both\ninformativity and grammaticality at the same time. Our approach consists of\nthree main components: (1) a merging method based on Multiword Expressions\n(MWE); (2) a mapping strategy based on synonymy between words; (3) a re-ranking\nstep to identify the best compression candidates generated using a POS-based\nlanguage model (POS-LM). We demonstrate the effectiveness of this novel\napproach using a dataset made of clusters of English newswire sentences. The\nobserved improvements on informativity and grammaticality of the generated\ncompressions show that our approach is superior to state-of-the-art MSC\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 06:39:57 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["ShafieiBavani", "Elaheh", ""], ["Ebrahimi", "Mohammad", ""], ["Wong", "Raymond", ""], ["Chen", "Fang", ""]]}, {"id": "1605.02257", "submitter": "Nathan Schneider", "authors": "Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Meredith Green,\n  Kathryn Conger, Tim O'Gorman, Martha Palmer", "title": "A corpus of preposition supersenses in English web reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first corpus annotated with preposition supersenses,\nunlexicalized categories for semantic functions that can be marked by English\nprepositions (Schneider et al., 2015). That scheme improves upon its\npredecessors to better facilitate comprehensive manual annotation. Moreover,\nunlike the previous schemes, the preposition supersenses are organized\nhierarchically. Our data will be publicly released on the web upon publication.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 01:38:34 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Schneider", "Nathan", ""], ["Hwang", "Jena D.", ""], ["Srikumar", "Vivek", ""], ["Green", "Meredith", ""], ["Conger", "Kathryn", ""], ["O'Gorman", "Tim", ""], ["Palmer", "Martha", ""]]}, {"id": "1605.02276", "submitter": "Manaal Faruqui", "authors": "Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer", "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", "comments": "The First Workshop on Evaluating Vector Space Representations for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lacking standardized extrinsic evaluation methods for vector representations\nof words, the NLP community has relied heavily on word similarity tasks as a\nproxy for intrinsic evaluation of word vectors. Word similarity evaluation,\nwhich correlates the distance between vectors and human judgments of semantic\nsimilarity is attractive, because it is computationally inexpensive and fast.\nIn this paper we present several problems associated with the evaluation of\nword vectors on word similarity datasets, and summarize existing solutions. Our\nstudy suggests that the use of word similarity tasks for evaluation of word\nvectors is not sustainable and calls for further research on evaluation\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 05:09:28 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 04:48:34 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 02:41:04 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Faruqui", "Manaal", ""], ["Tsvetkov", "Yulia", ""], ["Rastogi", "Pushpendre", ""], ["Dyer", "Chris", ""]]}, {"id": "1605.02442", "submitter": "Himani Mittal", "authors": "M. Syamala Devi and Himani Mittal", "title": "Machine Learning Techniques with Ontology for Subjective Answer\n  Evaluation", "comments": "11 pages, 5 figures, journal,\n  http://airccse.org/journal/ijnlc/current.html 2016", "journal-ref": null, "doi": "10.5121/ijnlc.2016.5201", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized Evaluation of English Essays is performed using Machine learning\ntechniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual\nEvaluation Understudy and Maximum Entropy. Ontology, a concept map of domain\nknowledge, can enhance the performance of these techniques. Use of Ontology\nmakes the evaluation process holistic as presence of keywords, synonyms, the\nright word combination and coverage of concepts can be checked. In this paper,\nthe above mentioned techniques are implemented both with and without Ontology\nand tested on common input data consisting of technical answers of Computer\nScience. Domain Ontology of Computer Graphics is designed and developed. The\nsoftware used for implementation includes Java Programming Language and tools\nsuch as MATLAB, Prot\\'eg\\'e, etc. Ten questions from Computer Graphics with\nsixty answers for each question are used for testing. The results are analyzed\nand it is concluded that the results are more accurate with use of Ontology.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 07:14:52 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Devi", "M. Syamala", ""], ["Mittal", "Himani", ""]]}, {"id": "1605.02457", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn", "title": "The Controlled Natural Language of Randall Munroe's Thing Explainer", "comments": null, "journal-ref": "Proceedings of the Fifth Workshop on Controlled Natural Language\n  (CNL 2016), Springer 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is rare that texts or entire books written in a Controlled Natural\nLanguage (CNL) become very popular, but exactly this has happened with a book\nthat has been published last year. Randall Munroe's Thing Explainer uses only\nthe 1'000 most often used words of the English language together with drawn\npictures to explain complicated things such as nuclear reactors, jet engines,\nthe solar system, and dishwashers. This restricted language is a very\ninteresting new case for the CNL community. I describe here its place in the\ncontext of existing approaches on Controlled Natural Languages, and I provide a\nfirst analysis from a scientific perspective, covering the word production\nrules and word distributions.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 07:48:40 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Kuhn", "Tobias", ""]]}, {"id": "1605.02592", "submitter": "Courtney Napoles", "authors": "Courtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault", "title": "GLEU Without Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GLEU metric was proposed for evaluating grammatical error corrections\nusing n-gram overlap with a set of reference sentences, as opposed to\nprecision/recall of specific annotated errors (Napoles et al., 2015). This\npaper describes improvements made to the GLEU metric that address problems that\narise when using an increasing number of reference sets. Unlike the originally\npresented metric, the modified metric does not require tuning. We recommend\nthat this version be used instead of the original version.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 14:05:57 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Napoles", "Courtney", ""], ["Sakaguchi", "Keisuke", ""], ["Post", "Matt", ""], ["Tetreault", "Joel", ""]]}, {"id": "1605.02697", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Marcus Rohrbach and Mario Fritz", "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "comments": "Improved version, it also has a final table from the VQA challenge,\n  and more baselines on DAQUAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Ask Your Neurons, a scalable, jointly\ntrained, end-to-end formulation to this problem.\n  In contrast to previous efforts, we are facing a multi-modal problem where\nthe language output (answer) is conditioned on visual and natural language\ninputs (image and question). We provide additional insights into the problem by\nanalyzing how much information is contained only in the language part for which\nwe provide a new human baseline. To study human consensus, which is related to\nthe ambiguities inherent in this challenging task, we propose two novel metrics\nand collect additional answers which extend the original DAQUAR dataset to\nDAQUAR-Consensus.\n  Moreover, we also extend our analysis to VQA, a large-scale question\nanswering about images dataset, where we investigate some particular design\nchoices and show the importance of stronger visual models. At the same time, we\nachieve strong performance of our model that still uses a global image\nrepresentation. Finally, based on such analysis, we refine our Ask Your Neurons\non DAQUAR, which also leads to a better performance on this challenging task.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:04:23 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 10:30:18 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Rohrbach", "Marcus", ""], ["Fritz", "Mario", ""]]}, {"id": "1605.02916", "submitter": "Dariusz Czerski", "authors": "Pawe{\\l} {\\L}ozi\\'nski, Dariusz Czerski, Mieczys{\\l}aw A. K{\\l}opotek", "title": "Grammatical Case Based IS-A Relation Extraction with Boosting for Polish", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-based methods of IS-A relation extraction rely heavily on so called\nHearst patterns. These are ways of expressing instance enumerations of a class\nin natural language. While these lexico-syntactic patterns prove quite useful,\nthey may not capture all taxonomical relations expressed in text. Therefore in\nthis paper we describe a novel method of IS-A relation extraction from\npatterns, which uses morpho-syntactical annotations along with grammatical case\nof noun phrases that constitute entities participating in IS-A relation. We\nalso describe a method for increasing the number of extracted relations that we\ncall pseudo-subclass boosting which has potential application in any\npattern-based relation extraction method. Experiments were conducted on a\ncorpus of about 0.5 billion web documents in Polish language.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 10:03:48 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["\u0141ozi\u0144ski", "Pawe\u0142", ""], ["Czerski", "Dariusz", ""], ["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1605.02945", "submitter": "Yuval Pinter", "authors": "Yuval Pinter, Roi Reichart, Idan Szpektor", "title": "The Yahoo Query Treebank, V. 1.0", "comments": "Co-released with the Webscope Dataset (L-28) and with Pinter et al.,\n  Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A description and annotation guidelines for the Yahoo Webscope release of\nQuery Treebank, Version 1.0, May 2016.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 11:29:28 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 17:20:26 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Pinter", "Yuval", ""], ["Reichart", "Roi", ""], ["Szpektor", "Idan", ""]]}, {"id": "1605.02948", "submitter": "Nasser Ghadiri", "authors": "Milad Moradi, Nasser Ghadiri", "title": "Different approaches for identifying important concepts in probabilistic\n  biomedical text summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization tools help users in biomedical domain to acquire\ntheir intended information from various textual resources more efficiently.\nSome of the biomedical text summarization systems put the basis of their\nsentence selection approach on the frequency of concepts extracted from the\ninput text. However, it seems that exploring other measures rather than the\nfrequency for identifying the valuable content of the input document, and\nconsidering the correlations existing between concepts may be more useful for\nthis type of summarization. In this paper, we describe a Bayesian summarizer\nfor biomedical text documents. The Bayesian summarizer initially maps the input\ntext to the Unified Medical Language System (UMLS) concepts, then it selects\nthe important ones to be used as classification features. We introduce\ndifferent feature selection approaches to identify the most important concepts\nof the text and to select the most informative content according to the\ndistribution of these concepts. We show that with the use of an appropriate\nfeature selection approach, the Bayesian biomedical summarizer can improve the\nperformance of summarization. We perform extensive evaluations on a corpus of\nscientific papers in biomedical domain. The results show that the Bayesian\nsummarizer outperforms the biomedical summarizers that rely on the frequency of\nconcepts, the domain-independent and baseline methods based on the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Moreover,\nthe results suggest that using the meaningfulness measure and considering the\ncorrelations of concepts in the feature selection step lead to a significant\nincrease in the performance of summarization.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 11:33:33 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 16:02:32 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 14:37:31 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Moradi", "Milad", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1605.03148", "submitter": "Haitao Mi", "authors": "Haitao Mi and Baskaran Sankaran and Zhiguo Wang and Abe Ittycheriah", "title": "Coverage Embedding Models for Neural Machine Translation", "comments": "6 pages; In Proceddings of EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we enhance the attention-based neural machine translation\n(NMT) by adding explicit coverage embedding models to alleviate issues of\nrepeating and dropping translations in NMT. For each source word, our model\nstarts with a full coverage embedding vector to track the coverage status, and\nthen keeps updating it with neural networks as the translation goes.\nExperiments on the large-scale Chinese-to-English task show that our enhanced\nmodel improves the translation quality significantly on various test sets over\nthe strong large vocabulary NMT system.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 18:44:34 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 15:10:34 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Mi", "Haitao", ""], ["Sankaran", "Baskaran", ""], ["Wang", "Zhiguo", ""], ["Ittycheriah", "Abe", ""]]}, {"id": "1605.03209", "submitter": "Haitao Mi", "authors": "Haitao Mi and Zhiguo Wang and Abe Ittycheriah", "title": "Vocabulary Manipulation for Neural Machine Translation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to capture rich language phenomena, neural machine translation\nmodels have to use a large vocabulary size, which requires high computing time\nand large memory usage. In this paper, we alleviate this issue by introducing a\nsentence-level or batch-level vocabulary, which is only a very small sub-set of\nthe full output vocabulary. For each sentence or batch, we only predict the\ntarget words in its sentence-level or batch-level vocabulary. Thus, we reduce\nboth the computing time and the memory usage. Our method simply takes into\naccount the translation options of each word or phrase in the source sentence,\nand picks a very small target vocabulary for each sentence based on a\nword-to-word translation model or a bilingual phrase library learned from a\ntraditional machine translation model. Experimental results on the large-scale\nEnglish-to-French task show that our method achieves better translation\nperformance by 1 BLEU point over the large vocabulary neural machine\ntranslation system of Jean et al. (2015).\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 20:50:56 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Mi", "Haitao", ""], ["Wang", "Zhiguo", ""], ["Ittycheriah", "Abe", ""]]}, {"id": "1605.03261", "submitter": "Junpei Zhong", "authors": "Junpei Zhong and Martin Peniak and Jun Tani and Tetsuya Ogata and\n  Angelo Cangelosi", "title": "Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics\n  Model for Generation and Generalisation of Noun-Verb Combinations with\n  Sensorimotor Inputs", "comments": "Submitted to Autonomous Robots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a neurorobotics cognitive model to explain the\nunderstanding and generalisation of nouns and verbs combinations when a vocal\ncommand consisting of a verb-noun sentence is provided to a humanoid robot.\nThis generalisation process is done via the grounding process: different\nobjects are being interacted, and associated, with different motor behaviours,\nfollowing a learning approach inspired by developmental language acquisition in\ninfants. This cognitive model is based on Multiple Time-scale Recurrent Neural\nNetworks (MTRNN).With the data obtained from object manipulation tasks with a\nhumanoid robot platform, the robotic agent implemented with this model can\nground the primitive embodied structure of verbs through training with\nverb-noun combination samples. Moreover, we show that a functional hierarchical\narchitecture, based on MTRNN, is able to generalise and produce novel\ncombinations of noun-verb sentences. Further analyses of the learned network\ndynamics and representations also demonstrate how the generalisation is\npossible via the exploitation of this functional hierarchical recurrent\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 02:31:21 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Zhong", "Junpei", ""], ["Peniak", "Martin", ""], ["Tani", "Jun", ""], ["Ogata", "Tetsuya", ""], ["Cangelosi", "Angelo", ""]]}, {"id": "1605.03284", "submitter": "Yuezhang Li", "authors": "Tian Tian and Yuezhang Li", "title": "Machine Comprehension Based on Learning to Rank", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine comprehension plays an essential role in NLP and has been widely\nexplored with dataset like MCTest. However, this dataset is too simple and too\nsmall for learning true reasoning abilities. \\cite{hermann2015teaching}\ntherefore release a large scale news article dataset and propose a deep LSTM\nreader system for machine comprehension. However, the training process is\nexpensive. We therefore try feature-engineered approach with semantics on the\nnew dataset to see how traditional machine learning technique and semantics can\nhelp with machine comprehension. Meanwhile, our proposed L2R reader system\nachieves good performance with efficiency and less training data.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 05:05:05 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 01:06:09 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Tian", "Tian", ""], ["Li", "Yuezhang", ""]]}, {"id": "1605.03481", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, William\n  W. Cohen", "title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "comments": "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text from social media provides a set of challenges that can cause\ntraditional NLP approaches to fail. Informal language, spelling errors,\nabbreviations, and special characters are all commonplace in these posts,\nleading to a prohibitively large vocabulary size for word-level approaches. We\npropose a character composition model, tweet2vec, which finds vector-space\nrepresentations of whole tweets by learning complex, non-local dependencies in\ncharacter sequences. The proposed model outperforms a word-level baseline at\npredicting user-annotated hashtags associated with the posts, doing\nsignificantly better when the input contains many out-of-vocabulary words or\nunusual character sequences. Our tweet2vec encoder is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:30:09 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 15:00:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Zhou", "Zhong", ""], ["Fitzpatrick", "Dylan", ""], ["Muehl", "Michael", ""], ["Cohen", "William W.", ""]]}, {"id": "1605.03664", "submitter": "Chris Kedzie", "authors": "Chris Kedzie, Fernando Diaz, and Kathleen McKeown", "title": "Real-Time Web Scale Event Summarization Using Sequential Decision Making", "comments": "in Proceedings of the 25th International Joint Conference on\n  Artificial Intelligence 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system based on sequential decision making for the online\nsummarization of massive document streams, such as those found on the web.\nGiven an event of interest (e.g. \"Boston marathon bombing\"), our system is able\nto filter the stream for relevance and produce a series of short text updates\ndescribing the event as it unfolds over time. Unlike previous work, our\napproach is able to jointly model the relevance, comprehensiveness, novelty,\nand timeliness required by time-sensitive queries. We demonstrate a 28.3%\nimprovement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 03:18:12 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Kedzie", "Chris", ""], ["Diaz", "Fernando", ""], ["McKeown", "Kathleen", ""]]}, {"id": "1605.03705", "submitter": "Marcus Rohrbach", "authors": "Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon,\n  Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele", "title": "Movie Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Description (AD) provides linguistic descriptions of movies and allows\nvisually impaired people to follow a movie along with their peers. Such\ndescriptions are by design mainly visual and thus naturally form an interesting\ndata source for computer vision and computational linguistics. In this work we\npropose a novel dataset which contains transcribed ADs, which are temporally\naligned to full length movies. In addition we also collected and aligned movie\nscripts used in prior work and compare the two sources of descriptions. In\ntotal the Large Scale Movie Description Challenge (LSMDC) contains a parallel\ncorpus of 118,114 sentences and video clips from 202 movies. First we\ncharacterize the dataset by benchmarking different approaches for generating\nvideo descriptions. Comparing ADs to scripts, we find that ADs are indeed more\nvisual and describe precisely what is shown rather than what should happen\naccording to the scripts created prior to movie production. Furthermore, we\npresent and compare the results of several teams who participated in a\nchallenge organized in the context of the workshop \"Describing and\nUnderstanding Video & The Large Scale Movie Description Challenge (LSMDC)\", at\nICCV 2015.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 07:34:08 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Rohrbach", "Anna", ""], ["Torabi", "Atousa", ""], ["Rohrbach", "Marcus", ""], ["Tandon", "Niket", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""], ["Schiele", "Bernt", ""]]}, {"id": "1605.03832", "submitter": "Yulia Tsvetkov", "authors": "Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample,\n  Patrick Littell, David Mortensen, Alan W Black, Lori Levin and Chris Dyer", "title": "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic\n  Representation Learning", "comments": "Proceedings of NAACL 2016; 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce polyglot language models, recurrent neural network models\ntrained to predict symbol sequences in many different languages using shared\nrepresentations of symbols and conditioning on typological information about\nthe language to be predicted. We apply these to the problem of modeling phone\nsequences---a domain in which universal symbol inventories and\ncross-linguistically shared feature representations are a natural fit.\nIntrinsic evaluation on held-out perplexity, qualitative analysis of the\nlearned representations, and extrinsic evaluation in two downstream\napplications that make use of phonetic features show (i) that polyglot models\nbetter generalize to held-out data than comparable monolingual models and (ii)\nthat polyglot phonetic feature representations are of higher quality than those\nlearned monolingually.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:37:51 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Tsvetkov", "Yulia", ""], ["Sitaram", "Sunayana", ""], ["Faruqui", "Manaal", ""], ["Lample", "Guillaume", ""], ["Littell", "Patrick", ""], ["Mortensen", "David", ""], ["Black", "Alan W", ""], ["Levin", "Lori", ""], ["Dyer", "Chris", ""]]}, {"id": "1605.03835", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in conditional recurrent language modelling have mainly\nfocused on network architectures (e.g., attention mechanism), learning\nalgorithms (e.g., scheduled sampling and sequence-level training) and novel\napplications (e.g., image/video description generation, speech recognition,\netc.) On the other hand, we notice that decoding algorithms/strategies have not\nbeen investigated as much, and it has become standard to use greedy or beam\nsearch. In this paper, we propose a novel decoding strategy motivated by an\nearlier observation that nonlinear hidden layers of a deep neural network\nstretch the data manifold. The proposed strategy is embarrassingly\nparallelizable without any communication overhead, while improving an existing\ndecoding algorithm. We extensively evaluate it with attention-based neural\nmachine translation on the task of En->Cz translation.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:39:50 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1605.03852", "submitter": "Yulia Tsvetkov", "authors": "Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney, Chris\n  Dyer", "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific\n  Word Representation Learning", "comments": "In proceedings of ACL 2016, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Bayesian optimization to learn curricula for word representation\nlearning, optimizing performance on downstream tasks that depend on the learned\nrepresentations as features. The curricula are modeled by a linear ranking\nfunction which is the scalar product of a learned weight vector and an\nengineered feature vector that characterizes the different aspects of the\ncomplexity of each instance in the training corpus. We show that learning the\ncurriculum improves performance on a variety of downstream tasks over random\norders and in comparison to the natural corpus order.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 15:15:58 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 18:35:29 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tsvetkov", "Yulia", ""], ["Faruqui", "Manaal", ""], ["Ling", "Wang", ""], ["MacWhinney", "Brian", ""], ["Dyer", "Chris", ""]]}, {"id": "1605.03924", "submitter": "Yuezhang Li", "authors": "Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, Katia\n  Sycara", "title": "Joint Embeddings of Hierarchical Categories and Entities", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the lack of structured knowledge applied in learning distributed\nrepresentation of categories, existing work cannot incorporate category\nhierarchies into entity information.~We propose a framework that embeds\nentities and categories into a semantic space by integrating structured\nknowledge and taxonomy hierarchy from large knowledge bases. The framework\nallows to compute meaningful semantic relatedness between entities and\ncategories.~Compared with the previous state of the art, our framework can\nhandle both single-word concepts and multiple-word concepts with superior\nperformance in concept categorization and semantic relatedness.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 18:45:18 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 01:04:25 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Li", "Yuezhang", ""], ["Zheng", "Ronghuo", ""], ["Tian", "Tian", ""], ["Hu", "Zhiting", ""], ["Iyer", "Rahul", ""], ["Sycara", "Katia", ""]]}, {"id": "1605.03956", "submitter": "Yingtao Tian", "authors": "Yingtao Tian, Vivek Kulkarni, Bryan Perozzi, Steven Skiena", "title": "On the Convergent Properties of Word Embedding Methods", "comments": "RepEval @ ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do word embeddings converge to learn similar things over different\ninitializations? How repeatable are experiments with word embeddings? Are all\nword embedding techniques equally reliable? In this paper we propose evaluating\nmethods for learning word representations by their consistency across\ninitializations. We propose a measure to quantify the similarity of the learned\nword representations under this setting (where they are subject to different\nrandom initializations). Our preliminary results illustrate that our metric not\nonly measures a intrinsic property of word embedding methods but also\ncorrelates well with other evaluation metrics on downstream tasks. We believe\nour methods are is useful in characterizing robustness -- an important property\nto consider when developing new word embedding methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:59:43 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Tian", "Yingtao", ""], ["Kulkarni", "Vivek", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1605.04002", "submitter": "Paul Tupper", "authors": "Paul Tupper and Bobak Shahriari", "title": "Which Learning Algorithms Can Generalize Identity-Based Rules to Novel\n  Inputs?", "comments": "6 pages, accepted abstract at COGSCI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for the analysis of learning algorithms that\nallows us to say when such algorithms can and cannot generalize certain\npatterns from training data to test data. In particular we focus on situations\nwhere the rule that must be learned concerns two components of a stimulus being\nidentical. We call such a basis for discrimination an identity-based rule.\nIdentity-based rules have proven to be difficult or impossible for certain\ntypes of learning algorithms to acquire from limited datasets. This is in\ncontrast to human behaviour on similar tasks. Here we provide a framework for\nrigorously establishing which learning algorithms will fail at generalizing\nidentity-based rules to novel stimuli. We use this framework to show that such\nalgorithms are unable to generalize identity-based rules to novel inputs unless\ntrained on virtually all possible inputs. We demonstrate these results\ncomputationally with a multilayer feedforward neural network.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 22:42:48 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Tupper", "Paul", ""], ["Shahriari", "Bobak", ""]]}, {"id": "1605.04013", "submitter": "EPTCS", "authors": "Stefano Gogioso (University of Oxford)", "title": "A Corpus-based Toy Model for DisCoCat", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 20-28", "doi": "10.4204/EPTCS.221.3", "report-no": null, "categories": "cs.CL cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional (DisCoCat) model of meaning\nrigorously connects distributional semantics and pregroup grammars, and has\nfound a variety of applications in computational linguistics. From a more\nabstract standpoint, the DisCoCat paradigm predicates the construction of a\nmapping from syntax to categorical semantics. In this work we present a\nconcrete construction of one such mapping, from a toy model of syntax for\ncorpora annotated with constituent structure trees, to categorical semantics\ntaking place in a category of free R-semimodules over an involutive commutative\nsemiring R.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 00:32:01 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 00:40:51 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Gogioso", "Stefano", "", "University of Oxford"]]}, {"id": "1605.04072", "submitter": "Pascale Fung Prof.", "authors": "Pascale Fung, Dario Bertero, Yan Wan, Anik Dey, Ricky Ho Yin Chan,\n  Farhad Bin Siddique, Yang Yang, Chien-Sheng Wu, Ruixi Lin", "title": "Towards Empathetic Human-Robot Interactions", "comments": "23 pages. Keynote at 17th International Conference on Intelligent\n  Text Processing and Computational Linguistics. To appear in Lecture Notes in\n  Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the late 1990s when speech companies began providing their\ncustomer-service software in the market, people have gotten used to speaking to\nmachines. As people interact more often with voice and gesture controlled\nmachines, they expect the machines to recognize different emotions, and\nunderstand other high level communication features such as humor, sarcasm and\nintention. In order to make such communication possible, the machines need an\nempathy module in them which can extract emotions from human speech and\nbehavior and can decide the correct response of the robot. Although research on\nempathetic robots is still in the early stage, we described our approach using\nsignal processing techniques, sentiment analysis and machine learning\nalgorithms to make robots that can \"understand\" human emotion. We propose Zara\nthe Supergirl as a prototype system of empathetic robots. It is a software\nbased virtual android, with an animated cartoon character to present itself on\nthe screen. She will get \"smarter\" and more empathetic through its deep\nlearning algorithms, and by gathering more data and learning from it. In this\npaper, we present our work so far in the areas of deep learning of emotion and\nsentiment recognition, as well as humor recognition. We hope to explore the\nfuture direction of android development and how it can help improve people's\nlives.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 07:31:50 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Fung", "Pascale", ""], ["Bertero", "Dario", ""], ["Wan", "Yan", ""], ["Dey", "Anik", ""], ["Chan", "Ricky Ho Yin", ""], ["Siddique", "Farhad Bin", ""], ["Yang", "Yang", ""], ["Wu", "Chien-Sheng", ""], ["Lin", "Ruixi", ""]]}, {"id": "1605.04122", "submitter": "Richard Moot", "authors": "Richard Moot (LaBRI), Christian Retor\\'e (TEXTE)", "title": "Natural Language Semantics and Computability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a reflexion on the computability of natural language semantics.\nIt does not contain a new model or new results in the formal semantics of\nnatural language: it is rather a computational analysis of the logical models\nand algorithms currently used in natural language semantics, defined as the\nmapping of a statement to logical formulas - formulas, because a statement can\nbe ambiguous. We argue that as long as possible world semantics is left out,\none can compute the semantic representation(s) of a given statement, including\naspects of lexical meaning. We also discuss the algorithmic complexity of this\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 10:46:22 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Moot", "Richard", "", "LaBRI"], ["Retor\u00e9", "Christian", "", "TEXTE"]]}, {"id": "1605.04227", "submitter": "Madhav Nimishakavi Mr", "authors": "Madhav Nimishakavi, Uday Singh Saini and Partha Talukdar", "title": "Relation Schema Induction using Tensor Factorization with Side\n  Information", "comments": "Proceedings of the 2016 Conference on Empirical Methods in Natural\n  Language Processing, November 2016. Austin, TX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of documents from a specific domain (e.g., medical research\njournals), how do we automatically build a Knowledge Graph (KG) for that\ndomain? Automatic identification of relations and their schemas, i.e., type\nsignature of arguments of relations (e.g., undergo(Patient, Surgery)), is an\nimportant first step towards this goal. We refer to this problem as Relation\nSchema Induction (RSI). In this paper, we propose Schema Induction using\nCoupled Tensor Factorization (SICTF), a novel tensor factorization method for\nrelation schema induction. SICTF factorizes Open Information Extraction\n(OpenIE) triples extracted from a domain corpus along with additional side\ninformation in a principled way to induce relation schemas. To the best of our\nknowledge, this is the first application of tensor factorization for the RSI\nproblem. Through extensive experiments on multiple real-world datasets, we find\nthat SICTF is not only more accurate than state-of-the-art baselines, but also\nsignificantly faster (about 14x faster).\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:44:04 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 04:57:09 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 04:53:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nimishakavi", "Madhav", ""], ["Saini", "Uday Singh", ""], ["Talukdar", "Partha", ""]]}, {"id": "1605.04238", "submitter": "Matilde Marcolli", "authors": "Yuri Manin and Matilde Marcolli", "title": "Semantic Spaces", "comments": "32 pages, TeX, 1 eps figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any natural language can be considered as a tool for producing large\ndatabases (consisting of texts, written, or discursive). This tool for its\ndescription in turn requires other large databases (dictionaries, grammars\netc.). Nowadays, the notion of database is associated with computer processing\nand computer memory. However, a natural language resides also in human brains\nand functions in human communication, from interpersonal to intergenerational\none. We discuss in this survey/research paper mathematical, in particular\ngeometric, constructions, which help to bridge these two worlds. In particular,\nin this paper we consider the Vector Space Model of semantics based on\nfrequency matrices, as used in Natural Language Processing. We investigate\nunderlying geometries, formulated in terms of Grassmannians, projective spaces,\nand flag varieties. We formulate the relation between vector space models and\nsemantic spaces based on semic axes in terms of projectability of subvarieties\nin Grassmannians and projective spaces. We interpret Latent Semantics as a\ngeometric flow on Grassmannians. We also discuss how to formulate G\\\"ardenfors'\nnotion of \"meeting of minds\" in our geometric setting.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 16:25:38 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Manin", "Yuri", ""], ["Marcolli", "Matilde", ""]]}, {"id": "1605.04278", "submitter": "Yevgeni Berzak", "authors": "Yevgeni Berzak, Jessica Kenney, Carolyn Spadine, Jing Xian Wang, Lucia\n  Lam, Keiko Sophie Mori, Sebastian Garza and Boris Katz", "title": "Universal Dependencies for Learner English", "comments": "Updated parsing experiments to EWT v1.3, improved grammatical error\n  marking, minor revisions. To appear in ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Treebank of Learner English (TLE), the first publicly\navailable syntactic treebank for English as a Second Language (ESL). The TLE\nprovides manually annotated POS tags and Universal Dependency (UD) trees for\n5,124 sentences from the Cambridge First Certificate in English (FCE) corpus.\nThe UD annotations are tied to a pre-existing error annotation of the FCE,\nwhereby full syntactic analyses are provided for both the original and error\ncorrected versions of each sentence. Further on, we delineate ESL annotation\nguidelines that allow for consistent syntactic treatment of ungrammatical\nEnglish. Finally, we benchmark POS tagging and dependency parsing performance\non the TLE dataset and measure the effect of grammatical errors on parsing\naccuracy. We envision the treebank to support a wide range of linguistic and\ncomputational research on second language acquisition as well as automatic\nprocessing of ungrammatical language. The treebank is available at\nuniversaldependencies.org. The annotation manual used in this project and a\ngraphical query engine are available at esltreebank.org.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 18:45:22 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 02:33:34 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Kenney", "Jessica", ""], ["Spadine", "Carolyn", ""], ["Wang", "Jing Xian", ""], ["Lam", "Lucia", ""], ["Mori", "Keiko Sophie", ""], ["Garza", "Sebastian", ""], ["Katz", "Boris", ""]]}, {"id": "1605.04359", "submitter": "Aman Madaan", "authors": "Aman Madaan, Sunita Sarawagi", "title": "Occurrence Statistics of Entities, Relations and Types on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of collecting reliable estimates of occurrence of entities on the\nopen web forms the premise for this report. The models learned for tagging\nentities cannot be expected to perform well when deployed on the web. This is\nowing to the severe mismatch in the distributions of such entities on the web\nand in the relatively diminutive training data. In this report, we build up the\ncase for maximum mean discrepancy for estimation of occurrence statistics of\nentities on the web, taking a review of named entity disambiguation techniques\nand related concepts along the way.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 01:13:48 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Madaan", "Aman", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "1605.04462", "submitter": "Tim Althoff", "authors": "Tim Althoff, Kevin Clark, Jure Leskovec", "title": "Large-scale Analysis of Counseling Conversations: An Application of\n  Natural Language Processing to Mental Health", "comments": "preprint of paper accepted to TACL, Transactions of the Association\n  for Computational Linguistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental illness is one of the most pressing public health issues of our time.\nWhile counseling and psychotherapy can be effective treatments, our knowledge\nabout how to conduct successful counseling conversations has been limited due\nto lack of large-scale data with labeled outcomes of the conversations. In this\npaper, we present a large-scale, quantitative study on the discourse of\ntext-message-based counseling conversations. We develop a set of novel\ncomputational discourse analysis methods to measure how various linguistic\naspects of conversations are correlated with conversation outcomes. Applying\ntechniques such as sequence-based conversation models, language model\ncomparisons, message clustering, and psycholinguistics-inspired word frequency\nanalyses, we discover actionable conversation strategies that are associated\nwith better conversation outcomes.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 20:02:05 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 16:55:12 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2016 20:45:55 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Althoff", "Tim", ""], ["Clark", "Kevin", ""], ["Leskovec", "Jure", ""]]}, {"id": "1605.04469", "submitter": "Ye Zhang", "authors": "Ye Zhang, Iain Marshall, Byron C. Wallace", "title": "Rationale-Augmented Convolutional Neural Networks for Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Convolutional Neural Network (CNN) model for text\nclassification that jointly exploits labels on documents and their component\nsentences. Specifically, we consider scenarios in which annotators explicitly\nmark sentences (or snippets) that support their overall document\ncategorization, i.e., they provide rationales. Our model exploits such\nsupervision via a hierarchical approach in which each document is represented\nby a linear combination of the vector representations of its component\nsentences. We propose a sentence-level convolutional model that estimates the\nprobability that a given sentence is a rationale, and we then scale the\ncontribution of each sentence to the aggregate document representation in\nproportion to these estimates. Experiments on five classification datasets that\nhave document labels and associated rationales demonstrate that our approach\nconsistently outperforms strong baselines. Moreover, our model naturally\nprovides explanations for its predictions.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 21:30:57 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 01:05:59 GMT"}, {"version": "v3", "created": "Sat, 24 Sep 2016 16:35:57 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Zhang", "Ye", ""], ["Marshall", "Iain", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1605.04475", "submitter": "Ryan Georgi", "authors": "Ryan Georgi and Fei Xia and William D. Lewis", "title": "Capturing divergence in dependency trees to improve syntactic projection", "comments": null, "journal-ref": null, "doi": "10.1007/s10579-014-9273-4", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining syntactic parses is a crucial part of many NLP pipelines. However,\nmost of the world's languages do not have large amounts of syntactically\nannotated corpora available for building parsers. Syntactic projection\ntechniques attempt to address this issue by using parallel corpora consisting\nof resource-poor and resource-rich language pairs, taking advantage of a parser\nfor the resource-rich language and word alignment between the languages to\nproject the parses onto the data for the resource-poor language. These\nprojection methods can suffer, however, when the two languages are divergent.\nIn this paper, we investigate the possibility of using small, parallel,\nannotated corpora to automatically detect divergent structural patterns between\ntwo languages. These patterns can then be used to improve structural projection\nalgorithms, allowing for better performing NLP tools for resource-poor\nlanguages, in particular those that may not have large amounts of annotated\ndata necessary for traditional, fully-supervised methods. While this detection\nprocess is not exhaustive, we demonstrate that common patterns of divergence\ncan be identified automatically without prior knowledge of a given language\npair, and the patterns can be used to improve performance of projection\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 22:11:07 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Georgi", "Ryan", ""], ["Xia", "Fei", ""], ["Lewis", "William D.", ""]]}, {"id": "1605.04481", "submitter": "Yevgeni Berzak", "authors": "Yevgeni Berzak, Yan Huang, Andrei Barbu, Anna Korhonen, Boris Katz", "title": "Anchoring and Agreement in Syntactic Annotations", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study on two key characteristics of human syntactic annotations:\nanchoring and agreement. Anchoring is a well known cognitive bias in human\ndecision making, where judgments are drawn towards pre-existing values. We\nstudy the influence of anchoring on a standard approach to creation of\nsyntactic resources where syntactic annotations are obtained via human editing\nof tagger and parser output. Our experiments demonstrate a clear anchoring\neffect and reveal unwanted consequences, including overestimation of parsing\nperformance and lower quality of annotations in comparison with human-based\nannotations. Using sentences from the Penn Treebank WSJ, we also report\nsystematically obtained inter-annotator agreement estimates for English\ndependency parsing. Our agreement results control for parser bias, and are\nconsequential in that they are on par with state of the art parsing performance\nfor English newswire. We discuss the impact of our findings on strategies for\nfuture annotation efforts and parser evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 00:26:26 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 16:49:41 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 22:34:47 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Huang", "Yan", ""], ["Barbu", "Andrei", ""], ["Korhonen", "Anna", ""], ["Katz", "Boris", ""]]}, {"id": "1605.04515", "submitter": "Lifeng Han", "authors": "Lifeng Han", "title": "Machine Translation Evaluation Resources and Methods: A Survey", "comments": "Accepted to present in \"Ireland Postgraduate Research Conference\n  2018\", Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Machine Translation (MT) evaluation survey that contains\nboth manual and automatic evaluation methods. The traditional human evaluation\ncriteria mainly include the intelligibility, fidelity, fluency, adequacy,\ncomprehension, and informativeness. The advanced human assessments include\ntask-oriented measures, post-editing, segment ranking, and extended criteriea,\netc. We classify the automatic evaluation methods into two categories,\nincluding lexical similarity scenario and linguistic features application. The\nlexical similarity methods contain edit distance, precision, recall, F-measure,\nand word order. The linguistic features can be divided into syntactic features\nand semantic features respectively. The syntactic features include part of\nspeech tag, phrase types and sentence structures, and the semantic features\ninclude named entity, synonyms, textual entailment, paraphrase, semantic roles,\nand language models. The deep learning models for evaluation are very newly\nproposed. Subsequently, we also introduce the evaluation methods for MT\nevaluation including different correlation scores, and the recent quality\nestimation (QE) tasks for MT.\n  This paper differs from the existing works\n\\cite{GALEprogram2009,EuroMatrixProject2007} from several aspects, by\nintroducing some recent development of MT evaluation measures, the different\nclassifications from manual to automatic evaluation measures, the introduction\nof recent QE tasks of MT, and the concise construction of the content.\n  We hope this work will be helpful for MT researchers to easily pick up some\nmetrics that are best suitable for their specific MT model development, and\nhelp MT evaluation researchers to get a general clue of how MT evaluation\nresearch developed. Furthermore, hopefully, this work can also shine some light\non other evaluation tasks, except for translation, of NLP fields.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 09:41:00 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 18:38:02 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 16:12:34 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 15:48:19 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 10:30:16 GMT"}, {"version": "v6", "created": "Sun, 19 Jun 2016 12:28:58 GMT"}, {"version": "v7", "created": "Tue, 10 Oct 2017 14:04:07 GMT"}, {"version": "v8", "created": "Wed, 19 Sep 2018 22:03:32 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Han", "Lifeng", ""]]}, {"id": "1605.04553", "submitter": "Dmitrijs Milajevs", "authors": "Dmitrijs Milajevs and Sascha Griffiths", "title": "A Proposal for Linguistic Similarity Datasets Based on Commonality Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity is a core notion that is used in psychology and two branches of\nlinguistics: theoretical and computational. The similarity datasets that come\nfrom the two fields differ in design: psychological datasets are focused around\na certain topic such as fruit names, while linguistic datasets contain words\nfrom various categories. The later makes humans assign low similarity scores to\nthe words that have nothing in common and to the words that have contrast in\nmeaning, making similarity scores ambiguous. In this work we discuss the\nsimilarity collection procedure for a multi-category dataset that avoids score\nambiguity and suggest changes to the evaluation procedure to reflect the\ninsights of psychological literature for word, phrase and sentence similarity.\nWe suggest to ask humans to provide a list of commonalities and differences\ninstead of numerical similarity scores and employ the structure of human\njudgements beyond pairwise similarity for model evaluation. We believe that the\nproposed approach will give rise to datasets that test meaning representation\nmodels more thoroughly with respect to the human treatment of similarity.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 14:00:06 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 16:55:20 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Milajevs", "Dmitrijs", ""], ["Griffiths", "Sascha", ""]]}, {"id": "1605.04569", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill Byrne", "title": "Syntactically Guided Neural Machine Translation", "comments": "ACL 2016", "journal-ref": null, "doi": "10.18653/v1/P16-2049", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of hierarchical phrase-based SMT lattices in\nend-to-end neural machine translation (NMT). Weight pushing transforms the\nHiero scores for complete translation hypotheses, with the full translation\ngrammar score and full n-gram language model score, into posteriors compatible\nwith NMT predictive probabilities. With a slightly modified NMT beam-search\ndecoder we find gains over both Hiero and NMT decoding alone, with practical\nadvantages in extending NMT to very large input and output vocabularies.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 15:53:02 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 08:52:37 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Stahlberg", "Felix", ""], ["Hasler", "Eva", ""], ["Waite", "Aurelien", ""], ["Byrne", "Bill", ""]]}, {"id": "1605.04655", "submitter": "Petr Baudi\\v{s}", "authors": "Petr Baudis, Silvestr Stanko and Jan Sedivy", "title": "Joint Learning of Sentence Embeddings for Relevance and Entailment", "comments": "repl4nlp workshop at ACL Berlin 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of Recognizing Textual Entailment within an\nInformation Retrieval context, where we must simultaneously determine the\nrelevancy as well as degree of entailment for individual pieces of evidence to\ndetermine a yes/no answer to a binary natural language question.\n  We compare several variants of neural networks for sentence embeddings in a\nsetting of decision-making based on evidence of varying relevance. We propose a\nbasic model to integrate evidence for entailment, show that joint training of\nthe sentence embeddings to model relevance and entailment is feasible even with\nno explicit per-evidence supervision, and show the importance of evaluating\nstrong baselines. We also demonstrate the benefit of carrying over text\ncomprehension model trained on an unrelated task for our small datasets.\n  Our research is motivated primarily by a new open dataset we introduce,\nconsisting of binary questions and news-based evidence snippets. We also apply\nthe proposed relevance-entailment model on a similar task of ranking\nmultiple-choice test answers, evaluating it on a preliminary dataset of school\ntest questions as well as the standard MCTest dataset, where we improve the\nneural model state-of-art.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 05:50:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 22:41:26 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Baudis", "Petr", ""], ["Stanko", "Silvestr", ""], ["Sedivy", "Jan", ""]]}, {"id": "1605.04800", "submitter": "Marcin Junczys-Dowmunt", "authors": "Marcin Junczys-Dowmunt and Roman Grundkiewicz", "title": "Log-linear Combinations of Monolingual and Bilingual Neural Machine\n  Translation Models for Automatic Post-Editing", "comments": "Submission to the WMT 2016 shared task on Automatic Post-Editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the submission of the AMU (Adam Mickiewicz University)\nteam to the Automatic Post-Editing (APE) task of WMT 2016. We explore the\napplication of neural translation models to the APE problem and achieve good\nresults by treating different models as components in a log-linear model,\nallowing for multiple inputs (the MT-output and the source) that are decoded to\nthe same target language (post-edited translations). A simple string-matching\npenalty integrated within the log-linear model is used to control for higher\nfaithfulness with regard to the raw machine translation output. To overcome the\nproblem of too little training data, we generate large amounts of artificial\ndata. Our submission improves over the uncorrected baseline on the unseen test\nset by -3.2\\% TER and +5.5\\% BLEU and outperforms any other system submitted to\nthe shared-task by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 15:15:05 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 13:15:50 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Junczys-Dowmunt", "Marcin", ""], ["Grundkiewicz", "Roman", ""]]}, {"id": "1605.04809", "submitter": "Marcin Junczys-Dowmunt", "authors": "Marcin Junczys-Dowmunt, Tomasz Dwojak, Rico Sennrich", "title": "The AMU-UEDIN Submission to the WMT16 News Translation Task:\n  Attention-based NMT Models as Feature Functions in Phrase-based SMT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on\nnews translation. We explore methods of decode-time integration of\nattention-based neural translation models with phrase-based statistical machine\ntranslation. Efficient batch-algorithms for GPU-querying are proposed and\nimplemented. For English-Russian, our system stays behind the state-of-the-art\npure neural models in terms of BLEU. Among restricted systems, manual\nevaluation places it in the first cluster tied with the pure neural model. For\nthe Russian-English task, our submission achieves the top BLEU result,\noutperforming the best pure neural system by 1.1 BLEU points and our own\nphrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the\nbest restricted system in its own cluster. In follow-up experiments we improve\nresults by additional 0.8 BLEU.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 15:34:19 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 12:15:46 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 13:22:46 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Junczys-Dowmunt", "Marcin", ""], ["Dwojak", "Tomasz", ""], ["Sennrich", "Rico", ""]]}, {"id": "1605.05087", "submitter": "Hirotaka Niitsuma", "authors": "Hirotaka Niitsuma and Minho Lee", "title": "Word2Vec is a special case of Kernel Correspondence Analysis and Kernels\n  for Natural Language Processing", "comments": "add expeiments and code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that correspondence analysis (CA) is equivalent to defining a Gini\nindex with appropriately scaled one-hot encoding. Using this relation, we\nintroduce a nonlinear kernel extension to CA. This extended CA gives a known\nanalysis for natural language via specialized kernels that use an appropriate\ncontingency table. We propose a semi-supervised CA, which is a special case of\nthe kernel extension to CA. Because CA requires excessive memory if applied to\nnumerous categories, CA has not been used for natural language processing. We\naddress this problem by introducing delayed evaluation to randomized singular\nvalue decomposition. The memory-efficient CA is then applied to a word-vector\nrepresentation task. We propose a tail-cut kernel, which is an extension to the\nskip-gram within the kernel extension to CA. Our tail-cut kernel outperforms\nexisting word-vector representation methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 10:07:34 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 19:13:22 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 17:49:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Niitsuma", "Hirotaka", ""], ["Lee", "Minho", ""]]}, {"id": "1605.05101", "submitter": "Xipeng Qiu", "authors": "Pengfei Liu, Xipeng Qiu, Xuanjing Huang", "title": "Recurrent Neural Network for Text Classification with Multi-Task\n  Learning", "comments": "IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based methods have obtained great progress on a variety of\nnatural language processing tasks. However, in most previous works, the models\nare learned based on single-task supervised objectives, which often suffer from\ninsufficient training data. In this paper, we use the multi-task learning\nframework to jointly learn across multiple related tasks. Based on recurrent\nneural network, we propose three different mechanisms of sharing information to\nmodel text with task-specific and shared layers. The entire network is trained\njointly on all these tasks. Experiments on four benchmark text classification\ntasks show that our proposed models can improve the performance of a task with\nthe help of other related tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 10:43:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Liu", "Pengfei", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1605.05110", "submitter": "Zhen Xu", "authors": "Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, Xiaolong Wang", "title": "Incorporating Loose-Structured Knowledge into Conversation Modeling via\n  Recall-Gate LSTM", "comments": "under review of IJCNN 2017; 10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling human conversations is the essence for building satisfying chat-bots\nwith multi-turn dialog ability. Conversation modeling will notably benefit from\ndomain knowledge since the relationships between sentences can be clarified due\nto semantic hints introduced by knowledge. In this paper, a deep neural network\nis proposed to incorporate background knowledge for conversation modeling.\nThrough a specially designed Recall gate, domain knowledge can be transformed\ninto the extra global memory of Long Short-Term Memory (LSTM), so as to enhance\nLSTM by cooperating with its local memory to capture the implicit semantic\nrelevance between sentences within conversations. In addition, this paper\nintroduces the loose structured domain knowledge base, which can be built with\nslight amount of manual work and easily adopted by the Recall gate. Our model\nis evaluated on the context-oriented response selecting task, and experimental\nresults on both two datasets have shown that our approach is promising for\nmodeling human conversations and building key components of automatic chatting\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:03:25 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 03:43:17 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Xu", "Zhen", ""], ["Liu", "Bingquan", ""], ["Wang", "Baoxun", ""], ["Sun", "Chengjie", ""], ["Wang", "Xiaolong", ""]]}, {"id": "1605.05134", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Deb Roy", "title": "A Semi-automatic Method for Efficient Detection of Stories on Social\n  Media", "comments": "ICWSM'16, May 17-20, Cologne, Germany. In Proceedings of the 10th\n  International AAAI Conference on Weblogs and Social Media (ICWSM 2016).\n  Cologne, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has become one of the main sources of news for many people. As\nreal-world events and emergencies unfold, Twitter is abuzz with hundreds of\nthousands of stories about the events. Some of these stories are harmless,\nwhile others could potentially be life-saving or sources of malicious rumors.\nThus, it is critically important to be able to efficiently track stories that\nspread on Twitter during these events. In this paper, we present a novel\nsemi-automatic tool that enables users to efficiently identify and track\nstories about real-world events on Twitter. We ran a user study with 25\nparticipants, demonstrating that compared to more conventional methods, our\ntool can increase the speed and the accuracy with which users can track stories\nabout real-world events.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 12:33:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05150", "submitter": "Soroush Vosoughi Dr", "authors": "Prashanth Vijayaraghavan, Soroush Vosoughi, Deb Roy", "title": "Automatic Detection and Categorization of Election-Related Tweets", "comments": "ICWSM'16, May 17-20, 2016, Cologne, Germany. In Proceedings of the\n  10th AAAI Conference on Weblogs and Social Media (ICWSM 2016). Cologne,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise in popularity of public social media and micro-blogging\nservices, most notably Twitter, the people have found a venue to hear and be\nheard by their peers without an intermediary. As a consequence, and aided by\nthe public nature of Twitter, political scientists now potentially have the\nmeans to analyse and understand the narratives that organically form, spread\nand decline among the public in a political campaign. However, the volume and\ndiversity of the conversation on Twitter, combined with its noisy and\nidiosyncratic nature, make this a hard task. Thus, advanced data mining and\nlanguage processing techniques are required to process and analyse the data. In\nthis paper, we present and evaluate a technical framework, based on recent\nadvances in deep neural networks, for identifying and analysing\nelection-related conversation on Twitter on a continuous, longitudinal basis.\nOur models can detect election-related tweets with an F-score of 0.92 and can\ncategorize these tweets into 22 topics with an F-score of 0.90.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 13:06:49 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vijayaraghavan", "Prashanth", ""], ["Vosoughi", "Soroush", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05156", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Deb Roy", "title": "Tweet Acts: A Speech Act Classifier for Twitter", "comments": "ICWSM'16, May 17-20, Cologne, Germany. In Proceedings of the 10th\n  AAAI Conference on Weblogs and Social Media (ICWSM 2016). Cologne, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech acts are a way to conceptualize speech as action. This holds true for\ncommunication on any platform, including social media platforms such as\nTwitter. In this paper, we explored speech act recognition on Twitter by\ntreating it as a multi-class classification problem. We created a taxonomy of\nsix speech acts for Twitter and proposed a set of semantic and syntactic\nfeatures. We trained and tested a logistic regression classifier using a data\nset of manually labelled tweets. Our method achieved a state-of-the-art\nperformance with an average F1 score of more than $0.70$. We also explored\nclassifiers with three different granularities (Twitter-wide, type-specific and\ntopic-specific) in order to find the right balance between generalization and\noverfitting for our task.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 13:31:14 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05166", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Helen Zhou, Deb Roy", "title": "Digital Stylometry: Linking Profiles Across Social Networks", "comments": "SocInfo'15, Beijing, China. In proceedings of the 7th International\n  Conference on Social Informatics (SocInfo 2015). Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ever growing number of users with accounts on multiple social\nmedia and networking sites. Consequently, there is increasing interest in\nmatching user accounts and profiles across different social networks in order\nto create aggregate profiles of users. In this paper, we present models for\nDigital Stylometry, which is a method for matching users through stylometry\ninspired techniques. We experimented with linguistic, temporal, and combined\ntemporal-linguistic models for matching user accounts, using standard and novel\ntechniques. Using publicly available data, our best model, a combined\ntemporal-linguistic one, was able to correctly match the accounts of 31% of\n5,612 distinct users across Twitter and Facebook.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 13:47:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Zhou", "Helen", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05172", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama", "title": "Siamese convolutional networks based on phonetic features for cognate\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we explore the use of convolutional networks (ConvNets) for\nthe purpose of cognate identification. We compare our architecture with binary\nclassifiers based on string similarity measures on different language families.\nOur experiments show that convolutional networks achieve competitive results\nacross concepts and across language families at the task of cognate\nidentification.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 14:07:43 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 12:29:08 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Rama", "Taraka", ""]]}, {"id": "1605.05195", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Helen Zhou, Deb Roy", "title": "Enhanced Twitter Sentiment Classification Using Contextual Information", "comments": "In proceedings of the 6th workshop on Computational Approaches to\n  Subjectivity, Sentiment & Social Media Analysis (WASSA) at EMNLP 2015", "journal-ref": null, "doi": "10.18653/v1/W15-2904", "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The rise in popularity and ubiquity of Twitter has made sentiment analysis of\ntweets an important and well-covered area of research. However, the 140\ncharacter limit imposed on tweets makes it hard to use standard linguistic\nmethods for sentiment classification. On the other hand, what tweets lack in\nstructure they make up with sheer volume and rich metadata. This metadata\nincludes geolocation, temporal and author information. We hypothesize that\nsentiment is dependent on all these contextual factors. Different locations,\ntimes and authors have different emotional valences. In this paper, we explored\nthis hypothesis by utilizing distant supervision to collect millions of\nlabelled tweets from different locations, times and authors. We used this data\nto analyse the variation of tweet sentiments across different authors, times\nand locations. Once we explored and understood the relationship between these\nvariables and sentiment, we used a Bayesian approach to combine these variables\nwith more standard linguistic features such as n-grams to create a Twitter\nsentiment classifier. This combined classifier outperforms the purely\nlinguistic classifier, showing that integrating the rich contextual information\navailable on Twitter into sentiment classification is a promising direction of\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 14:51:54 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 03:59:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Zhou", "Helen", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05303", "submitter": "Alejandro Ramos Soto", "authors": "A. Ramos-Soto, A. Bugar\\'in, S. Barro", "title": "Fuzzy Sets Across the Natural Language Generation Pipeline", "comments": "Paper features: 16 pages, 2 tables, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the implications of using fuzzy techniques (mainly those commonly\nused in the linguistic description/summarization of data discipline) from a\nnatural language generation perspective. For this, we provide an extensive\ndiscussion of some general convergence points and an exploration of the\nrelationship between the different tasks involved in the standard NLG system\npipeline architecture and the most common fuzzy approaches used in linguistic\nsummarization/description of data, such as fuzzy quantified statements,\nevaluation criteria or aggregation operators. Each individual discussion is\nillustrated with a related use case. Recent work made in the context of\ncross-fertilization of both research fields is also referenced. This paper\nencompasses general ideas that emerged as part of the PhD thesis \"Application\nof fuzzy sets in data-to-text systems\". It does not present a specific\napplication or a formal approach, but rather discusses current high-level\nissues and potential usages of fuzzy sets (focused on linguistic summarization\nof data) in natural language generation.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 19:45:49 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Ramos-Soto", "A.", ""], ["Bugar\u00edn", "A.", ""], ["Barro", "S.", ""]]}, {"id": "1605.05362", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar", "title": "Yelp Dataset Challenge: Review Rating Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review websites, such as TripAdvisor and Yelp, allow users to post online\nreviews for various businesses, products and services, and have been recently\nshown to have a significant influence on consumer shopping behaviour. An online\nreview typically consists of free-form text and a star rating out of 5. The\nproblem of predicting a user's star rating for a product, given the user's text\nreview for that product, is called Review Rating Prediction and has lately\nbecome a popular, albeit hard, problem in machine learning. In this paper, we\ntreat Review Rating Prediction as a multi-class classification problem, and\nbuild sixteen different prediction models by combining four feature extraction\nmethods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic\nIndexing, with four machine learning algorithms, (i) logistic regression, (ii)\nNaive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector\nClassification. We analyse the performance of each of these sixteen models to\ncome up with the best model for predicting the ratings from reviews. We use the\ndataset provided by Yelp for training and testing the models.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 20:52:33 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Asghar", "Nabiha", ""]]}, {"id": "1605.05414", "submitter": "Ryan Lowe T.", "authors": "Ryan Lowe, Iulian V. Serban, Mike Noseworthy, Laurent Charlin, Joelle\n  Pineau", "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "comments": "Accepted to SIGDIAL 2016 (short paper). 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open challenge in constructing dialogue systems is developing methods for\nautomatically learning dialogue strategies from large amounts of unlabelled\ndata. Recent work has proposed Next-Utterance-Classification (NUC) as a\nsurrogate task for building dialogue systems from text data. In this paper we\ninvestigate the performance of humans on this task to validate the relevance of\nNUC as a method of evaluation. Our results show three main findings: (1) humans\nare able to correctly classify responses at a rate much better than chance,\nthus confirming that the task is feasible, (2) human performance levels vary\nacross task domains (we consider 3 datasets) and expertise levels (novice vs\nexperts), thus showing that a range of performance is possible on this type of\ntask, (3) automated dialogue systems built using state-of-the-art machine\nlearning methods have similar performance to the human novices, but worse than\nthe experts, thus confirming the utility of this class of tasks for driving\nfurther research in automated dialogue systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 01:36:29 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 00:00:36 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Lowe", "Ryan", ""], ["Serban", "Iulian V.", ""], ["Noseworthy", "Mike", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1605.05416", "submitter": "Ryan Lowe T.", "authors": "Teng Long, Ryan Lowe, Jackie Chi Kit Cheung, Doina Precup", "title": "Leveraging Lexical Resources for Learning Entity Embeddings in\n  Multi-Relational Data", "comments": "6 pages. Accepted to ACL 2016 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in learning vector-space embeddings for multi-relational data has\nfocused on combining relational information derived from knowledge bases with\ndistributional information derived from large text corpora. We propose a simple\napproach that leverages the descriptions of entities or phrases available in\nlexical resources, in conjunction with distributional semantics, in order to\nderive a better initialization for training relational models. Applying this\ninitialization to the TransE model results in significant new state-of-the-art\nperformances on the WordNet dataset, decreasing the mean rank from the previous\nbest of 212 to 51. It also results in faster convergence of the entity\nrepresentations. We find that there is a trade-off between improving the mean\nrank and the hits@10 with this approach. This illustrates that much remains to\nbe understood regarding performance improvements in relational models.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 01:45:32 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Long", "Teng", ""], ["Lowe", "Ryan", ""], ["Cheung", "Jackie Chi Kit", ""], ["Precup", "Doina", ""]]}, {"id": "1605.05433", "submitter": "Stephen Roller", "authors": "Stephen Roller, Katrin Erk", "title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns\n  in Distributional Vectors for Lexical Entailment", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of predicting lexical entailment using distributional\nvectors. We perform a novel qualitative analysis of one existing model which\nwas previously shown to only measure the prototypicality of word pairs. We find\nthat the model strongly learns to identify hypernyms using Hearst patterns,\nwhich are well known to be predictive of lexical relations. We present a novel\nmodel which exploits this behavior as a method of feature extraction in an\niterative procedure similar to Principal Component Analysis. Our model combines\nthe extracted features with the strengths of other proposed models in the\nliterature, and matches or outperforms prior work on multiple data sets.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 04:10:41 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 20:31:51 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Roller", "Stephen", ""], ["Erk", "Katrin", ""]]}, {"id": "1605.05573", "submitter": "Xipeng Qiu", "authors": "Pengfei Liu, Xipeng Qiu, Xuanjing Huang", "title": "Modelling Interaction of Sentence Pair with coupled-LSTMs", "comments": "Submitted to IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is rising interest in modelling the interactions of two\nsentences with deep neural networks. However, most of the existing methods\nencode two sequences with separate encoders, in which a sentence is encoded\nwith little or no information from the other sentence. In this paper, we\npropose a deep architecture to model the strong interaction of sentence pair\nwith two coupled-LSTMs. Specifically, we introduce two coupled ways to model\nthe interdependences of two LSTMs, coupling the local contextualized\ninteractions of two sentences. We then aggregate these interactions and use a\ndynamic pooling to select the most informative features. Experiments on two\nvery large datasets demonstrate the efficacy of our proposed architecture and\nits superiority to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 13:33:21 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 01:28:43 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Liu", "Pengfei", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1605.05894", "submitter": "Muhammad Imran", "authors": "Muhammad Imran, Prasenjit Mitra, Carlos Castillo", "title": "Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of\n  Crisis-related Messages", "comments": "Accepted at the 10th Language Resources and Evaluation Conference\n  (LREC), 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging platforms such as Twitter provide active communication channels\nduring mass convergence and emergency events such as earthquakes, typhoons.\nDuring the sudden onset of a crisis situation, affected people post useful\ninformation on Twitter that can be used for situational awareness and other\nhumanitarian disaster response efforts, if processed timely and effectively.\nProcessing social media information pose multiple challenges such as parsing\nnoisy, brief and informal messages, learning information categories from the\nincoming stream of messages and classifying them into different classes among\nothers. One of the basic necessities of many of these tasks is the availability\nof data, in particular human-annotated data. In this paper, we present\nhuman-annotated Twitter corpora collected during 19 different crises that took\nplace between 2013 and 2015. To demonstrate the utility of the annotations, we\ntrain machine learning classifiers. Moreover, we publish first largest word2vec\nword embeddings trained on 52 million crisis-related tweets. To deal with\ntweets language issues, we present human-annotated normalized lexical resources\nfor different lexical variations.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 11:32:29 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 17:30:05 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Imran", "Muhammad", ""], ["Mitra", "Prasenjit", ""], ["Castillo", "Carlos", ""]]}, {"id": "1605.05906", "submitter": "Samuel L\\\"aubli", "authors": "Alena Zwahlen, Olivier Carnal, Samuel L\\\"aubli", "title": "Automatic TM Cleaning through MT and POS Tagging: Autodesk's Submission\n  to the NLP4TM 2016 Shared Task", "comments": "Presented at the 2nd Workshop on Natural Language Processing for\n  Translation Memories (NLP4TM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a machine learning based method to identify incorrect entries in\ntranslation memories. It extends previous work by Barbu (2015) through\nincorporating recall-based machine translation and part-of-speech-tagging\nfeatures. Our system ranked first in the Binary Classification (II) task for\ntwo out of three language pairs: English-Italian and English-Spanish.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 12:05:55 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Zwahlen", "Alena", ""], ["Carnal", "Olivier", ""], ["L\u00e4ubli", "Samuel", ""]]}, {"id": "1605.06069", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin,\n  Joelle Pineau, Aaron Courville, Yoshua Bengio", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating\n  Dialogues", "comments": "15 pages, 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential data often possesses a hierarchical structure with complex\ndependencies between subsequences, such as found between the utterances in a\ndialogue. In an effort to model this kind of generative process, we propose a\nneural network-based generative architecture, with latent stochastic variables\nthat span a variable number of time steps. We apply the proposed model to the\ntask of dialogue response generation and compare it with recent neural network\narchitectures. We evaluate the model performance through automatic evaluation\nmetrics and by carrying out a human evaluation. The experiments demonstrate\nthat our model improves upon recently proposed models and that the latent\nvariables facilitate the generation of long outputs and maintain the context.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 17:59:02 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 16:02:30 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 02:21:04 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Sordoni", "Alessandro", ""], ["Lowe", "Ryan", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1605.06083", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg", "title": "Stereotyping and Bias in the Flickr30K Dataset", "comments": "In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016),\n  pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An untested assumption behind the crowdsourced descriptions of the images in\nthe Flickr30K dataset (Young et al., 2014) is that they \"focus only on the\ninformation that can be obtained from the image alone\" (Hodosh et al., 2013, p.\n859). This paper presents some evidence against this assumption, and provides a\nlist of biases and unwarranted inferences that can be found in the Flickr30K\ndataset. Finally, it considers methods to find examples of these, and discusses\nhow we should deal with stereotype-driven descriptions in future applications.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 19:17:23 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["van Miltenburg", "Emiel", ""]]}, {"id": "1605.06304", "submitter": "Yang Lou Dr", "authors": "Yang Lou, Guanrong Chen, Zhengping Fan, Luna Xiang", "title": "Local communities obstruct global consensus: Naming game on\n  multi-local-world networks", "comments": "31 pages, 17 figures", "journal-ref": "Physica A 492 (2018) 1741-1752", "doi": "10.1016/j.physa.2017.11.094", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community structure is essential for social communications, where individuals\nbelonging to the same community are much more actively interacting and\ncommunicating with each other than those in different communities within the\nhuman society. Naming game, on the other hand, is a social communication model\nthat simulates the process of learning a name of an object within a community\nof humans, where the individuals can generally reach global consensus\nasymptotically through iterative pair-wise conversations. The underlying\nnetwork indicates the relationships among the individuals. In this paper, three\ntypical topologies, namely random-graph, small-world and scale-free networks,\nare employed, which are embedded with the multi-local-world community\nstructure, to study the naming game. Simulations show that 1) the convergence\nprocess to global consensus is getting slower as the community structure\nbecomes more prominent, and eventually might fail; 2) if the inter-community\nconnections are sufficiently dense, neither the number nor the size of the\ncommunities affects the convergence process; and 3) for different topologies\nwith the same average node-degree, local clustering of individuals obstruct or\nprohibit global consensus to take place. The results reveal the role of local\ncommunities in a global naming game in social network studies.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 11:52:01 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 11:02:51 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Lou", "Yang", ""], ["Chen", "Guanrong", ""], ["Fan", "Zhengping", ""], ["Xiang", "Luna", ""]]}, {"id": "1605.06319", "submitter": "Nikola Milo\\v{s}evi\\'c MSc", "authors": "Nikola Milosevic and Goran Nenadic", "title": "As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in\n  Serbian", "comments": "Phrase modelling, simile extraction, language resource building,\n  crowdsourcing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Similes are natural language expressions used to compare unlikely things,\nwhere the comparison is not taken literally. They are often used in everyday\ncommunication and are an important part of cultural heritage. Having an\nup-to-date corpus of similes is challenging, as they are constantly coined\nand/or adapted to the contemporary times. In this paper we present a\nmethodology for semi-automated collection of similes from the world wide web\nusing text mining techniques. We expanded an existing corpus of traditional\nsimiles (containing 333 similes) by collecting 446 additional expressions. We,\nalso, explore how crowdsourcing can be used to extract and curate new similes.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 12:20:27 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Milosevic", "Nikola", ""], ["Nenadic", "Goran", ""]]}, {"id": "1605.06353", "submitter": "Marcin Junczys-Dowmunt", "authors": "Marcin Junczys-Dowmunt and Roman Grundkiewicz", "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic\n  Grammatical Error Correction", "comments": "Accepted for publication at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study parameter tuning towards the M^2 metric, the standard\nmetric for automatic grammar error correction (GEC) tasks. After implementing\nM^2 as a scorer in the Moses tuning framework, we investigate interactions of\ndense and sparse features, different optimizers, and tuning strategies for the\nCoNLL-2014 shared task. We notice erratic behavior when optimizing sparse\nfeature weights with M^2 and offer partial solutions. We find that a bare-bones\nphrase-based SMT setup with task-specific parameter-tuning outperforms all\npreviously published results for the CoNLL-2014 test set by a large margin\n(46.37% M^2 over previously 41.75%, by an SMT system with neural features)\nwhile being trained on the same, publicly available data. Our newly introduced\ndense and sparse features widen that gap, and we improve the state-of-the-art\nto 49.49% M^2.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 13:43:56 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 08:42:23 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Junczys-Dowmunt", "Marcin", ""], ["Grundkiewicz", "Roman", ""]]}, {"id": "1605.06650", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Tengfei Liu, Leonard K.M. Poon, Zhourong\n  Chen and Farhan Khawar", "title": "Latent Tree Models for Hierarchical Topic Detection", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for hierarchical topic detection where topics are\nobtained by clustering documents in multiple ways. Specifically, we model\ndocument collections using a class of graphical models called hierarchical\nlatent tree models (HLTMs). The variables at the bottom level of an HLTM are\nobserved binary variables that represent the presence/absence of words in a\ndocument. The variables at other levels are binary latent variables, with those\nat the lowest latent level representing word co-occurrence patterns and those\nat higher levels representing co-occurrence of patterns at the level below.\nEach latent variable gives a soft partition of the documents, and document\nclusters in the partitions are interpreted as topics. Latent variables at high\nlevels of the hierarchy capture long-range word co-occurrence patterns and\nhence give thematically more general topics, while those at low levels of the\nhierarchy capture short-range word co-occurrence patterns and give thematically\nmore specific topics. Unlike LDA-based topic models, HLTMs do not refer to a\ndocument generation process and use word variables instead of token variables.\nThey use a tree structure to model the relationships between topics and words,\nwhich is conducive to the discovery of meaningful topics and topic hierarchies.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 14:36:33 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 08:59:14 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Liu", "Tengfei", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""], ["Khawar", "Farhan", ""]]}, {"id": "1605.06770", "submitter": "Longyue Wang", "authors": "Longyue Wang, Xiaojun Zhang, Zhaopeng Tu, Andy Way, Qun Liu", "title": "Automatic Construction of Discourse Corpora for Dialogue Translation", "comments": "7 pages, 3 figures, LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, a novel approach is proposed to automatically construct\nparallel discourse corpus for dialogue machine translation. Firstly, the\nparallel subtitle data and its corresponding monolingual movie script data are\ncrawled and collected from Internet. Then tags such as speaker and discourse\nboundary from the script data are projected to its subtitle data via an\ninformation retrieval approach in order to map monolingual discourse to\nbilingual texts. We not only evaluate the mapping results, but also integrate\nspeaker information into the translation. Experiments show our proposed method\ncan achieve 81.79% and 98.64% accuracy on speaker and dialogue boundary\nannotation, and speaker-based language model adaptation can obtain around 0.5\nBLEU points improvement in translation qualities. Finally, we publicly release\naround 100K parallel discourse data with manual speaker and dialogue boundary\nannotation.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 10:50:07 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Wang", "Longyue", ""], ["Zhang", "Xiaojun", ""], ["Tu", "Zhaopeng", ""], ["Way", "Andy", ""], ["Liu", "Qun", ""]]}, {"id": "1605.06778", "submitter": "Maximilian Schmitt", "authors": "Maximilian Schmitt and Bj\\\"orn W. Schuller", "title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words\n  Toolkit", "comments": "9 pages, 1 figure, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce openXBOW, an open-source toolkit for the generation of\nbag-of-words (BoW) representations from multimodal input. In the BoW principle,\nword histograms were first used as features in document classification, but the\nidea was and can easily be adapted to, e.g., acoustic or visual low-level\ndescriptors, introducing a prior step of vector quantisation. The openXBOW\ntoolkit supports arbitrary numeric input features and text input and\nconcatenates computed subbags to a final bag. It provides a variety of\nextensions and options. To our knowledge, openXBOW is the first publicly\navailable toolkit for the generation of crossmodal bags-of-words. The\ncapabilities of the tool are exemplified in two sample scenarios:\ntime-continuous speech-based emotion recognition and sentiment analysis in\ntweets where improved results over other feature representation forms were\nobserved.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 12:14:55 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Schmitt", "Maximilian", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "1605.06799", "submitter": "Victor Barger", "authors": "Andrea Webb Luangrath, Joann Peck, Victor A. Barger", "title": "Textual Paralanguage and its Implications for Marketing Communications", "comments": "Forthcoming in the Journal of Consumer Psychology", "journal-ref": "Journal of Consumer Psychology 27 (2017) 98-107", "doi": "10.1016/j.jcps.2016.05.002", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Both face-to-face communication and communication in online environments\nconvey information beyond the actual verbal message. In a traditional\nface-to-face conversation, paralanguage, or the ancillary meaning- and\nemotion-laden aspects of speech that are not actual verbal prose, gives\ncontextual information that allows interactors to more appropriately understand\nthe message being conveyed. In this paper, we conceptualize textual\nparalanguage (TPL), which we define as written manifestations of nonverbal\naudible, tactile, and visual elements that supplement or replace written\nlanguage and that can be expressed through words, symbols, images, punctuation,\ndemarcations, or any combination of these elements. We develop a typology of\ntextual paralanguage using data from Twitter, Facebook, and Instagram. We\npresent a conceptual framework of antecedents and consequences of brands' use\nof textual paralanguage. Implications for theory and practice are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 14:22:03 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Luangrath", "Andrea Webb", ""], ["Peck", "Joann", ""], ["Barger", "Victor A.", ""]]}, {"id": "1605.07133", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham and Marco Baroni", "title": "Towards Multi-Agent Communication-Based Language Learning", "comments": "9 pages, manuscript under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an interactive multimodal framework for language learning. Instead\nof being passively exposed to large amounts of natural text, our learners\n(implemented as feed-forward neural networks) engage in cooperative referential\ngames starting from a tabula rasa setup, and thus develop their own language\nfrom the need to communicate in order to succeed at the game. Preliminary\nexperiments provide promising results, but also suggest that it is important to\nensure that agents trained in this way do not develop an adhoc communication\ncode only effective for the game they are playing\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 18:46:46 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1605.07268", "submitter": "Matthieu Vernier", "authors": "Eliana Scheihing, Matthieu Vernier, Javiera Born, Julio Guerra, Luis\n  Carcamo", "title": "Classifying discourse in a CSCL platform to evaluate correlations with\n  Teacher Participation and Progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer-Supported learning, monitoring and engaging a group of learners\nis a complex task for teachers, especially when learners are working\ncollaboratively: Are my students motivated? What kind of progress are they\nmaking? Should I intervene? Is my communication and the didactic design adapted\nto my students? Our hypothesis is that the analysis of natural language\ninteractions between students, and between students and teachers, provide very\nvaluable information and could be used to produce qualitative indicators to\nhelp teachers' decisions. We develop an automatic approach in three steps (1)\nto explore the discursive functions of messages in a CSCL platform, (2) to\nclassify the messages automatically and (3) to evaluate correlations between\ndiscursive attitudes and other variables linked to the learning activity.\nResults tend to show that some types of discourse are correlated with a notion\nof Progress on the learning activities and the importance of emotive\nparticipation from the Teacher.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:39:26 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Scheihing", "Eliana", ""], ["Vernier", "Matthieu", ""], ["Born", "Javiera", ""], ["Guerra", "Julio", ""], ["Carcamo", "Luis", ""]]}, {"id": "1605.07333", "submitter": "Heike Adel", "authors": "Ngoc Thang Vu and Heike Adel and Pankaj Gupta and Hinrich Sch\\\"utze", "title": "Combining Recurrent and Convolutional Neural Networks for Relation\n  Classification", "comments": "NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates two different neural architectures for the task of\nrelation classification: convolutional neural networks and recurrent neural\nnetworks. For both models, we demonstrate the effect of different architectural\nchoices. We present a new context representation for convolutional neural\nnetworks for relation classification (extended middle context). Furthermore, we\npropose connectionist bi-directional recurrent neural networks and introduce\nranking loss for their optimization. Finally, we show that combining\nconvolutional and recurrent neural networks using a simple voting scheme is\naccurate enough to improve results. Our neural models achieve state-of-the-art\nresults on the SemEval 2010 relation classification task.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 08:20:12 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Vu", "Ngoc Thang", ""], ["Adel", "Heike", ""], ["Gupta", "Pankaj", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1605.07346", "submitter": "Abdelaziz Lakhfif", "authors": "Abdelaziz Lakhfif, Mohammed T. Laskri, Eric Atwell", "title": "Multi-Level Analysis and Annotation of Arabic Corpora for Text-to-Sign\n  Language MT", "comments": "Second Workshop on Arabic Corpus Linguistics (WACL-2), 22nd July\n  2013, Lancaster University, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an ongoing effort in lexical semantic analysis and\nannotation of Modern Standard Arabic (MSA) text, a semi automatic annotation\ntool concerned with the morphologic, syntactic, and semantic levels of\ndescription.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 09:19:05 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Lakhfif", "Abdelaziz", ""], ["Laskri", "Mohammed T.", ""], ["Atwell", "Eric", ""]]}, {"id": "1605.07366", "submitter": "Nikhilesh Bhatnagar", "authors": "Nikhilesh Bhatnagar, Radhika Mamidi", "title": "Experiments in Linear Template Combination using Genetic Algorithms", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Generation systems typically have two parts - strategic\n('what to say') and tactical ('how to say'). We present our experiments in\nbuilding an unsupervised corpus-driven template based tactical NLG system. We\nconsider templates as a sequence of words containing gaps. Our idea is based on\nthe observation that templates are grammatical locally (within their textual\nspan). We posit the construction of a sentence as a highly restricted sequence\nof such templates. This work is an attempt to explore the resulting search\nspace using Genetic Algorithms to arrive at acceptable solutions. We present a\nbaseline implementation of this approach which outputs gapped text.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 10:28:43 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Bhatnagar", "Nikhilesh", ""], ["Mamidi", "Radhika", ""]]}, {"id": "1605.07427", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald\n  Tesauro, Yoshua Bengio", "title": "Hierarchical Memory Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory networks are neural networks with an explicit memory component that\ncan be both read and written to by the network. The memory is often addressed\nin a soft way using a softmax function, making end-to-end training with\nbackpropagation possible. However, this is not computationally scalable for\napplications which require the network to read from extremely large memories.\nOn the other hand, it is well known that hard attention mechanisms based on\nreinforcement learning are challenging to train successfully. In this paper, we\nexplore a form of hierarchical memory network, which can be considered as a\nhybrid between hard and soft attention memory networks. The memory is organized\nin a hierarchical structure such that reading from it is done with less\ncomputation than soft attention over a flat memory, while also being easier to\ntrain than hard attention over a flat memory. Specifically, we propose to\nincorporate Maximum Inner Product Search (MIPS) in the training and inference\nprocedures for our hierarchical memory network. We explore the use of various\nstate-of-the art approximate MIPS techniques and report results on\nSimpleQuestions, a challenging large scale factoid question answering task.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:48:19 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Chandar", "Sarath", ""], ["Ahn", "Sungjin", ""], ["Larochelle", "Hugo", ""], ["Vincent", "Pascal", ""], ["Tesauro", "Gerald", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1605.07515", "submitter": "Michael Roth", "authors": "Michael Roth, Mirella Lapata", "title": "Neural Semantic Role Labeling with Dependency Path Embeddings", "comments": "Camera-ready ACL paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel model for semantic role labeling that makes use\nof neural sequence modeling techniques. Our approach is motivated by the\nobservation that complex syntactic structures and related phenomena, such as\nnested subordinations and nominal predicates, are not handled well by existing\nmodels. Our model treats such instances as sub-sequences of lexicalized\ndependency paths and learns suitable embedding representations. We\nexperimentally demonstrate that such embeddings can improve results over\nprevious state-of-the-art semantic role labelers, and showcase qualitative\nimprovements obtained by our method.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 15:54:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 09:08:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Roth", "Michael", ""], ["Lapata", "Mirella", ""]]}, {"id": "1605.07669", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su and Milica Gasic and Nikola Mrksic and Lina Rojas-Barahona\n  and Stefan Ultes and David Vandyke and Tsung-Hsien Wen and Steve Young", "title": "On-line Active Reward Learning for Policy Optimisation in Spoken\n  Dialogue Systems", "comments": "Accepted as a long paper in ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to compute an accurate reward function is essential for\noptimising a dialogue policy via reinforcement learning. In real-world\napplications, using explicit user feedback as the reward signal is often\nunreliable and costly to collect. This problem can be mitigated if the user's\nintent is known in advance or data is available to pre-train a task success\npredictor off-line. In practice neither of these apply for most real world\napplications. Here we propose an on-line learning framework whereby the\ndialogue policy is jointly trained alongside the reward model via active\nlearning with a Gaussian process model. This Gaussian process operates on a\ncontinuous space dialogue representation generated in an unsupervised fashion\nusing a recurrent neural network encoder-decoder. The experimental results\ndemonstrate that the proposed framework is able to significantly reduce data\nannotation costs and mitigate noisy user feedback in dialogue policy learning.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 21:56:08 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 14:01:07 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Su", "Pei-Hao", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Rojas-Barahona", "Lina", ""], ["Ultes", "Stefan", ""], ["Vandyke", "David", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1605.07683", "submitter": "Antoine Bordes", "authors": "Antoine Bordes and Y-Lan Boureau and Jason Weston", "title": "Learning End-to-End Goal-Oriented Dialog", "comments": "Accepted as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional dialog systems used in goal-oriented applications require a lot\nof domain-specific handcrafting, which hinders scaling up to new domains.\nEnd-to-end dialog systems, in which all components are trained from the dialogs\nthemselves, escape this limitation. But the encouraging success recently\nobtained in chit-chat dialog may not carry over to goal-oriented settings. This\npaper proposes a testbed to break down the strengths and shortcomings of\nend-to-end dialog systems in goal-oriented applications. Set in the context of\nrestaurant reservation, our tasks require manipulating sentences and symbols,\nso as to properly conduct conversations, issue API calls and use the outputs of\nsuch calls. We show that an end-to-end dialog system based on Memory Networks\ncan reach promising, yet imperfect, performance and learn to perform\nnon-trivial operations. We confirm those results by comparing our system to a\nhand-crafted slot-filling baseline on data from the second Dialog State\nTracking Challenge (Henderson et al., 2014a). We show similar result patterns\non data extracted from an online concierge service.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 23:09:58 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 20:47:49 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 00:11:20 GMT"}, {"version": "v4", "created": "Thu, 30 Mar 2017 23:02:22 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Bordes", "Antoine", ""], ["Boureau", "Y-Lan", ""], ["Weston", "Jason", ""]]}, {"id": "1605.07733", "submitter": "Radoslava Kraleva Dr.", "authors": "Radoslava Kraleva, Velin Kralev", "title": "On model architecture for a children's speech recognition interactive\n  dialog system", "comments": "6 pages, 2 figures, in proc. of conference FMNS 2009, Blagoevgrad,\n  Bulgaria", "journal-ref": "Third International Scientific Conference \"Mathematics and Natural\n  Sciences\", Vol. (1), pp. 106-111, 2009", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents a general model of the architecture of information\nsystems for the speech recognition of children. It presents a model of the\nspeech data stream and how it works. The result of these studies and presented\nveins architectural model shows that research needs to be focused on\nacoustic-phonetic modeling in order to improve the quality of children's speech\nrecognition and the sustainability of the systems to noise and changes in\ntransmission environment. Another important aspect is the development of more\naccurate algorithms for modeling of spontaneous child speech.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:57:42 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kraleva", "Radoslava", ""], ["Kralev", "Velin", ""]]}, {"id": "1605.07735", "submitter": "Radoslava Kraleva Dr.", "authors": "Radoslava Kraleva", "title": "Design and development a children's speech database", "comments": "8 pages, 2 figures, 1 table, conference FMNS 2011, Blagoevgrad,\n  Bulgaria", "journal-ref": "Fourth International Scientific Conference \"Mathematics and\n  Natural Sciences\" 2011, Bulgaria, Vol. (2), pp. 41-48", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The report presents the process of planning, designing and the development of\na database of spoken children's speech whose native language is Bulgarian. The\nproposed model is designed for children between the age of 4 and 6 without\nspeech disorders, and reflects their specific capabilities. At this age most\nchildren cannot read, there is no sustained concentration, they are emotional,\netc. The aim is to unite all the media information accompanying the recording\nand processing of spoken speech, thereby to facilitate the work of researchers\nin the field of speech recognition. This database will be used for the\ndevelopment of systems for children's speech recognition, children's speech\nsynthesis systems, games which allow voice control, etc. As a result of the\nproposed model a prototype system for speech recognition is presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 05:04:11 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kraleva", "Radoslava", ""]]}, {"id": "1605.07766", "submitter": "Kim Anh Nguyen", "authors": "Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu", "title": "Integrating Distributional Lexical Contrast into Word Embeddings for\n  Antonym-Synonym Distinction", "comments": "6 pages, 4 figures, InProc ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel vector representation that integrates lexical contrast\ninto distributional vectors and strengthens the most salient features for\ndetermining degrees of word similarity. The improved vectors significantly\noutperform standard models and distinguish antonyms from synonyms with an\naverage precision of 0.66-0.76 across word classes (adjectives, nouns, verbs).\nMoreover, we integrate the lexical contrast vectors into the objective function\nof a skip-gram model. The novel embedding outperforms state-of-the-art models\non predicting word similarities in SimLex-999, and on distinguishing antonyms\nfrom synonyms.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 08:05:37 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Nguyen", "Kim Anh", ""], ["Walde", "Sabine Schulte im", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1605.07843", "submitter": "Yichun Yin", "authors": "Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, Ming Zhou", "title": "Unsupervised Word and Dependency Path Embeddings for Aspect Term\n  Extraction", "comments": "IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a novel approach to aspect term extraction based on\nunsupervised learning of distributed representations of words and dependency\npaths. The basic idea is to connect two words (w1 and w2) with the dependency\npath (r) between them in the embedding space. Specifically, our method\noptimizes the objective w1 + r = w2 in the low-dimensional space, where the\nmulti-hop dependency paths are treated as a sequence of grammatical relations\nand modeled by a recurrent neural network. Then, we design the embedding\nfeatures that consider linear context and dependency context information, for\nthe conditional random field (CRF) based aspect term extraction. Experimental\nresults on the SemEval datasets show that, (1) with only embedding features, we\ncan achieve state-of-the-art results; (2) our embedding method which\nincorporates the syntactic information among words yields better performance\nthan other representative ones in aspect term extraction.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:01:46 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Yin", "Yichun", ""], ["Wei", "Furu", ""], ["Dong", "Li", ""], ["Xu", "Kaimeng", ""], ["Zhang", "Ming", ""], ["Zhou", "Ming", ""]]}, {"id": "1605.07844", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi, Mahsa S. Shahshahani, Amirhossein Tebbifakhr,\n  Heshaam Faili, and Azadeh Shakery", "title": "Dimension Projection among Languages based on Pseudo-relevant Documents\n  for Query Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using top-ranked documents in response to a query has been shown to be an\neffective approach to improve the quality of query translation in\ndictionary-based cross-language information retrieval. In this paper, we\npropose a new method for dictionary-based query translation based on dimension\nprojection of embedded vectors from the pseudo-relevant documents in the source\nlanguage to their equivalents in the target language. To this end, first we\nlearn low-dimensional vectors of the words in the pseudo-relevant collections\nseparately and then aim to find a query-dependent transformation matrix between\nthe vectors of translation pairs appeared in the collections. At the next step,\nrepresentation of each query term is projected to the target language and then,\nafter using a softmax function, a query-dependent translation model is built.\nFinally, the model is used for query translation. Our experiments on four CLEF\ncollections in French, Spanish, German, and Italian demonstrate that the\nproposed method outperforms a word embedding baseline based on bilingual\nshuffling and a further number of competitive baselines. The proposed method\nreaches up to 87% performance of machine translation (MT) in short queries and\nconsiderable improvements in verbose queries.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:04:43 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 11:19:10 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Shahshahani", "Mahsa S.", ""], ["Tebbifakhr", "Amirhossein", ""], ["Faili", "Heshaam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1605.07852", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi, Hossein Nasr Esfahani, Heshaam Faili, and Azadeh\n  Shakery", "title": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been multiple attempts to resolve various inflection matching\nproblems in information retrieval. Stemming is a common approach to this end.\nAmong many techniques for stemming, statistical stemming has been shown to be\neffective in a number of languages, particularly highly inflected languages. In\nthis paper we propose a method for finding affixes in different positions of a\nword. Common statistical techniques heavily rely on string similarity in terms\nof prefix and suffix matching. Since infixes are common in irregular/informal\ninflections in morphologically complex texts, it is required to find infixes\nfor stemming. In this paper we propose a method whose aim is to find\nstatistical inflectional rules based on minimum edit distance table of word\npairs and the likelihoods of the rules in a language. These rules are used to\nstatistically stem words and can be used in different text mining tasks.\nExperimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks\nindicate that the proposed method significantly outperforms all the baselines\nin terms of MAP.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:25:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 21:37:19 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Esfahani", "Hossein Nasr", ""], ["Faili", "Heshaam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1605.07869", "submitter": "Biao Zhang", "authors": "Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, Min Zhang", "title": "Variational Neural Machine Translation", "comments": "10 pages, accepted at emnlp 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of neural machine translation are often from a discriminative family\nof encoderdecoders that learn a conditional distribution of a target sentence\ngiven a source sentence. In this paper, we propose a variational model to learn\nthis conditional distribution for neural machine translation: a variational\nencoderdecoder model that can be trained end-to-end. Different from the vanilla\nencoder-decoder model that generates target translations from hidden\nrepresentations of source sentences alone, the variational model introduces a\ncontinuous latent variable to explicitly model underlying semantics of source\nsentences and to guide the generation of target translations. In order to\nperform efficient posterior inference and large-scale training, we build a\nneural posterior approximator conditioned on both the source and the target\nsides, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on both Chinese-English and English-\nGerman translation tasks show that the proposed variational neural machine\ntranslation achieves significant improvements over the vanilla neural machine\ntranslation baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 13:18:57 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 23:37:14 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Zhang", "Biao", ""], ["Xiong", "Deyi", ""], ["Su", "Jinsong", ""], ["Duan", "Hong", ""], ["Zhang", "Min", ""]]}, {"id": "1605.07874", "submitter": "Biao Zhang", "authors": "Biao Zhang, Deyi Xiong and Jinsong Su", "title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for\n  Learning Bilingual Phrase Embeddings", "comments": "7 pages, accepted by AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a bidimensional attention based recursive\nautoencoder (BattRAE) to integrate clues and sourcetarget interactions at\nmultiple levels of granularity into bilingual phrase representations. We employ\nrecursive autoencoders to generate tree structures of phrases with embeddings\nat different levels of granularity (e.g., words, sub-phrases and phrases). Over\nthese embeddings on the source and target side, we introduce a bidimensional\nattention network to learn their interactions encoded in a bidimensional\nattention matrix, from which we extract two soft attention weight distributions\nsimultaneously. These weight distributions enable BattRAE to generate\ncompositive phrase representations via convolution. Based on the learned phrase\nrepresentations, we further use a bilinear neural model, trained via a\nmax-margin method, to measure bilingual semantic similarity. To evaluate the\neffectiveness of BattRAE, we incorporate this semantic similarity as an\nadditional feature into a state-of-the-art SMT system. Extensive experiments on\nNIST Chinese-English test sets show that our model achieves a substantial\nimprovement of up to 1.63 BLEU points on average over the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 13:29:07 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 03:26:45 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Zhang", "Biao", ""], ["Xiong", "Deyi", ""], ["Su", "Jinsong", ""]]}, {"id": "1605.07891", "submitter": "Fernando Diaz", "authors": "Fernando Diaz, Bhaskar Mitra, Nick Craswell", "title": "Query Expansion with Locally-Trained Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous space word embeddings have received a great deal of attention in\nthe natural language processing and machine learning communities for their\nability to model term similarity and other relationships. We study the use of\nterm relatedness in the context of query expansion for ad hoc information\nretrieval. We demonstrate that word embeddings such as word2vec and GloVe, when\ntrained globally, underperform corpus and query specific embeddings for\nretrieval tasks. These results suggest that other tasks benefiting from global\nembeddings may also benefit from local embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:09:00 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:46:06 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Diaz", "Fernando", ""], ["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""]]}, {"id": "1605.07895", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar", "title": "Automatic Extraction of Causal Relations from Natural Language Texts: A\n  Comprehensive Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of cause-effect relationships from natural language\ntexts is a challenging open problem in Artificial Intelligence. Most of the\nearly attempts at its solution used manually constructed linguistic and\nsyntactic rules on small and domain-specific data sets. However, with the\nadvent of big data, the availability of affordable computing power and the\nrecent popularization of machine learning, the paradigm to tackle this problem\nhas slowly shifted. Machines are now expected to learn generic causal\nextraction rules from labelled data with minimal supervision, in a domain\nindependent-manner. In this paper, we provide a comprehensive survey of causal\nrelation extraction techniques from both paradigms, and analyse their relative\nstrengths and weaknesses, with recommendations for future work.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:23:21 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Asghar", "Nabiha", ""]]}, {"id": "1605.07912", "submitter": "Zhilin Yang", "authors": "Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W.\n  Cohen", "title": "Review Networks for Caption Generation", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel extension of the encoder-decoder framework, called a\nreview network. The review network is generic and can enhance any existing\nencoder- decoder model: in this paper, we consider RNN decoders with both CNN\nand RNN encoders. The review network performs a number of review steps with\nattention mechanism on the encoder hidden states, and outputs a thought vector\nafter each review step; the thought vectors are used as the input of the\nattention mechanism in the decoder. We show that conventional encoder-decoders\nare a special case of our framework. Empirically, we show that our framework\nimproves over state-of- the-art encoder-decoder systems on the tasks of image\ncaptioning and source code captioning.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:49:58 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 00:47:21 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 01:39:35 GMT"}, {"version": "v4", "created": "Thu, 27 Oct 2016 17:50:27 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Yang", "Zhilin", ""], ["Yuan", "Ye", ""], ["Wu", "Yuexin", ""], ["Salakhutdinov", "Ruslan", ""], ["Cohen", "William W.", ""]]}, {"id": "1605.07918", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim, Hwanjo Yu, Gary Geunbae Lee", "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks\n  with Feedback Negative Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies in Open Information Extraction (Open IE) are mainly based on\nextraction patterns. They manually define patterns or automatically learn them\nfrom a large corpus. However, these approaches are limited when grasping the\ncontext of a sentence, and they fail to capture implicit relations. In this\npaper, we address this problem with the following methods. First, we exploit\nlong short-term memory (LSTM) networks to extract higher-level features along\nthe shortest dependency paths, connecting headwords of relations and arguments.\nThe path-level features from LSTM networks provide useful clues regarding\ncontextual information and the validity of arguments. Second, we constructed\nsamples to train LSTM networks without the need for manual labeling. In\nparticular, feedback negative sampling picks highly negative samples among\nnon-positive samples through a model trained with positive samples. The\nexperimental results show that our approach produces more precise and abundant\nextractions than state-of-the-art open IE systems. To the best of our\nknowledge, this is the first work to apply deep learning to Open IE.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:59:46 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kim", "Byungsoo", ""], ["Yu", "Hwanjo", ""], ["Lee", "Gary Geunbae", ""]]}, {"id": "1605.08535", "submitter": "Xiaodong Gu", "authors": "Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim", "title": "Deep API Learning", "comments": "The paper is accepted at FSE 2016 (the 24th ACM SIGSOFT International\n  Symposium on the Foundations of Software Engineering)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers often wonder how to implement a certain functionality (e.g., how\nto parse XML files) using APIs. Obtaining an API usage sequence based on an\nAPI-related natural language query is very helpful in this regard. Given a\nquery, existing approaches utilize information retrieval models to search for\nmatching API sequences. These approaches treat queries and APIs as bag-of-words\n(i.e., keyword matching or word-to-word alignment) and lack a deep\nunderstanding of the semantics of the query.\n  We propose DeepAPI, a deep learning based approach to generate API usage\nsequences for a given natural language query. Instead of a bags-of-words\nassumption, it learns the sequence of words in a query and the sequence of\nassociated APIs. DeepAPI adapts a neural language model named RNN\nEncoder-Decoder. It encodes a word sequence (user query) into a fixed-length\ncontext vector, and generates an API sequence based on the context vector. We\nalso augment the RNN Encoder-Decoder by considering the importance of\nindividual APIs. We empirically evaluate our approach with more than 7 million\nannotated code snippets collected from GitHub. The results show that our\napproach generates largely accurate API sequences and outperforms the related\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 08:27:18 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 09:38:35 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 01:22:18 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Gu", "Xiaodong", ""], ["Zhang", "Hongyu", ""], ["Zhang", "Dongmei", ""], ["Kim", "Sunghun", ""]]}, {"id": "1605.08675", "submitter": "Piotr Przyby{\\l}a", "authors": "Piotr Przyby{\\l}a", "title": "Boosting Question Answering by Deep Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an open-domain factoid question answering system for Polish,\nRAFAEL, is presented. The system goes beyond finding an answering sentence; it\nalso extracts a single string, corresponding to the required entity. Herein the\nfocus is placed on different approaches to entity recognition, essential for\nretrieving information matching question constraints. Apart from traditional\napproach, including named entity recognition (NER) solutions, a novel\ntechnique, called Deep Entity Recognition (DeepER), is introduced and\nimplemented. It allows a comprehensive search of all forms of entity references\nmatching a given WordNet synset (e.g. an impressionist), based on a previously\nassembled entity library. It has been created by analysing the first sentences\nof encyclopaedia entries and disambiguation and redirect pages. DeepER also\nprovides automatic evaluation, which makes possible numerous experiments,\nincluding over a thousand questions from a quiz TV show answered on the grounds\nof Polish Wikipedia. The final results of a manual evaluation on a separate\nquestion set show that the strength of DeepER approach lies in its ability to\nanswer questions that demand answers beyond the traditional categories of named\nentities.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 14:57:37 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Przyby\u0142a", "Piotr", ""]]}, {"id": "1605.08764", "submitter": "Nazneen Fatema Rajani", "authors": "Nazneen Fatema Rajani and Raymond J. Mooney", "title": "Stacking With Auxiliary Features", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.04802", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling methods are well known for improving prediction accuracy. However,\nthey are limited in the sense that they cannot discriminate among component\nmodels effectively. In this paper, we propose stacking with auxiliary features\nthat learns to fuse relevant information from multiple systems to improve\nperformance. Auxiliary features enable the stacker to rely on systems that not\njust agree on an output but also the provenance of the output. We demonstrate\nour approach on three very different and difficult problems -- the Cold Start\nSlot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet\nobject detection tasks. We obtain new state-of-the-art results on the first two\ntasks and substantial improvements on the detection task, thus verifying the\npower and generality of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 19:31:54 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Rajani", "Nazneen Fatema", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1605.08889", "submitter": "John Lalor", "authors": "John P. Lalor, Hao Wu, Hong Yu", "title": "Building an Evaluation Scale using Item Response Theory", "comments": "To appear in the proceedings of EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of NLP methods requires testing against a previously vetted\ngold-standard test set and reporting standard metrics\n(accuracy/precision/recall/F1). The current assumption is that all items in a\ngiven test set are equal with regards to difficulty and discriminating power.\nWe propose Item Response Theory (IRT) from psychometrics as an alternative\nmeans for gold-standard test-set generation and NLP system evaluation. IRT is\nable to describe characteristics of individual items - their difficulty and\ndiscriminating power - and can account for these characteristics in its\nestimation of human intelligence or ability for an NLP task. In this paper, we\ndemonstrate IRT by generating a gold-standard test set for Recognizing Textual\nEntailment. By collecting a large number of human responses and fitting our IRT\nmodel, we show that our IRT model compares NLP systems with the performance in\na human population and is able to provide more insight into system performance\nthan standard evaluation metrics. We show that a high accuracy score does not\nalways imply a high IRT score, which depends on the item characteristics and\nthe response pattern.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 13:19:15 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 16:35:16 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Lalor", "John P.", ""], ["Wu", "Hao", ""], ["Yu", "Hong", ""]]}, {"id": "1605.08900", "submitter": "Duyu Tang", "authors": "Duyu Tang, Bing Qin, Ting Liu", "title": "Aspect Level Sentiment Classification with Deep Memory Network", "comments": "published in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep memory network for aspect level sentiment classification.\nUnlike feature-based SVM and sequential neural models such as LSTM, this\napproach explicitly captures the importance of each context word when inferring\nthe sentiment polarity of an aspect. Such importance degree and text\nrepresentation are calculated with multiple computational layers, each of which\nis a neural attention model over an external memory. Experiments on laptop and\nrestaurant datasets demonstrate that our approach performs comparable to\nstate-of-art feature based SVM system, and substantially better than LSTM and\nattention-based LSTM architectures. On both datasets we show that multiple\ncomputational layers could improve the performance. Moreover, our approach is\nalso fast. The deep memory network with 9 layers is 15 times faster than LSTM\nwith a CPU implementation.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 14:47:49 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 06:04:15 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Tang", "Duyu", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""]]}, {"id": "1605.09090", "submitter": "Yang Liu", "authors": "Yang Liu, Chengjie Sun, Lei Lin and Xiaolong Wang", "title": "Learning Natural Language Inference using Bidirectional LSTM model and\n  Inner-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a sentence encoding-based model for recognizing\ntext entailment. In our approach, the encoding of sentence is a two-stage\nprocess. Firstly, average pooling was used over word-level bidirectional LSTM\n(biLSTM) to generate a first-stage sentence representation. Secondly, attention\nmechanism was employed to replace average pooling on the same sentence for\nbetter representations. Instead of using target sentence to attend words in\nsource sentence, we utilized the sentence's first-stage representation to\nattend words appeared in itself, which is called \"Inner-Attention\" in our paper\n. Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus\nhas proved the effectiveness of \"Inner-Attention\" mechanism. With less number\nof parameters, our model outperformed the existing best sentence encoding-based\napproach by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 02:47:35 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Liu", "Yang", ""], ["Sun", "Chengjie", ""], ["Lin", "Lei", ""], ["Wang", "Xiaolong", ""]]}, {"id": "1605.09096", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Jure Leskovec, Dan Jurafsky", "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "comments": "Association for Computational Linguistics (ACL), 2016. Minor\n  corrections; improved methodology for Section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how words change their meanings over time is key to models of\nlanguage and cultural evolution, but historical data on meaning is scarce,\nmaking theories hard to develop and test. Word embeddings show promise as a\ndiachronic tool, but have not been carefully evaluated. We develop a robust\nmethodology for quantifying semantic change by evaluating word embeddings\n(PPMI, SVD, word2vec) against known historical changes. We then use this\nmethodology to reveal statistical laws of semantic evolution. Using six\nhistorical corpora spanning four languages and two centuries, we propose two\nquantitative laws of semantic change: (i) the law of conformity---the rate of\nsemantic change scales with an inverse power-law of word frequency; (ii) the\nlaw of innovation---independent of frequency, words that are more polysemous\nhave higher rates of semantic change.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 03:54:18 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 20:24:30 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 09:36:39 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 23:35:05 GMT"}, {"version": "v5", "created": "Mon, 10 Sep 2018 14:25:18 GMT"}, {"version": "v6", "created": "Thu, 25 Oct 2018 17:34:43 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1605.09186", "submitter": "Ozan \\c{C}a\\u{g}layan", "authors": "Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes\n  Garc\\'ia-Mart\\'inez, Fethi Bougares, Lo\\\"ic Barrault, Joost van de Weijer", "title": "Does Multimodality Help Human and Machine for Translation and Image\n  Captioning?", "comments": "7 pages, 2 figures, v4: Small clarification in section 4 title and\n  content", "journal-ref": null, "doi": "10.18653/v1/W16-2358", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the systems developed by LIUM and CVC for the WMT16\nMultimodal Machine Translation challenge. We explored various comparative\nmethods, namely phrase-based systems and attentional recurrent neural networks\nmodels trained using monomodal or multimodal data. We also performed a human\nevaluation in order to estimate the usefulness of multimodal data for human\nmachine translation and image description generation. Our systems obtained the\nbest results for both tasks according to the automatic evaluation metrics BLEU\nand METEOR.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 11:47:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 13:52:45 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 15:33:11 GMT"}, {"version": "v4", "created": "Tue, 16 Aug 2016 12:11:29 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Caglayan", "Ozan", ""], ["Aransa", "Walid", ""], ["Wang", "Yaxing", ""], ["Masana", "Marc", ""], ["Garc\u00eda-Mart\u00ednez", "Mercedes", ""], ["Bougares", "Fethi", ""], ["Barrault", "Lo\u00efc", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1605.09211", "submitter": "Brendan Jou", "authors": "Brendan Jou and Shih-Fu Chang", "title": "Going Deeper for Multilingual Visual Sentiment Detection", "comments": "technical report, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report details several improvements to the visual concept\ndetector banks built on images from the Multilingual Visual Sentiment Ontology\n(MVSO). The detector banks are trained to detect a total of 9,918\nsentiment-biased visual concepts from six major languages: English, Spanish,\nItalian, French, German and Chinese. In the original MVSO release,\nadjective-noun pair (ANP) detectors were trained for the six languages using an\nAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a\nmore extensive set of experiments, parameter tuning, and training runs, we\ndetail and release higher accuracy models for detecting ANPs across six\nlanguages from the same image pool and setting as in the original release using\na more modern architecture, GoogLeNet, providing comparable or better\nperformance with reduced network parameter cost.\n  In addition, since the image pool in MVSO can be corrupted by user noise from\nsocial interactions, we partitioned out a sub-corpus of MVSO images based on\ntag-restricted queries for higher fidelity labels. We show that as a result of\nthese higher fidelity labels, higher performing AlexNet-styled ANP detectors\ncan be trained using the tag-restricted image subset as compared to the models\nin full corpus. We release all these newly trained models for public research\nuse along with the list of tag-restricted images from the MVSO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 12:57:44 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1605.09553", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Junhua Mao, Fei Sha, Alan Yuille", "title": "Attention Correctness in Neural Image Captioning", "comments": "To appear in AAAI-17. See http://www.cs.jhu.edu/~cxliu/ for\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have recently been introduced in deep learning for\nvarious tasks in natural language processing and computer vision. But despite\ntheir popularity, the \"correctness\" of the implicitly-learned attention maps\nhas only been assessed qualitatively by visualization of several examples. In\nthis paper we focus on evaluating and improving the correctness of attention in\nneural image captioning models. Specifically, we propose a quantitative\nevaluation metric for the consistency between the generated attention maps and\nhuman annotations, using recently released datasets with alignment between\nregions in images and entities in captions. We then propose novel models with\ndifferent levels of explicit supervision for learning attention maps during\ntraining. The supervision can be strong when alignment between regions and\ncaption entities are available, or weak when only object segments and\ncategories are provided. We show on the popular Flickr30k and COCO datasets\nthat introducing supervision of attention maps during training solidly improves\nboth attention correctness and caption quality, showing the promise of making\nmachine perception more human-like.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 10:04:20 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 07:29:46 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Liu", "Chenxi", ""], ["Mao", "Junhua", ""], ["Sha", "Fei", ""], ["Yuille", "Alan", ""]]}, {"id": "1605.09564", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO), Lawrence Muchemi (TAO)", "title": "Determining the Characteristic Vocabulary for a Specialized Dictionary\n  using Word2vec and a Directed Crawler", "comments": null, "journal-ref": "GLOBALEX 2016: Lexicographic Resources for Human Language\n  Technology, May 2016, Portoroz, Slovenia. 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized dictionaries are used to understand concepts in specific domains,\nespecially where those concepts are not part of the general vocabulary, or\nhaving meanings that differ from ordinary languages. The first step in creating\na specialized dictionary involves detecting the characteristic vocabulary of\nthe domain in question. Classical methods for detecting this vocabulary involve\ngathering a domain corpus, calculating statistics on the terms found there, and\nthen comparing these statistics to a background or general language corpus.\nTerms which are found significantly more often in the specialized corpus than\nin the background corpus are candidates for the characteristic vocabulary of\nthe domain. Here we present two tools, a directed crawler, and a distributional\nsemantics package, that can be used together, circumventing the need of a\nbackground corpus. Both tools are available on the web.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 10:31:16 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"], ["Muchemi", "Lawrence", "", "TAO"]]}]