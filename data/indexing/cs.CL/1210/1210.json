[{"id": "1210.0252", "submitter": "Fethi Fkih", "authors": "Fethi Fkih, Mohamed Nazih Omri and Imen Toumia", "title": "A Linguistic Model for Terminology Extraction based Conditional Random\n  Fields", "comments": "This paper has been withdrawn by the author due to the poor\n  readability and the low quality of the English", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we show the possibility of using a linear Conditional Random\nFields (CRF) for terminology extraction from a specialized text corpus.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 22:55:45 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2014 11:25:46 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Fkih", "Fethi", ""], ["Omri", "Mohamed Nazih", ""], ["Toumia", "Imen", ""]]}, {"id": "1210.0794", "submitter": "Nahla Jlaiel", "authors": "Nahla Jlaiel, Khouloud Madhbouh, Mohamed Ben Ahmed", "title": "A Semantic Approach for Automatic Structuring and Analysis of Software\n  Process Patterns", "comments": "08 pages, 10 figures, Published with International Journal of\n  Computer Applications (IJCA)", "journal-ref": "Nahla Jlaiel, Khouloud Madhbouh and Mohamed Ben Ahmed. Article: A\n  Semantic Approach for Automatic Structuring and Analysis of Software Process\n  Patterns. International Journal of Computer Applications 54(15):24-31,\n  September 2012", "doi": "10.5120/8643-2503", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The main contribution of this paper, is to propose a novel semantic approach\nbased on a Natural Language Processing technique in order to ensure a semantic\nunification of unstructured process patterns which are expressed not only in\ndifferent formats but also, in different forms. This approach is implemented\nusing the GATE text engineering framework and then evaluated leading up to\nhigh-quality results motivating us to continue in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 14:56:07 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Jlaiel", "Nahla", ""], ["Madhbouh", "Khouloud", ""], ["Ahmed", "Mohamed Ben", ""]]}, {"id": "1210.0848", "submitter": "Son Doan", "authors": "Son Doan, Lucila Ohno-Machado, Nigel Collier", "title": "Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example\n  in Tracking Influenza-Like Illnesses", "comments": "10 pages, 5 figures, IEEE HISB 2012 conference, Sept 27-28, 2012, La\n  Jolla, California, US", "journal-ref": null, "doi": "10.1109/HISB.2012.21", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems that exploit publicly available user generated content such as\nTwitter messages have been successful in tracking seasonal influenza. We\ndeveloped a novel filtering method for Influenza-Like-Illnesses (ILI)-related\nmessages using 587 million messages from Twitter micro-blogs. We first filtered\nmessages based on syndrome keywords from the BioCaster Ontology, an extant\nknowledge model of laymen's terms. We then filtered the messages according to\nsemantic features such as negation, hashtags, emoticons, humor and geography.\nThe data covered 36 weeks for the US 2009 influenza season from 30th August\n2009 to 8th May 2010. Results showed that our system achieved the highest\nPearson correlation coefficient of 98.46% (p-value<2.2e-16), an improvement of\n3.98% over the previous state-of-the-art method. The results indicate that\nsimple NLP-based enhancements to existing approaches to mine Twitter data can\nincrease the value of this inexpensive resource.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 17:23:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Doan", "Son", ""], ["Ohno-Machado", "Lucila", ""], ["Collier", "Nigel", ""]]}, {"id": "1210.0852", "submitter": "Winfried G\\\"odert", "authors": "Winfried G\\\"odert", "title": "Detecting multiword phrases in mathematical text corpora", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for detecting multiword phrases in mathematical text\ncorpora. The method used is based on characteristic features of mathematical\nterminology. It makes use of a software tool named Lingo which allows to\nidentify words by means of previously defined dictionaries for specific word\nclasses as adjectives, personal names or nouns. The detection of multiword\ngroups is done algorithmically. Possible advantages of the method for indexing\nand information retrieval and conclusions for applying dictionary-based methods\nof automatic indexing instead of stemming procedures are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 17:41:58 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["G\u00f6dert", "Winfried", ""]]}, {"id": "1210.3312", "submitter": "Juan Manuel Torres Moreno", "authors": "Juan-Manuel Torres-Moreno", "title": "Artex is AnotheR TEXt summarizer", "comments": "11 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1209.3126", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes Artex, another algorithm for Automatic Text\nSummarization. In order to rank sentences, a simple inner product is calculated\nbetween each sentence, a document vector (text topic) and a lexical vector\n(vocabulary used by a sentence). Summaries are then generated by assembling the\nhighest ranked sentences. No ruled-based linguistic post-processing is\nnecessary in order to obtain summaries. Tests over several datasets (coming\nfrom Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),\nevaluation campaigns, etc.) in French, English and Spanish have shown that\nsummarizer achieves interesting results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 18:21:01 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "1210.3634", "submitter": "Robert Wahsltedt", "authors": "Robert Wahlstedt", "title": "Quick Summary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Quick Summary is an innovate implementation of an automatic document\nsummarizer that inputs a document in the English language and evaluates each\nsentence. The scanner or evaluator determines criteria based on its grammatical\nstructure and place in the paragraph. The program then asks the user to specify\nthe number of sentences the person wishes to highlight. For example should the\nuser ask to have three of the most important sentences, it would highlight the\nfirst and most important sentence in green. Commonly this is the sentence\ncontaining the conclusion. Then Quick Summary finds the second most important\nsentence usually called a satellite and highlights it in yellow. This is\nusually the topic sentence. Then the program finds the third most important\nsentence and highlights it in red. The implementations of this technology are\nuseful in a society of information overload when a person typically receives 42\nemails a day (Microsoft). The paper also is a candid look at difficulty that\nmachine learning has in textural translating. However, it speaks on how to\novercome the obstacles that historically prevented progress. This paper\nproposes mathematical meta-data criteria that justify the place of importance\nof a sentence. Just as tools for the study of relational symmetry in\nbio-informatics, this tool seeks to classify words with greater clarity.\n\"Survey Finds Workers Average Only Three Productive Days per Week.\" Microsoft\nNews Center. Microsoft. Web. 31 Mar. 2012.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 20:55:39 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Wahlstedt", "Robert", ""]]}, {"id": "1210.3729", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty and Sivaji Bandyopadhyay", "title": "Inference of Fine-grained Attributes of Bengali Corpus for Stylometry\n  Detection", "comments": "5 pages, 2 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1208.6268", "journal-ref": "Polibits (44) 2011, pp. 79-83", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stylometry, the science of inferring characteristics of the author from the\ncharacteristics of documents written by that author, is a problem with a long\nhistory and belongs to the core task of Text categorization that involves\nauthorship identification, plagiarism detection, forensic investigation,\ncomputer security, copyright and estate disputes etc. In this work, we present\na strategy for stylometry detection of documents written in Bengali. We adopt a\nset of fine-grained attribute features with a set of lexical markers for the\nanalysis of the text and use three semi-supervised measures for making\ndecisions. Finally, a majority voting approach has been taken for final\nclassification. The system is fully automatic and language-independent.\nEvaluation results of our attempt for Bengali author's stylometry detection\nshow reasonably promising accuracy in comparison to the baseline model.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2012 18:02:26 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Chakraborty", "Tanmoy", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "1210.3865", "submitter": "Chao-Lin Liu", "authors": "Chien-Liang Chen, Chao-Lin Liu, Yuan-Chen Chang, and Hsiang-Ping Tsai", "title": "Opinion Mining for Relating Subjective Expressions and Annual Earnings\n  in US Financial Statements", "comments": "24 pages, 3 figures, 13 tables, partially appeared in two conference\n  proceedings: (1) Proceedings of the IEEE International Conference on\n  e-Business Engineering 2011 and (2) Proceedings of the 2011 Conference on\n  Technologies and Applications of Artificial Intelligence; Journal of\n  Information Science and Engineering, 29(3), May 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial statements contain quantitative information and manager's\nsubjective evaluation of firm's financial status. Using information released in\nU.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for\nquality financial decisions. To extract such opinioned statements from the\nreports, we built tagging models based on the conditional random field (CRF)\ntechniques, considering a variety of combinations of linguistic factors\nincluding morphology, orthography, predicate-argument structure, syntax, and\nsimple semantics. Our results show that the CRF models are reasonably effective\nto find opinion holders in experiments when we adopted the popular MPQA corpus\nfor training and testing. The contribution of our paper is to identify opinion\npatterns in multiword expressions (MWEs) forms rather than in single word\nforms.\n  We find that the managers of corporations attempt to use more optimistic\nwords to obfuscate negative financial performance and to accentuate the\npositive financial performance. Our results also show that decreasing earnings\nwere often accompanied by ambiguous and mild statements in the reporting year\nand that increasing earnings were stated in assertive and positive way.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 00:58:11 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Chen", "Chien-Liang", ""], ["Liu", "Chao-Lin", ""], ["Chang", "Yuan-Chen", ""], ["Tsai", "Hsiang-Ping", ""]]}, {"id": "1210.3926", "submitter": "Julian McAuley", "authors": "Julian McAuley, Jure Leskovec, Dan Jurafsky", "title": "Learning Attitudes and Attributes from Multi-Aspect Reviews", "comments": "11 pages, 6 figures, extended version of our ICDM 2012 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of online reviews consist of plain-text feedback together with a\nsingle numeric score. However, there are multiple dimensions to products and\nopinions, and understanding the `aspects' that contribute to users' ratings may\nhelp us to better understand their individual preferences. For example, a\nuser's impression of an audiobook presumably depends on aspects such as the\nstory and the narrator, and knowing their opinions on these aspects may help us\nto recommend better products. In this paper, we build models for rating systems\nin which such dimensions are explicit, in the sense that users leave separate\nratings for each aspect of a product. By introducing new corpora consisting of\nfive million reviews, rated with between three and six aspects, we evaluate our\nmodels on three prediction tasks: First, we use our model to uncover which\nparts of a review discuss which of the rated aspects. Second, we use our model\nto summarize reviews, which for us means finding the sentences that best\nexplain a user's rating. Finally, since aspect ratings are optional in many of\nthe datasets we consider, we use our model to recover those ratings that are\nmissing from a user's evaluation. Our model matches state-of-the-art approaches\non existing small-scale datasets, while scaling to the real-world datasets we\nintroduce. Moreover, our model is able to `disentangle' content and sentiment\nwords: we automatically learn content words that are indicative of a particular\naspect as well as the aspect-specific sentiment words that are indicative of a\nparticular rating.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 07:36:57 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 16:14:35 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["McAuley", "Julian", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1210.4567", "submitter": "Jacob Eisenstein", "authors": "David Bamman, Jacob Eisenstein, and Tyler Schnoebelen", "title": "Gender identity and lexical variation in social media", "comments": "submission version", "journal-ref": "Journal of Sociolinguistics 18 (2014) 135-160", "doi": "10.1111/josl.12080", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study of the relationship between gender, linguistic style, and\nsocial networks, using a novel corpus of 14,000 Twitter users. Prior\nquantitative work on gender often treats this social variable as a female/male\nbinary; we argue for a more nuanced approach. By clustering Twitter users, we\nfind a natural decomposition of the dataset into various styles and topical\ninterests. Many clusters have strong gender orientations, but their use of\nlinguistic resources sometimes directly conflicts with the population-level\nlanguage statistics. We view these clusters as a more accurate reflection of\nthe multifaceted nature of gendered language styles. Previous corpus-based work\nhas also had little to say about individuals whose linguistic styles defy\npopulation-level gender patterns. To identify such individuals, we train a\nstatistical classifier, and measure the classifier confidence for each\nindividual in the dataset. Examining individuals whose language does not match\nthe classifier's model for their gender, we find that they have social networks\nthat include significantly fewer same-gender social connections and that, in\ngeneral, social network homophily is correlated with the use of same-gender\nlanguage markers. Pairing computational methods and social theory thus offers a\nnew perspective on how gender emerges as individuals position themselves\nrelative to audiences, topics, and mainstream gender norms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 20:22:56 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 15:04:32 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Bamman", "David", ""], ["Eisenstein", "Jacob", ""], ["Schnoebelen", "Tyler", ""]]}, {"id": "1210.4854", "submitter": "Hannaneh Hajishirzi", "authors": "Hannaneh Hajishirzi, Mohammad Rastegari, Ali Farhadi, Jessica K.\n  Hodgins", "title": "Semantic Understanding of Professional Soccer Commentaries", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-326-335", "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to the problem of semantic parsing via\nlearning the correspondences between complex sentences and rich sets of events.\nOur main intuition is that correct correspondences tend to occur more\nfrequently. Our model benefits from a discriminative notion of similarity to\nlearn the correspondence between sentence and an event and a ranking machinery\nthat scores the popularity of each correspondence. Our method can discover a\ngroup of events (called macro-events) that best describes a sentence. We\nevaluate our method on our novel dataset of professional soccer commentaries.\nThe empirical results show that our method significantly outperforms the\nstate-of-theart.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:37:21 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hajishirzi", "Hannaneh", ""], ["Rastegari", "Mohammad", ""], ["Farhadi", "Ali", ""], ["Hodgins", "Jessica K.", ""]]}, {"id": "1210.4871", "submitter": "Hui Lin", "authors": "Hui Lin, Jeff A. Bilmes", "title": "Learning Mixtures of Submodular Shells with Application to Document\n  Summarization", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-479-490", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to learn a mixture of submodular \"shells\" in a\nlarge-margin setting. A submodular shell is an abstract submodular function\nthat can be instantiated with a ground set and a set of parameters to produce a\nsubmodular function. A mixture of such shells can then also be so instantiated\nto produce a more complex submodular function. What our algorithm learns are\nthe mixture weights over such shells. We provide a risk bound guarantee when\nlearning in a large-margin structured-prediction setting using a projected\nsubgradient method when only approximate submodular optimization is possible\n(such as with submodular function maximization). We apply this method to the\nproblem of multi-document summarization and produce the best results reported\nso far on the widely used NIST DUC-05 through DUC-07 document summarization\ncorpora.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:30 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lin", "Hui", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1210.5268", "submitter": "Jacob Eisenstein", "authors": "Jacob Eisenstein, Brendan O'Connor, Noah A. Smith, Eric P. Xing", "title": "Diffusion of Lexical Change in Social Media", "comments": "preprint of PLOS-ONE paper from November 2014; PLoS ONE 9(11) e113114", "journal-ref": null, "doi": "10.1371/journal.pone.0113114", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-mediated communication is driving fundamental changes in the nature\nof written language. We investigate these changes by statistical analysis of a\ndataset comprising 107 million Twitter messages (authored by 2.7 million unique\nuser accounts). Using a latent vector autoregressive model to aggregate across\nthousands of words, we identify high-level patterns in diffusion of linguistic\nchange over the United States. Our model is robust to unpredictable changes in\nTwitter's sampling rate, and provides a probabilistic characterization of the\nrelationship of macro-scale linguistic influence to a set of demographic and\ngeographic predictors. The results of this analysis offer support for prior\narguments that focus on geographical proximity and population size. However,\ndemographic similarity -- especially with regard to race -- plays an even more\ncentral role, as cities with similar racial demographics are far more likely to\nshare linguistic influence. Rather than moving towards a single unified\n\"netspeak\" dialect, language evolution in computer-mediated communication\nreproduces existing fault lines in spoken American English.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 21:46:09 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2012 01:40:54 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2012 21:15:56 GMT"}, {"version": "v4", "created": "Mon, 24 Nov 2014 03:34:24 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Eisenstein", "Jacob", ""], ["O'Connor", "Brendan", ""], ["Smith", "Noah A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.5321", "submitter": "Koji Ohnishi", "authors": "Koji Ohnishi", "title": "The origin of Mayan languages from Formosan language group of\n  Austronesian", "comments": "6 pages, 1 Table. Proceedings of the 145th Annual Meeting of the\n  Linguistic Society of Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic body-part names (BBPNs) were defined as body-part names in Swadesh\nbasic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensively\nsearched for, by comparing with non-MY vocabulary, including ca.1300 basic\nwords of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs)\nin non-MY are listed in Table 1, as classified by language groups to which most\nsimilar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23\nmutually unrelated CG-items, of which 17.5 CG-items have their MSCs in\nAustronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5,\nwhich consists of 10.33 MSCs in Formosan, 1.83 MSCs in Western\nMalayo-Polynesian (W.MP), 0.33 in Central MP, 0.0 in SHWNG, and 5.0 in Oceanic\n[i.e., CSS(FORM)= 10.33, CSS(W.MP) = 1.88, ..., CSS(OC)= 5.0]. These CSSs for\nlanguage (sub)groups are also listed in the underline portion of every section\nof (Section1 - Section 6) in Table 1. Chi-squar test (degree of freedom = 1)\nusing [Eq 1] and [Eqs.2] revealed that MSCs of MY BBPNs are distributed in\nFormosan in significantly higher frequency (P < 0.001) than in other subgroups\nof AN, as well as than in non-AN languages. MY is thus concluded to have been\nderived from Formosan of AN. Eskimo shows some BBPN similarities to FORM and\nMY.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 05:50:58 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Ohnishi", "Koji", ""]]}, {"id": "1210.5486", "submitter": "Nisheeth Joshi", "authors": "Juhi Ameta, Nisheeth Joshi, Iti Mathur", "title": "A Lightweight Stemmer for Gujarati", "comments": "In Proceedings of 46th Annual Convention of Computer Society of India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gujarati is a resource poor language with almost no language processing tools\nbeing available. In this paper we have shown an implementation of a rule based\nstemmer of Gujarati. We have shown the creation of rules for stemming and the\nrichness in morphology that Gujarati possesses. We have also evaluated our\nresults by verifying it with a human expert.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 17:49:06 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2012 17:10:35 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Ameta", "Juhi", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1210.5517", "submitter": "Nisheeth Joshi", "authors": "Nisheeth Joshi, Iti Mathur", "title": "Design of English-Hindi Translation Memory for Efficient Translation", "comments": "Proceedings of National Conference in Recent Advances in Computer\n  Engineering, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing parallel corpora is an important and a difficult activity for\nMachine Translation. This requires manual annotation by Human Translators.\nTranslating same text again is a useless activity. There are tools available to\nimplement this for European Languages, but no such tool is available for Indian\nLanguages. In this paper we present a tool for Indian Languages which not only\nprovides automatic translations of the previously available translation but\nalso provides multiple translations, in cases where a sentence has multiple\ntranslations, in ranked list of suggestive translations for a sentence.\nMoreover this tool also lets translators have global and local saving options\nof their work, so that they may share it with others, which further lightens\nthe task.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 17:59:56 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1210.5581", "submitter": "Chao-Lin Liu", "authors": "Chia-Chi Tsai, Chao-Lin Liu, Wei-Jie Huang, Man-Kwan Shan", "title": "Hidden Trends in 90 Years of Harvard Business Review", "comments": "6 pages, 14 figures, Proceedings of 2012 International Conference on\n  Technologies and Applications of Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we demonstrate and discuss results of our mining the abstracts\nof the publications in Harvard Business Review between 1922 and 2012.\nTechniques for computing n-grams, collocations, basic sentiment analysis, and\nnamed-entity recognition were employed to uncover trends hidden in the\nabstracts. We present findings about international relationships, sentiment in\nHBR's abstracts, important international companies, influential technological\ninventions, renown researchers in management theories, US presidents via\nchronological analyses.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 06:09:11 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Tsai", "Chia-Chi", ""], ["Liu", "Chao-Lin", ""], ["Huang", "Wei-Jie", ""], ["Shan", "Man-Kwan", ""]]}, {"id": "1210.5751", "submitter": "Estelle Delpech", "authors": "Estelle Delpech (LINA), B\\'eatrice Daille (LINA), Emmanuel Morin\n  (LINA), Claire Lemaire", "title": "Extraction of domain-specific bilingual lexicon from comparable corpora:\n  compositional translation and ranking", "comments": "arXiv admin note: substantial text overlap with arXiv:1209.2400", "journal-ref": "COLING 2012, Mumbai : India (2012)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for extracting translations of morphologically\nconstructed terms from comparable corpora. The method is based on compositional\ntranslation and exploits translation equivalences at the morpheme-level, which\nallows for the generation of \"fertile\" translations (translation pairs in which\nthe target term has more words than the source term). Ranking methods relying\non corpus-based and translation-based features are used to select the best\ncandidate translation. We obtain an average precision of 91% on the Top1\ncandidate translation. The method was tested on two language pairs\n(English-French and English-German) and with a small specialized comparable\ncorpora (400k words per language).\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2012 19:06:11 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Delpech", "Estelle", "", "LINA"], ["Daille", "B\u00e9atrice", "", "LINA"], ["Morin", "Emmanuel", "", "LINA"], ["Lemaire", "Claire", ""]]}, {"id": "1210.5898", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Guantao Jin, Qingfeng Liu, Wei-Yun Chiu, Yih-Soong Yu", "title": "Some Chances and Challenges in Applying Language Technologies to\n  Historical Studies in Chinese", "comments": "15 pages, 9 figures, 2 tables; partially appeared in the Proceedings\n  of the Third International Conference of Digital Archives and Digital\n  Humanities", "journal-ref": "International Journal of Computational Linguistics and Chinese\n  Language Processing, 16(1-2), 27-46, 2011", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We report applications of language technology to analyzing historical\ndocuments in the Database for the Study of Modern Chinese Thoughts and\nLiterature (DSMCTL). We studied two historical issues with the reported\ntechniques: the conceptualization of \"huaren\" (Chinese people) and the attempt\nto institute constitutional monarchy in the late Qing dynasty. We also discuss\nresearch challenges for supporting sophisticated issues using our experience\nwith DSMCTL, the Database of Government Officials of the Republic of China, and\nthe Dream of the Red Chamber. Advanced techniques and tools for lexical,\nsyntactic, semantic, and pragmatic processing of language information, along\nwith more thorough data collection, are needed to strengthen the collaboration\nbetween historians and computer scientists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 13:56:46 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Jin", "Guantao", ""], ["Liu", "Qingfeng", ""], ["Chiu", "Wei-Yun", ""], ["Yu", "Yih-Soong", ""]]}, {"id": "1210.5965", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Classification Analysis Of Authorship Fiction Texts in The Space Of\n  Semantic Fields", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of naive Bayesian classifier (NB) and the classifier by the k nearest\nneighbors (kNN) in classification semantic analysis of authors' texts of\nEnglish fiction has been analysed. The authors' works are considered in the\nvector space the basis of which is formed by the frequency characteristics of\nsemantic fields of nouns and verbs. Highly precise classification of authors'\ntexts in the vector space of semantic fields indicates about the presence of\nparticular spheres of author's idiolect in this space which characterizes the\nindividual author's style.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 16:40:35 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1210.7137", "submitter": "Bernard Ycart", "authors": "Bernard Ycart (LJK)", "title": "Alberti's letter counts", "comments": null, "journal-ref": "Literary and Linguistic Computing (2013) 10.1093/llc/fqt034", "doi": "10.1093/llc/fqt034", "report-no": null, "categories": "math.HO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four centuries before modern statistical linguistics was born, Leon Battista\nAlberti (1404--1472) compared the frequency of vowels in Latin poems and\norations, making the first quantified observation of a stylistic difference\never. Using a corpus of 20 Latin texts (over 5 million letters), Alberti's\nobservations are statistically assessed. Letter counts prove that poets used\nsignificantly more a's, e's, and y's, whereas orators used more of the other\nvowels. The sample sizes needed to justify the assertions are studied, and\nproved to be within reach for Alberti's scholarship.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 13:16:30 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ycart", "Bernard", "", "LJK"]]}, {"id": "1210.7282", "submitter": "Ruggero Micheletto", "authors": "Robert Bishop and Ruggero Micheletto", "title": "The Hangulphabet: A Descriptive Alphabet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Hangulphabet, a new writing system that should prove\nuseful in a number of contexts. Using the Hangulphabet, a user can instantly\nsee voicing, manner and place of articulation of any phoneme found in human\nlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axis\nrepresenting the place of articulation and the y-axis representing manner of\narticulation. Each individual grapheme contains radicals from both axes where\nthe points intersect. The top radical represents manner of articulation where\nthe bottom represents place of articulation. A horizontal line running through\nthe middle of the bottom radical represents voicing. For vowels, place of\narticulation is located on a grid that represents the position of the tongue in\nthe mouth. This grid is similar to that of the IPA vowel chart (International\nPhonetic Association, 1999). The difference with the Hangulphabet being the\ntrapezoid representing the vocal apparatus is on a slight tilt. Place of\narticulation for a vowel is represented by a breakout figure from the grid.\nThis system can be used as an alternative to the International Phonetic\nAlphabet (IPA) or as a complement to it. Beginning students of linguistics may\nfind it particularly useful. A Hangulphabet font has been created to facilitate\nswitching between the Hangulphabet and the IPA.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 02:34:35 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Bishop", "Robert", ""], ["Micheletto", "Ruggero", ""]]}, {"id": "1210.7599", "submitter": "Krunoslav Zubrinic", "authors": "Krunoslav Zubrinic, Damir Kalpic, Mario Milicevic", "title": "The automatic creation of concept maps from documents written using\n  morphologically rich languages", "comments": "ISSN 0957-4174", "journal-ref": "Expert Systems with Applications, Volume 39, Issue 16, 2012, Pages\n  12709-12718", "doi": "10.1016/j.eswa.2012.04.065", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept map is a graphical tool for representing knowledge. They have been\nused in many different areas, including education, knowledge management,\nbusiness and intelligence. Constructing of concept maps manually can be a\ncomplex task; an unskilled person may encounter difficulties in determining and\npositioning concepts relevant to the problem area. An application that\nrecommends concept candidates and their position in a concept map can\nsignificantly help the user in that situation. This paper gives an overview of\ndifferent approaches to automatic and semi-automatic creation of concept maps\nfrom textual and non-textual sources. The concept map mining process is\ndefined, and one method suitable for the creation of concept maps from\nunstructured textual sources in highly inflected languages such as the Croatian\nlanguage is described in detail. Proposed method uses statistical and data\nmining techniques enriched with linguistic tools. With minor adjustments, that\nmethod can also be used for concept map mining from textual sources in other\nmorphologically rich languages.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 09:18:34 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 17:46:02 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Zubrinic", "Krunoslav", ""], ["Kalpic", "Damir", ""], ["Milicevic", "Mario", ""]]}, {"id": "1210.7917", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "The Model of Semantic Concepts Lattice For Data Mining Of Microblogs", "comments": "In Ukrainian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of semantic concept lattice for data mining of microblogs has been\nproposed in this work. It is shown that the use of this model is effective for\nthe semantic relations analysis and for the detection of associative rules of\nkey words.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 07:25:46 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1210.8436", "submitter": "Ciprian Chelba", "authors": "Maryam Kamvar and Ciprian Chelba", "title": "Optimal size, freshness and time-frame for voice search vocabulary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how to optimize the vocabulary for a voice\nsearch language model. The metric we optimize over is the out-of-vocabulary\n(OoV) rate since it is a strong indicator of user experience. In a departure\nfrom the usual way of measuring OoV rates, web search logs allow us to compute\nthe per-session OoV rate and thus estimate the percentage of users that\nexperience a given OoV rate. Under very conservative text normalization, we\nfind that a voice search vocabulary consisting of 2 to 2.5 million words\nextracted from 1 week of search query data will result in an aggregate OoV rate\nof 1%; at that size, the same OoV rate will also be experienced by 90% of\nusers. The number of words included in the vocabulary is a stable indicator of\nthe OoV rate. Altering the freshness of the vocabulary or the duration of the\ntime window over which the training data is gathered does not significantly\nchange the OoV rate. Surprisingly, a significantly larger vocabulary\n(approximately 10 million words) is required to guarantee OoV rates below 1%\nfor 95% of the users.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 18:52:01 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Kamvar", "Maryam", ""], ["Chelba", "Ciprian", ""]]}, {"id": "1210.8440", "submitter": "Ciprian Chelba", "authors": "Ciprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, Shankar\n  Kumar", "title": "Large Scale Language Modeling in Automatic Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models have been proven quite beneficial for a variety of\nautomatic speech recognition tasks in Google. We summarize results on Voice\nSearch and a few YouTube speech transcription tasks to highlight the impact\nthat one can expect from increasing both the amount of training data, and the\nsize of the language model estimated from such data. Depending on the task,\navailability and amount of training data used, language model size and amount\nof work and care put into integrating them in the lattice rescoring step we\nobserve reductions in word error rate between 6% and 10% relative, for systems\non a wide range of operating points between 17% and 52% word error rate.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 18:57:14 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Chelba", "Ciprian", ""], ["Bikel", "Dan", ""], ["Shugrina", "Maria", ""], ["Nguyen", "Patrick", ""], ["Kumar", "Shankar", ""]]}]