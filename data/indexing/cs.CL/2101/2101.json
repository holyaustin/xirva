[{"id": "2101.00010", "submitter": "Koustuv Sinha", "authors": "Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams", "title": "UnNatural Language Inference", "comments": "Accepted at ACL 2021 (Long Paper), 9 pages + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent investigations into the inner-workings of state-of-the-art large-scale\npre-trained Transformer-based Natural Language Understanding (NLU) models\nindicate that they appear to know humanlike syntax, at least to some extent. We\nprovide novel evidence that complicates this claim: we find that\nstate-of-the-art Natural Language Inference (NLI) models assign the same labels\nto permuted examples as they do to the original, i.e. they are largely\ninvariant to random word-order permutations. This behavior notably differs from\nthat of humans; we struggle with ungrammatical sentences. To measure the\nseverity of this issue, we propose a suite of metrics and investigate which\nproperties of particular permutations lead models to be word-order invariant.\nIn the MNLI dataset, for example, we find almost all (98.7%) examples contain\nat least one permutation which elicits the gold label. Models are sometimes\neven able to assign gold labels to permutations that they originally failed to\npredict correctly. We provide a comprehensive empirical evaluation of this\nphenomenon, and further show that this issue exists for both Transformers and\npre-Transformer RNN / ConvNet based encoders, as well as across multiple\nlanguages (English and Mandarin Chinese). Our code and data are available at\nhttps://github.com/facebookresearch/unlu.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 20:40:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 03:44:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sinha", "Koustuv", ""], ["Parthasarathi", "Prasanna", ""], ["Pineau", "Joelle", ""], ["Williams", "Adina", ""]]}, {"id": "2101.00027", "submitter": "Leo Gao", "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe,\n  Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn\n  Presser, Connor Leahy", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:00:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gao", "Leo", ""], ["Biderman", "Stella", ""], ["Black", "Sid", ""], ["Golding", "Laurence", ""], ["Hoppe", "Travis", ""], ["Foster", "Charles", ""], ["Phang", "Jason", ""], ["He", "Horace", ""], ["Thite", "Anish", ""], ["Nabeshima", "Noa", ""], ["Presser", "Shawn", ""], ["Leahy", "Connor", ""]]}, {"id": "2101.00036", "submitter": "Yuta Nakamura", "authors": "Yuta Nakamura (1 and 2), Shouhei Hanaoka (3), Yukihiro Nomura (4),\n  Naoto Hayashi (4), Osamu Abe (1 and 3), Shuntaro Yada (2), Shoko Wakamiya\n  (2), Eiji Aramaki (2) ((1) The University of Tokyo, (2) Nara Institute of\n  Science and Technology, (3) The Department of Radiology, The University of\n  Tokyo Hospital, (4) The Department of Computational Diagnostic Radiology and\n  Preventive Medicine, The University of Tokyo Hospital)", "title": "KART: Privacy Leakage Framework of Language Models Pre-trained with\n  Clinical Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, mainstream natural language pro-cessing (NLP) is empowered by\npre-trained language models. In the biomedical domain, only models pre-trained\nwith anonymized data have been published. This policy is acceptable, but there\nare two questions: Can the privacy policy of language models be different from\nthat of data? What happens if private language models are accidentally made\npublic? We empirically evaluated the privacy risk of language models, using\nseveral BERT models pre-trained with MIMIC-III corpus in different data\nanonymity and corpus sizes. We simulated model inversion attacks to obtain the\nclinical information of target individuals, whose full names are already known\nto attackers. The BERT models were probably low-risk because the Top-100\naccuracy of each attack was far below expected by chance. Moreover, most\nprivacy leakage situations have several common primary factors; therefore, we\nformalized various privacy leakage scenarios under a universal novel framework\nnamed Knowledge, Anonymization, Resource, and Target (KART) framework. The KART\nframework helps parameterize complex privacy leakage scenarios and simplifies\nthe comprehensive evaluation. Since the concept of the KART framework is domain\nagnostic, it can contribute to the establishment of privacy guidelines of\nlanguage models beyond the biomedical domain.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:06:18 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nakamura", "Yuta", "", "1 and 2"], ["Hanaoka", "Shouhei", "", "1 and 3"], ["Nomura", "Yukihiro", "", "1 and 3"], ["Hayashi", "Naoto", "", "1 and 3"], ["Abe", "Osamu", "", "1 and 3"], ["Yada", "Shuntaro", ""], ["Wakamiya", "Shoko", ""], ["Aramaki", "Eiji", ""]]}, {"id": "2101.00056", "submitter": "Rajaswa Patil", "authors": "Rajaswa Patil, Yaman Kumar Singla, Rajiv Ratn Shah, Mika Hama and\n  Roger Zimmermann", "title": "Towards Modelling Coherence in Spoken Discourse", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there has been significant progress towards modelling coherence in\nwritten discourse, the work in modelling spoken discourse coherence has been\nquite limited. Unlike the coherence in text, coherence in spoken discourse is\nalso dependent on the prosodic and acoustic patterns in speech. In this paper,\nwe model coherence in spoken discourse with audio-based coherence models. We\nperform experiments with four coherence-related tasks with spoken discourses.\nIn our experiments, we evaluate machine-generated speech against the speech\ndelivered by expert human speakers. We also compare the spoken discourses\ngenerated by human language learners of varying language proficiency levels.\nOur results show that incorporating the audio modality along with the text\nbenefits the coherence models in performing downstream coherence related tasks\nwith spoken discourses.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:18:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Patil", "Rajaswa", ""], ["Singla", "Yaman Kumar", ""], ["Shah", "Rajiv Ratn", ""], ["Hama", "Mika", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2101.00063", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang,\n  Jingjing Liu", "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets", "comments": "Accepted at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavily overparameterized language models such as BERT, XLNet and T5 have\nachieved impressive success in many NLP tasks. However, their high model\ncomplexity requires enormous computation resources and extremely long training\ntime for both pre-training and fine-tuning. Many works have studied model\ncompression on large NLP models, but only focusing on reducing inference time\nwhile still requiring an expensive training process. Other works use extremely\nlarge batch sizes to shorten the pre-training time, at the expense of higher\ncomputational resource demands. In this paper, inspired by the Early-Bird\nLottery Tickets recently studied for computer vision tasks, we propose\nEarlyBERT, a general computationally-efficient training algorithm applicable to\nboth pre-training and fine-tuning of large-scale language models. By slimming\nthe self-attention and fully-connected sub-layers inside a transformer, we are\nthe first to identify structured winning tickets in the early stage of BERT\ntraining. We apply those tickets towards efficient BERT training, and conduct\ncomprehensive pre-training and fine-tuning experiments on GLUE and SQuAD\ndownstream tasks. Our results show that EarlyBERT achieves comparable\nperformance to standard BERT, with 35~45% less training time. Code is available\nat https://github.com/VITA-Group/EarlyBERT.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:38:20 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:26:28 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Xiaohan", ""], ["Cheng", "Yu", ""], ["Wang", "Shuohang", ""], ["Gan", "Zhe", ""], ["Wang", "Zhangyang", ""], ["Liu", "Jingjing", ""]]}, {"id": "2101.00078", "submitter": "Anjalie Field", "authors": "Anjalie Field, Chan Young Park, Yulia Tsvetkov", "title": "Controlled Analyses of Social Biases in Wikipedia Bios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social biases on Wikipedia, a widely-read global platform, could greatly\ninfluence public opinion. While prior research has examined man/woman gender\nbias in biography articles, possible influences of confounding variables limit\nconclusions. In this work, we present a methodology for reducing the effects of\nconfounding variables in analyses of Wikipedia biography pages. Given a target\ncorpus for analysis (e.g. biography pages about women), we present a method for\nconstructing a comparison corpus that matches the target corpus in as many\nattributes as possible, except the target attribute (e.g. the gender of the\nsubject). We evaluate our methodology by developing metrics to measure how well\nthe comparison corpus aligns with the target corpus. We then examine how\narticles about gender and racial minorities (cisgender women, non-binary\npeople, transgender women, and transgender men; African American, Asian\nAmerican, and Hispanic/Latinx American people) differ from other articles,\nincluding analyses driven by social theories like intersectionality. In\naddition to identifying suspect social biases, our results show that failing to\ncontrol for confounding variables can result in different conclusions and mask\nbiases. Our contributions include methodology that facilitates further analyses\nof bias in Wikipedia articles, findings that can aid Wikipedia editors in\nreducing biases, and framework and evaluation metrics to guide future work in\nthis area.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:27:12 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Field", "Anjalie", ""], ["Park", "Chan Young", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2101.00117", "submitter": "Jean Maillard", "authors": "Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas\n  O\\u{g}uz, Veselin Stoyanov, Gargi Ghosh", "title": "Multi-task Retrieval for Knowledge-Intensive Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Retrieving relevant contexts from a large corpus is a crucial step for tasks\nsuch as open-domain question answering and fact checking. Although neural\nretrieval outperforms traditional methods like tf-idf and BM25, its performance\ndegrades considerably when applied to out-of-domain data.\n  Driven by the question of whether a neural retrieval model can be universal\nand perform robustly on a wide variety of problems, we propose a multi-task\ntrained model. Our approach not only outperforms previous methods in the\nfew-shot setting, but also rivals specialised neural retrievers, even when\nin-domain training data is abundant. With the help of our retriever, we improve\nexisting models for downstream tasks and closely match or improve the state of\nthe art on multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:16:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Maillard", "Jean", ""], ["Karpukhin", "Vladimir", ""], ["Petroni", "Fabio", ""], ["Yih", "Wen-tau", ""], ["O\u011fuz", "Barlas", ""], ["Stoyanov", "Veselin", ""], ["Ghosh", "Gargi", ""]]}, {"id": "2101.00121", "submitter": "Karen Hambardzumyan", "authors": "Karen Hambardzumyan, Hrant Khachatrian, Jonathan May", "title": "WARP: Word-level Adversarial ReProgramming", "comments": "Accepted ACL 2021 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from pretrained language models recently became the\ndominant approach for solving many NLP tasks. A common approach to transfer\nlearning for multiple tasks that maximize parameter sharing trains one or more\ntask-specific layers on top of the language model. In this paper, we present an\nalternative approach based on adversarial reprogramming, which extends earlier\nwork on automatic prompt generation. Adversarial reprogramming attempts to\nlearn task-specific word embeddings that, when concatenated to the input text,\ninstruct the language model to solve the specified task. Using up to 25K\ntrainable parameters per task, this approach outperforms all existing methods\nwith up to 25M trainable parameters on the public leaderboard of the GLUE\nbenchmark. Our method, initialized with task-specific human-readable prompts,\nalso works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks\nwith just 32 training samples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:41:03 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:13:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hambardzumyan", "Karen", ""], ["Khachatrian", "Hrant", ""], ["May", "Jonathan", ""]]}, {"id": "2101.00123", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad, Jianfeng Chi, Tu Le, Thomas Norton, Yuan Tian,\n  Kai-Wei Chang", "title": "Intent Classification and Slot Filling for Privacy Policies", "comments": "ACL 2021 (camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding privacy policies is crucial for users as it empowers them to\nlearn about the information that matters to them. Sentences written in a\nprivacy policy document explain privacy practices, and the constituent text\nspans convey further specific information about that practice. We refer to\npredicting the privacy practice explained in a sentence as intent\nclassification and identifying the text spans sharing specific information as\nslot filling. In this work, we propose PolicyIE, an English corpus consisting\nof 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of\nwebsites and mobile applications. PolicyIE corpus is a challenging real-world\nbenchmark with limited labeled examples reflecting the cost of collecting\nlarge-scale annotations from domain experts. We present two alternative neural\napproaches as baselines, (1) intent classification and slot filling as a joint\nsequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq)\nlearning task. The experiment results show that both approaches perform\ncomparably in intent classification, while the Seq2Seq method outperforms the\nsequence tagging approach in slot filling by a large margin. We perform a\ndetailed error analysis to reveal the challenges of the proposed corpus.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:44:41 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 21:11:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Chi", "Jianfeng", ""], ["Le", "Tu", ""], ["Norton", "Thomas", ""], ["Tian", "Yuan", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2101.00124", "submitter": "I-Hung Hsu", "authors": "I-Hung Hsu, Xiao Guo, Wael AbdAlmageed, Premkumar Natarajan, Nanyun\n  Peng", "title": "MrGCN: Mirror Graph Convolution Network for Relation Extraction with\n  Long-Term Dependencies", "comments": "13 pages, page 11-13 appendix, 7 figures. The first two authors\n  contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to capture complex linguistic structures and long-term\ndependencies among words in the passage is essential for relation extraction\n(RE) tasks. Graph neural networks (GNNs), one of the means to encode dependency\ngraphs, have been shown to be effective in prior works. However, relatively\nlittle attention has been paid to receptive fields of GNNs, which can be\ncrucial for tasks with extremely long text that requires discourse\nunderstanding. In this work, we leverage the idea of graph pooling and propose\nthe Mirror Graph Convolution Network, a GNN model with a pooling-unpooling\nstructure tailored to RE tasks. The pooling branch reduces the graph size and\nenables the GNN to obtain larger receptive fields within fewer layers; the\nunpooling branch restores the pooled graph to its original resolution for\ntoken-level RE tasks. Experiments on two discourse-level relation extraction\ndatasets demonstrate the effectiveness of our method, showing significant\nimprovements over prior methods especially when modeling long-term dependencies\nis necessary. Moreover, we propose Clause Matching (CM), a novel graph pooling\nmethod that merges nodes based on dependency relations in graph. CM can largely\nreduce the graph size while retaining the main semantics of the input text.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:52:53 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 07:33:34 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hsu", "I-Hung", ""], ["Guo", "Xiao", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""], ["Peng", "Nanyun", ""]]}, {"id": "2101.00130", "submitter": "Jingbo Shang", "authors": "Jiaman Wu, Dezhi Hong, Rajesh Gupta, Jingbo Shang", "title": "Sensei: Self-Supervised Sensor Name Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A sensor name, typically an alphanumeric string, encodes the key context\n(e.g., function and location) of a sensor needed for deploying smart building\napplications. Sensor names, however, are curated in a building vendor-specific\nmanner using different structures and vocabularies that are often esoteric.\nThey thus require tremendous manual effort to annotate on a per-building basis;\neven to just segment these sensor names into meaningful chunks. In this paper,\nwe propose a fully automated self-supervised framework, Sensei, which can learn\nto segment sensor names without any human annotation. Specifically, we employ a\nneural language model to capture the underlying sensor naming structure and\nthen induce self-supervision based on information from the language model to\nbuild the segmentation model. Extensive experiments on five real-world\nbuildings comprising thousands of sensors demonstrate the superiority of Sensei\nover baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 01:18:49 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wu", "Jiaman", ""], ["Hong", "Dezhi", ""], ["Gupta", "Rajesh", ""], ["Shang", "Jingbo", ""]]}, {"id": "2101.00133", "submitter": "Sewon Min", "authors": "Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi,\n  Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria\n  Palomaki, Colin Raffel, Adam Roberts, Tom Kwiatkowski, Patrick Lewis, Yuxiang\n  Wu, Heinrich K\\\"uttler, Linqing Liu, Pasquale Minervini, Pontus Stenetorp,\n  Sebastian Riedel, Sohee Yang, Minjoon Seo, Gautier Izacard, Fabio Petroni,\n  Lucas Hosseini, Nicola De Cao, Edouard Grave, Ikuya Yamada, Sonse Shimaoka,\n  Masatoshi Suzuki, Shumpei Miyawaki, Shun Sato, Ryo Takahashi, Jun Suzuki,\n  Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz, Hao Cheng, Yelong\n  Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, Barlas Oguz,\n  Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael\n  Schlichtkrull, Sonal Gupta, Yashar Mehdad, Wen-tau Yih", "title": "NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons\n  Learned", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the EfficientQA competition from NeurIPS 2020. The competition\nfocused on open-domain question answering (QA), where systems take natural\nlanguage questions as input and return natural language answers. The aim of the\ncompetition was to build systems that can predict correct answers while also\nsatisfying strict on-disk memory budgets. These memory budgets were designed to\nencourage contestants to explore the trade-off between storing large,\nredundant, retrieval corpora or the parameters of large learned models. In this\nreport, we describe the motivation and organization of the competition, review\nthe best submissions, and analyze system predictions to inform a discussion of\nevaluation for open-domain QA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 01:24:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Min", "Sewon", ""], ["Boyd-Graber", "Jordan", ""], ["Alberti", "Chris", ""], ["Chen", "Danqi", ""], ["Choi", "Eunsol", ""], ["Collins", "Michael", ""], ["Guu", "Kelvin", ""], ["Hajishirzi", "Hannaneh", ""], ["Lee", "Kenton", ""], ["Palomaki", "Jennimaria", ""], ["Raffel", "Colin", ""], ["Roberts", "Adam", ""], ["Kwiatkowski", "Tom", ""], ["Lewis", "Patrick", ""], ["Wu", "Yuxiang", ""], ["K\u00fcttler", "Heinrich", ""], ["Liu", "Linqing", ""], ["Minervini", "Pasquale", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""], ["Yang", "Sohee", ""], ["Seo", "Minjoon", ""], ["Izacard", "Gautier", ""], ["Petroni", "Fabio", ""], ["Hosseini", "Lucas", ""], ["De Cao", "Nicola", ""], ["Grave", "Edouard", ""], ["Yamada", "Ikuya", ""], ["Shimaoka", "Sonse", ""], ["Suzuki", "Masatoshi", ""], ["Miyawaki", "Shumpei", ""], ["Sato", "Shun", ""], ["Takahashi", "Ryo", ""], ["Suzuki", "Jun", ""], ["Fajcik", "Martin", ""], ["Docekal", "Martin", ""], ["Ondrej", "Karel", ""], ["Smrz", "Pavel", ""], ["Cheng", "Hao", ""], ["Shen", "Yelong", ""], ["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Chen", "Weizhu", ""], ["Gao", "Jianfeng", ""], ["Oguz", "Barlas", ""], ["Chen", "Xilun", ""], ["Karpukhin", "Vladimir", ""], ["Peshterliev", "Stan", ""], ["Okhonko", "Dmytro", ""], ["Schlichtkrull", "Michael", ""], ["Gupta", "Sonal", ""], ["Mehdad", "Yashar", ""], ["Yih", "Wen-tau", ""]]}, {"id": "2101.00146", "submitter": "Leibo Liu", "authors": "Leibo Liu, Oscar Perez-Concha, Anthony Nguyen, Vicki Bennett, Louisa\n  Jorm", "title": "De-identifying Hospital Discharge Summaries: An End-to-End Framework\n  using Ensemble of De-Identifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective:Electronic Medical Records (EMRs) contain clinical narrative text\nthat is of great potential value to medical researchers. However, this\ninformation is mixed with Protected Health Information (PHI) that presents\nrisks to patient and clinician confidentiality. This paper presents an\nend-to-end de-identification framework to automatically remove PHI from\nhospital discharge summaries. Materials and Methods:Our corpus included 600\nhospital discharge summaries which were extracted from the EMRs of two\nprincipal referral hospitals in Sydney, Australia. Our end-to-end\nde-identification framework consists of three components: 1) Annotation:\nlabelling of PHI in the 600 hospital discharge summaries using five pre-defined\ncategories: person, address, date of birth, individual identification number,\nphone/fax number; 2) Modelling: training and evaluating ensembles of named\nentity recognition (NER) models through the use of three natural language\nprocessing (NLP) toolkits (Stanza, FLAIR and spaCy) and both balanced and\nimbalanced datasets; and 3) De-identification: removing PHI from the hospital\ndischarge summaries. Results:The final model in our framework was an ensemble\nwhich combined six single models using both balanced and imbalanced datasets\nfor training majority voting. It achieved 0.9866 precision, 0.9862 recall and\n0.9864 F1 scores. The majority of false positives and false negatives were\nrelated to the person category. Discussion:Our study showed that the ensemble\nof different models which were trained using three different NLP toolkits upon\nbalanced and imbalanced datasets can achieve good results even with a\nrelatively small corpus. Conclusion:Our end-to-end framework provides a robust\nsolution to de-identifying clinical narrative corpuses safely. It can be easily\napplied to any kind of clinical narrative documents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:09:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Leibo", ""], ["Perez-Concha", "Oscar", ""], ["Nguyen", "Anthony", ""], ["Bennett", "Vicki", ""], ["Jorm", "Louisa", ""]]}, {"id": "2101.00148", "submitter": "Haoyue Shi", "authors": "Haoyue Shi, Luke Zettlemoyer, Sida I. Wang", "title": "Bilingual Lexicon Induction via Unsupervised Bitext Construction and\n  Word Alignment", "comments": "ACL-IJCNLP 2021 camera-ready version, with full supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual lexicons map words in one language to their translations in\nanother, and are typically induced by learning linear projections to align\nmonolingual word embedding spaces. In this paper, we show it is possible to\nproduce much higher quality lexicons with methods that combine (1) unsupervised\nbitext mining and (2) unsupervised word alignment. Directly applying a pipeline\nthat uses recent algorithms for both subproblems significantly improves induced\nlexicon quality and further gains are possible by learning to filter the\nresulting lexical entries, with both unsupervised and semi-supervised schemes.\nOur final model outperforms the state of the art on the BUCC 2020 shared task\nby 14 $F_1$ points averaged over 12 language pairs, while also providing a more\ninterpretable approach that allows for rich reasoning of word meaning in\ncontext. Further analysis of our output and the standard reference lexicons\nsuggests they are of comparable quality, and new benchmarks may be needed to\nmeasure further progress on this task.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:12:42 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 22:48:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shi", "Haoyue", ""], ["Zettlemoyer", "Luke", ""], ["Wang", "Sida I.", ""]]}, {"id": "2101.00151", "submitter": "Hung Le", "authors": "Hung Le and Chinnadhurai Sankar and Seungwhan Moon and Ahmad Beirami\n  and Alborz Geramifard and Satwik Kottur", "title": "DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded\n  Dialogue", "comments": "20 pages, 14 figures, 8 tables", "journal-ref": "Association for Computational Linguistics (2021)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A video-grounded dialogue system is required to understand both dialogue,\nwhich contains semantic dependencies from turn to turn, and video, which\ncontains visual cues of spatial and temporal scene variations. Building such\ndialogue systems is a challenging problem, involving various reasoning types on\nboth visual and language inputs. Existing benchmarks do not have enough\nannotations to thoroughly analyze dialogue systems and understand their\ncapabilities and limitations in isolation. These benchmarks are also not\nexplicitly designed to minimise biases that models can exploit without actual\nreasoning. To address these limitations, in this paper, we present DVD, a\nDiagnostic Dataset for Video-grounded Dialogues. The dataset is designed to\ncontain minimal biases and has detailed annotations for the different types of\nreasoning over the spatio-temporal space of video. Dialogues are synthesized\nover multiple question turns, each of which is injected with a set of\ncross-turn semantic relationships. We use DVD to analyze existing approaches,\nproviding interesting insights into their abilities and limitations. In total,\nDVD is built from $11k$ CATER synthetic videos and contains $10$ instances of\n$10$-round dialogues for each video, resulting in more than $100k$ dialogues\nand $1M$ question-answer pairs. Our code and dataset are publicly available at\nhttps://github.com/facebookresearch/DVDialogues.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:20:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:55:57 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Le", "Hung", ""], ["Sankar", "Chinnadhurai", ""], ["Moon", "Seungwhan", ""], ["Beirami", "Ahmad", ""], ["Geramifard", "Alborz", ""], ["Kottur", "Satwik", ""]]}, {"id": "2101.00153", "submitter": "Bin Liu", "authors": "Liu Bin, Wang Liang, Yin Guosheng", "title": "A Graph Total Variation Regularized Softmax for Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The softmax operator is one of the most important functions in machine\nlearning models. When applying neural networks to multi-category\nclassification, the correlations among different categories are often ignored.\nFor example, in text generation, a language model makes a choice of each new\nword based only on the former selection of its context. In this scenario, the\nlink statistics information of concurrent words based on a corpus (an analogy\nof the natural way of expression) is also valuable in choosing the next word,\nwhich can help to improve the sentence's fluency and smoothness. To fully\nexplore such important information, we propose a graph softmax function for\ntext generation. It is expected that the final classification result would be\ndominated by both the language model and graphical text relationships among\nwords. We use a graph total variation term to regularize softmax so as to\nincorporate the concurrent relationship into the language model. The total\nvariation of the generated words should be small locally. We apply the proposed\ngraph softmax to GPT2 for the text generation task. Experimental results\ndemonstrate that the proposed graph softmax achieves better BLEU and perplexity\nthan softmax. Human testers can also easily distinguish the text generated by\nthe graph softmax or softmax.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:29:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bin", "Liu", ""], ["Liang", "Wang", ""], ["Guosheng", "Yin", ""]]}, {"id": "2101.00154", "submitter": "Tianqing Fang", "authors": "Tianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, Bin He", "title": "DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense\n  Knowledge", "comments": "WWW 2021 paper. 12 pages and 6 Figures", "journal-ref": null, "doi": "10.1145/3442381.3450117", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge is crucial for artificial intelligence systems to\nunderstand natural language. Previous commonsense knowledge acquisition\napproaches typically rely on human annotations (for example, ATOMIC) or text\ngeneration models (for example, COMET.) Human annotation could provide\nhigh-quality commonsense knowledge, yet its high cost often results in\nrelatively small scale and low coverage. On the other hand, generation models\nhave the potential to automatically generate more knowledge. Nonetheless,\nmachine learning models often fit the training data well and thus struggle to\ngenerate high-quality novel knowledge. To address the limitations of previous\napproaches, in this paper, we propose an alternative commonsense knowledge\nacquisition framework DISCOS (from DIScourse to COmmonSense), which\nautomatically populates expensive complex commonsense knowledge to more\naffordable linguistic knowledge resources. Experiments demonstrate that we can\nsuccessfully convert discourse knowledge about eventualities from ASER, a\nlarge-scale discourse knowledge graph, into if-then commonsense knowledge\ndefined in ATOMIC without any additional annotation effort. Further study\nsuggests that DISCOS significantly outperforms previous supervised approaches\nin terms of novelty and diversity with comparable quality. In total, we can\nacquire 3.4M ATOMIC-like inferential commonsense knowledge by populating ATOMIC\non the core part of ASER. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/DISCOS-commonsense.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:30:38 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 12:43:37 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Fang", "Tianqing", ""], ["Zhang", "Hongming", ""], ["Wang", "Weiqi", ""], ["Song", "Yangqiu", ""], ["He", "Bin", ""]]}, {"id": "2101.00160", "submitter": "Hyunjae Kim", "authors": "Hyunjae Kim, Jaewoo Kang", "title": "How Do Your Biomedical Named Entity Models Generalize to Novel Entities?", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of biomedical literature on new biomedical concepts is rapidly\nincreasing, which necessitates a reliable biomedical named entity recognition\n(BioNER) model for identifying new and unseen entity mentions. However, it is\nquestionable whether existing BioNER models can effectively handle them. In\nthis work, we systematically analyze the three types of recognition abilities\nof BioNER models: memorization, synonym generalization, and concept\ngeneralization. We find that (1) BioNER models are overestimated in terms of\ntheir generalization ability, and (2) they tend to exploit dataset biases,\nwhich hinders the models' abilities to generalize. To enhance the\ngeneralizability, we present a simple debiasing method based on the data\nstatistics. Our method consistently improves the generalizability of the\nstate-of-the-art (SOTA) models on five benchmark datasets, allowing them to\nbetter perform on unseen entity mentions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 04:13:42 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kim", "Hyunjae", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2101.00167", "submitter": "Yi Cheng", "authors": "Yi Cheng, Sujian Li, Yueyuan Li", "title": "Unifying Discourse Resources with Dependency Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text-level discourse analysis, there are various discourse schemes but\nrelatively few labeled data, because discourse research is still immature and\nit is labor-intensive to annotate the inner logic of a text. In this paper, we\nattempt to unify multiple Chinese discourse corpora under different annotation\nschemes with discourse dependency framework by designing semi-automatic methods\nto convert them into dependency structures. We also implement several benchmark\ndependency parsers and research on how they can leverage the unified data to\nimprove performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:23:29 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 02:00:39 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 06:43:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cheng", "Yi", ""], ["Li", "Sujian", ""], ["Li", "Yueyuan", ""]]}, {"id": "2101.00173", "submitter": "Kai Yi", "authors": "Mohamed Elhoseiny, Kai Yi, Mohamed Elfeki", "title": "CIZSL++: Creativity Inspired Generative Zero-Shot Learning", "comments": "This paper is an extended version of a paper published on the\n  International Conference on Computer Vision (ICCV), held in Seoul, Republic\n  of Korea, October 27-Nov 2nd, 2019 CIZSL-v2 code is available here\n  https://github.com/Vision-CAIR/CIZSLv2. arXiv admin note: substantial text\n  overlap with arXiv:1904.01109", "journal-ref": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.pdf", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot learning (ZSL) aims at understanding unseen categories with no\ntraining examples from class-level descriptions. To improve the discriminative\npower of ZSL, we model the visual learning process of unseen categories with\ninspiration from the psychology of human creativity for producing novel art.\nFirst, we propose CIZSL-v1 as a creativity inspired model for generative ZSL.\nWe relate ZSL to human creativity by observing that ZSL is about recognizing\nthe unseen, and creativity is about creating a likable unseen. We introduce a\nlearning signal inspired by creativity literature that explores the unseen\nspace with hallucinated class-descriptions and encourages careful deviation of\ntheir visual feature generations from seen classes while allowing knowledge\ntransfer from seen to unseen classes. Second, CIZSL-v2 is proposed as an\nimproved version of CIZSL-v1 for generative zero-shot learning. CIZSL-v2\nconsists of an investigation of additional inductive losses for unseen classes\nalong with a semantic guided discriminator. Empirically, we show consistently\nthat CIZSL losses can improve generative ZSL models on the challenging task of\ngeneralized ZSL from a noisy text on CUB and NABirds datasets. We also show the\nadvantage of our approach to Attribute-based ZSL on AwA2, aPY, and SUN\ndatasets. We also show that CIZSL-v2 has improved performance compared to\nCIZSL-v1.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:47:57 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 09:08:51 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Yi", "Kai", ""], ["Elfeki", "Mohamed", ""]]}, {"id": "2101.00178", "submitter": "Hao Cheng", "authors": "Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen,\n  Jianfeng Gao", "title": "UnitedQA: A Hybrid Approach for Open Domain Question Answering", "comments": "ACL 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most of recent work under the retrieval-reader framework for\nopen-domain QA focuses on either extractive or generative reader exclusively.\nIn this paper, we study a hybrid approach for leveraging the strengths of both\nmodels. We apply novel techniques to enhance both extractive and generative\nreaders built upon recent pretrained neural language models, and find that\nproper training methods can provide large improvement over previous\nstate-of-the-art models. We demonstrate that a simple hybrid approach by\ncombining answers from both readers can efficiently take advantages of\nextractive and generative answer inference strategies and outperforms single\nmodels as well as homogeneous ensembles. Our approach outperforms previous\nstate-of-the-art models by 3.3 and 2.7 points in exact match on\nNaturalQuestions and TriviaQA respectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 06:36:16 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 18:07:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cheng", "Hao", ""], ["Shen", "Yelong", ""], ["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Chen", "Weizhu", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2101.00180", "submitter": "Sunil Gundapu", "authors": "Sunil Gundapu, Radhika Mamidi", "title": "Transformer based Automatic COVID-19 Fake News Detection System", "comments": "First Workshop on Combating Online Hostile Posts in Regional\n  Languages during Emergency Situation, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent rapid technological advancements in online social networks such as\nTwitter have led to a great incline in spreading false information and fake\nnews. Misinformation is especially prevalent in the ongoing coronavirus disease\n(COVID-19) pandemic, leading to individuals accepting bogus and potentially\ndeleterious claims and articles. Quick detection of fake news can reduce the\nspread of panic and confusion among the public. For our analysis in this paper,\nwe report a methodology to analyze the reliability of information shared on\nsocial media pertaining to the COVID-19 pandemic. Our best approach is based on\nan ensemble of three transformer models (BERT, ALBERT, and XLNET) to detecting\nfake news. This model was trained and evaluated in the context of the\nConstraintAI 2021 shared task COVID19 Fake News Detection in English. Our\nsystem obtained 0.9855 f1-score on testset and ranked 5th among 160 teams.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 06:49:27 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 06:38:26 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 15:18:40 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gundapu", "Sunil", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.00190", "submitter": "Xiang Lisa Li", "authors": "Xiang Lisa Li and Percy Liang", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning is the de facto way to leverage large pretrained language models\nto perform downstream tasks. However, it modifies all the language model\nparameters and therefore necessitates storing a full copy for each task. In\nthis paper, we propose prefix-tuning, a lightweight alternative to fine-tuning\nfor natural language generation tasks, which keeps language model parameters\nfrozen, but optimizes a small continuous task-specific vector (called the\nprefix). Prefix-tuning draws inspiration from prompting, allowing subsequent\ntokens to attend to this prefix as if it were \"virtual tokens\". We apply\nprefix-tuning to GPT-2 for table-to-text generation and to BART for\nsummarization. We find that by learning only 0.1\\% of the parameters,\nprefix-tuning obtains comparable performance in the full data setting,\noutperforms fine-tuning in low-data settings, and extrapolates better to\nexamples with topics unseen during training.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 08:00:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Xiang Lisa", ""], ["Liang", "Percy", ""]]}, {"id": "2101.00196", "submitter": "Zhengxuan Wu", "authors": "Zhengxuan Wu, Desmond C. Ong", "title": "On Explaining Your Explanations of BERT: An Empirical Study with\n  Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BERT, as one of the pretrianed language models, attracts the most attention\nin recent years for creating new benchmarks across GLUE tasks via fine-tuning.\nOne pressing issue is to open up the blackbox and explain the decision makings\nof BERT. A number of attribution techniques have been proposed to explain BERT\nmodels, but are often limited to sequence to sequence tasks. In this paper, we\nadapt existing attribution methods on explaining decision makings of BERT in\nsequence classification tasks. We conduct extensive analyses of four existing\nattribution methods by applying them to four different datasets in sentiment\nanalysis. We compare the reliability and robustness of each method via various\nablation studies. Furthermore, we test whether attribution methods explain\ngeneralized semantics across semantically similar tasks. Our work provides\nsolid guidance for using attribution methods to explain decision makings of\nBERT for downstream classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 08:45:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Ong", "Desmond C.", ""]]}, {"id": "2101.00204", "submitter": "Rifat Shahriyar", "authors": "Abhik Bhattacharjee, Tahmid Hasan, Kazi Samin, M. Sohel Rahman,\n  Anindya Iqbal, Rifat Shahriyar", "title": "BanglaBERT: Combating Embedding Barrier for Low-Resource Language\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training language models on large volume of data with self-supervised\nobjectives has become a standard practice in natural language processing.\nHowever, most such state-of-the-art models are available in only English and\nother resource-rich languages. Even in multilingual models, which are trained\non hundreds of languages, low-resource ones still remain underrepresented.\nBangla, the seventh most widely spoken language in the world, is still low in\nterms of resources. Few downstream task datasets for language understanding in\nBangla are publicly available, and there is a clear shortage of good quality\ndata for pre-training. In this work, we build a Bangla natural language\nunderstanding model pre-trained on 18.6 GB data we crawled from top Bangla\nsites on the internet. We introduce a new downstream task dataset and benchmark\non four tasks on sentence classification, document classification, natural\nlanguage understanding, and sequence tagging. Our model outperforms\nmultilingual baselines and previous state-of-the-art results by 1-6%. In the\nprocess, we identify a major shortcoming of multilingual models that hurt\nperformance for low-resource languages that don't share writing scripts with\nany high resource one, which we name the `Embedding Barrier'. We perform\nextensive experiments to study this barrier. We release all our datasets and\npre-trained models to aid future NLP research on Bangla and other low-resource\nlanguages. Our code and data are available at\nhttps://github.com/csebuetnlp/banglabert.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 09:28:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bhattacharjee", "Abhik", ""], ["Hasan", "Tahmid", ""], ["Samin", "Kazi", ""], ["Rahman", "M. Sohel", ""], ["Iqbal", "Anindya", ""], ["Shahriyar", "Rifat", ""]]}, {"id": "2101.00234", "submitter": "Machel Reid", "authors": "Machel Reid, Edison Marrese-Taylor and Yutaka Matsuo", "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in\n  Generative Transformers", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of the Transformer can arguably be described as a driving force\nbehind many of the recent advances in natural language processing. However,\ndespite their sizeable performance improvements, as recently shown, the model\nis severely over-parameterized, being parameter inefficient and computationally\nexpensive to train. Inspired by the success of parameter-sharing in pretrained\ndeep contextualized word representation encoders, we explore parameter-sharing\nmethods in Transformers, with a specific focus on encoder-decoder models for\nsequence-to-sequence tasks such as neural machine translation. We perform an\nanalysis of different parameter sharing/reduction methods and develop the\nSubformer, a parameter efficient Transformer-based model which combines the\nnewly proposed Sandwich-style parameter sharing technique - designed to\novercome the deficiencies in naive cross-layer parameter sharing for generative\nmodels - and self-attentive embedding factorization (SAFE). Experiments on\nmachine translation, abstractive summarization, and language modeling show that\nthe Subformer can outperform the Transformer even when using significantly\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 13:53:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Reid", "Machel", ""], ["Marrese-Taylor", "Edison", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2101.00259", "submitter": "Sajad Norouzi", "authors": "Sajad Norouzi, Keyi Tang, Yanshuai Cao", "title": "Code Generation from Natural Language with Less Prior and More\n  Monolingual Data", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training datasets for semantic parsing are typically small due to the higher\nexpertise required for annotation than most other NLP tasks. As a result,\nmodels for this application usually need additional prior knowledge to be built\ninto the architecture or algorithm. The increased dependency on human experts\nhinders automation and raises the development and maintenance costs in\npractice. This work investigates whether a generic transformer-based seq2seq\nmodel can achieve competitive performance with minimal code-generation-specific\ninductive bias design. By exploiting a relatively sizeable monolingual corpus\nof the target programming language, which is cheap to mine from the web, we\nachieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa.\nBoth are SOTA to the best of our knowledge. This positive evidence highlights a\npotentially easier path toward building accurate semantic parsers in practice.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:02:38 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:51:02 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Norouzi", "Sajad", ""], ["Tang", "Keyi", ""], ["Cao", "Yanshuai", ""]]}, {"id": "2101.00265", "submitter": "Xiaopeng Lu", "authors": "Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee", "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale\n  Text-to-Image Search with Weighted Bag-of-words", "comments": "Accepted to ACL2021 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:29:17 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:10:15 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lu", "Xiaopeng", ""], ["Zhao", "Tiancheng", ""], ["Lee", "Kyusong", ""]]}, {"id": "2101.00288", "submitter": "Tongshuang Wu", "authors": "Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel S. Weld", "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and\n  Improving Models", "comments": "ACL 2021, main conference, long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While counterfactual examples are useful for analysis and training of NLP\nmodels, current generation methods either rely on manual labor to create very\nfew counterfactuals, or only instantiate limited types of perturbations such as\nparaphrases or word substitutions. We present Polyjuice, a general-purpose\ncounterfactual generator that allows for control over perturbation types and\nlocations, trained by finetuning GPT-2 on multiple datasets of paired\nsentences. We show that Polyjuice produces diverse sets of realistic\ncounterfactuals, which in turn are useful in various distinct applications:\nimproving training and evaluation on three different tasks (with around 70%\nless annotation effort than manual generation), augmenting state-of-the-art\nexplanation techniques, and supporting systematic counterfactual error analysis\nby revealing behaviors easily missed by human experts.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:34:22 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 17:13:45 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wu", "Tongshuang", ""], ["Ribeiro", "Marco Tulio", ""], ["Heer", "Jeffrey", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2101.00294", "submitter": "Yuning Mao", "authors": "Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,\n  Jiawei Han, Weizhu Chen", "title": "Reader-Guided Passage Reranking for Open-Domain Question Answering", "comments": "Findings of ACL 2021 Camera-ready. TLDR: Reranking retrieved passages\n  by reader predictions can achieve 10~20 gains in top-1 retrieval accuracy and\n  1~4 gains in Exact Match (EM) without any training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:54:19 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 22:17:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mao", "Yuning", ""], ["He", "Pengcheng", ""], ["Liu", "Xiaodong", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["Han", "Jiawei", ""], ["Chen", "Weizhu", ""]]}, {"id": "2101.00297", "submitter": "Jeff Da", "authors": "Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut", "title": "Understanding Few-Shot Commonsense Knowledge Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing natural language processing systems with commonsense knowledge is a\ncritical challenge for achieving language understanding. Recently, commonsense\nknowledge models have emerged as a suitable approach for hypothesizing\nsituation-relevant commonsense knowledge on-demand in natural language\napplications. However, these systems are limited by the fixed set of relations\ncaptured by schemas of the knowledge bases on which they're trained.\n  To address this limitation, we investigate training commonsense knowledge\nmodels in a few-shot setting with limited tuples per commonsense relation in\nthe graph. We perform five separate studies on different dimensions of few-shot\ncommonsense knowledge learning, providing a roadmap on best practices for\ntraining these systems efficiently. Importantly, we find that human quality\nratings for knowledge produced from a few-shot trained system can achieve\nperformance within 6% of knowledge produced from fully supervised systems. This\nfew-shot performance enables coverage of a wide breadth of relations in future\ncommonsense systems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 19:01:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Da", "Jeff", ""], ["Bras", "Ronan Le", ""], ["Lu", "Ximing", ""], ["Choi", "Yejin", ""], ["Bosselut", "Antoine", ""]]}, {"id": "2101.00345", "submitter": "Yasumasa Onoe", "authors": "Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett", "title": "Modeling Fine-Grained Entity Types with Box Embeddings", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural entity typing models typically represent fine-grained entity types as\nvectors in a high-dimensional space, but such spaces are not well-suited to\nmodeling these types' complex interdependencies. We study the ability of box\nembeddings, which embed concepts as d-dimensional hyperrectangles, to capture\nhierarchies of types even when these relationships are not defined explicitly\nin the ontology. Our model represents both types and entity mentions as boxes.\nEach mention and its context are fed into a BERT-based model to embed that\nmention in our box space; essentially, this model leverages typological clues\npresent in the surface text to hypothesize a type representation for the\nmention. Box containment can then be used to derive both the posterior\nprobability of a mention exhibiting a given type and the conditional\nprobability relations between types themselves. We compare our approach with a\nvector-based typing model and observe state-of-the-art performance on several\nentity typing benchmarks. In addition to competitive typing performance, our\nbox-based model shows better performance in prediction consistency (predicting\na supertype and a subtype together) and confidence (i.e., calibration),\ndemonstrating that the box-based model captures the latent type hierarchies\nbetter than the vector-based model does.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 00:59:10 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 05:51:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Onoe", "Yasumasa", ""], ["Boratko", "Michael", ""], ["McCallum", "Andrew", ""], ["Durrett", "Greg", ""]]}, {"id": "2101.00371", "submitter": "Yue Dong", "authors": "Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D. Hwang, Antoine\n  Bosselut, Jackie Chi Kit Cheung, Yejin Choi", "title": "On-the-Fly Attention Modularization for Neural Generation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite considerable advancements with deep neural language models (LMs),\nneural text generation still suffers from degeneration: generated text is\nrepetitive, generic, self-inconsistent, and lacking commonsense. The empirical\nanalyses on sentence-level attention patterns reveal that neural text\ndegeneration may be associated with insufficient learning of inductive biases\nby the attention mechanism. Our findings motivate on-the-fly attention\nmodularization, a simple but effective method for injecting inductive biases\ninto attention computation during inference. The resulting text produced by the\nlanguage model with attention modularization can yield enhanced diversity and\ncommonsense reasoning while maintaining fluency and coherence.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 05:16:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dong", "Yue", ""], ["Bhagavatula", "Chandra", ""], ["Lu", "Ximing", ""], ["Hwang", "Jena D.", ""], ["Bosselut", "Antoine", ""], ["Cheung", "Jackie Chi Kit", ""], ["Choi", "Yejin", ""]]}, {"id": "2101.00376", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, Xiang Ren", "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic\n  Creativity and Commonsense Knowledge", "comments": "Accepted to ACL 2021 (Findings). Project page:\n  https://inklab.usc.edu/RiddleSense", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question: I have five fingers but I am not alive. What am I? Answer: a glove.\nAnswering such a riddle-style question is a challenging cognitive process, in\nthat it requires complex commonsense reasoning abilities, an understanding of\nfigurative language, and counterfactual reasoning skills, which are all\nimportant abilities for advanced natural language understanding (NLU). However,\nthere are currently no dedicated datasets aiming to test these abilities.\nHerein, we present RiddleSense, a new multiple-choice question answering task,\nwhich comes with the first large dataset (5.7k examples) for answering\nriddle-style commonsense questions. We systematically evaluate a wide range of\nmodels over the challenge, and point out that there is a large gap between the\nbest-supervised model and human performance -- suggesting intriguing future\nresearch in the direction of higher-order commonsense reasoning and linguistic\ncreativity towards building advanced NLU systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 05:28:15 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 22:50:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Wu", "Ziyi", ""], ["Yang", "Yichi", ""], ["Lee", "Dong-Ho", ""], ["Ren", "Xiang", ""]]}, {"id": "2101.00379", "submitter": "Sharon Levy", "authors": "Sharon Levy, Michael Saxon, William Yang Wang", "title": "Investigating Memorization of Conspiracy Theories in Text Generation", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of natural language generation (NLG) models can leave\nindividuals vulnerable to the generation of harmful information memorized by\nthe models, such as conspiracy theories. While previous studies examine\nconspiracy theories in the context of social media, they have not evaluated\ntheir presence in the new space of generative language models. In this work, we\ninvestigate the capability of language models to generate conspiracy theory\ntext. Specifically, we aim to answer: can we test pretrained generative\nlanguage models for the memorization and elicitation of conspiracy theories\nwithout access to the model's training data? We highlight the difficulties of\nthis task and discuss it in the context of memorization, generalization, and\nhallucination. Utilizing a new dataset consisting of conspiracy theory topics\nand machine-generated conspiracy theories helps us discover that many\nconspiracy theories are deeply rooted in the pretrained language models. Our\nexperiments demonstrate a relationship between model parameters such as size\nand temperature and their propensity to generate conspiracy theory text. These\nresults indicate the need for a more thorough review of NLG applications before\nrelease and an in-depth discussion of the drawbacks of memorization in\ngenerative language models.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 05:47:39 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:57:20 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 18:00:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Levy", "Sharon", ""], ["Saxon", "Michael", ""], ["Wang", "William Yang", ""]]}, {"id": "2101.00387", "submitter": "Jui Shah Shah", "authors": "Jui Shah, Yaman Kumar Singla, Changyou Chen, Rajiv Ratn Shah", "title": "What all do audio transformer models hear? Probing Acoustic\n  Representations for Language Delivery and its Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 06:29:12 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 22:46:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Shah", "Jui", ""], ["Singla", "Yaman Kumar", ""], ["Chen", "Changyou", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2101.00388", "submitter": "Houjin Yu", "authors": "Houjin Yu, Xian-Ling Mao, Zewen Chi, Wei Wei and Heyan Huang", "title": "A Robust and Domain-Adaptive Approach for Low-Resource Named Entity\n  Recognition", "comments": "Best Student Paper of 2020 IEEE International Conference on Knowledge\n  Graph (ICKG)", "journal-ref": "2020 IEEE International Conference on Knowledge Graph (ICKG) (pp.\n  297-304)-", "doi": "10.1109/ICBK50248.2020.00050", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has attracted much attention to build reliable named entity\nrecognition (NER) systems using limited annotated data. Nearly all existing\nworks heavily rely on domain-specific resources, such as external lexicons and\nknowledge bases. However, such domain-specific resources are often not\navailable, meanwhile it's difficult and expensive to construct the resources,\nwhich has become a key obstacle to wider adoption. To tackle the problem, in\nthis work, we propose a novel robust and domain-adaptive approach RDANER for\nlow-resource NER, which only uses cheap and easily obtainable resources.\nExtensive experiments on three benchmark datasets demonstrate that our approach\nachieves the best performance when only using cheap and easily obtainable\nresources, and delivers competitive results against state-of-the-art methods\nwhich use difficultly obtainable domainspecific resources. All our code and\ncorpora can be found on https://github.com/houking-can/RDANER.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 06:47:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yu", "Houjin", ""], ["Mao", "Xian-Ling", ""], ["Chi", "Zewen", ""], ["Wei", "Wei", ""], ["Huang", "Heyan", ""]]}, {"id": "2101.00389", "submitter": "Alexander Spangher", "authors": "Alexander Spangher, Jonathan May, Sz-rung Shiang and Lingjia Deng", "title": "Multitask Learning for Class-Imbalanced Discourse Classification", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small class-imbalanced datasets, common in many high-level semantic tasks\nlike discourse analysis, present a particular challenge to current\ndeep-learning architectures. In this work, we perform an extensive analysis on\nsentence-level classification approaches for the News Discourse dataset, one of\nthe largest high-level semantic discourse datasets recently published. We show\nthat a multitask approach can improve 7% Micro F1-score upon current\nstate-of-the-art benchmarks, due in part to label corrections across tasks,\nwhich improve performance for underrepresented classes. We also offer a\ncomparative review of additional techniques proposed to address resource-poor\nproblems in NLP, and show that none of these approaches can improve\nclassification accuracy in such a setting.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:13:41 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Spangher", "Alexander", ""], ["May", "Jonathan", ""], ["Shiang", "Sz-rung", ""], ["Deng", "Lingjia", ""]]}, {"id": "2101.00390", "submitter": "Changhan Wang", "authors": "Changhan Wang, Morgane Rivi\\`ere, Ann Lee, Anne Wu, Chaitanya\n  Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, Emmanuel Dupoux", "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation\n  Learning, Semi-Supervised Learning and Interpretation", "comments": "Accepted to ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps://github.com/facebookresearch/voxpopuli under an open license.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:24:21 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 04:04:57 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wang", "Changhan", ""], ["Rivi\u00e8re", "Morgane", ""], ["Lee", "Ann", ""], ["Wu", "Anne", ""], ["Talnikar", "Chaitanya", ""], ["Haziza", "Daniel", ""], ["Williamson", "Mary", ""], ["Pino", "Juan", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2101.00391", "submitter": "Najoung Kim", "authors": "Najoung Kim, Ellie Pavlick, Burcu Karagol Ayan, Deepak Ramachandran", "title": "Which Linguist Invented the Lightbulb? Presupposition Verification for\n  Question-Answering", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Question-Answering (QA) datasets contain unanswerable questions, but\ntheir treatment in QA systems remains primitive. Our analysis of the Natural\nQuestions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion\nof unanswerable questions ($\\sim$21%) can be explained based on the presence of\nunverifiable presuppositions. We discuss the shortcomings of current models in\nhandling such questions, and describe how an improved system could handle them.\nThrough a user preference study, we demonstrate that the oracle behavior of our\nproposed system that provides responses based on presupposition failure is\npreferred over the oracle behavior of existing QA systems. Then we discuss how\nour proposed system could be implemented, presenting a novel framework that\nbreaks down the problem into three steps: presupposition generation,\npresupposition verification and explanation generation. We report our progress\nin tackling each subproblem, and present a preliminary approach to integrating\nthese steps into an existing QA system. We find that adding presuppositions and\ntheir verifiability to an existing model yields modest gains in downstream\nperformance and unanswerability detection. The biggest bottleneck is the\nverification component, which needs to be substantially improved for the\nintegrated system to approach ideal behavior -- even transfer from the best\nentailment models currently falls short.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:26:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kim", "Najoung", ""], ["Pavlick", "Ellie", ""], ["Ayan", "Burcu Karagol", ""], ["Ramachandran", "Deepak", ""]]}, {"id": "2101.00394", "submitter": "Hao Fei", "authors": "Hao Fei, Meishan Zhang, Bobo Li, Donghong Ji", "title": "End-to-end Semantic Role Labeling with Neural Transition-based Model", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end semantic role labeling (SRL) has been received increasing\ninterest. It performs the two subtasks of SRL: predicate identification and\nargument role labeling, jointly. Recent work is mostly focused on graph-based\nneural models, while the transition-based framework with neural networks which\nhas been widely used in a number of closely-related tasks, has not been studied\nfor the joint task yet. In this paper, we present the first work of\ntransition-based neural models for end-to-end SRL. Our transition model\nincrementally discovers all sentential predicates as well as their arguments by\na set of transition actions. The actions of the two subtasks are executed\nmutually for full interactions. Besides, we suggest high-order compositions to\nextract non-local features, which can enhance the proposed transition model\nfurther. Experimental results on CoNLL09 and Universal Proposition Bank show\nthat our final model can produce state-of-the-art performance, and meanwhile\nkeeps highly efficient in decoding. We also conduct detailed experimental\nanalysis for a deep understanding of our proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:35:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fei", "Hao", ""], ["Zhang", "Meishan", ""], ["Li", "Bobo", ""], ["Ji", "Donghong", ""]]}, {"id": "2101.00396", "submitter": "Wei Zhu", "authors": "Wei Zhu, Daniel Cheung", "title": "Lex-BERT: Enhancing BERT based NER with lexicons", "comments": "Will incorporate new ideas, and more experiments, and better\n  descriptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we represent Lex-BERT, which incorporates the lexicon\ninformation into Chinese BERT for named entity recognition (NER) tasks in a\nnatural manner. Instead of using word embeddings and a newly designed\ntransformer layer as in FLAT, we identify the boundary of words in the\nsentences using special tokens, and the modified sentence will be encoded\ndirectly by BERT. Our model does not introduce any new parameters and are more\nefficient than FLAT. In addition, we do not require any word embeddings\naccompanying the lexicon collection. Experiments on Ontonotes and ZhCrossNER\nshow that our model outperforms FLAT and other baselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:43:21 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 07:30:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Wei", ""], ["Cheung", "Daniel", ""]]}, {"id": "2101.00403", "submitter": "Valentin Hofmann", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\\\"utze", "title": "Superbizarre Is Not Superb: Derivational Morphology Improves BERT's\n  Interpretation of Complex Words", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does the input segmentation of pretrained language models (PLMs) affect\ntheir interpretations of complex words? We present the first study\ninvestigating this question, taking BERT as the example PLM and focusing on its\nsemantic representations of English derivatives. We show that PLMs can be\ninterpreted as serial dual-route models, i.e., the meanings of complex words\nare either stored or else need to be computed from the subwords, which implies\nthat maximally meaningful input tokens should allow for the best generalization\non new words. This hypothesis is confirmed by a series of semantic probing\ntasks on which DelBERT (Derivation leveraging BERT), a model with derivational\ninput segmentation, substantially outperforms BERT with WordPiece segmentation.\nOur results suggest that the generalization capabilities of PLMs could be\nfurther improved if a morphologically-informed vocabulary of input tokens were\nused.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 08:36:48 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 16:05:02 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 14:00:26 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hofmann", "Valentin", ""], ["Pierrehumbert", "Janet B.", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2101.00406", "submitter": "Arman Cohan", "authors": "Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie\n  Cattan, Ido Dagan", "title": "Cross-Document Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new pretraining approach for language models that are geared\nto support multi-document NLP tasks. Our cross-document language model (CD-LM)\nimproves masked language modeling for these tasks with two key ideas. First, we\npretrain with multiple related documents in a single input, via cross-document\nmasking, which encourages the model to learn cross-document and long-range\nrelationships. Second, extending the recent Longformer model, we pretrain with\nlong contexts of several thousand tokens and introduce a new attention pattern\nthat uses sequence-level global attention to predict masked tokens, while\nretaining the familiar local attention elsewhere. We show that our CD-LM sets\nnew state-of-the-art results for several multi-text tasks, including\ncross-document event and entity coreference resolution, paper citation\nrecommendation, and documents plagiarism detection, while using a significantly\nreduced number of training parameters relative to prior works.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 09:01:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Caciularu", "Avi", ""], ["Cohan", "Arman", ""], ["Beltagy", "Iz", ""], ["Peters", "Matthew E.", ""], ["Cattan", "Arie", ""], ["Dagan", "Ido", ""]]}, {"id": "2101.00408", "submitter": "Devendra Singh Sachan", "authors": "Devendra Singh Sachan and Mostofa Patwary and Mohammad Shoeybi and\n  Neel Kant and Wei Ping and William L Hamilton and Bryan Catanzaro", "title": "End-to-End Training of Neural Retrievers for Open-Domain Question\n  Answering", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on training neural retrievers for open-domain question answering\n(OpenQA) has employed both supervised and unsupervised approaches. However, it\nremains unclear how unsupervised and supervised methods can be used most\neffectively for neural retrievers. In this work, we systematically study\nretriever pre-training. We first propose an approach of unsupervised\npre-training with the Inverse Cloze Task and masked salient spans, followed by\nsupervised finetuning using question-context pairs. This approach leads to\nabsolute gains of 2+ points over the previous best result in the top-20\nretrieval accuracy on Natural Questions and TriviaQA datasets.\n  We also explore two approaches for end-to-end supervised training of the\nreader and retriever components in OpenQA models. In the first approach, the\nreader considers each retrieved document separately while in the second\napproach, the reader considers all the retrieved documents together. Our\nexperiments demonstrate the effectiveness of these approaches as we obtain new\nstate-of-the-art results. On the Natural Questions dataset, we obtain a top-20\nretrieval accuracy of 84, an improvement of 5 points over the recent DPR model.\nIn addition, we achieve good results on answer extraction, outperforming recent\nmodels like REALM and RAG by 3+ points. We further scale up end-to-end training\nto large models and show consistent gains in performance over smaller models.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 09:05:34 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 02:46:38 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Sachan", "Devendra Singh", ""], ["Patwary", "Mostofa", ""], ["Shoeybi", "Mohammad", ""], ["Kant", "Neel", ""], ["Ping", "Wei", ""], ["Hamilton", "William L", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2101.00411", "submitter": "Haoyue Shi", "authors": "Haoyue Shi, Karen Livescu, Kevin Gimpel", "title": "Substructure Substitution: Structured Data Augmentation for NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of data augmentation methods, substructure substitution\n(SUB2), for natural language processing (NLP) tasks. SUB2 generates new\nexamples by substituting substructures (e.g., subtrees or subsequences) with\nones with the same label, which can be applied to many structured NLP tasks\nsuch as part-of-speech tagging and parsing. For more general tasks (e.g., text\nclassification) which do not have explicitly annotated substructures, we\npresent variations of SUB2 based on constituency parse trees, introducing\nstructure-aware data augmentation methods to general NLP tasks. For most cases,\ntraining with the augmented dataset by SUB2 achieves better performance than\ntraining with the original training set. Further experiments show that SUB2 has\nmore consistent performance than other investigated augmentation methods,\nacross different tasks and sizes of the seed dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 09:54:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shi", "Haoyue", ""], ["Livescu", "Karen", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2101.00416", "submitter": "Wangchunshu Zhou", "authors": "Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei", "title": "Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we generalize text infilling (e.g., masked language models) by\nproposing Sequence Span Rewriting (SSR) as a self-supervised\nsequence-to-sequence (seq2seq) pre-training objective. SSR provides more\nfine-grained learning signals for text representations by supervising the model\nto rewrite imperfect spans to ground truth, and it is more consistent than text\ninfilling with many downstream seq2seq tasks that rewrite a source sentences\ninto a target sentence. Our experiments with T5 models on various seq2seq tasks\nshow that SSR can substantially improve seq2seq pre-training. Moreover, we\nobserve SSR is especially helpful to improve pre-training a small-size seq2seq\nmodel with a powerful imperfect span generator, which indicates a new\nperspective of transferring knowledge from a large model to a smaller model for\nseq2seq pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:27:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Wangchunshu", ""], ["Ge", "Tao", ""], ["Xu", "Ke", ""], ["Wei", "Furu", ""]]}, {"id": "2101.00419", "submitter": "Zhao Meng", "authors": "Yiran Xing, Zai Shi, Zhao Meng, Gerhard Lakemeyer, Yunpu Ma, Roger\n  Wattenhofer", "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense\n  Generation", "comments": "ACL-IJCNLP 2021 main conference. The first three authors contribute\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Knowledge Enhanced Multimodal BART (KM-BART), which is a\nTransformer-based sequence-to-sequence model capable of reasoning about\ncommonsense knowledge from multimodal inputs of images and texts. We adapt the\ngenerative BART architecture to a multimodal model with visual and textual\ninputs. We further develop novel pretraining tasks to improve the model\nperformance on the Visual Commonsense Generation (VCG) task. In particular, our\npretraining task of Knowledge-based Commonsense Generation (KCG) boosts model\nperformance on the VCG task by leveraging commonsense knowledge from a large\nlanguage model pretrained on external commonsense knowledge graphs. To the best\nof our knowledge, we are the first to propose a dedicated task for improving\nmodel performance on the VCG task. Experimental results show that our model\nreaches state-of-the-art performance on the VCG task by applying these novel\npretraining tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:44:49 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 21:33:21 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Xing", "Yiran", ""], ["Shi", "Zai", ""], ["Meng", "Zhao", ""], ["Lakemeyer", "Gerhard", ""], ["Ma", "Yunpu", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2101.00420", "submitter": "Qinyuan Ye", "authors": "Qinyuan Ye, Xiang Ren", "title": "Learning to Generate Task-Specific Adapters from Task Description", "comments": "Accepted to ACL 2021. Camera-ready version. Code:\n  https://github.com/INK-USC/hypter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained text-to-text transformers such as BART have achieved impressive\nperformance across a range of NLP tasks. Recent study further shows that they\ncan learn to generalize to novel tasks, by including task descriptions as part\nof the source sequence and training the model with (source, target) examples.\nAt test time, these fine-tuned models can make inferences on new tasks using\nthe new task descriptions as part of the input. However, this approach has\npotential limitations, as the model learns to solve individual (source, target)\nexamples (i.e., at the instance level), instead of learning to solve tasks by\ntaking all examples within a task as a whole (i.e., at the task level). To this\nend, we introduce Hypter, a framework that improves text-to-text transformer's\ngeneralization ability to unseen tasks by training a hypernetwork to generate\ntask-specific, light-weight adapters from task descriptions. Experiments on\nZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves\nupon fine-tuning baselines. Notably, when using BART-Large as the main network,\nHypter brings 11.3% comparative improvement on ZEST dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:50:23 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 05:50:01 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ye", "Qinyuan", ""], ["Ren", "Xiang", ""]]}, {"id": "2101.00421", "submitter": "Nikolay Bogoychev Dr", "authors": "Nikolay Bogoychev and Pinzhen Chen", "title": "Decoding Time Lexical Domain Adaptation for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation systems are vulnerable to domain mismatch, especially\nwhen the task is low-resource. In this setting, out of domain translations are\noften of poor quality and prone to hallucinations, due to the translation model\npreferring to predict common words it has seen during training, as opposed to\nthe more uncommon ones from a different domain. We present two simple methods\nfor improving translation quality in this particular setting: First, we use\nlexical shortlisting in order to restrict the neural network predictions by IBM\nmodel computed alignments. Second, we perform $n$-best list reordering by\nreranking all translations based on the amount they overlap with each other.\nOur methods are computationally simpler and faster than alternative approaches,\nand show a moderate success on low-resource settings with explicit out of\ndomain test sets. However, our methods lose their effectiveness when the domain\nmismatch is too great, or in high resource setting.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:06:15 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 10:58:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bogoychev", "Nikolay", ""], ["Chen", "Pinzhen", ""]]}, {"id": "2101.00430", "submitter": "Gerard De Melo", "authors": "Abu Awal Md Shoeb and Gerard de Melo", "title": "Assessing Emoji Use in Modern Text Processing Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emojis have become ubiquitous in digital communication, due to their visual\nappeal as well as their ability to vividly convey human emotion, among other\nfactors. The growing prominence of emojis in social media and other instant\nmessaging also leads to an increased need for systems and tools to operate on\ntext containing emojis. In this study, we assess this support by considering\ntest sets of tweets with emojis, based on which we perform a series of\nexperiments investigating the ability of prominent NLP and text processing\ntools to adequately process them. In particular, we consider tokenization,\npart-of-speech tagging, as well as sentiment analysis. Our findings show that\nmany tools still have notable shortcomings when operating on text containing\nemojis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:38:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shoeb", "Abu Awal Md", ""], ["de Melo", "Gerard", ""]]}, {"id": "2101.00433", "submitter": "Michael Saxon", "authors": "Michael Saxon, Sharon Levy, Xinyi Wang, Alon Albalak, William Yang\n  Wang", "title": "Modeling Discolsive Transparency in NLP Application Descriptions", "comments": "14 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nPrevious work has suggested that a trade-off exists between greater disclosive\ntransparency and user confusion, where 'too much information' clouds a reader's\nunderstanding of what a system description means. We address both of these\nissues by connecting disclosive transparency to a \"replication room\" thought\nexperiment, where the person describing the system attempts to convey the\nrequisite information for a third party to reconstruct it. In this setting, the\ndegree to which the necessary information is conveyed represents the\ndescription's transparency, and the level of expertise needed by the third\nparty corresponds to potential user confusion. We introduce two neural language\nmodel-based probabilistic metrics to model these factors, and demonstrate that\nthey correlate with user and expert opinions of system transparency, making\nthem a valid objective proxy. Finally, we apply these metrics to study the\nrelationships between transparency, confusion, and user perceptions in a corpus\nof NLP demo abstracts.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:46:17 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:42:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Saxon", "Michael", ""], ["Levy", "Sharon", ""], ["Wang", "Xinyi", ""], ["Albalak", "Alon", ""], ["Wang", "William Yang", ""]]}, {"id": "2101.00434", "submitter": "Yuval Kirstain", "authors": "Yuval Kirstain, Ori Ram, Omer Levy", "title": "Coreference Resolution without Span Representations", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The introduction of pretrained language models has reduced many complex\ntask-specific NLP models to simple lightweight layers. An exception to this\ntrend is coreference resolution, where a sophisticated task-specific model is\nappended to a pretrained transformer encoder. While highly effective, the model\nhas a very large memory footprint -- primarily due to dynamically-constructed\nspan and span-pair representations -- which hinders the processing of complete\ndocuments and the ability to train on multiple instances in a single batch. We\nintroduce a lightweight end-to-end coreference model that removes the\ndependency on span representations, handcrafted features, and heuristics. Our\nmodel performs competitively with the current standard model, while being\nsimpler and more efficient.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:46:51 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:57:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kirstain", "Yuval", ""], ["Ram", "Ori", ""], ["Levy", "Omer", ""]]}, {"id": "2101.00436", "submitter": "Omar Khattab", "authors": "Omar Khattab, Christopher Potts, Matei Zaharia", "title": "Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-hop reasoning (i.e., reasoning across two or more documents) is a key\ningredient for NLP models that leverage large corpora to exhibit broad\nknowledge. To retrieve evidence passages, multi-hop models must contend with a\nfast-growing search space across the hops, represent complex queries that\ncombine multiple information needs, and resolve ambiguity about the best order\nin which to hop between training passages. We tackle these problems via Baleen,\na system that improves the accuracy and robustness of multi-hop retrieval. To\ntame the search space, we propose condensed retrieval, a pipeline that\nsummarizes the retrieved passages after each hop into a single compact context.\nTo model complex queries, we introduce a focused late interaction retriever\nthat allows different parts of the same query representation to match disparate\nrelevant passages. Lastly, to infer the hopping dependencies among unordered\ntraining passages, we devise latent hop ordering, a weak-supervision strategy\nin which the trained retriever itself selects the sequence of hops. We evaluate\nBaleen on retrieval for two-hop question answering and many-hop claim\nverification, establishing state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:52:20 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 09:56:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khattab", "Omar", ""], ["Potts", "Christopher", ""], ["Zaharia", "Matei", ""]]}, {"id": "2101.00438", "submitter": "Ori Ram", "authors": "Ori Ram and Yuval Kirstain and Jonathan Berant and Amir Globerson and\n  Omer Levy", "title": "Few-Shot Question Answering by Pretraining Span Selection", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In several question answering benchmarks, pretrained models have reached\nhuman parity through fine-tuning on an order of 100,000 annotated questions and\nanswers. We explore the more realistic few-shot setting, where only a few\nhundred training examples are available, and observe that standard models\nperform poorly, highlighting the discrepancy between current pretraining\nobjectives and question answering. We propose a new pretraining scheme tailored\nfor question answering: recurring span selection. Given a passage with multiple\nsets of recurring spans, we mask in each set all recurring spans but one, and\nask the model to select the correct span in the passage for each masked span.\nMasked spans are replaced with a special token, viewed as a question\nrepresentation, that is later used during fine-tuning to select the answer\nspan. The resulting model obtains surprisingly good results on multiple\nbenchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while\nmaintaining competitive performance in the high-resource setting.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:58:44 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:24:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ram", "Ori", ""], ["Kirstain", "Yuval", ""], ["Berant", "Jonathan", ""], ["Globerson", "Amir", ""], ["Levy", "Omer", ""]]}, {"id": "2101.00529", "submitter": "Pengchuan Zhang", "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang,\n  Lijuan Wang, Yejin Choi, Jianfeng Gao", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 23:35:27 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 01:27:16 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Pengchuan", ""], ["Li", "Xiujun", ""], ["Hu", "Xiaowei", ""], ["Yang", "Jianwei", ""], ["Zhang", "Lei", ""], ["Wang", "Lijuan", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2101.00540", "submitter": "Zeming Chen", "authors": "Zeming Chen", "title": "Attentive Tree-structured Network for Monotonicity Reasoning", "comments": "Proceeding of the First Workshop on Natural Logic Meets Machine\n  Learning, Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many state-of-art neural models designed for monotonicity reasoning perform\npoorly on downward inference. To address this shortcoming, we developed an\nattentive tree-structured neural network. It consists of a tree-based\nlong-short-term-memory network (Tree-LSTM) with soft attention. It is designed\nto model the syntactic parse tree information from the sentence pair of a\nreasoning task. A self-attentive aggregator is used for aligning the\nrepresentations of the premise and the hypothesis. We present our model and\nevaluate it using the Monotonicity Entailment Dataset (MED). We show and\nattempt to explain that our model outperforms existing models on MED.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 01:29:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Zeming", ""]]}, {"id": "2101.00542", "submitter": "Ye Lin", "authors": "Yanyang Li, Ye Lin, Tong Xiao, Jingbo Zhu", "title": "An Efficient Transformer Decoder with Compressed Sub-layers", "comments": "accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 02:05:01 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 06:30:58 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Yanyang", ""], ["Lin", "Ye", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""]]}, {"id": "2101.00563", "submitter": "Sahil Sidheekh", "authors": "Sahil Sidheekh", "title": "Learning Neural Networks on SVD Boosted Latent Spaces for Semantic\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 05:30:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sidheekh", "Sahil", ""]]}, {"id": "2101.00674", "submitter": "Dennis Ulmer", "authors": "Dennis Ulmer", "title": "Recoding latent sentence representations -- Dynamic gradient-based\n  activation modification in RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Recurrent Neural Networks (RNNs), encoding information in a suboptimal or\nerroneous way can impact the quality of representations based on later elements\nin the sequence and subsequently lead to wrong predictions and a worse model\nperformance. In humans, challenging cases like garden path sentences (an\ninstance of this being the infamous \"The horse raced past the barn fell\") can\nlead their language understanding astray. However, they are still able to\ncorrect their representation accordingly and recover when new information is\nencountered. Inspired by this, I propose an augmentation to standard RNNs in\nform of a gradient-based correction mechanism: This way I hope to enable such\nmodels to dynamically adapt their inner representation of a sentence, adding a\nway to correct deviations as soon as they occur. This could therefore lead to\nmore robust models using more flexible representations, even during inference\ntime.\n  I conduct different experiments in the context of language modeling, where\nthe impact of using such a mechanism is examined in detail. To this end, I look\nat modifications based on different kinds of time-dependent error signals and\nhow they influence the model performance. Furthermore, this work contains a\nstudy of the model's confidence in its predictions during training and for\nchallenging test samples and the effect of the manipulation thereof. Lastly, I\nalso study the difference in behavior of these novel models compared to a\nstandard LSTM baseline and investigate error cases in detail to identify points\nof future research. I show that while the proposed approach comes with\npromising theoretical guarantees and an appealing intuition, it is only able to\nproduce minor improvements over the baseline due to challenges in its practical\napplication and the efficacy of the tested model variants.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 17:54:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ulmer", "Dennis", ""]]}, {"id": "2101.00737", "submitter": "Xin Tan", "authors": "Xin Tan, Longyin Zhang and Guodong Zhou", "title": "Are Eliminated Spans Useless for Coreference Resolution? Not at all", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various neural-based methods have been proposed so far for joint mention\ndetection and coreference resolution. However, existing works on coreference\nresolution are mainly dependent on filtered mention representation, while other\nspans are largely neglected. In this paper, we aim at increasing the\nutilization rate of data and investigating whether those eliminated spans are\ntotally useless, or to what extent they can improve the performance of\ncoreference resolution. To achieve this, we propose a mention representation\nrefining strategy where spans highly related to mentions are well leveraged\nusing a pointer network for representation enhancing. Notably, we utilize an\nadditional loss term in this work to encourage the diversity between entity\nclusters. Experimental results on the document-level CoNLL-2012 Shared Task\nEnglish dataset show that eliminated spans are indeed much effective and our\napproach can achieve competitive results when compared with previous\nstate-of-the-art in coreference resolution.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 02:02:49 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 02:36:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Tan", "Xin", ""], ["Zhang", "Longyin", ""], ["Zhou", "Guodong", ""]]}, {"id": "2101.00760", "submitter": "Ning Bian", "authors": "Ning Bian, Xianpei Han, Bo Chen, Le Sun", "title": "Benchmarking Knowledge-Enhanced Commonsense Question Answering via\n  Knowledge-to-Text Transformation", "comments": "Accepted to AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fundamental ability of humans is to utilize commonsense knowledge in\nlanguage understanding and question answering. In recent years, many\nknowledge-enhanced Commonsense Question Answering (CQA) approaches have been\nproposed. However, it remains unclear: (1) How far can we get by exploiting\nexternal knowledge for CQA? (2) How much potential of knowledge has been\nexploited in current CQA models? (3) Which are the most promising directions\nfor future CQA? To answer these questions, we benchmark knowledge-enhanced CQA\nby conducting extensive experiments on multiple standard CQA datasets using a\nsimple and effective knowledge-to-text transformation framework. Experiments\nshow that: (1) Our knowledge-to-text framework is effective and achieves\nstate-of-the-art performance on CommonsenseQA dataset, providing a simple and\nstrong knowledge-enhanced baseline for CQA; (2) The potential of knowledge is\nstill far from being fully exploited in CQA -- there is a significant\nperformance gap from current models to our models with golden knowledge; and\n(3) Context-sensitive knowledge selection, heterogeneous knowledge\nexploitation, and commonsense-rich language models are promising CQA\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 04:29:03 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 03:32:41 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bian", "Ning", ""], ["Han", "Xianpei", ""], ["Chen", "Bo", ""], ["Sun", "Le", ""]]}, {"id": "2101.00816", "submitter": "Yue Mao", "authors": "Yue Mao, Yi Shen, Chao Yu, Longjun Cai", "title": "A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis", "comments": "to appear in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect based sentiment analysis (ABSA) involves three fundamental subtasks:\naspect term extraction, opinion term extraction, and aspect-level sentiment\nclassification. Early works only focused on solving one of these subtasks\nindividually. Some recent work focused on solving a combination of two\nsubtasks, e.g., extracting aspect terms along with sentiment polarities or\nextracting the aspect and opinion terms pair-wisely. More recently, the triple\nextraction task has been proposed, i.e., extracting the (aspect term, opinion\nterm, sentiment polarity) triples from a sentence. However, previous approaches\nfail to solve all subtasks in a unified end-to-end framework. In this paper, we\npropose a complete solution for ABSA. We construct two machine reading\ncomprehension (MRC) problems and solve all subtasks by joint training two\nBERT-MRC models with parameters sharing. We conduct experiments on these\nsubtasks, and results on several benchmark datasets demonstrate the\neffectiveness of our proposed framework, which significantly outperforms\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:47:53 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 02:49:57 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mao", "Yue", ""], ["Shen", "Yi", ""], ["Yu", "Chao", ""], ["Cai", "Longjun", ""]]}, {"id": "2101.00822", "submitter": "Le Fang", "authors": "Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen", "title": "Outline to Story: Fine-grained Controllable Story Generation from\n  Cascaded Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pretrained language models have shown thrilling generation\ncapabilities, especially when they generate consistent long text in thousands\nof words with ease. However, users of these models can only control the prefix\nof sentences or certain global aspects of generated text. It is challenging to\nsimultaneously achieve fine-grained controllability and preserve the\nstate-of-the-art unconditional text generation capability. In this paper, we\nfirst propose a new task named \"Outline to Story\" (O2S) as a test bed for\nfine-grained controllable generation of long text, which generates a\nmulti-paragraph story from cascaded events, i.e. a sequence of outline events\nthat guide subsequent paragraph generation. We then create dedicate datasets\nfor future benchmarks, built by state-of-the-art keyword extraction techniques.\nFinally, we propose an extremely simple yet strong baseline method for the O2S\ntask, which fine tunes pre-trained language models on augmented sequences of\noutline-story pairs with simple language modeling objective. Our method does\nnot introduce any new parameters or perform any architecture modification,\nexcept several special tokens as delimiters to build augmented sequences.\nExtensive experiments on various datasets demonstrate state-of-the-art\nconditional story generation performance with our model, achieving better\nfine-grained controllability and user flexibility. Our paper is among the first\nones by our knowledge to propose a model and to create datasets for the task of\n\"outline to story\". Our work also instantiates research interest of\nfine-grained controllable generation of open-domain long text, where\ncontrolling inputs are represented by short text.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:16:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fang", "Le", ""], ["Zeng", "Tao", ""], ["Liu", "Chaochun", ""], ["Bo", "Liefeng", ""], ["Dong", "Wen", ""], ["Chen", "Changyou", ""]]}, {"id": "2101.00828", "submitter": "Le Fang", "authors": "Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen", "title": "Transformer-based Conditional Variational Autoencoder for Controllable\n  Story Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:31:11 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:18:13 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Fang", "Le", ""], ["Zeng", "Tao", ""], ["Liu", "Chaochun", ""], ["Bo", "Liefeng", ""], ["Dong", "Wen", ""], ["Chen", "Changyou", ""]]}, {"id": "2101.00916", "submitter": "Mengge He", "authors": "Li Liu, Mengge He, Guanghui Xu, Mingkui Tan, Qi Wu", "title": "How to Train Your Agent to Read and Write", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading and writing research papers is one of the most privileged abilities\nthat a qualified researcher should master. However, it is difficult for new\nresearchers (\\eg{students}) to fully {grasp} this ability. It would be\nfascinating if we could train an intelligent agent to help people read and\nsummarize papers, and perhaps even discover and exploit the potential knowledge\nclues to write novel papers. Although there have been existing works focusing\non summarizing (\\emph{i.e.}, reading) the knowledge in a given text or\ngenerating (\\emph{i.e.}, writing) a text based on the given knowledge, the\nability of simultaneously reading and writing is still under development.\nTypically, this requires an agent to fully understand the knowledge from the\ngiven text materials and generate correct and fluent novel paragraphs, which is\nvery challenging in practice. In this paper, we propose a Deep ReAder-Writer\n(DRAW) network, which consists of a \\textit{Reader} that can extract knowledge\ngraphs (KGs) from input paragraphs and discover potential knowledge, a\ngraph-to-text \\textit{Writer} that generates a novel paragraph, and a\n\\textit{Reviewer} that reviews the generated paragraph from three different\naspects. Extensive experiments show that our DRAW network outperforms\nconsidered baselines and several state-of-the-art methods on AGENDA and\nM-AGENDA datasets. Our code and supplementary are released at\nhttps://github.com/menggehe/DRAW.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:22:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Li", ""], ["He", "Mengge", ""], ["Xu", "Guanghui", ""], ["Tan", "Mingkui", ""], ["Wu", "Qi", ""]]}, {"id": "2101.00939", "submitter": "Kun Zhou", "authors": "Kun Zhou, Xiaolei Wang, Yuanhang Zhou, Chenzhan Shang, Yuan Cheng,\n  Wayne Xin Zhao, Yaliang Li, Ji-Rong Wen", "title": "CRSLab: An Open-Source Toolkit for Building Conversational Recommender\n  System", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, conversational recommender system (CRS) has received much\nattention in the research community. However, existing studies on CRS vary in\nscenarios, goals and techniques, lacking unified, standardized implementation\nor comparison. To tackle this challenge, we propose an open-source CRS toolkit\nCRSLab, which provides a unified and extensible framework with highly-decoupled\nmodules to develop CRSs. Based on this framework, we collect 6 commonly-used\nhuman-annotated CRS datasets and implement 18 models that include recent\ntechniques such as graph neural network and pre-training models. Besides, our\ntoolkit provides a series of automatic evaluation protocols and a human-machine\ninteraction interface to test and compare different CRS methods. The project\nand documents are released at https://github.com/RUCAIBox/CRSLab.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:10:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Kun", ""], ["Wang", "Xiaolei", ""], ["Zhou", "Yuanhang", ""], ["Shang", "Chenzhan", ""], ["Cheng", "Yuan", ""], ["Zhao", "Wayne Xin", ""], ["Li", "Yaliang", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2101.01039", "submitter": "Suzan Verberne", "authors": "Ken Voskuil and Suzan Verberne", "title": "Improving reference mining in patents with BERT", "comments": "10 pages, 3 figures", "journal-ref": "Published in the 11th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR 2021)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the challenge of extracting scientific references\nfrom patents. We approach the problem as a sequence labelling task and\ninvestigate the merits of BERT models to the extraction of these long\nsequences. References in patents to scientific literature are relevant to study\nthe connection between science and industry. Most prior work only uses the\nfront-page citations for this analysis, which are provided in the metadata of\npatent archives. In this paper we build on prior work using Conditional Random\nFields (CRF) and Flair for reference extraction. We improve the quality of the\ntraining data and train three BERT-based models on the labelled data (BERT,\nbioBERT, sciBERT). We find that the improved training data leads to a large\nimprovement in the quality of the trained models. In addition, the BERT models\nbeat CRF and Flair, with recall scores around 97% obtained with cross\nvalidation. With the best model we label a large collection of 33 thousand\npatents, extract the citations, and match them to publications in the Web of\nScience database. We extract 50% more references than with the old training\ndata and methods: 735 thousand references in total. With these\npatent-publication links, follow-up research will further analyze which types\nof scientific work lead to inventions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:56:21 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 10:03:15 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 11:26:01 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Voskuil", "Ken", ""], ["Verberne", "Suzan", ""]]}, {"id": "2101.01142", "submitter": "Pawe{\\l} Ksieniewicz", "authors": "Michal Choras, Konstantinos Demestichas, Agata Gielczyk, Alvaro\n  Herrero, Pawel Ksieniewicz, Konstantina Remoundou, Daniel Urda, Michal\n  Wozniak", "title": "Advanced Machine Learning Techniques for Fake News (Online\n  Disinformation) Detection: A Systematic Mapping Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news has now grown into a big problem for societies and also a major\nchallenge for people fighting disinformation. This phenomenon plagues\ndemocratic elections, reputations of individual persons or organizations, and\nhas negatively impacted citizens, (e.g., during the COVID-19 pandemic in the US\nor Brazil). Hence, developing effective tools to fight this phenomenon by\nemploying advanced Machine Learning (ML) methods poses a significant challenge.\nThe following paper displays the present body of knowledge on the application\nof such intelligent tools in the fight against disinformation. It starts by\nshowing the historical perspective and the current role of fake news in the\ninformation war. Proposed solutions based solely on the work of experts are\nanalysed and the most important directions of the application of intelligent\nsystems in the detection of misinformation sources are pointed out.\nAdditionally, the paper presents some useful resources (mainly datasets useful\nwhen assessing ML solutions for fake news detection) and provides a short\noverview of the most important R&D projects related to this subject. The main\npurpose of this work is to analyse the current state of knowledge in detecting\nfake news; on the one hand to show possible solutions, and on the other hand to\nidentify the main challenges and methodological gaps to motivate future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 13:07:42 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Choras", "Michal", ""], ["Demestichas", "Konstantinos", ""], ["Gielczyk", "Agata", ""], ["Herrero", "Alvaro", ""], ["Ksieniewicz", "Pawel", ""], ["Remoundou", "Konstantina", ""], ["Urda", "Daniel", ""], ["Wozniak", "Michal", ""]]}, {"id": "2101.01213", "submitter": "Ana Sofia Medeiros Oliveira", "authors": "Sofia Oliveira and Daniel Loureiro and Al\\'ipio Jorge", "title": "Transformers and Transfer Learning for Improving Portuguese Semantic\n  Role Labeling", "comments": "30 pages, 3 figures; Fixed broken links in References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Role Labeling (SRL) is a core Natural Language Processing task. For\nEnglish, recent methods based on Transformer models have allowed for major\nimprovements over the previous state of the art. However, for low resource\nlanguages, and in particular for Portuguese, currently available SRL models are\nhindered by scarce training data. In this paper, we explore a model\narchitecture with only a pre-trained BERT-based model, a linear layer, softmax\nand Viterbi decoding. We substantially improve the state of the art performance\nin Portuguese by over 15$F_1$. Additionally, we improve SRL results in\nPortuguese corpora by exploiting cross-lingual transfer learning using\nmultilingual pre-trained models (XLM-R), and transfer learning from dependency\nparsing in Portuguese. We evaluate the various proposed approaches empirically\nand as result we present an heuristic that supports the choice of the most\nappropriate model considering the available resources.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:56:01 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 11:05:52 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Oliveira", "Sofia", ""], ["Loureiro", "Daniel", ""], ["Jorge", "Al\u00edpio", ""]]}, {"id": "2101.01228", "submitter": "Nicholas Botzer", "authors": "Nicholas Botzer, Yifan Ding, Tim Weninger", "title": "Reddit Entity Linking Dataset", "comments": "20 pages and 4 figures", "journal-ref": "Information Processing and Management Volume 58, Issue 3 (May\n  2021) 1-20", "doi": "10.1016/j.ipm.2020.102479", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and make publicly available an entity linking dataset from\nReddit that contains 17,316 linked entities, each annotated by three human\nannotators and then grouped into Gold, Silver, and Bronze to indicate\ninter-annotator agreement. We analyze the different errors and disagreements\nmade by annotators and suggest three types of corrections to the raw data.\nFinally, we tested existing entity linking models that are trained and tuned on\ntext from non-social media datasets. We find that, although these existing\nentity linking models perform very well on their original datasets, they\nperform poorly on this social media dataset. We also show that the majority of\nthese errors can be attributed to poor performance on the mention detection\nsubtask. These results indicate the need for better entity linking models that\ncan be applied to the enormous amount of social media text.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 20:34:04 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 17:54:48 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Botzer", "Nicholas", ""], ["Ding", "Yifan", ""], ["Weninger", "Tim", ""]]}, {"id": "2101.01321", "submitter": "Sehoon Kim", "authors": "Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer", "title": "I-BERT: Integer-only BERT Quantization", "comments": null, "journal-ref": "ICML 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer based models, like BERT and RoBERTa, have achieved\nstate-of-the-art results in many Natural Language Processing tasks. However,\ntheir memory footprint, inference latency, and power consumption are\nprohibitive efficient inference at the edge, and even at the data center. While\nquantization can be a viable solution for this, previous work on quantizing\nTransformer based models use floating-point arithmetic during inference, which\ncannot efficiently utilize integer-only logical units such as the recent Turing\nTensor Cores, or traditional integer-only ARM processors. In this work, we\npropose I-BERT, a novel quantization scheme for Transformer based models that\nquantizes the entire inference with integer-only arithmetic. Based on\nlightweight integer-only approximation methods for nonlinear operations, e.g.,\nGELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end\ninteger-only BERT inference without any floating point calculation. We evaluate\nour approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that\nfor both cases, I-BERT achieves similar (and slightly higher) accuracy as\ncompared to the full-precision baseline. Furthermore, our preliminary\nimplementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4\nGPU system as compared to FP32 inference. The framework has been developed in\nPyTorch and has been open-sourced.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 02:42:58 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 09:11:11 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 07:53:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kim", "Sehoon", ""], ["Gholami", "Amir", ""], ["Yao", "Zhewei", ""], ["Mahoney", "Michael W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2101.01334", "submitter": "Sonu Aggarwal", "authors": "Akshay Agarwal, Shashank Maiya, Sonu Aggarwal", "title": "Evaluating Empathetic Chatbots in Customer Service Settings", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Customer service is a setting that calls for empathy in live human agent\nresponses. Recent advances have demonstrated how open-domain chatbots can be\ntrained to demonstrate empathy when responding to live human utterances. We\nshow that a blended skills chatbot model that responds to customer queries is\nmore likely to resemble actual human agent response if it is trained to\nrecognize emotion and exhibit appropriate empathy, than a model without such\ntraining. For our analysis, we leverage a Twitter customer service dataset\ncontaining several million customer<->agent dialog examples in customer service\ncontexts from 20 well-known brands.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:34:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Agarwal", "Akshay", ""], ["Maiya", "Shashank", ""], ["Aggarwal", "Sonu", ""]]}, {"id": "2101.01337", "submitter": "Mohammed Alawad", "authors": "Mohammed Alawad, Shang Gao, Mayanka Chandra Shekar, S.M.Shamimul\n  Hasan, J. Blair Christian, Xiao-Cheng Wu, Eric B. Durbin, Jennifer Doherty,\n  Antoinette Stroup, Linda Coyle, Lynne Penberthy, Georgia Tourassi", "title": "Integration of Domain Knowledge using Medical Knowledge Graph Deep\n  Learning for Cancer Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key component of deep learning (DL) for natural language processing (NLP)\nis word embeddings. Word embeddings that effectively capture the meaning and\ncontext of the word that they represent can significantly improve the\nperformance of downstream DL models for various NLP tasks. Many existing word\nembeddings techniques capture the context of words based on word co-occurrence\nin documents and text; however, they often cannot capture broader\ndomain-specific relationships between concepts that may be crucial for the NLP\ntask at hand. In this paper, we propose a method to integrate external\nknowledge from medical terminology ontologies into the context captured by word\nembeddings. Specifically, we use a medical knowledge graph, such as the unified\nmedical language system (UMLS), to find connections between clinical terms in\ncancer pathology reports. This approach aims to minimize the distance between\nconnected clinical concepts. We evaluate the proposed approach using a\nMultitask Convolutional Neural Network (MT-CNN) to extract six cancer\ncharacteristics -- site, subsite, laterality, behavior, histology, and grade --\nfrom a dataset of ~900K cancer pathology reports. The results show that the\nMT-CNN model which uses our domain informed embeddings outperforms the same\nMT-CNN using standard word2vec embeddings across all tasks, with an improvement\nin the overall micro- and macro-F1 scores by 4.97\\%and 22.5\\%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:59:43 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Alawad", "Mohammed", ""], ["Gao", "Shang", ""], ["Shekar", "Mayanka Chandra", ""], ["Hasan", "S. M. Shamimul", ""], ["Christian", "J. Blair", ""], ["Wu", "Xiao-Cheng", ""], ["Durbin", "Eric B.", ""], ["Doherty", "Jennifer", ""], ["Stroup", "Antoinette", ""], ["Coyle", "Linda", ""], ["Penberthy", "Lynne", ""], ["Tourassi", "Georgia", ""]]}, {"id": "2101.01353", "submitter": "Weixin Zeng", "authors": "Weixin Zeng, Xiang Zhao, Jiuyang Tang, Xuemin Lin and Paul Groth", "title": "Reinforcement Learning based Collective Entity Alignment with Adaptive\n  Features", "comments": "Accepted by ACM TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) is the task of identifying the entities that refer to\nthe same real-world object but are located in different knowledge graphs (KGs).\nFor entities to be aligned, existing EA solutions treat them separately and\ngenerate alignment results as ranked lists of entities on the other side.\nNevertheless, this decision-making paradigm fails to take into account the\ninterdependence among entities. Although some recent efforts mitigate this\nissue by imposing the 1-to-1 constraint on the alignment process, they still\ncannot adequately model the underlying interdependence and the results tend to\nbe sub-optimal. To fill in this gap, in this work, we delve into the dynamics\nof the decision-making process, and offer a reinforcement learning (RL) based\nmodel to align entities collectively. Under the RL framework, we devise the\ncoherence and exclusiveness constraints to characterize the interdependence and\nrestrict collective alignment. Additionally, to generate more precise inputs to\nthe RL framework, we employ representative features to capture different\naspects of the similarity between entities in heterogeneous KGs, which are\nintegrated by an adaptive feature fusion strategy. Our proposal is evaluated on\nboth cross-lingual and mono-lingual EA benchmarks and compared against\nstate-of-the-art solutions. The empirical results verify its effectiveness and\nsuperiority.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 05:04:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zeng", "Weixin", ""], ["Zhao", "Xiang", ""], ["Tang", "Jiuyang", ""], ["Lin", "Xuemin", ""], ["Groth", "Paul", ""]]}, {"id": "2101.01391", "submitter": "Soroush Vosoughi Dr", "authors": "Ruibo Liu, Lili Wang, Chenyan Jia, Soroush Vosoughi", "title": "Political Depolarization of News Articles Using Attribute-aware Word\n  Embeddings", "comments": "In Proceedings of the 15th International AAAI Conference on Weblogs\n  and Social Media (ICWSM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Political polarization in the US is on the rise. This polarization negatively\naffects the public sphere by contributing to the creation of ideological echo\nchambers. In this paper, we focus on addressing one of the factors that\ncontributes to this polarity, polarized media. We introduce a framework for\ndepolarizing news articles. Given an article on a certain topic with a\nparticular ideological slant (eg., liberal or conservative), the framework\nfirst detects polar language in the article and then generates a new article\nwith the polar language replaced with neutral expressions. To detect polar\nwords, we train a multi-attribute-aware word embedding model that is aware of\nideology and topics on 360k full-length media articles. Then, for text\ngeneration, we propose a new algorithm called Text Annealing Depolarization\nAlgorithm (TADA). TADA retrieves neutral expressions from the word embedding\nmodel that not only decrease ideological polarity but also preserve the\noriginal argument of the text, while maintaining grammatical correctness. We\nevaluate our framework by comparing the depolarized output of our model in two\nmodes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics.\nBased on feedback from 161 human testers, our framework successfully\ndepolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs\nin fully-automatic mode. Furthermore, 81.2% of the testers agree that the\nnon-polar content information is well-preserved and 79% agree that\ndepolarization does not harm semantic correctness when they compare the\noriginal text and the depolarized text. Our work shows that data-driven methods\ncan help to locate political polarity and aid in the depolarization of\narticles.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:39:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 04:42:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Ruibo", ""], ["Wang", "Lili", ""], ["Jia", "Chenyan", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2101.01447", "submitter": "Hung-Ting Su", "authors": "Hung-Ting Su, Chen-Hsi Chang, Po-Wei Shen, Yu-Siang Wang, Ya-Liang\n  Chang, Yu-Cheng Chang, Pu-Jen Cheng and Winston H. Hsu", "title": "End-to-End Video Question-Answer Generation with Generator-Pretester\n  Network", "comments": "Accepted to TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel task, Video Question-Answer Generation (VQAG), for\nchallenging Video Question Answering (Video QA) task in multimedia. Due to\nexpensive data annotation costs, many widely used, large-scale Video QA\ndatasets such as Video-QA, MSVD-QA and MSRVTT-QA are automatically annotated\nusing Caption Question Generation (CapQG) which inputs captions instead of the\nvideo itself. As captions neither fully represent a video, nor are they always\npractically available, it is crucial to generate question-answer pairs based on\na video via Video Question-Answer Generation (VQAG). Existing video-to-text\n(V2T) approaches, despite taking a video as the input, only generate a question\nalone. In this work, we propose a novel model Generator-Pretester Network that\nfocuses on two components: (1) The Joint Question-Answer Generator (JQAG) which\ngenerates a question with its corresponding answer to allow Video Question\n\"Answering\" training. (2) The Pretester (PT) verifies a generated question by\ntrying to answer it and checks the pretested answer with both the model's\nproposed answer and the ground truth answer. We evaluate our system with the\nonly two available large-scale human-annotated Video QA datasets and achieves\nstate-of-the-art question generation performances. Furthermore, using our\ngenerated QA pairs only on the Video QA task, we can surpass some supervised\nbaselines. We apply our generated questions to Video QA applications and\nsurpasses some supervised baselines using generated questions only. As a\npre-training strategy, we outperform both CapQG and transfer learning\napproaches when employing semi-supervised (20%) or fully supervised learning\nwith annotated data. These experimental results suggest the novel perspectives\nfor Video QA training.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:46:06 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Su", "Hung-Ting", ""], ["Chang", "Chen-Hsi", ""], ["Shen", "Po-Wei", ""], ["Wang", "Yu-Siang", ""], ["Chang", "Ya-Liang", ""], ["Chang", "Yu-Cheng", ""], ["Cheng", "Pu-Jen", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2101.01476", "submitter": "Dat Quoc Nguyen", "authors": "Linh The Nguyen, Dat Quoc Nguyen", "title": "PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech\n  tagging, named entity recognition and dependency parsing", "comments": "To appear in Proceedings of NAACL 2021: Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first multi-task learning model -- named PhoNLP -- for joint\nVietnamese part-of-speech (POS) tagging, named entity recognition (NER) and\ndependency parsing. Experiments on Vietnamese benchmark datasets show that\nPhoNLP produces state-of-the-art results, outperforming a single-task learning\napproach that fine-tunes the pre-trained Vietnamese language model PhoBERT\n(Nguyen and Nguyen, 2020) for each task independently. We publicly release\nPhoNLP as an open-source toolkit under the Apache License 2.0. Although we\nspecify PhoNLP for Vietnamese, our PhoNLP training and evaluation command\nscripts in fact can directly work for other languages that have a pre-trained\nBERT-based language model and gold annotated corpora available for the three\ntasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve\nas a strong baseline and useful toolkit for future NLP research and\napplications to not only Vietnamese but also the other languages. Our PhoNLP is\navailable at: https://github.com/VinAIResearch/PhoNLP\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 12:13:09 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:31:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Nguyen", "Linh The", ""], ["Nguyen", "Dat Quoc", ""]]}, {"id": "2101.01628", "submitter": "David Noever", "authors": "David Noever, Josh Kalin, Matt Ciolino, Dom Hambrick, and Gerry Dozier", "title": "Local Translation Services for Neglected Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking advantage of computationally lightweight, but high-quality translators\nprompt consideration of new applications that address neglected languages.\nLocally run translators for less popular languages may assist data projects\nwith protected or personal data that may require specific compliance checks\nbefore posting to a public translation API, but which could render reasonable,\ncost-effective solutions if done with an army of local, small-scale pair\ntranslators. Like handling a specialist's dialect, this research illustrates\ntranslating two historically interesting, but obfuscated languages: 1)\nhacker-speak (\"l33t\") and 2) reverse (or \"mirror\") writing as practiced by\nLeonardo da Vinci. The work generalizes a deep learning architecture to\ntranslatable variants of hacker-speak with lite, medium, and hard vocabularies.\nThe original contribution highlights a fluent translator of hacker-speak in\nunder 50 megabytes and demonstrates a generator for augmenting future datasets\nwith greater than a million bilingual sentence pairs. The long short-term\nmemory, recurrent neural network (LSTM-RNN) extends previous work demonstrating\nan English-to-foreign translation service built from as little as 10,000\nbilingual sentence pairs. This work further solves the equivalent translation\nproblem in twenty-six additional (non-obfuscated) languages and rank orders\nthose models and their proficiency quantitatively with Italian as the most\nsuccessful and Mandarin Chinese as the most challenging. For neglected\nlanguages, the method prototypes novel services for smaller niche translations\nsuch as Kabyle (Algerian dialect) which covers between 5-7 million speakers but\none which for most enterprise translators, has not yet reached development. One\nanticipates the extension of this approach to other important dialects, such as\ntranslating technical (medical or legal) jargon and processing health records.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:25:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 20:09:07 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Noever", "David", ""], ["Kalin", "Josh", ""], ["Ciolino", "Matt", ""], ["Hambrick", "Dom", ""], ["Dozier", "Gerry", ""]]}, {"id": "2101.01634", "submitter": "Lorenzo De Mattei", "authors": "Lorenzo De Mattei, Michele Cafagna, Huiyuan Lai, Felice Dell'Orletta,\n  Malvina Nissim, Albert Gatt", "title": "On the interaction of automatic evaluation and task framing in headline\n  style transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An ongoing debate in the NLG community concerns the best way to evaluate\nsystems, with human evaluation often being considered the most reliable method,\ncompared to corpus-based metrics. However, tasks involving subtle textual\ndifferences, such as style transfer, tend to be hard for humans to perform. In\nthis paper, we propose an evaluation method for this task based on\npurposely-trained classifiers, showing that it better reflects system\ndifferences than traditional metrics such as BLEU and ROUGE.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:36:26 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["De Mattei", "Lorenzo", ""], ["Cafagna", "Michele", ""], ["Lai", "Huiyuan", ""], ["Dell'Orletta", "Felice", ""], ["Nissim", "Malvina", ""], ["Gatt", "Albert", ""]]}, {"id": "2101.01686", "submitter": "Binyuan Hui", "authors": "Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun,\n  Fei Huang, Luo Si, Pengfei Zhu, Xiaodan Zhu", "title": "Dynamic Hybrid Relation Network for Cross-Domain Context-Dependent\n  Semantic Parsing", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing has long been a fundamental problem in natural language\nprocessing. Recently, cross-domain context-dependent semantic parsing has\nbecome a new focus of research. Central to the problem is the challenge of\nleveraging contextual information of both natural language utterance and\ndatabase schemas in the interaction history. In this paper, we present a\ndynamic graph framework that is capable of effectively modelling contextual\nutterances, tokens, database schemas, and their complicated interaction as the\nconversation proceeds. The framework employs a dynamic memory decay mechanism\nthat incorporates inductive bias to integrate enriched contextual relation\nrepresentation, which is further enhanced with a powerful reranking model. At\nthe time of writing, we demonstrate that the proposed framework outperforms all\nexisting models by large margins, achieving new state-of-the-art performance on\ntwo large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the\nmodel attains a 55.8% question-match and 30.8% interaction-match accuracy on\nSParC, and a 46.8% question-match and 17.0% interaction-match accuracy on\nCoSQL.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:11:29 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Hui", "Binyuan", ""], ["Geng", "Ruiying", ""], ["Ren", "Qiyu", ""], ["Li", "Binhua", ""], ["Li", "Yongbin", ""], ["Sun", "Jian", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""], ["Zhu", "Pengfei", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2101.01761", "submitter": "Hieu Pham", "authors": "Hieu Pham, Quoc V. Le", "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are often over-parameterized and hence benefit from\naggressive regularization. Conventional regularization methods, such as Dropout\nor weight decay, do not leverage the structures of the network's inputs and\nhidden states. As a result, these conventional methods are less effective than\nmethods that leverage the structures, such as SpatialDropout and DropBlock,\nwhich randomly drop the values at certain contiguous areas in the hidden states\nand setting them to zero. Although the locations of dropout areas random, the\npatterns of SpatialDropout and DropBlock are manually designed and fixed. Here\nwe propose to learn the dropout patterns. In our method, a controller learns to\ngenerate a dropout pattern at every channel and layer of a target network, such\nas a ConvNet or a Transformer. The target network is then trained with the\ndropout pattern, and its resulting validation performance is used as a signal\nfor the controller to learn from. We show that this method works well for both\nimage recognition on CIFAR-10 and ImageNet, as well as language modeling on\nPenn Treebank and WikiText-2. The learned dropout patterns also transfers to\ndifferent tasks and datasets, such as from language model on Penn Treebank to\nEngligh-French translation on WMT 2014. Our code will be available.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:54:22 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Pham", "Hieu", ""], ["Le", "Quoc V.", ""]]}, {"id": "2101.01775", "submitter": "Yu Chen", "authors": "Yu Chen, Ananya Subburathinam, Ching-Hua Chen and Mohammed J. Zaki", "title": "Personalized Food Recommendation as Constrained Question Answering over\n  a Large-scale Food Knowledge Graph", "comments": "9 pages. Accepted by WSDM 2021. Final version", "journal-ref": null, "doi": "10.1145/3437963.3441816", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food recommendation has become an important means to help guide users to\nadopt healthy dietary habits. Previous works on food recommendation either i)\nfail to consider users' explicit requirements, ii) ignore crucial health\nfactors (e.g., allergies and nutrition needs), or iii) do not utilize the rich\nfood knowledge for recommending healthy recipes. To address these limitations,\nwe propose a novel problem formulation for food recommendation, modeling this\ntask as constrained question answering over a large-scale food knowledge\nbase/graph (KBQA). Besides the requirements from the user query, personalized\nrequirements from the user's dietary preferences and health guidelines are\nhandled in a unified way as additional constraints to the QA system. To\nvalidate this idea, we create a QA style dataset for personalized food\nrecommendation based on a large-scale food knowledge graph and health\nguidelines. Furthermore, we propose a KBQA-based personalized food\nrecommendation framework which is equipped with novel techniques for handling\nnegations and numerical comparisons in the queries. Experimental results on the\nbenchmark show that our approach significantly outperforms non-personalized\ncounterparts (average 59.7% absolute improvement across various evaluation\nmetrics), and is able to recommend more relevant and healthier recipes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:38:16 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Chen", "Yu", ""], ["Subburathinam", "Ananya", ""], ["Chen", "Ching-Hua", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "2101.01785", "submitter": "El Moatez Billah Nagoudi", "authors": "Muhammad Abdul-Mageed, AbdelRahim Elmadany, El Moatez Billah Nagoudi", "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic", "comments": "All authors contributed equally. The order is alphabetical", "journal-ref": "ACL-2021 camera ready version", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models (LMs) are currently integral to many natural\nlanguage processing systems. Although multilingual LMs were also introduced to\nserve many languages, these have limitations such as being costly at inference\ntime and the size and diversity of non-English data involved in their\npre-training. We remedy these issues for a collection of diverse Arabic\nvarieties by introducing two powerful deep bidirectional transformer-based\nmodels, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a\nnew benchmark for multi-dialectal Arabic language understanding evaluation.\nARLUE is built using 42 datasets targeting six different task clusters,\nallowing us to offer a series of standardized experiments under rich\nconditions. When fine-tuned on ARLUE, our models collectively achieve new\nstate-of-the-art results across the majority of tasks (37 out of 48\nclassification tasks, on the 42 datasets). Our best model acquires the highest\nARLUE score (77.40) across all six task clusters, outperforming all other\nmodels including XLM-R Large (~ 3.4 x larger size). Our models are publicly\navailable at https://github.com/UBC-NLP/marbert and ARLUE will be released\nthrough the same repository.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 06:32:55 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:44:27 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 20:39:46 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Elmadany", "AbdelRahim", ""], ["Nagoudi", "El Moatez Billah", ""]]}, {"id": "2101.01853", "submitter": "Xuankai Chang", "authors": "Xuankai Chang, Naoyuki Kanda, Yashesh Gaur, Xiaofei Wang, Zhong Meng,\n  Takuya Yoshioka", "title": "Hypothesis Stitcher for End-to-End Speaker-attributed ASR on Long-form\n  Multi-talker Recordings", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An end-to-end (E2E) speaker-attributed automatic speech recognition (SA-ASR)\nmodel was proposed recently to jointly perform speaker counting, speech\nrecognition and speaker identification. The model achieved a low\nspeaker-attributed word error rate (SA-WER) for monaural overlapped speech\ncomprising an unknown number of speakers. However, the E2E modeling approach is\nsusceptible to the mismatch between the training and testing conditions. It has\nyet to be investigated whether the E2E SA-ASR model works well for recordings\nthat are much longer than samples seen during training. In this work, we first\napply a known decoding technique that was developed to perform single-speaker\nASR for long-form audio to our E2E SA-ASR task. Then, we propose a novel method\nusing a sequence-to-sequence model, called hypothesis stitcher. The model takes\nmultiple hypotheses obtained from short audio segments that are extracted from\nthe original long-form input, and it then outputs a fused single hypothesis. We\npropose several architectural variations of the hypothesis stitcher model and\ncompare them with the conventional decoding methods. Experiments using\nLibriSpeech and LibriCSS corpora show that the proposed method significantly\nimproves SA-WER especially for long-form multi-talker recordings.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 03:36:09 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Chang", "Xuankai", ""], ["Kanda", "Naoyuki", ""], ["Gaur", "Yashesh", ""], ["Wang", "Xiaofei", ""], ["Meng", "Zhong", ""], ["Yoshioka", "Takuya", ""]]}, {"id": "2101.01880", "submitter": "Sugam Garg", "authors": "Sugam Garg, Harichandana and Sumit Kumar", "title": "On-Device Document Classification using multimodal features", "comments": "8th ACM IKDD CODS and 26th COMAD 2-4 January 2021", "journal-ref": null, "doi": "10.1145/3430984.3431030", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  From small screenshots to large videos, documents take up a bulk of space in\na modern smartphone. Documents in a phone can accumulate from various sources,\nand with the high storage capacity of mobiles, hundreds of documents are\naccumulated in a short period. However, searching or managing documents remains\nan onerous task, since most search methods depend on meta-information or only\ntext in a document. In this paper, we showcase that a single modality is\ninsufficient for classification and present a novel pipeline to classify\ndocuments on-device, thus preventing any private user data transfer to server.\nFor this task, we integrate an open-source library for Optical Character\nRecognition (OCR) and our novel model architecture in the pipeline. We optimise\nthe model for size, a necessary metric for on-device inference. We benchmark\nour classification model with a standard multimodal dataset FOOD-101 and\nshowcase competitive results with the previous State of the Art with 30% model\ncompression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:36:58 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Garg", "Sugam", ""], ["Harichandana", "", ""], ["Kumar", "Sumit", ""]]}, {"id": "2101.01896", "submitter": "Xiangchen Song", "authors": "Jieyu Zhang, Xiangchen Song, Ying Zeng, Jiaze Chen, Jiaming Shen,\n  Yuning Mao, Lei Li", "title": "Taxonomy Completion via Triplet Matching Network", "comments": "AAA1 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically constructing taxonomy finds many applications in e-commerce and\nweb search. One critical challenge is as data and business scope grow in real\napplications, new concepts are emerging and needed to be added to the existing\ntaxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an\nappropriate hypernym concept from the taxonomy for a new query concept. In this\npaper, we formulate a new task, \"taxonomy completion\", by discovering both the\nhypernym and hyponym concepts for a query. We propose Triplet Matching Network\n(TMN), to find the appropriate <hypernym, hyponym> pairs for a given query\nconcept. TMN consists of one primal scorer and multiple auxiliary scorers.\nThese auxiliary scorers capture various fine-grained signals (e.g., query to\nhypernym or query to hyponym semantics), and the primal scorer makes a holistic\nprediction on <query, hypernym, hyponym> triplet based on the internal feature\nrepresentations of all auxiliary scorers. Also, an innovative channel-wise\ngating mechanism that retains task-specific information in concept\nrepresentations is introduced to further boost model performance. Experiments\non four real-world large-scale datasets show that TMN achieves the best\nperformance on both taxonomy completion task and the previous taxonomy\nexpansion task, outperforming existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:19:55 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 01:36:22 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 09:51:39 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Jieyu", ""], ["Song", "Xiangchen", ""], ["Zeng", "Ying", ""], ["Chen", "Jiaze", ""], ["Shen", "Jiaming", ""], ["Mao", "Yuning", ""], ["Li", "Lei", ""]]}, {"id": "2101.01907", "submitter": "Hailin Wang", "authors": "Hailin Wang, Ke Qin, Rufai Yusuf Zakari, Guoming Lu, Jin Yin", "title": "Deep Neural Network Based Relation Extraction: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Knowledge is a formal way of understanding the world, providing a human-level\ncognition and intelligence for the next-generation artificial intelligence\n(AI). One of the representations of knowledge is semantic relations between\nentities. An effective way to automatically acquire this important knowledge,\ncalled Relation Extraction (RE), a sub-task of information extraction, plays a\nvital role in Natural Language Processing (NLP). Its purpose is to identify\nsemantic relations between entities from natural language text. To date, there\nare several studies for RE in previous works, which have documented these\ntechniques based on Deep Neural Networks (DNNs) become a prevailing technique\nin this research. Especially, the supervised and distant supervision methods\nbased on DNNs are the most popular and reliable solutions for RE. This article\n1) introduces some general concepts, and further 2) gives a comprehensive\noverview of DNNs in RE from two points of view: supervised RE, which attempts\nto improve the standard RE systems, and distant supervision RE, which adopts\nDNNs to design sentence encoder and de-noise method. We further 3) cover some\nnovel methods and recent trends as well as discuss possible future research\ndirections for this task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:53:05 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 19:13:54 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wang", "Hailin", ""], ["Qin", "Ke", ""], ["Zakari", "Rufai Yusuf", ""], ["Lu", "Guoming", ""], ["Yin", "Jin", ""]]}, {"id": "2101.01910", "submitter": "Xiaopeng Lu", "authors": "Xiaopeng Lu, Kyusong Lee, Tiancheng Zhao", "title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question\n  Answering", "comments": "Accepted to EACL2021 demo track (7 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although open-domain question answering (QA) draws great attention in recent\nyears, it requires large amounts of resources for building the full system and\nis often difficult to reproduce previous results due to complex configurations.\nIn this paper, we introduce SF-QA: simple and fair evaluation framework for\nopen-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,\nwhich makes the task itself easily accessible and reproducible to research\ngroups without enough computing resources. The proposed evaluation framework is\npublicly available and anyone can contribute to the code and evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:02:41 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:02:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lu", "Xiaopeng", ""], ["Lee", "Kyusong", ""], ["Zhao", "Tiancheng", ""]]}, {"id": "2101.01926", "submitter": "Xuekai Li", "authors": "Tongtong Wu, Xuekai Li, Yuan-Fang Li, Reza Haffari, Guilin Qi, Yujin\n  Zhu and Guoqiang Xu", "title": "Curriculum-Meta Learning for Order-Robust Continual Relation Extraction", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual relation extraction is an important task that focuses on extracting\nnew facts incrementally from unstructured text. Given the sequential arrival\norder of the relations, this task is prone to two serious challenges, namely\ncatastrophic forgetting and order-sensitivity. We propose a novel\ncurriculum-meta learning method to tackle the above two challenges in continual\nrelation extraction. We combine meta learning and curriculum learning to\nquickly adapt model parameters to a new task and to reduce interference of\npreviously seen tasks on the current task. We design a novel relation\nrepresentation learning method through the distribution of domain and range\ntypes of relations. Such representations are utilized to quantify the\ndifficulty of tasks for the construction of curricula. Moreover, we also\npresent novel difficulty-based metrics to quantitatively measure the extent of\norder-sensitivity of a given model, suggesting new ways to evaluate model\nrobustness. Our comprehensive experiments on three benchmark datasets show that\nour proposed method outperforms the state-of-the-art techniques. The code is\navailable at the anonymous GitHub repository:\nhttps://github.com/wutong8023/AAAI_CML.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:52:34 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:25:10 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 10:06:40 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wu", "Tongtong", ""], ["Li", "Xuekai", ""], ["Li", "Yuan-Fang", ""], ["Haffari", "Reza", ""], ["Qi", "Guilin", ""], ["Zhu", "Yujin", ""], ["Xu", "Guoqiang", ""]]}, {"id": "2101.02157", "submitter": "Sofian Chaybouti", "authors": "Sofian Chaybouti, Achraf Saghe, Aymen Shabou", "title": "EfficientQA : a RoBERTa Based Phrase-Indexed Question-Answering System", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art extractive question answering models achieve superhuman\nperformances on the SQuAD benchmark. Yet, they are unreasonably heavy and need\nexpensive GPU computing to answer questions in a reasonable time. Thus, they\ncannot be used for real-world queries on hundreds of thousands of documents in\nthe open-domain question answering paradigm. In this paper, we explore the\npossibility to transfer the natural language understanding of language models\ninto dense vectors representing questions and answer candidates, in order to\nmake the task of question-answering compatible with a simple nearest neighbor\nsearch task. This new model, that we call EfficientQA, takes advantage from the\npair of sequences kind of input of BERT-based models to build meaningful dense\nrepresentations of candidate answers. These latter are extracted from the\ncontext in a question-agnostic fashion. Our model achieves state-of-the-art\nresults in Phrase-Indexed Question Answering (PIQA) beating the previous\nstate-of-art by 1.3 points in exact-match and 1.4 points in f1-score. These\nresults show that dense vectors are able to embed very rich semantic\nrepresentations of sequences, although these ones were built from language\nmodels not originally trained for the use-case. Thus, in order to build more\nresource efficient NLP systems in the future, training language models that are\nbetter adapted to build dense representations of phrases is one of the\npossibilities.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:46:05 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:40:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chaybouti", "Sofian", ""], ["Saghe", "Achraf", ""], ["Shabou", "Aymen", ""]]}, {"id": "2101.02158", "submitter": "Kenneth Clarkson", "authors": "Kenneth L. Clarkson and Sanjana Sahayaraj", "title": "Order Embeddings from Merged Ontologies using Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, low resource method to produce order embeddings from\nontologies. Such embeddings map words to vectors so that order relations on the\nwords, such as hypernymy/hyponymy, are represented in a direct way. Our method\nuses sketching techniques, in particular countsketch, for dimensionality\nreduction. We also study methods to merge ontologies, in particular those in\nmedical domains, so that order relations are preserved. We give computational\nresults for medical ontologies and for wordnet, showing that our merging\ntechniques are effective and our embedding yields an accurate representation in\nboth generic and specialised domains.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:50:55 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Sahayaraj", "Sanjana", ""]]}, {"id": "2101.02235", "submitter": "Mor Geva", "authors": "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan\n  Berant", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit\n  Reasoning Strategies", "comments": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2021. Author's final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A key limitation in current datasets for multi-hop reasoning is that the\nrequired steps for answering the question are mentioned in it explicitly. In\nthis work, we introduce StrategyQA, a question answering (QA) benchmark where\nthe required reasoning steps are implicit in the question, and should be\ninferred using a strategy. A fundamental challenge in this setup is how to\nelicit such creative questions from crowdsourcing workers, while covering a\nbroad range of potential strategies. We propose a data collection procedure\nthat combines term-based priming to inspire annotators, careful control over\nthe annotator population, and adversarial filtering for eliminating reasoning\nshortcuts. Moreover, we annotate each question with (1) a decomposition into\nreasoning steps for answering it, and (2) Wikipedia paragraphs that contain the\nanswers to each step. Overall, StrategyQA includes 2,780 examples, each\nconsisting of a strategy question, its decomposition, and evidence paragraphs.\nAnalysis shows that questions in StrategyQA are short, topic-diverse, and cover\na wide range of strategies. Empirically, we show that humans perform well (87%)\non this task, while our best baseline reaches an accuracy of $\\sim$66%.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:14:23 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Geva", "Mor", ""], ["Khashabi", "Daniel", ""], ["Segal", "Elad", ""], ["Khot", "Tushar", ""], ["Roth", "Dan", ""], ["Berant", "Jonathan", ""]]}, {"id": "2101.02244", "submitter": "Anamaria Crisan", "authors": "Anamaria Crisan, Michael Correll", "title": "User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text\n  Analytics", "comments": "16 Pages, 9 Figures, CHI 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic models are widely used analysis techniques for clustering documents and\nsurfacing thematic elements of text corpora. These models remain challenging to\noptimize and often require a \"human-in-the-loop\" approach where domain experts\nuse their knowledge to steer and adjust. However, the fragility,\nincompleteness, and opacity of these models means even minor changes could\ninduce large and potentially undesirable changes in resulting model. In this\npaper we conduct a simulation-based analysis of human-centered interactions\nwith topic models, with the objective of measuring the sensitivity of topic\nmodels to common classes of user actions. We find that user interactions have\nimpacts that differ in magnitude but often negatively affect the quality of the\nresulting modelling in a way that can be difficult for the user to evaluate. We\nsuggest the incorporation of sensitivity and \"multiverse\" analyses to topic\nmodel interfaces to surface and overcome these deficiencies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:44:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Crisan", "Anamaria", ""], ["Correll", "Michael", ""]]}, {"id": "2101.02258", "submitter": "Yair Lakretz", "authors": "Yair Lakretz, Th\\'eo Desbordes, Jean-R\\'emi King, Beno\\^it Crabb\\'e,\n  Maxime Oquab, Stanislas Dehaene", "title": "Can RNNs learn Recursive Nested Subject-Verb Agreements?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental principles of contemporary linguistics states that\nlanguage processing requires the ability to extract recursively nested tree\nstructures. However, it remains unclear whether and how this code could be\nimplemented in neural circuits. Recent advances in Recurrent Neural Networks\n(RNNs), which achieve near-human performance in some language tasks, provide a\ncompelling model to address such questions. Here, we present a new framework to\nstudy recursive processing in RNNs, using subject-verb agreement as a probe\ninto the representations of the neural network. We trained six distinct types\nof RNNs on a simplified probabilistic context-free grammar designed to\nindependently manipulate the length of a sentence and the depth of its\nsyntactic tree. All RNNs generalized to subject-verb dependencies longer than\nthose seen during training. However, none systematically generalized to deeper\ntree structures, even those with a structural bias towards learning nested tree\n(i.e., stack-RNNs). In addition, our analyses revealed primacy and recency\neffects in the generalization patterns of LSTM-based models, showing that these\nmodels tend to perform well on the outer- and innermost parts of a\ncenter-embedded tree structure, but poorly on its middle levels. Finally,\nprobing the internal states of the model during the processing of sentences\nwith nested tree structures, we found a complex encoding of grammatical\nagreement information (e.g. grammatical number), in which all the information\nfor multiple words nouns was carried by a single unit. Taken together, these\nresults indicate how neural networks may extract bounded nested tree\nstructures, without learning a systematic recursive rule.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 20:47:02 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Lakretz", "Yair", ""], ["Desbordes", "Th\u00e9o", ""], ["King", "Jean-R\u00e9mi", ""], ["Crabb\u00e9", "Beno\u00eet", ""], ["Oquab", "Maxime", ""], ["Dehaene", "Stanislas", ""]]}, {"id": "2101.02346", "submitter": "Amirmohammad Kazameini", "authors": "Yang Li, Amirmohammad Kazameini, Yash Mehta, Erik Cambria", "title": "Multitask Learning for Emotion and Personality Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent years, deep learning-based automated personality trait detection\nhas received a lot of attention, especially now, due to the massive digital\nfootprints of an individual. Moreover, many researchers have demonstrated that\nthere is a strong link between personality traits and emotions. In this paper,\nwe build on the known correlation between personality traits and emotional\nbehaviors, and propose a novel multitask learning framework, SoGMTL that\nsimultaneously predicts both of them. We also empirically evaluate and discuss\ndifferent information-sharing mechanisms between the two tasks. To ensure the\nhigh quality of the learning process, we adopt a MAML-like framework for model\noptimization. Our more computationally efficient CNN-based multitask model\nachieves the state-of-the-art performance across multiple famous personality\nand emotion datasets, even outperforming Language Model based models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:09:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Li", "Yang", ""], ["Kazameini", "Amirmohammad", ""], ["Mehta", "Yash", ""], ["Cambria", "Erik", ""]]}, {"id": "2101.02351", "submitter": "Sohom Ghosh", "authors": "Ankush Chopra, Shruti Agrawal and Sohom Ghosh", "title": "Applying Transfer Learning for Improving Domain-Specific Search\n  Experience Using Query to Question Similarity", "comments": "8 pages, accepted in the Proceedings of the 3rd International\n  Conference on Algorithms, Computing and Artificial Intelligence (ACAI), 2020", "journal-ref": null, "doi": "10.1145/3446132.3446403", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search is one of the most common platforms used to seek information. However,\nusers mostly get overloaded with results whenever they use such a platform to\nresolve their queries. Nowadays, direct answers to queries are being provided\nas a part of the search experience. The question-answer (QA) retrieval process\nplays a significant role in enriching the search experience. Most off-the-shelf\nSemantic Textual Similarity models work fine for well-formed search queries,\nbut their performances degrade when applied to a domain-specific setting having\nincomplete or grammatically ill-formed search queries in prevalence. In this\npaper, we discuss a framework for calculating similarities between a given\ninput query and a set of predefined questions to retrieve the question which\nmatches to it the most. We have used it for the financial domain, but the\nframework is generalized for any domain-specific search engine and can be used\nin other domains as well. We use Siamese network [6] over Long Short-Term\nMemory (LSTM) [3] models to train a classifier which generates unnormalized and\nnormalized similarity scores for a given pair of questions. Moreover, for each\nof these question pairs, we calculate three other similarity scores: cosine\nsimilarity between their average word2vec embeddings [15], cosine similarity\nbetween their sentence embeddings [7] generated using RoBERTa [17] and their\ncustomized fuzzy-match score. Finally, we develop a metaclassifier using\nSupport Vector Machines [19] for combining these five scores to detect if a\ngiven pair of questions is similar. We benchmark our model's performance\nagainst existing State Of The Art (SOTA) models on Quora Question Pairs (QQP)\ndataset as well as a dataset specific to the financial domain.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:27:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Chopra", "Ankush", ""], ["Agrawal", "Shruti", ""], ["Ghosh", "Sohom", ""]]}, {"id": "2101.02352", "submitter": "JianGang Liu", "authors": "Yao Chen, Jiangang Liu, Zhe Zhang, Shiping Wen, Wenjun Xiong", "title": "M\\\"{o}biusE: Knowledge Graph Embedding on M\\\"{o}bius Ring", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2021.107181", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel Knowledge Graph Embedding (KGE) strategy,\ncalled M\\\"{o}biusE, in which the entities and relations are embedded to the\nsurface of a M\\\"{o}bius ring. The proposition of such a strategy is inspired by\nthe classic TorusE, in which the addition of two arbitrary elements is subject\nto a modulus operation. In this sense, TorusE naturally guarantees the critical\nboundedness of embedding vectors in KGE. However, the nonlinear property of\naddition operation on Torus ring is uniquely derived by the modulus operation,\nwhich in some extent restricts the expressiveness of TorusE. As a further\ngeneralization of TorusE, M\\\"{o}biusE also uses modulus operation to preserve\nthe closeness of addition operation on it, but the coordinates on M\\\"{o}bius\nring interacts with each other in the following way: {\\em \\color{red} any\nvector on the surface of a M\\\"{o}bius ring moves along its parametric trace\nwill goes to the right opposite direction after a cycle}. Hence, M\\\"{o}biusE\nassumes much more nonlinear representativeness than that of TorusE, and in turn\nit generates much more precise embedding results. In our experiments,\nM\\\"{o}biusE outperforms TorusE and other classic embedding strategies in\nseveral key indicators.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:35:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chen", "Yao", ""], ["Liu", "Jiangang", ""], ["Zhang", "Zhe", ""], ["Wen", "Shiping", ""], ["Xiong", "Wenjun", ""]]}, {"id": "2101.02359", "submitter": "Xiangyang Li", "authors": "Xiangyang Li, Yu Xia, Xiang Long, Zheng Li, Sujian Li", "title": "Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News\n  Detection in English", "comments": "3rd solution of 'Constraint@AAAI2021 - COVID19 Fake News Detection in\n  English'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe our system for the AAAI 2021 shared task of\nCOVID-19 Fake News Detection in English, where we achieved the 3rd position\nwith the weighted F1 score of 0.9859 on the test set. Specifically, we proposed\nan ensemble method of different pre-trained language models such as BERT,\nRoberta, Ernie, etc. with various training strategies including\nwarm-up,learning rate schedule and k-fold cross-validation. We also conduct an\nextensive analysis of the samples that are not correctly classified. The code\nis available\nat:https://github.com/archersama/3rd-solution-COVID19-Fake-News-Detection-in-English.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 04:01:13 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Li", "Xiangyang", ""], ["Xia", "Yu", ""], ["Long", "Xiang", ""], ["Li", "Zheng", ""], ["Li", "Sujian", ""]]}, {"id": "2101.02394", "submitter": "Yingjie Gu", "authors": "Yingjie Gu, Xiaoye Qu, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan\n  and Xiaolin Gui", "title": "Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking (EL) for the rapidly growing short text (e.g. search queries\nand news titles) is critical to industrial applications. Most existing\napproaches relying on adequate context for long text EL are not effective for\nthe concise and sparse short text. In this paper, we propose a novel framework\ncalled Multi-turn Multiple-choice Machine reading comprehension (M3}) to solve\nthe short text EL from a new perspective: a query is generated for each\nambiguous mention exploiting its surrounding context, and an option selection\nmodule is employed to identify the golden entity from candidates using the\nquery. In this way, M3 framework sufficiently interacts limited context with\ncandidate entities during the encoding process, as well as implicitly considers\nthe dissimilarities inside the candidate bunch in the selection stage. In\naddition, we design a two-stage verifier incorporated into M3 to address the\ncommonly existed unlinkable problem in short text. To further consider the\ntopical coherence and interdependence among referred entities, M3 leverages a\nmulti-turn fashion to deal with mentions in a sequence manner by retrospecting\nhistorical cues. Evaluation shows that our M3 framework achieves the\nstate-of-the-art performance on five Chinese and English datasets for the\nreal-world short text EL.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:17:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gu", "Yingjie", ""], ["Qu", "Xiaoye", ""], ["Wang", "Zhefeng", ""], ["Huai", "Baoxing", ""], ["Yuan", "Nicholas Jing", ""], ["Gui", "Xiaolin", ""]]}, {"id": "2101.02398", "submitter": "Rohan Saha", "authors": "Rohan Saha", "title": "Homonym Identification using BERT -- Using a Clustering Approach", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.29120.07681", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Homonym identification is important for WSD that require coarse-grained\npartitions of senses. The goal of this project is to determine whether\ncontextual information is sufficient for identifying a homonymous word. To\ncapture the context, BERT embeddings are used as opposed to Word2Vec, which\nconflates senses into one vector. SemCor is leveraged to retrieve the\nembeddings. Various clustering algorithms are applied to the embeddings.\nFinally, the embeddings are visualized in a lower-dimensional space to\nunderstand the feasibility of the clustering process.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:26:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Saha", "Rohan", ""]]}, {"id": "2101.02415", "submitter": "Ying Sheng", "authors": "Yichao Zhou, Ying Sheng, Nguyen Vo, Nick Edmonds, Sandeep Tata", "title": "Simplified DOM Trees for Transferable Attribute Extraction from the Web", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a steady need to precisely extract structured knowledge from\nthe web (i.e. HTML documents). Given a web page, extracting a structured object\nalong with various attributes of interest (e.g. price, publisher, author, and\ngenre for a book) can facilitate a variety of downstream applications such as\nlarge-scale knowledge base construction, e-commerce product search, and\npersonalized recommendation. Considering each web page is rendered from an HTML\nDOM tree, existing approaches formulate the problem as a DOM tree node tagging\ntask. However, they either rely on computationally expensive visual feature\nengineering or are incapable of modeling the relationship among the tree nodes.\nIn this paper, we propose a novel transferable method, Simplified DOM Trees for\nAttribute Extraction (SimpDOM), to tackle the problem by efficiently retrieving\nuseful context for each node by leveraging the tree structure. We study two\nchallenging experimental settings: (i) intra-vertical few-shot extraction, and\n(ii) cross-vertical fewshot extraction with out-of-domain knowledge, to\nevaluate our approach. Extensive experiments on the SWDE public dataset show\nthat SimpDOM outperforms the state-of-the-art (SOTA) method by 1.44% on the F1\nscore. We also find that utilizing knowledge from a different vertical\n(cross-vertical extraction) is surprisingly useful and helps beat the SOTA by a\nfurther 1.37%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:41:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhou", "Yichao", ""], ["Sheng", "Ying", ""], ["Vo", "Nguyen", ""], ["Edmonds", "Nick", ""], ["Tata", "Sandeep", ""]]}, {"id": "2101.02522", "submitter": "Vincent Aranega", "authors": "Ronie Salgado, Marcus Denker (RMOD), St\\'ephane Ducasse (RMOD), Anne\n  Etien (RMOD), Vincent Aranega (RMOD)", "title": "Towards a Smart Data Processing and Storage Model", "comments": null, "journal-ref": "IWST20: International Workshop on Smalltalk Technologies, Sep\n  2020, Novi Sad, Serbia", "doi": null, "report-no": null, "categories": "cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several domains it is crucial to store and manipulate data whose origin\nneeds to be completely traceable to guarantee the consistency, trustworthiness\nand reliability on the data itself typically for ethical and legal reasons. It\nis also important to guarantee that such properties are also carried further\nwhen such data is composed and processed into new data. In this article we\npresent the main requirements and theorethical problems that arise by the\ndesign of a system supporting data with such capabilities. We present an\narchitecture for implementing a system as well as a prototype developed in\nPharo.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:52:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Salgado", "Ronie", "", "RMOD"], ["Denker", "Marcus", "", "RMOD"], ["Ducasse", "St\u00e9phane", "", "RMOD"], ["Etien", "Anne", "", "RMOD"], ["Aranega", "Vincent", "", "RMOD"]]}, {"id": "2101.02661", "submitter": "Oscar Sainz", "authors": "Oscar Sainz and German Rigau", "title": "Ask2Transformers: Zero-Shot Domain labelling with Pre-trained Language\n  Models", "comments": "Accepted on Proceedings of the 11th Global WordNet Conference (GWC\n  2021).", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a system that exploits different pre-trained\nLanguage Models for assigning domain labels to WordNet synsets without any kind\nof supervision. Furthermore, the system is not restricted to use a particular\nset of domain labels. We exploit the knowledge encoded within different\noff-the-shelf pre-trained Language Models and task formulations to infer the\ndomain label of a particular WordNet definition. The proposed zero-shot system\nachieves a new state-of-the-art on the English dataset used in the evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:04:39 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:57:42 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Sainz", "Oscar", ""], ["Rigau", "German", ""]]}, {"id": "2101.02797", "submitter": "Nisreen Ali", "authors": "Nisreen AbdAllah and Serestina Viriri", "title": "Off-Line Arabic Handwritten Words Segmentation using Morphological\n  Operators", "comments": "16 pages,27 figures", "journal-ref": "Signal & Image Processing: An International Journal (SIPIJ)\n  Vol.11, No.6, December 2020", "doi": "10.5121/sipij.2020.11602", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main aim of this study is the assessment and discussion of a model for\nhand-written Arabic through segmentation. The framework is proposed based on\nthree steps: pre-processing, segmentation, and evaluation. In the\npre-processing step, morphological operators are applied for Connecting Gaps\n(CGs) in written words. Gaps happen when pen lifting-off during writing,\nscanning documents, or while converting images to binary type. In the\nsegmentation step, first removed the small diacritics then bounded a connected\ncomponent to segment offline words. Huge data was utilized in the proposed\nmodel for applying a variety of handwriting styles so that to be more\ncompatible with real-life applications. Consequently, on the automatic\nevaluation stage, selected randomly 1,131 images from the IESK-ArDB database,\nand then segmented into sub-words. After small gaps been connected, the model\nperformance evaluation had been reached 88% against the standard ground truth\nof the database. The proposed model achieved the highest accuracy when compared\nwith the related works.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 23:38:53 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["AbdAllah", "Nisreen", ""], ["Viriri", "Serestina", ""]]}, {"id": "2101.02875", "submitter": "Mohanad Al-Mousa", "authors": "Mohannad AlMousa, Rachid Benlamri, Richard Khoury", "title": "A Novel Word Sense Disambiguation Approach Using WordNet Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various applications in computational linguistics and artificial intelligence\nrely on high-performing word sense disambiguation techniques to solve\nchallenging tasks such as information retrieval, machine translation, question\nanswering, and document clustering. While text comprehension is intuitive for\nhumans, machines face tremendous challenges in processing and interpreting a\nhuman's natural language. This paper presents a novel knowledge-based word\nsense disambiguation algorithm, namely Sequential Contextual Similarity Matrix\nMultiplication (SCSMM). The SCSMM algorithm combines semantic similarity,\nheuristic knowledge, and document context to respectively exploit the merits of\nlocal context between consecutive terms, human knowledge about terms, and a\ndocument's main topic in disambiguating terms. Unlike other algorithms, the\nSCSMM algorithm guarantees the capture of the maximum sentence context while\nmaintaining the terms' order within the sentence. The proposed algorithm\noutperformed all other algorithms when disambiguating nouns on the combined\ngold standard datasets, while demonstrating comparable results to current\nstate-of-the-art word sense disambiguation systems when dealing with each\ndataset separately. Furthermore, the paper discusses the impact of granularity\nlevel, ambiguity rate, sentence size, and part of speech distribution on the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:47:32 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["AlMousa", "Mohannad", ""], ["Benlamri", "Rachid", ""], ["Khoury", "Richard", ""]]}, {"id": "2101.02906", "submitter": "Anwar Alnawas", "authors": "Anwar Alnawas and Nursal ARICI", "title": "Effect of Word Embedding Variable Parameters on Arabic Sentiment\n  Analysis Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media such as Twitter, Facebook, etc. has led to a generated growing\nnumber of comments that contains users opinions. Sentiment analysis research\ndeals with these comments to extract opinions which are positive or negative.\nArabic language is a rich morphological language; thus, classical techniques of\nEnglish sentiment analysis cannot be used for Arabic. Word embedding technique\ncan be considered as one of successful methods to gaping the morphological\nproblem of Arabic. Many works have been done for Arabic sentiment analysis\nbased on word embedding, but there is no study focused on variable parameters.\nThis study will discuss three parameters (Window size, Dimension of vector and\nNegative Sample) for Arabic sentiment analysis using DBOW and DMPV\narchitectures. A large corpus of previous works generated to learn word\nrepresentations and extract features. Four binary classifiers (Logistic\nRegression, Decision Tree, Support Vector Machine and Naive Bayes) are used to\ndetect sentiment. The performance of classifiers evaluated based on; Precision,\nRecall and F1-score.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:31:00 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Alnawas", "Anwar", ""], ["ARICI", "Nursal", ""]]}, {"id": "2101.03024", "submitter": "Sourav Ghosh", "authors": "Sonal Kumari, Vibhav Agarwal, Bharath Challa, Kranti Chalamalasetti,\n  Sourav Ghosh, Harshavardhana, Barath Raj Kandur Raja", "title": "LiteMuL: A Lightweight On-Device Sequence Tagger using Multi-task\n  Learning", "comments": "Published in 2021 IEEE 15th International Conference on Semantic\n  Computing (ICSC); Candidate for Best Paper Award", "journal-ref": "2021 IEEE 15th International Conference on Semantic Computing\n  (ICSC), Laguna Hills, CA, USA, 2021, pp. 1-8", "doi": "10.1109/ICSC50631.2021.00007", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Named entity detection and Parts-of-speech tagging are the key tasks for many\nNLP applications. Although the current state of the art methods achieved near\nperfection for long, formal, structured text there are hindrances in deploying\nthese models on memory-constrained devices such as mobile phones. Furthermore,\nthe performance of these models is degraded when they encounter short,\ninformal, and casual conversations. To overcome these difficulties, we present\nLiteMuL - a lightweight on-device sequence tagger that can efficiently process\nthe user conversations using a Multi-Task Learning (MTL) approach. To the best\nof our knowledge, the proposed model is the first on-device MTL neural model\nfor sequence tagging. Our LiteMuL model is about 2.39 MB in size and achieved\nan accuracy of 0.9433 (for NER), 0.9090 (for POS) on the CoNLL 2003 dataset.\nThe proposed LiteMuL not only outperforms the current state of the art results\nbut also surpasses the results of our proposed on-device task-specific models,\nwith accuracy gains of up to 11% and model-size reduction by 50%-56%. Our model\nis competitive with other MTL approaches for NER and POS tasks while outshines\nthem with a low memory footprint. We also evaluated our model on custom-curated\nuser conversations and observed impressive results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:15:54 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:31:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kumari", "Sonal", ""], ["Agarwal", "Vibhav", ""], ["Challa", "Bharath", ""], ["Chalamalasetti", "Kranti", ""], ["Ghosh", "Sourav", ""], ["Harshavardhana", "", ""], ["Raja", "Barath Raj Kandur", ""]]}, {"id": "2101.03025", "submitter": "Sourav Ghosh", "authors": "Vibhav Agarwal, Sourav Ghosh, Kranti Chalamalasetti, Bharath Challa,\n  Sonal Kumari, Harshavardhana, Barath Raj Kandur Raja", "title": "EmpLite: A Lightweight Sequence Labeling Model for Emphasis Selection of\n  Short Texts", "comments": "Accepted for publication in ICON 2020: 17th International Conference\n  on Natural Language Processing", "journal-ref": "17th International Conference on Natural Language Processing\n  (ICON), Patna, India, December 18 - 21, 2020, pages 19-26, ACL Anthology:\n  2020.icon-1.3", "doi": null, "report-no": "2020.icon-1.3 (ACL Anthology)", "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Word emphasis in textual content aims at conveying the desired intention by\nchanging the size, color, typeface, style (bold, italic, etc.), and other\ntypographical features. The emphasized words are extremely helpful in drawing\nthe readers' attention to specific information that the authors wish to\nemphasize. However, performing such emphasis using a soft keyboard for social\nmedia interactions is time-consuming and has an associated learning curve. In\nthis paper, we propose a novel approach to automate the emphasis word detection\non short written texts. To the best of our knowledge, this work presents the\nfirst lightweight deep learning approach for smartphone deployment of emphasis\nselection. Experimental results show that our approach achieves comparable\naccuracy at a much lower model size than existing models. Our best lightweight\nmodel has a memory footprint of 2.82 MB with a matching score of 0.716 on\nSemEval-2020 public benchmark dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:00:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Agarwal", "Vibhav", ""], ["Ghosh", "Sourav", ""], ["Chalamalasetti", "Kranti", ""], ["Challa", "Bharath", ""], ["Kumari", "Sonal", ""], ["Harshavardhana", "", ""], ["Raja", "Barath Raj Kandur", ""]]}, {"id": "2101.03026", "submitter": "Carlos Badenes-Olmedo", "authors": "Carlos Badenes-Olmedo, Jose-Luis Redondo Garc\\'ia, Oscar Corcho", "title": "Scalable Cross-lingual Document Similarity through Language-specific\n  Concept Hierarchies", "comments": "Accepted at the 10th International Conference on Knowledge Capture\n  (K-CAP 2019)", "journal-ref": "AACM Proceedings of the 10th International Conference on Knowledge\n  Capture, pages = 147-153, K-CAP 19 (2020)", "doi": "10.1145/3360901.3364444", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ongoing growth in number of digital articles in a wider set of\nlanguages and the expanding use of different languages, we need annotation\nmethods that enable browsing multi-lingual corpora. Multilingual probabilistic\ntopic models have recently emerged as a group of semi-supervised machine\nlearning models that can be used to perform thematic explorations on\ncollections of texts in multiple languages. However, these approaches require\ntheme-aligned training data to create a language-independent space. This\nconstraint limits the amount of scenarios that this technique can offer\nsolutions to train and makes it difficult to scale up to situations where a\nhuge collection of multi-lingual documents are required during the training\nphase. This paper presents an unsupervised document similarity algorithm that\ndoes not require parallel or comparable corpora, or any other type of\ntranslation resource. The algorithm annotates topics automatically created from\ndocuments in a single language with cross-lingual labels and describes\ndocuments by hierarchies of multi-lingual concepts from independently-trained\nmodels. Experiments performed on the English, Spanish and French editions of\nJCR-Acquis corpora reveal promising results on classifying and sorting\ndocuments by similar content.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:42:40 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Badenes-Olmedo", "Carlos", ""], ["Garc\u00eda", "Jose-Luis Redondo", ""], ["Corcho", "Oscar", ""]]}, {"id": "2101.03027", "submitter": "Alexis Michaud", "authors": "Oliver Adams, Benjamin Galliot (LACITO), Guillaume Wisniewski (LLF\n  UMR7110), Nicholas Lambourne, Ben Foley, Rahasya Sanders-Dwyer, Janet Wiles,\n  Alexis Michaud (LACITO), S\\'everine Guillaume (LACITO), Laurent Besacier\n  (LIG), Christopher Cox, Katya Aplonova (LLACAN), Guillaume Jacques (CRLAO),\n  Nathan Hill", "title": "User-friendly automatic transcription of low-resource languages:\n  Plugging ESPnet into Elpis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on progress integrating the speech recognition toolkit\nESPnet into Elpis, a web front-end originally designed to provide access to the\nKaldi automatic speech recognition toolkit. The goal of this work is to make\nend-to-end speech recognition models available to language workers via a\nuser-friendly graphical interface. Encouraging results are reported on (i)\ndevelopment of an ESPnet recipe for use in Elpis, with preliminary results on\ndata sets previously used for training acoustic models with the Persephone\ntoolkit along with a new data set that had not previously been used in speech\nrecognition, and (ii) incorporating ESPnet into Elpis along with UI\nenhancements and a CUDA-supported Dockerfile.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:06:21 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 07:23:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Adams", "Oliver", "", "LACITO"], ["Galliot", "Benjamin", "", "LACITO"], ["Wisniewski", "Guillaume", "", "LLF\n  UMR7110"], ["Lambourne", "Nicholas", "", "LACITO"], ["Foley", "Ben", "", "LACITO"], ["Sanders-Dwyer", "Rahasya", "", "LACITO"], ["Wiles", "Janet", "", "LACITO"], ["Michaud", "Alexis", "", "LACITO"], ["Guillaume", "S\u00e9verine", "", "LACITO"], ["Besacier", "Laurent", "", "LIG"], ["Cox", "Christopher", "", "LLACAN"], ["Aplonova", "Katya", "", "LLACAN"], ["Jacques", "Guillaume", "", "CRLAO"], ["Hill", "Nathan", ""]]}, {"id": "2101.03028", "submitter": "Chenghao Huang", "authors": "Qi Wu, Peng Wang, Chenghao Huang", "title": "MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask\n  Learning for Sentiment Analysis of Code-Mixed Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) has been applied to various fields\nincluding text classification and sentiment analysis. In the shared task of\nsentiment analysis of code-mixed tweets, which is a part of the SemEval-2020\ncompetition~\\cite{patwa2020sentimix}, we preprocess datasets by replacing emoji\nand deleting uncommon characters and so on, and then fine-tune the\nBidirectional Encoder Representation from Transformers(BERT) to perform the\nbest. After exhausting top3 submissions, Our team MeisterMorxrc achieves an\naveraged F1 score of 0.730 in this task, and and our codalab username is\nMeisterMorxrc.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:42:14 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wu", "Qi", ""], ["Wang", "Peng", ""], ["Huang", "Chenghao", ""]]}, {"id": "2101.03029", "submitter": "Mansooreh Karami", "authors": "Mansooreh Karami, Ahmadreza Mosallanezhad, Michelle V Mancenido, Huan\n  Liu", "title": "\"Let's Eat Grandma\": When Punctuation Matters in Sentence Representation\n  for Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network-based embeddings have been the mainstream approach for\ncreating a vector representation of the text to capture lexical and semantic\nsimilarities and dissimilarities. In general, existing encoding methods dismiss\nthe punctuation as insignificant information; consequently, they are routinely\neliminated in the pre-processing phase as they are shown to improve task\nperformance. In this paper, we hypothesize that punctuation could play a\nsignificant role in sentiment analysis and propose a novel representation model\nto improve syntactic and contextual performance. We corroborate our findings by\nconducting experiments on publicly available datasets and verify that our model\ncan identify the sentiments more accurately over other state-of-the-art\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 19:07:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Karami", "Mansooreh", ""], ["Mosallanezhad", "Ahmadreza", ""], ["Mancenido", "Michelle V", ""], ["Liu", "Huan", ""]]}, {"id": "2101.03204", "submitter": "Xiaonan Jing", "authors": "Yifei Hu, Xiaonan Jing, Youlim Ko, Julia Taylor Rayz", "title": "Misspelling Correction with Pre-trained Contextual Language Model", "comments": "Accepted by 2020 IEEE 19th International Conference on Cognitive\n  Informatics & Cognitive Computing (ICCI* CC). IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Spelling irregularities, known now as spelling mistakes, have been found for\nseveral centuries. As humans, we are able to understand most of the misspelled\nwords based on their location in the sentence, perceived pronunciation, and\ncontext. Unlike humans, computer systems do not possess the convenient auto\ncomplete functionality of which human brains are capable. While many programs\nprovide spelling correction functionality, many systems do not take context\ninto account. Moreover, Artificial Intelligence systems function in the way\nthey are trained on. With many current Natural Language Processing (NLP)\nsystems trained on grammatically correct text data, many are vulnerable against\nadversarial examples, yet correctly spelled text processing is crucial for\nlearning. In this paper, we investigate how spelling errors can be corrected in\ncontext, with a pre-trained language model BERT. We present two experiments,\nbased on BERT and the edit distance algorithm, for ranking and selecting\ncandidate corrections. The results of our experiments demonstrated that when\ncombined properly, contextual word embeddings of BERT and edit distance are\ncapable of effectively correcting spelling errors.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:11:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hu", "Yifei", ""], ["Jing", "Xiaonan", ""], ["Ko", "Youlim", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2101.03207", "submitter": "Sayar Ghosh Roy", "authors": "Sayar Ghosh Roy, Ujwal Narayan, Tathagata Raha, Zubair Abid, Vasudeva\n  Varma", "title": "Leveraging Multilingual Transformers for Hate Speech Detection", "comments": "To be published in: FIRE (Working Notes) 2020, Hate Speech and\n  Offensive Content Identification in Indo-European Languages, HASOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting and classifying instances of hate in social media text has been a\nproblem of interest in Natural Language Processing in the recent years. Our\nwork leverages state of the art Transformer language models to identify hate\nspeech in a multilingual setting. Capturing the intent of a post or a comment\non social media involves careful evaluation of the language style, semantic\ncontent and additional pointers such as hashtags and emojis. In this paper, we\nlook at the problem of identifying whether a Twitter post is hateful and\noffensive or not. We further discriminate the detected toxic content into one\nof the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN)\nand (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text\nencoder at the base, we are able to successfully identify and classify hate\nspeech from multiple languages. On the provided testing corpora, we achieve\nMacro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi\nrespectively while performing hate speech detection and of 60.70, 53.28 and\n49.74 during fine-grained classification. In our experiments, we show the\nefficacy of Perspective API features for hate speech classification and the\neffects of exploiting a multilingual training scheme. A feature selection study\nis provided to illustrate impacts of specific features upon the architecture's\nclassification head.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:23:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Sayar Ghosh", ""], ["Narayan", "Ujwal", ""], ["Raha", "Tathagata", ""], ["Abid", "Zubair", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03208", "submitter": "Xiaonan Jing", "authors": "Xiaonan Jing, Julia Taylor Rayz", "title": "Graph-of-Tweets: A Graph Merging Approach to Sub-event Identification", "comments": "Accepted by 2020 IEEE/WIC/ACM International Joint Conference on Web\n  Intelligence and Intelligent Agent Technology (WI-IAT) Workshop on Data\n  Analytics on Social Media (DASM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph structures are powerful tools for modeling the relationships between\ntextual elements. Graph-of-Words (GoW) has been adopted in many Natural\nLanguage tasks to encode the association between terms. However, GoW provides\nfew document-level relationships in cases when the connections between\ndocuments are also essential. For identifying sub-events on social media like\nTwitter, features from both word- and document-level can be useful as they\nsupply different information of the event. We propose a hybrid Graph-of-Tweets\n(GoT) model which combines the word- and document-level structures for modeling\nTweets. To compress large amount of raw data, we propose a graph merging method\nwhich utilizes FastText word embeddings to reduce the GoW. Furthermore, we\npresent a novel method to construct GoT with the reduced GoW and a Mutual\nInformation (MI) measure. Finally, we identify maximal cliques to extract\npopular sub-events. Our model showed promising results on condensing\nlexical-level information and capturing keywords of sub-events.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:24:25 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Jing", "Xiaonan", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2101.03216", "submitter": "Alexandre Duval", "authors": "Alexandre Duval, Thomas Lamson, Gael de Leseleuc de Kerouara and\n  Matthias Gall\\'e", "title": "Breaking Writer's Block: Low-cost Fine-tuning of Natural Language\n  Generation Models", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is standard procedure these days to solve Information Extraction task by\nfine-tuning large pre-trained language models. This is not the case for\ngeneration task, which relies on a variety of techniques for controlled\nlanguage generation. In this paper, we describe a system that fine-tunes a\nnatural language generation model for the problem of solving Writer's Block.\nThe fine-tuning changes the conditioning to also include the right context in\naddition to the left context, as well as an optional list of entities, the\nsize, the genre and a summary of the paragraph that the human author wishes to\ngenerate. Our proposed fine-tuning obtains excellent results, even with a small\nnumber of epochs and a total cost of USD 150. The system can be accessed as a\nweb-service, and all the code is released. A video showcasing the interface and\nthe model is also available.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 11:19:11 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:03:32 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Duval", "Alexandre", ""], ["Lamson", "Thomas", ""], ["de Kerouara", "Gael de Leseleuc", ""], ["Gall\u00e9", "Matthias", ""]]}, {"id": "2101.03229", "submitter": "Linda Liu", "authors": "Linda Liu, Yile Gu, Aditya Gourav, Ankur Gandhe, Shashank Kalmane,\n  Denis Filimonov, Ariya Rastrow, Ivan Bulyko", "title": "Domain-aware Neural Language Models for Speech Recognition", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As voice assistants become more ubiquitous, they are increasingly expected to\nsupport and perform well on a wide variety of use-cases across different\ndomains. We present a domain-aware rescoring framework suitable for achieving\ndomain-adaptation during second-pass rescoring in production settings. In our\nframework, we fine-tune a domain-general neural language model on several\ndomains, and use an LSTM-based domain classification model to select the\nappropriate domain-adapted model to use for second-pass rescoring. This\ndomain-aware rescoring improves the word error rate by up to 2.4% and slot word\nerror rate by up to 4.1% on three individual domains -- shopping, navigation,\nand music -- compared to domain general rescoring. These improvements are\nobtained while maintaining accuracy for the general use case.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:08:32 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 19:42:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Liu", "Linda", ""], ["Gu", "Yile", ""], ["Gourav", "Aditya", ""], ["Gandhe", "Ankur", ""], ["Kalmane", "Shashank", ""], ["Filimonov", "Denis", ""], ["Rastrow", "Ariya", ""], ["Bulyko", "Ivan", ""]]}, {"id": "2101.03235", "submitter": "Krishna Yadav", "authors": "Krishna Yadav, Lakshya Choudhary", "title": "Key Phrase Extraction & Applause Prediction", "comments": "4 pages, 8 figures best project award winner.\n  https://krishna19039.medium.com/key-phrase-extraction-applause-prediction-7b397c7ad76d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increase in content availability over the internet it is very\ndifficult to get noticed. It has become an upmost the priority of the blog\nwriters to get some feedback over their creations to be confident about the\nimpact of their article. We are training a machine learning model to learn\npopular article styles, in the form of vector space representations using\nvarious word embeddings, and their popularity based on claps and tags.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 12:49:43 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yadav", "Krishna", ""], ["Choudhary", "Lakshya", ""]]}, {"id": "2101.03236", "submitter": "Ping Yu", "authors": "Ping Yu, Ruiyi Zhang, Yang Zhao, Yizhe Zhang, Chunyuan Li, Changyou\n  Chen", "title": "SDA: Improving Text Generation with Self Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Data augmentation has been widely used to improve deep neural networks in\nmany research fields, such as computer vision. However, less work has been done\nin the context of text, partially due to its discrete nature and the complexity\nof natural languages. In this paper, we propose to improve the standard maximum\nlikelihood estimation (MLE) paradigm by incorporating a self-imitation-learning\nphase for automatic data augmentation. Unlike most existing sentence-level\naugmentation strategies, which are only applied to specific models, our method\nis more general and could be easily adapted to any MLE-based training\nprocedure. In addition, our framework allows task-specific evaluation metrics\nto be designed to flexibly control the generated sentences, for example, in\nterms of controlling vocabulary usage and avoiding nontrivial repetitions.\nExtensive experimental results demonstrate the superiority of our method on two\nsynthetic and several standard real datasets, significantly improving related\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 01:15:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yu", "Ping", ""], ["Zhang", "Ruiyi", ""], ["Zhao", "Yang", ""], ["Zhang", "Yizhe", ""], ["Li", "Chunyuan", ""], ["Chen", "Changyou", ""]]}, {"id": "2101.03237", "submitter": "Amirreza Shirani", "authors": "Amirreza Shirani, Giai Tran, Hieu Trinh, Franck Dernoncourt, Nedim\n  Lipka, Paul Asente, Jose Echevarria, and Thamar Solorio", "title": "Learning to Emphasize: Dataset and Shared Task Models for Selecting\n  Emphasis in Presentation Slides", "comments": "In Proceedings of Content Authoring and Design (CAD21) workshop at\n  the Thirty-fifth AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Presentation slides have become a common addition to the teaching material.\nEmphasizing strong leading words in presentation slides can allow the audience\nto direct the eye to certain focal points instead of reading the entire slide,\nretaining the attention to the speaker during the presentation. Despite a large\nvolume of studies on automatic slide generation, few studies have addressed the\nautomation of design assistance during the creation process. Motivated by this\ndemand, we study the problem of Emphasis Selection (ES) in presentation slides,\ni.e., choosing candidates for emphasis, by introducing a new dataset containing\npresentation slides with a wide variety of topics, each is annotated with\nemphasis words in a crowdsourced setting. We evaluate a range of\nstate-of-the-art models on this novel dataset by organizing a shared task and\ninviting multiple researchers to model emphasis in this new domain. We present\nthe main findings and compare the results of these models, and by examining the\nchallenges of the dataset, we provide different analysis components.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 06:54:55 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Shirani", "Amirreza", ""], ["Tran", "Giai", ""], ["Trinh", "Hieu", ""], ["Dernoncourt", "Franck", ""], ["Lipka", "Nedim", ""], ["Asente", "Paul", ""], ["Echevarria", "Jose", ""], ["Solorio", "Thamar", ""]]}, {"id": "2101.03269", "submitter": "Masayuki Asahara", "authors": "Masayuki Asahara", "title": "A Gamification of Japanese Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gamification approaches have been used as a way for creating language\nresources for NLP. It is also used for presenting and teaching the algorithms\nin NLP and linguistic phenomena. This paper argues about a design of\ngamification for Japanese syntactic dependendency parsing for the latter\nobjective. The user interface design is based on a transition-based shift\nreduce dependency parsing which needs only two actions of SHIFT (not attach)\nand REDUCE (attach) in Japanese dependency structure. We assign the two actions\nfor two-way directional control on a gamepad or other devices. We also design\nthe target sentences from psycholinguistics researches.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 01:42:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Asahara", "Masayuki", ""]]}, {"id": "2101.03287", "submitter": "Liang Lin", "authors": "Fuyu Wang and Xiaodan Liang and Lin Xu and Liang Lin", "title": "Unifying Relational Sentence Generation and Retrieval for Medical Image\n  Report Composition", "comments": "To appear in IEEE Transactions on Cybernetics 2021. We attempt to\n  resolve the challenging medical report composition task by i) enforcing the\n  semantic consistency of medical terms to be incorporated into the final\n  reports; and ii) encouraging the sentence generation for rare abnormal\n  descriptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond generating long and topic-coherent paragraphs in traditional\ncaptioning tasks, the medical image report composition task poses more\ntask-oriented challenges by requiring both the highly-accurate medical term\ndiagnosis and multiple heterogeneous forms of information including impression\nand findings. Current methods often generate the most common sentences due to\ndataset bias for individual case, regardless of whether the sentences properly\ncapture key entities and relationships. Such limitations severely hinder their\napplicability and generalization capability in medical report composition where\nthe most critical sentences lie in the descriptions of abnormal diseases that\nare relatively rare. Moreover, some medical terms appearing in one report are\noften entangled with each other and co-occurred, e.g. symptoms associated with\na specific disease. To enforce the semantic consistency of medical terms to be\nincorporated into the final reports and encourage the sentence generation for\nrare abnormal descriptions, we propose a novel framework that unifies template\nretrieval and sentence generation to handle both common and rare abnormality\nwhile ensuring the semantic-coherency among the detected medical terms.\nSpecifically, our approach exploits hybrid-knowledge co-reasoning: i) explicit\nrelationships among all abnormal medical terms to induce the visual attention\nlearning and topic representation encoding for better topic-oriented symptoms\ndescriptions; ii) adaptive generation mode that changes between the template\nretrieval and sentence generation according to a contextual topic encoder.\nExperimental results on two medical report benchmarks demonstrate the\nsuperiority of the proposed framework in terms of both human and metrics\nevaluation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:33:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Fuyu", ""], ["Liang", "Xiaodan", ""], ["Xu", "Lin", ""], ["Lin", "Liang", ""]]}, {"id": "2101.03289", "submitter": "Minh Nguyen", "authors": "Minh Van Nguyen, Viet Lai, Amir Pouran Ben Veyseh, and Thien Huu\n  Nguyen", "title": "Trankit: A Light-Weight Transformer-based Toolkit for Multilingual\n  Natural Language Processing", "comments": "Camera-ready version for EACL 2021 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce Trankit, a light-weight Transformer-based Toolkit for\nmultilingual Natural Language Processing (NLP). It provides a trainable\npipeline for fundamental NLP tasks over 100 languages, and 90 pretrained\npipelines for 56 languages. Built on a state-of-the-art pretrained language\nmodel, Trankit significantly outperforms prior multilingual NLP pipelines over\nsentence segmentation, part-of-speech tagging, morphological feature tagging,\nand dependency parsing while maintaining competitive performance for\ntokenization, multi-word token expansion, and lemmatization over 90 Universal\nDependencies treebanks. Despite the use of a large pretrained transformer, our\ntoolkit is still efficient in memory usage and speed. This is achieved by our\nnovel plug-and-play mechanism with Adapters where a multilingual pretrained\ntransformer is shared across pipelines for different languages. Our toolkit\nalong with pretrained models and code are publicly available at:\nhttps://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also\navailable at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video\nfor Trankit at: https://youtu.be/q0KGP3zGjGc.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:55:52 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 19:10:10 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 16:30:55 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 04:27:46 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Van Nguyen", "Minh", ""], ["Lai", "Viet", ""], ["Veyseh", "Amir Pouran Ben", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2101.03291", "submitter": "Omar Sharif", "authors": "Omar Sharif, Eftekhar Hossain, Mohammed Moshiul Hoque", "title": "Combating Hostility: Covid-19 Fake News and Hostile Post Detection in\n  Social Media", "comments": "Shared task description paper in CONSTRAINT workshop collocated with\n  AAAI-2021, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper illustrates a detail description of the system and its results\nthat developed as a part of the participation at CONSTRAINT shared task in\nAAAI-2021. The shared task comprises two tasks: a) COVID19 fake news detection\nin English b) Hostile post detection in Hindi. Task-A is a binary\nclassification problem with fake and real class, while task-B is a multi-label\nmulti-class classification task with five hostile classes (i.e. defame, fake,\nhate, offense, non-hostile). Various techniques are used to perform the\nclassification task, including SVM, CNN, BiLSTM, and CNN+BiLSTM with tf-idf and\nWord2Vec embedding techniques. Results indicate that SVM with tf-idf features\nachieved the highest 94.39% weighted $f_1$ score on the test set in task-A.\nLabel powerset SVM with n-gram features obtained the maximum coarse-grained and\nfine-grained $f_1$ score of 86.03% and 50.98% on the task-B test set\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 05:15:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sharif", "Omar", ""], ["Hossain", "Eftekhar", ""], ["Hoque", "Mohammed Moshiul", ""]]}, {"id": "2101.03303", "submitter": "Anurag Roy", "authors": "Anurag Roy, Shalmoli Ghosh, Kripabandhu Ghosh, Saptarshi Ghosh", "title": "An Unsupervised Normalization Algorithm for Noisy Text: A Case Study for\n  Information Retrieval and Stance Detection", "comments": "Will be appearing in the ACM Journal of Data and Information Quality.\n  Implementation available at https://github.com/ranarag/UnsupClean", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of textual data available today contains various types of\n'noise', such as OCR noise in digitized documents, noise due to informal\nwriting style of users on microblogging sites, and so on. To enable tasks such\nas search/retrieval and classification over all the available data, we need\nrobust algorithms for text normalization, i.e., for cleaning different kinds of\nnoise in the text. There have been several efforts towards cleaning or\nnormalizing noisy text; however, many of the existing text normalization\nmethods are supervised and require language-dependent resources or large\namounts of training data that is difficult to obtain. We propose an\nunsupervised algorithm for text normalization that does not need any training\ndata / human intervention. The proposed algorithm is applicable to text over\ndifferent languages, and can handle both machine-generated and human-generated\nnoise. Experiments over several standard datasets show that text normalization\nthrough the proposed algorithm enables better retrieval and stance detection,\nas compared to that using several baseline text normalization methods.\nImplementation of our algorithm can be found at\nhttps://github.com/ranarag/UnsupClean.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:57:09 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Anurag", ""], ["Ghosh", "Shalmoli", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2101.03305", "submitter": "Ting Jiang", "authors": "Ting Jiang, Deqing Wang, Leilei Sun, Huayi Yang, Zhengyang Zhao,\n  Fuzhen Zhuang", "title": "LightXML: Transformer with Dynamic Negative Sampling for\n  High-Performance Extreme Multi-label Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme Multi-label text Classification (XMC) is a task of finding the most\nrelevant labels from a large label set. Nowadays deep learning-based methods\nhave shown significant success in XMC. However, the existing methods (e.g.,\nAttentionXML and X-Transformer etc) still suffer from 1) combining several\nmodels to train and predict for one dataset, and 2) sampling negative labels\nstatically during the process of training label ranking model, which reduces\nboth the efficiency and accuracy of the model. To address the above problems,\nwe proposed LightXML, which adopts end-to-end training and dynamic negative\nlabels sampling. In LightXML, we use generative cooperative networks to recall\nand rank labels, in which label recalling part generates negative and positive\nlabels, and label ranking part distinguishes positive labels from these labels.\nThrough these networks, negative labels are sampled dynamically during label\nranking part training by feeding with the same text representation. Extensive\nexperiments show that LightXML outperforms state-of-the-art methods in five\nextreme multi-label datasets with much smaller model size and lower\ncomputational complexity. In particular, on the Amazon dataset with 670K\nlabels, LightXML can reduce the model size up to 72% compared to AttentionXML.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 07:04:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Jiang", "Ting", ""], ["Wang", "Deqing", ""], ["Sun", "Leilei", ""], ["Yang", "Huayi", ""], ["Zhao", "Zhengyang", ""], ["Zhuang", "Fuzhen", ""]]}, {"id": "2101.03343", "submitter": "Chen Yang", "authors": "Chen Yang (University of Science and Technology of China)", "title": "Learning Better Sentence Representation with Syntax Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence semantic understanding is a key topic in the field of natural\nlanguage processing. Recently, contextualized word representations derived from\npre-trained language models such as ELMO and BERT have shown significant\nimprovements for a wide range of semantic tasks, e.g. question answering, text\nclassification and sentiment analysis. However, how to add external knowledge\nto further improve the semantic modeling capability of model is worth probing.\nIn this paper, we propose a novel approach to combining syntax information with\na pre-trained language model. In order to evaluate the effect of the\npre-training model, first, we introduce RNN-based and Transformer-based\npre-trained language models; secondly, to better integrate external knowledge,\nsuch as syntactic information integrate with the pre-training model, we propose\na dependency syntax expansion (DSE) model. For evaluation, we have selected two\nsubtasks: sentence completion task and biological relation extraction task. The\nexperimental results show that our model achieves 91.2\\% accuracy,\noutperforming the baseline model by 37.8\\% on sentence completion task. And it\nalso gets competitive performance by 75.1\\% $F_{1}$ score on relation\nextraction task.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 12:15:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yang", "Chen", "", "University of Science and Technology of China"]]}, {"id": "2101.03382", "submitter": "Sayar Ghosh Roy", "authors": "Tathagata Raha, Sayar Ghosh Roy, Ujwal Narayan, Zubair Abid, Vasudeva\n  Varma", "title": "Task Adaptive Pretraining of Transformers for Hostility Detection", "comments": "To be published in: Proceedings of the First Workshop on Combating\n  Online Hostile Posts in Regional Languages during Emergency Situation\n  (CONSTRAINT) at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying adverse and hostile content on the web and more particularly, on\nsocial media, has become a problem of paramount interest in recent years. With\ntheir ever increasing popularity, fine-tuning of pretrained Transformer-based\nencoder models with a classifier head are gradually becoming the new baseline\nfor natural language classification tasks. In our work, we explore the gains\nattributed to Task Adaptive Pretraining (TAPT) prior to fine-tuning of\nTransformer-based architectures. We specifically study two problems, namely,\n(a) Coarse binary classification of Hindi Tweets into Hostile or Not, and (b)\nFine-grained multi-label classification of Tweets into four categories: hate,\nfake, offensive, and defamation. Building up on an architecture which takes\nemojis and segmented hashtags into consideration for classification, we are\nable to experimentally showcase the performance upgrades due to TAPT. Our\nsystem (with team name 'iREL IIIT') ranked first in the 'Hostile Post Detection\nin Hindi' shared task with an F1 score of 97.16% for coarse-grained detection\nand a weighted F1 score of 62.96% for fine-grained multi-label classification\non the provided blind test corpora.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 15:45:26 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Raha", "Tathagata", ""], ["Roy", "Sayar Ghosh", ""], ["Narayan", "Ujwal", ""], ["Abid", "Zubair", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03392", "submitter": "Yongfeng Zhang", "authors": "Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang", "title": "Generate Natural Language Explanations for Recommendation", "comments": "Accepted to the SIGIR 2019 Workshop on ExplainAble Recommendation and\n  Search, Paris, France, July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing personalized explanations for recommendations can help users to\nunderstand the underlying insight of the recommendation results, which is\nhelpful to the effectiveness, transparency, persuasiveness and trustworthiness\nof recommender systems. Current explainable recommendation models mostly\ngenerate textual explanations based on pre-defined sentence templates. However,\nthe expressiveness power of template-based explanation sentences are limited to\nthe pre-defined expressions, and manually defining the expressions require\nsignificant human efforts. Motivated by this problem, we propose to generate\nfree-text natural language explanations for personalized recommendation. In\nparticular, we propose a hierarchical sequence-to-sequence model (HSS) for\npersonalized explanation generation. Different from conventional sentence\ngeneration in NLP research, a great challenge of explanation generation in\ne-commerce recommendation is that not all sentences in user reviews are of\nexplanation purpose. To solve the problem, we further propose an auto-denoising\nmechanism based on topical item feature words for sentence generation.\nExperiments on various e-commerce product domains show that our approach can\nnot only improve the recommendation accuracy, but also the explanation quality\nin terms of the offline measures and feature words coverage. This research is\none of the initial steps to grant intelligent agents with the ability to\nexplain itself based on natural language sentences.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 17:00:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Hanxiong", ""], ["Chen", "Xu", ""], ["Shi", "Shaoyun", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.03431", "submitter": "Shane Storks", "authors": "Shane Storks, Qiaozi Gao, Govind Thattai, Gokhan Tur", "title": "Are We There Yet? Learning to Localize in Embodied Instruction Following", "comments": "Accepted to HAI @ AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied instruction following is a challenging problem requiring an agent to\ninfer a sequence of primitive actions to achieve a goal environment state from\ncomplex language and visual inputs. Action Learning From Realistic Environments\nand Directives (ALFRED) is a recently proposed benchmark for this problem\nconsisting of step-by-step natural language instructions to achieve subgoals\nwhich compose to an ultimate high-level goal. Key challenges for this task\ninclude localizing target locations and navigating to them through visual\ninputs, and grounding language instructions to visual appearance of objects. To\naddress these challenges, in this study, we augment the agent's field of view\nduring navigation subgoals with multiple viewing angles, and train the agent to\npredict its relative spatial relation to the target location at each timestep.\nWe also improve language grounding by introducing a pre-trained object\ndetection module to the model pipeline. Empirical studies show that our\napproach exceeds the baseline model performance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 21:49:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Storks", "Shane", ""], ["Gao", "Qiaozi", ""], ["Thattai", "Govind", ""], ["Tur", "Gokhan", ""]]}, {"id": "2101.03453", "submitter": "Ashim Gupta", "authors": "Ashim Gupta, Giorgi Kvernadze, Vivek Srikumar", "title": "BERT & Family Eat Word Salad: Experiments with Text Understanding", "comments": "Accepted at AAAI 2021, Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the response of large models from the BERT family to\nincoherent inputs that should confuse any model that claims to understand\nnatural language. We define simple heuristics to construct such examples. Our\nexperiments show that state-of-the-art models consistently fail to recognize\nthem as ill-formed, and instead produce high confidence predictions on them. As\na consequence of this phenomenon, models trained on sentences with randomly\npermuted word order perform close to state-of-the-art models. To alleviate\nthese issues, we show that if models are explicitly trained to recognize\ninvalid inputs, they can be robust to such attacks without a drop in\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 01:32:57 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 12:58:59 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Gupta", "Ashim", ""], ["Kvernadze", "Giorgi", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2101.03485", "submitter": "S. Sarthak", "authors": "Sarthak, Shikhar Shukla, Karm Veer Arya", "title": "Detecting Hostile Posts using Relational Graph Convolutional Network", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is based on the submission to the competition Hindi Constraint\nconducted by AAAI@2021 for detection of hostile posts in Hindi on social media\nplatforms. Here, a model is presented for detection and classification of\nhostile posts and further classify into fake, offensive, hate and defamation\nusing Relational Graph Convolutional Networks. Unlike other existing work, our\napproach is focused on using semantic meaning along with contextutal\ninformation for better classification. The results from AAAI@2021 indicates\nthat the proposed model is performing at par with Google's XLM-RoBERTa on the\ngiven dataset. Our best submission with RGCN achieves an F1 score of 0.97 (7th\nRank) on coarse-grained evaluation and achieved best performance on identifying\nfake posts. Among all submissions to the challenge, our classification system\nwith XLM-Roberta secured 2nd rank on fine-grained classification.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 06:50:22 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:09:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Sarthak", "", ""], ["Shukla", "Shikhar", ""], ["Arya", "Karm Veer", ""]]}, {"id": "2101.03526", "submitter": "Yan Xiao", "authors": "Yan Xiao, Yaochu Jin, and Kuangrong Hao", "title": "Adaptive Prototypical Networks with Label Words and Joint Representation\n  Learning for Few-Shot Relation Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relation classification (RC) task is one of fundamental tasks of information\nextraction, aiming to detect the relation information between entity pairs in\nunstructured natural language text and generate structured data in the form of\nentity-relation triple. Although distant supervision methods can effectively\nalleviate the problem of lack of training data in supervised learning, they\nalso introduce noise into the data, and still cannot fundamentally solve the\nlong-tail distribution problem of the training instances. In order to enable\nthe neural network to learn new knowledge through few instances like humans,\nthis work focuses on few-shot relation classification (FSRC), where a\nclassifier should generalize to new classes that have not been seen in the\ntraining set, given only a number of samples for each class. To make full use\nof the existing information and get a better feature representation for each\ninstance, we propose to encode each class prototype in an adaptive way from two\naspects. First, based on the prototypical networks, we propose an adaptive\nmixture mechanism to add label words to the representation of the class\nprototype, which, to the best of our knowledge, is the first attempt to\nintegrate the label information into features of the support samples of each\nclass so as to get more interactive class prototypes. Second, to more\nreasonably measure the distances between samples of each category, we introduce\na loss function for joint representation learning to encode each support\ninstance in an adaptive manner. Extensive experiments have been conducted on\nFewRel under different few-shot (FS) settings, and the results show that the\nproposed adaptive prototypical networks with label words and joint\nrepresentation learning has not only achieved significant improvements in\naccuracy, but also increased the generalization ability of few-shot RC models.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 11:25:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xiao", "Yan", ""], ["Jin", "Yaochu", ""], ["Hao", "Kuangrong", ""]]}, {"id": "2101.03545", "submitter": "Saikat Dutta", "authors": "Sourya Dipta Das, Ayan Basak and Saikat Dutta", "title": "A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection", "comments": "Accepted to CONSTRAINT Workshop, AAAI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of social media has increased manifold in the past few\ndecades as it helps people from even the most remote corners of the world stay\nconnected. With the COVID-19 pandemic raging, social media has become more\nrelevant and widely used than ever before, and along with this, there has been\na resurgence in the circulation of fake news and tweets that demand immediate\nattention. In this paper, we describe our Fake News Detection system that\nautomatically identifies whether a tweet related to COVID-19 is \"real\" or\n\"fake\", as a part of CONSTRAINT COVID19 Fake News Detection in English\nchallenge. We have used an ensemble model consisting of pre-trained models that\nhas helped us achieve a joint 8th position on the leader board. We have\nachieved an F1-score of 0.9831 against a top score of 0.9869. Post completion\nof the competition, we have been able to drastically improve our system by\nincorporating a novel heuristic algorithm based on username handles and link\ndomains in tweets fetching an F1-score of 0.9883 and achieving state-of-the art\nresults on the given dataset.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:21:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Basak", "Ayan", ""], ["Dutta", "Saikat", ""]]}, {"id": "2101.03553", "submitter": "Sayar Ghosh Roy", "authors": "Sayar Ghosh Roy, Nikhil Pinnaparaju, Risubh Jain, Manish Gupta,\n  Vasudeva Varma", "title": "Summaformers @ LaySumm 20, LongSumm 20", "comments": "Proceedings of the First Workshop on Scholarly Document Processing\n  (SDP) at EMNLP 2020", "journal-ref": "In Proceedings of the First Workshop on Scholarly Document\n  Processing, pages 336 - 343, 2020, Online. Association for Computational\n  Linguistics", "doi": "10.18653/v1/2020.sdp-1.39", "report-no": "IIIT/TR/2020/75", "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic text summarization has been widely studied as an important task in\nnatural language processing. Traditionally, various feature engineering and\nmachine learning based systems have been proposed for extractive as well as\nabstractive text summarization. Recently, deep learning based, specifically\nTransformer-based systems have been immensely popular. Summarization is a\ncognitively challenging task - extracting summary worthy sentences is\nlaborious, and expressing semantics in brief when doing abstractive\nsummarization is complicated. In this paper, we specifically look at the\nproblem of summarizing scientific research papers from multiple domains. We\ndifferentiate between two types of summaries, namely, (a) LaySumm: A very short\nsummary that captures the essence of the research paper in layman terms\nrestricting overtly specific technical jargon and (b) LongSumm: A much longer\ndetailed summary aimed at providing specific insights into various ideas\ntouched upon in the paper. While leveraging latest Transformer-based models,\nour systems are simple, intuitive and based on how specific paper sections\ncontribute to human summaries of the two types described above. Evaluations\nagainst gold standard summaries using ROUGE metrics prove the effectiveness of\nour approach. On blind test corpora, our system ranks first and third for the\nLongSumm and LaySumm tasks respectively.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:48:12 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Sayar Ghosh", ""], ["Pinnaparaju", "Nikhil", ""], ["Jain", "Risubh", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03634", "submitter": "Hiroyoshi Komatsu", "authors": "Hiroyoshi Komatsu", "title": "The Logic for a Mildly Context-Sensitive Fragment of the Lambek-Grishin\n  Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While context-free grammars are characterized by a simple proof-theoretic\ngrammatical formalism namely categorial grammar and its logic the Lambek\ncalculus, no such characterizations were known for tree-adjoining grammars, and\neven for any mildly context-sensitive languages classes in the last forty years\ndespite some efforts. We settle this problem in this paper. On the basis of the\nexisting fragment of the Lambek-Grishin calculus which captures tree-adjoining\nlanguages, we present a logic called HLG: a proof-theoretic characterization of\ntree-adjoining languages based on the Lambek-Grishin calculus restricted to\nHyperedge-replacement grammar with rank two studied by Moot. HLG is defined in\ndisplay calculus with cut-admissibility. Several new techniques are introduced\nfor the proofs, such as purely structural connectives, usefulness, and a\ngraph-theoretic argument on proof nets for HLG.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 22:28:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Komatsu", "Hiroyoshi", ""]]}, {"id": "2101.03700", "submitter": "Danqing Zhu", "authors": "Danqing Zhu, Wangli Lin, Yang Zhang, Qiwei Zhong, Guanxiong Zeng,\n  Weilin Wu, Jiayu Tang", "title": "AT-BERT: Adversarial Training BERT for Acronym Identification Winning\n  Solution for SDU@AAAI-21", "comments": "Accepted to SDU @ AAAI 2021, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acronym identification focuses on finding the acronyms and the phrases that\nhave been abbreviated, which is crucial for scientific document understanding\ntasks. However, the limited size of manually annotated datasets hinders further\nimprovement for the problem. Recent breakthroughs of language models\npre-trained on large corpora clearly show that unsupervised pre-training can\nvastly improve the performance of downstream tasks. In this paper, we present\nan Adversarial Training BERT method named AT-BERT, our winning solution to\nacronym identification task for Scientific Document Understanding (SDU)\nChallenge of AAAI 2021. Specifically, the pre-trained BERT is adopted to\ncapture better semantic representation. Then we incorporate the FGM adversarial\ntraining strategy into the fine-tuning of BERT, which makes the model more\nrobust and generalized. Furthermore, an ensemble mechanism is devised to\ninvolve the representations learned from multiple BERT variants. Assembling all\nthese components together, the experimental results on the SciAI dataset show\nthat our proposed approach outperforms all other competitive state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:02:34 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 08:38:45 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhu", "Danqing", ""], ["Lin", "Wangli", ""], ["Zhang", "Yang", ""], ["Zhong", "Qiwei", ""], ["Zeng", "Guanxiong", ""], ["Wu", "Weilin", ""], ["Tang", "Jiayu", ""]]}, {"id": "2101.03717", "submitter": "Thomas Felber", "authors": "Thomas Felber", "title": "Constraint 2021: Machine Learning Models for COVID-19 Fake News\n  Detection Shared Task", "comments": "Constraint 2021, AAAI 21, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this system paper we present our contribution to the Constraint 2021\nCOVID-19 Fake News Detection Shared Task, which poses the challenge of\nclassifying COVID-19 related social media posts as either fake or real. In our\nsystem, we address this challenge by applying classical machine learning\nalgorithms together with several linguistic features, such as n-grams,\nreadability, emotional tone and punctuation. In terms of pre-processing, we\nexperiment with various steps like stop word removal, stemming/lemmatization,\nlink removal and more. We find our best performing system to be based on a\nlinear SVM, which obtains a weighted average F1 score of 95.19% on test data,\nwhich lands a place in the middle of the leaderboard (place 80 of 167).\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:57:32 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 00:06:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Felber", "Thomas", ""]]}, {"id": "2101.03737", "submitter": "Gaole He", "authors": "Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao and Ji-Rong Wen", "title": "Improving Multi-hop Knowledge Base Question Answering by Learning\n  Intermediate Supervision Signals", "comments": "WSDM 2021 camer-ready version. 9 pages, code on\n  https://github.com/RichardHGL/WSDM2021_NSM", "journal-ref": null, "doi": "10.1145/3437963.3441753", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer\nentities that are multiple hops away in the Knowledge Base (KB) from the\nentities in the question. A major challenge is the lack of supervision signals\nat intermediate steps. Therefore, multi-hop KBQA algorithms can only receive\nthe feedback from the final answer, which makes the learning unstable or\nineffective.\n  To address this challenge, we propose a novel teacher-student approach for\nthe multi-hop KBQA task. In our approach, the student network aims to find the\ncorrect answer to the query, while the teacher network tries to learn\nintermediate supervision signals for improving the reasoning capacity of the\nstudent network. The major novelty lies in the design of the teacher network,\nwhere we utilize both forward and backward reasoning to enhance the learning of\nintermediate entity distributions. By considering bidirectional reasoning, the\nteacher network can produce more reliable intermediate supervision signals,\nwhich can alleviate the issue of spurious reasoning. Extensive experiments on\nthree benchmark datasets have demonstrated the effectiveness of our approach on\nthe KBQA task. The code to reproduce our analysis is available at\nhttps://github.com/RichardHGL/WSDM2021_NSM.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 07:49:50 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 09:07:52 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["He", "Gaole", ""], ["Lan", "Yunshi", ""], ["Jiang", "Jing", ""], ["Zhao", "Wayne Xin", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2101.03778", "submitter": "Ekaterina Artemova", "authors": "Alexander Podolskiy and Dmitry Lipin and Andrey Bout and Ekaterina\n  Artemova and Irina Piontkovskaya", "title": "Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain\n  Detection", "comments": "to appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Real-life applications, heavily relying on machine learning, such as dialog\nsystems, demand out-of-domain detection methods. Intent classification models\nshould be equipped with a mechanism to distinguish seen intents from unseen\nones so that the dialog agent is capable of rejecting the latter and avoiding\nundesired behavior. However, despite increasing attention paid to the task, the\nbest practices for out-of-domain intent detection have not yet been fully\nestablished.\n  This paper conducts a thorough comparison of out-of-domain intent detection\nmethods. We prioritize the methods, not requiring access to out-of-domain data\nduring training, gathering of which is extremely time- and labor-consuming due\nto lexical and stylistic variation of user utterances. We evaluate multiple\ncontextual encoders and methods, proven to be efficient, on three standard\ndatasets for intent classification, expanded with out-of-domain utterances. Our\nmain findings show that fine-tuning Transformer-based encoders on in-domain\ndata leads to superior results. Mahalanobis distance, together with utterance\nrepresentations, derived from Transformer-based encoders, outperforms other\nmethods by a wide margin and establishes new state-of-the-art results for all\ndatasets.\n  The broader analysis shows that the reason for success lies in the fact that\nthe fine-tuned Transformer is capable of constructing homogeneous\nrepresentations of in-domain utterances, revealing geometrical disparity to out\nof domain utterances. In turn, the Mahalanobis distance captures this disparity\neasily.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:10:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Podolskiy", "Alexander", ""], ["Lipin", "Dmitry", ""], ["Bout", "Andrey", ""], ["Artemova", "Ekaterina", ""], ["Piontkovskaya", "Irina", ""]]}, {"id": "2101.03841", "submitter": "Yejin Bang", "authors": "Yejin Bang, Etsuko Ishii, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung", "title": "Model Generalization on COVID-19 Fake News Detection", "comments": "CONSTRAINT Workshop 2021 (Camera Ready Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amid the pandemic COVID-19, the world is facing unprecedented infodemic with\nthe proliferation of both fake and real information. Considering the\nproblematic consequences that the COVID-19 fake-news have brought, the\nscientific community has put effort to tackle it. To contribute to this fight\nagainst the infodemic, we aim to achieve a robust model for the COVID-19\nfake-news detection task proposed at CONSTRAINT 2021 (FakeNews-19) by taking\ntwo separate approaches: 1) fine-tuning transformers based language models with\nrobust loss functions and 2) removing harmful training instances through\ninfluence calculation. We further evaluate the robustness of our models by\nevaluating on different COVID-19 misinformation test set (Tweets-19) to\nunderstand model generalization ability. With the first approach, we achieve\n98.13% for weighted F1 score (W-F1) for the shared task, whereas 38.18% W-F1 on\nthe Tweets-19 highest. On the contrary, by performing influence data cleansing,\nour model with 99% cleansing percentage can achieve 54.33% W-F1 score on\nTweets-19 with a trade-off. By evaluating our models on two COVID-19 fake-news\ntest sets, we suggest the importance of model generalization ability in this\ntask to step forward to tackle the COVID-19 fake-news problem in online social\nmedia platforms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:23:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bang", "Yejin", ""], ["Ishii", "Etsuko", ""], ["Cahyawijaya", "Samuel", ""], ["Ji", "Ziwei", ""], ["Fung", "Pascale", ""]]}, {"id": "2101.03916", "submitter": "Sourav Ghosh", "authors": "Sourav Ghosh, Sourabh Vasant Gothe, Chandramouli Sanchi, Barath Raj\n  Kandur Raja", "title": "edATLAS: An Efficient Disambiguation Algorithm for Texting in Languages\n  with Abugida Scripts", "comments": "Published in 2021 IEEE 15th International Conference on Semantic\n  Computing (ICSC)", "journal-ref": "2021 IEEE 15th International Conference on Semantic Computing\n  (ICSC), Laguna Hills, CA, USA, 2021, pp. 325-332", "doi": "10.1109/ICSC50631.2021.00061", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Abugida refers to a phonogram writing system where each syllable is\nrepresented using a single consonant or typographic ligature, along with a\ndefault vowel or optional diacritic(s) to denote other vowels. However, texting\nin these languages has some unique challenges in spite of the advent of devices\nwith soft keyboard supporting custom key layouts. The number of characters in\nthese languages is large enough to require characters to be spread over\nmultiple views in the layout. Having to switch between views many times to type\na single word hinders the natural thought process. This prevents popular usage\nof native keyboard layouts. On the other hand, supporting romanized scripts\n(native words transcribed using Latin characters) with language model based\nsuggestions is also set back by the lack of uniform romanization rules.\n  To this end, we propose a disambiguation algorithm and showcase its\nusefulness in two novel mutually non-exclusive input methods for languages\nnatively using the abugida writing system: (a) disambiguation of ambiguous\ninput for abugida scripts, and (b) disambiguation of word variants in romanized\nscripts. We benchmark these approaches using public datasets, and show an\nimprovement in typing speed by 19.49%, 25.13%, and 14.89%, in Hindi, Bengali,\nand Thai, respectively, using Ambiguous Input, owing to the human ease of\nlocating keys combined with the efficiency of our inference method. Our Word\nVariant Disambiguation (WDA) maps valid variants of romanized words, previously\ntreated as Out-of-Vocab, to a vocabulary of 100k words with high accuracy,\nleading to an increase in Error Correction F1 score by 10.03% and Next Word\nPrediction (NWP) by 62.50% on average.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:16:34 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:07:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ghosh", "Sourav", ""], ["Gothe", "Sourabh Vasant", ""], ["Sanchi", "Chandramouli", ""], ["Raja", "Barath Raj Kandur", ""]]}, {"id": "2101.03963", "submitter": "Sourav Ghosh", "authors": "Sourabh Vasant Gothe, Sourav Ghosh, Sharmila Mani, Guggilla Bhanodai,\n  Ankur Agarwal, Chandramouli Sanchi", "title": "Language Detection Engine for Multilingual Texting on Mobile Devices", "comments": "2020 IEEE 14th International Conference on Semantic Computing (ICSC).\n  Accessible at https://ieeexplore.ieee.org/document/9031474", "journal-ref": "2020 IEEE 14th International Conference on Semantic Computing\n  (ICSC), San Diego, CA, USA, 2020, pp. 279-286", "doi": "10.1109/ICSC.2020.00057", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  More than 2 billion mobile users worldwide type in multiple languages in the\nsoft keyboard. On a monolingual keyboard, 38% of falsely auto-corrected words\nare valid in another language. This can be easily avoided by detecting the\nlanguage of typed words and then validating it in its respective language.\nLanguage detection is a well-known problem in natural language processing. In\nthis paper, we present a fast, light-weight and accurate Language Detection\nEngine (LDE) for multilingual typing that dynamically adapts to user intended\nlanguage in real-time. We propose a novel approach where the fusion of\ncharacter N-gram model and logistic regression based selector model is used to\nidentify the language. Additionally, we present a unique method of reducing the\ninference time significantly by parameter reduction technique. We also discuss\nvarious optimizations fabricated across LDE to resolve ambiguity in input text\namong the languages with the same character pattern. Our method demonstrates an\naverage accuracy of 94.5% for Indian languages in Latin script and that of 98%\nfor European languages on the code-switched data. This model outperforms\nfastText by 60.39% and ML-Kit by 23.67% in F1 score for European languages. LDE\nis faster on mobile device with an average inference time of 25.91\nmicroseconds.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:49:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gothe", "Sourabh Vasant", ""], ["Ghosh", "Sourav", ""], ["Mani", "Sharmila", ""], ["Bhanodai", "Guggilla", ""], ["Agarwal", "Ankur", ""], ["Sanchi", "Chandramouli", ""]]}, {"id": "2101.03967", "submitter": "Sourav Ghosh", "authors": "Sharmila Mani, Sourabh Vasant Gothe, Sourav Ghosh, Ajay Kumar Mishra,\n  Prakhar Kulshreshtha, Bhargavi M, Muthu Kumaran", "title": "Real-Time Optimized N-gram For Mobile Devices", "comments": "2019 IEEE 13th International Conference on Semantic Computing (ICSC).\n  Accessible at https://ieeexplore.ieee.org/document/8665639", "journal-ref": "2019 IEEE 13th International Conference on Semantic Computing\n  (ICSC), Newport Beach, CA, USA, 2019, pp. 87-92", "doi": "10.1109/ICOSC.2019.8665639", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the increasing number of mobile devices, there has been continuous\nresearch on generating optimized Language Models (LMs) for soft keyboard. In\nspite of advances in this domain, building a single LM for low-end feature\nphones as well as high-end smartphones is still a pressing need. Hence, we\npropose a novel technique, Optimized N-gram (Op-Ngram), an end-to-end N-gram\npipeline that utilises mobile resources efficiently for faster Word Completion\n(WC) and Next Word Prediction (NWP). Op-Ngram applies Stupid Backoff and\npruning strategies to generate a light-weight model. The LM loading time on\nmobile is linear with respect to model size. We observed that Op-Ngram gives\n37% improvement in Language Model (LM)-ROM size, 76% in LM-RAM size, 88% in\nloading time and 89% in average suggestion time as compared to SORTED array\nvariant of BerkeleyLM. Moreover, our method shows significant performance\nimprovement over KenLM as well.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:51:26 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mani", "Sharmila", ""], ["Gothe", "Sourabh Vasant", ""], ["Ghosh", "Sourav", ""], ["Mishra", "Ajay Kumar", ""], ["Kulshreshtha", "Prakhar", ""], ["M", "Bhargavi", ""], ["Kumaran", "Muthu", ""]]}, {"id": "2101.03988", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Boshko Koloski, Timen Stepi\\v{s}nik Perdih, Senja Pollak and Bla\\v{z}\n  \\v{S}krlj", "title": "Identification of COVID-19 related Fake News via Neural Stacking", "comments": "Published at CONSTRAIN 2021 (AAAI)", "journal-ref": null, "doi": "10.1007/978-3-030-73696-5_17", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:52:37 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 18:35:03 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Koloski", "Boshko", ""], ["Perdih", "Timen Stepi\u0161nik", ""], ["Pollak", "Senja", ""], ["\u0160krlj", "Bla\u017e", ""]]}, {"id": "2101.04030", "submitter": "Seba Susan", "authors": "Ritam Mallick, Seba Susan, Vaibhaw Agrawal, Rizul Garg, Prateek Rawal", "title": "Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural\n  Machine Translation", "comments": "Accepted in 36th ACM/SIGAPP Symposium On Applied Computing 2021", "journal-ref": null, "doi": "10.1145/3412841.3442099", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural Machine Translation model is a sequence-to-sequence converter based on\nneural networks. Existing models use recurrent neural networks to construct\nboth the encoder and decoder modules. In alternative research, the recurrent\nnetworks were substituted by convolutional neural networks for capturing the\nsyntactic structure in the input sentence and decreasing the processing time.\nWe incorporate the goodness of both approaches by proposing a\nconvolutional-recurrent encoder for capturing the context information as well\nas the sequential information from the source sentence. Word embedding and\nposition embedding of the source sentence is performed prior to the\nconvolutional encoding layer which is basically a n-gram feature extractor\ncapturing phrase-level context information. The rectified output of the\nconvolutional encoding layer is added to the original embedding vector, and the\nsum is normalized by layer normalization. The normalized output is given as a\nsequential input to the recurrent encoding layer that captures the temporal\ninformation in the sequence. For the decoder, we use the attention-based\nrecurrent neural network. Translation task on the German-English dataset\nverifies the efficacy of the proposed approach from the higher BLEU scores\nachieved as compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:03:52 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 07:55:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mallick", "Ritam", ""], ["Susan", "Seba", ""], ["Agrawal", "Vaibhaw", ""], ["Garg", "Rizul", ""], ["Rawal", "Prateek", ""]]}, {"id": "2101.04076", "submitter": "Melanie Laffin", "authors": "Shwetha Bharadwaj, Melanie Laffin", "title": "Automating the Compilation of Potential Core-Outcomes for Clinical\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to increased access to clinical trial outcomes and analysis, researchers\nand scientists are able to iterate or improve upon relevant approaches more\neffectively. However, the metrics and related results of clinical trials\ntypically do not follow any standardization in their reports, making it more\ndifficult for researchers to parse the results of different trials. The\nobjective of this paper is to describe an automated method utilizing natural\nlanguage processing in order to describe the probable core outcomes of clinical\ntrials, in order to alleviate the issues around disparate clinical trial\noutcomes. As the nature of this process is domain specific, BioBERT was\nemployed in order to conduct a multi-class entity normalization task. In\naddition to BioBERT, an unsupervised feature-based approach making use of only\nthe encoder output embedding representations for the outcomes and labels was\nutilized. Finally, cosine similarity was calculated across the vectors to\nobtain the semantic similarity. This method was able to both harness the\ndomain-specific context of each of the tokens from the learned embeddings of\nthe BioBERT model as well as a more stable metric of sentence similarity. Some\ncommon outcomes identified using the Jaccard similarity in each of the\nclassifications were compiled, and while some are untenable, a pipeline for\nwhich this automation process could be conducted was established.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:14:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bharadwaj", "Shwetha", ""], ["Laffin", "Melanie", ""]]}, {"id": "2101.04109", "submitter": "Zijian Zhang", "authors": "Zijian Zhang, Koustav Rudra, Avishek Anand", "title": "Explain and Predict, and then Predict Again", "comments": "Accepted in the WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441758", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A desirable property of learning systems is to be both effective and\ninterpretable. Towards this goal, recent models have been proposed that first\ngenerate an extractive explanation from the input text and then generate a\nprediction on just the explanation called explain-then-predict models. These\nmodels primarily consider the task input as a supervision signal in learning an\nextractive explanation and do not effectively integrate rationales data as an\nadditional inductive bias to improve task performance. We propose a novel yet\nsimple approach ExPred, that uses multi-task learning in the explanation\ngeneration phase effectively trading-off explanation and prediction losses. And\nthen we use another prediction network on just the extracted explanations for\noptimizing the task performance. We conduct an extensive evaluation of our\napproach on three diverse language datasets -- fact verification, sentiment\nclassification, and QA -- and find that we substantially outperform existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:36:52 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 05:19:23 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhang", "Zijian", ""], ["Rudra", "Koustav", ""], ["Anand", "Avishek", ""]]}, {"id": "2101.04144", "submitter": "Raviraj Joshi", "authors": "Ramchandra Joshi, Rushabh Karnavat, Kaustubh Jirapure, Raviraj Joshi", "title": "Evaluation of Deep Learning Models for Hostility Detection in Hindi Text", "comments": "Accepted at IEEE I2CT 2021", "journal-ref": null, "doi": "10.1109/I2CT51068.2021.9418073", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media platform is a convenient medium to express personal thoughts\nand share useful information. It is fast, concise, and has the ability to reach\nmillions. It is an effective place to archive thoughts, share artistic content,\nreceive feedback, promote products, etc. Despite having numerous advantages\nthese platforms have given a boost to hostile posts. Hate speech and derogatory\nremarks are being posted for personal satisfaction or political gain. The\nhostile posts can have a bullying effect rendering the entire platform\nexperience hostile. Therefore detection of hostile posts is important to\nmaintain social media hygiene. The problem is more pronounced languages like\nHindi which are low in resources. In this work, we present approaches for\nhostile text detection in the Hindi language. The proposed approaches are\nevaluated on the Constraint@AAAI 2021 Hindi hostility detection dataset. The\ndataset consists of hostile and non-hostile texts collected from social media\nplatforms. The hostile posts are further segregated into overlapping classes of\nfake, offensive, hate, and defamation. We evaluate a host of deep learning\napproaches based on CNN, LSTM, and BERT for this multi-label classification\nproblem. The pre-trained Hindi fast text word embeddings by IndicNLP and\nFacebook are used in conjunction with CNN and LSTM models. Two variations of\npre-trained multilingual transformer language models mBERT and IndicBERT are\nused. We show that the performance of BERT based models is best. Moreover, CNN\nand LSTM models also perform competitively with BERT based models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:10:57 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 14:25:06 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 16:27:40 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 06:44:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Joshi", "Ramchandra", ""], ["Karnavat", "Rushabh", ""], ["Jirapure", "Kaustubh", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2101.04158", "submitter": "Po-Ting Lai", "authors": "Po-Ting Lai and Zhiyong Lu", "title": "BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph\n  Transformer", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A biomedical relation statement is commonly expressed in multiple sentences\nand consists of many concepts, including gene, disease, chemical, and mutation.\nTo automatically extract information from biomedical literature, existing\nbiomedical text-mining approaches typically formulate the problem as a\ncross-sentence n-ary relation-extraction task that detects relations among n\nentities across multiple sentences, and use either a graph neural network (GNN)\nwith long short-term memory (LSTM) or an attention mechanism. Recently,\nTransformer has been shown to outperform LSTM on many natural language\nprocessing (NLP) tasks. In this work, we propose a novel architecture that\ncombines Bidirectional Encoder Representations from Transformers with Graph\nTransformer (BERT-GT), through integrating a neighbor-attention mechanism into\nthe BERT architecture. Unlike the original Transformer architecture, which\nutilizes the whole sentence(s) to calculate the attention of the current token,\nthe neighbor-attention mechanism in our method calculates its attention\nutilizing only its neighbor tokens. Thus, each token can pay attention to its\nneighbor information with little noise. We show that this is critically\nimportant when the text is very long, as in cross-sentence or abstract-level\nrelation-extraction tasks. Our benchmarking results show improvements of 5.44%\nand 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and\nchemical-protein relation datasets, suggesting BERT-GT is a robust approach\nthat is applicable to other biomedical relation extraction tasks or datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:34:55 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lai", "Po-Ting", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2101.04197", "submitter": "Radu Tudor Ionescu", "authors": "Anca Maria Tache, Mihaela Gaman, Radu Tudor Ionescu", "title": "Clustering Word Embeddings with Self-Organizing Maps. Application on\n  LaRoSeDa -- A Large Romanian Sentiment Data Set", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Romanian is one of the understudied languages in computational linguistics,\nwith few resources available for the development of natural language processing\ntools. In this paper, we introduce LaRoSeDa, a Large Romanian Sentiment Data\nSet, which is composed of 15,000 positive and negative reviews collected from\none of the largest Romanian e-commerce platforms. We employ two sentiment\nclassification methods as baselines for our new data set, one based on\nlow-level features (character n-grams) and one based on high-level features\n(bag-of-word-embeddings generated by clustering word embeddings with k-means).\nAs an additional contribution, we replace the k-means clustering algorithm with\nself-organizing maps (SOMs), obtaining better results because the generated\nclusters of word embeddings are closer to the Zipf's law distribution, which is\nknown to govern natural language. We also demonstrate the generalization\ncapacity of using SOMs for the clustering of word embeddings on another\nrecently-introduced Romanian data set, for text categorization by topic.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 21:19:22 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Tache", "Anca Maria", ""], ["Gaman", "Mihaela", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2101.04200", "submitter": "Maged Eljazzar", "authors": "Ali M. Alagrami, Maged M. Eljazzar", "title": "Smartajweed Automatic Recognition of Arabic Quranic Recitation Rules", "comments": "8 pages, the paper already published in airccse", "journal-ref": "airccse 2020", "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tajweed is a set of rules to read the Quran in a correct Pronunciation of the\nletters with all its Qualities, while Reciting the Quran. which means you have\nto give every letter in the Quran its due of characteristics and apply it to\nthis particular letter in this specific situation while reading, which may\ndiffer in other times. These characteristics include melodic rules, like where\nto stop and for how long, when to merge two letters in pronunciation or when to\nstretch some, or even when to put more strength on some letters over other.\nMost of the papers focus mainly on the main recitation rules and the\npronunciation but not (Ahkam AL Tajweed) which give different rhythm and\ndifferent melody to the pronunciation with every different rule of (Tajweed).\nWhich is also considered very important and essential in Reading the Quran as\nit can give different meanings to the words. In this paper we discuss in detail\nfull system for automatic recognition of Quran Recitation Rules (Tajweed) by\nusing support vector machine and threshold scoring system\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 11:24:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Alagrami", "Ali M.", ""], ["Eljazzar", "Maged M.", ""]]}, {"id": "2101.04226", "submitter": "Arif Usta", "authors": "Arif Usta, Akifhan Karakayali and \\\"Ozg\\\"ur Ulusoy", "title": "DBTagger: Multi-Task Learning for Keyword Mapping in NLIDBs Using\n  Bi-Directional Recurrent Neural Networks", "comments": "To appear in VLDB 2021", "journal-ref": null, "doi": "10.14778/3446095.3446103", "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating Natural Language Queries (NLQs) to Structured Query Language\n(SQL) in interfaces deployed in relational databases is a challenging task,\nwhich has been widely studied in database community recently. Conventional rule\nbased systems utilize series of solutions as a pipeline to deal with each step\nof this task, namely stop word filtering, tokenization, stemming/lemmatization,\nparsing, tagging, and translation. Recent works have mostly focused on the\ntranslation step overlooking the earlier steps by using ad-hoc solutions. In\nthe pipeline, one of the most critical and challenging problems is keyword\nmapping; constructing a mapping between tokens in the query and relational\ndatabase elements (tables, attributes, values, etc.). We define the keyword\nmapping problem as a sequence tagging problem, and propose a novel deep\nlearning based supervised approach that utilizes POS tags of NLQs. Our proposed\napproach, called \\textit{DBTagger} (DataBase Tagger), is an end-to-end and\nschema independent solution, which makes it practical for various relational\ndatabases. We evaluate our approach on eight different datasets, and report new\nstate-of-the-art accuracy results, $92.4\\%$ on the average. Our results also\nindicate that DBTagger is faster than its counterparts up to $10000$ times and\nscalable for bigger databases.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:54:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Usta", "Arif", ""], ["Karakayali", "Akifhan", ""], ["Ulusoy", "\u00d6zg\u00fcr", ""]]}, {"id": "2101.04229", "submitter": "Pavel Kalaidin", "authors": "Evgeny Lagutin and Daniil Gavrilov and Pavel Kalaidin", "title": "Implicit Unlikelihood Training: Improving Neural Text Generation with\n  Reinforcement Learning", "comments": "accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood training and maximization-based decoding result in dull and\nrepetitive generated texts even when using powerful language models (Holtzman\net al., 2019). Adding a loss function for regularization was shown to improve\ntext generation output by helping avoid unwanted properties, such as\ncontradiction or repetition (Li at al., 2020). In this work, we propose\nfine-tuning a language model by using policy gradient reinforcement learning,\ndirectly optimizing for better generation. We apply this approach to minimizing\nrepetition in generated text, and show that, when combined with unlikelihood\ntraining (Welleck et al., 2020), our method further reduces repetition without\nimpacting the language model quality. We also evaluate other methods for\nimproving generation at training and decoding time, and compare them using\nvarious metrics aimed at control for better text generation output.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:10:01 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lagutin", "Evgeny", ""], ["Gavrilov", "Daniil", ""], ["Kalaidin", "Pavel", ""]]}, {"id": "2101.04255", "submitter": "Dominic Widdows", "authors": "Dominic Widdows and Kirsty Kitto and Trevor Cohen", "title": "Quantum Mathematics in Artificial Intelligence", "comments": "Manuscript updated to correct one author's email address, and with\n  some extra references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the decade since 2010, successes in artificial intelligence have been at\nthe forefront of computer science and technology, and vector space models have\nsolidified a position at the forefront of artificial intelligence. At the same\ntime, quantum computers have become much more powerful, and announcements of\nmajor advances are frequently in the news.\n  The mathematical techniques underlying both these areas have more in common\nthan is sometimes realized. Vector spaces took a position at the axiomatic\nheart of quantum mechanics in the 1930s, and this adoption was a key motivation\nfor the derivation of logic and probability from the linear geometry of vector\nspaces. Quantum interactions between particles are modelled using the tensor\nproduct, which is also used to express objects and operations in artificial\nneural networks.\n  This paper describes some of these common mathematical areas, including\nexamples of how they are used in artificial intelligence (AI), particularly in\nautomated reasoning and natural language processing (NLP). Techniques discussed\ninclude vector spaces, scalar products, subspaces and implication, orthogonal\nprojection and negation, dual vectors, density matrices, positive operators,\nand tensor products. Application areas include information retrieval,\ncategorization and implication, modelling word-senses and disambiguation,\ninference in knowledge bases, and semantic composition.\n  Some of these approaches can potentially be implemented on quantum hardware.\nMany of the practical steps in this implementation are in early stages, and\nsome are already realized. Explaining some of the common mathematical tools can\nhelp researchers in both AI and quantum computing further exploit these\noverlaps, recognizing and exploring new directions along the way.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 01:35:56 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 20:58:51 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 17:36:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Widdows", "Dominic", ""], ["Kitto", "Kirsty", ""], ["Cohen", "Trevor", ""]]}, {"id": "2101.04257", "submitter": "Joosung Lee", "authors": "Joosung Lee", "title": "Transforming Multi-Conditioned Generation from Meaning Representation", "comments": "Accepted at RANLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In task-oriented conversation systems, natural language generation systems\nthat generate sentences with specific information related to conversation flow\nare useful. Our study focuses on language generation by considering various\ninformation representing the meaning of utterances as multiple conditions of\ngeneration. NLG from meaning representations, the conditions for sentence\nmeaning, generally goes through two steps: sentence planning and surface\nrealization. However, we propose a simple one-stage framework to generate\nutterances directly from MR (Meaning Representation). Our model is based on\nGPT2 and generates utterances with flat conditions on slot and value pairs,\nwhich does not need to determine the structure of the sentence. We evaluate\nseveral systems in the E2E dataset with 6 automatic metrics. Our system is a\nsimple method, but it demonstrates comparable performance to previous systems\nin automated metrics. In addition, using only 10\\% of the data set without any\nother techniques, our model achieves comparable performance, and shows the\npossibility of performing zero-shot generation and expanding to other datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 01:45:06 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 04:42:34 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lee", "Joosung", ""]]}, {"id": "2101.04355", "submitter": "Ilias Chalkidis", "authors": "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Ion\n  Androutsopoulos", "title": "Neural Contract Element Extraction Revisited: Letters from Sesame Street", "comments": "6 pages", "journal-ref": "updated version of the paper presented at Document Intelligence\n  Workshop (NeurIPS 2019 Workshop)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We investigate contract element extraction. We show that LSTM-based encoders\nperform better than dilated CNNs, Transformers, and BERT in this task. We also\nfind that domain-specific WORD2VEC embeddings outperform generic pre-trained\nGLOVE embeddings. Morpho-syntactic features in the form of POS tag and token\nshape embeddings, as well as context-aware ELMO embeddings do not improve\nperformance. Several of these observations contradict choices or findings of\nprevious work on contract element extraction and generic sequence labeling\ntasks, indicating that contract element extraction requires careful\ntask-specific choices. Analyzing the results of (i) plain TRANSFORMER-based and\n(ii) BERT-based models, we find that in the examined task, where the entities\nare highly context-sensitive, the lack of recurrency in TRANSFORMERs greatly\naffects their performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:02:22 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 13:55:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chalkidis", "Ilias", ""], ["Fergadiotis", "Manos", ""], ["Malakasiotis", "Prodromos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "2101.04356", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "On the Calibration and Uncertainty of Neural Learning to Rank Models", "comments": "Accepted for publication in the 16th conference of the European\n  Chapter of the Association for Computational Linguistics (EACL'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to the Probability Ranking Principle (PRP), ranking documents in\ndecreasing order of their probability of relevance leads to an optimal document\nranking for ad-hoc retrieval. The PRP holds when two conditions are met: [C1]\nthe models are well calibrated, and, [C2] the probabilities of relevance are\nreported with certainty. We know however that deep neural networks (DNNs) are\noften not well calibrated and have several sources of uncertainty, and thus\n[C1] and [C2] might not be satisfied by neural rankers. Given the success of\nneural Learning to Rank (L2R) approaches-and here, especially BERT-based\napproaches-we first analyze under which circumstances deterministic, i.e.\noutputs point estimates, neural rankers are calibrated. Then, motivated by our\nfindings we use two techniques to model the uncertainty of neural rankers\nleading to the proposed stochastic rankers, which output a predictive\ndistribution of relevance as opposed to point estimates. Our experimental\nresults on the ad-hoc retrieval task of conversation response ranking reveal\nthat (i) BERT-based rankers are not robustly calibrated and that stochastic\nBERT-based rankers yield better calibration; and (ii) uncertainty estimation is\nbeneficial for both risk-aware neural ranking, i.e.taking into account the\nuncertainty when ranking documents, and for predicting unanswerable\nconversational contexts.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:05:46 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2101.04406", "submitter": "Dimitris Gkoumas", "authors": "Dimitris Gkoumas, Qiuchi Li, Shahram Dehdashti, Massimo Melucci, Yijun\n  Yu, Dawei Song", "title": "Quantum Cognitively Motivated Decision Fusion for Video Sentiment\n  Analysis", "comments": "The uploaded version is a preprint of the accepted AAAI-21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video sentiment analysis as a decision-making process is inherently complex,\ninvolving the fusion of decisions from multiple modalities and the so-caused\ncognitive biases. Inspired by recent advances in quantum cognition, we show\nthat the sentiment judgment from one modality could be incompatible with the\njudgment from another, i.e., the order matters and they cannot be jointly\nmeasured to produce a final decision. Thus the cognitive process exhibits\n\"quantum-like\" biases that cannot be captured by classical probability\ntheories. Accordingly, we propose a fundamentally new, quantum cognitively\nmotivated fusion strategy for predicting sentiment judgments. In particular, we\nformulate utterances as quantum superposition states of positive and negative\nsentiment judgments, and uni-modal classifiers as mutually incompatible\nobservables, on a complex-valued Hilbert space with positive-operator valued\nmeasures. Experiments on two benchmarking datasets illustrate that our model\nsignificantly outperforms various existing decision level and a range of\nstate-of-the-art content-level fusion approaches. The results also show that\nthe concept of incompatibility allows effective handling of all combination\npatterns, including those extreme cases that are wrongly predicted by all\nuni-modal classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:06:04 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 19:54:05 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Gkoumas", "Dimitris", ""], ["Li", "Qiuchi", ""], ["Dehdashti", "Shahram", ""], ["Melucci", "Massimo", ""], ["Yu", "Yijun", ""], ["Song", "Dawei", ""]]}, {"id": "2101.04456", "submitter": "Sudeep Deepak Shivnikar", "authors": "Sudeep Deepak Shivnikar, Himanshu Arora, Harichandana B S S", "title": "A character representation enhanced on-device Intent Classification", "comments": "Accepted for publication in ICON 2020: 17th International Conference\n  on Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Intent classification is an important task in natural language understanding\nsystems. Existing approaches have achieved perfect scores on the benchmark\ndatasets. However they are not suitable for deployment on low-resource devices\nlike mobiles, tablets, etc. due to their massive model size. Therefore, in this\npaper, we present a novel light-weight architecture for intent classification\nthat can run efficiently on a device. We use character features to enrich the\nword representation. Our experiments prove that our proposed model outperforms\nexisting approaches and achieves state-of-the-art results on benchmark\ndatasets. We also report that our model has tiny memory footprint of ~5 MB and\nlow inference time of ~2 milliseconds, which proves its efficiency in a\nresource-constrained environment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:02:05 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Shivnikar", "Sudeep Deepak", ""], ["Arora", "Himanshu", ""], ["S", "Harichandana B S", ""]]}, {"id": "2101.04547", "submitter": "Damian Pascual", "authors": "Sumu Zhao, Damian Pascual, Gino Brunner, Roger Wattenhofer", "title": "Of Non-Linearity and Commutativity in BERT", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide new insights into the transformer architecture, and\nin particular, its best-known variant, BERT. First, we propose a method to\nmeasure the degree of non-linearity of different elements of transformers.\nNext, we focus our investigation on the feed-forward networks (FFN) inside\ntransformers, which contain 2/3 of the model parameters and have so far not\nreceived much attention. We find that FFNs are an inefficient yet important\narchitectural element and that they cannot simply be replaced by attention\nblocks without a degradation in performance. Moreover, we study the\ninteractions between layers in BERT and show that, while the layers exhibit\nsome hierarchical structure, they extract features in a fuzzy manner. Our\nresults suggest that BERT has an inductive bias towards layer commutativity,\nwhich we find is mainly due to the skip connections. This provides a\njustification for the strong performance of recurrent and weight-shared\ntransformer models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:29:38 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 09:58:42 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 10:23:01 GMT"}, {"version": "v4", "created": "Fri, 7 May 2021 07:38:46 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhao", "Sumu", ""], ["Pascual", "Damian", ""], ["Brunner", "Gino", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2101.04615", "submitter": "Chau-Wai Wong", "authors": "Jiele Wu, Chau-Wai Wong, Xinyan Zhao, Xianpeng Liu", "title": "Toward Effective Automated Content Analysis via Crowdsourcing", "comments": "Corrected minor typos. Camera-ready version for the 2021 IEEE\n  International Conference on Multimedia and Expo (ICME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many computer scientists use the aggregated answers of online workers to\nrepresent ground truth. Prior work has shown that aggregation methods such as\nmajority voting are effective for measuring relatively objective features. For\nsubjective features such as semantic connotation, online workers, known for\noptimizing their hourly earnings, tend to deteriorate in the quality of their\nresponses as they work longer. In this paper, we aim to address this issue by\nproposing a quality-aware semantic data annotation system. We observe that with\ntimely feedback on workers' performance quantified by quality scores, better\ninformed online workers can maintain the quality of their labeling throughout\nan extended period of time. We validate the effectiveness of the proposed\nannotation system through i) evaluating performance based on an expert-labeled\ndataset, and ii) demonstrating machine learning tasks that can lead to\nconsistent learning behavior with 70%-80% accuracy. Our results suggest that\nwith our system, researchers can collect high-quality answers of subjective\nsemantic features at a large scale.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:14:18 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:59:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wu", "Jiele", ""], ["Wong", "Chau-Wai", ""], ["Zhao", "Xinyan", ""], ["Liu", "Xianpeng", ""]]}, {"id": "2101.04617", "submitter": "Zhi Hong", "authors": "Zhi Hong, J. Gregory Pauloski, Logan Ward, Kyle Chard, Ben Blaiszik,\n  and Ian Foster", "title": "AI- and HPC-enabled Lead Generation for SARS-CoV-2: Models and Processes\n  to Extract Druglike Molecules Contained in Natural Language Text", "comments": "17 single-column pages, 6 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers worldwide are seeking to repurpose existing drugs or discover new\ndrugs to counter the disease caused by severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2). A promising source of candidates for such studies\nis molecules that have been reported in the scientific literature to be\ndrug-like in the context of coronavirus research. We report here on a project\nthat leverages both human and artificial intelligence to detect references to\ndrug-like molecules in free text. We engage non-expert humans to create a\ncorpus of labeled text, use this labeled corpus to train a named entity\nrecognition model, and employ the trained model to extract 10912 drug-like\nmolecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of\n198875 papers. Performance analyses show that our automated extraction model\ncan achieve performance on par with that of non-expert humans.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:15:43 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hong", "Zhi", ""], ["Pauloski", "J. Gregory", ""], ["Ward", "Logan", ""], ["Chard", "Kyle", ""], ["Blaiszik", "Ben", ""], ["Foster", "Ian", ""]]}, {"id": "2101.04727", "submitter": "Hossein Rajaby Faghihi", "authors": "Hossein Rajaby Faghihi, Roshanak Mirzaee, Sudarshan Paliwal, and\n  Parisa Kordjamshidi", "title": "Latent Alignment of Procedural Concepts in Multimodal Recipes", "comments": "Published in ALVR 2020, a workshop in ACL 2020", "journal-ref": "Proceedings of the First Workshop on Advances in Language and\n  Vision Research 2020 (26-31)", "doi": "10.18653/v1/2020.alvr-1.5", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel alignment mechanism to deal with procedural reasoning on a\nnewly released multimodal QA dataset, named RecipeQA. Our model is solving the\ntextual cloze task which is a reading comprehension on a recipe containing\nimages and instructions. We exploit the power of attention networks,\ncross-modal representations, and a latent alignment space between instructions\nand candidate answers to solve the problem. We introduce constrained\nmax-pooling which refines the max-pooling operation on the alignment matrix to\nimpose disjoint constraints among the outputs of the model. Our evaluation\nresult indicates a 19\\% improvement over the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:55:53 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Faghihi", "Hossein Rajaby", ""], ["Mirzaee", "Roshanak", ""], ["Paliwal", "Sudarshan", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2101.04758", "submitter": "Muhammad Khalifa", "authors": "Muhammad Khalifa and Muhammad Abdul-Mageed and Khaled Shaalan", "title": "Self-Training Pre-Trained Language Models for Zero- and Few-Shot\n  Multi-Dialectal Arabic Sequence Labeling", "comments": "Accepted at EACL 2021 (Camera Ready Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A sufficient amount of annotated data is usually required to fine-tune\npre-trained language models for downstream tasks. Unfortunately, attaining\nlabeled data can be costly, especially for multiple language varieties and\ndialects. We propose to self-train pre-trained language models in zero- and\nfew-shot scenarios to improve performance on data-scarce varieties using only\nresources from data-rich ones. We demonstrate the utility of our approach in\nthe context of Arabic sequence labeling by using a language model fine-tuned on\nModern Standard Arabic (MSA) only to predict named entities (NE) and\npart-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show\nthat self-training is indeed powerful, improving zero-shot MSA-to-DA transfer\nby as large as \\texttildelow 10\\% F$_1$ (NER) and 2\\% accuracy (POS tagging).\nWe acquire even better performance in few-shot scenarios with limited amounts\nof labeled data. We conduct an ablation study and show that the performance\nboost observed directly results from the unlabeled DA examples used for\nself-training. Our work opens up opportunities for developing DA models\nexploiting only MSA resources and it can be extended to other languages and\ntasks. Our code and fine-tuned models can be accessed at\nhttps://github.com/mohammadKhalifa/zero-shot-arabic-dialects.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:29:30 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 00:00:36 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 20:48:15 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 23:36:04 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Khalifa", "Muhammad", ""], ["Abdul-Mageed", "Muhammad", ""], ["Shaalan", "Khaled", ""]]}, {"id": "2101.04817", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Shuyuan Xu, Bo Liu, Zuohui Fu, Shuchang Liu, Xu Chen,\n  Yongfeng Zhang", "title": "Discrete Knowledge Graph Embedding based on Discrete Optimization", "comments": "Accepted at the AAAI-20 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a discrete knowledge graph (KG) embedding (DKGE) method,\nwhich projects KG entities and relations into the Hamming space based on a\ncomputationally tractable discrete optimization algorithm, to solve the\nformidable storage and computation cost challenges in traditional continuous\ngraph embedding methods. The convergence of DKGE can be guaranteed\ntheoretically. Extensive experiments demonstrate that DKGE achieves superior\naccuracy than classical hashing functions that map the effective continuous\nembeddings into discrete codes. Besides, DKGE reaches comparable accuracy with\nmuch lower computational complexity and storage compared to many continuous\ngraph embedding methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:23:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Li", "Yunqi", ""], ["Xu", "Shuyuan", ""], ["Liu", "Bo", ""], ["Fu", "Zuohui", ""], ["Liu", "Shuchang", ""], ["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.04840", "submitter": "Nazneen Fatema Rajani", "authors": "Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan\n  Zheng, Caiming Xiong, Mohit Bansal, Christopher R\\'e", "title": "Robustness Gym: Unifying the NLP Evaluation Landscape", "comments": "34 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive performance on standard benchmarks, deep neural networks\nare often brittle when deployed in real-world systems. Consequently, recent\nresearch has focused on testing the robustness of such models, resulting in a\ndiverse set of evaluation methodologies ranging from adversarial attacks to\nrule-based data transformations. In this work, we identify challenges with\nevaluating NLP systems and propose a solution in the form of Robustness Gym\n(RG), a simple and extensible evaluation toolkit that unifies 4 standard\nevaluation paradigms: subpopulations, transformations, evaluation sets, and\nadversarial attacks. By providing a common platform for evaluation, Robustness\nGym enables practitioners to compare results from all 4 evaluation paradigms\nwith just a few clicks, and to easily develop and share novel evaluation\nmethods using a built-in set of abstractions. To validate Robustness Gym's\nutility to practitioners, we conducted a real-world case study with a\nsentiment-modeling team, revealing performance degradations of 18%+. To verify\nthat Robustness Gym can aid novel research analyses, we perform the first study\nof state-of-the-art commercial and academic named entity linking (NEL) systems,\nas well as a fine-grained analysis of state-of-the-art summarization models.\nFor NEL, commercial systems struggle to link rare entities and lag their\nacademic counterparts by 10%+, while state-of-the-art summarization models\nstruggle on examples that require abstraction and distillation, degrading by\n9%+. Robustness Gym can be found at https://robustnessgym.com/\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:37:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Goel", "Karan", ""], ["Rajani", "Nazneen", ""], ["Vig", "Jesse", ""], ["Tan", "Samson", ""], ["Wu", "Jason", ""], ["Zheng", "Stephan", ""], ["Xiong", "Caiming", ""], ["Bansal", "Mohit", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2101.04899", "submitter": "Raviraj Joshi", "authors": "Atharva Kulkarni, Meet Mandhane, Manali Likhitkar, Gayatri Kshirsagar,\n  Jayashree Jagdale, Raviraj Joshi", "title": "Experimental Evaluation of Deep Learning models for Marathi Text\n  Classification", "comments": "Accepted at ICMISC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Marathi language is one of the prominent languages used in India. It is\npredominantly spoken by the people of Maharashtra. Over the past decade, the\nusage of language on online platforms has tremendously increased. However,\nresearch on Natural Language Processing (NLP) approaches for Marathi text has\nnot received much attention. Marathi is a morphologically rich language and\nuses a variant of the Devanagari script in the written form. This works aims to\nprovide a comprehensive overview of available resources and models for Marathi\ntext classification. We evaluate CNN, LSTM, ULMFiT, and BERT based models on\ntwo publicly available Marathi text classification datasets and present a\ncomparative analysis. The pre-trained Marathi fast text word embeddings by\nFacebook and IndicNLP are used in conjunction with word-based models. We show\nthat basic single layer models based on CNN and LSTM coupled with FastText\nembeddings perform on par with the BERT based models on the available datasets.\nWe hope our paper aids focused research and experiments in the area of Marathi\nNLP.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:21:27 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 13:08:27 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kulkarni", "Atharva", ""], ["Mandhane", "Meet", ""], ["Likhitkar", "Manali", ""], ["Kshirsagar", "Gayatri", ""], ["Jagdale", "Jayashree", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2101.04921", "submitter": "Segwang Kim", "authors": "Segwang Kim, Hyoungwook Nam, Joonyoung Kim, Kyomin Jung", "title": "Neural Sequence-to-grid Module for Learning Symbolic Rules", "comments": "9 pages, 9 figures, AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logical reasoning tasks over symbols, such as learning arithmetic operations\nand computer program evaluations, have become challenges to deep learning. In\nparticular, even state-of-the-art neural networks fail to achieve\n\\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks,\nwhereas humans can easily extend learned symbolic rules. To resolve this\ndifficulty, we propose a neural sequence-to-grid (seq2grid) module, an input\npreprocessor that automatically segments and aligns an input sequence into a\ngrid. As our module outputs a grid via a novel differentiable mapping, any\nneural network structure taking a grid input, such as ResNet or TextCNN, can be\njointly trained with our module in an end-to-end fashion. Extensive experiments\nshow that neural networks having our module as an input preprocessor achieve\nOOD generalization on various arithmetic and algorithmic problems including\nnumber sequence prediction problems, algebraic word problems, and computer\nprogram evaluation problems while other state-of-the-art sequence transduction\nmodels cannot. Moreover, we verify that our module enhances TextCNN to solve\nthe bAbI QA tasks without external memory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:53:14 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 23:10:25 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kim", "Segwang", ""], ["Nam", "Hyoungwook", ""], ["Kim", "Joonyoung", ""], ["Jung", "Kyomin", ""]]}, {"id": "2101.04922", "submitter": "Mingyu Derek Ma", "authors": "Mingyu Derek Ma, Jiao Sun, Mu Yang, Kung-Hsiang Huang, Nuan Wen,\n  Shikhar Singh, Rujun Han and Nanyun Peng", "title": "EventPlus: A Temporal Event Understanding Pipeline", "comments": "To appear at NAACL 2021 (Demonstrations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present EventPlus, a temporal event understanding pipeline that integrates\nvarious state-of-the-art event understanding components including event trigger\nand type detection, event argument detection, event duration and temporal\nrelation extraction. Event information, especially event temporal knowledge, is\na type of common sense knowledge that helps people understand how stories\nevolve and provides predictive hints for future events. EventPlus as the first\ncomprehensive temporal event understanding pipeline provides a convenient tool\nfor users to quickly obtain annotations about events and their temporal\ninformation for any user-provided document. Furthermore, we show EventPlus can\nbe easily adapted to other domains (e.g., biomedical domain). We make EventPlus\npublicly available to facilitate event-related information extraction and\ndownstream applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:00:50 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 21:33:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ma", "Mingyu Derek", ""], ["Sun", "Jiao", ""], ["Yang", "Mu", ""], ["Huang", "Kung-Hsiang", ""], ["Wen", "Nuan", ""], ["Singh", "Shikhar", ""], ["Han", "Rujun", ""], ["Peng", "Nanyun", ""]]}, {"id": "2101.04965", "submitter": "Mohammad Ahmad", "authors": "Mohammed Azhan, Mohammad Ahmad", "title": "LaDiff ULMFiT: A Layer Differentiated training approach for ULMFiT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our paper, we present Deep Learning models with a layer differentiated\ntraining method which were used for the SHARED TASK@ CONSTRAINT 2021 sub-tasks\nCOVID19 Fake News Detection in English and Hostile Post Detection in Hindi. We\npropose a Layer Differentiated training procedure for training a pre-trained\nULMFiT arXiv:1801.06146 model. We used special tokens to annotate specific\nparts of the tweets to improve language understanding and gain insights on the\nmodel making the tweets more interpretable. The other two submissions included\na modified RoBERTa model and a simple Random Forest Classifier. The proposed\napproach scored a precision and f1 score of 0.96728972 and 0.967324832\nrespectively for sub-task \"COVID19 Fake News Detection in English\". Also,\nCoarse-Grained Hostility f1 Score and Weighted FineGrained f1 score of 0.908648\nand 0.533907 respectively for sub-task Hostile Post Detection in Hindi. The\nproposed approach ranked 61st out of 164 in the sub-task \"COVID19 Fake News\nDetection in English and 18th out of 45 in the sub-task Hostile Post Detection\nin Hindi\".\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:52:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Azhan", "Mohammed", ""], ["Ahmad", "Mohammad", ""]]}, {"id": "2101.04966", "submitter": "Philip John Gorinski", "authors": "Ieva Stali\\=unait\\.e, Philip John Gorinski, Ignacio Iacobacci", "title": "Improving Commonsense Causal Reasoning by Adversarial Training and Data\n  Augmentation", "comments": "7 pages + pages references, 4 figures, 3 tables, paper accepted at\n  AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the plausibility of causal relations between clauses is a\ncommonsense reasoning task that requires complex inference ability. The general\napproach to this task is to train a large pretrained language model on a\nspecific dataset. However, the available training data for the task is often\nscarce, which leads to instability of model training or reliance on the shallow\nfeatures of the dataset. This paper presents a number of techniques for making\nmodels more robust in the domain of causal reasoning. Firstly, we perform\nadversarial training by generating perturbed inputs through synonym\nsubstitution. Secondly, based on a linguistic theory of discourse connectives,\nwe perform data augmentation using a discourse parser for detecting causally\nlinked clauses in large text, and a generative language model for generating\ndistractors. Both methods boost model performance on the Choice of Plausible\nAlternatives (COPA) dataset, as well as on a Balanced COPA dataset, which is a\nmodified version of the original data that has been developed to avoid\nsuperficial cues, leading to a more challenging benchmark. We show a\nstatistically significant improvement in performance and robustness on both\ndatasets, even with only a small number of additionally generated data points.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:55:29 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Stali\u016bnait\u0117", "Ieva", ""], ["Gorinski", "Philip John", ""], ["Iacobacci", "Ignacio", ""]]}, {"id": "2101.04997", "submitter": "Soumya Chatterjee", "authors": "Soumya Chatterjee, Ayush Maheshwari, Ganesh Ramakrishnan, Saketha Nath\n  Jagaralpudi", "title": "Joint Learning of Hyperbolic Label Embeddings for Hierarchical\n  Multi-label Classification", "comments": "10 pages, 2 figures. To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of multi-label classification where the labels lie in\na hierarchy. However, unlike most existing works in hierarchical multi-label\nclassification, we do not assume that the label-hierarchy is known. Encouraged\nby the recent success of hyperbolic embeddings in capturing hierarchical\nrelations, we propose to jointly learn the classifier parameters as well as the\nlabel embeddings. Such a joint learning is expected to provide a twofold\nadvantage: i) the classifier generalizes better as it leverages the prior\nknowledge of existence of a hierarchy over the labels, and ii) in addition to\nthe label co-occurrence information, the label-embedding may benefit from the\nmanifold structure of the input datapoints, leading to embeddings that are more\nfaithful to the label hierarchy. We propose a novel formulation for the joint\nlearning and empirically evaluate its efficacy. The results show that the joint\nlearning improves over the baseline that employs label co-occurrence based\npre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve\nstate-of-the-art generalization on standard benchmarks. We also present\nevaluation of the hyperbolic embeddings obtained by joint learning and show\nthat they represent the hierarchy more accurately than the other alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:58:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chatterjee", "Soumya", ""], ["Maheshwari", "Ayush", ""], ["Ramakrishnan", "Ganesh", ""], ["Jagaralpudi", "Saketha Nath", ""]]}, {"id": "2101.04998", "submitter": "Arkadipta De", "authors": "Arkadipta De, Venkatesh E, Kaushal Kumar Maurya, Maunendra Sankar\n  Desarkar", "title": "Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine\n  Tuned Multilingual Embeddings", "comments": "Accepted at Constrain 2021 Workshop in AAAI 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the wide adoption of social media platforms like Facebook, Twitter,\netc., there is an emerging need of detecting online posts that can go against\nthe community acceptance standards. The hostility detection task has been well\nexplored for resource-rich languages like English, but is unexplored for\nresource-constrained languages like Hindidue to the unavailability of large\nsuitable data. We view this hostility detection as a multi-label multi-class\nclassification problem. We propose an effective neural network-based technique\nfor hostility detection in Hindi posts. We leverage pre-trained multilingual\nBidirectional Encoder Representations of Transformer (mBERT) to obtain the\ncontextual representations of Hindi posts. We have performed extensive\nexperiments including different pre-processing techniques, pre-trained models,\nneural architectures, hybrid strategies, etc. Our best performing neural\nclassifier model includes One-vs-the-Rest approach where we obtained 92.60%,\n81.14%,69.59%, 75.29% and 73.01% F1 scores for hostile, fake, hate, offensive,\nand defamation labels respectively. The proposed model outperformed the\nexisting baseline models and emerged as the state-of-the-art model for\ndetecting hostility in the Hindi posts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:00:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["De", "Arkadipta", ""], ["E", "Venkatesh", ""], ["Maurya", "Kaushal Kumar", ""], ["Desarkar", "Maunendra Sankar", ""]]}, {"id": "2101.05004", "submitter": "Lina Rojas-Barahona", "authors": "Lina M. Rojas-Barahona", "title": "Is the User Enjoying the Conversation? A Case Study on the Impact on the\n  Reward Function", "comments": "Accepted at the Human in the Loop Dialogue Systems, 34st Conference\n  on Neural Information Processing Systems (NeurIPS 2020). Paper updated with\n  minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of user satisfaction in policy learning task-oriented dialogue\nsystems has long been a subject of research interest. Most current models for\nestimating the user satisfaction either (i) treat out-of-context short-texts,\nsuch as product reviews, or (ii) rely on turn features instead of on\ndistributed semantic representations. In this work we adopt deep neural\nnetworks that use distributed semantic representation learning for estimating\nthe user satisfaction in conversations. We evaluate the impact of modelling\ncontext length in these networks. Moreover, we show that the proposed\nhierarchical network outperforms state-of-the-art quality estimators.\nFurthermore, we show that applying these networks to infer the reward function\nin a Partial Observable Markov Decision Process (POMDP) yields to a great\nimprovement in the task success rate.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:13:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Rojas-Barahona", "Lina M.", ""]]}, {"id": "2101.05056", "submitter": "Manav Kaushik", "authors": "Manav Kaushik, Van Tung Pham, Eng Siong Chng", "title": "End-to-End Speaker Height and age estimation using Attention Mechanism\n  with LSTM-RNN", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic height and age estimation of speakers using acoustic features is\nwidely used for the purpose of human-computer interaction, forensics, etc. In\nthis work, we propose a novel approach of using attention mechanism to build an\nend-to-end architecture for height and age estimation. The attention mechanism\nis combined with Long Short-Term Memory(LSTM) encoder which is able to capture\nlong-term dependencies in the input acoustic features. We modify the\nconventionally used Attention -- which calculates context vectors the sum of\nattention only across timeframes -- by introducing a modified context vector\nwhich takes into account total attention across encoder units as well, giving\nus a new cross-attention mechanism. Apart from this, we also investigate a\nmulti-task learning approach for jointly estimating speaker height and age. We\ntrain and test our model on the TIMIT corpus. Our model outperforms several\napproaches in the literature. We achieve a root mean square error (RMSE) of\n6.92cm and6.34cm for male and female heights respectively and RMSE of 7.85years\nand 8.75years for male and females ages respectively. By tracking the attention\nweights allocated to different phones, we find that Vowel phones are most\nimportant whistlestop phones are least important for the estimation task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 13:41:18 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kaushik", "Manav", ""], ["Pham", "Van Tung", ""], ["Chng", "Eng Siong", ""]]}, {"id": "2101.05162", "submitter": "B. Mansurov", "authors": "B. Mansurov and A. Mansurov", "title": "Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration", "comments": "9 pages, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce a data-driven approach to transliterating Uzbek\ndictionary words from the Cyrillic script into the Latin script, and vice\nversa. We heuristically align characters of words in the source script with\nsub-strings of the corresponding words in the target script and train a\ndecision tree classifier that learns these alignments. On the test set, our\nCyrillic to Latin model achieves a character level micro-averaged F1 score of\n0.9992, and our Latin to Cyrillic model achieves the score of 0.9959. Our\ncontribution is a novel method of producing machine transliterated texts for\nthe low-resource Uzbek language.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:59:43 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Mansurov", "B.", ""], ["Mansurov", "A.", ""]]}, {"id": "2101.05208", "submitter": "Dexin Wang", "authors": "Dexin Wang and Deyi Xiong", "title": "Efficient Object-Level Visual Context Modeling for Multimodal Machine\n  Translation: Masking Irrelevant Objects Helps Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual context provides grounding information for multimodal machine\ntranslation (MMT). However, previous MMT models and probing studies on visual\nfeatures suggest that visual information is less explored in MMT as it is often\nredundant to textual information. In this paper, we propose an object-level\nvisual context modeling framework (OVC) to efficiently capture and explore\nvisual information for multimodal machine translation. With detected objects,\nthe proposed OVC encourages MMT to ground translation on desirable visual\nobjects by masking irrelevant objects in the visual modality. We equip the\nproposed with an additional object-masking loss to achieve this goal. The\nobject-masking loss is estimated according to the similarity between masked\nobjects and the source texts so as to encourage masking source-irrelevant\nobjects. Additionally, in order to generate vision-consistent target words, we\nfurther propose a vision-weighted translation loss for OVC. Experiments on MMT\ndatasets demonstrate that the proposed OVC model outperforms state-of-the-art\nMMT models and analyses show that masking irrelevant objects helps grounding in\nMMT.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 11:10:00 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wang", "Dexin", ""], ["Xiong", "Deyi", ""]]}, {"id": "2101.05225", "submitter": "Rohan Alexander", "authors": "Ke-Li Chiu and Rohan Alexander", "title": "On consistency scores in text data with an implementation in R", "comments": "13 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a reproducible cleaning process for the text\nextracted from PDFs using n-gram models. Our approach compares the originally\nextracted text with the text generated from, or expected by, these models using\nearlier text as stimulus. To guide this process, we introduce the notion of a\nconsistency score, which refers to the proportion of text that is expected by\nthe model. This is used to monitor changes during the cleaning process, and\nacross different corpuses. We illustrate our process on text from the book Jane\nEyre and introduce both a Shiny application and an R package to make our\nprocess easier for others to adopt.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:37:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chiu", "Ke-Li", ""], ["Alexander", "Rohan", ""]]}, {"id": "2101.05313", "submitter": "Tobias Bleisch", "authors": "Qiong Hu, Tobias Bleisch, Petko Petkov, Tuomo Raitio, Erik Marchi,\n  Varun Lakshminarasimhan", "title": "Whispered and Lombard Neural Speech Synthesis", "comments": "To appear in SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable for a text-to-speech system to take into account the\nenvironment where synthetic speech is presented, and provide appropriate\ncontext-dependent output to the user. In this paper, we present and compare\nvarious approaches for generating different speaking styles, namely, normal,\nLombard, and whisper speech, using only limited data. The following systems are\nproposed and assessed: 1) Pre-training and fine-tuning a model for each style.\n2) Lombard and whisper speech conversion through a signal processing based\napproach. 3) Multi-style generation using a single model based on a speaker\nverification model. Our mean opinion score and AB preference listening tests\nshow that 1) we can generate high quality speech through the\npre-training/fine-tuning approach for all speaking styles. 2) Although our\nspeaker verification (SV) model is not explicitly trained to discriminate\ndifferent speaking styles, and no Lombard and whisper voice is used for\npre-training this system, the SV model can be used as a style encoder for\ngenerating different style embeddings as input for the Tacotron system. We also\nshow that the resulting synthetic Lombard speech has a significant positive\nimpact on intelligibility gain.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:22:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Hu", "Qiong", ""], ["Bleisch", "Tobias", ""], ["Petkov", "Petko", ""], ["Raitio", "Tuomo", ""], ["Marchi", "Erik", ""], ["Lakshminarasimhan", "Varun", ""]]}, {"id": "2101.05365", "submitter": "Mike Lindow", "authors": "Mike Lindow, David DeFranza, Arul Mishra, Himanshu Mishra", "title": "Scared into Action: How Partisanship and Fear are Associated with\n  Reactions to Public Health Directives", "comments": "54 pages, 11 figures", "journal-ref": null, "doi": "10.31234/osf.io/8me7q", "report-no": null, "categories": "econ.GN cs.CL q-fin.EC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differences in political ideology are increasingly appearing as an impediment\nto successful bipartisan communication from local leadership. For example,\nrecent empirical findings have shown that conservatives are less likely to\nadhere to COVID-19 health directives. This behavior is in direct contradiction\nto past research which indicates that conservatives are more rule abiding,\nprefer to avoid loss, and are more prevention-motivated than liberals. We\nreconcile this disconnect between recent empirical findings and past research\nby using insights gathered from press releases, millions of tweets, and\nmobility data capturing local movement in retail, grocery, workplace, parks,\nand transit domains during COVID-19 shelter-in-place orders. We find that\nconservatives adhere to health directives when they express more fear of the\nvirus. In order to better understand this phenomenon, we analyze both official\nand citizen communications and find that press releases from local and federal\ngovernment, along with the number of confirmed COVID-19 cases, lead to an\nincrease in expressions of fear on Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 17:29:10 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Lindow", "Mike", ""], ["DeFranza", "David", ""], ["Mishra", "Arul", ""], ["Mishra", "Himanshu", ""]]}, {"id": "2101.05400", "submitter": "Manuel Ciosici", "authors": "Manuel R. Ciosici, Joseph Cummings, Mitchell DeHaven, Alex Hedges,\n  Yash Kankanampati, Dong-Ho Lee, Ralph Weischedel, Marjorie Freedman", "title": "Machine-Assisted Script Curation", "comments": "Identical to the NAACL 2021 Demo version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We describe Machine-Aided Script Curator (MASC), a system for human-machine\ncollaborative script authoring. Scripts produced with MASC include (1) English\ndescriptions of sub-events that comprise a larger, complex event; (2) event\ntypes for each of those events; (3) a record of entities expected to\nparticipate in multiple sub-events; and (4) temporal sequencing between the\nsub-events. MASC automates portions of the script creation process with\nsuggestions for event types, links to Wikidata, and sub-events that may have\nbeen forgotten. We illustrate how these automations are useful to the script\nwriter with a few case-study scripts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:19:21 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 16:33:01 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ciosici", "Manuel R.", ""], ["Cummings", "Joseph", ""], ["DeHaven", "Mitchell", ""], ["Hedges", "Alex", ""], ["Kankanampati", "Yash", ""], ["Lee", "Dong-Ho", ""], ["Weischedel", "Ralph", ""], ["Freedman", "Marjorie", ""]]}, {"id": "2101.05405", "submitter": "Huseyin Inan", "authors": "Huseyin A. Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor\n  R\\\"uhle, James Withers, Robert Sim", "title": "Training Data Leakage Analysis in Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural network based language models lead to successful\ndeployments of such models, improving user experience in various applications.\nIt has been demonstrated that strong performance of language models comes along\nwith the ability to memorize rare training samples, which poses serious privacy\nthreats in case the model is trained on confidential user content. In this\nwork, we introduce a methodology that investigates identifying the user content\nin the training data that could be leaked under a strong and realistic threat\nmodel. We propose two metrics to quantify user-level data leakage by measuring\na model's ability to produce unique sentence fragments within training data.\nOur metrics further enable comparing different models trained on the same data\nin terms of privacy. We demonstrate our approach through extensive numerical\nstudies on both RNN and Transformer based models. We further illustrate how the\nproposed metrics can be utilized to investigate the efficacy of mitigations\nlike differentially private training or API hardening.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:57:32 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 23:53:08 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Inan", "Huseyin A.", ""], ["Ramadan", "Osman", ""], ["Wutschitz", "Lukas", ""], ["Jones", "Daniel", ""], ["R\u00fchle", "Victor", ""], ["Withers", "James", ""], ["Sim", "Robert", ""]]}, {"id": "2101.05469", "submitter": "Jason Wei", "authors": "Jason Wei, Chengyu Huang, Shiqi Xu, Soroush Vosoughi", "title": "Text Augmentation in a Multi-Task View", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional data augmentation aims to increase the coverage of the input\ndistribution by generating augmented examples that strongly resemble original\nsamples in an online fashion where augmented examples dominate training. In\nthis paper, we propose an alternative perspective -- a multi-task view (MTV) of\ndata augmentation -- in which the primary task trains on original examples and\nthe auxiliary task trains on augmented examples. In MTV data augmentation, both\noriginal and augmented samples are weighted substantively during training,\nrelaxing the constraint that augmented examples must resemble original data and\nthereby allowing us to apply stronger levels of augmentation. In empirical\nexperiments using four common data augmentation techniques on three benchmark\ntext classification datasets, we find that the MTV leads to higher and more\nrobust performance improvements than traditional augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 05:59:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wei", "Jason", ""], ["Huang", "Chengyu", ""], ["Xu", "Shiqi", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2101.05478", "submitter": "Anvesh Rao Vijjini", "authors": "Akshay Krishna Sheshadri, Anvesh Rao Vijjini, Sukhdeep Kharbanda", "title": "WER-BERT: Automatic WER Estimation with BERT in a Balanced Ordinal\n  Classification Paradigm", "comments": "Accepted Long Paper at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Speech Recognition (ASR) systems are evaluated using Word Error\nRate (WER), which is calculated by comparing the number of errors between the\nground truth and the transcription of the ASR system. This calculation,\nhowever, requires manual transcription of the speech signal to obtain the\nground truth. Since transcribing audio signals is a costly process, Automatic\nWER Evaluation (e-WER) methods have been developed to automatically predict the\nWER of a speech system by only relying on the transcription and the speech\nsignal features. While WER is a continuous variable, previous works have shown\nthat positing e-WER as a classification problem is more effective than\nregression. However, while converting to a classification setting, these\napproaches suffer from heavy class imbalance. In this paper, we propose a new\nbalanced paradigm for e-WER in a classification setting. Within this paradigm,\nwe also propose WER-BERT, a BERT based architecture with speech features for\ne-WER. Furthermore, we introduce a distance loss function to tackle the ordinal\nnature of e-WER classification. The proposed approach and paradigm are\nevaluated on the Librispeech dataset and a commercial (black box) ASR system,\nGoogle Cloud's Speech-to-Text API. The results and experiments demonstrate that\nWER-BERT establishes a new state-of-the-art in automatic WER estimation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:26:28 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 15:18:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sheshadri", "Akshay Krishna", ""], ["Vijjini", "Anvesh Rao", ""], ["Kharbanda", "Sukhdeep", ""]]}, {"id": "2101.05494", "submitter": "Ojasv Kamal", "authors": "Ojasv Kamal, Adarsh Kumar and Tejas Vaidhya", "title": "Hostility Detection in Hindi leveraging Pre-Trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hostile content on social platforms is ever increasing. This has led to the\nneed for proper detection of hostile posts so that appropriate action can be\ntaken to tackle them. Though a lot of work has been done recently in the\nEnglish Language to solve the problem of hostile content online, similar works\nin Indian Languages are quite hard to find. This paper presents a transfer\nlearning based approach to classify social media (i.e Twitter, Facebook, etc.)\nposts in Hindi Devanagari script as Hostile or Non-Hostile. Hostile posts are\nfurther analyzed to determine if they are Hateful, Fake, Defamation, and\nOffensive. This paper harnesses attention based pre-trained models fine-tuned\non Hindi data with Hostile-Non hostile task as Auxiliary and fusing its\nfeatures for further sub-tasks classification. Through this approach, we\nestablish a robust and consistent model without any ensembling or complex\npre-processing. We have presented the results from our approach in\nCONSTRAINT-2021 Shared Task on hostile post detection where our model performs\nextremely well with 3rd runner up in terms of Weighted Fine-Grained F1 Score.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 08:04:32 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kamal", "Ojasv", ""], ["Kumar", "Adarsh", ""], ["Vaidhya", "Tejas", ""]]}, {"id": "2101.05499", "submitter": "Zeyd Boukhers", "authors": "Ipek Baris and Zeyd Boukhers", "title": "ECOL: Early Detection of COVID Lies Using Content, Prior Knowledge and\n  Source Information", "comments": "to be published in Constraint-2021 Workshop @ AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media platforms are vulnerable to fake news dissemination, which\ncauses negative consequences such as panic and wrong medication in the\nhealthcare domain. Therefore, it is important to automatically detect fake news\nin an early stage before they get widely spread. This paper analyzes the impact\nof incorporating content information, prior knowledge, and credibility of\nsources into models for the early detection of fake news. We propose a\nframework modeling those features by using BERT language model and external\nsources, namely Simple English Wikipedia and source reliability tags. The\nconducted experiments on CONSTRAINT datasets demonstrated the benefit of\nintegrating these features for the early detection of fake news in the\nhealthcare domain.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 08:39:50 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Baris", "Ipek", ""], ["Boukhers", "Zeyd", ""]]}, {"id": "2101.05509", "submitter": "Ben Chen", "authors": "Ben Chen, Bin Chen, Dehong Gao, Qijin Chen, Chengfu Huo, Xiaonan Meng,\n  Weijun Ren, Yang Zhou", "title": "Transformer-based Language Model Fine-tuning Methods for COVID-19 Fake\n  News Detection", "comments": "9 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the pandemic of COVID-19, relevant fake news is spreading all over the\nsky throughout the social media. Believing in them without discrimination can\ncause great trouble to people's life. However, universal language models may\nperform weakly in these fake news detection for lack of large-scale annotated\ndata and sufficient semantic understanding of domain-specific knowledge. While\nthe model trained on corresponding corpora is also mediocre for insufficient\nlearning. In this paper, we propose a novel transformer-based language model\nfine-tuning approach for these fake news detection. First, the token vocabulary\nof individual model is expanded for the actual semantics of professional\nphrases. Second, we adapt the heated-up softmax loss to distinguish the\nhard-mining samples, which are common for fake news because of the\ndisambiguation of short text. Then, we involve adversarial training to improve\nthe model's robustness. Last, the predicted features extracted by universal\nlanguage model RoBERTa and domain-specific model CT-BERT are fused by one\nmultiple layer perception to integrate fine-grained and high-level specific\nrepresentations. Quantitative experimental results evaluated on existing\nCOVID-19 fake news dataset show its superior performances compared to the\nstate-of-the-art methods among various evaluation metrics. Furthermore, the\nbest weighted average F1 score achieves 99.02%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:05:42 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 15:53:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Ben", ""], ["Chen", "Bin", ""], ["Gao", "Dehong", ""], ["Chen", "Qijin", ""], ["Huo", "Chengfu", ""], ["Meng", "Xiaonan", ""], ["Ren", "Weijun", ""], ["Zhou", "Yang", ""]]}, {"id": "2101.05525", "submitter": "Adriana Stan PhD", "authors": "Dan Oneata, Alexandru Caranica, Adriana Stan, Horia Cucu", "title": "An evaluation of word-level confidence estimation for end-to-end\n  automatic speech recognition", "comments": "Accepted at SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the confidence (or conversely the uncertainty) of a prediction is\na highly desirable trait of an automatic system, as it improves the robustness\nand usefulness in downstream tasks. In this paper we investigate confidence\nestimation for end-to-end automatic speech recognition (ASR). Previous work has\naddressed confidence measures for lattice-based ASR, while current machine\nlearning research mostly focuses on confidence measures for unstructured deep\nlearning. However, as the ASR systems are increasingly being built upon deep\nend-to-end methods, there is little work that tries to develop confidence\nmeasures in this context. We fill this gap by providing an extensive benchmark\nof popular confidence methods on four well-known speech datasets. There are two\nchallenges we overcome in adapting existing methods: working on structured data\n(sequences) and obtaining confidences at a coarser level than the predictions\n(words instead of tokens). Our results suggest that a strong baseline can be\nobtained by scaling the logits by a learnt temperature, followed by estimating\nthe confidence as the negative entropy of the predictive distribution and,\nfinally, sum pooling to aggregate at word level.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:51:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Oneata", "Dan", ""], ["Caranica", "Alexandru", ""], ["Stan", "Adriana", ""], ["Cucu", "Horia", ""]]}, {"id": "2101.05593", "submitter": "Renato Stoffalette Joao", "authors": "Renato Stoffalette Joao", "title": "On the Temporality of Priors in Entity Linking", "comments": null, "journal-ref": "2020 European Conference on Information Retrieval", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity linking is a fundamental task in natural language processing which\ndeals with the lexical ambiguity in texts. An important component in entity\nlinking approaches is the mention-to-entity prior probability. Even though\nthere is a large number of works in entity linking, the existing approaches do\nnot explicitly consider the time aspect, specifically the temporality of an\nentity's prior probability. We posit that this prior probability is temporal in\nnature and affects the performance of entity linking systems. In this paper we\nsystematically study the effect of the prior on the entity linking performance\nover the temporal validity of both texts and KBs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 13:58:31 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Joao", "Renato Stoffalette", ""]]}, {"id": "2101.05611", "submitter": "Guangneng Hu", "authors": "Guangneng Hu, Qiang Yang", "title": "TrNews: Heterogeneous User-Interest Transfer Learning for News\n  Recommendation", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate how to solve the cross-corpus news recommendation for unseen\nusers in the future. This is a problem where traditional content-based\nrecommendation techniques often fail. Luckily, in real-world recommendation\nservices, some publisher (e.g., Daily news) may have accumulated a large corpus\nwith lots of consumers which can be used for a newly deployed publisher (e.g.,\nPolitical news). To take advantage of the existing corpus, we propose a\ntransfer learning model (dubbed as TrNews) for news recommendation to transfer\nthe knowledge from a source corpus to a target corpus. To tackle the\nheterogeneity of different user interests and of different word distributions\nacross corpora, we design a translator-based transfer-learning strategy to\nlearn a representation mapping between source and target corpora. The learned\ntranslator can be used to generate representations for unseen users in the\nfuture. We show through experiments on real-world datasets that TrNews is\nbetter than various baselines in terms of four metrics. We also show that our\ntranslator is effective among existing transfer strategies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:52:53 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:31:04 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Guangneng", ""], ["Yang", "Qiang", ""]]}, {"id": "2101.05634", "submitter": "Renato Stoffalette Joao", "authors": "Renato Stoffalette Jo\\~ao and Pavlos Fafalios and Stefan Dietze", "title": "Better Together -- An Ensemble Learner for Combining the Results of\n  Ready-made Entity Linking Systems", "comments": "SAC '20: Proceedings of the 35th Annual ACM Symposium on Applied\n  Computing", "journal-ref": null, "doi": "10.1145/3341105.3373883", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity linking (EL) is the task of automatically identifying entity mentions\nin text and resolving them to a corresponding entity in a reference knowledge\nbase like Wikipedia. Throughout the past decade, a plethora of EL systems and\npipelines have become available, where performance of individual systems varies\nheavily across corpora, languages or domains. Linking performance varies even\nbetween different mentions in the same text corpus, where, for instance, some\nEL approaches are better able to deal with short surface forms while others may\nperform better when more context information is available. To this end, we\nargue that performance may be optimised by exploiting results from distinct EL\nsystems on the same corpus, thereby leveraging their individual strengths on a\nper-mention basis. In this paper, we introduce a supervised approach which\nexploits the output of multiple ready-made EL systems by predicting the correct\nlink on a per-mention basis. Experimental results obtained on existing ground\ntruth datasets and exploiting three state-of-the-art EL systems show the\neffectiveness of our approach and its capacity to significantly outperform the\nindividual EL systems as well as a set of baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:42:57 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jo\u00e3o", "Renato Stoffalette", ""], ["Fafalios", "Pavlos", ""], ["Dietze", "Stefan", ""]]}, {"id": "2101.05646", "submitter": "Cengiz Acart\\\"urk", "authors": "Cengiz Acarturk, Melih Sirlanci, Pinar Gurkan Balikcioglu, Deniz\n  Demirci, Nazenin Sahin, Ozge Acar Kucuk", "title": "Malicious Code Detection: Run Trace Output Analysis by LSTM", "comments": "11 pages, 5 figures, 5 tables, accepted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3049200", "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malicious software threats and their detection have been gaining importance\nas a subdomain of information security due to the expansion of ICT applications\nin daily settings. A major challenge in designing and developing anti-malware\nsystems is the coverage of the detection, particularly the development of\ndynamic analysis methods that can detect polymorphic and metamorphic malware\nefficiently. In the present study, we propose a methodological framework for\ndetecting malicious code by analyzing run trace outputs by Long Short-Term\nMemory (LSTM). We developed models of run traces of malicious and benign\nPortable Executable (PE) files. We created our dataset from run trace outputs\nobtained from dynamic analysis of PE files. The obtained dataset was in the\ninstruction format as a sequence and was called Instruction as a Sequence Model\n(ISM). By splitting the first dataset into basic blocks, we obtained the second\none called Basic Block as a Sequence Model (BSM). The experiments showed that\nthe ISM achieved an accuracy of 87.51% and a false positive rate of 18.34%,\nwhile BSM achieved an accuracy of 99.26% and a false positive rate of 2.62%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:00:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Acarturk", "Cengiz", ""], ["Sirlanci", "Melih", ""], ["Balikcioglu", "Pinar Gurkan", ""], ["Demirci", "Deniz", ""], ["Sahin", "Nazenin", ""], ["Kucuk", "Ozge Acar", ""]]}, {"id": "2101.05656", "submitter": "Renato Stoffalette Joao", "authors": "Renato Stoffalette Jo\\~ao", "title": "On Informative Tweet Identification For Tracking Mass Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Twitter has been heavily used as an important channel for communicating and\ndiscussing about events in real-time. In such major events, many uninformative\ntweets are also published rapidly by many users, making it hard to follow the\nevents. In this paper, we address this problem by investigating machine\nlearning methods for automatically identifying informative tweets among those\nthat are relevant to a target event. We examine both traditional approaches\nwith a rich set of handcrafted features and state of the art approaches with\nautomatically learned features. We further propose a hybrid model that\nleverages both the handcrafted features and the automatically learned ones. Our\nexperiments on several large datasets of real-world events show that the latter\napproaches significantly outperform the former and our proposed model performs\nthe best, suggesting highly effective mechanisms for tracking mass events.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:10:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jo\u00e3o", "Renato Stoffalette", ""]]}, {"id": "2101.05667", "submitter": "Jimmy Lin", "authors": "Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin", "title": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained\n  Sequence-to-Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a design pattern for tackling text ranking problems, dubbed\n\"Expando-Mono-Duo\", that has been empirically validated for a number of ad hoc\nretrieval tasks in different domains. At the core, our design relies on\npretrained sequence-to-sequence models within a standard multi-stage ranking\narchitecture. \"Expando\" refers to the use of document expansion techniques to\nenrich keyword representations of texts prior to inverted indexing. \"Mono\" and\n\"Duo\" refer to components in a reranking pipeline based on a pointwise model\nand a pairwise model that rerank initial candidates retrieved using keyword\nsearch. We present experimental results from the MS MARCO passage and document\nranking tasks, the TREC 2020 Deep Learning Track, and the TREC-COVID challenge\nthat validate our design. In all these tasks, we achieve effectiveness that is\nat or near the state of the art, in some cases using a zero-shot approach that\ndoes not exploit any training data from the target task. To support\nreplicability, implementations of our design pattern are open-sourced in the\nPyserini IR toolkit and PyGaggle neural reranking library.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:29:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pradeep", "Ronak", ""], ["Nogueira", "Rodrigo", ""], ["Lin", "Jimmy", ""]]}, {"id": "2101.05701", "submitter": "Elena Shushkevich", "authors": "Elena Shushkevich and John Cardiff", "title": "TUDublin team at Constraint@AAAI2021 -- COVID19 Fake News Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to the participation of the TUDublin team in\nConstraint@AAAI2021 - COVID19 Fake News Detection Challenge. Today, the problem\nof fake news detection is more acute than ever in connection with the pandemic.\nThe number of fake news is increasing rapidly and it is necessary to create AI\ntools that allow us to identify and prevent the spread of false information\nabout COVID-19 urgently. The main goal of the work was to create a model that\nwould carry out a binary classification of messages from social media as real\nor fake news in the context of COVID-19. Our team constructed the ensemble\nconsisting of Bidirectional Long Short Term Memory, Support Vector Machine,\nLogistic Regression, Naive Bayes and a combination of Logistic Regression and\nNaive Bayes. The model allowed us to achieve 0.94 F1-score, which is within 5\\%\nof the best result.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:25:32 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Shushkevich", "Elena", ""], ["Cardiff", "John", ""]]}, {"id": "2101.05716", "submitter": "Gijs Wijnholds", "authors": "Gijs Wijnholds, Michael Moortgat", "title": "SICKNL: A Dataset for Dutch Natural Language Inference", "comments": "To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SICK-NL (read: signal), a dataset targeting Natural Language\nInference in Dutch. SICK-NL is obtained by translating the SICK dataset of\nMarelli et al. (2014)from English into Dutch. Having a parallel inference\ndataset allows us to compare both monolingual and multilingual NLP models for\nEnglish and Dutch on the two tasks. In the paper, we motivate and detail the\ntranslation process, perform a baseline evaluation on both the original SICK\ndataset and its Dutch incarnation SICK-NL, taking inspiration from Dutch\nskipgram embeddings and contextualised embedding models. In addition, we\nencapsulate two phenomena encountered in the translation to formulate stress\ntests and verify how well the Dutch models capture syntactic restructurings\nthat do not affect semantics. Our main finding is all models perform worse on\nSICK-NL than on SICK, indicating that the Dutch dataset is more challenging\nthan the English original. Results on the stress tests show that models don't\nfully capture word order freedom in Dutch, warranting future systematic\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:42:57 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wijnholds", "Gijs", ""], ["Moortgat", "Michael", ""]]}, {"id": "2101.05779", "submitter": "Giovanni Paolini", "authors": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro\n  Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, Stefano\n  Soatto", "title": "Structured Prediction as Translation between Augmented Natural Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework, Translation between Augmented Natural Languages\n(TANL), to solve many structured prediction language tasks including joint\nentity and relation extraction, nested named entity recognition, relation\nclassification, semantic role labeling, event extraction, coreference\nresolution, and dialogue state tracking. Instead of tackling the problem by\ntraining task-specific discriminative classifiers, we frame it as a translation\ntask between augmented natural languages, from which the task-relevant\ninformation can be easily extracted. Our approach can match or outperform\ntask-specific models on all tasks, and in particular, achieves new\nstate-of-the-art results on joint entity and relation extraction (CoNLL04, ADE,\nNYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and\nsemantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while\nusing the same architecture and hyperparameters for all tasks and even when\ntraining a single model to solve all tasks at the same time (multi-task\nlearning). Finally, we show that our framework can also significantly improve\nthe performance in a low-resource regime, thanks to better use of label\nsemantics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:32:21 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 22:08:48 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Paolini", "Giovanni", ""], ["Athiwaratkun", "Ben", ""], ["Krone", "Jason", ""], ["Ma", "Jie", ""], ["Achille", "Alessandro", ""], ["Anubhai", "Rishita", ""], ["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.05783", "submitter": "Abubakar Abid", "authors": "Abubakar Abid, Maheen Farooqi, James Zou", "title": "Persistent Anti-Muslim Bias in Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been observed that large-scale language models capture undesirable\nsocietal biases, e.g. relating to race and gender; yet religious bias has been\nrelatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual\nlanguage model, captures persistent Muslim-violence bias. We probe GPT-3 in\nvarious ways, including prompt completion, analogical reasoning, and story\ngeneration, to understand this anti-Muslim bias, demonstrating that it appears\nconsistently and creatively in different uses of the model and that it is\nsevere even compared to biases about other religious groups. For instance,\n\"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is\nmapped to \"money\" in 5% of test cases. We quantify the positive distraction\nneeded to overcome this bias with adversarial text prompts, and find that use\nof the most positive 6 adjectives reduces violent completions for \"Muslims\"\nfrom 66% to 20%, but which is still higher than for other religious groups.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:41:55 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 17:02:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Abid", "Abubakar", ""], ["Farooqi", "Maheen", ""], ["Zou", "James", ""]]}, {"id": "2101.05786", "submitter": "Peter Gloor", "authors": "Sebastian Duerr, Peter A. Gloor", "title": "Persuasive Natural Language Generation -- A Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This literature review focuses on the use of Natural Language Generation\n(NLG) to automatically detect and generate persuasive texts. Extending previous\nresearch on automatic identification of persuasion in text, we concentrate on\ngenerative aspects through conceptualizing determinants of persuasion in five\nbusiness-focused categories: benevolence, linguistic appropriacy, logical\nargumentation, trustworthiness, tools and datasets. These allow NLG to increase\nan existing message's persuasiveness. Previous research illustrates key aspects\nin each of the above mentioned five categories. A research agenda to further\nstudy persuasive NLG is developed. The review includes analysis of\nseventy-seven articles, outlining the existing body of knowledge and showing\nthe steady progress in this research field.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:44:51 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Duerr", "Sebastian", ""], ["Gloor", "Peter A.", ""]]}, {"id": "2101.05875", "submitter": "Ramya Akula", "authors": "Ramya Akula, Ivan Garibay", "title": "Interpretable Multi-Head Self-Attention model for Sarcasm Detection in\n  social media", "comments": null, "journal-ref": null, "doi": "10.3390/e23040394", "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sarcasm is a linguistic expression often used to communicate the opposite of\nwhat is said, usually something that is very unpleasant with an intention to\ninsult or ridicule. Inherent ambiguity in sarcastic expressions, make sarcasm\ndetection very difficult. In this work, we focus on detecting sarcasm in\ntextual conversations from various social networking platforms and online\nmedia. To this end, we develop an interpretable deep learning model using\nmulti-head self-attention and gated recurrent units. Multi-head self-attention\nmodule aids in identifying crucial sarcastic cue-words from the input, and the\nrecurrent units learn long-range dependencies between these cue-words to better\nclassify the input text. We show the effectiveness of our approach by achieving\nstate-of-the-art results on multiple datasets from social networking platforms\nand online media. Models trained using our proposed approach are easily\ninterpretable and enable identifying sarcastic cues in the input text which\ncontribute to the final classification score. We visualize the learned\nattention weights on few sample input texts to showcase the effectiveness and\ninterpretability of our model.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 21:39:35 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Akula", "Ramya", ""], ["Garibay", "Ivan", ""]]}, {"id": "2101.05938", "submitter": "Jing Jin", "authors": "Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, Zhiliang Gan", "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with\n  Learned Step Size Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, transformer-based language models such as BERT have shown\ntremendous performance improvement for a range of natural language processing\ntasks. However, these language models usually are computation expensive and\nmemory intensive during inference. As a result, it is difficult to deploy them\non resource-restricted devices. To improve the inference performance, as well\nas reduce the model size while maintaining the model accuracy, we propose a\nnovel quantization method named KDLSQ-BERT that combines knowledge distillation\n(KD) with learned step size quantization (LSQ) for language model quantization.\nThe main idea of our method is that the KD technique is leveraged to transfer\nthe knowledge from a \"teacher\" model to a \"student\" model when exploiting LSQ\nto quantize that \"student\" model during the quantization training process.\nExtensive experiment results on GLUE benchmark and SQuAD demonstrate that our\nproposed KDLSQ-BERT not only performs effectively when doing different bit\n(e.g. 2-bit $\\sim$ 8-bit) quantization, but also outperforms the existing BERT\nquantization methods, and even achieves comparable performance as the\nfull-precision base-line model while obtaining 14.9x compression ratio. Our\ncode will be public available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 02:21:28 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Jin", "Jing", ""], ["Liang", "Cai", ""], ["Wu", "Tiancheng", ""], ["Zou", "Liqin", ""], ["Gan", "Zhiliang", ""]]}, {"id": "2101.05953", "submitter": "Sundeep Teki", "authors": "Ayush Gupta, Rohan Sukumaran, Kevin John, Sundeep Teki", "title": "Hostility Detection and Covid-19 Fake News Detection in Social Media", "comments": "13 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Withtheadventofsocialmedia,therehasbeenanextremely rapid increase in the\ncontent shared online. Consequently, the propagation of fake news and hostile\nmessages on social media platforms has also skyrocketed. In this paper, we\naddress the problem of detecting hostile and fake content in the Devanagari\n(Hindi) script as a multi-class, multi-label problem. Using NLP techniques, we\nbuild a model that makes use of an abusive language detector coupled with\nfeatures extracted via Hindi BERT and Hindi FastText models and metadata. Our\nmodel achieves a 0.97 F1 score on coarse grain evaluation on Hostility\ndetection task. Additionally, we built models to identify fake news related to\nCovid-19 in English tweets. We leverage entity information extracted from the\ntweets along with textual representations learned from word embeddings and\nachieve a 0.93 F1 score on the English fake news detection task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:24:36 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Gupta", "Ayush", ""], ["Sukumaran", "Rohan", ""], ["John", "Kevin", ""], ["Teki", "Sundeep", ""]]}, {"id": "2101.05972", "submitter": "Hyunjae Kim", "authors": "Buru Chang, Inggeol Lee, Hyunjae Kim, Jaewoo Kang", "title": "\"Killing Me\" Is Not a Spoiler: Spoiler Detection Model using Graph\n  Neural Networks with Dependency Relation-Aware Attention Mechanism", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning-based spoiler detection models have been proposed\nrecently to protect users from spoilers on review websites. Although dependency\nrelations between context words are important for detecting spoilers, current\nattention-based spoiler detection models are insufficient for utilizing\ndependency relations. To address this problem, we propose a new spoiler\ndetection model called SDGNN that is based on syntax-aware graph neural\nnetworks. In the experiments on two real-world benchmark datasets, we show that\nour SDGNN outperforms the existing spoiler detection models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 05:34:17 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chang", "Buru", ""], ["Lee", "Inggeol", ""], ["Kim", "Hyunjae", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2101.05988", "submitter": "X C", "authors": "Xing Cao, Yun Liu", "title": "Coarse-grained decomposition and fine-grained interaction for multi-hop\n  question answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances regarding question answering and reading comprehension have\nresulted in models that surpass human performance when the answer is contained\nin a single, continuous passage of text, requiring only single-hop reasoning.\nHowever, in actual scenarios, lots of complex queries require multi-hop\nreasoning. The key to the Question Answering task is semantic feature\ninteraction between documents and questions, which is widely processed by\nBi-directional Attention Flow (Bi-DAF), but Bi-DAF generally captures only the\nsurface semantics of words in complex questions and fails to capture implied\nsemantic feature of intermediate answers. As a result, Bi-DAF partially ignores\npart of the contexts related to the question and cannot extract the most\nimportant parts of multiple documents. In this paper we propose a new model\narchitecture for multi-hop question answering, by applying two completion\nstrategies: (1) Coarse-Grain complex question Decomposition (CGDe) strategy are\nintroduced to decompose complex question into simple ones under the condition\nof without any additional annotations (2) Fine-Grained Interaction (FGIn)\nstrategy are introduced to better represent each word in the document and\nextract more comprehensive and accurate sentences related to the inference\npath. The above two strategies are combined and tested on the SQuAD and\nHotpotQA datasets, and the experimental results show that our method\noutperforms state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 06:56:34 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Cao", "Xing", ""], ["Liu", "Yun", ""]]}, {"id": "2101.06004", "submitter": "Chander Shekhar", "authors": "Chander Shekhar, Bhavya Bagla, Kaushal Kumar Maurya, Maunendra Sankar\n  Desarkar", "title": "Walk in Wild: An Ensemble Approach for Hostility Detection in Hindi\n  Posts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the reach of the internet increases, pejorative terms started flooding\nover social media platforms. This leads to the necessity of identifying hostile\ncontent on social media platforms. Identification of hostile contents on\nlow-resource languages like Hindi poses different challenges due to its diverse\nsyntactic structure compared to English. In this paper, we develop a simple\nensemble based model on pre-trained mBERT and popular classification algorithms\nlike Artificial Neural Network (ANN) and XGBoost for hostility detection in\nHindi posts. We formulated this problem as binary classification (hostile and\nnon-hostile class) and multi-label multi-class classification problem (for more\nfine-grained hostile classes). We received third overall rank in the\ncompetition and weighted F1-scores of ~0.969 and ~0.61 on the binary and\nmulti-label multi-class classification tasks respectively.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:49:27 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Shekhar", "Chander", ""], ["Bagla", "Bhavya", ""], ["Maurya", "Kaushal Kumar", ""], ["Desarkar", "Maunendra Sankar", ""]]}, {"id": "2101.06053", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Lea Schumann, Bj\\\"orn Schuller", "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset:\n  Collection, Insights and Improvements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truly real-life data presents a strong, but exciting challenge for sentiment\nand emotion research. The high variety of possible `in-the-wild' properties\nmakes large datasets such as these indispensable with respect to building\nrobust machine learning models. A sufficient quantity of data covering a deep\nvariety in the challenges of each modality to force the exploratory analysis of\nthe interplay of all modalities has not yet been made available in this\ncontext. In this contribution, we present MuSe-CaR, a first of its kind\nmultimodal dataset. The data is publicly available as it recently served as the\ntesting bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on\nthe tasks of emotion, emotion-target engagement, and trustworthiness\nrecognition by means of comprehensively integrating the audio-visual and\nlanguage modalities. Furthermore, we give a thorough overview of the dataset in\nterms of collection and annotation, including annotation tiers not used in this\nyear's MuSe 2020. In addition, for one of the sub-challenges - predicting the\nlevel of trustworthiness - no participant outperformed the baseline model, and\nso we propose a simple, but highly efficient Multi-Head-Attention network that\nexceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 %\nimprovement).\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:40:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Schumann", "Lea", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "2101.06066", "submitter": "Lun Yiu Nie", "authors": "Mudit Chaudhary, Borislav Dzodzo, Sida Huang, Chun Hei Lo, Mingzhi\n  Lyu, Lun Yiu Nie, Jinbo Xing, Tianhua Zhang, Xiaoying Zhang, Jingyan Zhou,\n  Hong Cheng, Wai Lam, Helen Meng", "title": "Unstructured Knowledge Access in Task-oriented Dialog Modeling using\n  Language Inference, Knowledge Retrieval and Knowledge-Integrative Response\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog systems enriched with external knowledge can handle user queries that\nare outside the scope of the supporting databases/APIs. In this paper, we\nfollow the baseline provided in DSTC9 Track 1 and propose three subsystems,\nKDEAK, KnowleDgEFactor, and Ens-GPT, which form the pipeline for a\ntask-oriented dialog system capable of accessing unstructured knowledge.\nSpecifically, KDEAK performs knowledge-seeking turn detection by formulating\nthe problem as natural language inference using knowledge from dialogs,\ndatabases and FAQs. KnowleDgEFactor accomplishes the knowledge selection task\nby formulating a factorized knowledge/document retrieval problem with three\nmodules performing domain, entity and knowledge level analyses. Ens-GPT\ngenerates a response by first processing multiple knowledge snippets, followed\nby an ensemble algorithm that decides if the response should be solely derived\nfrom a GPT2-XL model, or regenerated in combination with the top-ranking\nknowledge snippet. Experimental results demonstrate that the proposed pipeline\nsystem outperforms the baseline and generates high-quality responses, achieving\nat least 58.77% improvement on BLEU-4 score.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:24:32 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chaudhary", "Mudit", ""], ["Dzodzo", "Borislav", ""], ["Huang", "Sida", ""], ["Lo", "Chun Hei", ""], ["Lyu", "Mingzhi", ""], ["Nie", "Lun Yiu", ""], ["Xing", "Jinbo", ""], ["Zhang", "Tianhua", ""], ["Zhang", "Xiaoying", ""], ["Zhou", "Jingyan", ""], ["Cheng", "Hong", ""], ["Lam", "Wai", ""], ["Meng", "Helen", ""]]}, {"id": "2101.06071", "submitter": "Tomohiro Nakamura", "authors": "Tomohiro Nakamura, Tomoya Miyashita, Soh Ohara", "title": "Hierarchical Multitask Learning with Dependency Parsing for Japanese\n  Semantic Role Labeling Improves Performance of Argument Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of FrameNet and PropBank, many semantic role labeling (SRL)\nsystems have been proposed in English. Although research on Japanese predicate\nargument structure analysis (PASA) has been conducted, most studies focused on\nsurface cases. There are only few previous works on Japanese SRL for deep\ncases, and their models' accuracies are low. Therefore, we propose a\nhierarchical multitask learning method with dependency parsing (DP) and show\nthat our model achieves state-of-the-art results in Japanese SRL. Also, we\nconduct experiments with a joint model that performs both argument\nidentification and argument classification simultaneously. The result suggests\nthat multitasking with DP is mainly effective for argument identification.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:41:20 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 11:58:31 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Nakamura", "Tomohiro", ""], ["Miyashita", "Tomoya", ""], ["Ohara", "Soh", ""]]}, {"id": "2101.06111", "submitter": "Andreas L Opdahl", "authors": "Andreas L Opdahl", "title": "Knowledge Graphs and Natural-Language Processing", "comments": "In Big Data in Emergency Management: Exploitation Techniques for\n  Social and Mobile Data (pp. 75-91). Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-48099-8", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emergency-relevant data comes in many varieties. It can be high volume and\nhigh velocity, and reaction times are critical, calling for efficient and\npowerful techniques for data analysis and management. Knowledge graphs\nrepresent data in a rich, flexible, and uniform way that is well matched with\nthe needs of emergency management. They build on existing standards, resources,\ntechniques, and tools for semantic data and computing. This chapter explains\nthe most important semantic technologies and how they support knowledge graphs.\nWe proceed to discuss their benefits and challenges and give examples of\nrelevant semantic data sources and vocabularies. Natural-language texts -- in\nparticular those collected from social media such as Twitter -- is a type of\ndata source that poses particular analysis challenges. We therefore include an\noverview of techniques for processing natural-language texts.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:53:28 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Opdahl", "Andreas L", ""]]}, {"id": "2101.06125", "submitter": "Ana Guerberof-Arenas Dr", "authors": "Ana Guerberof Arenas and Antonio Toral", "title": "The Impact of Post-editing and Machine Translation on Creativity and\n  Reading Experience", "comments": "28 pages, 10 tables, 4 figures. Translation Spaces (2020)", "journal-ref": null, "doi": "10.1075/ts.20035.gue", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article presents the results of a study involving the translation of a\nfictional story from English into Catalan in three modalities:\nmachine-translated (MT), post-edited (MTPE) and translated without aid (HT).\nEach translation was analysed to evaluate its creativity. Subsequently, a\ncohort of 88 Catalan participants read the story in a randomly assigned\nmodality and completed a survey. The results show that HT presented a higher\ncreativity score if compared to MTPE and MT. HT also ranked higher in narrative\nengagement, and translation reception, while MTPE ranked marginally higher in\nenjoyment. HT and MTPE show no statistically significant differences in any\ncategory, whereas MT does in all variables tested. We conclude that creativity\nis highest when professional translators intervene in the process, especially\nwhen working without any aid. We hypothesize that creativity in translation\ncould be the factor that enhances reading engagement and the reception of\ntranslated literary texts.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:11:11 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Arenas", "Ana Guerberof", ""], ["Toral", "Antonio", ""]]}, {"id": "2101.06150", "submitter": "Mathieu Roche", "authors": "Sarah Valentin, Elena Arsevska, Aline Vilain, Val\\'erie De Waele,\n  Renaud Lancelot, Mathieu Roche", "title": "Annotation of epidemiological information in animal disease-related news\n  articles: guidelines", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a method for annotation of epidemiological information\nin animal disease-related news articles. The annotation guidelines are generic\nand aim to embrace all animal or zoonotic infectious diseases, regardless of\nthe pathogen involved or its way of transmission (e.g. vector-borne, airborne,\nby contact). The framework relies on the successive annotation of all the\nsentences from a news article. The annotator evaluates the sentences in a\nspecific epidemiological context, corresponding to the publication of the news\narticle.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:48:01 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Valentin", "Sarah", ""], ["Arsevska", "Elena", ""], ["Vilain", "Aline", ""], ["De Waele", "Val\u00e9rie", ""], ["Lancelot", "Renaud", ""], ["Roche", "Mathieu", ""]]}, {"id": "2101.06172", "submitter": "Yevgeniy Puzikov", "authors": "Yevgeniy Puzikov, Simoes Stanley, Iryna Gurevych and Immanuel\n  Schweizer", "title": "Empirical Evaluation of Supervision Signals for Style Transfer Models", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Text style transfer has gained increasing attention from the research\ncommunity over the recent years. However, the proposed approaches vary in many\nways, which makes it hard to assess the individual contribution of the model\ncomponents. In style transfer, the most important component is the optimization\ntechnique used to guide the learning in the absence of parallel training data.\nIn this work we empirically compare the dominant optimization paradigms which\nprovide supervision signals during training: backtranslation, adversarial\ntraining and reinforcement learning. We find that backtranslation has\nmodel-specific limitations, which inhibits training style transfer models.\nReinforcement learning shows the best performance gains, while adversarial\ntraining, despite its popularity, does not offer an advantage over the latter\nalternative. In this work we also experiment with Minimum Risk Training, a\npopular technique in the machine translation community, which, to our\nknowledge, has not been empirically evaluated in the task of style transfer. We\nfill this research gap and empirically show its efficacy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:33:30 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Puzikov", "Yevgeniy", ""], ["Stanley", "Simoes", ""], ["Gurevych", "Iryna", ""], ["Schweizer", "Immanuel", ""]]}, {"id": "2101.06323", "submitter": "Jason Zhu", "authors": "Jason Yue Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus\n  Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, Huasha Zhao", "title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored\n  Search", "comments": "Jason Yue Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus\n  Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. 2021.\n  TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search.\n  In Proceedings of the Web Conference 2021 (WWW 21), April 19-23, 2021,\n  Ljubljana, Slovenia. ACM, New York, NY, USA, 10 pages. https:\n  //doi.org/10.1145/3442381.3449842", "journal-ref": null, "doi": "10.1145/3442381.3449842", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text encoders based on C-DSSM or transformers have demonstrated strong\nperformance in many Natural Language Processing (NLP) tasks. Low latency\nvariants of these models have also been developed in recent years in order to\napply them in the field of sponsored search which has strict computational\nconstraints. However these models are not the panacea to solve all the Natural\nLanguage Understanding (NLU) challenges as the pure semantic information in the\ndata is not sufficient to fully identify the user intents. We propose the\nTextGNN model that naturally extends the strong twin tower structured encoders\nwith the complementary graph information from user historical behaviors, which\nserves as a natural guide to help us better understand the intents and hence\ngenerate better language representations. The model inherits all the benefits\nof twin tower models such as C-DSSM and TwinBERT so that it can still be used\nin the low latency environment while achieving a significant performance gain\nthan the strong encoder-only counterpart baseline models in both offline\nevaluations and online production system. In offline experiments, the model\nachieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for\nlong-tail low-frequency Ads, and in the online A/B testing, the model shows a\n2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:12:47 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 17:29:32 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 04:23:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhu", "Jason Yue", ""], ["Cui", "Yanling", ""], ["Liu", "Yuming", ""], ["Sun", "Hao", ""], ["Li", "Xue", ""], ["Pelger", "Markus", ""], ["Yang", "Tianqi", ""], ["Zhang", "Liangjie", ""], ["Zhang", "Ruofei", ""], ["Zhao", "Huasha", ""]]}, {"id": "2101.06326", "submitter": "Alex John Quijano", "authors": "Alex John Quijano, Sam Nguyen, and Juanita Ordonez", "title": "Grid Search Hyperparameter Benchmarking of BERT, ALBERT, and LongFormer\n  on DuoRC", "comments": "9 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "LLNL-TR-817729", "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this project is to evaluate three language models named BERT,\nALBERT, and LongFormer on the Question Answering dataset called DuoRC. The\nlanguage model task has two inputs, a question, and a context. The context is a\nparagraph or an entire document while the output is the answer based on the\ncontext. The goal is to perform grid search hyperparameter fine-tuning using\nDuoRC. Pretrained weights of the models are taken from the Huggingface library.\nDifferent sets of hyperparameters are used to fine-tune the models using two\nversions of DuoRC which are the SelfRC and the ParaphraseRC. The results show\nthat the ALBERT (pretrained using the SQuAD1 dataset) has an F1 score of 76.4\nand an accuracy score of 68.52 after fine-tuning on the SelfRC dataset. The\nLongformer model (pretrained using the SQuAD and SelfRC datasets) has an F1\nscore of 52.58 and an accuracy score of 46.60 after fine-tuning on the\nParaphraseRC dataset. The current results outperformed the results from the\nprevious model by DuoRC.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:28:32 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:44:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Quijano", "Alex John", ""], ["Nguyen", "Sam", ""], ["Ordonez", "Juanita", ""]]}, {"id": "2101.06351", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Diyi Yang", "title": "Weakly-Supervised Hierarchical Models for Predicting Persuasive\n  Strategies in Good-faith Textual Requests", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling persuasive language has the potential to better facilitate our\ndecision-making processes. Despite its importance, computational modeling of\npersuasion is still in its infancy, largely due to the lack of benchmark\ndatasets that can provide quantitative labels of persuasive strategies to\nexpedite this line of research. To this end, we introduce a large-scale\nmulti-domain text corpus for modeling persuasive strategies in good-faith text\nrequests. Moreover, we design a hierarchical weakly-supervised latent variable\nmodel that can leverage partially labeled data to predict such associated\npersuasive strategies for each sentence, where the supervision comes from both\nthe overall document-level labels and very limited sentence-level labels.\nExperimental results showed that our proposed method outperformed existing\nsemi-supervised baselines significantly. We have publicly released our code at\nhttps://github.com/GT-SALT/Persuasion_Strategy_WVAE.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 02:31:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Jiaao", ""], ["Yang", "Diyi", ""]]}, {"id": "2101.06353", "submitter": "Shoffan Saifullah", "authors": "Shoffan Saifullah, Yuli Fauziah, Agus Sasmito Aribowo", "title": "Comparison of Machine Learning for Sentiment Analysis in Detecting\n  Anxiety Based on Social Media Data", "comments": null, "journal-ref": "Jurnal Informatika,15(1), 2021, 45-55", "doi": "10.26555/jifo.v15i1.a20111", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  All groups of people felt the impact of the COVID-19 pandemic. This situation\ntriggers anxiety, which is bad for everyone. The government's role is very\ninfluential in solving these problems with its work program. It also has many\npros and cons that cause public anxiety. For that, it is necessary to detect\nanxiety to improve government programs that can increase public expectations.\nThis study applies machine learning to detecting anxiety based on social media\ncomments regarding government programs to deal with this pandemic. This concept\nwill adopt a sentiment analysis in detecting anxiety based on positive and\nnegative comments from netizens. The machine learning methods implemented\ninclude K-NN, Bernoulli, Decision Tree Classifier, Support Vector Classifier,\nRandom Forest, and XG-boost. The data sample used is the result of crawling\nYouTube comments. The data used amounted to 4862 comments consisting of\nnegative and positive data with 3211 and 1651. Negative data identify anxiety,\nwhile positive data identifies hope (not anxious). Machine learning is\nprocessed based on feature extraction of count-vectorization and TF-IDF. The\nresults showed that the sentiment data amounted to 3889 and 973 in testing, and\ntraining with the greatest accuracy was the random forest with feature\nextraction of vectorization count and TF-IDF of 84.99% and 82.63%,\nrespectively. The best precision test is K-NN, while the best recall is\nXG-Boost. Thus, Random Forest is the best accurate to detect someone's anxiety\nbased-on data from social media.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 02:47:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Saifullah", "Shoffan", ""], ["Fauziah", "Yuli", ""], ["Aribowo", "Agus Sasmito", ""]]}, {"id": "2101.06368", "submitter": "Ian Stewart", "authors": "Ian Stewart, Diyi Yang, Jacob Eisenstein", "title": "Tuiteamos o pongamos un tuit? Investigating the Social Constraints of\n  Loanword Integration in Spanish Social Media", "comments": null, "journal-ref": "Society for Computation in Linguistics, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speakers of non-English languages often adopt loanwords from English to\nexpress new or unusual concepts. While these loanwords may be borrowed\nunchanged, speakers may also integrate the words to fit the constraints of\ntheir native language, e.g. creating Spanish \"tuitear\" from English \"tweet.\"\nLinguists have often considered the process of loanword integration to be more\ndependent on language-internal constraints, but sociolinguistic constraints\nsuch as speaker background remain only qualitatively understood. We investigate\nthe role of social context and speaker background in Spanish speakers' use of\nintegrated loanwords on social media. We find first that newspaper authors use\nthe integrated forms of loanwords and native words more often than social media\nauthors, showing that integration is associated with formal domains. In social\nmedia, we find that speaker background and expectations of formality explain\nloanword and native word integration, such that authors who use more Spanish\nand who write to a wider audience tend to use integrated verb forms more often.\nThis study shows that loanword integration reflects not only language-internal\nconstraints but also social expectations that vary by conversation and speaker.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 04:42:44 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Stewart", "Ian", ""], ["Yang", "Diyi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "2101.06397", "submitter": "Sufeng Duan", "authors": "Sufeng Duan, Hai Zhao, Rui Wang", "title": "To Understand Representation of Layer-aware Sequence Encoders as\n  Multi-order-graph", "comments": "arXiv admin note: text overlap with arXiv:2009.07489", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified explanation of representation for\nlayer-aware neural sequence encoders, which regards the representation as a\nrevisited multigraph called multi-order-graph (MoG), so that model encoding can\nbe viewed as a processing to capture all subgraphs in MoG. The relationship\nreflected by Multi-order-graph, called $n$-order dependency, can present what\nexisting simple directed graph explanation cannot present. Our proposed MoG\nexplanation allows to precisely observe every step of the generation of\nrepresentation, put diverse relationship such as syntax into a unifiedly\ndepicted framework. Based on the proposed MoG explanation, we further propose a\ngraph-based self-attention network empowered Graph-Transformer by enhancing the\nability of capturing subgraph information over the current models.\nGraph-Transformer accommodates different subgraphs into different groups, which\nallows model to focus on salient subgraphs. Result of experiments on neural\nmachine translation tasks show that the MoG-inspired model can yield effective\nperformance improvement.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:12:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Duan", "Sufeng", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""]]}, {"id": "2101.06399", "submitter": "Zixu Wang", "authors": "Zixu Wang, Yishu Miao, Lucia Specia", "title": "Latent Variable Models for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional models for Visual Question Answering (VQA) explore deterministic\napproaches with various types of image features, question features, and\nattention mechanisms. However, there exist other modalities that can be\nexplored in addition to image and question pairs to bring extra information to\nthe models. In this work, we propose latent variable models for VQA where extra\ninformation (e.g. captions and answer categories) are incorporated as latent\nvariables to improve inference, which in turn benefits question-answering\nperformance. Experiments on the VQA v2.0 benchmarking dataset demonstrate the\neffectiveness of our proposed models in that they improve over strong\nbaselines, especially those that do not rely on extensive language-vision\npre-training.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:21:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Zixu", ""], ["Miao", "Yishu", ""], ["Specia", "Lucia", ""]]}, {"id": "2101.06400", "submitter": "Bingning Wang Dr.", "authors": "Bingning Wang, Ting Yao, Weipeng Chen, Jingfang Xu and Xiaochuan Wang", "title": "ComQA:Compositional Question Answering via Hierarchical Graph Neural\n  Networks", "comments": "Accepted by WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of deep learning techniques and large scale datasets,\nthe question answering (QA) systems have been quickly improved, providing more\naccurate and satisfying answers. However, current QA systems either focus on\nthe sentence-level answer, i.e., answer selection, or phrase-level answer,\ni.e., machine reading comprehension. How to produce compositional answers has\nnot been throughout investigated. In compositional question answering, the\nsystems should assemble several supporting evidence from the document to\ngenerate the final answer, which is more difficult than sentence-level or\nphrase-level QA. In this paper, we present a large-scale compositional question\nanswering dataset containing more than 120k human-labeled questions. The answer\nin this dataset is composed of discontiguous sentences in the corresponding\ndocument. To tackle the ComQA problem, we proposed a hierarchical graph neural\nnetworks, which represents the document from the low-level word to the\nhigh-level sentence. We also devise a question selection and node selection\ntask for pre-training. Our proposed model achieves a significant improvement\nover previous machine reading comprehension methods and pre-training methods.\nCodes and dataset can be found at \\url{https://github.com/benywon/ComQA}.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:23:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Bingning", ""], ["Yao", "Ting", ""], ["Chen", "Weipeng", ""], ["Xu", "Jingfang", ""], ["Wang", "Xiaochuan", ""]]}, {"id": "2101.06423", "submitter": "Liang Pang", "authors": "Liang Pang, Yanyan Lan, Xueqi Cheng", "title": "Match-Ignition: Plugging PageRank into Transformer for Long-form Text\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models cannot\nwell address the long-form text matching problem. That is because there are\nusually many noises in the setting of long-form text matching, and it is\ndifficult for existing semantic text matching to capture the key matching\nsignals from this noisy information. Besides, these models are computationally\nexpensive because they simply use all textual data indiscriminately in the\nmatching process. To tackle the effectiveness and efficiency problem, we\npropose a novel hierarchical noise filtering model in this paper, namely\nMatch-Ignition. The basic idea is to plug the well-known PageRank algorithm\ninto the Transformer, to identify and filter both sentence and word level noisy\ninformation in the matching process. Noisy sentences are usually easy to detect\nbecause the sentence is the basic unit of a long-form text, so we directly use\nPageRank to filter such information, based on a sentence similarity graph.\nWhile words need to rely on their contexts to express concrete meanings, so we\npropose to jointly learn the filtering process and the matching process, to\nreflect the contextual dependencies between words. Specifically, a word graph\nis first built based on the attention scores in each self-attention block of\nTransformer, and keywords are then selected by applying PageRank on this graph.\nIn this way, noisy words will be filtered out layer by layer in the matching\nprocess. Experimental results show that Match-Ignition outperforms both\ntraditional text matching models for short text and recent long-form text\nmatching models. We also conduct detailed analysis to show that Match-Ignition\ncan efficiently capture important sentences or words, which are helpful for\nlong-form text matching.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 10:34:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2101.06514", "submitter": "A.B. Siddique", "authors": "A.B. Siddique, Fuad Jamour, Vagelis Hristidis", "title": "Linguistically-Enriched and Context-Aware Zero-shot Slot Filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Slot filling is identifying contiguous spans of words in an utterance that\ncorrespond to certain parameters (i.e., slots) of a user request/query. Slot\nfilling is one of the most important challenges in modern task-oriented dialog\nsystems. Supervised learning approaches have proven effective at tackling this\nchallenge, but they need a significant amount of labeled training data in a\ngiven domain. However, new domains (i.e., unseen in training) may emerge after\ndeployment. Thus, it is imperative that these models seamlessly adapt and fill\nslots from both seen and unseen domains -- unseen domains contain unseen slot\ntypes with no training data, and even seen slots in unseen domains are\ntypically presented in different contexts. This setting is commonly referred to\nas zero-shot slot filling. Little work has focused on this setting, with\nlimited experimental evaluation. Existing models that mainly rely on\ncontext-independent embedding-based similarity measures fail to detect slot\nvalues in unseen domains or do so only partially. We propose a new zero-shot\nslot filling neural model, LEONA, which works in three steps. Step one acquires\ndomain-oblivious, context-aware representations of the utterance word by\nexploiting (a) linguistic features; (b) named entity recognition cues; (c)\ncontextual embeddings from pre-trained language models. Step two fine-tunes\nthese rich representations and produces slot-independent tags for each word.\nStep three exploits generalizable context-aware utterance-slot similarity\nfeatures at the word level, uses slot-independent tags, and contextualizes them\nto produce slot-specific predictions for each word. Our thorough evaluation on\nfour diverse public datasets demonstrates that our approach consistently\noutperforms the SOTA models by 17.52%, 22.15%, 17.42%, and 17.95% on average\nfor unseen domains on SNIPS, ATIS, MultiWOZ, and SGD datasets, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 20:18:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Siddique", "A. B.", ""], ["Jamour", "Fuad", ""], ["Hristidis", "Vagelis", ""]]}, {"id": "2101.06561", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie,\n  Jungo Kasai, Yejin Choi, Noah A. Smith, Daniel S. Weld", "title": "GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leaderboards have eased model development for many NLP datasets by\nstandardizing their evaluation and delegating it to an independent external\nrepository. Their adoption, however, is so far limited to tasks that can be\nreliably evaluated in an automatic manner. This work introduces GENIE, an\nextensible human evaluation leaderboard, which brings the ease of leaderboards\nto text generation tasks. GENIE automatically posts leaderboard submissions to\ncrowdsourcing platforms asking human annotators to evaluate them on various\naxes (e.g., correctness, conciseness, fluency) and compares their answers to\nvarious automatic metrics. We introduce several datasets in English to GENIE,\nrepresenting four core challenges in text generation: machine translation,\nsummarization, commonsense reasoning, and machine comprehension. We provide\nformal granular evaluation metrics and identify areas for future research. We\nmake GENIE publicly available and hope that it will spur progress in language\ngeneration models as well as their automatic and manual evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:40:47 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 19:26:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Khashabi", "Daniel", ""], ["Stanovsky", "Gabriel", ""], ["Bragg", "Jonathan", ""], ["Lourie", "Nicholas", ""], ["Kasai", "Jungo", ""], ["Choi", "Yejin", ""], ["Smith", "Noah A.", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2101.06644", "submitter": "Theophile Sautory", "authors": "Theophile Sautory, Nuri Cingillioglu, Alessandra Russo", "title": "HySTER: A Hybrid Spatio-Temporal Event Reasoner", "comments": "Preprint accepted by the 35th AAAI Conference on Artificial\n  Intelligence (AAAI-21) Workshop on Hybrid Artificial Intelligence (HAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Video Question Answering (VideoQA) consists in answering natural\nlanguage questions about a video and serves as a proxy to evaluate the\nperformance of a model in scene sequence understanding. Most methods designed\nfor VideoQA up-to-date are end-to-end deep learning architectures which\nstruggle at complex temporal and causal reasoning and provide limited\ntransparency in reasoning steps. We present the HySTER: a Hybrid\nSpatio-Temporal Event Reasoner to reason over physical events in videos. Our\nmodel leverages the strength of deep learning methods to extract information\nfrom video frames with the reasoning capabilities and explainability of\nsymbolic artificial intelligence in an answer set programming framework. We\ndefine a method based on general temporal, causal and physics rules which can\nbe transferred across tasks. We apply our model to the CLEVRER dataset and\ndemonstrate state-of-the-art results in question answering accuracy. This work\nsets the foundations for the incorporation of inductive logic programming in\nthe field of VideoQA.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:07:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sautory", "Theophile", ""], ["Cingillioglu", "Nuri", ""], ["Russo", "Alessandra", ""]]}, {"id": "2101.06699", "submitter": "Cheng Yi", "authors": "Cheng Yi, Shiyu Zhou, Bo Xu", "title": "Efficiently Fusing Pretrained Acoustic and Linguistic Encoders for\n  Low-resource Speech Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3071668", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models have achieved impressive results on the task of automatic\nspeech recognition (ASR). For low-resource ASR tasks, however, labeled data can\nhardly satisfy the demand of end-to-end models. Self-supervised acoustic\npre-training has already shown its amazing ASR performance, while the\ntranscription is still inadequate for language modeling in end-to-end models.\nIn this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a\npre-trained linguistic encoder (BERT) into an end-to-end ASR model. The fused\nmodel only needs to learn the transfer from speech to language during\nfine-tuning on limited labeled data. The length of the two modalities is\nmatched by a monotonic attention mechanism without additional parameters.\nBesides, a fully connected layer is introduced for the hidden mapping between\nmodalities. We further propose a scheduled fine-tuning strategy to preserve and\nutilize the text context modeling ability of the pre-trained linguistic\nencoder. Experiments show our effective utilizing of pre-trained modules. Our\nmodel achieves better recognition performance on CALLHOME corpus (15 hours)\nthan other end-to-end models.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 16:12:44 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 13:27:57 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Yi", "Cheng", ""], ["Zhou", "Shiyu", ""], ["Xu", "Bo", ""]]}, {"id": "2101.06761", "submitter": "Peng Gao", "authors": "Peng Gao, Fei Shao, Xiaoyuan Liu, Xusheng Xiao, Haoyuan Liu, Zheng\n  Qin, Fengyuan Xu, Prateek Mittal, Sanjeev R. Kulkarni, Dawn Song", "title": "A System for Efficiently Hunting for Cyber Threats in Computer Systems\n  Using Threat Intelligence", "comments": "Accepted paper at ICDE 2021 demonstrations track. arXiv admin note:\n  substantial text overlap with arXiv:2010.13637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-based cyber threat hunting has emerged as an important solution to\ncounter sophisticated cyber attacks. However, existing approaches require\nnon-trivial efforts of manual query construction and have overlooked the rich\nexternal knowledge about threat behaviors provided by open-source Cyber Threat\nIntelligence (OSCTI). To bridge the gap, we build ThreatRaptor, a system that\nfacilitates cyber threat hunting in computer systems using OSCTI. Built upon\nmature system auditing frameworks, ThreatRaptor provides (1) an unsupervised,\nlight-weight, and accurate NLP pipeline that extracts structured threat\nbehaviors from unstructured OSCTI text, (2) a concise and expressive\ndomain-specific query language, TBQL, to hunt for malicious system activities,\n(3) a query synthesis mechanism that automatically synthesizes a TBQL query\nfrom the extracted threat behaviors, and (4) an efficient query execution\nengine to search the big system audit logging data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:44:09 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 06:39:48 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gao", "Peng", ""], ["Shao", "Fei", ""], ["Liu", "Xiaoyuan", ""], ["Xiao", "Xusheng", ""], ["Liu", "Haoyuan", ""], ["Qin", "Zheng", ""], ["Xu", "Fengyuan", ""], ["Mittal", "Prateek", ""], ["Kulkarni", "Sanjeev R.", ""], ["Song", "Dawn", ""]]}, {"id": "2101.06779", "submitter": "Saket Dingliwal", "authors": "Saket Dingliwal, Bill Gao, Sanchit Agarwal, Chien-Wei Lin, Tagyoung\n  Chung, Dilek Hakkani-Tur", "title": "Few Shot Dialogue State Tracking using Meta-learning", "comments": "To appear in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dialogue State Tracking (DST) forms a core component of automated chatbot\nbased systems designed for specific goals like hotel, taxi reservation, tourist\ninformation, etc. With the increasing need to deploy such systems in new\ndomains, solving the problem of zero/few-shot DST has become necessary. There\nhas been a rising trend for learning to transfer knowledge from resource-rich\ndomains to unknown domains with minimal need for additional data. In this work,\nwe explore the merits of meta-learning algorithms for this transfer and hence,\npropose a meta-learner D-REPTILE specific to the DST problem. With extensive\nexperimentation, we provide clear evidence of benefits over conventional\napproaches across different domains, methods, base models, and datasets with\nsignificant (5-25%) improvement over the baseline in a low-data setting. Our\nproposed meta-learner is agnostic of the underlying model and hence any\nexisting state-of-the-art DST system can improve its performance on unknown\ndomains using our training strategy.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:47:06 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 21:08:04 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 04:13:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dingliwal", "Saket", ""], ["Gao", "Bill", ""], ["Agarwal", "Sanchit", ""], ["Lin", "Chien-Wei", ""], ["Chung", "Tagyoung", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2101.06803", "submitter": "Nikos Papasarantopoulos", "authors": "Nikos Papasarantopoulos, Shay B. Cohen", "title": "Narration Generation for Cartoon Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on text generation from multimodal inputs has largely focused on\nstatic images, and less on video data. In this paper, we propose a new task,\nnarration generation, that is complementing videos with narration texts that\nare to be interjected in several places. The narrations are part of the video\nand contribute to the storyline unfolding in it. Moreover, they are\ncontext-informed, since they include information appropriate for the timeframe\nof video they cover, and also, do not need to include every detail shown in\ninput scenes, as a caption would. We collect a new dataset from the animated\ntelevision series Peppa Pig. Furthermore, we formalize the task of narration\ngeneration as including two separate tasks, timing and content generation, and\npresent a set of models on the new task.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:23:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Papasarantopoulos", "Nikos", ""], ["Cohen", "Shay B.", ""]]}, {"id": "2101.06804", "submitter": "Jiachang Liu", "authors": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin,\n  Weizhu Chen", "title": "What Makes Good In-Context Examples for GPT-$3$?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPT-$3$ has attracted lots of attention due to its superior performance\nacross a wide range of NLP tasks, especially with its powerful and versatile\nin-context few-shot learning ability. Despite its success, we found that the\nempirical results of GPT-$3$ depend heavily on the choice of in-context\nexamples. In this work, we investigate whether there are more effective\nstrategies for judiciously selecting in-context examples (relative to random\nsampling) that better leverage GPT-$3$'s few-shot capabilities. Inspired by the\nrecent success of leveraging a retrieval module to augment large-scale neural\nnetwork models, we propose to retrieve examples that are semantically-similar\nto a test sample to formulate its corresponding prompt. Intuitively, the\nin-context examples selected with such a strategy may serve as more informative\ninputs to unleash GPT-$3$'s extensive knowledge. We evaluate the proposed\napproach on several natural language understanding and generation benchmarks,\nwhere the retrieval-based prompt selection approach consistently outperforms\nthe random baseline. Moreover, it is observed that the sentence encoders\nfine-tuned on task-related datasets yield even more helpful retrieval results.\nNotably, significant gains are observed on tasks such as table-to-text\ngeneration (41.9% on the ToTTo dataset) and open-domain question answering\n(45.5% on the NQ dataset). We hope our investigation could help understand the\nbehaviors of GPT-$3$ and large-scale pre-trained LMs in general and enhance\ntheir few-shot capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:38:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Jiachang", ""], ["Shen", "Dinghan", ""], ["Zhang", "Yizhe", ""], ["Dolan", "Bill", ""], ["Carin", "Lawrence", ""], ["Chen", "Weizhu", ""]]}, {"id": "2101.06829", "submitter": "Tianxing He", "authors": "Tianxing He, Bryan McCann, Caiming Xiong, Ehsan Hosseini-Asl", "title": "Joint Energy-based Model Training for Better Calibrated Natural Language\n  Understanding Models", "comments": null, "journal-ref": "EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore joint energy-based model (EBM) training during the\nfinetuning of pretrained text encoders (e.g., Roberta) for natural language\nunderstanding (NLU) tasks. Our experiments show that EBM training can help the\nmodel reach a better calibration that is competitive to strong baselines, with\nlittle or no loss in accuracy. We discuss three variants of energy functions\n(namely scalar, hidden, and sharp-hidden) that can be defined on top of a text\nencoder, and compare them in experiments. Due to the discreteness of text data,\nwe adopt noise contrastive estimation (NCE) to train the energy-based model. To\nmake NCE training more effective, we train an auto-regressive noise model with\nthe masked language model (MLM) objective.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 01:41:31 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:36:31 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["He", "Tianxing", ""], ["McCann", "Bryan", ""], ["Xiong", "Caiming", ""], ["Hosseini-Asl", "Ehsan", ""]]}, {"id": "2101.06880", "submitter": "Qintong Li", "authors": "Qintong Li, Piji Li, Xinyi Li, Zhaochun Ren, Zhumin Chen, Maarten de\n  Rijke", "title": "Abstractive Opinion Tagging", "comments": "Accepted by WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441804", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, opinion tags refer to a ranked list of tags provided by the\ne-commerce platform that reflect characteristics of reviews of an item. To\nassist consumers to quickly grasp a large number of reviews about an item,\nopinion tags are increasingly being applied by e-commerce platforms. Current\nmechanisms for generating opinion tags rely on either manual labelling or\nheuristic methods, which is time-consuming and ineffective. In this paper, we\npropose the abstractive opinion tagging task, where systems have to\nautomatically generate a ranked list of opinion tags that are based on, but\nneed not occur in, a given set of user-generated reviews.\n  The abstractive opinion tagging task comes with three main challenges: (1)\nthe noisy nature of reviews; (2) the formal nature of opinion tags vs. the\ncolloquial language usage in reviews; and (3) the need to distinguish between\ndifferent items with very similar aspects. To address these challenges, we\npropose an abstractive opinion tagging framework, named AOT-Net, to generate a\nranked list of opinion tags given a large number of reviews. First, a\nsentence-level salience estimation component estimates each review's salience\nscore. Next, a review clustering and ranking component ranks reviews in two\nsteps: first, reviews are grouped into clusters and ranked by cluster size;\nthen, reviews within each cluster are ranked by their distance to the cluster\ncenter. Finally, given the ranked reviews, a rank-aware opinion tagging\ncomponent incorporates an alignment feature and alignment loss to generate a\nranked list of opinion tags. To facilitate the study of this task, we create\nand release a large-scale dataset, called eComTag, crawled from real-world\ne-commerce websites. Extensive experiments conducted on the eComTag dataset\nverify the effectiveness of the proposed AOT-Net in terms of various evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:08:15 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 11:39:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Qintong", ""], ["Li", "Piji", ""], ["Li", "Xinyi", ""], ["Ren", "Zhaochun", ""], ["Chen", "Zhumin", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2101.06887", "submitter": "Dmitry Krotov", "authors": "Yuchen Liang, Chaitanya K. Ryali, Benjamin Hoover, Leopold Grinberg,\n  Saket Navlakha, Mohammed J. Zaki, Dmitry Krotov", "title": "Can a Fruit Fly Learn Word Embeddings?", "comments": "Accepted for publication at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mushroom body of the fruit fly brain is one of the best studied systems\nin neuroscience. At its core it consists of a population of Kenyon cells, which\nreceive inputs from multiple sensory modalities. These cells are inhibited by\nthe anterior paired lateral neuron, thus creating a sparse high dimensional\nrepresentation of the inputs. In this work we study a mathematical\nformalization of this network motif and apply it to learning the correlational\nstructure between words and their context in a corpus of unstructured text, a\ncommon natural language processing (NLP) task. We show that this network can\nlearn semantic representations of words and can generate both static and\ncontext-dependent word embeddings. Unlike conventional methods (e.g., BERT,\nGloVe) that use dense representations for word embedding, our algorithm encodes\nsemantic meaning of words and their context in the form of sparse binary hash\ncodes. The quality of the learned representations is evaluated on word\nsimilarity analysis, word-sense disambiguation, and document classification. It\nis shown that not only can the fruit fly network motif achieve performance\ncomparable to existing methods in NLP, but, additionally, it uses only a\nfraction of the computational resources (shorter training time and smaller\nmemory footprint).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:41:50 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 19:50:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liang", "Yuchen", ""], ["Ryali", "Chaitanya K.", ""], ["Hoover", "Benjamin", ""], ["Grinberg", "Leopold", ""], ["Navlakha", "Saket", ""], ["Zaki", "Mohammed J.", ""], ["Krotov", "Dmitry", ""]]}, {"id": "2101.06938", "submitter": "Yongqi Li", "authors": "Yongqi Li, Wenjie Li, Liqiang Nie", "title": "Incremental Knowledge Based Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past years, Knowledge-Based Question Answering (KBQA), which aims to\nanswer natural language questions using facts in a knowledge base, has been\nwell developed. Existing approaches often assume a static knowledge base.\nHowever, the knowledge is evolving over time in the real world. If we directly\napply a fine-tuning strategy on an evolving knowledge base, it will suffer from\na serious catastrophic forgetting problem. In this paper, we propose a new\nincremental KBQA learning framework that can progressively expand learning\ncapacity as humans do. Specifically, it comprises a margin-distilled loss and a\ncollaborative exemplar selection method, to overcome the catastrophic\nforgetting problem by taking advantage of knowledge distillation. We reorganize\nthe SimpleQuestion dataset to evaluate the proposed incremental learning\nsolution to KBQA. The comprehensive experiments demonstrate its effectiveness\nand efficiency when working with the evolving knowledge base.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:03:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Yongqi", ""], ["Li", "Wenjie", ""], ["Nie", "Liqiang", ""]]}, {"id": "2101.06949", "submitter": "Harsh Patel", "authors": "Harsh Patel", "title": "HinFlair: pre-trained contextual string embeddings for pos tagging and\n  text classification in the Hindi language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:23:35 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Patel", "Harsh", ""]]}, {"id": "2101.06969", "submitter": "Zhengyan Zhang", "authors": "Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi,\n  Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun", "title": "Red Alarm for Pre-trained Models: Universal Vulnerability to\n  Neuron-Level Backdoor Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained models (PTMs) have been widely used in various downstream tasks.\nThe parameters of PTMs are distributed on the Internet and may suffer backdoor\nattacks. In this work, we demonstrate the universal vulnerability of PTMs,\nwhere fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary\ndownstream tasks. Specifically, attackers can add a simple pre-training task,\nwhich restricts the output representations of trigger instances to pre-defined\nvectors, namely neuron-level backdoor attack (NeuBA). If the backdoor\nfunctionality is not eliminated during fine-tuning, the triggers can make the\nfine-tuned model predict fixed labels by pre-defined vectors. In the\nexperiments of both natural language processing (NLP) and computer vision (CV),\nwe show that NeuBA absolutely controls the predictions for trigger instances\nwithout any knowledge of downstream tasks. Finally, we apply several defense\nmethods to NeuBA and find that model pruning is a promising direction to resist\nNeuBA by excluding backdoored neurons. Our findings sound a red alarm for the\nwide use of PTMs. Our source code and models are available at\n\\url{https://github.com/thunlp/NeuBA}.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:18:42 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 05:23:52 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 08:30:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Zhengyan", ""], ["Xiao", "Guangxuan", ""], ["Li", "Yongwei", ""], ["Lv", "Tian", ""], ["Qi", "Fanchao", ""], ["Liu", "Zhiyuan", ""], ["Wang", "Yasheng", ""], ["Jiang", "Xin", ""], ["Sun", "Maosong", ""]]}, {"id": "2101.06980", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Aldo Lipani, Sophia Althammer, Markus\n  Zlabinger, Allan Hanbury", "title": "Mitigating the Position Bias of Transformer Models in Passage Re-Ranking", "comments": "Accepted at ECIR 2021 (Full paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning models and their evaluation strongly depends on\nthe quality of the underlying dataset. When we search for a relevant piece of\ninformation it may appear anywhere in a given passage. However, we observe a\nbias in the position of the correct answer in the text in two popular Question\nAnswering datasets used for passage re-ranking. The excessive favoring of\nearlier positions inside passages is an unwanted artefact. This leads to three\ncommon Transformer-based re-ranking models to ignore relevant parts in unseen\npassages. More concerningly, as the evaluation set is taken from the same\nbiased distribution, the models overfitting to that bias overestimate their\ntrue effectiveness. In this work we analyze position bias on datasets, the\ncontextualized representations, and their effect on retrieval results. We\npropose a debiasing method for retrieval datasets. Our results show that a\nmodel trained on a position-biased dataset exhibits a significant decrease in\nre-ranking effectiveness when evaluated on a debiased dataset. We demonstrate\nthat by mitigating the position bias, Transformer-based re-ranking models are\nequally effective on a biased and debiased dataset, as well as more effective\nin a transfer-learning setting between two differently biased datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:38:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Lipani", "Aldo", ""], ["Althammer", "Sophia", ""], ["Zlabinger", "Markus", ""], ["Hanbury", "Allan", ""]]}, {"id": "2101.06983", "submitter": "Luyu Gao", "authors": "Luyu Gao, Yunyi Zhang, Jiawei Han, Jamie Callan", "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup", "comments": "RepL4NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has been applied successfully to learn vector\nrepresentations of text. Previous research demonstrated that learning\nhigh-quality representations benefits from batch-wise contrastive loss with a\nlarge number of negatives. In practice, the technique of in-batch negative is\nused, where for each example in a batch, other batch examples' positives will\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\nstill conditions each example's loss on all batch examples and requires fitting\nthe entire large batch into GPU memory. This paper introduces a gradient\ncaching technique that decouples backpropagation between contrastive loss and\nthe encoder, removing encoder backward pass data dependency along the batch\ndimension. As a result, gradients can be computed for one subset of the batch\nat a time, leading to almost constant memory usage.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:42:34 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 16:37:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gao", "Luyu", ""], ["Zhang", "Yunyi", ""], ["Han", "Jiawei", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.07086", "submitter": "Guy Rotman", "authors": "Guy Rotman, Amir Feder and Roi Reichart", "title": "Model Compression for Domain Adaptation through Causal Effect Estimation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent improvements in the predictive quality of natural language processing\nsystems are often dependent on a substantial increase in the number of model\nparameters. This has led to various attempts of compressing such models, but\nexisting methods have not considered the differences in the predictive power of\nvarious model components or in the generalizability of the compressed models.\nTo understand the connection between model compression and out-of-distribution\ngeneralization, we define the task of compressing language representation\nmodels such that they perform best in a domain adaptation setting. We choose to\naddress this problem from a causal perspective, attempting to estimate the\n\\textit{average treatment effect} (ATE) of a model component, such as a single\nlayer, on the model's predictions. Our proposed ATE-guided Model Compression\nscheme (AMoC), generates many model candidates, differing by the model\ncomponents that were removed. Then, we select the best candidate through a\nstepwise regression model that utilizes the ATE to predict the expected\nperformance on the target domain. AMoC outperforms strong baselines on 46 of 60\ndomain pairs across two text classification tasks, with an average improvement\nof more than 3\\% in F1 above the strongest baseline.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 14:18:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rotman", "Guy", ""], ["Feder", "Amir", ""], ["Reichart", "Roi", ""]]}, {"id": "2101.07120", "submitter": "Aravindh Gowtham Bommisetty", "authors": "Mohan Bharath B, Aravindh Gowtham B, Akhil M", "title": "Neural Abstractive Text Summarizer for Telugu Language", "comments": "11 pages, 2 figures. Presented the paper at Third International\n  Conference on Soft Computing and Signal Processing (ICSCSP 2020) and is\n  currently in production. It will soon be published in springer Advances in\n  Intelligent Systems and Computing (AISC) series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstractive Text Summarization is the process of constructing semantically\nrelevant shorter sentences which captures the essence of the overall meaning of\nthe source text. It is actually difficult and very time consuming for humans to\nsummarize manually large documents of text. Much of work in abstractive text\nsummarization is being done in English and almost no significant work has been\nreported in Telugu abstractive text summarization. So, we would like to propose\nan abstractive text summarization approach for Telugu language using Deep\nlearning. In this paper we are proposing an abstractive text summarization Deep\nlearning model for Telugu language. The proposed architecture is based on\nencoder-decoder sequential models with attention mechanism. We have applied\nthis model on manually created dataset to generate a one sentence summary of\nthe source text and have got good results measured qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:22:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["B", "Mohan Bharath", ""], ["B", "Aravindh Gowtham", ""], ["M", "Akhil", ""]]}, {"id": "2101.07138", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou", "title": "Teach me how to Label: Labeling Functions from Natural Language with\n  Text-to-text Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated data has become the most important bottleneck in training accurate\nmachine learning models, especially for areas that require domain expertise. A\nrecent approach to deal with the above issue proposes using natural language\nexplanations instead of labeling individual data points, thereby increasing\nhuman annotators' efficiency as well as decreasing costs substantially. This\npaper focuses on the task of turning these natural language descriptions into\nPython labeling functions by following a novel approach to semantic parsing\nwith pre-trained text-to-text Transformers. In a series of experiments our\napproach achieves a new state of the art on the semantic parsing benchmark\nCoNaLa, surpassing the previous best approach by 3.7 BLEU points. Furthermore,\non a manually constructed dataset of natural language descriptions-labeling\nfunctions pairs we achieve a BLEU of 0.39. Our approach can be regarded as a\nstepping stone towards models that are taught how to label in natural language,\ninstead of being provided specific labeled samples. Our code, constructed\ndataset and models are available at\nhttps://github.com/ypapanik/t5-for-code-generation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:04:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Papanikolaou", "Yannis", ""]]}, {"id": "2101.07140", "submitter": "Pradyumna Tambwekar", "authors": "Pradyumna Tambwekar, Andrew Silva, Nakul Gopalan, Matthew Gombolay", "title": "Interpretable Policy Specification and Synthesis through Natural\n  Language and RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy specification is a process by which a human can initialize a robot's\nbehaviour and, in turn, warm-start policy optimization via Reinforcement\nLearning (RL). While policy specification/design is inherently a collaborative\nprocess, modern methods based on Learning from Demonstration or Deep RL lack\nthe model interpretability and accessibility to be classified as such. Current\nstate-of-the-art methods for policy specification rely on black-box models,\nwhich are an insufficient means of collaboration for non-expert users: These\nmodels provide no means of inspecting policies learnt by the agent and are not\nfocused on creating a usable modality for teaching robot behaviour. In this\npaper, we propose a novel machine learning framework that enables humans to 1)\nspecify, through natural language, interpretable policies in the form of\neasy-to-understand decision trees, 2) leverage these policies to warm-start\nreinforcement learning and 3) outperform baselines that lack our natural\nlanguage initialization mechanism. We train our approach by collecting a\nfirst-of-its-kind corpus mapping free-form natural language policy descriptions\nto decision tree-based policies. We show that our novel framework translates\nnatural language to decision trees with a 96% and 97% accuracy on a held-out\ncorpus across two domains, respectively. Finally, we validate that policies\ninitialized with natural language commands are able to significantly outperform\nrelevant baselines (p < 0.001) that do not benefit from our natural\nlanguage-based warm-start technique.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:07:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tambwekar", "Pradyumna", ""], ["Silva", "Andrew", ""], ["Gopalan", "Nakul", ""], ["Gombolay", "Matthew", ""]]}, {"id": "2101.07339", "submitter": "Joshua Kim", "authors": "Joshua Y. Kim, Greyson Y. Kim, Chunfeng Liu, Rafael A. Calvo, Silas\n  C.R. Taylor, Kalina Yacef", "title": "MONAH: Multi-Modal Narratives for Humans to analyze conversations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In conversational analyses, humans manually weave multimodal information into\nthe transcripts, which is significantly time-consuming. We introduce a system\nthat automatically expands the verbatim transcripts of video-recorded\nconversations using multimodal data streams. This system uses a set of\npreprocessing rules to weave multimodal annotations into the verbatim\ntranscripts and promote interpretability. Our feature engineering contributions\nare two-fold: firstly, we identify the range of multimodal features relevant to\ndetect rapport-building; secondly, we expand the range of multimodal\nannotations and show that the expansion leads to statistically significant\nimprovements in detecting rapport-building.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:55:58 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 02:25:23 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kim", "Joshua Y.", ""], ["Kim", "Greyson Y.", ""], ["Liu", "Chunfeng", ""], ["Calvo", "Rafael A.", ""], ["Taylor", "Silas C. R.", ""], ["Yacef", "Kalina", ""]]}, {"id": "2101.07343", "submitter": "Bence Bial", "authors": "Attila Nagy, Bence Bial, Judit \\'Acs", "title": "Automatic punctuation restoration with BERT models", "comments": "11 pages, 6 figures, source code at\n  https://github.com/attilanagy234/neural-punctuator", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach for automatic punctuation restoration with BERT models\nfor English and Hungarian. For English, we conduct our experiments on Ted\nTalks, a commonly used benchmark for punctuation restoration, while for\nHungarian we evaluate our models on the Szeged Treebank dataset. Our best\nmodels achieve a macro-averaged $F_1$-score of 79.8 in English and 82.2 in\nHungarian. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:13:01 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Nagy", "Attila", ""], ["Bial", "Bence", ""], ["\u00c1cs", "Judit", ""]]}, {"id": "2101.07382", "submitter": "Nikos Voskarides", "authors": "Svitlana Vakulenko, Nikos Voskarides, Zhucheng Tu, Shayne Longpre", "title": "A Comparison of Question Rewriting Methods for Conversational Passage\n  Retrieval", "comments": "ECIR 2021 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational passage retrieval relies on question rewriting to modify the\noriginal question so that it no longer depends on the conversation history.\nSeveral methods for question rewriting have recently been proposed, but they\nwere compared under different retrieval pipelines. We bridge this gap by\nthoroughly evaluating those question rewriting methods on the TREC CAsT 2019\nand 2020 datasets under the same retrieval pipeline. We analyze the effect of\ndifferent types of question rewriting methods on retrieval performance and show\nthat by combining question rewriting methods of different types we can achieve\nstate-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:17:52 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Voskarides", "Nikos", ""], ["Tu", "Zhucheng", ""], ["Longpre", "Shayne", ""]]}, {"id": "2101.07393", "submitter": "Austin W. Hanjie", "authors": "Austin W. Hanjie, Victor Zhong, Karthik Narasimhan", "title": "Grounding Language to Entities and Dynamics for Generalization in\n  Reinforcement Learning", "comments": "Accepted to ICML 2021. Note author list and name changes from\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of natural language to drive the generalization of\ncontrol policies and introduce the new multi-task environment Messenger with\nfree-form text manuals describing the environment dynamics. Unlike previous\nwork, Messenger does not assume prior knowledge connecting text and state\nobservations $-$ the control policy must simultaneously ground the game manual\nto entity symbols and dynamics in the environment. We develop a new model, EMMA\n(Entity Mapper with Multi-modal Attention) which uses an entity-conditioned\nattention module that allows for selective focus over relevant descriptions in\nthe manual for each entity in the environment. EMMA is end-to-end\ndifferentiable and learns a latent grounding of entities and dynamics from text\nto observations using only environment rewards. EMMA achieves successful\nzero-shot generalization to unseen games with new dynamics, obtaining a 40%\nhigher win rate compared to multiple baselines. However, win rate on the\nhardest stage of Messenger remains low (10%), demonstrating the need for\nadditional work in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:59:16 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:34:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hanjie", "Austin W.", ""], ["Zhong", "Victor", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2101.07396", "submitter": "Panos Achlioptas", "authors": "Panos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed\n  Elhoseiny, Leonidas Guibas", "title": "ArtEmis: Affective Language for Visual Art", "comments": "https://artemisdataset.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel large-scale dataset and accompanying machine learning\nmodels aimed at providing a detailed understanding of the interplay between\nvisual content, its emotional effect, and explanations for the latter in\nlanguage. In contrast to most existing annotation datasets in computer vision,\nwe focus on the affective experience triggered by visual artworks and ask the\nannotators to indicate the dominant emotion they feel for a given image and,\ncrucially, to also provide a grounded verbal explanation for their emotion\nchoice. As we demonstrate below, this leads to a rich set of signals for both\nthe objective content and the affective impact of an image, creating\nassociations with abstract concepts (e.g., \"freedom\" or \"love\"), or references\nthat go beyond what is directly visible, including visual similes and\nmetaphors, or subjective references to personal experiences. We focus on visual\nart (e.g., paintings, artistic photographs) as it is a prime example of imagery\ncreated to elicit emotional responses from its viewers. Our dataset, termed\nArtEmis, contains 439K emotion attributions and explanations from humans, on\n81K artworks from WikiArt. Building on this data, we train and demonstrate a\nseries of captioning systems capable of expressing and explaining emotions from\nvisual stimuli. Remarkably, the captions produced by these systems often\nsucceed in reflecting the semantic and abstract content of the image, going\nwell beyond systems trained on existing datasets. The collected dataset and\ndeveloped methods are available at https://artemisdataset.org.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:03:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Achlioptas", "Panos", ""], ["Ovsjanikov", "Maks", ""], ["Haydarov", "Kilichbek", ""], ["Elhoseiny", "Mohamed", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.07397", "submitter": "Yi Zhang", "authors": "Qingyuan Hu, Yi Zhang, Kanishka Misra, Julia Rayz", "title": "Exploring Lexical Irregularities in Hypothesis-Only Models of Natural\n  Language Inference", "comments": "Accepted by 2020 IEEE 19th International Conference on Cognitive\n  Informatics & Cognitive Computing (ICCI* CC). IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) is\nthe task of predicting the entailment relation between a pair of sentences\n(premise and hypothesis). This task has been described as a valuable testing\nground for the development of semantic representations, and is a key component\nin natural language understanding evaluation benchmarks. Models that understand\nentailment should encode both, the premise and the hypothesis. However,\nexperiments by Poliak et al. revealed a strong preference of these models\ntowards patterns observed only in the hypothesis, based on a 10 dataset\ncomparison. Their results indicated the existence of statistical irregularities\npresent in the hypothesis that bias the model into performing competitively\nwith the state of the art. While recast datasets provide large scale generation\nof NLI instances due to minimal human intervention, the papers that generate\nthem do not provide fine-grained analysis of the potential statistical patterns\nthat can bias NLI models. In this work, we analyze hypothesis-only models\ntrained on one of the recast datasets provided in Poliak et al. for word-level\npatterns. Our results indicate the existence of potential lexical biases that\ncould contribute to inflating the model performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:08:06 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 22:52:27 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 01:37:22 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Hu", "Qingyuan", ""], ["Zhang", "Yi", ""], ["Misra", "Kanishka", ""], ["Rayz", "Julia", ""]]}, {"id": "2101.07417", "submitter": "Negin Karisani", "authors": "Negin Karisani, Daniel E. Platt, Saugata Basu and Laxmi Parida", "title": "Inferring COVID-19 Biological Pathways from Clinical Phenotypes via\n  Topological Analysis", "comments": "Proceedings of the 5th International Workshop on Health Intelligence,\n  AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 has caused thousands of deaths around the world and also resulted in\na large international economic disruption. Identifying the pathways associated\nwith this illness can help medical researchers to better understand the\nproperties of the condition. This process can be carried out by analyzing the\nmedical records. It is crucial to develop tools and models that can aid\nresearchers with this process in a timely manner. However, medical records are\noften unstructured clinical notes, and this poses significant challenges to\ndeveloping the automated systems. In this article, we propose a pipeline to aid\npractitioners in analyzing clinical notes and revealing the pathways associated\nwith this disease. Our pipeline relies on topological properties and consists\nof three steps: 1) pre-processing the clinical notes to extract the salient\nconcepts, 2) constructing a feature space of the patients to characterize the\nextracted concepts, and finally, 3) leveraging the topological properties to\ndistill the available knowledge and visualize the result. Our experiments on a\npublicly available dataset of COVID-19 clinical notes testify that our pipeline\ncan indeed extract meaningful pathways.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:27:03 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Karisani", "Negin", ""], ["Platt", "Daniel E.", ""], ["Basu", "Saugata", ""], ["Parida", "Laxmi", ""]]}, {"id": "2101.07450", "submitter": "David Martinez", "authors": "David Martinez Iraola and Antonio Jimeno Yepes", "title": "Single versus Multiple Annotation for Named Entity Recognition of\n  Mutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The focus of this paper is to address the knowledge acquisition bottleneck\nfor Named Entity Recognition (NER) of mutations, by analysing different\napproaches to build manually-annotated data. We address first the impact of\nusing a single annotator vs two annotators, in order to measure whether\nmultiple annotators are required. Once we evaluate the performance loss when\nusing a single annotator, we apply different methods to sample the training\ndata for second annotation, aiming at improving the quality of the dataset\nwithout requiring a full pass. We use held-out double-annotated data to build\ntwo scenarios with different types of rankings: similarity-based and confidence\nbased. We evaluate both approaches on: (i) their ability to identify training\ninstances that are erroneous (cases where single-annotator labels differ from\ndouble-annotation after discussion), and (ii) on Mutation NER performance for\nstate-of-the-art classifiers after integrating the fixes at different\nthresholds.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:54:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Iraola", "David Martinez", ""], ["Yepes", "Antonio Jimeno", ""]]}, {"id": "2101.07597", "submitter": "Chengyi Wang", "authors": "Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,\n  Michael Zeng and Xuedong Huang", "title": "UniSpeech: Unified Speech Representation Learning with Labeled and\n  Unlabeled Data", "comments": "accepted by ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified pre-training approach called UniSpeech to\nlearn speech representations with both unlabeled and labeled data, in which\nsupervised phonetic CTC learning and phonetically-aware contrastive\nself-supervised learning are conducted in a multi-task learning manner. The\nresultant representations can capture information more correlated with phonetic\nstructures and improve the generalization across languages and domains. We\nevaluate the effectiveness of UniSpeech for cross-lingual representation\nlearning on public CommonVoice corpus. The results show that UniSpeech\noutperforms self-supervised pretraining and supervised transfer learning for\nspeech recognition by a maximum of 13.4% and 17.8% relative phone error rate\nreductions respectively (averaged over all testing languages). The\ntransferability of UniSpeech is also demonstrated on a domain-shift speech\nrecognition task, i.e., a relative word error rate reduction of 6% against the\nprevious approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:53:43 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:17:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Chengyi", ""], ["Wu", "Yu", ""], ["Qian", "Yao", ""], ["Kumatani", "Kenichi", ""], ["Liu", "Shujie", ""], ["Wei", "Furu", ""], ["Zeng", "Michael", ""], ["Huang", "Xuedong", ""]]}, {"id": "2101.07609", "submitter": "Chengzhi Zhang", "authors": "Shutian Ma, Heng Zhang, Chengzhi Zhang, Xiaozhong Liu", "title": "Chronological Citation Recommendation with Time Preference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citation recommendation is an important task to assist scholars in finding\ncandidate literature to cite. Traditional studies focus on static models of\nrecommending citations, which do not explicitly distinguish differences between\npapers that are caused by temporal variations. Although, some researchers have\ninvestigated chronological citation recommendation by adding time related\nfunction or modeling textual topics dynamically. These solutions can hardly\ncope with function generalization or cold-start problems when there is no\ninformation for user profiling or there are isolated papers never being cited.\nWith the rise and fall of science paradigms, scientific topics tend to change\nand evolve over time. People would have the time preference when citing papers,\nsince most of the theoretical basis exist in classical readings that published\nin old time, while new techniques are proposed in more recent papers. To\nexplore chronological citation recommendation, this paper wants to predict the\ntime preference based on user queries, which is a probability distribution of\nciting papers published in different time slices. Then, we use this time\npreference to re-rank the initial citation list obtained by content-based\nfiltering. Experimental results demonstrate that task performance can be\nfurther enhanced by time preference and it's flexible to be added in other\ncitation recommendation frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:18:05 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ma", "Shutian", ""], ["Zhang", "Heng", ""], ["Zhang", "Chengzhi", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "2101.07614", "submitter": "Chengzhi Zhang", "authors": "Chengzhi Zhang, Lifan Liu, Yuzhuo Wang", "title": "Characterizing References from Different Disciplines: A Perspective of\n  Citation Content Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidisciplinary cooperation is now common in research since social issues\ninevitably involve multiple disciplines. In research articles, reference\ninformation, especially citation content, is an important representation of\ncommunication among different disciplines. Analyzing the distribution\ncharacteristics of references from different disciplines in research articles\nis basic to detecting the sources of referred information and identifying\ncontributions of different disciplines. This work takes articles in PLoS as the\ndata and characterizes the references from different disciplines based on\nCitation Content Analysis (CCA). First, we download 210,334 full-text articles\nfrom PLoS and collect the information of the in-text citations. Then, we\nidentify the discipline of each reference in these academic articles. To\ncharacterize the distribution of these references, we analyze three\ncharacteristics, namely, the number of citations, the average cited intensity\nand the average citation length. Finally, we conclude that the distributions of\nreferences from different disciplines are significantly different. Although\nmost references come from Natural Science, Humanities and Social Sciences play\nimportant roles in the Introduction and Background sections of the articles.\nBasic disciplines, such as Mathematics, mainly provide research methods in the\narticles in PLoS. Citations mentioned in the Results and Discussion sections of\narticles are mainly in-discipline citations, such as citations from Nursing and\nMedicine in PLoS.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:30:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhang", "Chengzhi", ""], ["Liu", "Lifan", ""], ["Wang", "Yuzhuo", ""]]}, {"id": "2101.07632", "submitter": "Hung-Ting Su", "authors": "Chen-Hsi Chang, Hung-Ting Su, Jui-heng Hsu, Yu-Siang Wang, Yu-Cheng\n  Chang, Zhe Yu Liu, Ya-Liang Chang, Wen-Feng Cheng, Ke-Jyun Wang and Winston\n  H. Hsu", "title": "Situation and Behavior Understanding by Trope Detection on Films", "comments": "WWW 2021. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human ability of deep cognitive skills are crucial for the development of\nvarious real-world applications that process diverse and abundant user\ngenerated input. While recent progress of deep learning and natural language\nprocessing have enabled learning system to reach human performance on some\nbenchmarks requiring shallow semantics, such human ability still remains\nchallenging for even modern contextual embedding models, as pointed out by many\nrecent studies. Existing machine comprehension datasets assume sentence-level\ninput, lack of casual or motivational inferences, or could be answered with\nquestion-answer bias. Here, we present a challenging novel task, trope\ndetection on films, in an effort to create a situation and behavior\nunderstanding for machines. Tropes are storytelling devices that are frequently\nused as ingredients in recipes for creative works. Comparing to existing movie\ntag prediction tasks, tropes are more sophisticated as they can vary widely,\nfrom a moral concept to a series of circumstances, and embedded with\nmotivations and cause-and-effects. We introduce a new dataset, Tropes in Movie\nSynopses (TiMoS), with 5623 movie synopses and 95 different tropes collecting\nfrom a Wikipedia-style database, TVTropes. We present a multi-stream\ncomprehension network (MulCom) leveraging multi-level attention of words,\nsentences, and role relations. Experimental result demonstrates that modern\nmodels including BERT contextual embedding, movie tag prediction systems, and\nrelational networks, perform at most 37% of human performance (23.97/64.87) in\nterms of F1 score. Our MulCom outperforms all modern baselines, by 1.5 to 5.0\nF1 score and 1.5 to 3.0 mean of average precision (mAP) score. We also provide\na detailed analysis and human evaluation to pave ways for future research.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:09:54 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 03:51:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chang", "Chen-Hsi", ""], ["Su", "Hung-Ting", ""], ["Hsu", "Jui-heng", ""], ["Wang", "Yu-Siang", ""], ["Chang", "Yu-Cheng", ""], ["Liu", "Zhe Yu", ""], ["Chang", "Ya-Liang", ""], ["Cheng", "Wen-Feng", ""], ["Wang", "Ke-Jyun", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2101.07668", "submitter": "Simon Hengchen", "authors": "Simon Hengchen and Nina Tahmasebi and Dominik Schlechtweg and Haim\n  Dubossarsky", "title": "Challenges for Computational Lexical Semantic Change", "comments": "To appear in: Nina Tahmasebi, Lars Borin, Adam Jatowt, Yang Xu, Simon\n  Hengchen (eds). Computational Approaches to Semantic Change. Berlin: Language\n  Science Press. [preliminary page numbering]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The computational study of lexical semantic change (LSC) has taken off in the\npast few years and we are seeing increasing interest in the field, from both\ncomputational sciences and linguistics. Most of the research so far has focused\non methods for modelling and detecting semantic change using large diachronic\ntextual data, with the majority of the approaches employing neural embeddings.\nWhile methods that offer easy modelling of diachronic text are one of the main\nreasons for the spiking interest in LSC, neural models leave many aspects of\nthe problem unsolved. The field has several open and complex challenges. In\nthis chapter, we aim to describe the most important of these challenges and\noutline future directions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:01:30 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hengchen", "Simon", ""], ["Tahmasebi", "Nina", ""], ["Schlechtweg", "Dominik", ""], ["Dubossarsky", "Haim", ""]]}, {"id": "2101.07695", "submitter": "Stefano M. Iacus", "authors": "Tiziana Carpi, Airo Hino, Stefano Maria Iacus, Giuseppe Porro", "title": "Twitter Subjective Well-Being Indicator During COVID-19 Pandemic: A\n  Cross-Country Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CL q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study analyzes the impact of the COVID-19 pandemic on the subjective\nwell-being as measured through Twitter data indicators for Japan and Italy. It\nturns out that, overall, the subjective well-being dropped by 11.7% for Italy\nand 8.3% for Japan in the first nine months of 2020 compared to the last two\nmonths of 2019 and even more compared to the historical mean of the indexes.\nThrough a data science approach we try to identify the possible causes of this\ndrop down by considering several explanatory variables including, climate and\nair quality data, number of COVID-19 cases and deaths, Facebook Covid and flu\nsymptoms global survey, Google Trends data and coronavirus-related searches,\nGoogle mobility data, policy intervention measures, economic variables and\ntheir Google Trends proxies, as well as health and stress proxy variables based\non big data. We show that a simple static regression model is not able to\ncapture the complexity of well-being and therefore we propose a dynamic elastic\nnet approach to show how different group of factors may impact the well-being\nin different periods, even over a short time length, and showing further\ncountry-specific aspects. Finally, a structural equation modeling analysis\ntries to address the causal relationships among the COVID-19 factors and\nsubjective well-being showing that, overall, prolonged mobility\nrestrictions,flu and Covid-like symptoms, economic uncertainty, social\ndistancing and news about the pandemic have negative effects on the subjective\nwell-being.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:51:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Carpi", "Tiziana", ""], ["Hino", "Airo", ""], ["Iacus", "Stefano Maria", ""], ["Porro", "Giuseppe", ""]]}, {"id": "2101.07714", "submitter": "Ashish Sharma", "authors": "Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim\n  Althoff", "title": "Towards Facilitating Empathic Conversations in Online Mental Health\n  Support: A Reinforcement Learning Approach", "comments": "Published at WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online peer-to-peer support platforms enable conversations between millions\nof people who seek and provide mental health support. If successful, web-based\nmental health conversations could improve access to treatment and reduce the\nglobal disease burden. Psychologists have repeatedly demonstrated that empathy,\nthe ability to understand and feel the emotions and experiences of others, is a\nkey component leading to positive outcomes in supportive conversations.\nHowever, recent studies have shown that highly empathic conversations are rare\nin online mental health platforms.\n  In this paper, we work towards improving empathy in online mental health\nsupport conversations. We introduce a new task of empathic rewriting which aims\nto transform low-empathy conversational posts to higher empathy. Learning such\ntransformations is challenging and requires a deep understanding of empathy\nwhile maintaining conversation quality through text fluency and specificity to\nthe conversational context. Here we propose PARTNER, a deep reinforcement\nlearning agent that learns to make sentence-level edits to posts in order to\nincrease the expressed level of empathy while maintaining conversation quality.\nOur RL agent leverages a policy network, based on a transformer language model\nadapted from GPT-2, which performs the dual task of generating candidate\nempathic sentences and adding those sentences at appropriate positions. During\ntraining, we reward transformations that increase empathy in posts while\nmaintaining text fluency, context specificity and diversity. Through a\ncombination of automatic and human evaluation, we demonstrate that PARTNER\nsuccessfully generates more empathic, specific, and diverse responses and\noutperforms NLP methods from related tasks like style transfer and empathic\ndialogue generation. Our work has direct implications for facilitating empathic\nconversations on web-based platforms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:37:58 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 21:30:22 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 17:58:38 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sharma", "Ashish", ""], ["Lin", "Inna W.", ""], ["Miner", "Adam S.", ""], ["Atkins", "David C.", ""], ["Althoff", "Tim", ""]]}, {"id": "2101.07769", "submitter": "Peng Gao", "authors": "Peng Gao, Xiaoyuan Liu, Edward Choi, Bhavna Soman, Chinmaya Mishra,\n  Kate Farris, Dawn Song", "title": "A System for Automated Open-Source Threat Intelligence Gathering and\n  Management", "comments": "Accepted paper at SIGMOD 2021 demonstrations track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To remain aware of the fast-evolving cyber threat landscape, open-source\nCyber Threat Intelligence (OSCTI) has received growing attention from the\ncommunity. Commonly, knowledge about threats is presented in a vast number of\nOSCTI reports. Despite the pressing need for high-quality OSCTI, existing OSCTI\ngathering and management platforms, however, have primarily focused on\nisolated, low-level Indicators of Compromise. On the other hand, higher-level\nconcepts (e.g., adversary tactics, techniques, and procedures) and their\nrelationships have been overlooked, which contain essential knowledge about\nthreat behaviors that is critical to uncovering the complete threat scenario.\nTo bridge the gap, we propose SecurityKG, a system for automated OSCTI\ngathering and management. SecurityKG collects OSCTI reports from various\nsources, uses a combination of AI and NLP techniques to extract high-fidelity\nknowledge about threat behaviors, and constructs a security knowledge graph.\nSecurityKG also provides a UI that supports various types of interactivity to\nfacilitate knowledge graph exploration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:31:35 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 20:50:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gao", "Peng", ""], ["Liu", "Xiaoyuan", ""], ["Choi", "Edward", ""], ["Soman", "Bhavna", ""], ["Mishra", "Chinmaya", ""], ["Farris", "Kate", ""], ["Song", "Dawn", ""]]}, {"id": "2101.07924", "submitter": "Chengzhi Zhang", "authors": "Heng Zhang, Chengzhi Zhang", "title": "Using Full-text Content of Academic Articles to Build a Methodology\n  Taxonomy of Information Science in China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research on the construction of traditional information science methodology\ntaxonomy is mostly conducted manually. From the limited corpus, researchers\nhave attempted to summarize some of the research methodology entities into\nseveral abstract levels (generally three levels); however, they have been\nunable to provide a more granular hierarchy. Moreover, updating the methodology\ntaxonomy is traditionally a slow process. In this study, we collected full-text\nacademic papers related to information science. First, we constructed a basic\nmethodology taxonomy with three levels by manual annotation. Then, the word\nvectors of the research methodology entities were trained using the full-text\ndata. Accordingly, the research methodology entities were clustered and the\nbasic methodology taxonomy was expanded using the clustering results to obtain\na methodology taxonomy with more levels. This study provides new concepts for\nconstructing a methodology taxonomy of information science. The proposed\nmethodology taxonomy is semi-automated; it is more detailed than conventional\nschemes and the speed of taxonomy renewal has been enhanced.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:56:43 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2101.07942", "submitter": "Rishav Chakravarti", "authors": "Rishav Chakravarti, Avirup Sil", "title": "Towards Confident Machine Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable progress on academic benchmarks for the Reading\nComprehension (RC) task with State-of-the-Art models closing the gap with human\nperformance on extractive question answering. Datasets such as SQuAD 2.0 & NQ\nhave also introduced an auxiliary task requiring models to predict when a\nquestion has no answer in the text. However, in production settings, it is also\nnecessary to provide confidence estimates for the performance of the underlying\nRC model at both answer extraction and \"answerability\" detection. We propose a\nnovel post-prediction confidence estimation model, which we call Mr.C (short\nfor Mr. Confident), that can be trained to improve a system's ability to\nrefrain from making incorrect predictions with improvements of up to 4 points\nas measured by Area Under the Curve (AUC) scores. Mr.C can benefit from a novel\nwhite-box feature that leverages the underlying RC model's gradients.\nPerformance prediction is particularly important in cases of domain shift (as\nmeasured by training RC models on SQUAD 2.0 and evaluating on NQ), where Mr.C\nnot only improves AUC, but also traditional answerability prediction (as\nmeasured by a 5 point improvement in F1).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 03:02:12 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 04:32:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Chakravarti", "Rishav", ""], ["Sil", "Avirup", ""]]}, {"id": "2101.07947", "submitter": "Zekang Li", "authors": "Zekang Li, Zongjia Li, Jinchao Zhang, Yang Feng and Jie Zhou", "title": "WeChat AI & ICT's Submission for DSTC9 Interactive Dialogue Evaluation\n  Track", "comments": "DSTC9@AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participate in the DSTC9 Interactive Dialogue Evaluation Track (Gunasekara\net al. 2020) sub-task 1 (Knowledge Grounded Dialogue) and sub-task 2\n(Interactive Dialogue). In sub-task 1, we employ a pre-trained language model\nto generate topic-related responses and propose a response ensemble method for\nresponse selection. In sub-task2, we propose a novel Dialogue Planning Model\n(DPM) to capture conversation flow in the interaction with humans. We also\ndesign an integrated open-domain dialogue system containing pre-process,\ndialogue model, scoring model, and post-process, which can generate fluent,\ncoherent, consistent, and humanlike responses. We tie 1st on human ratings and\nalso get the highest Meteor, and Bert-score in sub-task 1, and rank 3rd on\ninteractive human evaluation in sub-task 2.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 03:19:50 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 03:03:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Zekang", ""], ["Li", "Zongjia", ""], ["Zhang", "Jinchao", ""], ["Feng", "Yang", ""], ["Zhou", "Jie", ""]]}, {"id": "2101.07960", "submitter": "Jane Greenberg", "authors": "Jane Greenberg, Xintong Zhao, Joseph Adair, Joan Boone and Xiaohua\n  Tony Hu", "title": "HIVE-4-MAT: Advancing the Ontology Infrastructure for Materials Science", "comments": "12 pages, 1 table, 7 figures, Presented at \"MTSR '20: 14th\n  International Conference on Metadata and Semantics Research,\" and forthcoming\n  in conference proceedings. Research supported by NSF OAC: #1940239", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduces HIVE-4-MAT - Helping Interdisciplinary Vocabulary Engineering for\nMaterials Science, an automatic linked data ontology application. Covers\ncontextual background for materials science, shared ontology infrastructures,\nand reviews the knowledge extraction and indexing process. HIVE-4-MAT's\nvocabulary browsing, term search and selection, and knowledge extraction and\nindexing are reviewed, and plans to integrate named entity recognition.\nConclusion highlights next steps with relation extraction to support better\nontologies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:40:09 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Greenberg", "Jane", ""], ["Zhao", "Xintong", ""], ["Adair", "Joseph", ""], ["Boone", "Joan", ""], ["Hu", "Xiaohua Tony", ""]]}, {"id": "2101.07973", "submitter": "Varad Bhatnagar", "authors": "Varad Bhatnagar, Prince Kumar, Sairam Moghili and Pushpak\n  Bhattacharyya", "title": "Divide and Conquer: An Ensemble Approach for Hostile Post Detection in\n  Hindi", "comments": null, "journal-ref": "CONSTRAINT @AAAI 2021 Combating Online Hostile Posts in Regional\n  Languages during Emergency Situation pp244-255", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently the NLP community has started showing interest towards the\nchallenging task of Hostile Post Detection. This paper present our system for\nShared Task at Constraint2021 on \"Hostile Post Detection in Hindi\". The data\nfor this shared task is provided in Hindi Devanagari script which was collected\nfrom Twitter and Facebook. It is a multi-label multi-class classification\nproblem where each data instance is annotated into one or more of the five\nclasses: fake, hate, offensive, defamation, and non-hostile. We propose a two\nlevel architecture which is made up of BERT based classifiers and statistical\nclassifiers to solve this problem. Our team 'Albatross', scored 0.9709 Coarse\ngrained hostility F1 score measure on Hostile Post Detection in Hindi subtask\nand secured 2nd rank out of 45 teams for the task. Our submission is ranked 2nd\nand 3rd out of a total of 156 submissions with Coarse grained hostility F1\nscore of 0.9709 and 0.9703 respectively. Our fine grained scores are also very\nencouraging and can be improved with further finetuning. The code is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:38:07 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bhatnagar", "Varad", ""], ["Kumar", "Prince", ""], ["Moghili", "Sairam", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2101.08087", "submitter": "Mohammad Kasra Habib", "authors": "Mohammad Kasra Habib", "title": "The Challenges of Persian User-generated Textual Content: A Machine\n  Learning-Based Approach", "comments": "12 Pages bib inc., 5 Figures and 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years a lot of research papers and studies have been published on\nthe development of effective approaches that benefit from a large amount of\nuser-generated content and build intelligent predictive models on top of them.\nThis research applies machine learning-based approaches to tackle the hurdles\nthat come with Persian user-generated textual content. Unfortunately, there is\nstill inadequate research in exploiting machine learning approaches to\nclassify/cluster Persian text. Further, analyzing Persian text suffers from a\nlack of resources; specifically from datasets and text manipulation tools.\nSince the syntax and semantics of the Persian language is different from\nEnglish and other languages, the available resources from these languages are\nnot instantly usable for Persian. In addition, recognition of nouns and\npronouns, parts of speech tagging, finding words' boundary, stemming or\ncharacter manipulations for Persian language are still unsolved issues that\nrequire further studying. Therefore, efforts have been made in this research to\naddress some of the challenges. This presented approach uses a\nmachine-translated datasets to conduct sentiment analysis for the Persian\nlanguage. Finally, the dataset has been rehearsed with different classifiers\nand feature engineering approaches. The results of the experiments have shown\npromising state-of-the-art performance in contrast to the previous efforts; the\nbest classifier was Support Vector Machines which achieved a precision of\n91.22%, recall of 91.71%, and F1 score of 91.46%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:57:59 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Habib", "Mohammad Kasra", ""]]}, {"id": "2101.08091", "submitter": "Xiaoqi Huang", "authors": "H. Weld, X. Huang, S. Long, J. Poon, S. C. Han (School of Computer\n  Science, The University of Sydney)", "title": "A survey of joint intent detection and slot-filling models in natural\n  language understanding", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent classification and slot filling are two critical tasks for natural\nlanguage understanding. Traditionally the two tasks have been deemed to proceed\nindependently. However, more recently, joint models for intent classification\nand slot filling have achieved state-of-the-art performance, and have proved\nthat there exists a strong relationship between the two tasks. This article is\na compilation of past work in natural language understanding, especially joint\nintent classification and slot filling. We observe three milestones in this\nresearch so far: Intent detection to identify the speaker's intention, slot\nfilling to label each word token in the speech/text, and finally, joint intent\nclassification and slot filling tasks. In this article, we describe trends,\napproaches, issues, data sets, evaluation metrics in intent classification and\nslot filling. We also discuss representative performance values, describe\nshared tasks, and provide pointers to future work, as given in prior works. To\ninterpret the state-of-the-art trends, we provide multiple tables that describe\nand summarise past research along different dimensions, including the types of\nfeatures, base approaches, and dataset domain used.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 12:15:11 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 05:08:26 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 03:25:17 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Weld", "H.", "", "School of Computer\n  Science, The University of Sydney"], ["Huang", "X.", "", "School of Computer\n  Science, The University of Sydney"], ["Long", "S.", "", "School of Computer\n  Science, The University of Sydney"], ["Poon", "J.", "", "School of Computer\n  Science, The University of Sydney"], ["Han", "S. C.", "", "School of Computer\n  Science, The University of Sydney"]]}, {"id": "2101.08106", "submitter": "Lingyun Feng", "authors": "Lingyun Feng, Minghui Qiu, Yaliang Li, Hai-Tao Zheng, Ying Shen", "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation", "comments": "AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite pre-trained language models such as BERT have achieved appealing\nperformance in a wide range of natural language processing tasks, they are\ncomputationally expensive to be deployed in real-time applications. A typical\nmethod is to adopt knowledge distillation to compress these large pre-trained\nmodels (teacher models) to small student models. However, for a target domain\nwith scarce training data, the teacher can hardly pass useful knowledge to the\nstudent, which yields performance degradation for the student models. To tackle\nthis problem, we propose a method to learn to augment for data-scarce domain\nBERT knowledge distillation, by learning a cross-domain manipulation scheme\nthat automatically augments the target with the help of resource-rich source\ndomains. Specifically, the proposed method generates samples acquired from a\nstationary distribution near the target data and adopts a reinforced selector\nto automatically refine the augmentation strategy according to the performance\nof the student. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms state-of-the-art baselines on four different tasks,\nand for the data-scarce domains, the compressed student models even perform\nbetter than the original large teacher model, with much fewer parameters (only\n${\\sim}13.3\\%$) when only a few labeled examples available.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:07:39 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 12:27:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Feng", "Lingyun", ""], ["Qiu", "Minghui", ""], ["Li", "Yaliang", ""], ["Zheng", "Hai-Tao", ""], ["Shen", "Ying", ""]]}, {"id": "2101.08114", "submitter": "Jose Manuel Gomez-Perez", "authors": "Andres Garcia-Silva and Jose Manuel Gomez-Perez", "title": "Classifying Scientific Publications with BERT -- Is Self-Attention a\n  Feature Selection Method?", "comments": "Paper accepted for publication at ECIR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the self-attention mechanism of BERT in a fine-tuning scenario\nfor the classification of scientific articles over a taxonomy of research\ndisciplines. We observe how self-attention focuses on words that are highly\nrelated to the domain of the article. Particularly, a small subset of\nvocabulary words tends to receive most of the attention. We compare and\nevaluate the subset of the most attended words with feature selection methods\nnormally used for text classification in order to characterize self-attention\nas a possible feature selection approach. Using ConceptNet as ground truth, we\nalso find that attended words are more related to the research fields of the\narticles. However, conventional feature selection methods are still a better\noption to learn classifiers from scratch. This result suggests that, while\nself-attention identifies domain-relevant terms, the discriminatory information\nin BERT is encoded in the contextualized outputs and the classification layer.\nIt also raises the question whether injecting feature selection methods in the\nself-attention mechanism could further optimize single sequence classification\nusing transformers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:22:26 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Garcia-Silva", "Andres", ""], ["Gomez-Perez", "Jose Manuel", ""]]}, {"id": "2101.08133", "submitter": "Artem Shelmanov", "authors": "Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov,\n  Daniil Larionov, Nikita Khromov, Olga Kozlova, Ekaterina Artemova, Dmitry V.\n  Dylov, and Alexander Panchenko", "title": "Active Learning for Sequence Tagging with Deep Pre-trained Models and\n  Bayesian Uncertainty Estimates", "comments": "In Proceedings of the 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating training data for sequence tagging of texts is usually very\ntime-consuming. Recent advances in transfer learning for natural language\nprocessing in conjunction with active learning open the possibility to\nsignificantly reduce the necessary annotation budget. We are the first to\nthoroughly investigate this powerful combination for the sequence tagging task.\nWe conduct an extensive empirical study of various Bayesian uncertainty\nestimation methods and Monte Carlo dropout options for deep pre-trained models\nin the active learning framework and find the best combinations for different\ntypes of models. Besides, we also demonstrate that to acquire instances during\nactive learning, a full-size Transformer can be substituted with a distilled\nversion, which yields better computational performance and reduces obstacles\nfor applying deep active learning in practice.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:59:25 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 15:50:11 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Shelmanov", "Artem", ""], ["Puzyrev", "Dmitri", ""], ["Kupriyanova", "Lyubov", ""], ["Belyakov", "Denis", ""], ["Larionov", "Daniil", ""], ["Khromov", "Nikita", ""], ["Kozlova", "Olga", ""], ["Artemova", "Ekaterina", ""], ["Dylov", "Dmitry V.", ""], ["Panchenko", "Alexander", ""]]}, {"id": "2101.08197", "submitter": "Rafael Ferreira", "authors": "Rafael Ferreira, Mariana Leite, David Semedo and Joao Magalhaes", "title": "Open-Domain Conversational Search Assistant with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain conversational search assistants aim at answering user questions\nabout open topics in a conversational manner. In this paper we show how the\nTransformer architecture achieves state-of-the-art results in key IR tasks,\nleveraging the creation of conversational assistants that engage in open-domain\nconversational search with single, yet informative, answers. In particular, we\npropose an open-domain abstractive conversational search agent pipeline to\naddress two major challenges: first, conversation context-aware search and\nsecond, abstractive search-answers generation. To address the first challenge,\nthe conversation context is modeled with a query rewriting method that unfolds\nthe context of the conversation up to a specific moment to search for the\ncorrect answers. These answers are then passed to a Transformer-based re-ranker\nto further improve retrieval performance. The second challenge, is tackled with\nrecent Abstractive Transformer architectures to generate a digest of the top\nmost relevant passages. Experiments show that Transformers deliver a solid\nperformance across all tasks in conversational search, outperforming the best\nTREC CAsT 2019 baseline.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:02:15 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ferreira", "Rafael", ""], ["Leite", "Mariana", ""], ["Semedo", "David", ""], ["Magalhaes", "Joao", ""]]}, {"id": "2101.08201", "submitter": "Rajkumar Pujari", "authors": "Deepak Gupta, Rajkumar Pujari, Asif Ekbal, Pushpak Bhattacharyya,\n  Anutosh Maitra, Tom Jain, Shubhashis Sengupta", "title": "Can Taxonomy Help? Improving Semantic Question Matching using Question\n  Taxonomy", "comments": "Paper was accepted at COLING 2018, presented as a poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a hybrid technique for semantic question matching.\nIt uses our proposed two-layered taxonomy for English questions by augmenting\nstate-of-the-art deep learning models with question classes obtained from a\ndeep learning based question classifier. Experiments performed on three\nopen-domain datasets demonstrate the effectiveness of our proposed approach. We\nachieve state-of-the-art results on partial ordering question ranking (POQR)\nbenchmark dataset. Our empirical analysis shows that coupling standard\ndistributional features (provided by the question encoder) with knowledge from\ntaxonomy is more effective than either deep learning (DL) or taxonomy-based\nknowledge alone.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:23:04 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Gupta", "Deepak", ""], ["Pujari", "Rajkumar", ""], ["Ekbal", "Asif", ""], ["Bhattacharyya", "Pushpak", ""], ["Maitra", "Anutosh", ""], ["Jain", "Tom", ""], ["Sengupta", "Shubhashis", ""]]}, {"id": "2101.08231", "submitter": "Zi-Yi Dou", "authors": "Zi-Yi Dou, Graham Neubig", "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word alignment over parallel corpora has a wide variety of applications,\nincluding learning translation lexicons, cross-lingual transfer of language\nprocessing tools, and automatic evaluation or analysis of translation outputs.\nThe great majority of past work on word alignment has worked by performing\nunsupervised learning on parallel texts. Recently, however, other work has\ndemonstrated that pre-trained contextualized word embeddings derived from\nmultilingually trained language models (LMs) prove an attractive alternative,\nachieving competitive results on the word alignment task even in the absence of\nexplicit training on parallel data. In this paper, we examine methods to marry\nthe two approaches: leveraging pre-trained LMs but fine-tuning them on parallel\ntext with objectives designed to improve alignment quality, and proposing\nmethods to effectively extract alignments from these fine-tuned models. We\nperform experiments on five language pairs and demonstrate that our model can\nconsistently outperform previous state-of-the-art models of all varieties. In\naddition, we demonstrate that we are able to train multilingual word aligners\nthat can obtain robust performance on different language pairs. Our aligner,\nAWESOME (Aligning Word Embedding Spaces of Multilingual Encoders), with\npre-trained models is available at https://github.com/neulab/awesome-align\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 17:54:47 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 23:24:00 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 09:40:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dou", "Zi-Yi", ""], ["Neubig", "Graham", ""]]}, {"id": "2101.08248", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Arturs Backurs, Karl Stratos", "title": "Generating (Formulaic) Text by Splicing Together Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to tackle conditional text generation tasks, especially those\nwhich require generating formulaic text, by splicing together segments of text\nfrom retrieved \"neighbor\" source-target pairs. Unlike recent work that\nconditions on retrieved neighbors in an encoder-decoder setting but generates\ntext token-by-token, left-to-right, we learn a policy that directly manipulates\nsegments of neighbor text (i.e., by inserting or replacing them) to form an\noutput. Standard techniques for training such a policy require an oracle\nderivation for each generation, and we prove that finding the shortest such\nderivation can be reduced to parsing under a particular weighted context-free\ngrammar. We find that policies learned in this way allow for interpretable\ntable-to-text or headline generation that is competitive with neighbor-based\ntoken-level policies on automatic metrics, though on all but one dataset\nneighbor-based policies underperform a strong neighborless baseline. In all\ncases, however, generating by splicing is faster.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 18:43:11 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:44:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wiseman", "Sam", ""], ["Backurs", "Arturs", ""], ["Stratos", "Karl", ""]]}, {"id": "2101.08333", "submitter": "Shuyang Li", "authors": "Shuyang Li, Jin Cao, Mukund Sridhar, Henghui Zhu, Shang-Wen Li, Wael\n  Hamza, Julian McAuley", "title": "Zero-shot Generalization in Dialog State Tracking through Generative\n  Question Answering", "comments": "Accepted as a Long Paper at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog State Tracking (DST), an integral part of modern dialog systems, aims\nto track user preferences and constraints (slots) in task-oriented dialogs. In\nreal-world settings with constantly changing services, DST systems must\ngeneralize to new domains and unseen slot types. Existing methods for DST do\nnot generalize well to new slot names and many require known ontologies of slot\ntypes and values for inference. We introduce a novel ontology-free framework\nthat supports natural language queries for unseen constraints and slots in\nmulti-domain task-oriented dialogs. Our approach is based on generative\nquestion-answering using a conditional language model pre-trained on\nsubstantive English sentences. Our model improves joint goal accuracy in\nzero-shot domain adaptation settings by up to 9% (absolute) over the previous\nstate-of-the-art on the MultiWOZ 2.1 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 21:47:20 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Li", "Shuyang", ""], ["Cao", "Jin", ""], ["Sridhar", "Mukund", ""], ["Zhu", "Henghui", ""], ["Li", "Shang-Wen", ""], ["Hamza", "Wael", ""], ["McAuley", "Julian", ""]]}, {"id": "2101.08370", "submitter": "Robert Litschko", "authors": "Robert Litschko and Ivan Vuli\\'c and Simone Paolo Ponzetto and Goran\n  Glava\\v{s}", "title": "Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual\n  Retrieval", "comments": "accepted at ECIR'21 (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pretrained multilingual text encoders based on neural Transformer\narchitectures, such as multilingual BERT (mBERT) and XLM, have achieved strong\nperformance on a myriad of language understanding tasks. Consequently, they\nhave been adopted as a go-to paradigm for multilingual and cross-lingual\nrepresentation learning and transfer, rendering cross-lingual word embeddings\n(CLWEs) effectively obsolete. However, questions remain to which extent this\nfinding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual\nIR (CLIR) tasks. Therefore, in this work we present a systematic empirical\nstudy focused on the suitability of the state-of-the-art multilingual encoders\nfor cross-lingual document and sentence retrieval tasks across a large number\nof language pairs. In contrast to supervised language understanding, our\nresults indicate that for unsupervised document-level CLIR -- a setup with no\nrelevance judgments for IR-specific fine-tuning -- pretrained encoders fail to\nsignificantly outperform models based on CLWEs. For sentence-level CLIR, we\ndemonstrate that state-of-the-art performance can be achieved. However, the\npeak performance is not met using the general-purpose multilingual text\nencoders `off-the-shelf', but rather relying on their variants that have been\nfurther specialized for sentence understanding tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 00:15:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Litschko", "Robert", ""], ["Vuli\u0107", "Ivan", ""], ["Ponzetto", "Simone Paolo", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2101.08382", "submitter": "Qingxiu Dong", "authors": "Qingxiu Dong, Xiaojun Wan, Yue Cao", "title": "ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ParaSCI, the first large-scale paraphrase dataset in the\nscientific field, including 33,981 paraphrase pairs from ACL (ParaSCI-ACL) and\n316,063 pairs from arXiv (ParaSCI-arXiv). Digging into characteristics and\ncommon patterns of scientific papers, we construct this dataset though\nintra-paper and inter-paper methods, such as collecting citations to the same\npaper or aggregating definitions by scientific terms. To take advantage of\nsentences paraphrased partially, we put up PDBERT as a general paraphrase\ndiscovering method. The major advantages of paraphrases in ParaSCI lie in the\nprominent length and textual diversity, which is complementary to existing\nparaphrase datasets. ParaSCI obtains satisfactory results on human evaluation\nand downstream tasks, especially long paraphrase generation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:10:06 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 14:01:05 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Dong", "Qingxiu", ""], ["Wan", "Xiaojun", ""], ["Cao", "Yue", ""]]}, {"id": "2101.08386", "submitter": "Simone Brugiapaglia", "authors": "S. Brugiapaglia, M. Liu, P. Tupper", "title": "Invariance, encodings, and generalization: learning identity effects\n  with neural networks", "comments": "arXiv admin note: text overlap with arXiv:2005.04330", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Often in language and other areas of cognition, whether two components of an\nobject are identical or not determines if it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na framework in which we can rigorously prove that algorithms satisfying simple\ncriteria cannot make the correct inference. We then show that a broad class of\nlearning algorithms including deep feedforward neural networks trained via\ngradient-based algorithms (such as stochastic gradient descent or the Adam\nmethod) satisfy our criteria, dependent on the encoding of inputs. In some\nbroader circumstances we are able to provide adversarial examples that the\nnetwork necessarily classifies incorrectly. Finally, we demonstrate our theory\nwith computational experiments in which we explore the effect of different\ninput encodings on the ability of algorithms to generalize to novel inputs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:28:15 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 19:55:16 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 23:06:58 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Brugiapaglia", "S.", ""], ["Liu", "M.", ""], ["Tupper", "P.", ""]]}, {"id": "2101.08426", "submitter": "Yutao Zhu", "authors": "Yutao Zhu, Jian-Yun Nie, Kun Zhou, Pan Du, Zhicheng Dou", "title": "Content Selection Network for Document-grounded Retrieval-based Chatbots", "comments": "ECIR 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding human-machine conversation in a document is an effective way to\nimprove the performance of retrieval-based chatbots. However, only a part of\nthe document content may be relevant to help select the appropriate response at\na round. It is thus crucial to select the part of document content relevant to\nthe current conversation context. In this paper, we propose a document content\nselection network (CSN) to perform explicit selection of relevant document\ncontents, and filter out the irrelevant parts. We show in experiments on two\npublic document-grounded conversation datasets that CSN can effectively help\nselect the relevant document contents to the conversation context, and it\nproduces better results than the state-of-the-art approaches. Our code and\ndatasets are available at https://github.com/DaoD/CSN.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:47:06 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhu", "Yutao", ""], ["Nie", "Jian-Yun", ""], ["Zhou", "Kun", ""], ["Du", "Pan", ""], ["Dou", "Zhicheng", ""]]}, {"id": "2101.08454", "submitter": "Amir Hussein", "authors": "Amir Hussein, Shinji Watanabe, Ahmed Ali", "title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:55:29 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 16:10:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hussein", "Amir", ""], ["Watanabe", "Shinji", ""], ["Ali", "Ahmed", ""]]}, {"id": "2101.08523", "submitter": "Ashutosh Modi", "authors": "Vijit Malik and Ashwani Bhat and Ashutosh Modi", "title": "Adv-OLM: Generating Textual Adversaries via OLM", "comments": "5 Pages + 1 Page references + 3 Pages Appendix, Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models are susceptible to adversarial examples that have\nimperceptible perturbations in the original input, resulting in adversarial\nattacks against these models. Analysis of these attacks on the state of the art\ntransformers in NLP can help improve the robustness of these models against\nsuch adversarial inputs. In this paper, we present Adv-OLM, a black-box attack\nmethod that adapts the idea of Occlusion and Language Models (OLM) to the\ncurrent state of the art attack methods. OLM is used to rank words of a\nsentence, which are later substituted using word replacement strategies. We\nexperimentally show that our approach outperforms other attack methods for\nseveral text classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:04:56 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Malik", "Vijit", ""], ["Bhat", "Ashwani", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2101.08698", "submitter": "Qingkai Zeng", "authors": "Qingkai Zeng, Mengxia Yu, Wenhao Yu, Tianwen Jiang, Tim Weninger and\n  Meng Jiang", "title": "Validating Label Consistency in NER Data Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data annotation plays a crucial role in ensuring your named entity\nrecognition (NER) projects are trained with the right information to learn\nfrom. Producing the most accurate labels is a challenge due to the complexity\ninvolved with annotation. Label inconsistency between multiple subsets of data\nannotation (e.g., training set and test set, or multiple training subsets) is\nan indicator of label mistakes. In this work, we present an empirical method to\nexplore the relationship between label (in-)consistency and NER model\nperformance. It can be used to validate the label consistency (or catches the\ninconsistency) in multiple sets of NER data annotation. In experiments, our\nmethod identified the label inconsistency of test data in SCIERC and CoNLL03\ndatasets (with 26.7% and 5.4% label mistakes). It validated the consistency in\nthe corrected version of both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:19:00 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zeng", "Qingkai", ""], ["Yu", "Mengxia", ""], ["Yu", "Wenhao", ""], ["Jiang", "Tianwen", ""], ["Weninger", "Tim", ""], ["Jiang", "Meng", ""]]}, {"id": "2101.08700", "submitter": "Terry Ruas Ph.D.", "authors": "Terry Ruas, William Grosky, Aiko Aizawa", "title": "Multi-sense embeddings through a word sense disambiguation process", "comments": null, "journal-ref": "Expert Systems with Applications. Volume 136, 1 December 2019,\n  Pages 288-303", "doi": "10.1016/j.eswa.2019.06.026", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural Language Understanding has seen an increasing number of publications\nin the last few years, especially after robust word embeddings models became\nprominent, when they proved themselves able to capture and represent semantic\nrelationships from massive amounts of data. Nevertheless, traditional models\noften fall short in intrinsic issues of linguistics, such as polysemy and\nhomonymy. Any expert system that makes use of natural language in its core, can\nbe affected by a weak semantic representation of text, resulting in inaccurate\noutcomes based on poor decisions. To mitigate such issues, we propose a novel\napproach called Most Suitable Sense Annotation (MSSA), that disambiguates and\nannotates each word by its specific sense, considering the semantic effects of\nits context. Our approach brings three main contributions to the semantic\nrepresentation scenario: (i) an unsupervised technique that disambiguates and\nannotates words by their senses, (ii) a multi-sense embeddings model that can\nbe extended to any traditional word embeddings algorithm, and (iii) a recurrent\nmethodology that allows our models to be re-used and their representations\nrefined. We test our approach on six different benchmarks for the word\nsimilarity task, showing that our approach can produce state-of-the-art results\nand outperforms several more complex state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:22:34 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ruas", "Terry", ""], ["Grosky", "William", ""], ["Aizawa", "Aiko", ""]]}, {"id": "2101.08890", "submitter": "Prabhu Kaliamoorthi Mr", "authors": "Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson", "title": "Distilling Large Language Models into Tiny and Effective Students using\n  pQRNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained multilingual models like mBERT, XLM-R achieve state of the\nart results on language understanding tasks. However, they are not well suited\nfor latency critical applications on both servers and edge devices. It's\nimportant to reduce the memory and compute resources required by these models.\nTo this end, we propose pQRNN, a projection-based embedding-free neural encoder\nthat is tiny and effective for natural language processing tasks. Without\npre-training, pQRNNs significantly outperform LSTM models with pre-trained\nembeddings despite being 140x smaller. With the same number of parameters, they\noutperform transformer baselines thereby showcasing their parameter efficiency.\nAdditionally, we show that pQRNNs are effective student architectures for\ndistilling large pre-trained language models. We perform careful ablations\nwhich study the effect of pQRNN parameters, data augmentation, and distillation\nsettings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN\nstudents achieve 95.9\\% of the performance of an mBERT teacher while being 350x\nsmaller. On mATIS, a popular parsing task, pQRNN students on average are able\nto get to 97.1\\% of the teacher while again being 350x smaller. Our strong\nresults suggest that our approach is great for latency-sensitive applications\nwhile being able to leverage large mBERT-like models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:45:50 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kaliamoorthi", "Prabhu", ""], ["Siddhant", "Aditya", ""], ["Li", "Edward", ""], ["Johnson", "Melvin", ""]]}, {"id": "2101.08904", "submitter": "Xiong Liu", "authors": "Zhaoyi Chen, Xiong Liu, William Hogan, Elizabeth Shenkman, Jiang Bian", "title": "Applications of artificial intelligence in drug development using\n  real-world data", "comments": null, "journal-ref": "Drug Discovery Today 2020", "doi": "10.1016/j.drudis.2020.12.013", "report-no": "PMID: 33358699", "categories": "cs.CY cs.CL cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The US Food and Drug Administration (FDA) has been actively promoting the use\nof real-world data (RWD) in drug development. RWD can generate important\nreal-world evidence reflecting the real-world clinical environment where the\ntreatments are used. Meanwhile, artificial intelligence (AI), especially\nmachine- and deep-learning (ML/DL) methods, have been increasingly used across\nmany stages of the drug development process. Advancements in AI have also\nprovided new strategies to analyze large, multidimensional RWD. Thus, we\nconducted a rapid review of articles from the past 20 years, to provide an\noverview of the drug development studies that use both AI and RWD. We found\nthat the most popular applications were adverse event detection, trial\nrecruitment, and drug repurposing. Here, we also discuss current research gaps\nand future opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 01:13:54 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 17:59:01 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Zhaoyi", ""], ["Liu", "Xiong", ""], ["Hogan", "William", ""], ["Shenkman", "Elizabeth", ""], ["Bian", "Jiang", ""]]}, {"id": "2101.08942", "submitter": "Ye Liu", "authors": "Ye Liu, Yao Wan, Jian-Guo Zhang, Wenting Zhao, Philip S. Yu", "title": "Enriching Non-Autoregressive Transformer with Syntactic and\n  SemanticStructures for Neural Machine Translation", "comments": "10 pages, Appear in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-autoregressive models have boosted the efficiency of neural machine\ntranslation through parallelized decoding at the cost of effectiveness when\ncomparing with the autoregressive counterparts. In this paper, we claim that\nthe syntactic and semantic structures among natural language are critical for\nnon-autoregressive machine translation and can further improve the performance.\nHowever, these structures are rarely considered in the existing\nnon-autoregressive models. Inspired by this intuition, we propose to\nincorporate the explicit syntactic and semantic structures of languages into a\nnon-autoregressive Transformer, for the task of neural machine translation.\nMoreover, we also consider the intermediate latent alignment within target\nsentences to better learn the long-term token dependencies. Experimental\nresults on two real-world datasets (i.e., WMT14 En-De and WMT16 En-Ro) show\nthat our model achieves a significantly faster speed, as well as keeps the\ntranslation quality when compared with several state-of-the-art\nnon-autoregressive models.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 04:12:17 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Liu", "Ye", ""], ["Wan", "Yao", ""], ["Zhang", "Jian-Guo", ""], ["Zhao", "Wenting", ""], ["Yu", "Philip S.", ""]]}, {"id": "2101.08962", "submitter": "Tong Chen", "authors": "Tong Chen, Sirou Zhu, Yiming Wen, Zhaomin Zheng", "title": "Knowledge Graph Completion with Text-aided Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Knowledge Graph Completion is a task of expanding the knowledge graph/base\nthrough estimating possible entities, or proper nouns, that can be connected\nusing a set of predefined relations, or verb/predicates describing\ninterconnections of two things. Generally, we describe this problem as adding\nnew edges to a current network of vertices and edges. Traditional approaches\nmainly focus on using the existing graphical information that is intrinsic of\nthe graph and train the corresponding embeddings to describe the information;\nhowever, we think that the corpus that are related to the entities should also\ncontain information that can positively influence the embeddings to better make\npredictions. In our project, we try numerous ways of using extracted or raw\ntextual information to help existing KG embedding frameworks reach better\nprediction results, in the means of adding a similarity function to the\nregularization part in the loss function. Results have shown that we have made\ndecent improvements over baseline KG embedding methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 06:10:09 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Chen", "Tong", ""], ["Zhu", "Sirou", ""], ["Wen", "Yiming", ""], ["Zheng", "Zhaomin", ""]]}, {"id": "2101.09004", "submitter": "Suman Dowlagar", "authors": "Suman Dowlagar, Radhika Mamidi", "title": "CMSAOne@Dravidian-CodeMix-FIRE2020: A Meta Embedding and Transformer\n  model for Code-Mixed Sentiment Analysis on Social Media Text", "comments": "FIRE 2020: Forum for Information Retrieval Evaluation, December\n  16-20, 2020, Hyderabad, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-mixing(CM) is a frequently observed phenomenon that uses multiple\nlanguages in an utterance or sentence. CM is mostly practiced on various social\nmedia platforms and in informal conversations. Sentiment analysis (SA) is a\nfundamental step in NLP and is well studied in the monolingual text.\nCode-mixing adds a challenge to sentiment analysis due to its non-standard\nrepresentations. This paper proposes a meta embedding with a transformer method\nfor sentiment analysis on the Dravidian code-mixed dataset. In our method, we\nused meta embeddings to capture rich text representations. We used the proposed\nmethod for the Task: \"Sentiment Analysis for Dravidian Languages in Code-Mixed\nText\", and it achieved an F1 score of $0.58$ and $0.66$ for the given Dravidian\ncode mixed data sets. The code is provided in the Github\nhttps://github.com/suman101112/fire-2020-Dravidian-CodeMix.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:48:27 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dowlagar", "Suman", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.09007", "submitter": "Suman Dowlagar", "authors": "Suman Dowlagar, Radhika Mamidi", "title": "HASOCOne@FIRE-HASOC2020: Using BERT and Multilingual BERT models for\n  Hate Speech Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hateful and Toxic content has become a significant concern in today's world\ndue to an exponential rise in social media. The increase in hate speech and\nharmful content motivated researchers to dedicate substantial efforts to the\nchallenging direction of hateful content identification. In this task, we\npropose an approach to automatically classify hate speech and offensive\ncontent. We have used the datasets obtained from FIRE 2019 and 2020 shared\ntasks. We perform experiments by taking advantage of transfer learning models.\nWe observed that the pre-trained BERT model and the multilingual-BERT model\ngave the best results. The code is made publically available at\nhttps://github.com/suman101112/hasoc-fire-2020.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:55:32 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dowlagar", "Suman", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.09009", "submitter": "Suman Dowlagar", "authors": "Suman Dowlagar, Radhika Mamidi", "title": "Does a Hybrid Neural Network based Feature Selection Model Improve Text\n  Classification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text classification is a fundamental problem in the field of natural language\nprocessing. Text classification mainly focuses on giving more importance to all\nthe relevant features that help classify the textual data. Apart from these,\nthe text can have redundant or highly correlated features. These features\nincrease the complexity of the classification algorithm. Thus, many\ndimensionality reduction methods were proposed with the traditional machine\nlearning classifiers. The use of dimensionality reduction methods with machine\nlearning classifiers has achieved good results. In this paper, we propose a\nhybrid feature selection method for obtaining relevant features by combining\nvarious filter-based feature selection methods and fastText classifier. We then\npresent three ways of implementing a feature selection and neural network\npipeline. We observed a reduction in training time when feature selection\nmethods are used along with neural networks. We also observed a slight increase\nin accuracy on some datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:12:19 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dowlagar", "Suman", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.09012", "submitter": "Suman Dowlagar", "authors": "Suman Dowlagar, Radhika Mamidi", "title": "Multilingual Pre-Trained Transformers and Convolutional NN\n  Classification Models for Technical Domain Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a transfer learning system to perform technical\ndomain identification on multilingual text data. We have submitted two runs,\none uses the transformer model BERT, and the other uses XLM-ROBERTa with the\nCNN model for text classification. These models allowed us to identify the\ndomain of the given sentences for the ICON 2020 shared Task, TechDOfication:\nTechnical Domain Identification. Our system ranked the best for the subtasks\n1d, 1g for the given TechDOfication dataset.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:18:02 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dowlagar", "Suman", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.09015", "submitter": "Suman Dowlagar", "authors": "Suman Dowlagar, Radhika Mamidi", "title": "Unsupervised Technical Domain Terms Extraction using Term Extractor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Terminology extraction, also known as term extraction, is a subtask of\ninformation extraction. The goal of terminology extraction is to extract\nrelevant words or phrases from a given corpus automatically. This paper focuses\non the unsupervised automated domain term extraction method that considers\nchunking, preprocessing, and ranking domain-specific terms using relevance and\ncohesion functions for ICON 2020 shared task 2: TermTraction.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:24:09 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dowlagar", "Suman", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.09023", "submitter": "Terry Ruas Ph.D.", "authors": "Terry Ruas, Charles Henrique Porto Ferreira, William Grosky,\n  Fabr\\'icio Olivetti de Fran\\c{c}a, D\\'ebora Maria Rossi Medeiros", "title": "Enhanced word embeddings using multi-semantic representation through\n  lexical chains", "comments": null, "journal-ref": "Information Sciences. Volume 532, September 2020, Pages 16-32", "doi": "10.1016/j.ins.2020.04.048", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The relationship between words in a sentence often tells us more about the\nunderlying semantic content of a document than its actual words, individually.\nIn this work, we propose two novel algorithms, called Flexible Lexical Chain II\nand Fixed Lexical Chain II. These algorithms combine the semantic relations\nderived from lexical chains, prior knowledge from lexical databases, and the\nrobustness of the distributional hypothesis in word embeddings as building\nblocks forming a single system. In short, our approach has three main\ncontributions: (i) a set of techniques that fully integrate word embeddings and\nlexical chains; (ii) a more robust semantic representation that considers the\nlatent relation between words in a document; and (iii) lightweight word\nembeddings models that can be extended to any natural language task. We intend\nto assess the knowledge of pre-trained models to evaluate their robustness in\nthe document classification task. The proposed techniques are tested against\nseven word embeddings algorithms using five different machine learning\nclassifiers over six scenarios in the document classification task. Our results\nshow the integration between lexical chains and word embeddings representations\nsustain state-of-the-art results, even against more complex systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:43:33 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ruas", "Terry", ""], ["Ferreira", "Charles Henrique Porto", ""], ["Grosky", "William", ""], ["de Fran\u00e7a", "Fabr\u00edcio Olivetti", ""], ["Medeiros", "D\u00e9bora Maria Rossi", ""]]}, {"id": "2101.09069", "submitter": "Simon Hengchen", "authors": "Valerio Perrone and Simon Hengchen and Marco Palma and Alessandro\n  Vatri and Jim Q. Smith and Barbara McGillivray", "title": "Lexical semantic change for Ancient Greek and Latin", "comments": "To appear in: Nina Tahmasebi, Lars Borin, Adam Jatowt, Yang Xu, Simon\n  Hengchen (eds). Computational Approaches to Semantic Change. Berlin: Language\n  Science Press. [preliminary page numbering]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Change and its precondition, variation, are inherent in languages. Over time,\nnew words enter the lexicon, others become obsolete, and existing words acquire\nnew senses. Associating a word's correct meaning in its historical context is a\ncentral challenge in diachronic research. Historical corpora of classical\nlanguages, such as Ancient Greek and Latin, typically come with rich metadata,\nand existing models are limited by their inability to exploit contextual\ninformation beyond the document timestamp. While embedding-based methods\nfeature among the current state of the art systems, they are lacking in the\ninterpretative power. In contrast, Bayesian models provide explicit and\ninterpretable representations of semantic change phenomena. In this chapter we\nbuild on GASC, a recent computational approach to semantic change based on a\ndynamic Bayesian mixture model. In this model, the evolution of word senses\nover time is based not only on distributional information of lexical nature,\nbut also on text genres. We provide a systematic comparison of dynamic Bayesian\nmixture models for semantic change with state-of-the-art embedding-based\nmodels. On top of providing a full description of meaning change over time, we\nshow that Bayesian mixture models are highly competitive approaches to detect\nbinary semantic change in both Ancient Greek and Latin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:04:08 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Perrone", "Valerio", ""], ["Hengchen", "Simon", ""], ["Palma", "Marco", ""], ["Vatri", "Alessandro", ""], ["Smith", "Jim Q.", ""], ["McGillivray", "Barbara", ""]]}, {"id": "2101.09079", "submitter": "Yevgeniy Puzikov", "authors": "Yevgeniy Puzikov", "title": "Evaluation Discrepancy Discovery: A Sentence Compression Case-study", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Reliable evaluation protocols are of utmost importance for reproducible NLP\nresearch. In this work, we show that sometimes neither metric nor conventional\nhuman evaluation is sufficient to draw conclusions about system performance.\nUsing sentence compression as an example task, we demonstrate how a system can\ngame a well-established dataset to achieve state-of-the-art results. In\ncontrast with the results reported in previous work that showed correlation\nbetween human judgements and metric scores, our manual analysis of\nstate-of-the-art system outputs demonstrates that high metric scores may only\nindicate a better fit to the data, but not better outputs, as perceived by\nhumans.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:28:41 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Puzikov", "Yevgeniy", ""]]}, {"id": "2101.09083", "submitter": "Dennis Pinto", "authors": "Dennis Pinto, Jose-Mar\\'ia Arnau, Antonio Gonz\\'alez", "title": "Exploiting Beam Search Confidence for Energy-Efficient Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With computers getting more and more powerful and integrated in our daily\nlives, the focus is increasingly shifting towards more human-friendly\ninterfaces, making Automatic Speech Recognition (ASR) a central player as the\nideal means of interaction with machines. Consequently, interest in speech\ntechnology has grown in the last few years, with more systems being proposed\nand higher accuracy levels being achieved, even surpassing \\textit{Human\nAccuracy}. While ASR systems become increasingly powerful, the computational\ncomplexity also increases, and the hardware support have to keep pace. In this\npaper, we propose a technique to improve the energy-efficiency and performance\nof ASR systems, focusing on low-power hardware for edge devices. We focus on\noptimizing the DNN-based Acoustic Model evaluation, as we have observed it to\nbe the main bottleneck in state-of-the-art ASR systems, by leveraging run-time\ninformation from the Beam Search. By doing so, we reduce energy and execution\ntime of the acoustic model evaluation by 25.6% and 25.9%, respectively, with\nnegligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:35:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Pinto", "Dennis", ""], ["Arnau", "Jose-Mar\u00eda", ""], ["Gonz\u00e1lez", "Antonio", ""]]}, {"id": "2101.09090", "submitter": "Caglar Demir", "authors": "Caglar Demir and Diego Moussallem and Axel-Cyrille Ngonga Ngomo", "title": "A shallow neural model for relation prediction", "comments": "15th IEEE International Conference on Semantic Computing, ICSC-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge graph completion refers to predicting missing triples. Most\napproaches achieve this goal by predicting entities, given an entity and a\nrelation. We predict missing triples via the relation prediction. To this end,\nwe frame the relation prediction problem as a multi-label classification\nproblem and propose a shallow neural model (SHALLOM) that accurately infers\nmissing relations from entities. SHALLOM is analogous to C-BOW as both\napproaches predict a central token (p) given surrounding tokens ((s,o)). Our\nexperiments indicate that SHALLOM outperforms state-of-the-art approaches on\nthe FB15K-237 and WN18RR with margins of up to $3\\%$ and $8\\%$ (absolute),\nrespectively, while requiring a maximum training time of 8 minutes on these\ndatasets. We ensure the reproducibility of our results by providing an\nopen-source implementation including training and evaluation scripts at\n{\\url{https://github.com/dice-group/Shallom}.}\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:10:11 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Demir", "Caglar", ""], ["Moussallem", "Diego", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2101.09101", "submitter": "Ming Liang", "authors": "Ming Liang and Kui Xue and Tong Ruan", "title": "A multi-perspective combined recall and rank framework for Chinese\n  procedure terminology normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical terminology normalization aims to map the clinical mention to\nterminologies come from a knowledge base, which plays an important role in\nanalyzing Electronic Health Record(EHR) and many downstream tasks. In this\npaper, we focus on Chinese procedure terminology normalization. The expression\nof terminologies are various and one medical mention may be linked to multiple\nterminologies. Previous study explores some methods such as multi-class\nclassification or learning to rank(LTR) to sort the terminologies by literature\nand semantic information. However, these information is inadequate to find the\nright terminologies, particularly in multi-implication cases. In this work, we\npropose a combined recall and rank framework to solve the above problems. This\nframework is composed of a multi-task candidate generator(MTCG), a keywords\nattentive ranker(KAR) and a fusion block(FB). MTCG is utilized to predict the\nmention implication number and recall candidates with semantic similarity. KAR\nis based on Bert with a keywords attentive mechanism which focuses on keywords\nsuch as procedure sites and procedure types. FB merges the similarity come from\nMTCG and KAR to sort the terminologies from different perspectives. Detailed\nexperimental analysis shows our proposed framework has a remarkable improvement\non both performance and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:37:10 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Liang", "Ming", ""], ["Xue", "Kui", ""], ["Ruan", "Tong", ""]]}, {"id": "2101.09115", "submitter": "Madhura Pande", "authors": "Madhura Pande, Aakriti Budhraja, Preksha Nema, Pratyush Kumar and\n  Mitesh M. Khapra", "title": "The heads hypothesis: A unifying statistical approach towards\n  understanding multi-headed attention in BERT", "comments": "accepted at AAAI 2021 (Main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-headed attention heads are a mainstay in transformer-based models.\nDifferent methods have been proposed to classify the role of each attention\nhead based on the relations between tokens which have high pair-wise attention.\nThese roles include syntactic (tokens with some syntactic relation), local\n(nearby tokens), block (tokens in the same sentence) and delimiter (the special\n[CLS], [SEP] tokens). There are two main challenges with existing methods for\nclassification: (a) there are no standard scores across studies or across\nfunctional roles, and (b) these scores are often average quantities measured\nacross sentences without capturing statistical significance. In this work, we\nformalize a simple yet effective score that generalizes to all the roles of\nattention heads and employs hypothesis testing on this score for robust\ninference. This provides us the right lens to systematically analyze attention\nheads and confidently comment on many commonly posed questions on analyzing the\nBERT model. In particular, we comment on the co-location of multiple functional\nroles in the same attention head, the distribution of attention heads across\nlayers, and effect of fine-tuning for specific NLP tasks on these functional\nroles.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:10:59 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Pande", "Madhura", ""], ["Budhraja", "Aakriti", ""], ["Nema", "Preksha", ""], ["Kumar", "Pratyush", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "2101.09149", "submitter": "Orion Weller", "authors": "Orion Weller and Matthias Sperber and Christian Gollan and Joris\n  Kluivers", "title": "Streaming Models for Joint Speech Recognition and Translation", "comments": "Camera Ready for EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using end-to-end models for speech translation (ST) has increasingly been the\nfocus of the ST community. These models condense the previously cascaded\nsystems by directly converting sound waves into translated text. However,\ncascaded models have the advantage of including automatic speech recognition\noutput, useful for a variety of practical ST systems that often display\ntranscripts to the user alongside the translations. To bridge this gap, recent\nwork has shown initial progress into the feasibility for end-to-end models to\nproduce both of these outputs. However, all previous work has only looked at\nthis problem from the consecutive perspective, leaving uncertainty on whether\nthese approaches are effective in the more challenging streaming setting. We\ndevelop an end-to-end streaming ST model based on a re-translation approach and\ncompare against standard cascading approaches. We also introduce a novel\ninference method for the joint case, interleaving both transcript and\ntranslation in generation and removing the need to use separate decoders. Our\nevaluation across a range of metrics capturing accuracy, latency, and\nconsistency shows that our end-to-end models are statistically similar to\ncascading models, while having half the number of parameters. We also find that\nboth systems provide strong translation quality at low latency, keeping 99% of\nconsecutive quality at a lag of just under a second.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:16:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Weller", "Orion", ""], ["Sperber", "Matthias", ""], ["Gollan", "Christian", ""], ["Kluivers", "Joris", ""]]}, {"id": "2101.09157", "submitter": "Daniel Buschek", "authors": "Daniel Buschek, Martin Z\\\"urn, Malin Eiband", "title": "The Impact of Multiple Parallel Phrase Suggestions on Email Input and\n  Composition Behaviour of Native and Non-Native English Writers", "comments": "21 pages, 4 figures, ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445372", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an in-depth analysis of the impact of multi-word suggestion\nchoices from a neural language model on user behaviour regarding input and text\ncomposition in email writing. Our study for the first time compares different\nnumbers of parallel suggestions, and use by native and non-native English\nwriters, to explore a trade-off of \"efficiency vs ideation\", emerging from\nrecent literature. We built a text editor prototype with a neural language\nmodel (GPT-2), refined in a prestudy with 30 people. In an online study\n(N=156), people composed emails in four conditions (0/1/3/6 parallel\nsuggestions). Our results reveal (1) benefits for ideation, and costs for\nefficiency, when suggesting multiple phrases; (2) that non-native speakers\nbenefit more from more suggestions; and (3) further insights into behaviour\npatterns. We discuss implications for research, the design of interactive\nsuggestion systems, and the vision of supporting writers with AI instead of\nreplacing them.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:32:32 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Buschek", "Daniel", ""], ["Z\u00fcrn", "Martin", ""], ["Eiband", "Malin", ""]]}, {"id": "2101.09244", "submitter": "Zitao Shen", "authors": "Zitao Shen, Yoonkwon Yi, Anusha Bompelli, Fang Yu, Yanshan Wang, Rui\n  Zhang", "title": "Extracting Lifestyle Factors for Alzheimer's Disease from Clinical Notes\n  Using Deep Learning with Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since no effective therapies exist for Alzheimer's disease (AD), prevention\nhas become more critical through lifestyle factor changes and interventions.\nAnalyzing electronic health records (EHR) of patients with AD can help us\nbetter understand lifestyle's effect on AD. However, lifestyle information is\ntypically stored in clinical narratives. Thus, the objective of the study was\nto demonstrate the feasibility of natural language processing (NLP) models to\nclassify lifestyle factors (e.g., physical activity and excessive diet) from\nclinical texts. We automatically generated labels for the training data by\nusing a rule-based NLP algorithm. We conducted weak supervision for pre-trained\nBidirectional Encoder Representations from Transformers (BERT) models on the\nweakly labeled training corpus. These models include the BERT base model,\nPubMedBERT(abstracts + full text), PubMedBERT(only abstracts), Unified Medical\nLanguage System (UMLS) BERT, Bio BERT, and Bio-clinical BERT. We performed two\ncase studies: physical activity and excessive diet, in order to validate the\neffectiveness of BERT models in classifying lifestyle factors for AD. These\nmodels were compared on the developed Gold Standard Corpus (GSC) on the two\ncase studies. The PubmedBERT(Abs) model achieved the best performance for\nphysical activity, with its precision, recall, and F-1 scores of 0.96, 0.96,\nand 0.96, respectively. Regarding classifying excessive diet, the Bio BERT\nmodel showed the highest performance with perfect precision, recall, and F-1\nscores. The proposed approach leveraging weak supervision could significantly\nincrease the sample size, which is required for training the deep learning\nmodels. The study also demonstrates the effectiveness of BERT models for\nextracting lifestyle factors for Alzheimer's disease from clinical notes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:55:03 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 03:42:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Shen", "Zitao", ""], ["Yi", "Yoonkwon", ""], ["Bompelli", "Anusha", ""], ["Yu", "Fang", ""], ["Wang", "Yanshan", ""], ["Zhang", "Rui", ""]]}, {"id": "2101.09276", "submitter": "Seokhwan Kim", "authors": "Seokhwan Kim, Mihail Eric, Behnam Hedayatnia, Karthik Gopalakrishnan,\n  Yang Liu, Chao-Wei Huang, Dilek Hakkani-Tur", "title": "Beyond Domain APIs: Task-oriented Conversational Modeling with\n  Unstructured Knowledge Access Track in DSTC9", "comments": "To be presented at AAAI-21 DSTC9 Workshop. arXiv admin note:\n  substantial text overlap with arXiv:2006.03533, arXiv:2011.06486", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior work on task-oriented dialogue systems are restricted to a limited\ncoverage of domain APIs, while users oftentimes have domain related requests\nthat are not covered by the APIs. This challenge track aims to expand the\ncoverage of task-oriented dialogue systems by incorporating external\nunstructured knowledge sources. We define three tasks: knowledge-seeking turn\ndetection, knowledge selection, and knowledge-grounded response generation. We\nintroduce the data sets and the neural baseline models for three tasks. The\nchallenge track received a total of 105 entries from 24 participating teams. In\nthe evaluation results, the ensemble methods with different large-scale\npretrained language models achieved high performances with improved knowledge\nselection capability and better generalization into unseen data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:57:56 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 17:23:35 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 00:08:27 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Kim", "Seokhwan", ""], ["Eric", "Mihail", ""], ["Hedayatnia", "Behnam", ""], ["Gopalakrishnan", "Karthik", ""], ["Liu", "Yang", ""], ["Huang", "Chao-Wei", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2101.09294", "submitter": "Eddie Yang", "authors": "Eddie Yang, Margaret E. Roberts", "title": "Censorship of Online Encyclopedias: Implications for NLP Models", "comments": "Accepted for publication at ACM FAccT 2021", "journal-ref": null, "doi": "10.1145/3442188.3445916", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While artificial intelligence provides the backbone for many tools people use\naround the world, recent work has brought to attention that the algorithms\npowering AI are not free of politics, stereotypes, and bias. While most work in\nthis area has focused on the ways in which AI can exacerbate existing\ninequalities and discrimination, very little work has studied how governments\nactively shape training data. We describe how censorship has affected the\ndevelopment of Wikipedia corpuses, text data which are regularly used for\npre-trained inputs into NLP algorithms. We show that word embeddings trained on\nBaidu Baike, an online Chinese encyclopedia, have very different associations\nbetween adjectives and a range of concepts about democracy, freedom, collective\naction, equality, and people and historical events in China than its regularly\nblocked but uncensored counterpart - Chinese language Wikipedia. We examine the\nimplications of these discrepancies by studying their use in downstream AI\napplications. Our paper shows how government repression, censorship, and\nself-censorship may impact training data and the applications that draw from\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:09:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Eddie", ""], ["Roberts", "Margaret E.", ""]]}, {"id": "2101.09311", "submitter": "Elena Tutubalina Dr.", "authors": "Zulfat Miftahutdinov, Artur Kadurin, Roman Kudrin, and Elena\n  Tutubalina", "title": "Drug and Disease Interpretation Learning with Biomedical Entity\n  Representation Transformer", "comments": "Accepted to the 43rd European Conference on Information Retrieval\n  (ECIR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept normalization in free-form texts is a crucial step in every\ntext-mining pipeline. Neural architectures based on Bidirectional Encoder\nRepresentations from Transformers (BERT) have achieved state-of-the-art results\nin the biomedical domain. In the context of drug discovery and development,\nclinical trials are necessary to establish the efficacy and safety of drugs. We\ninvestigate the effectiveness of transferring concept normalization from the\ngeneral biomedical domain to the clinical trials domain in a zero-shot setting\nwith an absence of labeled data. We propose a simple and effective two-stage\nneural approach based on fine-tuned BERT architectures. In the first stage, we\ntrain a metric learning model that optimizes relative similarity of mentions\nand concepts via triplet loss. The model is trained on available labeled\ncorpora of scientific abstracts to obtain vector embeddings of concept names\nand entity mentions from texts. In the second stage, we find the closest\nconcept name representation in an embedding space to a given clinical mention.\nWe evaluated several models, including state-of-the-art architectures, on a\ndataset of abstracts and a real-world dataset of trial records with\ninterventions and conditions mapped to drug and disease terminologies.\nExtensive experiments validate the effectiveness of our approach in knowledge\ntransfer from the scientific literature to clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:01:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Miftahutdinov", "Zulfat", ""], ["Kadurin", "Artur", ""], ["Kudrin", "Roman", ""], ["Tutubalina", "Elena", ""]]}, {"id": "2101.09313", "submitter": "James O' Neill", "authors": "James O' Neill and Danushka Bollegala", "title": "$k$-Neighbor Based Curriculum Sampling for Sequence Prediction", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.05916", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-step ahead prediction in language models is challenging due to the\ndiscrepancy between training and test time processes. At test time, a sequence\npredictor is required to make predictions given past predictions as the input,\ninstead of the past targets that are provided during training. This difference,\nknown as exposure bias, can lead to the compounding of errors along a generated\nsequence at test time. To improve generalization in neural language models and\naddress compounding errors, we propose \\textit{Nearest-Neighbor Replacement\nSampling} -- a curriculum learning-based method that gradually changes an\ninitially deterministic teacher policy to a stochastic policy. A token at a\ngiven time-step is replaced with a sampled nearest neighbor of the past target\nwith a truncated probability proportional to the cosine similarity between the\noriginal word and its top $k$ most similar words. This allows the learner to\nexplore alternatives when the current policy provided by the teacher is\nsub-optimal or difficult to learn from. The proposed method is straightforward,\nonline and requires little additional memory requirements. We report our\nfindings on two language modelling benchmarks and find that the proposed method\nfurther improves performance when used in conjunction with scheduled sampling.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:07:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2101.09345", "submitter": "Fouzi Harrag", "authors": "Fouzi Harrag, Maria Debbah, Kareem Darwish, Ahmed Abdelali", "title": "BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets", "comments": null, "journal-ref": "Proceedings of the Fifth Arabic Natural Language Processing\n  Workshop (WANLP @ COLING 2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last two decades, we have progressively turned to the Internet and\nsocial media to find news, entertain conversations and share opinion. Recently,\nOpenAI has developed a ma-chine learning system called GPT-2 for Generative\nPre-trained Transformer-2, which can pro-duce deepfake texts. It can generate\nblocks of text based on brief writing prompts that look like they were written\nby humans, facilitating the spread false or auto-generated text. In line with\nthis progress, and in order to counteract potential dangers, several methods\nhave been pro-posed for detecting text written by these language models. In\nthis paper, we propose a transfer learning based model that will be able to\ndetect if an Arabic sentence is written by humans or automatically generated by\nbots. Our dataset is based on tweets from a previous work, which we have\ncrawled and extended using the Twitter API. We used GPT2-Small-Arabic to\ngenerate fake Arabic Sentences. For evaluation, we compared different recurrent\nneural network (RNN) word embeddings based baseline models, namely: LSTM,\nBI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new\ntransfer-learning model has obtained an accuracy up to 98%. To the best of our\nknowledge, this work is the first study where ARABERT and GPT2 were combined to\ndetect and classify the Arabic auto-generated texts.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 21:50:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Harrag", "Fouzi", ""], ["Debbah", "Maria", ""], ["Darwish", "Kareem", ""], ["Abdelali", "Ahmed", ""]]}, {"id": "2101.09368", "submitter": "Jens Kaiser", "authors": "Jens Kaiser, Sinan Kurtyigit, Serge Kotchourko, Dominik Schlechtweg", "title": "Effects of Pre- and Post-Processing on type-based Embeddings in Lexical\n  Semantic Change Detection", "comments": "9 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lexical semantic change detection is a new and innovative research field. The\noptimal fine-tuning of models including pre- and post-processing is largely\nunclear. We optimize existing models by (i) pre-training on large corpora and\nrefining on diachronic target corpora tackling the notorious small data\nproblem, and (ii) applying post-processing transformations that have been shown\nto improve performance on synchronic tasks. Our results provide a guide for the\napplication and optimization of lexical semantic change detection models across\nvarious learning scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 22:34:15 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 19:32:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kaiser", "Jens", ""], ["Kurtyigit", "Sinan", ""], ["Kotchourko", "Serge", ""], ["Schlechtweg", "Dominik", ""]]}, {"id": "2101.09374", "submitter": "Fanghua Ye", "authors": "Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, Shenghui Li, Emine\n  Yilmaz", "title": "Slot Self-Attentive Dialogue State Tracking", "comments": "11 pages, to appear at The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An indispensable component in task-oriented dialogue systems is the dialogue\nstate tracker, which keeps track of users' intentions in the course of\nconversation. The typical approach towards this goal is to fill in multiple\npre-defined slots that are essential to complete the task. Although various\ndialogue state tracking methods have been proposed in recent years, most of\nthem predict the value of each slot separately and fail to consider the\ncorrelations among slots. In this paper, we propose a slot self-attention\nmechanism that can learn the slot correlations automatically. Specifically, a\nslot-token attention is first utilized to obtain slot-specific features from\nthe dialogue context. Then a stacked slot self-attention is applied on these\nfeatures to learn the correlations among slots. We conduct comprehensive\nexperiments on two multi-domain task-oriented dialogue datasets, including\nMultiWOZ 2.0 and MultiWOZ 2.1. The experimental results demonstrate that our\napproach achieves state-of-the-art performance on both datasets, verifying the\nnecessity and effectiveness of taking slot correlations into consideration.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 22:48:51 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ye", "Fanghua", ""], ["Manotumruksa", "Jarana", ""], ["Zhang", "Qiang", ""], ["Li", "Shenghui", ""], ["Yilmaz", "Emine", ""]]}, {"id": "2101.09421", "submitter": "Gita Sukthankar", "authors": "Ayesha Enayet and Gita Sukthankar", "title": "Analyzing Team Performance with Embeddings from Multiparty Dialogues", "comments": "To be published in the 15th IEEE International Conference on Semantic\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Good communication is indubitably the foundation of effective teamwork. Over\ntime teams develop their own communication styles and often exhibit\nentrainment, a conversational phenomena in which humans synchronize their\nlinguistic choices. This paper examines the problem of predicting team\nperformance from embeddings learned from multiparty dialogues such that teams\nwith similar conflict scores lie close to one another in vector space.\nEmbeddings were extracted from three types of features: 1) dialogue acts 2)\nsentiment polarity 3) syntactic entrainment. Although all of these features can\nbe used to effectively predict team performance, their utility varies by the\nteamwork phase. We separate the dialogues of players playing a cooperative game\ninto stages: 1) early (knowledge building) 2) middle (problem-solving) and 3)\nlate (culmination). Unlike syntactic entrainment, both dialogue act and\nsentiment embeddings are effective for classifying team performance, even\nduring the initial phase. This finding has potential ramifications for the\ndevelopment of conversational agents that facilitate teaming.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 05:18:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Enayet", "Ayesha", ""], ["Sukthankar", "Gita", ""]]}, {"id": "2101.09427", "submitter": "Abhishek Potnis", "authors": "Abhishek V. Potnis, Rajat C. Shinde, Surya S. Durbha", "title": "Towards Natural Language Question Answering over Earth Observation\n  Linked Data using Attention-based Neural Machine Translation", "comments": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2020", "journal-ref": null, "doi": "10.1109/IGARSS39084.2020.9323183", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increase in Geospatial Linked Open Data being adopted and published\nover the web, there is a need to develop intuitive interfaces and systems for\nseamless and efficient exploratory analysis of such rich heterogeneous\nmulti-modal datasets. This work is geared towards improving the exploration\nprocess of Earth Observation (EO) Linked Data by developing a natural language\ninterface to facilitate querying. Questions asked over Earth Observation Linked\nData have an inherent spatio-temporal dimension and can be represented using\nGeoSPARQL. This paper seeks to study and analyze the use of RNN-based neural\nmachine translation with attention for transforming natural language questions\ninto GeoSPARQL queries. Specifically, it aims to assess the feasibility of a\nneural approach for identifying and mapping spatial predicates in natural\nlanguage to GeoSPARQL's topology vocabulary extension including - Egenhofer and\nRCC8 relations. The queries can then be executed over a triple store to yield\nanswers for the natural language questions. A dataset consisting of mappings\nfrom natural language questions to GeoSPARQL queries over the Corine Land\nCover(CLC) Linked Data has been created to train and validate the deep neural\nnetwork. From our experiments, it is evident that neural machine translation\nwith attention is a promising approach for the task of translating spatial\npredicates in natural language questions to GeoSPARQL queries.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 06:12:20 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Potnis", "Abhishek V.", ""], ["Shinde", "Rajat C.", ""], ["Durbha", "Surya S.", ""]]}, {"id": "2101.09449", "submitter": "Rajdeep Mukherjee", "authors": "Rajdeep Mukherjee, Shreyas Shetty, Subrata Chattopadhyay, Subhadeep\n  Maji, Samik Datta and Pawan Goyal", "title": "Reproducibility, Replicability and Beyond: Assessing Production\n  Readiness of Aspect Based Sentiment Analysis in the Wild", "comments": "12 pages, accepted at ECIR 2021", "journal-ref": null, "doi": "10.1007/978-3-030-72240-1_7", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the exponential growth of online marketplaces and user-generated content\ntherein, aspect-based sentiment analysis has become more important than ever.\nIn this work, we critically review a representative sample of the models\npublished during the past six years through the lens of a practitioner, with an\neye towards deployment in production. First, our rigorous empirical evaluation\nreveals poor reproducibility: an average 4-5% drop in test accuracy across the\nsample. Second, to further bolster our confidence in empirical evaluation, we\nreport experiments on two challenging data slices, and observe a consistent\n12-55% drop in accuracy. Third, we study the possibility of transfer across\ndomains and observe that as little as 10-25% of the domain-specific training\ndataset, when used in conjunction with datasets from other domains within the\nsame locale, largely closes the gap between complete cross-domain and complete\nin-domain predictive performance. Lastly, we open-source two large-scale\nannotated review corpora from a large e-commerce portal in India in order to\naid the study of replicability and transfer, with the hope that it will fuel\nfurther growth of the field.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:45:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Mukherjee", "Rajdeep", ""], ["Shetty", "Shreyas", ""], ["Chattopadhyay", "Subrata", ""], ["Maji", "Subhadeep", ""], ["Datta", "Samik", ""], ["Goyal", "Pawan", ""]]}, {"id": "2101.09459", "submitter": "Chongming Gao", "authors": "Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, Tat-Seng\n  Chua", "title": "Advances and Challenges in Conversational Recommender Systems: A Survey", "comments": "33 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n  Considerable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 08:53:15 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:26:00 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 09:10:08 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 15:45:37 GMT"}, {"version": "v5", "created": "Sun, 7 Feb 2021 03:58:16 GMT"}, {"version": "v6", "created": "Thu, 27 May 2021 04:10:53 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gao", "Chongming", ""], ["Lei", "Wenqiang", ""], ["He", "Xiangnan", ""], ["de Rijke", "Maarten", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2101.09464", "submitter": "Akanksha Nalhotra", "authors": "Akanksha Malhotra and Sudhir Kamle", "title": "ARTH: Algorithm For Reading Text Handily -- An AI Aid for People having\n  Word Processing Issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this project is to solve one of the major problems faced by\nthe people having word processing issues like trauma, or mild mental\ndisability. \"ARTH\" is the short form of Algorithm for Reading Handily. ARTH is\na self-learning set of algorithms that is an intelligent way of fulfilling the\nneed for \"reading and understanding the text effortlessly\" which adjusts\naccording to the needs of every user. The research project propagates in two\nsteps. In the first step, the algorithm tries to identify the difficult words\npresent in the text based on two features -- the number of syllables and usage\nfrequency -- using a clustering algorithm. After the analysis of the clusters,\nthe algorithm labels these clusters, according to their difficulty level. In\nthe second step, the algorithm interacts with the user. It aims to test the\nuser's comprehensibility of the text and his/her vocabulary level by taking an\nautomatically generated quiz. The algorithm identifies the clusters which are\ndifficult for the user, based on the result of the analysis. The meaning of\nperceived difficult words is displayed next to them. The technology \"ARTH\"\nfocuses on the revival of the joy of reading among those people, who have a\npoor vocabulary or any word processing issues.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 09:39:45 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Malhotra", "Akanksha", ""], ["Kamle", "Sudhir", ""]]}, {"id": "2101.09465", "submitter": "Lu Chen", "authors": "Lu Chen, Xingyu Chen, Zihan Zhao, Danyang Zhang, Jiabao Ji, Ao Luo,\n  Yuxuan Xiong, Kai Yu", "title": "WebSRC: A Dataset for Web-Based Structural Reading Comprehension", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search is an essential way for human to obtain information, but it's\nstill a great challenge for machines to understand the contents of web pages.\nIn this paper, we introduce the task of web-based structural reading\ncomprehension. Given a web page and a question about it, the task is to find an\nanswer from the web page. This task requires a system not only to understand\nthe semantics of texts but also the structure of the web page. Moreover, we\nproposed WebSRC, a novel Web-based Structural Reading Comprehension dataset.\nWebSRC consists of 0.44M question-answer pairs, which are collected from 6.5K\nweb pages with corresponding HTML source code, screenshots, and metadata. Each\nquestion in WebSRC requires a certain structural understanding of a web page to\nanswer, and the answer is either a text span on the web page or yes/no. We\nevaluate various strong baselines on our dataset to show the difficulty of our\ntask. We also investigate the usefulness of structural information and visual\nfeatures. Our dataset and task are publicly available at\nhttps://speechlab-sjtu.github.io/WebSRC/.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 09:43:44 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Lu", ""], ["Chen", "Xingyu", ""], ["Zhao", "Zihan", ""], ["Zhang", "Danyang", ""], ["Ji", "Jiabao", ""], ["Luo", "Ao", ""], ["Xiong", "Yuxuan", ""], ["Yu", "Kai", ""]]}, {"id": "2101.09469", "submitter": "Victor Junqiu Wei", "authors": "Junqiu Wei, Qun Liu, Yinpeng Guo, Xin Jiang", "title": "Training Multilingual Pre-trained Language Model with Byte-level\n  Subwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pre-trained language models have achieved great successes in various\nnatural language understanding (NLU) tasks due to its capacity to capture the\ndeep contextualized information in text by pre-training on large-scale corpora.\nOne of the fundamental components in pre-trained language models is the\nvocabulary, especially for training multilingual models on many different\nlanguages. In the technical report, we present our practices on training\nmultilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte\nPair Encoding). In the experiment, we adopted the architecture of NEZHA as the\nunderlying pre-trained language model and the results show that NEZHA trained\nwith byte-level subwords consistently outperforms Google multilingual BERT and\nvanilla NEZHA by a notable margin in several multilingual NLU tasks. We release\nthe source code of our byte-level vocabulary building tools and the\nmultilingual pre-trained language models.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 10:01:28 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:37:37 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wei", "Junqiu", ""], ["Liu", "Qun", ""], ["Guo", "Yinpeng", ""], ["Jiang", "Xin", ""]]}, {"id": "2101.09523", "submitter": "Masahiro Kaneko", "authors": "Masahiro Kaneko and Danushka Bollegala", "title": "Debiasing Pre-trained Contextualised Embeddings", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In comparison to the numerous debiasing methods proposed for the static\nnon-contextualised word embeddings, the discriminative biases in contextualised\nembeddings have received relatively little attention. We propose a fine-tuning\nmethod that can be applied at token- or sentence-levels to debias pre-trained\ncontextualised embeddings. Our proposed method can be applied to any\npre-trained contextualised embedding model, without requiring to retrain those\nmodels. Using gender bias as an illustrative example, we then conduct a\nsystematic study using several state-of-the-art (SoTA) contextualised\nrepresentations on multiple benchmark datasets to evaluate the level of biases\nencoded in different contextualised embeddings before and after debiasing using\nthe proposed method. We find that applying token-level debiasing for all tokens\nand across all layers of a contextualised embedding model produces the best\nperformance. Interestingly, we observe that there is a trade-off between\ncreating an accurate vs. unbiased contextualised embedding model, and different\ncontextualised embedding models respond differently to this trade-off.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 15:28:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kaneko", "Masahiro", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2101.09525", "submitter": "Masahiro Kaneko", "authors": "Masahiro Kaneko and Danushka Bollegala", "title": "Dictionary-based Debiasing of Pre-trained Word Embeddings", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word embeddings trained on large corpora have shown to encode high levels of\nunfair discriminatory gender, racial, religious and ethnic biases.\n  In contrast, human-written dictionaries describe the meanings of words in a\nconcise, objective and an unbiased manner.\n  We propose a method for debiasing pre-trained word embeddings using\ndictionaries, without requiring access to the original training resources or\nany knowledge regarding the word embedding algorithms used.\n  Unlike prior work, our proposed method does not require the types of biases\nto be pre-defined in the form of word lists, and learns the constraints that\nmust be satisfied by unbiased word embeddings automatically from dictionary\ndefinitions of the words.\n  Specifically, we learn an encoder to generate a debiased version of an input\nword embedding such that it\n  (a) retains the semantics of the pre-trained word embeddings,\n  (b) agrees with the unbiased definition of the word according to the\ndictionary, and\n  (c) remains orthogonal to the vector space spanned by any biased basis\nvectors in the pre-trained word embedding space.\n  Experimental results on standard benchmark datasets show that the proposed\nmethod can accurately remove unfair biases encoded in pre-trained word\nembeddings, while preserving useful semantics.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 15:44:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kaneko", "Masahiro", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2101.09579", "submitter": "Idan Rejwan", "authors": "Idan Rejwan and Avi Caciularu", "title": "On the Evolution of Word Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most natural languages have a predominant or fixed word order. For example,\nin English, the word order used most often is Subject-Verb-Object. This work\nattempts to explain this phenomena as well as other typological findings\nregarding word order from a functional perspective. That is, we target the\nquestion of whether fixed word order gives a functional advantage, that may\nexplain why these languages are common. To this end, we consider an\nevolutionary model of language and show, both theoretically and using a genetic\nalgorithm-based simulation, that an optimal language is one with fixed word\norder. We also show that adding information to the sentence, such as case\nmarkers and noun-verb distinction, reduces the need for fixed word order, in\naccordance with the typological findings.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 20:30:17 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rejwan", "Idan", ""], ["Caciularu", "Avi", ""]]}, {"id": "2101.09624", "submitter": "Taejin Park", "authors": "Tae Jin Park, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J. Han, Shinji\n  Watanabe, Shrikanth Narayanan", "title": "A Review of Speaker Diarization: Recent Advances with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization is a task to label audio or video recordings with classes\nthat correspond to speaker identity, or in short, a task to identify \"who spoke\nwhen\". In the early years, speaker diarization algorithms were developed for\nspeech recognition on multispeaker audio recordings to enable speaker adaptive\nprocessing. These algorithms also gained their own value as a standalone\napplication over time to provide speaker-specific metainformation for\ndownstream tasks such as audio retrieval. More recently, with the emergence of\ndeep learning technology, which has driven revolutionary changes in research\nand practices across speech application domains, rapid advancements have been\nmade for speaker diarization. In this paper, we review not only the historical\ndevelopment of speaker diarization technology but also the recent advancements\nin neural speaker diarization approaches. Furthermore, we discuss how speaker\ndiarization systems have been integrated with speech recognition applications\nand how the recent surge of deep learning is leading the way of jointly\nmodeling these two components to be complementary to each other. By considering\nsuch exciting technical trends, we believe that this paper is a valuable\ncontribution to the community to provide a survey work by consolidating the\nrecent developments with neural methods and thus facilitating further progress\ntoward a more efficient speaker diarization.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 01:28:05 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:40:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Park", "Tae Jin", ""], ["Kanda", "Naoyuki", ""], ["Dimitriadis", "Dimitrios", ""], ["Han", "Kyu J.", ""], ["Watanabe", "Shinji", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2101.09635", "submitter": "Lalita Lowphansirikul", "authors": "Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, Sarana\n  Nutanong", "title": "WangchanBERTa: Pretraining transformer-based Thai Language Models", "comments": "24 pages, edited the citation of the syllable-level tokenizer from\n  [Chormai et al., 2020] to [Phatthiyaphaibun et al., 2020] as the authors used\n  the syllable-level tokenizer from PyThaiNLP [Phatthiyaphaibun et al., 2020]\n  in the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based language models, more specifically BERT-based architectures\nhave achieved state-of-the-art performance in many downstream tasks. However,\nfor a relatively low-resource language such as Thai, the choices of models are\nlimited to training a BERT-based model based on a much smaller dataset or\nfinetuning multi-lingual models, both of which yield suboptimal downstream\nperformance. Moreover, large-scale multi-lingual pretraining does not take into\naccount language-specific features for Thai. To overcome these limitations, we\npretrain a language model based on RoBERTa-base architecture on a large,\ndeduplicated, cleaned training set (78GB in total size), curated from diverse\ndomains of social media posts, news articles and other publicly available\ndatasets. We apply text processing rules that are specific to Thai most\nimportantly preserving spaces, which are important chunk and sentence\nboundaries in Thai before subword tokenization. We also experiment with\nword-level, syllable-level and SentencePiece tokenization with a smaller\ndataset to explore the effects on tokenization on downstream performance. Our\nmodel wangchanberta-base-att-spm-uncased trained on the 78.5GB dataset\noutperforms strong baselines (NBSVM, CRF and ULMFit) and multi-lingual models\n(XLMR and mBERT) on both sequence classification and token classification tasks\nin human-annotated, mono-lingual contexts.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 03:06:34 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 04:14:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lowphansirikul", "Lalita", ""], ["Polpanumas", "Charin", ""], ["Jantrakulchai", "Nawat", ""], ["Nutanong", "Sarana", ""]]}, {"id": "2101.09647", "submitter": "Jatin Ganhotra", "authors": "Jatin Ganhotra, Sachindra Joshi", "title": "Does Dialog Length matter for Next Response Selection task? An Empirical\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the last few years, the release of BERT, a multilingual transformer based\nmodel, has taken the NLP community by storm. BERT-based models have achieved\nstate-of-the-art results on various NLP tasks, including dialog tasks. One of\nthe limitation of BERT is the lack of ability to handle long text sequence. By\ndefault, BERT has a maximum wordpiece token sequence length of 512. Recently,\nthere has been renewed interest to tackle the BERT limitation to handle long\ntext sequences with the addition of new self-attention based architectures.\nHowever, there has been little to no research on the impact of this limitation\nwith respect to dialog tasks. Dialog tasks are inherently different from other\nNLP tasks due to: a) the presence of multiple utterances from multiple\nspeakers, which may be interlinked to each other across different turns and b)\nlonger length of dialogs. In this work, we empirically evaluate the impact of\ndialog length on the performance of BERT model for the Next Response Selection\ndialog task on four publicly available and one internal multi-turn dialog\ndatasets. We observe that there is little impact on performance with long\ndialogs and even the simplest approach of truncating input works really well.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 05:39:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ganhotra", "Jatin", ""], ["Joshi", "Sachindra", ""]]}, {"id": "2101.09656", "submitter": "Aobo Yang", "authors": "Aobo Yang, Nan Wang, Hongbo Deng, Hongning Wang", "title": "Explanation as a Defense of Recommendation", "comments": "WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441726", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Textual explanations have proved to help improve user satisfaction on\nmachine-made recommendations. However, current mainstream solutions loosely\nconnect the learning of explanation with the learning of recommendation: for\nexample, they are often separately modeled as rating prediction and content\ngeneration tasks. In this work, we propose to strengthen their connection by\nenforcing the idea of sentiment alignment between a recommendation and its\ncorresponding explanation. At training time, the two learning tasks are joined\nby a latent sentiment vector, which is encoded by the recommendation module and\nused to make word choices for explanation generation. At both training and\ninference time, the explanation module is required to generate explanation text\nthat matches sentiment predicted by the recommendation module. Extensive\nexperiments demonstrate our solution outperforms a rich set of baselines in\nboth recommendation and explanation tasks, especially on the improved quality\nof its generated explanations. More importantly, our user studies confirm our\ngenerated explanations help users better recognize the differences between\nrecommended items and understand why an item is recommended.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 06:34:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Aobo", ""], ["Wang", "Nan", ""], ["Deng", "Hongbo", ""], ["Wang", "Hongning", ""]]}, {"id": "2101.09667", "submitter": "Md Abul Bashar", "authors": "Fahim Shahriar, Md Abul Bashar", "title": "Automatic Monitoring Social Dynamics During Big Incidences: A Case Study\n  of COVID-19 in Bangladesh", "comments": "Very minor change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Newspapers are trustworthy media where people get the most reliable and\ncredible information compared with other sources. On the other hand, social\nmedia often spread rumors and misleading news to get more traffic and\nattention. Careful characterization, evaluation, and interpretation of\nnewspaper data can provide insight into intrigue and passionate social issues\nto monitor any big social incidence. This study analyzed a large set of\nspatio-temporal Bangladeshi newspaper data related to the COVID-19 pandemic.\nThe methodology included volume analysis, topic analysis, automated\nclassification, and sentiment analysis of news articles to get insight into the\nCOVID-19 pandemic in different sectors and regions in Bangladesh over a period\nof time. This analysis will help the government and other organizations to\nfigure out the challenges that have arisen in society due to this pandemic,\nwhat steps should be taken immediately and in the post-pandemic period, how the\ngovernment and its allies can come together to address the crisis in the\nfuture, keeping these problems in mind.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 07:46:17 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 16:47:37 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shahriar", "Fahim", ""], ["Bashar", "Md Abul", ""]]}, {"id": "2101.09688", "submitter": "Pasquale Minervini", "authors": "Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van\n  Breugel, Pasquale Minervini", "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and\n  Fine-tuned Language Models", "comments": "Proceedings of the 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes two intuitive metrics, skew and stereotype, that quantify\nand analyse the gender bias present in contextual language models when tackling\nthe WinoBias pronoun resolution task. We find evidence that gender stereotype\ncorrelates approximately negatively with gender skew in out-of-the-box models,\nsuggesting that there is a trade-off between these two forms of bias. We\ninvestigate two methods to mitigate bias. The first approach is an online\nmethod which is effective at removing skew at the expense of stereotype. The\nsecond, inspired by previous work on ELMo, involves the fine-tuning of BERT\nusing an augmented gender-balanced dataset. We show that this reduces both skew\nand stereotype relative to its unaugmented fine-tuned counterpart. However, we\nfind that existing gender bias benchmarks do not fully probe professional bias\nas pronoun resolution may be obfuscated by cross-correlations from other\nmanifestations of gender prejudice. Our code is available online, at\nhttps://github.com/12kleingordon34/NLP_masters_project.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 10:57:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:17:41 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Manela", "Daniel de Vassimon", ""], ["Errington", "David", ""], ["Fisher", "Thomas", ""], ["van Breugel", "Boris", ""], ["Minervini", "Pasquale", ""]]}, {"id": "2101.09693", "submitter": "Mohsen Ahmadzadeh", "authors": "Mohsen Ahmadzadeh, Mehdi Kamal, Ali Afzali-Kusha, Massoud Pedram", "title": "A2P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented\n  Neural Networks", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, to limit the number of required attention inference hops in\nmemory-augmented neural networks, we propose an online adaptive approach called\nA2P-MANN. By exploiting a small neural network classifier, an adequate number\nof attention inference hops for the input query is determined. The technique\nresults in elimination of a large number of unnecessary computations in\nextracting the correct answer. In addition, to further lower computations in\nA2P-MANN, we suggest pruning weights of the final FC (fully-connected) layers.\nTo this end, two pruning approaches, one with negligible accuracy loss and the\nother with controllable loss on the final accuracy, are developed. The efficacy\nof the technique is assessed by using the twenty question-answering (QA) tasks\nof bAbI dataset. The analytical assessment reveals, on average, more than 42%\nfewer computations compared to the baseline MANN at the cost of less than 1%\naccuracy loss. In addition, when used along with the previously published\nzero-skipping technique, a computation count reduction of up to 68% is\nachieved. Finally, when the proposed approach (without zero-skipping) is\nimplemented on the CPU and GPU platforms, up to 43% runtime reduction is\nachieved.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:02:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ahmadzadeh", "Mohsen", ""], ["Kamal", "Mehdi", ""], ["Afzali-Kusha", "Ali", ""], ["Pedram", "Massoud", ""]]}, {"id": "2101.09698", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Hanqing Lu", "title": "Fast Sequence Generation with Multi-Agent Reinforcement Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.04690", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoregressive sequence Generation models have achieved state-of-the-art\nperformance in areas like machine translation and image captioning. These\nmodels are autoregressive in that they generate each word by conditioning on\npreviously generated words, which leads to heavy latency during inference.\nRecently, non-autoregressive decoding has been proposed in machine translation\nto speed up the inference time by generating all words in parallel. Typically,\nthese models use the word-level cross-entropy loss to optimize each word\nindependently. However, such a learning process fails to consider the\nsentence-level consistency, thus resulting in inferior generation quality of\nthese non-autoregressive models. In this paper, we propose a simple and\nefficient model for Non-Autoregressive sequence Generation (NAG) with a novel\ntraining paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL\nformulates NAG as a multi-agent reinforcement learning system where element\npositions in the target sequence are viewed as agents that learn to\ncooperatively maximize a sentence-level reward. On MSCOCO image captioning\nbenchmark, our NAG method achieves a performance comparable to state-of-the-art\nautoregressive models, while brings 13.9x decoding speedup. On WMT14 EN-DE\nmachine translation dataset, our method outperforms cross-entropy trained\nbaseline by 6.0 BLEU points while achieves the greatest decoding speedup of\n17.46x.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:16:45 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["Lu", "Hanqing", ""]]}, {"id": "2101.09704", "submitter": "Lin Xiao", "authors": "Lin Xiao, Xiangliang Zhang, Liping Jing, Chi Huang, Mingyang Song", "title": "Does Head Label Help for Long-Tailed Multi-Label Text Classification", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label text classification (MLTC) aims to annotate documents with the\nmost relevant labels from a number of candidate labels. In real applications,\nthe distribution of label frequency often exhibits a long tail, i.e., a few\nlabels are associated with a large number of documents (a.k.a. head labels),\nwhile a large fraction of labels are associated with a small number of\ndocuments (a.k.a. tail labels). To address the challenge of insufficient\ntraining data on tail label classification, we propose a Head-to-Tail Network\n(HTTN) to transfer the meta-knowledge from the data-rich head labels to\ndata-poor tail labels. The meta-knowledge is the mapping from few-shot network\nparameters to many-shot network parameters, which aims to promote the\ngeneralizability of tail classifiers. Extensive experimental results on three\nbenchmark datasets demonstrate that HTTN consistently outperforms the\nstate-of-the-art methods. The code and hyper-parameter settings are released\nfor reproducibility\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:31:39 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xiao", "Lin", ""], ["Zhang", "Xiangliang", ""], ["Jing", "Liping", ""], ["Huang", "Chi", ""], ["Song", "Mingyang", ""]]}, {"id": "2101.09743", "submitter": "Rajkumar Pujari", "authors": "Rajkumar Pujari and Swara Desai and Niloy Ganguly and Pawan Goyal", "title": "A Novel Two-stage Framework for Extracting Opinionated Sentences from\n  News Articles", "comments": "Presented as a talk at TextGraphs-9: the workshop on Graph-based\n  Methods for Natural Language Processing at EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel two-stage framework to extract opinionated\nsentences from a given news article. In the first stage, Naive Bayes classifier\nby utilizing the local features assigns a score to each sentence - the score\nsignifies the probability of the sentence to be opinionated. In the second\nstage, we use this prior within the HITS (Hyperlink-Induced Topic Search)\nschema to exploit the global structure of the article and relation between the\nsentences. In the HITS schema, the opinionated sentences are treated as Hubs\nand the facts around these opinions are treated as the Authorities. The\nalgorithm is implemented and evaluated against a set of manually marked data.\nWe show that using HITS significantly improves the precision over the baseline\nNaive Bayes classifier. We also argue that the proposed method actually\ndiscovers the underlying structure of the article, thus extracting various\nopinions, grouped with supporting facts as well as other supporting opinions\nfrom the article.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:24:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pujari", "Rajkumar", ""], ["Desai", "Swara", ""], ["Ganguly", "Niloy", ""], ["Goyal", "Pawan", ""]]}, {"id": "2101.09755", "submitter": "Shijie Geng", "authors": "Shijie Geng, Peng Gao, Zuohui Fu, Yongfeng Zhang", "title": "RomeBERT: Robust Training of Multi-Exit BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT has achieved superior performances on Natural Language Understanding\n(NLU) tasks. However, BERT possesses a large number of parameters and demands\ncertain resources to deploy. For acceleration, Dynamic Early Exiting for BERT\n(DeeBERT) has been proposed recently, which incorporates multiple exits and\nadopts a dynamic early-exit mechanism to ensure efficient inference. While\nobtaining an efficiency-performance tradeoff, the performances of early exits\nin multi-exit BERT are significantly worse than late exits. In this paper, we\nleverage gradient regularized self-distillation for RObust training of\nMulti-Exit BERT (RomeBERT), which can effectively solve the performance\nimbalance problem between early and late exits. Moreover, the proposed RomeBERT\nadopts a one-stage joint training strategy for multi-exits and the BERT\nbackbone while DeeBERT needs two stages that require more training time.\nExtensive experiments on GLUE datasets are performed to demonstrate the\nsuperiority of our approach. Our code is available at\nhttps://github.com/romebert/RomeBERT.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:03:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Geng", "Shijie", ""], ["Gao", "Peng", ""], ["Fu", "Zuohui", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.09763", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, Dawei Zhu, Dietrich Klakow", "title": "Analysing the Noise Model Error for Realistic Noisy Label Data", "comments": "Accepted at AAAI 2021, additional material at\n  https://github.com/uds-lsv/noise-estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant and weak supervision allow to obtain large amounts of labeled\ntraining data quickly and cheaply, but these automatic annotations tend to\ncontain a high amount of errors. A popular technique to overcome the negative\neffects of these noisy labels is noise modelling where the underlying noise\nprocess is modelled. In this work, we study the quality of these estimated\nnoise models from the theoretical side by deriving the expected error of the\nnoise model. Apart from evaluating the theoretical results on commonly used\nsynthetic noise, we also publish NoisyNER, a new noisy label dataset from the\nNLP domain that was obtained through a realistic distant supervision technique.\nIt provides seven sets of labels with differing noise patterns to evaluate\ndifferent noise levels on the same instances. Parallel, clean labels are\navailable making it possible to study scenarios where a small amount of\ngold-standard data can be leveraged. Our theoretical results and the\ncorresponding experiments give insights into the factors that influence the\nnoise model estimation like the noise distribution and the sampling technique.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:45:15 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 11:14:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Zhu", "Dawei", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2101.09765", "submitter": "Milad Alshomary", "authors": "Milad Alshomary, Wei-Fan Chen, Timon Gurcke, and Henning Wachsmuth", "title": "Belief-based Generation of Argumentative Claims", "comments": "Almost 9 pages, 1 figure, EACL-21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When engaging in argumentative discourse, skilled human debaters tailor\nclaims to the beliefs of the audience, to construct effective arguments.\nRecently, the field of computational argumentation witnessed extensive effort\nto address the automatic generation of arguments. However, existing approaches\ndo not perform any audience-specific adaptation. In this work, we aim to bridge\nthis gap by studying the task of belief-based claim generation: Given a\ncontroversial topic and a set of beliefs, generate an argumentative claim\ntailored to the beliefs. To tackle this task, we model the people's prior\nbeliefs through their stances on controversial topics and extend\nstate-of-the-art text generation models to generate claims conditioned on the\nbeliefs. Our automatic evaluation confirms the ability of our approach to adapt\nclaims to a set of given beliefs. In a manual study, we additionally evaluate\nthe generated claims in terms of informativeness and their likelihood to be\nuttered by someone with a respective belief. Our results reveal the limitations\nof modeling users' beliefs based on their stances, but demonstrate the\npotential of encoding beliefs into argumentative texts, laying the ground for\nfuture exploration of audience reach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 18:07:02 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:03:53 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Alshomary", "Milad", ""], ["Chen", "Wei-Fan", ""], ["Gurcke", "Timon", ""], ["Wachsmuth", "Henning", ""]]}, {"id": "2101.09773", "submitter": "Hongyin Luo", "authors": "Hongyin Luo, Shang-Wen Li, James Glass", "title": "Knowledge Grounded Conversational Symptom Detection with Graph Memory\n  Networks", "comments": "Appears in the Proceedings of the 3rd Clinical Natural Language\n  Processing Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel goal-oriented dialog task, automatic symptom\ndetection. We build a system that can interact with patients through dialog to\ndetect and collect clinical symptoms automatically, which can save a doctor's\ntime interviewing the patient. Given a set of explicit symptoms provided by the\npatient to initiate a dialog for diagnosing, the system is trained to collect\nimplicit symptoms by asking questions, in order to collect more information for\nmaking an accurate diagnosis. After getting the reply from the patient for each\nquestion, the system also decides whether current information is enough for a\nhuman doctor to make a diagnosis. To achieve this goal, we propose two neural\nmodels and a training pipeline for the multi-step reasoning task. We also build\na knowledge graph as additional inputs to further improve model performance.\nExperiments show that our model significantly outperforms the baseline by 4%,\ndiscovering 67% of implicit symptoms on average with a limited number of\nquestions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 18:50:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Luo", "Hongyin", ""], ["Li", "Shang-Wen", ""], ["Glass", "James", ""]]}, {"id": "2101.09788", "submitter": "Stephan Meylan", "authors": "Stephan C. Meylan, Sathvik Nair, Thomas L. Griffiths", "title": "Evaluating Models of Robust Word Recognition with Serial Reproduction", "comments": null, "journal-ref": "Cognition Volume 210, May 2021, 104553", "doi": "10.1016/j.cognition.2020.104553", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken communication occurs in a \"noisy channel\" characterized by high levels\nof environmental noise, variability within and between speakers, and lexical\nand syntactic ambiguity. Given these properties of the received linguistic\ninput, robust spoken word recognition -- and language processing more generally\n-- relies heavily on listeners' prior knowledge to evaluate whether candidate\ninterpretations of that input are more or less likely. Here we compare several\nbroad-coverage probabilistic generative language models in their ability to\ncapture human linguistic expectations. Serial reproduction, an experimental\nparadigm where spoken utterances are reproduced by successive participants\nsimilar to the children's game of \"Telephone,\" is used to elicit a sample that\nreflects the linguistic expectations of English-speaking adults. When we\nevaluate a suite of probabilistic generative language models against the\nyielded chains of utterances, we find that those models that make use of\nabstract representations of preceding linguistic context (i.e., phrase\nstructure) best predict the changes made by people in the course of serial\nreproduction. A logistic regression model predicting which words in an\nutterance are most likely to be lost or changed in the course of spoken\ntransmission corroborates this result. We interpret these findings in light of\nresearch highlighting the interaction of memory-based constraints and\nrepresentations in language processing.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:16:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Meylan", "Stephan C.", ""], ["Nair", "Sathvik", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "2101.09810", "submitter": "Bilal Ghanem", "authors": "Bilal Ghanem, Simone Paolo Ponzetto, Paolo Rosso, Francisco Rangel", "title": "FakeFlow: Fake News Detection by Modeling the Flow of Affective\n  Information", "comments": "9 pages, 6 figures, EACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news articles often stir the readers' attention by means of emotional\nappeals that arouse their feelings. Unlike in short news texts, authors of\nlonger articles can exploit such affective factors to manipulate readers by\nadding exaggerations or fabricating events, in order to affect the readers'\nemotions. To capture this, we propose in this paper to model the flow of\naffective information in fake news articles using a neural architecture. The\nproposed model, FakeFlow, learns this flow by combining topic and affective\ninformation extracted from text. We evaluate the model's performance with\nseveral experiments on four real-world datasets. The results show that FakeFlow\nachieves superior results when compared against state-of-the-art methods, thus\nconfirming the importance of capturing the flow of the affective information in\nnews articles.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 21:55:28 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ghanem", "Bilal", ""], ["Ponzetto", "Simone Paolo", ""], ["Rosso", "Paolo", ""], ["Rangel", "Francisco", ""]]}, {"id": "2101.09865", "submitter": "Yufei Wang", "authors": "Yufei Wang and Ian D. Wood and Stephen Wan and Mark Johnson", "title": "ECOL-R: Encouraging Copying in Novel Object Captioning with\n  Reinforcement Learning", "comments": "long paper accepted @ EACL-2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novel Object Captioning is a zero-shot Image Captioning task requiring\ndescribing objects not seen in the training captions, but for which information\nis available from external object detectors. The key challenge is to select and\ndescribe all salient detected novel objects in the input images. In this paper,\nwe focus on this challenge and propose the ECOL-R model (Encouraging Copying of\nObject Labels with Reinforced Learning), a copy-augmented transformer model\nthat is encouraged to accurately describe the novel object labels. This is\nachieved via a specialised reward function in the SCST reinforcement learning\nframework (Rennie et al., 2017) that encourages novel object mentions while\nmaintaining the caption quality. We further restrict the SCST training to the\nimages where detected objects are mentioned in reference captions to train the\nECOL-R model. We additionally improve our copy mechanism via Abstract Labels,\nwhich transfer knowledge from known to novel object types, and a Morphological\nSelector, which determines the appropriate inflected forms of novel object\nlabels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et\nal., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:41:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Yufei", ""], ["Wood", "Ian D.", ""], ["Wan", "Stephen", ""], ["Johnson", "Mark", ""]]}, {"id": "2101.09893", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Franck Dernoncourt, Walter Chang, Thien Huu\n  Nguyen", "title": "MadDog: A Web-based System for Acronym Identification and Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acronyms and abbreviations are the short-form of longer phrases and they are\nubiquitously employed in various types of writing. Despite their usefulness to\nsave space in writing and reader's time in reading, they also provide\nchallenges for understanding the text especially if the acronym is not defined\nin the text or if it is used far from its definition in long texts. To\nalleviate this issue, there are considerable efforts both from the research\ncommunity and software developers to build systems for identifying acronyms and\nfinding their correct meanings in the text. However, none of the existing works\nprovide a unified solution capable of processing acronyms in various domains\nand to be publicly available. Thus, we provide the first web-based acronym\nidentification and disambiguation system which can process acronyms from\nvarious domains including scientific, biomedical, and general domains. The\nweb-based system is publicly available at http://iq.cs.uoregon.edu:5000 and a\ndemo video is available at https://youtu.be/IkSh7LqI42M. The system source code\nis also available at https://github.com/amirveyseh/MadDog.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 04:49:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Dernoncourt", "Franck", ""], ["Chang", "Walter", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2101.09901", "submitter": "Hexin Cao", "authors": "Liang Zhao, Hexin Cao, Yunsong Zhao", "title": "GP: Context-free Grammar Pre-training for Text-to-SQL Parsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for Text-to-SQL parsing, Grammar Pre-training (GP), is proposed\nto decode deep relations between question and database. Firstly, to better\nutilize the information of databases, a random value is added behind a question\nword which is recognized as a column, and the new sentence serves as the model\ninput. Secondly, initialization of vectors for decoder part is optimized, with\nreference to the former encoding so that question information can be concerned.\nFinally, a new approach called flooding level is adopted to get the non-zero\ntraining loss which can generalize better results. By encoding the sentence\nwith GRAPPA and RAT-SQL model, we achieve better performance on spider, a\ncross-DB Text-to-SQL dataset (72.8 dev, 69.8 test). Experiments show that our\nmethod is easier to converge during training and has excellent robustness.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 05:41:31 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 15:35:56 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 01:41:14 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhao", "Liang", ""], ["Cao", "Hexin", ""], ["Zhao", "Yunsong", ""]]}, {"id": "2101.09914", "submitter": "Lei Huang", "authors": "Lei Huang, Jiecong Lin, Xiangtao Li, Linqi Song and Ka-Chun Wong", "title": "EGFI: Drug-Drug Interaction Extraction and Generation with Fusion of\n  Enriched Entity and Sentence Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth in literature accumulates diverse and yet comprehensive\nbiomedical knowledge hidden to be mined such as drug interactions. However, it\nis difficult to extract the heterogeneous knowledge to retrieve or even\ndiscover the latest and novel knowledge in an efficient manner. To address such\na problem, we propose EGFI for extracting and consolidating drug interactions\nfrom large-scale medical literature text data. Specifically, EGFI consists of\ntwo parts: classification and generation. In the classification part, EGFI\nencompasses the language model BioBERT which has been comprehensively\npre-trained on biomedical corpus. In particular, we propose the multi-head\nattention mechanism and pack BiGRU to fuse multiple semantic information for\nrigorous context modeling. In the generation part, EGFI utilizes another\npre-trained language model BioGPT-2 where the generation sentences are selected\nbased on filtering rules. We evaluated the classification part on \"DDIs 2013\"\ndataset and \"DTIs\" dataset, achieving the FI score of 0.842 and 0.720\nrespectively. Moreover, we applied the classification part to distinguish\nhigh-quality generated sentences and verified with the exiting growth truth to\nconfirm the filtered sentences. The generated sentences that are not recorded\nin DrugBank and DDIs 2013 dataset also demonstrate the potential of EGFI to\nidentify novel drug relationships.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 06:52:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Huang", "Lei", ""], ["Lin", "Jiecong", ""], ["Li", "Xiangtao", ""], ["Song", "Linqi", ""], ["Wong", "Ka-Chun", ""]]}, {"id": "2101.09969", "submitter": "Kuldeep Singh", "authors": "Manoj Prabhakar Kannan Ravi, Kuldeep Singh, Isaiah Onando Mulang',\n  Saeedeh Shekarpour, Johannes Hoffart, Jens Lehmann", "title": "CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and\n  Wikidata", "comments": "accepted in EACL 2021 (full paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CHOLAN, a modular approach to target end-to-end\nentity linking (EL) over knowledge bases. CHOLAN consists of a pipeline of two\ntransformer-based models integrated sequentially to accomplish the EL task. The\nfirst transformer model identifies surface forms (entity mentions) in a given\ntext. For each mention, a second transformer model is employed to classify the\ntarget entity among a predefined candidates list. The latter transformer is fed\nby an enriched context captured from the sentence (i.e. local context), and\nentity description gained from Wikipedia. Such external contexts have not been\nused in the state of the art EL approaches. Our empirical study was conducted\non two well-known knowledge bases (i.e., Wikidata and Wikipedia). The empirical\nresults suggest that CHOLAN outperforms state-of-the-art approaches on standard\ndatasets such as CoNLL-AIDA, MSNBC, AQUAINT, ACE2004, and T-REx.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:19:58 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:50:09 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ravi", "Manoj Prabhakar Kannan", ""], ["Singh", "Kuldeep", ""], ["Mulang'", "Isaiah Onando", ""], ["Shekarpour", "Saeedeh", ""], ["Hoffart", "Johannes", ""], ["Lehmann", "Jens", ""]]}, {"id": "2101.09990", "submitter": "Xiajing Li", "authors": "Xiajing Li, Marios Daoutis", "title": "Unsupervised Key-phrase Extraction and Clustering for Classification\n  Scheme in Scientific Publications", "comments": "Accepted to SDU@AAAI 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several methods have been explored for automating parts of Systematic Mapping\n(SM) and Systematic Review (SR) methodologies. Challenges typically evolve\naround the gaps in semantic understanding of text, as well as lack of domain\nand background knowledge necessary to bridge that gap. In this paper we\ninvestigate possible ways of automating parts of the SM/SR process, i.e. that\nof extracting keywords and key-phrases from scientific documents using\nunsupervised methods, which are then used as a basis to construct the\ncorresponding Classification Scheme using semantic key-phrase clustering\ntechniques. Specifically, we explore the effect of ensemble scores measure in\nkey-phrase extraction, we explore semantic network based word embedding in\nembedding representation of phrase semantics and finally we also explore how\nclustering can be used to group related key-phrases. The evaluation is\nconducted on a dataset of publications pertaining the domain of \"Explainable\nAI\" which we constructed using standard publicly available digital libraries\nand sets of indexing terms (keywords). Results shows that: ensemble ranking\nscore does improve the key-phrase extraction performance. Semantic-network\nbased word embedding based on the ConceptNet Semantic Network has similar\nperformance with contextualized word embedding, however the former are\ncomputationally more efficient. Finally Semantic key-phrase clustering at\nterm-level can group similar terms together that can be suitable for\nclassification scheme.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:17:33 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 20:31:42 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Li", "Xiajing", ""], ["Daoutis", "Marios", ""]]}, {"id": "2101.09995", "submitter": "Vinodkumar Prabhakaran", "authors": "Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee Doshi,\n  Vinodkumar Prabhakaran", "title": "Re-imagining Algorithmic Fairness in India and Beyond", "comments": null, "journal-ref": "Proceedings of the 2021 conference on Fairness, Accountability,\n  and Transparency", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional algorithmic fairness is West-centric, as seen in its sub-groups,\nvalues, and methods. In this paper, we de-center algorithmic fairness and\nanalyse AI power in India. Based on 36 qualitative interviews and a discourse\nanalysis of algorithmic deployments in India, we find that several assumptions\nof algorithmic fairness are challenged. We find that in India, data is not\nalways reliable due to socio-economic factors, ML makers appear to follow\ndouble standards, and AI evokes unquestioning aspiration. We contend that\nlocalising model fairness alone can be window dressing in India, where the\ndistance between models and oppressed communities is large. Instead, we\nre-imagine algorithmic fairness in India and provide a roadmap to\nre-contextualise data and models, empower oppressed communities, and enable\nFair-ML ecosystems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:20:57 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:30:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Sambasivan", "Nithya", ""], ["Arnesen", "Erin", ""], ["Hutchinson", "Ben", ""], ["Doshi", "Tulsee", ""], ["Prabhakaran", "Vinodkumar", ""]]}, {"id": "2101.10014", "submitter": "Joseph Marvin Imperial", "authors": "Clark Emmanuel Paulo, Arvin Ken Ramirez, David Clarence Reducindo,\n  Rannie Mark Mateo, Joseph Marvin Imperial", "title": "A Simple Disaster-Related Knowledge Base for Intelligent Agents", "comments": "7 tables, 1 figure, presented at 34th Pacific Asia Conference on\n  Language, Information and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe our efforts in establishing a simple knowledge\nbase by building a semantic network composed of concepts and word relationships\nin the context of disasters in the Philippines. Our primary source of data is a\ncollection of news articles scraped from various Philippine news websites.\nUsing word embeddings, we extract semantically similar and co-occurring words\nfrom an initial seed words list. We arrive at an expanded ontology with a total\nof 450 word assertions. We let experts from the fields of linguistics,\ndisasters, and weather science evaluate our knowledge base and arrived at an\nagreeability rate of 64%. We then perform a time-based analysis of the\nassertions to identify important semantic changes captured by the knowledge\nbase such as the (a) trend of roles played by human entities, (b) memberships\nof human entities, and (c) common association of disaster-related words. The\ncontext-specific knowledge base developed from this study can be adapted by\nintelligent agents such as chat bots integrated in platforms such as Facebook\nMessenger for answering disaster-related queries.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:31:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Paulo", "Clark Emmanuel", ""], ["Ramirez", "Arvin Ken", ""], ["Reducindo", "David Clarence", ""], ["Mateo", "Rannie Mark", ""], ["Imperial", "Joseph Marvin", ""]]}, {"id": "2101.10035", "submitter": "Toms Bergmanis", "authors": "Toms Bergmanis and M\\=arcis Pinnis", "title": "Facilitating Terminology Translation with Target Lemma Annotations", "comments": "accepted for EACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most of the recent work on terminology integration in machine translation has\nassumed that terminology translations are given already inflected in forms that\nare suitable for the target language sentence. In day-to-day work of\nprofessional translators, however, it is seldom the case as translators work\nwith bilingual glossaries where terms are given in their dictionary forms;\nfinding the right target language form is part of the translation process. We\nargue that the requirement for apriori specified target language forms is\nunrealistic and impedes the practical applicability of previous work. In this\nwork, we propose to train machine translation systems using a source-side data\naugmentation method that annotates randomly selected source language words with\ntheir target language lemmas. We show that systems trained on such augmented\ndata are readily usable for terminology integration in real-life translation\nscenarios. Our experiments on terminology translation into the morphologically\ncomplex Baltic and Uralic languages show an improvement of up to 7 BLEU points\nover baseline systems with no means for terminology integration and an average\nimprovement of 4 BLEU points over the previous work. Results of the human\nevaluation indicate a 47.7% absolute improvement over the previous work in term\ntranslation accuracy when translating into Latvian.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:07:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bergmanis", "Toms", ""], ["Pinnis", "M\u0101rcis", ""]]}, {"id": "2101.10038", "submitter": "Hassan Alhuzali", "authors": "Hassan Alhuzali, Sophia Ananiadou", "title": "SpanEmo: Casting Multi-label Emotion Classification as Span-prediction", "comments": "12 pages, 4 figures, 7 tables, accepted at EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition (ER) is an important task in Natural Language Processing\n(NLP), due to its high impact in real-world applications from health and\nwell-being to author profiling, consumer analysis and security. Current\napproaches to ER, mainly classify emotions independently without considering\nthat emotions can co-exist. Such approaches overlook potential ambiguities, in\nwhich multiple emotions overlap. We propose a new model \"SpanEmo\" casting\nmulti-label emotion classification as span-prediction, which can aid ER models\nto learn associations between labels and words in a sentence. Furthermore, we\nintroduce a loss function focused on modelling multiple co-existing emotions in\nthe input sentence. Experiments performed on the SemEval2018 multi-label\nemotion data over three language sets (i.e., English, Arabic and Spanish)\ndemonstrate our method's effectiveness. Finally, we present different analyses\nthat illustrate the benefits of our method in terms of improving the model\nperformance and learning meaningful associations between emotion classes and\nwords in the sentence.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:11:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Alhuzali", "Hassan", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "2101.10044", "submitter": "Ozan Caglayan", "authors": "Ozan Caglayan, Menekse Kuyu, Mustafa Sercan Amac, Pranava Madhyastha,\n  Erkut Erdem, Aykut Erdem, Lucia Specia", "title": "Cross-lingual Visual Pre-training for Multimodal Machine Translation", "comments": "Accepted to EACL 2021 (Camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models have been shown to improve performance in many\nnatural language tasks substantially. Although the early focus of such models\nwas single language pre-training, recent advances have resulted in\ncross-lingual and visual pre-training methods. In this paper, we combine these\ntwo approaches to learn visually-grounded cross-lingual representations.\nSpecifically, we extend the translation language modelling (Lample and Conneau,\n2019) with masked region classification and perform pre-training with three-way\nparallel vision & language corpora. We show that when fine-tuned for multimodal\nmachine translation, these models obtain state-of-the-art performance. We also\nprovide qualitative insights into the usefulness of the learned grounded\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:46:41 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 19:11:47 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Caglayan", "Ozan", ""], ["Kuyu", "Menekse", ""], ["Amac", "Mustafa Sercan", ""], ["Madhyastha", "Pranava", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""], ["Specia", "Lucia", ""]]}, {"id": "2101.10070", "submitter": "Huda Hakami", "authors": "Danushka Bollegala, Huda Hakami, Yuichi Yoshida and Ken-ichi\n  Kawarabayashi", "title": "RelWalk A Latent Variable Model Approach to Knowledge Graph Embedding", "comments": "Accepted in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Embedding entities and relations of a knowledge graph in a low-dimensional\nspace has shown impressive performance in predicting missing links between\nentities. Although progresses have been achieved, existing methods are\nheuristically motivated and theoretical understanding of such embeddings is\ncomparatively underdeveloped. This paper extends the random walk model (Arora\net al., 2016a) of word embeddings to Knowledge Graph Embeddings (KGEs) to\nderive a scoring function that evaluates the strength of a relation R between\ntwo entities h (head) and t (tail). Moreover, we show that marginal loss\nminimisation, a popular objective used in much prior work in KGE, follows\nnaturally from the log-likelihood ratio maximisation under the probabilities\nestimated from the KGEs according to our theoretical relationship. We propose a\nlearning objective motivated by the theoretical analysis to learn KGEs from a\ngiven knowledge graph. Using the derived objective, accurate KGEs are learnt\nfrom FB15K237 and WN18RR benchmark datasets, providing empirical evidence in\nsupport of the theory.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:31:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bollegala", "Danushka", ""], ["Hakami", "Huda", ""], ["Yoshida", "Yuichi", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "2101.10096", "submitter": "Oren Tsur", "authors": "Yotam Shichel, Meir Kalech, Oren Tsur", "title": "With Measured Words: Simple Sentence Selection for Black-Box\n  Optimization of Sentence Compression Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sentence Compression is the task of generating a shorter, yet grammatical\nversion of a given sentence, preserving the essence of the original sentence.\nThis paper proposes a Black-Box Optimizer for Compression (B-BOC): given a\nblack-box compression algorithm and assuming not all sentences need be\ncompressed -- find the best candidates for compression in order to maximize\nboth compression rate and quality. Given a required compression ratio, we\nconsider two scenarios: (i) single-sentence compression, and (ii)\nsentences-sequence compression. In the first scenario, our optimizer is trained\nto predict how well each sentence could be compressed while meeting the\nspecified ratio requirement. In the latter, the desired compression ratio is\napplied to a sequence of sentences (e.g., a paragraph) as a whole, rather than\non each individual sentence. To achieve that, we use B-BOC to assign an optimal\ncompression ratio to each sentence, then cast it as a Knapsack problem, which\nwe solve using bounded dynamic programming. We evaluate B-BOC on both scenarios\non three datasets, demonstrating that our optimizer improves both accuracy and\nRouge-F1-score compared to direct application of other compression algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:00:56 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Shichel", "Yotam", ""], ["Kalech", "Meir", ""], ["Tsur", "Oren", ""]]}, {"id": "2101.10112", "submitter": "Ashiqur Rahman KhudaBukhsh", "authors": "Ashiqur R. KhudaBukhsh, Rupak Sarkar, Mark S. Kamlet, Tom M. Mitchell", "title": "Fringe News Networks: Dynamics of US News Viewership following the 2020\n  Presidential Election", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing political polarization of the American electorate over the last\nseveral decades has been widely studied and documented. During the\nadministration of President Donald Trump, charges of \"fake news\" made social\nand news media not only the means but, to an unprecedented extent, the topic of\npolitical communication. Using data from before the November 3rd, 2020 US\nPresidential election, recent work has demonstrated the viability of using\nYouTube's social media ecosystem to obtain insights into the extent of US\npolitical polarization as well as the relationship between this polarization\nand the nature of the content and commentary provided by different US news\nnetworks. With that work as background, this paper looks at the sharp\ntransformation of the relationship between news consumers and here-to-fore\n\"fringe\" news media channels in the 64 days between the US presidential\nelection and the violence that took place at US Capitol on January 6th. This\npaper makes two distinct types of contributions. The first is to introduce a\nnovel methodology to analyze large social media data to study the dynamics of\nsocial political news networks and their viewers. The second is to provide\ninsights into what actually happened regarding US political social media\nchannels and their viewerships during this volatile 64 day period.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:42:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["KhudaBukhsh", "Ashiqur R.", ""], ["Sarkar", "Rupak", ""], ["Kamlet", "Mark S.", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "2101.10164", "submitter": "Oren Tsur", "authors": "Aviv Ben Haim, Oren Tsur", "title": "Open-Mindedness and Style Coordination in Argumentative Discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Linguistic accommodation is the process in which speakers adjust their\naccent, diction, vocabulary, and other aspects of language according to the\ncommunication style of one another. Previous research has shown how linguistic\naccommodation correlates with gaps in the power and status of the speakers and\nthe way it promotes approval and discussion efficiency. In this work, we\nprovide a novel perspective on the phenomena, exploring its correlation with\nthe open-mindedness of a speaker, rather than to her social status. We process\nthousands of unstructured argumentative discussions that took place in Reddit's\nChange My View (CMV) subreddit, demonstrating that open-mindedness relates to\nthe assumed role of a speaker in different contexts. On the discussion level,\nwe surprisingly find that discussions that reach agreement present lower levels\nof accommodation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:24:55 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Haim", "Aviv Ben", ""], ["Tsur", "Oren", ""]]}, {"id": "2101.10196", "submitter": "Katikapalli Subramanyam Kalyan", "authors": "Katikapalli Subramanyam Kalyan and Sivanesan Sangeetha", "title": "A Hybrid Approach to Measure Semantic Relatedness in Biomedical Concepts", "comments": "preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This work aimed to demonstrate the effectiveness of a hybrid\napproach based on Sentence BERT model and retrofitting algorithm to compute\nrelatedness between any two biomedical concepts. Materials and Methods: We\ngenerated concept vectors by encoding concept preferred terms using ELMo, BERT,\nand Sentence BERT models. We used BioELMo and Clinical ELMo. We used Ontology\nKnowledge Free (OKF) models like PubMedBERT, BioBERT, BioClinicalBERT, and\nOntology Knowledge Injected (OKI) models like SapBERT, CoderBERT, KbBERT, and\nUmlsBERT. We trained all the BERT models using Siamese network on SNLI and STSb\ndatasets to allow the models to learn more semantic information at the phrase\nor sentence level so that they can represent multi-word concepts better.\nFinally, to inject ontology relationship knowledge into concept vectors, we\nused retrofitting algorithm and concepts from various UMLS relationships. We\nevaluated our hybrid approach on four publicly available datasets which also\nincludes the recently released EHR-RelB dataset. EHR-RelB is the largest\npublicly available relatedness dataset in which 89% of terms are multi-word\nwhich makes it more challenging. Results: Sentence BERT models mostly\noutperformed corresponding BERT models. The concept vectors generated using the\nSentence BERT model based on SapBERT and retrofitted using UMLS-related\nconcepts achieved the best results on all four datasets. Conclusions: Sentence\nBERT models are more effective compared to BERT models in computing relatedness\nscores in most of the cases. Injecting ontology knowledge into concept vectors\nfurther enhances their quality and contributes to better relatedness scores.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:01:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kalyan", "Katikapalli Subramanyam", ""], ["Sangeetha", "Sivanesan", ""]]}, {"id": "2101.10213", "submitter": "Yongliang Shen", "authors": "Yongliang Shen, Xinyin Ma, Yechun Tang, Weiming Lu", "title": "A Trigger-Sense Memory Flow Framework for Joint Entity and Relation\n  Extraction", "comments": "12 pages, 6 figures, Accepted to WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint entity and relation extraction framework constructs a unified model to\nperform entity recognition and relation extraction simultaneously, which can\nexploit the dependency between the two tasks to mitigate the error propagation\nproblem suffered by the pipeline model. Current efforts on joint entity and\nrelation extraction focus on enhancing the interaction between entity\nrecognition and relation extraction through parameter sharing, joint decoding,\nor other ad-hoc tricks (e.g., modeled as a semi-Markov decision process, cast\nas a multi-round reading comprehension task). However, there are still two\nissues on the table. First, the interaction utilized by most methods is still\nweak and uni-directional, which is unable to model the mutual dependency\nbetween the two tasks. Second, relation triggers are ignored by most methods,\nwhich can help explain why humans would extract a relation in the sentence.\nThey're essential for relation extraction but overlooked. To this end, we\npresent a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and\nrelation extraction. We build a memory module to remember category\nrepresentations learned in entity recognition and relation extraction tasks.\nAnd based on it, we design a multi-level memory flow attention mechanism to\nenhance the bi-directional interaction between entity recognition and relation\nextraction. Moreover, without any human annotations, our model can enhance\nrelation trigger information in a sentence through a trigger sensor module,\nwhich improves the model performance and makes model predictions with better\ninterpretation. Experiment results show that our proposed framework achieves\nstate-of-the-art results by improves the relation F1 to 52.44% (+3.2%) on\nSciERC, 66.49% (+4.9%) on ACE05, 72.35% (+0.6%) on CoNLL04 and 80.66% (+2.3%)\non ADE.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:24:04 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:59:53 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 07:54:45 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Shen", "Yongliang", ""], ["Ma", "Xinyin", ""], ["Tang", "Yechun", ""], ["Lu", "Weiming", ""]]}, {"id": "2101.10219", "submitter": "Nikos Voskarides", "authors": "Ida Mele, Cristina Ioana Muntean, Mohammad Aliannejadi, Nikos\n  Voskarides", "title": "MICROS: Mixed-Initiative ConveRsatiOnal Systems Workshop", "comments": "ECIR 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 1st edition of the workshop on Mixed-Initiative ConveRsatiOnal Systems\n(MICROS@ECIR2021) aims at investigating and collecting novel ideas and\ncontributions in the field of conversational systems. Oftentimes, the users\nfulfill their information need using smartphones and home assistants. This has\nrevolutionized the way users access online information, thus posing new\nchallenges compared to traditional search and recommendation. The first edition\nof MICROS will have a particular focus on mixed-initiative conversational\nsystems. Indeed, conversational systems need to be proactive, proposing not\nonly answers but also possible interpretations for ambiguous or vague requests.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:31:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mele", "Ida", ""], ["Muntean", "Cristina Ioana", ""], ["Aliannejadi", "Mohammad", ""], ["Voskarides", "Nikos", ""]]}, {"id": "2101.10244", "submitter": "Ronen Tamari", "authors": "Ronen Tamari, Fan Bai, Alan Ritter, Gabriel Stanovsky", "title": "Process-Level Representation of Scientific Protocols with Interactive\n  Annotation", "comments": "EACL 2021 camera ready. Data, models and code at\n  https://textlabs.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Process Execution Graphs (PEG), a document-level representation of\nreal-world wet lab biochemistry protocols, addressing challenges such as\ncross-sentence relations, long-range coreference, grounding, and implicit\narguments. We manually annotate PEGs in a corpus of complex lab protocols with\na novel interactive textual simulator that keeps track of entity traits and\nsemantic constraints during annotation. We use this data to develop\ngraph-prediction models, finding them to be good at entity identification and\nlocal relation extraction, while our corpus facilitates further exploration of\nchallenging long-range relations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:18:20 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 06:58:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Tamari", "Ronen", ""], ["Bai", "Fan", ""], ["Ritter", "Alan", ""], ["Stanovsky", "Gabriel", ""]]}, {"id": "2101.10250", "submitter": "Gabriella Skitalinskaya", "authors": "Gabriella Skitalinskaya, Jonas Klaff and Henning Wachsmuth", "title": "Learning From Revisions: Quality Assessment of Claims in Argumentation\n  at Scale", "comments": "Accepted as a long paper at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing the quality of arguments and of the claims the arguments are\ncomposed of has become a key task in computational argumentation. However, even\nif different claims share the same stance on the same topic, their assessment\ndepends on the prior perception and weighting of the different aspects of the\ntopic being discussed. This renders it difficult to learn topic-independent\nquality indicators. In this paper, we study claim quality assessment\nirrespective of discussed aspects by comparing different revisions of the same\nclaim. We compile a large-scale corpus with over 377k claim revision pairs of\nvarious types from kialo.com, covering diverse topics from politics, ethics,\nentertainment, and others. We then propose two tasks: (a) assessing which claim\nof a revision pair is better, and (b) ranking all versions of a claim by\nquality. Our first experiments with embedding-based logistic regression and\ntransformer-based neural networks show promising results, suggesting that\nlearned indicators generalize well across topics. In a detailed error analysis,\nwe give insights into what quality dimensions of claims can be assessed\nreliably. We provide the data and scripts needed to reproduce all results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:32:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Skitalinskaya", "Gabriella", ""], ["Klaff", "Jonas", ""], ["Wachsmuth", "Henning", ""]]}, {"id": "2101.10253", "submitter": "Daniela Mihai", "authors": "Daniela Mihai and Jonathon Hare", "title": "The emergence of visual semantics through communication games", "comments": "arXiv admin note: text overlap with arXiv:1911.05546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of communication systems between agents which learn to play\nreferential signalling games with realistic images has attracted a lot of\nattention recently. The majority of work has focused on using fixed, pretrained\nimage feature extraction networks which potentially bias the information the\nagents learn to communicate. In this work, we consider a signalling game\nsetting in which a `sender' agent must communicate the information about an\nimage to a `receiver' who must select the correct image from many distractors.\nWe investigate the effect of the feature extractor's weights and of the task\nbeing solved on the visual semantics learned by the models. We first\ndemonstrate to what extent the use of pretrained feature extraction networks\ninductively bias the visual semantics conveyed by emergent communication\nchannel and quantify the visual semantics that are induced.\n  We then go on to explore ways in which inductive biases can be introduced to\nencourage the emergence of semantically meaningful communication without the\nneed for any form of supervised pretraining of the visual feature extractor. We\nimpose various augmentations to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game. Our work\nbridges a gap between emergent communication research and self-supervised\nfeature learning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:43:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2101.10273", "submitter": "Yufang Hou", "authors": "Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin and Debasis\n  Ganguly", "title": "TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of\n  Tasks Datasets and Metrics", "comments": "accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tasks, Datasets and Evaluation Metrics are important concepts for\nunderstanding experimental scientific papers. However, most previous work on\ninformation extraction for scientific literature mainly focuses on the\nabstracts only, and does not treat datasets as a separate type of entity (Zadeh\nand Schumann, 2016; Luan et al., 2018). In this paper, we present a new corpus\nthat contains domain expert annotations for Task (T), Dataset (D), Metric (M)\nentities on 2,000 sentences extracted from NLP papers. We report experiment\nresults on TDM extraction using a simple data augmentation strategy and apply\nour tagger to around 30,000 NLP papers from the ACL Anthology. The corpus is\nmade publicly available to the community for fostering research on scientific\npublication summarization (Erera et al., 2019) and knowledge discovery.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:54:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hou", "Yufang", ""], ["Jochim", "Charles", ""], ["Gleize", "Martin", ""], ["Bonin", "Francesca", ""], ["Ganguly", "Debasis", ""]]}, {"id": "2101.10281", "submitter": "Mark Neumann", "authors": "Mark Neumann, Zejiang Shen, Sam Skjonsberg", "title": "PAWLS: PDF Annotation With Labels and Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adobe's Portable Document Format (PDF) is a popular way of distributing\nview-only documents with a rich visual markup. This presents a challenge to NLP\npractitioners who wish to use the information contained within PDF documents\nfor training models or data analysis, because annotating these documents is\ndifficult. In this paper, we present PDF Annotation with Labels and Structure\n(PAWLS), a new annotation tool designed specifically for the PDF document\nformat. PAWLS is particularly suited for mixed-mode annotation and scenarios in\nwhich annotators require extended context to annotate accurately. PAWLS\nsupports span-based textual annotation, N-ary relations and freeform,\nnon-textual bounding boxes, all of which can be exported in convenient formats\nfor training multi-modal machine learning models. A read-only PAWLS server is\navailable at https://pawls.apps.allenai.org/ and the source code is available\nat https://github.com/allenai/pawls.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:02:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Neumann", "Mark", ""], ["Shen", "Zejiang", ""], ["Skjonsberg", "Sam", ""]]}, {"id": "2101.10368", "submitter": "Ishan Tarunesh", "authors": "Ishan Tarunesh, Sushil Khyalia, Vishwajeet Kumar, Ganesh Ramakrishnan,\n  Preethi Jyothi", "title": "Meta-Learning for Effective Multi-task and Multilingual Modelling", "comments": "In Proceedings of The 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language processing (NLP) tasks (e.g. question-answering in English)\nbenefit from knowledge of other tasks (e.g. named entity recognition in\nEnglish) and knowledge of other languages (e.g. question-answering in Spanish).\nSuch shared representations are typically learned in isolation, either across\ntasks or across languages. In this work, we propose a meta-learning approach to\nlearn the interactions between both tasks and languages. We also investigate\nthe role of different sampling strategies used during meta-learning. We present\nexperiments on five different tasks and six different languages from the XTREME\nmultilingual benchmark dataset. Our meta-learned model clearly improves in\nperformance compared to competitive baseline models that also include\nmulti-task baselines. We also present zero-shot evaluations on unseen target\nlanguages to demonstrate the utility of our proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:30:26 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 16:35:02 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 13:12:31 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tarunesh", "Ishan", ""], ["Khyalia", "Sushil", ""], ["Kumar", "Vishwajeet", ""], ["Ramakrishnan", "Ganesh", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2101.10376", "submitter": "Arman Sarjou", "authors": "Arman Sarjou", "title": "The Power of Language: Understanding Sentiment Towards the Climate\n  Emergency using Twitter Data", "comments": "6 Pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how attitudes towards the Climate Emergency vary can hold the\nkey to driving policy changes for effective action to mitigate climate related\nrisk. The Oil and Gas industry account for a significant proportion of global\nemissions and so it could be speculated that there is a relationship between\nCrude Oil Futures and sentiment towards the Climate Emergency. Using Latent\nDirichlet Allocation for Topic Modelling on a bespoke Twitter dataset, this\nstudy shows that it is possible to split the conversation surrounding the\nClimate Emergency into 3 distinct topics. Forecasting Crude Oil Futures using\nSeasonal AutoRegressive Integrated Moving Average Modelling gives promising\nresults with a root mean squared error of 0.196 and 0.209 on the training and\ntesting data respectively. Understanding variation in attitudes towards climate\nemergency provides inconclusive results which could be improved using\nspatial-temporal analysis methods such as Density Based Clustering (DBSCAN).\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:51:10 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sarjou", "Arman", ""]]}, {"id": "2101.10382", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe", "title": "Curriculum Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:08:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""], ["Rota", "Paolo", ""], ["Sebe", "Nicu", ""]]}, {"id": "2101.10421", "submitter": "Daria Dzendzik", "authors": "Daria Dzendzik, Carl Vogel, Jennifer Foster", "title": "English Machine Reading Comprehension Datasets: A Survey", "comments": "Dataset survey paper: 10 pages, 5 figures, 1 table + attachment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys 54 English Machine Reading Comprehension datasets, with a\nview to providing a convenient resource for other researchers interested in\nthis problem. We categorize the datasets according to their question and answer\nform and compare them across various dimensions including size, vocabulary,\ndata source, method of creation, human performance level, and first question\nword. Our analysis reveals that Wikipedia is by far the most common data source\nand that there is a relative lack of why, when, and where questions across\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:15:06 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Dzendzik", "Daria", ""], ["Vogel", "Carl", ""], ["Foster", "Jennifer", ""]]}, {"id": "2101.10435", "submitter": "Maria Leonor Pacheco", "authors": "Manuel Widmoser, Maria Leonor Pacheco, Jean Honorio, Dan Goldwasser", "title": "Randomized Deep Structured Prediction for Discourse-Level Processing", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressive text encoders such as RNNs and Transformer Networks have been at\nthe center of NLP models in recent work. Most of the effort has focused on\nsentence-level tasks, capturing the dependencies between words in a single\nsentence, or pairs of sentences. However, certain tasks, such as argumentation\nmining, require accounting for longer texts and complicated structural\ndependencies between them. Deep structured prediction is a general framework to\ncombine the complementary strengths of expressive neural encoders and\nstructured inference for highly structured domains. Nevertheless, when the need\narises to go beyond sentences, most work relies on combining the output scores\nof independently trained classifiers. One of the main reasons for this is that\nconstrained inference comes at a high computational cost. In this paper, we\nexplore the use of randomized inference to alleviate this concern and show that\nwe can efficiently leverage deep structured prediction and expressive neural\nencoders for a set of tasks involving complicated argumentative structures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:49:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Widmoser", "Manuel", ""], ["Pacheco", "Maria Leonor", ""], ["Honorio", "Jean", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2101.10448", "submitter": "Alan Ansell", "authors": "Alan Ansell, Felipe Bravo-Marquez, Bernhard Pfahringer", "title": "PolyLM: Learning about Polysemy through Language Modeling", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To avoid the \"meaning conflation deficiency\" of word embeddings, a number of\nmodels have aimed to embed individual word senses. These methods at one time\nperformed well on tasks such as word sense induction (WSI), but they have since\nbeen overtaken by task-specific techniques which exploit contextualized\nembeddings. However, sense embeddings and contextualization need not be\nmutually exclusive. We introduce PolyLM, a method which formulates the task of\nlearning sense embeddings as a language modeling problem, allowing\ncontextualization techniques to be applied. PolyLM is based on two underlying\nassumptions about word senses: firstly, that the probability of a word\noccurring in a given context is equal to the sum of the probabilities of its\nindividual senses occurring; and secondly, that for a given occurrence of a\nword, one of its senses tends to be much more plausible in the context than the\nothers. We evaluate PolyLM on WSI, showing that it performs considerably better\nthan previous sense embedding techniques, and matches the current\nstate-of-the-art specialized WSI method despite having six times fewer\nparameters. Code and pre-trained models are available at\nhttps://github.com/AlanAnsell/PolyLM.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:09:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ansell", "Alan", ""], ["Bravo-Marquez", "Felipe", ""], ["Pfahringer", "Bernhard", ""]]}, {"id": "2101.10496", "submitter": "Lane Schwartz", "authors": "Lane Schwartz and Emily Chen and Hyunji Hayley Park and Edward Jahn\n  and Sylvia L.R. Schreiner", "title": "A Digital Corpus of St. Lawrence Island Yupik", "comments": "ComputEL-4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  St. Lawrence Island Yupik (ISO 639-3: ess) is an endangered polysynthetic\nlanguage in the Inuit-Yupik language family indigenous to Alaska and Chukotka.\nThis work presents a step-by-step pipeline for the digitization of written\ntexts, and the first publicly available digital corpus for St. Lawrence Island\nYupik, created using that pipeline. This corpus has great potential for future\nlinguistic inquiry and research in NLP. It was also developed for use in Yupik\nlanguage education and revitalization, with a primary goal of enabling easy\naccess to Yupik texts by educators and by members of the Yupik community. A\nsecondary goal is to support development of language technology such as\nspell-checkers, text-completion systems, interactive e-books, and language\nlearning apps for use by the Yupik community.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:14:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Schwartz", "Lane", ""], ["Chen", "Emily", ""], ["Park", "Hyunji Hayley", ""], ["Jahn", "Edward", ""], ["Schreiner", "Sylvia L. R.", ""]]}, {"id": "2101.10504", "submitter": "Ming Zhao", "authors": "Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander Ku, Jason\n  Baldridge, Eugene Ie", "title": "On the Evaluation of Vision-and-Language Navigation Instructions", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision-and-Language Navigation wayfinding agents can be enhanced by\nexploiting automatically generated navigation instructions. However, existing\ninstruction generators have not been comprehensively evaluated, and the\nautomatic evaluation metrics used to develop them have not been validated.\nUsing human wayfinders, we show that these generators perform on par with or\nonly slightly better than a template-based generator and far worse than human\ninstructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are\nineffective for evaluating grounded navigation instructions. To improve\ninstruction evaluation, we propose an instruction-trajectory compatibility\nmodel that operates without reference instructions. Our model shows the highest\ncorrelation with human wayfinding outcomes when scoring individual\ninstructions. For ranking instruction generation systems, if reference\ninstructions are available we recommend using SPICE.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:03:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhao", "Ming", ""], ["Anderson", "Peter", ""], ["Jain", "Vihan", ""], ["Wang", "Su", ""], ["Ku", "Alexander", ""], ["Baldridge", "Jason", ""], ["Ie", "Eugene", ""]]}, {"id": "2101.10524", "submitter": "Abhinav Arora", "authors": "Arash Einolghozati, Abhinav Arora, Lorena Sainz-Maza Lecanda, Anuj\n  Kumar, Sonal Gupta", "title": "El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic\n  Parsing", "comments": null, "journal-ref": "EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being able to parse code-switched (CS) utterances, such as Spanish+English or\nHindi+English, is essential to democratize task-oriented semantic parsing\nsystems for certain locales. In this work, we focus on Spanglish\n(Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances\nalongside their semantic parses. We examine the CS generalizability of various\nCross-lingual (XL) models and exhibit the advantage of pre-trained XL language\nmodels when data for only one language is present. As such, we focus on\nimproving the pre-trained models for the case when only English corpus\nalongside either zero or a few CS training instances are available. We propose\ntwo data augmentation methods for the zero-shot and the few-shot settings:\nfine-tune using translate-and-align and augment using a generation model\nfollowed by match-and-filter. Combining the few-shot setting with the above\nimprovements decreases the initial 30-point accuracy gap between the zero-shot\nand the full-data settings by two thirds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 02:40:44 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:28:49 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 08:09:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Einolghozati", "Arash", ""], ["Arora", "Abhinav", ""], ["Lecanda", "Lorena Sainz-Maza", ""], ["Kumar", "Anuj", ""], ["Gupta", "Sonal", ""]]}, {"id": "2101.10535", "submitter": "Weixin Zeng", "authors": "Weixin Zeng, Xiang Zhao, Jiuyang Tang, Xinyi Li, Minnan Luo, Qinghua\n  Zheng", "title": "Towards Entity Alignment in the Open World: An Unsupervised Approach", "comments": "Accepted by DASFAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) aims to discover the equivalent entities in different\nknowledge graphs (KGs). It is a pivotal step for integrating KGs to increase\nknowledge coverage and quality. Recent years have witnessed a rapid increase of\nEA frameworks. However, state-of-the-art solutions tend to rely on labeled data\nfor model training. Additionally, they work under the closed-domain setting and\ncannot deal with entities that are unmatchable. To address these deficiencies,\nwe offer an unsupervised framework that performs entity alignment in the open\nworld. Specifically, we first mine useful features from the side information of\nKGs. Then, we devise an unmatchable entity prediction module to filter out\nunmatchable entities and produce preliminary alignment results. These\npreliminary results are regarded as the pseudo-labeled data and forwarded to\nthe progressive learning framework to generate structural representations,\nwhich are integrated with the side information to provide a more comprehensive\nview for alignment. Finally, the progressive learning framework gradually\nimproves the quality of structural embeddings and enhances the alignment\nperformance by enriching the pseudo-labeled data with alignment results from\nthe previous round. Our solution does not require labeled data and can\neffectively filter out unmatchable entities. Comprehensive experimental\nevaluations validate its superiority.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:10:24 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zeng", "Weixin", ""], ["Zhao", "Xiang", ""], ["Tang", "Jiuyang", ""], ["Li", "Xinyi", ""], ["Luo", "Minnan", ""], ["Zheng", "Qinghua", ""]]}, {"id": "2101.10537", "submitter": "Joseph Marvin Imperial", "authors": "Joseph Marvin Imperial, Ethel Ong", "title": "Application of Lexical Features Towards Improvement of Filipino\n  Readability Identification of Children's Literature", "comments": "8 tables, 1 figure. Presented at the Philippine Computing Science\n  Congress 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proper identification of grade levels of children's reading materials is an\nimportant step towards effective learning. Recent studies in readability\nassessment for the English domain applied modern approaches in natural language\nprocessing (NLP) such as machine learning (ML) techniques to automate the\nprocess. There is also a need to extract the correct linguistic features when\nmodeling readability formulas. In the context of the Filipino language, limited\nwork has been done [1, 2], especially in considering the language's lexical\ncomplexity as main features. In this paper, we explore the use of lexical\nfeatures towards improving the development of readability identification of\nchildren's books written in Filipino. Results show that combining lexical\nfeatures (LEX) consisting of type-token ratio, lexical density, lexical\nvariation, foreign word count with traditional features (TRAD) used by previous\nworks such as sentence length, average syllable length, polysyllabic words,\nword, sentence, and phrase counts increased the performance of readability\nmodels by almost a 5% margin (from 42% to 47.2%). Further analysis and ranking\nof the most important features were shown to identify which features contribute\nthe most in terms of reading complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:54:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Imperial", "Joseph Marvin", ""], ["Ong", "Ethel", ""]]}, {"id": "2101.10539", "submitter": "Mohammed Mustafa Abdelgwad", "authors": "Mohammed M.Abdelgwad, Taysir Hassan A Soliman, Ahmed I.Taloba, Mohamed\n  Fawzy Farghaly", "title": "Arabic aspect based sentiment analysis using bidirectional GRU based\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based Sentiment analysis (ABSA) accomplishes a fine-grained analysis\nthat defines the aspects of a given document or sentence and the sentiments\nconveyed regarding each aspect. This level of analysis is the most detailed\nversion that is capable of exploring the nuanced viewpoints of the reviews.\nMost of the research available in ABSA focuses on English language with very\nfew work available on Arabic. Most previous work in Arabic has been based on\nregular methods of machine learning that mainly depends on a group of rare\nresources and tools for analyzing and processing Arabic content such as\nlexicons, but the lack of those resources presents another challenge. To\novercome these obstacles, Deep Learning (DL)-based methods are proposed using\ntwo models based on Gated Recurrent Units (GRU) neural networks for ABSA. The\nfirst one is a DL model that takes advantage of the representations on both\nwords and characters via the combination of bidirectional GRU, Convolutional\nneural network (CNN), and Conditional Random Field (CRF) which makes up\n(BGRU-CNN-CRF) model to extract the main opinionated aspects (OTE). The second\nis an interactive attention network based on bidirectional GRU (IAN-BGRU) to\nidentify sentiment polarity toward extracted aspects. We evaluated our models\nusing the benchmarked Arabic hotel reviews dataset. The results indicate that\nthe proposed methods are better than baseline research on both tasks having\n38.5% enhancement in F1-score for opinion target extraction (T2) and 7.5% in\naccuracy for aspect-based sentiment polarity classification (T3). Obtaining F1\nscore of 69.44% for T2, and accuracy of 83.98% for T3.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 02:54:30 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 05:01:16 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 10:32:15 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Abdelgwad", "Mohammed M.", ""], ["Soliman", "Taysir Hassan A", ""], ["Taloba", "Ahmed I.", ""], ["Farghaly", "Mohamed Fawzy", ""]]}, {"id": "2101.10545", "submitter": "Sayan Sinha", "authors": "Ritam Dutt and Sayan Sinha, Rishabh Joshi, Surya Shekhar Chakraborty,\n  Meredith Riggs, Xinru Yan, Haogang Bao, Carolyn Penstein Ros\\'e", "title": "RESPER: Computationally Modelling Resisting Strategies in Persuasive\n  Conversations", "comments": "Accepted as a long paper at the 16th Conference of the European\n  Chapter of the Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modelling persuasion strategies as predictors of task outcome has several\nreal-world applications and has received considerable attention from the\ncomputational linguistics community. However, previous research has failed to\naccount for the resisting strategies employed by an individual to foil such\npersuasion attempts. Grounded in prior literature in cognitive and social\npsychology, we propose a generalised framework for identifying resisting\nstrategies in persuasive conversations. We instantiate our framework on two\ndistinct datasets comprising persuasion and negotiation conversations. We also\nleverage a hierarchical sequence-labelling neural architecture to infer the\naforementioned resisting strategies automatically. Our experiments reveal the\nasymmetry of power roles in non-collaborative goal-directed conversations and\nthe benefits accrued from incorporating resisting strategies on the final\nconversation outcome. We also investigate the role of different resisting\nstrategies on the conversation outcome and glean insights that corroborate with\npast findings. We also make the code and the dataset of this work publicly\navailable at https://github.com/americast/resper.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:44:17 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Dutt", "Ritam", ""], ["Sinha", "Sayan", ""], ["Joshi", "Rishabh", ""], ["Chakraborty", "Surya Shekhar", ""], ["Riggs", "Meredith", ""], ["Yan", "Xinru", ""], ["Bao", "Haogang", ""], ["Ros\u00e9", "Carolyn Penstein", ""]]}, {"id": "2101.10565", "submitter": "Katharina Kann", "authors": "Katharina Kann and Mauro M. Monsalve-Mercado", "title": "Coloring the Black Box: What Synesthesia Tells Us about Character\n  Embeddings", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to their word- or sentence-level counterparts, character\nembeddings are still poorly understood. We aim at closing this gap with an\nin-depth study of English character embeddings. For this, we use resources from\nresearch on grapheme-color synesthesia -- a neuropsychological phenomenon where\nletters are associated with colors, which give us insight into which characters\nare similar for synesthetes and how characters are organized in color space.\nComparing 10 different character embeddings, we ask: How similar are character\nembeddings to a synesthete's perception of characters? And how similar are\ncharacter embeddings extracted from different models? We find that LSTMs agree\nwith humans more than transformers. Comparing across tasks, grapheme-to-phoneme\nconversion results in the most human-like character embeddings. Finally, ELMo\nembeddings differ from both humans and other models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:21:58 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kann", "Katharina", ""], ["Monsalve-Mercado", "Mauro M.", ""]]}, {"id": "2101.10573", "submitter": "Vicky Zayats", "authors": "Vicky Zayats, Kristina Toutanova, and Mari Ostendorf", "title": "Representations for Question Answering from Documents with Tables and\n  Text", "comments": "To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables in Web documents are pervasive and can be directly used to answer many\nof the queries searched on the Web, motivating their integration in question\nanswering. Very often information presented in tables is succinct and hard to\ninterpret with standard language representations. On the other hand, tables\noften appear within textual context, such as an article describing the table.\nUsing the information from an article as additional context can potentially\nenrich table representations. In this work we aim to improve question answering\nfrom tables by refining table representations based on information from\nsurrounding text. We also present an effective method to combine text and\ntable-based predictions for question answering from full documents, obtaining\nsignificant improvements on the Natural Questions dataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:52:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zayats", "Vicky", ""], ["Toutanova", "Kristina", ""], ["Ostendorf", "Mari", ""]]}, {"id": "2101.10579", "submitter": "Kuan-Hao Huang", "authors": "Kuan-Hao Huang, Kai-Wei Chang", "title": "Generating Syntactically Controlled Paraphrases without Using Annotated\n  Parallel Pairs", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paraphrase generation plays an essential role in natural language process\n(NLP), and it has many downstream applications. However, training supervised\nparaphrase models requires many annotated paraphrase pairs, which are usually\ncostly to obtain. On the other hand, the paraphrases generated by existing\nunsupervised approaches are usually syntactically similar to the source\nsentences and are limited in diversity. In this paper, we demonstrate that it\nis possible to generate syntactically various paraphrases without the need for\nannotated paraphrase pairs. We propose Syntactically controlled Paraphrase\nGenerator (SynPG), an encoder-decoder based model that learns to disentangle\nthe semantics and the syntax of a sentence from a collection of unannotated\ntexts. The disentanglement enables SynPG to control the syntax of output\nparaphrases by manipulating the embedding in the syntactic space. Extensive\nexperiments using automatic metrics and human evaluation show that SynPG\nperforms better syntactic control than unsupervised baselines, while the\nquality of the generated paraphrases is competitive. We also demonstrate that\nthe performance of SynPG is competitive or even better than supervised models\nwhen the unannotated data is large. Finally, we show that the syntactically\ncontrolled paraphrases generated by SynPG can be utilized for data augmentation\nto improve the robustness of NLP models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:13:52 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Huang", "Kuan-Hao", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2101.10587", "submitter": "Sunil Mohan", "authors": "Sunil Mohan and Rico Angell and Nick Monath and Andrew McCallum", "title": "Low Resource Recognition and Linking of Biomedical Concepts from a Large\n  Ontology", "comments": null, "journal-ref": null, "doi": "10.1145/3459930.3469524", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tools to explore scientific literature are essential for scientists,\nespecially in biomedicine, where about a million new papers are published every\nyear. Many such tools provide users the ability to search for specific entities\n(e.g. proteins, diseases) by tracking their mentions in papers. PubMed, the\nmost well known database of biomedical papers, relies on human curators to add\nthese annotations. This can take several weeks for new papers, and not all\npapers get tagged. Machine learning models have been developed to facilitate\nthe semantic indexing of scientific papers. However their performance on the\nmore comprehensive ontologies of biomedical concepts does not reach the levels\nof typical entity recognition problems studied in NLP. In large part this is\ndue to their low resources, where the ontologies are large, there is a lack of\ndescriptive text defining most entities, and labeled data can only cover a\nsmall portion of the ontology. In this paper, we develop a new model that\novercomes these challenges by (1) generalizing to entities unseen at training\ntime, and (2) incorporating linking predictions into the mention segmentation\ndecisions. Our approach achieves new state-of-the-art results for the UMLS\nontology in both traditional recognition/linking (+8 F1 pts) as well as\nsemantic indexing-based evaluation (+10 F1 pts).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:41:12 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 18:02:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Mohan", "Sunil", ""], ["Angell", "Rico", ""], ["Monath", "Nick", ""], ["McCallum", "Andrew", ""]]}, {"id": "2101.10642", "submitter": "Hyunjin Choi", "authors": "Hyunjin Choi, Judong Kim, Seongho Joe, and Youngjune Gwon", "title": "Evaluation of BERT and ALBERT Sentence Embedding Performance on\n  Downstream NLP Tasks", "comments": "6 pages, 2 figures, to be published in 25th International Conference\n  on Pattern Recognition, ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contextualized representations from a pre-trained language model are central\nto achieve a high performance on downstream NLP task. The pre-trained BERT and\nA Lite BERT (ALBERT) models can be fine-tuned to give state-ofthe-art results\nin sentence-pair regressions such as semantic textual similarity (STS) and\nnatural language inference (NLI). Although BERT-based models yield the [CLS]\ntoken vector as a reasonable sentence embedding, the search for an optimal\nsentence embedding scheme remains an active research area in computational\nlinguistics. This paper explores on sentence embedding models for BERT and\nALBERT. In particular, we take a modified BERT network with siamese and triplet\nnetwork structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to\ncreate Sentence-ALBERT (SALBERT). We also experiment with an outer CNN\nsentence-embedding network for SBERT and SALBERT. We evaluate performances of\nall sentence-embedding models considered using the STS and NLI datasets. The\nempirical results indicate that our CNN architecture improves ALBERT models\nsubstantially more than BERT models for STS benchmark. Despite significantly\nfewer model parameters, ALBERT sentence embedding is highly competitive to BERT\nin downstream NLP evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:14:06 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Choi", "Hyunjin", ""], ["Kim", "Judong", ""], ["Joe", "Seongho", ""], ["Gwon", "Youngjune", ""]]}, {"id": "2101.10649", "submitter": "Hyunjin Choi", "authors": "Hyunjin Choi, Judong Kim, Seongho Joe, Seungjai Min, Youngjune Gwon", "title": "Analyzing Zero-shot Cross-lingual Transfer in Supervised NLP Tasks", "comments": "6 pages, 4 figures, to be published in 25th International Conference\n  on Pattern Recognition, ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In zero-shot cross-lingual transfer, a supervised NLP task trained on a\ncorpus in one language is directly applicable to another language without any\nadditional training. A source of cross-lingual transfer can be as\nstraightforward as lexical overlap between languages (e.g., use of the same\nscripts, shared subwords) that naturally forces text embeddings to occupy a\nsimilar representation space. Recently introduced cross-lingual language model\n(XLM) pretraining brings out neural parameter sharing in Transformer-style\nnetworks as the most important factor for the transfer. In this paper, we aim\nto validate the hypothetically strong cross-lingual transfer properties induced\nby XLM pretraining. Particularly, we take XLM-RoBERTa (XLMR) in our experiments\nthat extend semantic textual similarity (STS), SQuAD and KorQuAD for machine\nreading comprehension, sentiment analysis, and alignment of sentence embeddings\nunder various cross-lingual settings. Our results indicate that the presence of\ncross-lingual transfer is most pronounced in STS, sentiment analysis the next,\nand MRC the last. That is, the complexity of a downstream task softens the\ndegree of crosslingual transfer. All of our results are empirically observed\nand measured, and we make our code and data publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:21:25 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Choi", "Hyunjin", ""], ["Kim", "Judong", ""], ["Joe", "Seongho", ""], ["Min", "Seungjai", ""], ["Gwon", "Youngjune", ""]]}, {"id": "2101.10650", "submitter": "Raoul Blin", "authors": "Raoul Blin", "title": "Neural machine translation, corpus and frugality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In machine translation field, in both academia and industry, there is a\ngrowing interest in increasingly powerful systems, using corpora of several\nhundred million to several billion examples. These systems represent the\nstate-of-the-art. Here we defend the idea of developing in parallel <<frugal>>\nbilingual translation systems, trained with relatively small corpora. Based on\nthe observation of a standard human professional translator, we estimate that\nthe corpora should be composed at maximum of a monolingual sub-corpus of 75\nmillion examples for the source language, a second monolingual sub-corpus of 6\nmillion examples for the target language, and an aligned bilingual sub-corpus\nof 6 million bi-examples. A less desirable alternative would be an aligned\nbilingual corpus of 47.5 million bi-examples.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:22:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Blin", "Raoul", ""]]}, {"id": "2101.10708", "submitter": "Zhuang Li", "authors": "Zhuang Li, Lizhen Qu, Shuo Huang, Gholamreza Haffari", "title": "Few-Shot Semantic Parsing for New Predicates", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we investigate the problems of semantic parsing in a few-shot\nlearning setting. In this setting, we are provided with utterance-logical form\npairs per new predicate. The state-of-the-art neural semantic parsers achieve\nless than 25% accuracy on benchmark datasets when k= 1. To tackle this problem,\nwe proposed to i) apply a designated meta-learning method to train the model;\nii) regularize attention scores with alignment statistics; iii) apply a\nsmoothing technique in pre-training. As a result, our method consistently\noutperforms all the baselines in both one and two-shot settings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:08:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Li", "Zhuang", ""], ["Qu", "Lizhen", ""], ["Huang", "Shuo", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2101.10713", "submitter": "Hitomi Yanaka", "authors": "Hitomi Yanaka, Koji Mineshima, Kentaro Inui", "title": "Exploring Transitivity in Neural NLI Models through Veridicality", "comments": "accepted by EACL2021 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of deep neural networks in natural language\nprocessing, the extent to which they can demonstrate human-like generalization\ncapacities for natural language understanding remains unclear. We explore this\nissue in the domain of natural language inference (NLI), focusing on the\ntransitivity of inference relations, a fundamental property for systematically\ndrawing inferences. A model capturing transitivity can compose basic inference\npatterns and draw new inferences. We introduce an analysis method using\nsynthetic and naturalistic NLI datasets involving clause-embedding verbs to\nevaluate whether models can perform transitivity inferences composed of\nveridical inferences and arbitrary inference types. We find that current NLI\nmodels do not perform consistently well on transitivity inference tasks,\nsuggesting that they lack the generalization capacity for drawing composite\ninferences from provided training examples. The data and code for our analysis\nare publicly available at https://github.com/verypluming/transitivity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:18:35 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yanaka", "Hitomi", ""], ["Mineshima", "Koji", ""], ["Inui", "Kentaro", ""]]}, {"id": "2101.10717", "submitter": "Yi Zhu", "authors": "Yi Zhu, Ehsan Shareghi, Yingzhen Li, Roi Reichart, Anna Korhonen", "title": "Combining Deep Generative Models and Multi-lingual Pretraining for\n  Semi-supervised Document Classification", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning through deep generative models and multi-lingual\npretraining techniques have orchestrated tremendous success across different\nareas of NLP. Nonetheless, their development has happened in isolation, while\nthe combination of both could potentially be effective for tackling\ntask-specific labelled data shortage. To bridge this gap, we combine\nsemi-supervised deep generative models and multi-lingual pretraining to form a\npipeline for document classification task. Compared to strong supervised\nlearning baselines, our semi-supervised classification framework is highly\ncompetitive and outperforms the state-of-the-art counterparts in low-resource\nsettings across several languages.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:26:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhu", "Yi", ""], ["Shareghi", "Ehsan", ""], ["Li", "Yingzhen", ""], ["Reichart", "Roi", ""], ["Korhonen", "Anna", ""]]}, {"id": "2101.10726", "submitter": "Prodromos Malakasiotis", "authors": "Ilias Chalkidis, Manos Fergadiotis, Nikolaos Manginas, Eva Katakalou\n  and Prodromos Malakasiotis", "title": "Regulatory Compliance through Doc2Doc Information Retrieval: A case\n  study in EU/UK legislation where text similarity has limitations", "comments": "Accepted for publication by EACL 2021, 13 pages including references\n  and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Major scandals in corporate history have urged the need for regulatory\ncompliance, where organizations need to ensure that their controls (processes)\ncomply with relevant laws, regulations, and policies. However, keeping track of\nthe constantly changing legislation is difficult, thus organizations are\nincreasingly adopting Regulatory Technology (RegTech) to facilitate the\nprocess. To this end, we introduce regulatory information retrieval (REG-IR),\nan application of document-to-document information retrieval (DOC2DOC IR),\nwhere the query is an entire document making the task more challenging than\ntraditional IR where the queries are short. Furthermore, we compile and release\ntwo datasets based on the relationships between EU directives and UK\nlegislation. We experiment on these datasets using a typical two-step pipeline\napproach comprising a pre-fetcher and a neural re-ranker. Experimenting with\nvarious pre-fetchers from BM25 to k nearest neighbors over representations from\nseveral BERT models, we show that fine-tuning a BERT model on an in-domain\nclassification task produces the best representations for IR. We also show that\nneural re-rankers under-perform due to contradicting supervision, i.e., similar\nquery-document pairs with opposite labels. Thus, they are biased towards the\npre-fetcher's score. Interestingly, applying a date filter further improves the\nperformance, showcasing the importance of the time dimension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:38:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chalkidis", "Ilias", ""], ["Fergadiotis", "Manos", ""], ["Manginas", "Nikolaos", ""], ["Katakalou", "Eva", ""], ["Malakasiotis", "Prodromos", ""]]}, {"id": "2101.10759", "submitter": "Xutan Peng", "authors": "Xutan Peng, Yi Zheng, Chenghua Lin, Advaith Siddharthan", "title": "Summarising Historical Text in Modern Languages", "comments": "To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of historical text summarisation, where documents in\nhistorical forms of a language are summarised in the corresponding modern\nlanguage. This is a fundamentally important routine to historians and digital\nhumanities researchers but has never been automated. We compile a high-quality\ngold-standard text summarisation dataset, which consists of historical German\nand Chinese news from hundreds of years ago summarised in modern German or\nChinese. Based on cross-lingual transfer learning techniques, we propose a\nsummarisation model that can be trained even with no cross-lingual (historical\nto modern) parallel data, and further benchmark it against state-of-the-art\nalgorithms. We report automatic and human evaluations that distinguish the\nhistoric to modern language summarisation task from standard cross-lingual\nsummarisation (i.e., modern to modern language), highlight the distinctness and\nvalue of our dataset, and demonstrate that our transfer learning approach\noutperforms standard cross-lingual benchmarks on this task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:00:07 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:17:02 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Peng", "Xutan", ""], ["Zheng", "Yi", ""], ["Lin", "Chenghua", ""], ["Siddharthan", "Advaith", ""]]}, {"id": "2101.10848", "submitter": "Veysel Kocaman Vk", "authors": "Veysel Kocaman, David Talby", "title": "Spark NLP: Natural Language Understanding at Scale", "comments": "=Accepted as a publication in Elsevier, Software Impacts Journal.\n  arXiv admin note: substantial text overlap with arXiv:2012.04005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spark NLP is a Natural Language Processing (NLP) library built on top of\nApache Spark ML. It provides simple, performant and accurate NLP annotations\nfor machine learning pipelines that can scale easily in a distributed\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\nhealthcare organizations as the worlds most widely used NLP library in the\nenterprise.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:11:52 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kocaman", "Veysel", ""], ["Talby", "David", ""]]}, {"id": "2101.10917", "submitter": "Christine De Kock", "authors": "Christine de Kock and Andreas Vlachos", "title": "I Beg to Differ: A study of constructive disagreement in online\n  conversations", "comments": "Accepted to appear in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disagreements are pervasive in human communication. In this paper we\ninvestigate what makes disagreement constructive. To this end, we construct\nWikiDisputes, a corpus of 7 425 Wikipedia Talk page conversations that contain\ncontent disputes, and define the task of predicting whether disagreements will\nbe escalated to mediation by a moderator. We evaluate feature-based models with\nlinguistic markers from previous work, and demonstrate that their performance\nis improved by using features that capture changes in linguistic markers\nthroughout the conversations, as opposed to averaged values. We develop a\nvariety of neural models and show that taking into account the structure of the\nconversation improves predictive accuracy, exceeding that of feature-based\nmodels. We assess our best neural model in terms of both predictive accuracy\nand uncertainty by evaluating its behaviour when it is only exposed to the\nbeginning of the conversation, finding that model accuracy improves and\nuncertainty reduces as models are exposed to more information.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:36:43 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["de Kock", "Christine", ""], ["Vlachos", "Andreas", ""]]}, {"id": "2101.10927", "submitter": "Artur Kulmizev", "authors": "Vinit Ravishankar, Artur Kulmizev, Mostafa Abdou, Anders S{\\o}gaard,\n  Joakim Nivre", "title": "Attention Can Reflect Syntactic Structure (If You Let It)", "comments": null, "journal-ref": "EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the popularization of the Transformer as a general-purpose feature\nencoder for NLP, many studies have attempted to decode linguistic structure\nfrom its novel multi-head attention mechanism. However, much of such work\nfocused almost exclusively on English -- a language with rigid word order and a\nlack of inflectional morphology. In this study, we present decoding experiments\nfor multilingual BERT across 18 languages in order to test the generalizability\nof the claim that dependency syntax is reflected in attention patterns. We show\nthat full trees can be decoded above baseline accuracy from single attention\nheads, and that individual relations are often tracked by the same heads across\nlanguages. Furthermore, in an attempt to address recent debates about the\nstatus of attention as an explanatory mechanism, we experiment with fine-tuning\nmBERT on a supervised parsing objective while freezing different series of\nparameters. Interestingly, in steering the objective to learn explicit\nlinguistic structure, we find much of the same structure represented in the\nresulting attention patterns, with interesting differences with respect to\nwhich parameters are frozen.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:49:16 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ravishankar", "Vinit", ""], ["Kulmizev", "Artur", ""], ["Abdou", "Mostafa", ""], ["S\u00f8gaard", "Anders", ""], ["Nivre", "Joakim", ""]]}, {"id": "2101.10952", "submitter": "Debanjan Ghosh", "authors": "Debanjan Ghosh, Ritvik Shrivastava, and Smaranda Muresan", "title": "\"Laughing at you or with you\": The Role of Sarcasm in Shaping the\n  Disagreement Space", "comments": "Accepted in the 16th conference of the European Chapter of the\n  Association for Computational Linguistics (EACL). Long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting arguments in online interactions is useful to understand how\nconflicts arise and get resolved. Users often use figurative language, such as\nsarcasm, either as persuasive devices or to attack the opponent by an ad\nhominem argument. To further our understanding of the role of sarcasm in\nshaping the disagreement space, we present a thorough experimental setup using\na corpus annotated with both argumentative moves (agree/disagree) and sarcasm.\nWe exploit joint modeling in terms of (a) applying discrete features that are\nuseful in detecting sarcasm to the task of argumentative relation\nclassification (agree/disagree/none), and (b) multitask learning for\nargumentative relation classification and sarcasm detection using deep learning\narchitectures (e.g., dual Long Short-Term Memory (LSTM) with hierarchical\nattention and Transformer-based architectures). We demonstrate that modeling\nsarcasm improves the argumentative relation classification task\n(agree/disagree/none) in all setups.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:19:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ghosh", "Debanjan", ""], ["Shrivastava", "Ritvik", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2101.11038", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke\n  Zettlemoyer, Sonal Gupta", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:18:27 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Aghajanyan", "Armen", ""], ["Gupta", "Anchit", ""], ["Shrivastava", "Akshat", ""], ["Chen", "Xilun", ""], ["Zettlemoyer", "Luke", ""], ["Gupta", "Sonal", ""]]}, {"id": "2101.11040", "submitter": "Zhiyi Ma", "authors": "Zhiyi Ma, Sergey Edunov, Michael Auli", "title": "A Comparison of Approaches to Document-level Machine Translation", "comments": "10 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level machine translation conditions on surrounding sentences to\nproduce coherent translations. There has been much recent work in this area\nwith the introduction of custom model architectures and decoding algorithms.\nThis paper presents a systematic comparison of selected approaches from the\nliterature on two benchmarks for which document-level phenomena evaluation\nsuites exist. We find that a simple method based purely on back-translating\nmonolingual document-level data performs as well as much more elaborate\nalternatives, both in terms of document-level metrics as well as human\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:21:09 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ma", "Zhiyi", ""], ["Edunov", "Sergey", ""], ["Auli", "Michael", ""]]}, {"id": "2101.11043", "submitter": "Isabel Papadimitriou", "authors": "Isabel Papadimitriou, Ethan A. Chi, Richard Futrell, Kyle Mahowald", "title": "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how Multilingual BERT (mBERT) encodes grammar by examining how\nthe high-order grammatical feature of morphosyntactic alignment (how different\nlanguages define what counts as a \"subject\") is manifested across the embedding\nspaces of different languages. To understand if and how morphosyntactic\nalignment affects contextual embedding spaces, we train classifiers to recover\nthe subjecthood of mBERT embeddings in transitive sentences (which do not\ncontain overt information about morphosyntactic alignment) and then evaluate\nthem zero-shot on intransitive sentences (where subjecthood classification\ndepends on alignment), within and across languages. We find that the resulting\nclassifier distributions reflect the morphosyntactic alignment of their\ntraining languages. Our results demonstrate that mBERT representations are\ninfluenced by high-level grammatical features that are not manifested in any\none input sentence, and that this is robust across languages. Further examining\nthe characteristics that our classifiers rely on, we find that features such as\npassive voice, animacy and case strongly correlate with classification\ndecisions, suggesting that mBERT does not encode subjecthood purely\nsyntactically, but that subjecthood embedding is continuous and dependent on\nsemantic and discourse factors, as is proposed in much of the functional\nlinguistics literature. Together, these results provide insight into how\ngrammatical features manifest in contextual embedding spaces, at a level of\nabstraction not covered by previous work.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:21:59 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Papadimitriou", "Isabel", ""], ["Chi", "Ethan A.", ""], ["Futrell", "Richard", ""], ["Mahowald", "Kyle", ""]]}, {"id": "2101.11059", "submitter": "Muthu Kumar Chandrasekaran", "authors": "Kailash Karthik Saravanakumar, Miguel Ballesteros, Muthu Kumar\n  Chandrasekaran, Kathleen McKeown", "title": "Event-Driven News Stream Clustering using Entity-Aware Contextual\n  Embeddings", "comments": "To appear in Proceedings of The 16th Conference of the European\n  Chapter of the Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for online news stream clustering that is a variant of\nthe non-parametric streaming K-means algorithm. Our model uses a combination of\nsparse and dense document representations, aggregates document-cluster\nsimilarity along these multiple representations and makes the clustering\ndecision using a neural classifier. The weighted document-cluster similarity\nmodel is learned using a novel adaptation of the triplet loss into a linear\nclassification objective. We show that the use of a suitable fine-tuning\nobjective and external knowledge in pre-trained transformer models yields\nsignificant improvements in the effectiveness of contextual embeddings for\nclustering. Our model achieves a new state-of-the-art on a standard stream\nclustering dataset of English documents.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:58:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Saravanakumar", "Kailash Karthik", ""], ["Ballesteros", "Miguel", ""], ["Chandrasekaran", "Muthu Kumar", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2101.11109", "submitter": "Djam\\'e Seddah", "authors": "Benjamin Muller and Yanai Elazar and Beno\\^it Sagot and Djam\\'e Seddah", "title": "First Align, then Predict: Understanding the Cross-Lingual Ability of\n  Multilingual BERT", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multilingual pretrained language models have demonstrated remarkable\nzero-shot cross-lingual transfer capabilities. Such transfer emerges by\nfine-tuning on a task of interest in one language and evaluating on a distinct\nlanguage, not seen during the fine-tuning. Despite promising results, we still\nlack a proper understanding of the source of this transfer. Using a novel layer\nablation technique and analyses of the model's internal representations, we\nshow that multilingual BERT, a popular multilingual language model, can be\nviewed as the stacking of two sub-networks: a multilingual encoder followed by\na task-specific language-agnostic predictor. While the encoder is crucial for\ncross-lingual transfer and remains mostly unchanged during fine-tuning, the\ntask predictor has little importance on the transfer and can be reinitialized\nduring fine-tuning. We present extensive experiments with three distinct tasks,\nseventeen typologically diverse languages and multiple domains to support our\nhypothesis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:12:38 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Muller", "Benjamin", ""], ["Elazar", "Yanai", ""], ["Sagot", "Beno\u00eet", ""], ["Seddah", "Djam\u00e9", ""]]}, {"id": "2101.11112", "submitter": "Bing Li", "authors": "Bing Li, Yujie He and Wenjin Xu", "title": "Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New\n  Approach Using XLM-RoBERTa Alignment", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach for cross-lingual Named Entity Recognition (NER)\nzero-shot transfer using parallel corpora. We built an entity alignment model\non top of XLM-RoBERTa to project the entities detected on the English part of\nthe parallel data to the target language sentences, whose accuracy surpasses\nall previous unsupervised models. With the alignment model we can get\npseudo-labeled NER data set in the target language to train task-specific\nmodel. Unlike using translation methods, this approach benefits from natural\nfluency and nuances in target-language original corpus. We also propose a\nmodified loss function similar to focal loss but assigns weights in the\nopposite direction to further improve the model training on noisy\npseudo-labeled data set. We evaluated this proposed approach over 4 target\nlanguages on benchmark data sets and got competitive F1 scores compared to most\nrecent SOTA models. We also gave extra discussions about the impact of parallel\ncorpus size and domain on the final transfer performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:19:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Bing", ""], ["He", "Yujie", ""], ["Xu", "Wenjin", ""]]}, {"id": "2101.11122", "submitter": "Bing Li", "authors": "Bing Li", "title": "Named Entity Recognition in the Style of Object Detection", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a two-stage method for named entity recognition\n(NER), especially for nested NER. We borrowed the idea from the two-stage\nObject Detection in computer vision and the way how they construct the loss\nfunction. First, a region proposal network generates region candidates and then\na second-stage model discriminates and classifies the entity and makes the\nfinal prediction. We also designed a special loss function for the second-stage\ntraining that predicts the entityness and entity type at the same time. The\nmodel is built on top of pretrained BERT encoders, and we tried both BERT base\nand BERT large models. For experiments, we first applied it to flat NER tasks\nsuch as CoNLL2003 and OntoNotes 5.0 and got comparable results with traditional\nNER models using sequence labeling methodology. We then tested the model on the\nnested named entity recognition task ACE2005 and Genia, and got F1 score of\n85.6$\\%$ and 76.8$\\%$ respectively. In terms of the second-stage training, we\nfound that adding extra randomly selected regions plays an important role in\nimproving the precision. We also did error profiling to better evaluate the\nperformance of the model in different circumstances for potential improvements\nin the future.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:47:05 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Bing", ""]]}, {"id": "2101.11131", "submitter": "Beilei Xiang", "authors": "Beilei Xiang, Changbing Yang, Yu Li, Alex Warstadt and Katharina Kann", "title": "CLiMP: A Benchmark for Chinese Language Model Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistically informed analyses of language models (LMs) contribute to the\nunderstanding and improvement of these models. Here, we introduce the corpus of\nChinese linguistic minimal pairs (CLiMP), which can be used to investigate what\nknowledge Chinese LMs acquire. CLiMP consists of sets of 1,000 minimal pairs\n(MPs) for 16 syntactic contrasts in Mandarin, covering 9 major Mandarin\nlinguistic phenomena. The MPs are semi-automatically generated, and human\nagreement with the labels in CLiMP is 95.8%. We evaluated 11 different LMs on\nCLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier-noun\nagreement and verb complement selection are the phenomena that models generally\nperform best at. However, models struggle the most with the ba construction,\nbinding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8%\naverage accuracy, while the performances of LSTMs and 5-grams are only\nmoderately above chance level.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:16:29 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Xiang", "Beilei", ""], ["Yang", "Changbing", ""], ["Li", "Yu", ""], ["Warstadt", "Alex", ""], ["Kann", "Katharina", ""]]}, {"id": "2101.11134", "submitter": "Alexandry Augustin", "authors": "A. Augustin, A. Papangelis, M. Kotti, P. Vougiouklis, J. Hare, N.\n  Braunschweiler", "title": "Open-domain Topic Identification of Out-of-domain Utterances using\n  Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Users of spoken dialogue systems (SDS) expect high quality interactions\nacross a wide range of diverse topics. However, the implementation of SDS\ncapable of responding to every conceivable user utterance in an informative way\nis a challenging problem. Multi-domain SDS must necessarily identify and deal\nwith out-of-domain (OOD) utterances to generate appropriate responses as users\ndo not always know in advance what domains the SDS can handle. To address this\nproblem, we extend the current state-of-the-art in multi-domain SDS by\nestimating the topic of OOD utterances using external knowledge representation\nfrom Wikipedia. Experimental results on real human-to-human dialogues showed\nthat our approach does not degrade domain prediction performance when compared\nto the base model. But more significantly, our joint training achieves more\naccurate predictions of the nearest Wikipedia article by up to about 30% when\ncompared to the benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:46:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Augustin", "A.", ""], ["Papangelis", "A.", ""], ["Kotti", "M.", ""], ["Vougiouklis", "P.", ""], ["Hare", "J.", ""], ["Braunschweiler", "N.", ""]]}, {"id": "2101.11155", "submitter": "Shubhanshu Mishra", "authors": "Sudhanshu Mishra, Shivangi Prasad, Shubhanshu Mishra", "title": "Exploring multi-task multi-lingual learning of transformer models for\n  hate speech and offensive speech identification in social media", "comments": "\"To be published in SN Computer Science at\n  https://doi.org/10.1007/s42979-021-00455-5\" \"30 pages, 6 figures\" \"Code\n  available at https://github.com/socialmediaie/MTML_HateSpeech\"", "journal-ref": null, "doi": "10.1007/s42979-021-00455-5", "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hate Speech has become a major content moderation issue for online social\nmedia platforms. Given the volume and velocity of online content production, it\nis impossible to manually moderate hate speech related content on any platform.\nIn this paper we utilize a multi-task and multi-lingual approach based on\nrecently proposed Transformer Neural Networks to solve three sub-tasks for hate\nspeech. These sub-tasks were part of the 2019 shared task on hate speech and\noffensive content (HASOC) identification in Indo-European languages. We expand\non our submission to that competition by utilizing multi-task models which are\ntrained using three approaches, a) multi-task learning with separate task\nheads, b) back-translation, and c) multi-lingual training. Finally, we\ninvestigate the performance of various models and identify instances where the\nTransformer based models perform differently and better. We show that it is\npossible to to utilize different combined approaches to obtain models that can\ngeneralize easily on different languages and tasks, while trading off slight\naccuracy (in some cases) for a much reduced inference time compute cost. We\nopen source an updated version of our HASOC 2019 code with the new improvements\nat https://github.com/socialmediaie/MTML_HateSpeech.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:25:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mishra", "Sudhanshu", ""], ["Prasad", "Shivangi", ""], ["Mishra", "Shubhanshu", ""]]}, {"id": "2101.11177", "submitter": "Stefan Larson", "authors": "Jacob Solawetz, Stefan Larson", "title": "LSOIE: A Large-Scale Dataset for Supervised Open Information Extraction", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Information Extraction (OIE) systems seek to compress the factual\npropositions of a sentence into a series of n-ary tuples. These tuples are\nuseful for downstream tasks in natural language processing like knowledge base\ncreation, textual entailment, and natural language understanding. However,\ncurrent OIE datasets are limited in both size and diversity. We introduce a new\ndataset by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset\n(LSOIE). Our LSOIE dataset is 20 times larger than the next largest\nhuman-annotated OIE dataset. We construct and evaluate several benchmark OIE\nmodels on LSOIE, providing baselines for future improvements on the task. Our\nLSOIE data, models, and code are made publicly available\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 02:49:26 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Solawetz", "Jacob", ""], ["Larson", "Stefan", ""]]}, {"id": "2101.11178", "submitter": "Yutao Zhu", "authors": "Yutao Zhu, Kun Zhou, Jian-Yun Nie, Shengchao Liu, Zhicheng Dou", "title": "Neural Sentence Ordering Based on Constraint Graphs", "comments": "AAAI 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence ordering aims at arranging a list of sentences in the correct order.\nBased on the observation that sentence order at different distances may rely on\ndifferent types of information, we devise a new approach based on\nmulti-granular orders between sentences. These orders form multiple constraint\ngraphs, which are then encoded by Graph Isomorphism Networks and fused into\nsentence representations. Finally, sentence order is determined using the\norder-enhanced sentence representations. Our experiments on five benchmark\ndatasets show that our method outperforms all the existing baselines\nsignificantly, achieving a new state-of-the-art performance. The results\ndemonstrate the advantage of considering multiple types of order information\nand using graph neural networks to integrate sentence content and order\ninformation for the task. Our code is available at\nhttps://github.com/DaoD/ConstraintGraph4NSO.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 02:53:10 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:22:46 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhu", "Yutao", ""], ["Zhou", "Kun", ""], ["Nie", "Jian-Yun", ""], ["Liu", "Shengchao", ""], ["Dou", "Zhicheng", ""]]}, {"id": "2101.11204", "submitter": "Jiaxin Bai", "authors": "Jiaxin Bai, Hongming Zhang, Yangqiu Song, and Kun Xu", "title": "Joint Coreference Resolution and Character Linking for Multiparty\n  Conversation", "comments": "EACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Character linking, the task of linking mentioned people in conversations to\nthe real world, is crucial for understanding the conversations. For the\nefficiency of communication, humans often choose to use pronouns (e.g., \"she\")\nor normal phrases (e.g., \"that girl\") rather than named entities (e.g.,\n\"Rachel\") in the spoken language, which makes linking those mentions to real\npeople a much more challenging than a regular entity linking task. To address\nthis challenge, we propose to incorporate the richer context from the\ncoreference relations among different mentions to help the linking. On the\nother hand, considering that finding coreference clusters itself is not a\ntrivial task and could benefit from the global character information, we\npropose to jointly solve these two tasks. Specifically, we propose C$^2$, the\njoint learning model of Coreference resolution and Character linking. The\nexperimental results demonstrate that C$^2$ can significantly outperform\nprevious works on both tasks. Further analyses are conducted to analyze the\ncontribution of all modules in the proposed model and the effect of all\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:47:04 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 08:25:29 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bai", "Jiaxin", ""], ["Zhang", "Hongming", ""], ["Song", "Yangqiu", ""], ["Xu", "Kun", ""]]}, {"id": "2101.11212", "submitter": "Muhammad Asif Ali", "authors": "Muhammad Asif Ali, Yifang Sun, Bing Li, Wei Wang", "title": "Fine-Grained Named Entity Typing over Distantly Supervised Data via\n  Refinement in Hyperbolic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-Grained Named Entity Typing (FG-NET) aims at classifying the entity\nmentions into a wide range of entity types (usually hundreds) depending upon\nthe context. While distant supervision is the most common way to acquire\nsupervised training data, it brings in label noise, as it assigns type labels\nto the entity mentions irrespective of mentions' context. In attempts to deal\nwith the label noise, leading research on the FG-NET assumes that the\nfine-grained entity typing data possesses a euclidean nature, which restraints\nthe ability of the existing models in combating the label noise. Given the fact\nthat the fine-grained type hierarchy exhibits a hierarchal structure, it makes\nhyperbolic space a natural choice to model the FG-NET data. In this research,\nwe propose FGNET-HR, a novel framework that benefits from the hyperbolic\ngeometry in combination with the graph structures to perform entity typing in a\nperformance-enhanced fashion. FGNET-HR initially uses LSTM networks to encode\nthe mention in relation with its context, later it forms a graph to\ndistill/refine the mention's encodings in the hyperbolic space. Finally, the\nrefined mention encoding is used for entity typing. Experimentation using\ndifferent benchmark datasets shows that FGNET-HR improves the performance on\nFG-NET by up to 3.5% in terms of strict accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:39:05 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ali", "Muhammad Asif", ""], ["Sun", "Yifang", ""], ["Li", "Bing", ""], ["Wang", "Wei", ""]]}, {"id": "2101.11214", "submitter": "Siddhant Garg", "authors": "Siddhant Garg, Goutham Ramakrishnan, Varun Thumbe", "title": "Towards Robustness to Label Noise in Text Classification via Noise\n  Modeling", "comments": "Accepted at ICLR 2021 RobustML and S2D-OLAD Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large datasets in NLP suffer from noisy labels, due to erroneous automatic\nand human annotation procedures. We study the problem of text classification\nwith label noise, and aim to capture this noise through an auxiliary noise\nmodel over the classifier. We first assign a probability score to each training\nsample of having a noisy label, through a beta mixture model fitted on the\nlosses at an early epoch of training. Then, we use this score to selectively\nguide the learning of the noise model and classifier. Our empirical evaluation\non two text classification tasks shows that our approach can improve over the\nbaseline accuracy, and prevent over-fitting to the noise.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:41:57 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 02:48:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Garg", "Siddhant", ""], ["Ramakrishnan", "Goutham", ""], ["Thumbe", "Varun", ""]]}, {"id": "2101.11216", "submitter": "Kemal Kurniawan", "authors": "Kemal Kurniawan, Lea Frermann, Philip Schulz, Trevor Cohn", "title": "PPT: Parsimonious Parser Transfer for Unsupervised Cross-Lingual\n  Adaptation", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-lingual transfer is a leading technique for parsing low-resource\nlanguages in the absence of explicit supervision. Simple `direct transfer' of a\nlearned model based on a multilingual input encoding has provided a strong\nbenchmark. This paper presents a method for unsupervised cross-lingual transfer\nthat improves over direct transfer systems by using their output as implicit\nsupervision as part of self-training on unlabelled text in the target language.\nThe method assumes minimal resources and provides maximal flexibility by (a)\naccepting any pre-trained arc-factored dependency parser; (b) assuming no\naccess to source language data; (c) supporting both projective and\nnon-projective parsing; and (d) supporting multi-source transfer. With English\nas the source language, we show significant improvements over state-of-the-art\ntransfer models on both distant and nearby languages, despite our conceptually\nsimpler approach. We provide analyses of the choice of source languages for\nmulti-source transfer, and the advantage of non-projective parsing. Our code is\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:52:26 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kurniawan", "Kemal", ""], ["Frermann", "Lea", ""], ["Schulz", "Philip", ""], ["Cohn", "Trevor", ""]]}, {"id": "2101.11268", "submitter": "Suyuchen Wang", "authors": "Suyuchen Wang, Ruihui Zhao, Xi Chen, Yefeng Zheng and Bang Liu", "title": "Enquire One's Parent and Child Before Decision: Fully Exploit\n  Hierarchical Structure for Self-Supervised Taxonomy Expansion", "comments": "12 pages, 6 figures. To appear in The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taxonomy is a hierarchically structured knowledge graph that plays a crucial\nrole in machine intelligence. The taxonomy expansion task aims to find a\nposition for a new term in an existing taxonomy to capture the emerging\nknowledge in the world and keep the taxonomy dynamically updated. Previous\ntaxonomy expansion solutions neglect valuable information brought by the\nhierarchical structure and evaluate the correctness of merely an added edge,\nwhich downgrade the problem to node-pair scoring or mini-path classification.\nIn this paper, we propose the Hierarchy Expansion Framework (HEF), which fully\nexploits the hierarchical structure's properties to maximize the coherence of\nexpanded taxonomy. HEF makes use of taxonomy's hierarchical structure in\nmultiple aspects: i) HEF utilizes subtrees containing most relevant nodes as\nself-supervision data for a complete comparison of parental and sibling\nrelations; ii) HEF adopts a coherence modeling module to evaluate the coherence\nof a taxonomy's subtree by integrating hypernymy relation detection and several\ntree-exclusive features; iii) HEF introduces the Fitting Score for position\nselection, which explicitly evaluates both path and level selections and takes\nfull advantage of parental relations to interchange information for\ndisambiguation and self-correction. Extensive experiments show that by better\nexploiting the hierarchical structure and optimizing taxonomy's coherence, HEF\nvastly surpasses the prior state-of-the-art on three benchmark datasets by an\naverage improvement of 46.7% in accuracy and 32.3% in mean reciprocal rank.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:57:47 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Wang", "Suyuchen", ""], ["Zhao", "Ruihui", ""], ["Chen", "Xi", ""], ["Zheng", "Yefeng", ""], ["Liu", "Bang", ""]]}, {"id": "2101.11272", "submitter": "Kyosuke Nishida", "authors": "Ryota Tanaka, Kyosuke Nishida, Sen Yoshida", "title": "VisualMRC: Machine Reading Comprehension on Document Images", "comments": "Accepted as a full paper at AAAI 2021. The first two authors have\n  equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on machine reading comprehension have focused on text-level\nunderstanding but have not yet reached the level of human understanding of the\nvisual layout and content of real-world documents. In this study, we introduce\na new visual machine reading comprehension dataset, named VisualMRC, wherein\ngiven a question and a document image, a machine reads and comprehends texts in\nthe image to answer the question in natural language. Compared with existing\nvisual question answering (VQA) datasets that contain texts in images,\nVisualMRC focuses more on developing natural language understanding and\ngeneration abilities. It contains 30,000+ pairs of a question and an\nabstractive answer for 10,000+ document images sourced from multiple domains of\nwebpages. We also introduce a new model that extends existing\nsequence-to-sequence models, pre-trained with large-scale text corpora, to take\ninto account the visual layout and content of documents. Experiments with\nVisualMRC show that this model outperformed the base sequence-to-sequence\nmodels and a state-of-the-art VQA model. However, its performance is still\nbelow that of humans on most automatic evaluation metrics. The dataset will\nfacilitate research aimed at connecting vision and language understanding.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:03:06 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:13:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tanaka", "Ryota", ""], ["Nishida", "Kyosuke", ""], ["Yoshida", "Sen", ""]]}, {"id": "2101.11287", "submitter": "Lucas Weber", "authors": "Lucas Weber, Jaap Jumelet, Elia Bruni and Dieuwke Hupkes", "title": "Language Modelling as a Multi-Task Problem", "comments": "Accepted for publication at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to study language modelling as a multi-task\nproblem, bringing together three strands of research: multi-task learning,\nlinguistics, and interpretability. Based on hypotheses derived from linguistic\ntheory, we investigate whether language models adhere to learning principles of\nmulti-task learning during training. To showcase the idea, we analyse the\ngeneralisation behaviour of language models as they learn the linguistic\nconcept of Negative Polarity Items (NPIs). Our experiments demonstrate that a\nmulti-task setting naturally emerges within the objective of the more general\ntask of language modelling.We argue that this insight is valuable for\nmulti-task learning, linguistics and interpretability research and can lead to\nexciting new findings in all three domains.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:47:42 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Weber", "Lucas", ""], ["Jumelet", "Jaap", ""], ["Bruni", "Elia", ""], ["Hupkes", "Dieuwke", ""]]}, {"id": "2101.11298", "submitter": "Julius Steen", "authors": "Julius Steen and Katja Markert", "title": "How to Evaluate a Summarizer: Study Design and Statistical Analysis for\n  Manual Linguistic Quality Evaluation", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual evaluation is essential to judge progress on automatic text\nsummarization. However, we conduct a survey on recent summarization system\npapers that reveals little agreement on how to perform such evaluation studies.\nWe conduct two evaluation experiments on two aspects of summaries' linguistic\nquality (coherence and repetitiveness) to compare Likert-type and ranking\nannotations and show that best choice of evaluation method can vary from one\naspect to another. In our survey, we also find that study parameters such as\nthe overall number of annotators and distribution of annotators to annotation\nitems are often not fully reported and that subsequent statistical analysis\nignores grouping factors arising from one annotator judging multiple summaries.\nUsing our evaluation experiments, we show that the total number of annotators\ncan have a strong impact on study power and that current statistical analysis\nmethods can inflate type I error rates up to eight-fold. In addition, we\nhighlight that for the purpose of system comparison the current practice of\neliciting multiple judgements per summary leads to less powerful and reliable\nannotations given a fixed study budget.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:14:15 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Steen", "Julius", ""], ["Markert", "Katja", ""]]}, {"id": "2101.11302", "submitter": "Niels van der Heijden", "authors": "Niels van der Heijden, Helen Yannakoudakis, Pushkar Mishra, Ekaterina\n  Shutova", "title": "Multilingual and cross-lingual document classification: A meta-learning\n  approach", "comments": "11 pages, 1 figure", "journal-ref": "Association for Computational Linguistics, Proceedings of the 16th\n  Conference of the European Chapter of the Association for Computational\n  Linguistics: Main Volume, 2021, 1966--1976", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The great majority of languages in the world are considered under-resourced\nfor the successful application of deep learning methods. In this work, we\npropose a meta-learning approach to document classification in limited-resource\nsetting and demonstrate its effectiveness in two different settings: few-shot,\ncross-lingual adaptation to previously unseen languages; and multilingual joint\ntraining when limited target-language data is available during training. We\nconduct a systematic comparison of several meta-learning methods, investigate\nmultiple settings in terms of data availability and show that meta-learning\nthrives in settings with a heterogeneous task distribution. We propose a\nsimple, yet effective adjustment to existing meta-learning methods which allows\nfor better and more stable learning, and set a new state of the art on several\nlanguages while performing on-par on others, using only a small amount of\nlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:22:56 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 10:24:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["van der Heijden", "Niels", ""], ["Yannakoudakis", "Helen", ""], ["Mishra", "Pushkar", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2101.11310", "submitter": "Chris Emmery", "authors": "Chris Emmery, \\'Akos K\\'ad\\'ar, Grzegorz Chrupa{\\l}a", "title": "Adversarial Stylometry in the Wild: Transferable Lexical Substitution\n  Attacks on Author Profiling", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Written language contains stylistic cues that can be exploited to\nautomatically infer a variety of potentially sensitive author information.\nAdversarial stylometry intends to attack such models by rewriting an author's\ntext. Our research proposes several components to facilitate deployment of\nthese adversarial attacks in the wild, where neither data nor target models are\naccessible. We introduce a transformer-based extension of a lexical replacement\nattack, and show it achieves high transferability when trained on a weakly\nlabeled corpus -- decreasing target model performance below chance. While not\ncompletely inconspicuous, our more successful attacks also prove notably less\ndetectable by humans. Our framework therefore provides a promising direction\nfor future privacy-preserving adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:42:44 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Emmery", "Chris", ""], ["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "2101.11332", "submitter": "Yevgen Matusevych", "authors": "Yevgen Matusevych, Herman Kamper, Thomas Schatz, Naomi H. Feldman,\n  Sharon Goldwater", "title": "A phonetic model of non-native spoken word processing", "comments": "Accepted for publication in Proceedings of EACL-2021. 11 pages, 5\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-native speakers show difficulties with spoken word processing. Many\nstudies attribute these difficulties to imprecise phonological encoding of\nwords in the lexical memory. We test an alternative hypothesis: that some of\nthese difficulties can arise from the non-native speakers' phonetic perception.\nWe train a computational model of phonetic learning, which has no access to\nphonology, on either one or two languages. We first show that the model\nexhibits predictable behaviors on phone-level and word-level discrimination\ntasks. We then test the model on a spoken word processing task, showing that\nphonology may not be necessary to explain some of the word processing effects\nobserved in non-native speakers. We run an additional analysis of the model's\nlexical representation space, showing that the two training languages are not\nfully separated in that space, similarly to the languages of a bilingual human\nspeaker.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:46:21 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 13:21:42 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Matusevych", "Yevgen", ""], ["Kamper", "Herman", ""], ["Schatz", "Thomas", ""], ["Feldman", "Naomi H.", ""], ["Goldwater", "Sharon", ""]]}, {"id": "2101.11349", "submitter": "Zhenqiao Song", "authors": "Zhenqiao Song, Jiaze Chen, Hao Zhou and Lei Li", "title": "Triangular Bidword Generation for Sponsored Search Auction", "comments": "9 pages, 5 figures, accepted by WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441819", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored search auction is a crucial component of modern search engines. It\nrequires a set of candidate bidwords that advertisers can place bids on.\nExisting methods generate bidwords from search queries or advertisement\ncontent. However, they suffer from the data noise in <query, bidword> and\n<advertisement, bidword> pairs. In this paper, we propose a triangular bidword\ngeneration model (TRIDENT), which takes the high-quality data of paired <query,\nadvertisement> as a supervision signal to indirectly guide the bidword\ngeneration process. Our proposed model is simple yet effective: by using\nbidword as the bridge between search query and advertisement, the generation of\nsearch query, advertisement and bidword can be jointly learned in the\ntriangular training framework. This alleviates the problem that the training\ndata of bidword may be noisy. Experimental results, including automatic and\nhuman evaluations, show that our proposed TRIDENT can generate relevant and\ndiverse bidwords for both search queries and advertisements. Our evaluation on\nonline real data validates the effectiveness of the TRIDENT's generated\nbidwords for product search.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:25:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Song", "Zhenqiao", ""], ["Chen", "Jiaze", ""], ["Zhou", "Hao", ""], ["Li", "Lei", ""]]}, {"id": "2101.11360", "submitter": "Yen-Ting Lin", "authors": "Yen-Ting Lin, Yun-Nung Chen", "title": "An Empirical Study of Cross-Lingual Transferability in Generative\n  Dialogue State Tracker", "comments": "DSTC9 Workshop - AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a rapid development in data-driven task-oriented dialogue\nsystems with the benefit of large-scale datasets. However, the progress of\ndialogue systems in low-resource languages lags far behind due to the lack of\nhigh-quality data. To advance the cross-lingual technology in building dialog\nsystems, DSTC9 introduces the task of cross-lingual dialog state tracking,\nwhere we test the DST module in a low-resource language given the rich-resource\ntraining dataset.\n  This paper studies the transferability of a cross-lingual generative dialogue\nstate tracking system using a multilingual pre-trained seq2seq model. We\nexperiment under different settings, including joint-training or pre-training\non cross-lingual and cross-ontology datasets. We also find out the low\ncross-lingual transferability of our approaches and provides investigation and\ndiscussion.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:45:55 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lin", "Yen-Ting", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2101.11363", "submitter": "Hyunjae Lee", "authors": "Hyunjae Lee, Jaewoong Yoon, Bonggyu Hwang, Seongho Joe, Seungjai Min,\n  Youngjune Gwon", "title": "KoreALBERT: Pretraining a Lite BERT Model for Korean Language\n  Understanding", "comments": "7 pages, 1 figure, to be published in 25th International Conference\n  on Pattern Recognition, ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Lite BERT (ALBERT) has been introduced to scale up deep bidirectional\nrepresentation learning for natural languages. Due to the lack of pretrained\nALBERT models for Korean language, the best available practice is the\nmultilingual model or resorting back to the any other BERT-based model. In this\npaper, we develop and pretrain KoreALBERT, a monolingual ALBERT model\nspecifically for Korean language understanding. We introduce a new training\nobjective, namely Word Order Prediction (WOP), and use alongside the existing\nMLM and SOP criteria to the same architecture and model parameters. Despite\nhaving significantly fewer model parameters (thus, quicker to train), our\npretrained KoreALBERT outperforms its BERT counterpart on 6 different NLU\ntasks. Consistent with the empirical results in English by Lan et al.,\nKoreALBERT seems to improve downstream task performance involving\nmulti-sentence encoding for Korean language. The pretrained KoreALBERT is\npublicly available to encourage research and application development for Korean\nNLP.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:48:53 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lee", "Hyunjae", ""], ["Yoon", "Jaewoong", ""], ["Hwang", "Bonggyu", ""], ["Joe", "Seongho", ""], ["Min", "Seungjai", ""], ["Gwon", "Youngjune", ""]]}, {"id": "2101.11374", "submitter": "Yichao Du", "authors": "Yichao Du, Pengfei Luo, Xudong Hong, Tong Xu, Zhe Zhang, Chao Ren, Yi\n  Zheng, Enhong Chen", "title": "Inheritance-guided Hierarchical Assignment for Clinical Automatic\n  Diagnosis", "comments": "17 pages, 5 figures, DASFAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical diagnosis, which aims to assign diagnosis codes for a patient based\non the clinical note, plays an essential role in clinical decision-making.\nConsidering that manual diagnosis could be error-prone and time-consuming, many\nintelligent approaches based on clinical text mining have been proposed to\nperform automatic diagnosis. However, these methods may not achieve\nsatisfactory results due to the following challenges. First, most of the\ndiagnosis codes are rare, and the distribution is extremely unbalanced. Second,\nexisting methods are challenging to capture the correlation between diagnosis\ncodes. Third, the lengthy clinical note leads to the excessive dispersion of\nkey information related to codes. To tackle these challenges, we propose a\nnovel framework to combine the inheritance-guided hierarchical assignment and\nco-occurrence graph propagation for clinical automatic diagnosis. Specifically,\nwe propose a hierarchical joint prediction strategy to address the challenge of\nunbalanced codes distribution. Then, we utilize graph convolutional neural\nnetworks to obtain the correlation and semantic representations of medical\nontology. Furthermore, we introduce multi attention mechanisms to extract\ncrucial information. Finally, extensive experiments on MIMIC-III dataset\nclearly validate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:16:51 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Du", "Yichao", ""], ["Luo", "Pengfei", ""], ["Hong", "Xudong", ""], ["Xu", "Tong", ""], ["Zhang", "Zhe", ""], ["Ren", "Chao", ""], ["Zheng", "Yi", ""], ["Chen", "Enhong", ""]]}, {"id": "2101.11420", "submitter": "Arya Roy", "authors": "Arya Roy", "title": "Recent Trends in Named Entity Recognition (NER)", "comments": "27 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1708.02709 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large amounts of computer-readable textual data and\nhardware that can process the data has shifted the focus of knowledge projects\ntowards deep learning architecture. Natural Language Processing, particularly\nthe task of Named Entity Recognition is no exception. The bulk of the learning\nmethods that have produced state-of-the-art results have changed the deep\nlearning model, the training method used, the training data itself or the\nencoding of the output of the NER system. In this paper, we review significant\nlearning methods that have been employed for NER in the recent past and how\nthey came about from the linear learning methods of the past. We also cover the\nprogress of related tasks that are upstream or downstream to NER, e.g.,\nsequence tagging, entity linking, etc., wherever the processes in question have\nalso improved NER results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:18:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Roy", "Arya", ""]]}, {"id": "2101.11422", "submitter": "Harsh Jaykumar Jalan", "authors": "Sreyan Ghosh, Sonal Kumar, Harsh Jalan, Hemant Yadav, Rajiv Ratn Shah", "title": "Cisco at AAAI-CAD21 shared task: Predicting Emphasis in Presentation\n  Slides using Contextualized Embeddings", "comments": "7 pages, 5 figures, 10 tables Submitted as a part of CAD-21 workshop\n  at AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes our proposed system for the AAAI-CAD21 shared task:\nPredicting Emphasis in Presentation Slides. In this specific task, given the\ncontents of a slide we are asked to predict the degree of emphasis to be laid\non each word in the slide. We propose 2 approaches to this problem including a\nBiLSTM-ELMo approach and a transformers based approach based on RoBERTa and\nXLNet architectures. We achieve a score of 0.518 on the evaluation leaderboard\nwhich ranks us 3rd and 0.543 on the post-evaluation leaderboard which ranks us\n1st at the time of writing the paper.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 10:43:12 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 07:34:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Ghosh", "Sreyan", ""], ["Kumar", "Sonal", ""], ["Jalan", "Harsh", ""], ["Yadav", "Hemant", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2101.11423", "submitter": "Xiao Fu", "authors": "Xiao Fu and Guijun Zhang", "title": "A More Efficient Chinese Named Entity Recognition base on BERT and\n  Syntactic Analysis", "comments": "11pages,3figures,3tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new Named entity recognition (NER) method to effectively make\nuse of the results of Part-of-speech (POS) tagging, Chinese word segmentation\n(CWS) and parsing while avoiding NER error caused by POS tagging error. This\npaper first uses Stanford natural language process (NLP) tool to annotate\nlarge-scale untagged data so as to reduce the dependence on the tagged data;\nthen a new NLP model, g-BERT model, is designed to compress Bidirectional\nEncoder Representations from Transformers (BERT) model in order to reduce\ncalculation quantity; finally, the model is evaluated based on Chinese NER\ndataset. The experimental results show that the calculation quantity in g-BERT\nmodel is reduced by 60% and performance improves by 2% with Test F1 to 96.5\ncompared with that in BERT model.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:33:39 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Fu", "Xiao", ""], ["Zhang", "Guijun", ""]]}, {"id": "2101.11424", "submitter": "Xianyang Song", "authors": "Weipeng Jing, Xianyang Song, Donglin Di, Houbing Song", "title": "geoGAT: Graph Model Based on Attention Mechanism for Geographic Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of geographic information processing. There are few researches on\ngeographic text classification. However, the application of this task in\nChinese is relatively rare. In our work, we intend to implement a method to\nextract text containing geographical entities from a large number of network\ntext. The geographic information in these texts is of great practical\nsignificance to transportation, urban and rural planning, disaster relief and\nother fields. We use the method of graph convolutional neural network with\nattention mechanism to achieve this function. Graph attention networks is an\nimprovement of graph convolutional neural networks. Compared with GCN, the\nadvantage of GAT is that the attention mechanism is proposed to weight the sum\nof the characteristics of adjacent nodes. In addition, We construct a Chinese\ndataset containing geographical classification from multiple datasets of\nChinese text classification. The Macro-F Score of the geoGAT we used reached\n95\\% on the new Chinese dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:32:15 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Jing", "Weipeng", ""], ["Song", "Xianyang", ""], ["Di", "Donglin", ""], ["Song", "Houbing", ""]]}, {"id": "2101.11425", "submitter": "Venktesh Viswanathan", "authors": "Akansha Gautam, Venktesh V, Sarah Masud", "title": "Fake News Detection System using XLNet model with Topic Distributions:\n  CONSTRAINT@AAAI2021 Shared Task", "comments": "Accepted at CONSTRAINT@AAAI2021 Shared Task for the CONSTRAINT\n  workshop, collocated with AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ease of access to information, and its rapid dissemination over the\ninternet (both velocity and volume), it has become challenging to filter out\ntruthful information from fake ones. The research community is now faced with\nthe task of automatic detection of fake news, which carries real-world\nsocio-political impact. One such research contribution came in the form of the\nConstraint@AAA12021 Shared Task on COVID19 Fake News Detection in English. In\nthis paper, we shed light on a novel method we proposed as a part of this\nshared task. Our team introduced an approach to combine topical distributions\nfrom Latent Dirichlet Allocation (LDA) with contextualized representations from\nXLNet. We also compared our method with existing baselines to show that XLNet +\nTopic Distributions outperforms other approaches by attaining an F1-score of\n0.967.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:23:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Gautam", "Akansha", ""], ["V", "Venktesh", ""], ["Masud", "Sarah", ""]]}, {"id": "2101.11429", "submitter": "Gong Cheng", "authors": "Xiao Li, Yawei Sun, Gong Cheng", "title": "TSQA: Tabular Scenario Based Question Answering", "comments": "9 pages, accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scenario-based question answering (SQA) has attracted an increasing research\ninterest. Compared with the well-studied machine reading comprehension (MRC),\nSQA is a more challenging task: a scenario may contain not only a textual\npassage to read but also structured data like tables, i.e., tabular scenario\nbased question answering (TSQA). AI applications of TSQA such as answering\nmultiple-choice questions in high-school exams require synthesizing data in\nmultiple cells and combining tables with texts and domain knowledge to infer\nanswers. To support the study of this task, we construct GeoTSQA. This dataset\ncontains 1k real questions contextualized by tabular scenarios in the geography\ndomain. To solve the task, we extend state-of-the-art MRC methods with TTGen, a\nnovel table-to-text generator. It generates sentences from variously\nsynthesized tabular data and feeds the downstream MRC method with the most\nuseful sentences. Its sentence ranking model fuses the information in the\nscenario, question, and domain knowledge. Our approach outperforms a variety of\nstrong baseline methods on GeoTSQA.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 02:00:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Xiao", ""], ["Sun", "Yawei", ""], ["Cheng", "Gong", ""]]}, {"id": "2101.11430", "submitter": "Fei Teng", "authors": "Shu Yuan Hu and Fei Teng", "title": "An Explainable CNN Approach for Medical Codes Prediction from Clinical\n  Text", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Method: We develop CNN-based methods for automatic ICD coding based on\nclinical text from intensive care unit (ICU) stays. We come up with the Shallow\nand Wide Attention convolutional Mechanism (SWAM), which allows our model to\nlearn local and low-level features for each label. The key idea behind our\nmodel design is to look for the presence of informative snippets in the\nclinical text that correlated with each code, and we infer that there exists a\ncorrespondence between \"informative snippet\" and convolution filter. Results:\nWe evaluate our approach on MIMIC-III, an open-access dataset of ICU medical\nrecords. Our approach substantially outperforms previous results on top-50\nmedical code prediction on MIMIC-III dataset. We attribute this improvement to\nSWAM, by which the wide architecture gives the model ability to more\nextensively learn the unique features of different codes, and we prove it by\nablation experiment. Besides, we perform manual analysis of the performance\nimbalance between different codes, and preliminary conclude the characteristics\nthat determine the difficulty of learning specific codes. Conclusions: We\npresent SWAM, an explainable CNN approach for multi-label document\nclassification, which employs a wide convolution layer to learn local and\nlow-level features for each label, yields strong improvements over previous\nmetrics on the ICD-9 code prediction task, while providing satisfactory\nexplanations for its internal mechanics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 02:05:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Shu Yuan", ""], ["Teng", "Fei", ""]]}, {"id": "2101.11431", "submitter": "Nicola Melluso", "authors": "Silvia Fareri, Nicola Melluso, Filippo Chiarello, Gualtiero Fantoni", "title": "SkillNER: Mining and Mapping Soft Skills from any Text", "comments": null, "journal-ref": "Expert Systems With Applications 184 (2021) 115544", "doi": "10.1016/j.eswa.2021.115544", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:14:05 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:12:46 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Fareri", "Silvia", ""], ["Melluso", "Nicola", ""], ["Chiarello", "Filippo", ""], ["Fantoni", "Gualtiero", ""]]}, {"id": "2101.11432", "submitter": "Mahboobeh Parsapoor", "authors": "Hillary Ngai, Yoona Park, John Chen and Mahboobeh Parsapoor (Mah\n  Parsa)", "title": "Transformer-Based Models for Question Answering on COVID19", "comments": "7 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In response to the Kaggle's COVID-19 Open Research Dataset (CORD-19)\nchallenge, we have proposed three transformer-based question-answering systems\nusing BERT, ALBERT, and T5 models. Since the CORD-19 dataset is unlabeled, we\nhave evaluated the question-answering models' performance on two labeled\nquestions answers datasets \\textemdash CovidQA and CovidGQA. The BERT-based QA\nsystem achieved the highest F1 score (26.32), while the ALBERT-based QA system\nachieved the highest Exact Match (13.04). However, numerous challenges are\nassociated with developing high-performance question-answering systems for the\nongoing COVID-19 pandemic and future pandemics. At the end of this paper, we\ndiscuss these challenges and suggest potential solutions to address them.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:06:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ngai", "Hillary", "", "Mah\n  Parsa"], ["Park", "Yoona", "", "Mah\n  Parsa"], ["Chen", "John", "", "Mah\n  Parsa"], ["Parsapoor", "Mahboobeh", "", "Mah\n  Parsa"]]}, {"id": "2101.11433", "submitter": "Artem Artemov", "authors": "A. Artemov, A. Veselovskiy, I. Khasenevich, I. Bolokhov", "title": "Analysis of Basic Emotions in Texts Based on BERT Vector Representation", "comments": "8 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the following paper the authors present a GAN-type model and the most\nimportant stages of its development for the task of emotion recognition in\ntext. In particular, we propose an approach for generating a synthetic dataset\nof all possible emotions combinations based on manually labelled incomplete\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:11:21 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 12:47:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Artemov", "A.", ""], ["Veselovskiy", "A.", ""], ["Khasenevich", "I.", ""], ["Bolokhov", "I.", ""]]}, {"id": "2101.11434", "submitter": "Fatemah Husain", "authors": "Fatemah Husain and Ozlem Uzuner", "title": "Exploratory Arabic Offensive Language Dataset Analysis", "comments": "83 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper adding more insights towards resources and datasets used in Arabic\noffensive language research. The main goal of this paper is to guide\nresearchers in Arabic offensive language in selecting appropriate datasets\nbased on their content, and in creating new Arabic offensive language resources\nto support and complement the available ones.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:45:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Husain", "Fatemah", ""], ["Uzuner", "Ozlem", ""]]}, {"id": "2101.11436", "submitter": "Yakup Kutlu", "authors": "Kadir Tohma, Yakup Kutlu", "title": "Challenges Encountered in Turkish Natural Language Processing Studies", "comments": "8 pages, Natural and Engineering Sciences", "journal-ref": "Natural and Engineering Sciences, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language processing is a branch of computer science that combines\nartificial intelligence with linguistics. It aims to analyze a language element\nsuch as writing or speaking with software and convert it into information.\nConsidering that each language has its own grammatical rules and vocabulary\ndiversity, the complexity of the studies in this field is somewhat\nunderstandable. For instance, Turkish is a very interesting language in many\nways. Examples of this are agglutinative word structure, consonant/vowel\nharmony, a large number of productive derivational morphemes (practically\ninfinite vocabulary), derivation and syntactic relations, a complex emphasis on\nvocabulary and phonological rules. In this study, the interesting features of\nTurkish in terms of natural language processing are mentioned. In addition,\nsummary info about natural language processing techniques, systems and various\nsources developed for Turkish are given.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:30:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Tohma", "Kadir", ""], ["Kutlu", "Yakup", ""]]}, {"id": "2101.11437", "submitter": "Christoph Beierle", "authors": "Anne-Kathrin Schumann, Christoph Beierle, Norbert Bl\\\"o{\\ss}ner", "title": "Using Finite-State Machines to Automatically Scan Classical Greek\n  Hexameter", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully automatic approach to the scansion of Classical\nGreek hexameter verse. In particular, the paper describes an algorithm that\nuses deterministic finite-state automata and local linguistic rules to\nimplement a targeted search for valid spondeus patterns and, in addition, a\nweighted finite-state transducer to correct and complete partial analyses and\nto reject invalid candidates. The paper also details the results of an\nempirical evaluation of the annotation quality resulting from this approach on\nhand-annotated data. It is shown that a finite-state approach provides quick\nand linguistically sound analyses of hexameter verses as well as an efficient\nformalisation of linguistic knowledge. The project code is available (see\nhttps://github.com/anetschka/greek_scansion).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:59:46 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Schumann", "Anne-Kathrin", ""], ["Beierle", "Christoph", ""], ["Bl\u00f6\u00dfner", "Norbert", ""]]}, {"id": "2101.11469", "submitter": "Minsu Jang", "authors": "Minsu Jang, Sangwon Seo, Dohyung Kim, Jaeyeon Lee, Jaehong Kim,\n  Jun-Hwan Ahn", "title": "VOTE400(Voide Of The Elderly 400 Hours): A Speech Dataset to Study Voice\n  Interface for Elderly-Care", "comments": "3 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a large-scale Korean speech dataset, called VOTE400,\nthat can be used for analyzing and recognizing voices of the elderly people.\nThe dataset includes about 300 hours of continuous dialog speech and 100 hours\nof read speech, both recorded by the elderly people aged 65 years or over. A\npreliminary experiment showed that speech recognition system trained with\nVOTE400 can outperform conventional systems in speech recognition of elderly\npeople's voice. This work is a multi-organizational effort led by ETRI and\nMINDs Lab Inc. for the purpose of advancing the speech recognition performance\nof the elderly-care robots.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:28:05 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Jang", "Minsu", ""], ["Seo", "Sangwon", ""], ["Kim", "Dohyung", ""], ["Lee", "Jaeyeon", ""], ["Kim", "Jaehong", ""], ["Ahn", "Jun-Hwan", ""]]}, {"id": "2101.11477", "submitter": "Maha Alkhairy", "authors": "Maha Alkhairy", "title": "Medical Segment Coloring of Clinical Notes", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning-based method to identify the segments of\na clinical note corresponding to ICD-9 broad categories which are further\ncolor-coded with respect to 17 ICD-9 categories. The proposed Medical Segment\nColorer (MSC) architecture is a pipeline framework that works in three stages:\n(1) word categorization, (2) phrase allocation, and (3) document\nclassification. MSC uses gated recurrent unit neural networks (GRUs) to map\nfrom an input document to word multi-labels to phrase allocations, and uses\nstatistical median to map phrase allocation to document multi-label. We compute\nvariable length segment coloring from overlapping phrase allocation\nprobabilities. These cross-level bidirectional contextual links identify\nadaptive context and then produce segment coloring. We train and evaluate MSC\nusing the document labeled MIMIC-III clinical notes. Training is conducted\nsolely using document multi-labels without any information on phrases,\nsegments, or words. In addition to coloring a clinical note, MSC generates as\nbyproducts document multi-labeling and word tagging -- creation of ICD9\ncategory keyword lists based on segment coloring. Performance comparison of MSC\nbyproduct document multi-labels versus methods whose purpose is to produce\njustifiable document multi-labels is 64% vs 52.4% micro-average F1-score\nagainst the CAML (CNN attention multi label) method. For evaluation of MSC\nsegment coloring results, medical practitioners independently assigned the\ncolors to broad ICD9 categories given a sample of 40 colored notes and a sample\nof 50 words related to each category based on the word tags. Binary scoring of\nthis evaluation has a median value of 83.3% and mean of 63.7%.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:49:37 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Alkhairy", "Maha", ""]]}, {"id": "2101.11492", "submitter": "Laura P\\'erez-Mayos", "authors": "Laura P\\'erez-Mayos, Roberto Carlini, Miguel Ballesteros, Leo Wanner", "title": "On the Evolution of Syntactic Information Encoded by BERT's\n  Contextualized Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptation of pretrained language models to solve supervised tasks has\nbecome a baseline in NLP, and many recent works have focused on studying how\nlinguistic information is encoded in the pretrained sentence representations.\nAmong other information, it has been shown that entire syntax trees are\nimplicitly embedded in the geometry of such models. As these models are often\nfine-tuned, it becomes increasingly important to understand how the encoded\nknowledge evolves along the fine-tuning. In this paper, we analyze the\nevolution of the embedded syntax trees along the fine-tuning process of BERT\nfor six different tasks, covering all levels of the linguistic structure.\nExperimental results show that the encoded syntactic information is forgotten\n(PoS tagging), reinforced (dependency and constituency parsing) or preserved\n(semantics-related tasks) in different ways along the fine-tuning process\ndepending on the task.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:41:09 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 17:26:56 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["P\u00e9rez-Mayos", "Laura", ""], ["Carlini", "Roberto", ""], ["Ballesteros", "Miguel", ""], ["Wanner", "Leo", ""]]}, {"id": "2101.11562", "submitter": "Ting Yao", "authors": "Yehao Li and Yingwei Pan and Ting Yao and Jingwen Chen and Tao Mei", "title": "Scheduled Sampling in Vision-Language Pretraining with Decoupled\n  Encoder-Decoder Network", "comments": "AAAI 2021; Code is publicly available at:\n  https://github.com/YehLi/TDEN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having impressive vision-language (VL) pretraining with BERT-based\nencoder for VL understanding, the pretraining of a universal encoder-decoder\nfor both VL understanding and generation remains challenging. The difficulty\noriginates from the inherently different peculiarities of the two disciplines,\ne.g., VL understanding tasks capitalize on the unrestricted message passing\nacross modalities, while generation tasks only employ visual-to-textual message\npassing. In this paper, we start with a two-stream decoupled design of\nencoder-decoder structure, in which two decoupled cross-modal encoder and\ndecoder are involved to separately perform each type of proxy tasks, for\nsimultaneous VL understanding and generation pretraining. Moreover, for VL\npretraining, the dominant way is to replace some input visual/word tokens with\nmask tokens and enforce the multi-modal encoder/decoder to reconstruct the\noriginal tokens, but no mask token is involved when fine-tuning on downstream\ntasks. As an alternative, we propose a primary scheduled sampling strategy that\nelegantly mitigates such discrepancy via pretraining encoder-decoder in a\ntwo-pass manner. Extensive experiments demonstrate the compelling\ngeneralizability of our pretrained encoder-decoder by fine-tuning on four VL\nunderstanding and generation downstream tasks. Source code is available at\n\\url{https://github.com/YehLi/TDEN}.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:36:57 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Yehao", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Chen", "Jingwen", ""], ["Mei", "Tao", ""]]}, {"id": "2101.11575", "submitter": "Theresa Breiner", "authors": "Tania Chakraborty, Manasa Prasad, Theresa Breiner, Sandy Ritchie, Daan\n  van Esch", "title": "Mining Large-Scale Low-Resource Pronunciation Data From Wikipedia", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pronunciation modeling is a key task for building speech technology in new\nlanguages, and while solid grapheme-to-phoneme (G2P) mapping systems exist,\nlanguage coverage can stand to be improved. The information needed to build G2P\nmodels for many more languages can easily be found on Wikipedia, but\nunfortunately, it is stored in disparate formats. We report on a system we\nbuilt to mine a pronunciation data set in 819 languages from loosely structured\ntables within Wikipedia. The data includes phoneme inventories, and for 63\nlow-resource languages, also includes the grapheme-to-phoneme (G2P) mapping. 54\nof these languages do not have easily findable G2P mappings online otherwise.\nWe turned the information from Wikipedia into a structured, machine-readable\nTSV format, and make the resulting data set publicly available so it can be\nimproved further and used in a variety of applications involving low-resource\nlanguages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:04:54 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chakraborty", "Tania", ""], ["Prasad", "Manasa", ""], ["Breiner", "Theresa", ""], ["Ritchie", "Sandy", ""], ["van Esch", "Daan", ""]]}, {"id": "2101.11577", "submitter": "Ke Hu", "authors": "Ke Hu, Ruoming Pang, Tara N. Sainath, Trevor Strohman", "title": "Transformer Based Deliberation for Two-Pass Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive speech recognition systems must generate words quickly while also\nproducing accurate results. Two-pass models excel at these requirements by\nemploying a first-pass decoder that quickly emits words, and a second-pass\ndecoder that requires more context but is more accurate. Previous work has\nestablished that a deliberation network can be an effective second-pass model.\nThe model attends to two kinds of inputs at once: encoded audio frames and the\nhypothesis text from the first-pass model. In this work, we explore using\ntransformer layers instead of long-short term memory (LSTM) layers for\ndeliberation rescoring. In transformer layers, we generalize the\n\"encoder-decoder\" attention to attend to both encoded audio and first-pass text\nhypotheses. The output context vectors are then combined by a merger layer.\nCompared to LSTM-based deliberation, our best transformer deliberation achieves\n7% relative word error rate improvements along with a 38% reduction in\ncomputation. We also compare against non-deliberation transformer rescoring,\nand find a 9% relative improvement.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:05:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Ke", ""], ["Pang", "Ruoming", ""], ["Sainath", "Tara N.", ""], ["Strohman", "Trevor", ""]]}, {"id": "2101.11707", "submitter": "Kinjal Basu", "authors": "Kinjal Basu, Sarat Varanasi, Farhad Shakerin, Joaquin Arias, Gopal\n  Gupta", "title": "Knowledge-driven Natural Language Understanding of English Text and its\n  Applications", "comments": "Preprint. Accepted by the 35th AAAI Conference (AAAI-21) Main Tracks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the meaning of a text is a fundamental challenge of natural\nlanguage understanding (NLU) research. An ideal NLU system should process a\nlanguage in a way that is not exclusive to a single task or a dataset. Keeping\nthis in mind, we have introduced a novel knowledge driven semantic\nrepresentation approach for English text. By leveraging the VerbNet lexicon, we\nare able to map syntax tree of the text to its commonsense meaning represented\nusing basic knowledge primitives. The general purpose knowledge represented\nfrom our approach can be used to build any reasoning based NLU system that can\nalso provide justification. We applied this approach to construct two NLU\napplications that we present here: SQuARE (Semantic-based Question Answering\nand Reasoning Engine) and StaCACK (Stateful Conversational Agent using\nCommonsense Knowledge). Both these systems work by \"truly understanding\" the\nnatural language text they process and both provide natural language\nexplanations for their responses while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:02:50 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Basu", "Kinjal", ""], ["Varanasi", "Sarat", ""], ["Shakerin", "Farhad", ""], ["Arias", "Joaquin", ""], ["Gupta", "Gopal", ""]]}, {"id": "2101.11710", "submitter": "James Powell", "authors": "James Powell and Kari Sentz", "title": "Tracking Short-Term Temporal Linguistic Dynamics to Characterize\n  Candidate Therapeutics for COVID-19 in the CORD-19 Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific literature tends to grow as a function of funding and interest in\na given field. Mining such literature can reveal trends that may not be\nimmediately apparent. The CORD-19 corpus represents a growing corpus of\nscientific literature associated with COVID-19. We examined the intersection of\na set of candidate therapeutics identified in a drug-repurposing study with\ntemporal instances of the CORD-19 corpus to determine if it was possible to\nfind and measure changes associated with them over time. We propose that the\ntechniques we used could form the basis of a tool to pre-screen new candidate\ntherapeutics early in the research process.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 23:24:05 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Powell", "James", ""], ["Sentz", "Kari", ""]]}, {"id": "2101.11718", "submitter": "Jwala Dhamala", "authors": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada\n  Pruksachatkun, Kai-Wei Chang, Rahul Gupta", "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language\n  Generation", "comments": null, "journal-ref": null, "doi": "10.1145/3442188.3445924", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning techniques have enabled machines to generate\ncohesive open-ended text when prompted with a sequence of words as context.\nWhile these models now empower many downstream applications from conversation\nbots to automatic storytelling, they have been shown to generate texts that\nexhibit social biases. To systematically study and benchmark social biases in\nopen-ended language generation, we introduce the Bias in Open-Ended Language\nGeneration Dataset (BOLD), a large-scale dataset that consists of 23,679\nEnglish text generation prompts for bias benchmarking across five domains:\nprofession, gender, race, religion, and political ideology. We also propose new\nautomated metrics for toxicity, psycholinguistic norms, and text gender\npolarity to measure social biases in open-ended text generation from multiple\nangles. An examination of text generated from three popular language models\nreveals that the majority of these models exhibit a larger social bias than\nhuman-written Wikipedia text across all domains. With these results we\nhighlight the need to benchmark biases in open-ended language generation and\ncaution users of language generation models on downstream tasks to be cognizant\nof these embedded prejudices.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:07:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Dhamala", "Jwala", ""], ["Sun", "Tony", ""], ["Kumar", "Varun", ""], ["Krishna", "Satyapriya", ""], ["Pruksachatkun", "Yada", ""], ["Chang", "Kai-Wei", ""], ["Gupta", "Rahul", ""]]}, {"id": "2101.11739", "submitter": "Hugh Perkins", "authors": "Hugh Perkins", "title": "Compositionality Through Language Transmission, using Artificial Neural\n  Networks", "comments": "Updated figure layout, fixed typos, modified title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an architecture and process for using the Iterated Learning Model\n(\"ILM\") for artificial neural networks. We show that ILM does not lead to the\nsame clear compositionality as observed using DCGs, but does lead to a modest\nimprovement in compositionality, as measured by holdout accuracy and topologic\nsimilarity. We show that ILM can lead to an anti-correlation between holdout\naccuracy and topologic rho. We demonstrate that ILM can increase\ncompositionality when using non-symbolic high-dimensional images as input.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:08:16 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 04:02:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Perkins", "Hugh", ""]]}, {"id": "2101.11753", "submitter": "Manoj Kumar", "authors": "Manoj Kumar, Varun Kumar, Hadrien Glaude, Cyprien delichy, Aman Alok\n  and Rahul Gupta", "title": "ProtoDA: Efficient Transfer Learning for Few-Shot Intent Classification", "comments": "Accepted at IEEE Spoken Language Technology Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Practical sequence classification tasks in natural language processing often\nsuffer from low training data availability for target classes. Recent works\ntowards mitigating this problem have focused on transfer learning using\nembeddings pre-trained on often unrelated tasks, for instance, language\nmodeling. We adopt an alternative approach by transfer learning on an ensemble\nof related tasks using prototypical networks under the meta-learning paradigm.\nUsing intent classification as a case study, we demonstrate that increasing\nvariability in training tasks can significantly improve classification\nperformance. Further, we apply data augmentation in conjunction with\nmeta-learning to reduce sampling bias. We make use of a conditional generator\nfor data augmentation that is trained directly using the meta-learning\nobjective and simultaneously with prototypical networks, hence ensuring that\ndata augmentation is customized to the task. We explore augmentation in the\nsentence embedding space as well as prototypical embedding space. Combining\nmeta-learning with augmentation provides upto 6.49% and 8.53% relative F1-score\nimprovements over the best performing systems in the 5-shot and 10-shot\nlearning, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 00:19:13 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kumar", "Manoj", ""], ["Kumar", "Varun", ""], ["Glaude", "Hadrien", ""], ["delichy", "Cyprien", ""], ["Alok", "Aman", ""], ["Gupta", "Rahul", ""]]}, {"id": "2101.11802", "submitter": "Amrita Saha", "authors": "Amrita Saha, Shafiq Joty, Steven C.H. Hoi", "title": "Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Module Networks (NMNs) have been quite successful in incorporating\nexplicit reasoning as learnable modules in various question answering tasks,\nincluding the most generic form of numerical reasoning over text in Machine\nReading Comprehension (MRC). However, to achieve this, contemporary NMNs need\nstrong supervision in executing the query as a specialized program over\nreasoning modules and fail to generalize to more open-ended settings without\nsuch supervision. Hence we propose Weakly-Supervised Neuro-Symbolic Module\nNetwork (WNSMN) trained with answers as the sole supervision for numerical\nreasoning based MRC. It learns to execute a noisy heuristic program obtained\nfrom the dependency parsing of the query, as discrete actions over both neural\nand symbolic reasoning modules and trains it end-to-end in a reinforcement\nlearning framework with discrete reward from answer matching. On the\nnumerical-answer subset of DROP, WNSMN out-performs NMN by 32% and the\nreasoning-free language model GenBERT by 8% in exact match accuracy when\ntrained under comparable weak supervised settings. This showcases the\neffectiveness and generalizability of modular networks that can handle explicit\ndiscrete reasoning over noisy programs in an end-to-end manner.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:36:09 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Saha", "Amrita", ""], ["Joty", "Shafiq", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2101.11836", "submitter": "Hrituraj Singh", "authors": "Hrituraj Singh, Gaurav Verma, Aparna Garimella, Balaji Vasan\n  Srinivasan", "title": "DRAG: Director-Generator Language Modelling Framework for Non-Parallel\n  Author Stylized Rewriting", "comments": "Accepted as Long Paper to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author stylized rewriting is the task of rewriting an input text in a\nparticular author's style. Recent works in this area have leveraged\nTransformer-based language models in a denoising autoencoder setup to generate\nauthor stylized text without relying on a parallel corpus of data. However,\nthese approaches are limited by the lack of explicit control of target\nattributes and being entirely data-driven. In this paper, we propose a\nDirector-Generator framework to rewrite content in the target author's style,\nspecifically focusing on certain target attributes. We show that our proposed\nframework works well even with a limited-sized target author corpus. Our\nexperiments on corpora consisting of relatively small-sized text authored by\nthree distinct authors show significant improvements upon existing works to\nrewrite input texts in target author's style. Our quantitative and qualitative\nanalyses further show that our model has better meaning retention and results\nin more fluent generations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 06:52:40 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Singh", "Hrituraj", ""], ["Verma", "Gaurav", ""], ["Garimella", "Aparna", ""], ["Srinivasan", "Balaji Vasan", ""]]}, {"id": "2101.11888", "submitter": "Johannes Bjerva", "authors": "Johannes Bjerva and Isabelle Augenstein", "title": "Does Typological Blinding Impede Cross-Lingual Sharing?", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bridging the performance gap between high- and low-resource languages has\nbeen the focus of much previous work. Typological features from databases such\nas the World Atlas of Language Structures (WALS) are a prime candidate for\nthis, as such data exists even for very low-resource languages. However,\nprevious work has only found minor benefits from using typological information.\nOur hypothesis is that a model trained in a cross-lingual setting will pick up\non typological cues from the input data, thus overshadowing the utility of\nexplicitly using such features. We verify this hypothesis by blinding a model\nto typological information, and investigate how cross-lingual sharing and\nperformance is impacted. Our model is based on a cross-lingual architecture in\nwhich the latent weights governing the sharing between languages is learnt\nduring training. We show that (i) preventing this model from exploiting\ntypology severely reduces performance, while a control experiment reaffirms\nthat (ii) encouraging sharing according to typology somewhat improves\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:32:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bjerva", "Johannes", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2101.11889", "submitter": "David Harbecke", "authors": "David Harbecke", "title": "Explaining Natural Language Processing Classifiers with Occlusion and\n  Language Modeling", "comments": "Master's Thesis at University of Potsdam without Acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are powerful statistical learners. However, their\npredictions do not come with an explanation of their process. To analyze these\nmodels, explanation methods are being developed. We present a novel explanation\nmethod, called OLM, for natural language processing classifiers. This method\ncombines occlusion and language modeling, which are techniques central to\nexplainability and NLP, respectively. OLM gives explanations that are\ntheoretically sound and easy to understand.\n  We make several contributions to the theory of explanation methods. Axioms\nfor explanation methods are an interesting theoretical concept to explore their\nbasics and deduce methods. We introduce a new axiom, give its intuition and\nshow it contradicts another existing axiom. Additionally, we point out\ntheoretical difficulties of existing gradient-based and some occlusion-based\nexplanation methods in natural language processing. We provide an extensive\nargument why evaluation of explanation methods is difficult. We compare OLM to\nother explanation methods and underline its uniqueness experimentally. Finally,\nwe investigate corner cases of OLM and discuss its validity and possible\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:44:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Harbecke", "David", ""]]}, {"id": "2101.11891", "submitter": "Shreya Gupta", "authors": "Shreya Gupta, Parantak Singh, Megha Sundriyal, Md Shad Akhtar, Tanmoy\n  Chakraborty", "title": "LESA: Linguistic Encapsulation and Semantic Amalgamation Based\n  Generalised Claim Detection from Online Content", "comments": "9 pages (plus 2 pages of references), 1 figure, 9 tables, accepted at\n  EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The conceptualization of a claim lies at the core of argument mining. The\nsegregation of claims is complex, owing to the divergence in textual syntax and\ncontext across different distributions. Another pressing issue is the\nunavailability of labeled unstructured text for experimentation. In this paper,\nwe propose LESA, a framework which aims at advancing headfirst into expunging\nthe former issue by assembling a source-independent generalized model that\ncaptures syntactic features through part-of-speech and dependency embeddings,\nas well as contextual features through a fine-tuned language model. We resolve\nthe latter issue by annotating a Twitter dataset which aims at providing a\ntesting ground on a large unstructured dataset. Experimental results show that\nLESA improves upon the state-of-the-art performance across six benchmark claim\ndatasets by an average of 3 claim-F1 points for in-domain experiments and by 2\nclaim-F1 points for general-domain experiments. On our dataset too, LESA\noutperforms existing baselines by 1 claim-F1 point on the in-domain experiments\nand 2 claim-F1 points on the general-domain experiments. We also release\ncomprehensive data annotation guidelines compiled during the annotation phase\n(which was missing in the current literature).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:51:30 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Gupta", "Shreya", ""], ["Singh", "Parantak", ""], ["Sundriyal", "Megha", ""], ["Akhtar", "Md Shad", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2101.11911", "submitter": "Emanuele Bugliarello", "authors": "Emanuele Bugliarello, Desmond Elliott", "title": "The Role of Syntactic Planning in Compositional Image Captioning", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has focused on generalizing to images drawn from the same\ndistribution as the training set, and not to the more challenging problem of\ngeneralizing to different distributions of images. Recently, Nikolaus et al.\n(2019) introduced a dataset to assess compositional generalization in image\ncaptioning, where models are evaluated on their ability to describe images with\nunseen adjective-noun and noun-verb compositions. In this work, we investigate\ndifferent methods to improve compositional generalization by planning the\nsyntactic structure of a caption. Our experiments show that jointly modeling\ntokens and syntactic tags enhances generalization in both RNN- and\nTransformer-based models, while also improving performance on standard metrics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:26:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bugliarello", "Emanuele", ""], ["Elliott", "Desmond", ""]]}, {"id": "2101.11954", "submitter": "Tathagata Raha", "authors": "Tathagata Raha, Vijayasaradhi Indurthi, Aayush Upadhyaya, Jeevesh\n  Kataria, Pramud Bommakanti, Vikram Keswani, Vasudeva Varma", "title": "Identifying COVID-19 Fake News in Social Media", "comments": "CONSTRAINT@AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evolution of social media platforms have empowered everyone to access\ninformation easily. Social media users can easily share information with the\nrest of the world. This may sometimes encourage spread of fake news, which can\nresult in undesirable consequences. In this work, we train models which can\nidentify health news related to COVID-19 pandemic as real or fake. Our models\nachieve a high F1-score of 98.64%. Our models achieve second place on the\nleaderboard, tailing the first position with a very narrow margin 0.05% points.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:12:50 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 22:27:07 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Raha", "Tathagata", ""], ["Indurthi", "Vijayasaradhi", ""], ["Upadhyaya", "Aayush", ""], ["Kataria", "Jeevesh", ""], ["Bommakanti", "Pramud", ""], ["Keswani", "Vikram", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.11956", "submitter": "Pere-Llu\\'is Huguet Cabot", "authors": "Pere-Llu\\'is Huguet-Cabot and David Abadi and Agneta Fischer and\n  Ekaterina Shutova", "title": "Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions", "comments": "Camera-ready version in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational modelling of political discourse tasks has become an\nincreasingly important area of research in natural language processing.\nPopulist rhetoric has risen across the political sphere in recent years;\nhowever, computational approaches to it have been scarce due to its complex\nnature. In this paper, we present the new $\\textit{Us vs. Them}$ dataset,\nconsisting of 6861 Reddit comments annotated for populist attitudes and the\nfirst large-scale computational models of this phenomenon. We investigate the\nrelationship between populist mindsets and social groups, as well as a range of\nemotions typically associated with these. We set a baseline for two tasks\nrelated to populist attitudes and present a set of multi-task learning models\nthat leverage and demonstrate the importance of emotion and group\nidentification as auxiliary tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:18:19 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:53:40 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2021 17:42:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Huguet-Cabot", "Pere-Llu\u00eds", ""], ["Abadi", "David", ""], ["Fischer", "Agneta", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2101.11958", "submitter": "Shuailong Liang", "authors": "Shuailong Liang, Lahari Poddar, Gyuri Szarvas", "title": "Attention Guided Dialogue State Tracking with Sparse Supervision", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to Dialogue State Tracking (DST) rely on turn level\ndialogue state annotations, which are expensive to acquire in large scale. In\ncall centers, for tasks like managing bookings or subscriptions, the user goal\ncan be associated with actions (e.g.~API calls) issued by customer service\nagents. These action logs are available in large volumes and can be utilized\nfor learning dialogue states. However, unlike turn-level annotations, such\nlogged actions are only available sparsely across the dialogue, providing only\na form of weak supervision for DST models. To efficiently learn DST with sparse\nlabels, we extend a state-of-the-art encoder-decoder model. The model learns a\nslot-aware representation of dialogue history, which focuses on relevant turns\nto guide the decoder. We present results on two public multi-domain DST\ndatasets (MultiWOZ and Schema Guided Dialogue) in both settings i.e. training\nwith turn-level and with sparse supervision. The proposed approach improves\nover baseline in both settings. More importantly, our model trained with sparse\nsupervision is competitive in performance to fully supervised baselines, while\nbeing more data and cost efficient.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:18:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Liang", "Shuailong", ""], ["Poddar", "Lahari", ""], ["Szarvas", "Gyuri", ""]]}, {"id": "2101.11959", "submitter": "Ali Basirat", "authors": "Ali Basirat and Joakim Nivre", "title": "Syntactic Nuclei in Dependency Parsing -- A Multilingual Exploration", "comments": "Accepted at EACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard models for syntactic dependency parsing take words to be the\nelementary units that enter into dependency relations. In this paper, we\ninvestigate whether there are any benefits from enriching these models with the\nmore abstract notion of nucleus proposed by Tesni\\`{e}re. We do this by showing\nhow the concept of nucleus can be defined in the framework of Universal\nDependencies and how we can use composition functions to make a\ntransition-based dependency parser aware of this concept. Experiments on 12\nlanguages show that nucleus composition gives small but significant\nimprovements in parsing accuracy. Further analysis reveals that the improvement\nmainly concerns a small number of dependency relations, including nominal\nmodifiers, relations of coordination, main predicates, and direct objects.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:22:30 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:45:52 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Basirat", "Ali", ""], ["Nivre", "Joakim", ""]]}, {"id": "2101.11974", "submitter": "Zeerak Waseem Butt", "authors": "Zeerak Waseem, Smarika Lulz, Joachim Bingel, Isabelle Augenstein", "title": "Disembodied Machine Learning: On the Illusion of Objectivity in NLP", "comments": "In review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Learning seeks to identify and encode bodies of knowledge within\nprovided datasets. However, data encodes subjective content, which determines\nthe possible outcomes of the models trained on it. Because such subjectivity\nenables marginalisation of parts of society, it is termed (social) `bias' and\nsought to be removed. In this paper, we contextualise this discourse of bias in\nthe ML community against the subjective choices in the development process.\nThrough a consideration of how choices in data and model development construct\nsubjectivity, or biases that are represented in a model, we argue that\naddressing and mitigating biases is near-impossible. This is because both data\nand ML models are objects for which meaning is made in each step of the\ndevelopment pipeline, from data selection over annotation to model training and\nanalysis. Accordingly, we find the prevalent discourse of bias limiting in its\nability to address social marginalisation. We recommend to be conscientious of\nthis, and to accept that de-biasing methods only correct for a fraction of\nbiases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:58:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Waseem", "Zeerak", ""], ["Lulz", "Smarika", ""], ["Bingel", "Joachim", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2101.11978", "submitter": "Rodrigo Agerri", "authors": "Elena Zotova, Rodrigo Agerri, German Rigau", "title": "Semi-automatic Generation of Multilingual Datasets for Stance Detection\n  in Twitter", "comments": "Stance detection, multilingualism, text categorization, fake news,\n  deep learning", "journal-ref": "Expert Systems with Applications, 170 (2021), Elsevier", "doi": "10.1016/j.eswa.2020.114547", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Popular social media networks provide the perfect environment to study the\nopinions and attitudes expressed by users. While interactions in social media\nsuch as Twitter occur in many natural languages, research on stance detection\n(the position or attitude expressed with respect to a specific topic) within\nthe Natural Language Processing field has largely been done for English.\nAlthough some efforts have recently been made to develop annotated data in\nother languages, there is a telling lack of resources to facilitate\nmultilingual and crosslingual research on stance detection. This is partially\ndue to the fact that manually annotating a corpus of social media texts is a\ndifficult, slow and costly process. Furthermore, as stance is a highly domain-\nand topic-specific phenomenon, the need for annotated data is specially\ndemanding. As a result, most of the manually labeled resources are hindered by\ntheir relatively small size and skewed class distribution. This paper presents\na method to obtain multilingual datasets for stance detection in Twitter.\nInstead of manually annotating on a per tweet basis, we leverage user-based\ninformation to semi-automatically label large amounts of tweets. Empirical\nmonolingual and cross-lingual experimentation and qualitative analysis show\nthat our method helps to overcome the aforementioned difficulties to build\nlarge, balanced and multilingual labeled corpora. We believe that our method\ncan be easily adapted to easily generate labeled social media data for other\nNatural Language Processing tasks and domains.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:05:09 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zotova", "Elena", ""], ["Agerri", "Rodrigo", ""], ["Rigau", "German", ""]]}, {"id": "2101.12015", "submitter": "Vinicius Carid\\'a", "authors": "Paulo Finardi, Jos\\'e Di\\'e Viegas, Gustavo T. Ferreira, Alex F.\n  Mansano, Vinicius F. Carid\\'a", "title": "BERTa\\'u: Ita\\'u BERT for digital customer service", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, three major topics received increased interest: deep\nlearning, NLP and conversational agents. Bringing these three topics together\nto create an amazing digital customer experience and indeed deploy in\nproduction and solve real-world problems is something innovative and\ndisruptive. We introduce a new Portuguese financial domain language\nrepresentation model called BERTa\\'u. BERTa\\'u is an uncased BERT-base trained\nfrom scratch with data from the Ita\\'u virtual assistant chatbot solution. Our\nnovel contribution is that BERTa\\'u pretrained language model requires less\ndata, reached state-of-the-art performance in three NLP tasks, and generates a\nsmaller and lighter model that makes the deployment feasible. We developed\nthree tasks to validate our model: information retrieval with Frequently Asked\nQuestions (FAQ) from Ita\\'u bank, sentiment analysis from our virtual assistant\ndata, and a NER solution. All proposed tasks are real-world solutions in\nproduction on our environment and the usage of a specialist model proved to be\neffective when compared to Google BERT multilingual and the DPRQuestionEncoder\nfrom Facebook, available at Hugging Face. The BERTa\\'u improves the performance\nin 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%\nin NER F1 score and can also represent the same sequence in up to 66% fewer\ntokens when compared to \"shelf models\".\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:29:03 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 01:46:41 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 23:26:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Finardi", "Paulo", ""], ["Viegas", "Jos\u00e9 Di\u00e9", ""], ["Ferreira", "Gustavo T.", ""], ["Mansano", "Alex F.", ""], ["Carid\u00e1", "Vinicius F.", ""]]}, {"id": "2101.12027", "submitter": "S. M. Sadiq - Ur - Rahman Shifath", "authors": "S.M. Sadiq-Ur-Rahman Shifath, Mohammad Faiyaz Khan, and Md. Saiful\n  Islam", "title": "A transformer based approach for fighting COVID-19 fake news", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The rapid outbreak of COVID-19 has caused humanity to come to a stand-still\nand brought with it a plethora of other problems. COVID-19 is the first\npandemic in history when humanity is the most technologically advanced and\nrelies heavily on social media platforms for connectivity and other benefits.\nUnfortunately, fake news and misinformation regarding this virus is also\navailable to people and causing some massive problems. So, fighting this\ninfodemic has become a significant challenge. We present our solution for the\n\"Constraint@AAAI2021 - COVID19 Fake News Detection in English\" challenge in\nthis work. After extensive experimentation with numerous architectures and\ntechniques, we use eight different transformer-based pre-trained models with\nadditional layers to construct a stacking ensemble classifier and fine-tuned\nthem for our purpose. We achieved 0.979906542 accuracy, 0.979913119 precision,\n0.979906542 recall, and 0.979907901 f1-score on the test dataset of the\ncompetition.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:43:42 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shifath", "S. M. Sadiq-Ur-Rahman", ""], ["Khan", "Mohammad Faiyaz", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "2101.12056", "submitter": "Kirill Milintsevich", "authors": "Kirill Milintsevich and Kairit Sirts", "title": "Enhancing Sequence-to-Sequence Neural Lemmatization with External\n  Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel hybrid approach to lemmatization that enhances the seq2seq\nneural model with additional lemmas extracted from an external lexicon or a\nrule-based system. During training, the enhanced lemmatizer learns both to\ngenerate lemmas via a sequential decoder and copy the lemma characters from the\nexternal candidates supplied during run-time. Our lemmatizer enhanced with\ncandidates extracted from the Apertium morphological analyzer achieves\nstatistically significant improvements compared to baseline models not\nutilizing additional lemma information, achieves an average accuracy of 97.25%\non a set of 23 UD languages, which is 0.55% higher than obtained with the\nStanford Stanza model on the same set of languages. We also compare with other\nmethods of integrating external data into lemmatization and show that our\nenhanced system performs considerably better than a simple lexicon extension\nmethod based on the Stanza system, and it achieves complementary improvements\nw.r.t. the data augmentation method.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:14:20 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Milintsevich", "Kirill", ""], ["Sirts", "Kairit", ""]]}, {"id": "2101.12059", "submitter": "Xudong Lin", "authors": "Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh,\n  Lorenzo Torresani", "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From\n  Multimodal Inputs", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\textsc{Vx2Text}, a framework for text generation from multimodal\ninputs consisting of video plus text, speech, or audio. In order to leverage\ntransformer networks, which have been shown to be effective at modeling\nlanguage, each modality is first converted into a set of language embeddings by\na learnable tokenizer. This allows our approach to perform multimodal fusion in\nthe language space, thus eliminating the need for ad-hoc cross-modal fusion\nmodules. To address the non-differentiability of tokenization on continuous\ninputs (e.g., video or audio), we utilize a relaxation scheme that enables\nend-to-end training. Furthermore, unlike prior encoder-only models, our network\nincludes an autoregressive decoder to generate open-ended text from the\nmultimodal embeddings fused by the language encoder. This renders our approach\nfully generative and makes it directly applicable to different \"video+$x$ to\ntext\" problems without the need to design specialized network heads for each\ntask. The proposed framework is not only conceptually simple but also\nremarkably effective: experiments demonstrate that our approach based on a\nsingle architecture outperforms the state-of-the-art on three video-based\ntext-generation tasks -- captioning, question answering and audio-visual\nscene-aware dialog.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:22:36 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 23:21:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lin", "Xudong", ""], ["Bertasius", "Gedas", ""], ["Wang", "Jue", ""], ["Chang", "Shih-Fu", ""], ["Parikh", "Devi", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2101.12073", "submitter": "Thomas Dopierre", "authors": "Thomas Dopierre, Christophe Gravier, Wilfried Logerais", "title": "A Neural Few-Shot Text Classification Reality Check", "comments": "Accepted at the 16th conference of the European Chapter of the\n  Association for Computational Linguistics (EACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern classification models tend to struggle when the amount of annotated\ndata is scarce. To overcome this issue, several neural few-shot classification\nmodels have emerged, yielding significant progress over time, both in Computer\nVision and Natural Language Processing. In the latter, such models used to rely\non fixed word embeddings before the advent of transformers. Additionally, some\nmodels used in Computer Vision are yet to be tested in NLP applications. In\nthis paper, we compare all these models, first adapting those made in the field\nof image processing to NLP, and second providing them access to transformers.\nWe then test these models equipped with the same transformer-based encoder on\nthe intent detection task, known for having a large number of classes. Our\nresults reveal that while methods perform almost equally on the ARSC dataset,\nthis is not the case for the Intent Detection task, where the most recent and\nsupposedly best competitors perform worse than older and simpler ones (while\nall are given access to transformers). We also show that a simple baseline is\nsurprisingly strong. All the new developed models, as well as the evaluation\nframework, are made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:46:14 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Dopierre", "Thomas", ""], ["Gravier", "Christophe", ""], ["Logerais", "Wilfried", ""]]}, {"id": "2101.12093", "submitter": "Luca Soldaini", "authors": "Rujun Han, Luca Soldaini, Alessandro Moschitti", "title": "Modeling Context in Answer Sentence Selection Systems on a Latency\n  Budget", "comments": "Accepted as a short paper at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Sentence Selection (AS2) is an efficient approach for the design of\nopen-domain Question Answering (QA) systems. In order to achieve low latency,\ntraditional AS2 models score question-answer pairs individually, ignoring any\ninformation from the document each potential answer was extracted from. In\ncontrast, more computationally expensive models designed for machine reading\ncomprehension tasks typically receive one or more passages as input, which\noften results in better accuracy. In this work, we present an approach to\nefficiently incorporate contextual information in AS2 models. For each answer\ncandidate, we first use unsupervised similarity techniques to extract relevant\nsentences from its source document, which we then feed into an efficient\ntransformer architecture fine-tuned for AS2. Our best approach, which leverages\na multi-way attention architecture to efficiently encode context, improves 6%\nto 11% over noncontextual state of the art in AS2 with minimal impact on system\nlatency. All experiments in this work were conducted in English.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:24:48 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:30:29 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Han", "Rujun", ""], ["Soldaini", "Luca", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2101.12175", "submitter": "Patrick Xia", "authors": "Patrick Xia, Guanghui Qin, Siddharth Vashishtha, Yunmo Chen, Tongfei\n  Chen, Chandler May, Craig Harman, Kyle Rawlins, Aaron Steven White, Benjamin\n  Van Durme", "title": "LOME: Large Ontology Multilingual Extraction", "comments": "2021 EACL System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LOME, a system for performing multilingual information extraction.\nGiven a text document as input, our core system identifies spans of textual\nentity and event mentions with a FrameNet (Baker et al., 1998) parser. It\nsubsequently performs coreference resolution, fine-grained entity typing, and\ntemporal relation prediction between events. By doing so, the system constructs\nan event and entity focused knowledge graph. We can further apply third-party\nmodules for other types of annotation, like relation extraction. Our\n(multilingual) first-party modules either outperform or are competitive with\nthe (monolingual) state-of-the-art. We achieve this through the use of\nmultilingual encoders like XLM-R (Conneau et al., 2020) and leveraging\nmultilingual training data. LOME is available as a Docker container on Docker\nHub. In addition, a lightweight version of the system is accessible as a web\ndemo.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:28:59 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:35:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xia", "Patrick", ""], ["Qin", "Guanghui", ""], ["Vashishtha", "Siddharth", ""], ["Chen", "Yunmo", ""], ["Chen", "Tongfei", ""], ["May", "Chandler", ""], ["Harman", "Craig", ""], ["Rawlins", "Kyle", ""], ["White", "Aaron Steven", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2101.12294", "submitter": "Pedro Colon-Hernandez", "authors": "Pedro Colon-Hernandez, Catherine Havasi, Jason Alonso, Matthew\n  Huggins, Cynthia Breazeal", "title": "Combining pre-trained language models and structured knowledge", "comments": "Updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, transformer-based language models have achieved state of the\nart performance in various NLP benchmarks. These models are able to extract\nmostly distributional information with some semantics from unstructured text,\nhowever it has proven challenging to integrate structured information, such as\nknowledge graphs into these models. We examine a variety of approaches to\nintegrate structured knowledge into current language models and determine\nchallenges, and possible opportunities to leverage both structured and\nunstructured information sources. From our survey, we find that there are still\nopportunities at exploiting adapter-based injections and that it may be\npossible to further combine various of the explored approaches into one system.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:54:03 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 18:02:25 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Colon-Hernandez", "Pedro", ""], ["Havasi", "Catherine", ""], ["Alonso", "Jason", ""], ["Huggins", "Matthew", ""], ["Breazeal", "Cynthia", ""]]}, {"id": "2101.12406", "submitter": "Abhisek Dash", "authors": "Anurag Shandilya, Abhisek Dash, Abhijnan Chakraborty, Kripabandhu\n  Ghosh, Saptarshi Ghosh", "title": "Fairness for Whom? Understanding the Reader's Perception of Fairness in\n  Text Summarization", "comments": "This work has been accepted at International Workshop on Fair and\n  Interpretable Learning Algorithms 2020 (FILA 2020), which was held in\n  conjunction with IEEE BigData 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the surge in user-generated textual information, there has been a recent\nincrease in the use of summarization algorithms for providing an overview of\nthe extensive content. Traditional metrics for evaluation of these algorithms\n(e.g. ROUGE scores) rely on matching algorithmic summaries to human-generated\nones. However, it has been shown that when the textual contents are\nheterogeneous, e.g., when they come from different socially salient groups,\nmost existing summarization algorithms represent the social groups very\ndifferently compared to their distribution in the original data. To mitigate\nsuch adverse impacts, some fairness-preserving summarization algorithms have\nalso been proposed. All of these studies have considered normative notions of\nfairness from the perspective of writers of the contents, neglecting the\nreaders' perceptions of the underlying fairness notions. To bridge this gap, in\nthis work, we study the interplay between the fairness notions and how readers\nperceive them in textual summaries. Through our experiments, we show that\nreader's perception of fairness is often context-sensitive. Moreover, standard\nROUGE evaluation metrics are unable to quantify the perceived (un)fairness of\nthe summaries. To this end, we propose a human-in-the-loop metric and an\nautomated graph-based methodology to quantify the perceived bias in textual\nsummaries. We demonstrate their utility by quantifying the (un)fairness of\nseveral summaries of heterogeneous socio-political microblog datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:14:34 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 04:26:52 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shandilya", "Anurag", ""], ["Dash", "Abhisek", ""], ["Chakraborty", "Abhijnan", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2101.12409", "submitter": "Shengsheng Zhang", "authors": "Shengsheng Zhang, Yaping Huang, Yun Chen, Liner Yang, Chencheng Wang,\n  Erhong Yang", "title": "Few-Shot Domain Adaptation for Grammatical Error Correction via\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing Grammatical Error Correction (GEC) methods based on\nsequence-to-sequence mainly focus on how to generate more pseudo data to obtain\nbetter performance. Few work addresses few-shot GEC domain adaptation. In this\npaper, we treat different GEC domains as different GEC tasks and propose to\nextend meta-learning to few-shot GEC domain adaptation without using any pseudo\ndata. We exploit a set of data-rich source domains to learn the initialization\nof model parameters that facilitates fast adaptation on new resource-poor\ntarget domains. We adapt GEC model to the first language (L1) of the second\nlanguage learner. To evaluate the proposed method, we use nine L1s as source\ndomains and five L1s as target domains. Experiment results on the L1 GEC domain\nadaptation dataset demonstrate that the proposed approach outperforms the\nmulti-task transfer learning baseline by 0.50 $F_{0.5}$ score on average and\nenables us to effectively adapt to a new L1 domain with only 200 parallel\nsentences.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:28:55 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Zhang", "Shengsheng", ""], ["Huang", "Yaping", ""], ["Chen", "Yun", ""], ["Yang", "Liner", ""], ["Wang", "Chencheng", ""], ["Yang", "Erhong", ""]]}, {"id": "2101.12462", "submitter": "Benjamin Marie", "authors": "Benjamin Marie, Atsushi Fujita", "title": "Synthesizing Monolingual Data for Neural Machine Translation", "comments": "Preliminary work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In neural machine translation (NMT), monolingual data in the target language\nare usually exploited through a method so-called \"back-translation\" to\nsynthesize additional training parallel data. The synthetic data have been\nshown helpful to train better NMT, especially for low-resource language pairs\nand domains. Nonetheless, large monolingual data in the target domains or\nlanguages are not always available to generate large synthetic parallel data.\nIn this work, we propose a new method to generate large synthetic parallel data\nleveraging very small monolingual data in a specific domain. We fine-tune a\npre-trained GPT-2 model on such small in-domain monolingual data and use the\nresulting model to generate a large amount of synthetic in-domain monolingual\ndata. Then, we perform back-translation, or forward translation, to generate\nsynthetic in-domain parallel data. Our preliminary experiments on three\nlanguage pairs and five domains show the effectiveness of our method to\ngenerate fully synthetic but useful in-domain parallel data for improving NMT\nin all configurations. We also show promising results in extreme adaptation for\npersonalized NMT.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:17:40 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Marie", "Benjamin", ""], ["Fujita", "Atsushi", ""]]}, {"id": "2101.12608", "submitter": "Mostafa Abdou", "authors": "Mostafa Abdou, Ana Valeria Gonzalez, Mariya Toneva, Daniel\n  Hershcovich, Anders S{\\o}gaard", "title": "Does injecting linguistic structure into language models lead to better\n  alignment with brain recordings?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscientists evaluate deep neural networks for natural language processing\nas possible candidate models for how language is processed in the brain. These\nmodels are often trained without explicit linguistic supervision, but have been\nshown to learn some linguistic structure in the absence of such supervision\n(Manning et al., 2020), potentially questioning the relevance of symbolic\nlinguistic theories in modeling such cognitive processes (Warstadt and Bowman,\n2020). We evaluate across two fMRI datasets whether language models align\nbetter with brain recordings, if their attention is biased by annotations from\nsyntactic or semantic formalisms. Using structure from dependency or minimal\nrecursion semantic annotations, we find alignments improve significantly for\none of the datasets. For another dataset, we see more mixed results. We present\nan extensive analysis of these results. Our proposed approach enables the\nevaluation of more targeted hypotheses about the composition of meaning in the\nbrain, expanding the range of possible scientific inferences a neuroscientist\ncould make, and opens up new opportunities for cross-pollination between\ncomputational neuroscience and linguistics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:42:02 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Abdou", "Mostafa", ""], ["Gonzalez", "Ana Valeria", ""], ["Toneva", "Mariya", ""], ["Hershcovich", "Daniel", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2101.12637", "submitter": "James Ravenscroft", "authors": "James Ravenscroft and Arie Cattan and Amanda Clare and Ido Dagan and\n  Maria Liakata", "title": "CD2CR: Co-reference Resolution Across Documents and Domains", "comments": "9 pages, 5 figures, accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-document co-reference resolution (CDCR) is the task of identifying and\nlinking mentions to entities and concepts across many text documents. Current\nstate-of-the-art models for this task assume that all documents are of the same\ntype (e.g. news articles) or fall under the same theme. However, it is also\ndesirable to perform CDCR across different domains (type or theme). A\nparticular use case we focus on in this paper is the resolution of entities\nmentioned across scientific work and newspaper articles that discuss them.\nIdentifying the same entities and corresponding concepts in both scientific\narticles and news can help scientists understand how their work is represented\nin mainstream media. We propose a new task and English language dataset for\ncross-document cross-domain co-reference resolution (CD$^2$CR). The task aims\nto identify links between entities across heterogeneous document types. We show\nthat in this cross-domain, cross-document setting, existing CDCR models do not\nperform well and we provide a baseline model that outperforms current\nstate-of-the-art CDCR models on CD$^2$CR. Our data set, annotation tool and\nguidelines as well as our model for cross-document cross-domain co-reference\nare all supplied as open access open source resources.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:18:30 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ravenscroft", "James", ""], ["Cattan", "Arie", ""], ["Clare", "Amanda", ""], ["Dagan", "Ido", ""], ["Liakata", "Maria", ""]]}, {"id": "2101.12640", "submitter": "Leshem Choshen", "authors": "Leshem Choshen, Omri Abend", "title": "Transition based Graph Decoder for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While a number of works showed gains from incorporating source-side symbolic\nsyntactic and semantic structure into neural machine translation (NMT), much\nfewer works addressed the decoding of such structure.\n  We propose a general Transformer-based approach for tree and graph decoding\nbased on generating a sequence of transitions, inspired by a similar approach\nthat uses RNNs by Dyer (2016).\n  Experiments with using the proposed decoder with Universal Dependencies\nsyntax on English-German, German-English and English-Russian show improved\nperformance over the standard Transformer decoder, as well as over ablated\nversions of the model.\\tacltxt{\\footnote{All code implementing the presented\nmodels will be released upon acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:20:45 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Choshen", "Leshem", ""], ["Abend", "Omri", ""]]}, {"id": "2101.12672", "submitter": "Nguyen Thanh Chinh", "authors": "Thanh Chinh Nguyen, Van Nha Nguyen", "title": "NLPBK at VLSP-2020 shared task: Compose transformer pretrained models\n  for Reliable Intelligence Identification on Social network", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our method for tuning a transformer-based pretrained\nmodel, to adaptation with Reliable Intelligence Identification on Vietnamese\nSNSs problem. We also proposed a model that combines bert-base pretrained\nmodels with some metadata features, such as the number of comments, number of\nlikes, images of SNS documents,... to improved results for VLSP shared task:\nReliable Intelligence Identification on Vietnamese SNSs. With appropriate\ntraining techniques, our model is able to achieve 0.9392 ROC-AUC on public test\nset and the final version settles at top 2 ROC-AUC (0.9513) on private test\nset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:19:28 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Nguyen", "Thanh Chinh", ""], ["Nguyen", "Van Nha", ""]]}, {"id": "2101.12729", "submitter": "Jordi Luque", "authors": "Martin Kocour, Guillermo C\\'ambara, Jordi Luque, David Bonet, Mireia\n  Farr\\'us, Martin Karafi\\'at, Karel Vesel\\'y and Jan ''Honza'' \\^Cernock\\'y", "title": "BCN2BRNO: ASR System Fusion for Albayzin 2020 Speech to Text Challenge", "comments": "fusion, end-to-end model, hybrid model, semisupervised, automatic\n  speech recognition, convolutional neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes joint effort of BUT and Telef\\'onica Research on\ndevelopment of Automatic Speech Recognition systems for Albayzin 2020\nChallenge. We compare approaches based on either hybrid or end-to-end models.\nIn hybrid modelling, we explore the impact of SpecAugment layer on performance.\nFor end-to-end modelling, we used a convolutional neural network with gated\nlinear units (GLUs). The performance of such model is also evaluated with an\nadditional n-gram language model to improve word error rates. We further\ninspect source separation methods to extract speech from noisy environment\n(i.e. TV shows). More precisely, we assess the effect of using a neural-based\nmusic separator named Demucs. A fusion of our best systems achieved 23.33% WER\nin official Albayzin 2020 evaluations. Aside from techniques used in our final\nsubmitted systems, we also describe our efforts in retrieving high quality\ntranscripts for training.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:40:54 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kocour", "Martin", ""], ["C\u00e1mbara", "Guillermo", ""], ["Luque", "Jordi", ""], ["Bonet", "David", ""], ["Farr\u00fas", "Mireia", ""], ["Karafi\u00e1t", "Martin", ""], ["Vesel\u00fd", "Karel", ""], ["\u0108ernock\u00fd", "Jan ''Honza''", ""]]}, {"id": "2101.12732", "submitter": "Jordi Luque", "authors": "David Bonet, Guillermo C\\'ambara, Fernando L\\'opez, Pablo G\\'omez,\n  Carlos Segura, Jordi Luque", "title": "Speech Enhancement for Wake-Up-Word detection in Voice Assistants", "comments": "keyword spotting, speech enhancement, wake-up-word, deep learning,\n  convolutional neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyword spotting and in particular Wake-Up-Word (WUW) detection is a very\nimportant task for voice assistants. A very common issue of voice assistants is\nthat they get easily activated by background noise like music, TV or background\nspeech that accidentally triggers the device. In this paper, we propose a\nSpeech Enhancement (SE) model adapted to the task of WUW detection that aims at\nincreasing the recognition rate and reducing the false alarms in the presence\nof these types of noises. The SE model is a fully-convolutional denoising\nauto-encoder at waveform level and is trained using a log-Mel Spectrogram and\nwaveform reconstruction losses together with the BCE loss of a simple WUW\nclassification network. A new database has been purposely prepared for the task\nof recognizing the WUW in challenging conditions containing negative samples\nthat are very phonetically similar to the keyword. The database is extended\nwith public databases and an exhaustive data augmentation to simulate different\nnoises and environments. The results obtained by concatenating the SE with a\nsimple and state-of-the-art WUW detectors show that the SE does not have a\nnegative impact on the recognition rate in quiet environments while increasing\nthe performance in the presence of noise, especially when the SE and WUW\ndetector are trained jointly end-to-end.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:44:05 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Bonet", "David", ""], ["C\u00e1mbara", "Guillermo", ""], ["L\u00f3pez", "Fernando", ""], ["G\u00f3mez", "Pablo", ""], ["Segura", "Carlos", ""], ["Luque", "Jordi", ""]]}, {"id": "2101.12736", "submitter": "Osman Ramadan", "authors": "Osman Ramadan, James Withers, Douglas Orr", "title": "N-grams Bayesian Differential Privacy", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy has gained popularity in machine learning as a strong\nprivacy guarantee, in contrast to privacy mitigation techniques such as\nk-anonymity. However, applying differential privacy to n-gram counts\nsignificantly degrades the utility of derived language models due to their\nlarge vocabularies. We propose a differential privacy mechanism that uses\npublic data as a prior in a Bayesian setup to provide tighter bounds on the\nprivacy loss metric epsilon, and thus better privacy-utility trade-offs. It\nfirst transforms the counts to log space, approximating the distribution of the\npublic and private data as Gaussian. The posterior distribution is then\nevaluated and softmax is applied to produce a probability distribution. This\ntechnique achieves up to 85% reduction in KL divergence compared to previously\nknown mechanisms at epsilon equals 0.1. We compare our mechanism to k-anonymity\nin a n-gram language modelling task and show that it offers competitive\nperformance at large vocabulary sizes, while also providing superior privacy\nprotection.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:48:49 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ramadan", "Osman", ""], ["Withers", "James", ""], ["Orr", "Douglas", ""]]}]