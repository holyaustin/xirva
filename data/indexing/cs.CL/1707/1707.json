[{"id": "1707.00061", "submitter": "Su Lin Blodgett", "authors": "Su Lin Blodgett, Brendan O'Connor", "title": "Racial Disparity in Natural Language Processing: A Case Study of Social\n  Media African-American English", "comments": "Presented as a talk at the 2017 Workshop on Fairness, Accountability,\n  and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 22:57:50 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Blodgett", "Su Lin", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1707.00079", "submitter": "Hany Hassan Awadalla", "authors": "Hany Hassan and Mostafa Elaraby and Ahmed Tawfik", "title": "Synthetic Data for Neural Machine Translation of Spoken-Dialects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel approach to generate synthetic data for\ntraining Neural Machine Translation systems. The proposed approach transforms a\ngiven parallel corpus between a written language and a target language to a\nparallel corpus between a spoken dialect variant and the target language. Our\napproach is language independent and can be used to generate data for any\nvariant of the source language such as slang or spoken dialect or even for a\ndifferent language that is closely related to the source language.\n  The proposed approach is based on local embedding projection of distributed\nrepresentations which utilizes monolingual embeddings to transform parallel\ndata across language variants. We report experimental results on Levantine to\nEnglish translation using Neural Machine Translation. We show that the\ngenerated data can improve a very large scale system by more than 2.8 Bleu\npoints using synthetic spoken data which shows that it can be used to provide a\nreliable translation system for a spoken dialect that does not have sufficient\nparallel data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 01:21:22 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 22:35:42 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Hassan", "Hany", ""], ["Elaraby", "Mostafa", ""], ["Tawfik", "Ahmed", ""]]}, {"id": "1707.00110", "submitter": "Melody Guan", "authors": "Denny Britz, Melody Y. Guan, Minh-Thang Luong", "title": "Efficient Attention using a Fixed-Size Memory Representation", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard content-based attention mechanism typically used in\nsequence-to-sequence models is computationally expensive as it requires the\ncomparison of large encoder and decoder states at each time step. In this work,\nwe propose an alternative attention mechanism based on a fixed size memory\nrepresentation that is more efficient. Our technique predicts a compact set of\nK attention contexts during encoding and lets the decoder compute an efficient\nlookup that does not need to consult the memory. We show that our approach\nperforms on-par with the standard attention mechanism while yielding inference\nspeedups of 20% for real-world translation tasks and more for tasks with longer\nsequences. By visualizing attention scores we demonstrate that our models learn\ndistinct, meaningful alignments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 08:16:24 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Britz", "Denny", ""], ["Guan", "Melody Y.", ""], ["Luong", "Minh-Thang", ""]]}, {"id": "1707.00117", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Lifeng Hua, Lei Li, Hang Su, Tian Wang, Ning Chen, Bo Zhang", "title": "SAM: Semantic Attribute Modulation for Language Modeling and Style\n  Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Semantic Attribute Modulation (SAM) for language\nmodeling and style variation. The semantic attribute modulation includes\nvarious document attributes, such as titles, authors, and document categories.\nWe consider two types of attributes, (title attributes and category\nattributes), and a flexible attribute selection scheme by automatically scoring\nthem via an attribute attention mechanism. The semantic attributes are embedded\ninto the hidden semantic space as the generation inputs. With the attributes\nproperly harnessed, our proposed SAM can generate interpretable texts with\nregard to the input attributes. Qualitative analysis, including word semantic\nanalysis and attention values, shows the interpretability of SAM. On several\ntypical text datasets, we empirically demonstrate the superiority of the\nSemantic Attribute Modulated language model with different combinations of\ndocument attributes. Moreover, we present a style variation for the lyric\ngeneration using SAM, which shows a strong connection between the style\nvariation and the semantic attributes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 09:00:28 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:59:04 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 03:53:00 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Hu", "Wenbo", ""], ["Hua", "Lifeng", ""], ["Li", "Lei", ""], ["Su", "Hang", ""], ["Wang", "Tian", ""], ["Chen", "Ning", ""], ["Zhang", "Bo", ""]]}, {"id": "1707.00130", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Milica Gasic, and Steve\n  Young", "title": "Sample-efficient Actor-Critic Reinforcement Learning with Supervised\n  Data for Dialogue Management", "comments": "Accepted as a long paper in SigDial 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) methods have significant potential for\ndialogue policy optimisation. However, they suffer from a poor performance in\nthe early stages of learning. This is especially problematic for on-line\nlearning with real users. Two approaches are introduced to tackle this problem.\nFirstly, to speed up the learning process, two sample-efficient neural networks\nalgorithms: trust region actor-critic with experience replay (TRACER) and\nepisodic natural actor-critic with experience replay (eNACER) are presented.\nFor TRACER, the trust region helps to control the learning step size and avoid\ncatastrophic model changes. For eNACER, the natural gradient identifies the\nsteepest ascent direction in policy space to speed up the convergence. Both\nmodels employ off-policy learning with experience replay to improve\nsample-efficiency. Secondly, to mitigate the cold start issue, a corpus of\ndemonstration data is utilised to pre-train the models prior to on-line\nreinforcement learning. Combining these two approaches, we demonstrate a\npractical approach to learn deep RL-based dialogue policies and demonstrate\ntheir effectiveness in a task-oriented information seeking domain.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 09:56:31 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 08:55:16 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Su", "Pei-Hao", ""], ["Budzianowski", "Pawel", ""], ["Ultes", "Stefan", ""], ["Gasic", "Milica", ""], ["Young", "Steve", ""]]}, {"id": "1707.00166", "submitter": "Liyuan Liu", "authors": "Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji and Jiawei\n  Han", "title": "Heterogeneous Supervision for Relation Extraction: A Representation\n  Learning Approach", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction is a fundamental task in information extraction. Most\nexisting methods have heavy reliance on annotations labeled by human experts,\nwhich are costly and time-consuming. To overcome this drawback, we propose a\nnovel framework, REHession, to conduct relation extractor learning using\nannotations from heterogeneous information source, e.g., knowledge base and\ndomain heuristics. These annotations, referred as heterogeneous supervision,\noften conflict with each other, which brings a new challenge to the original\nrelation extraction task: how to infer the true label from noisy labels for a\ngiven instance. Identifying context information as the backbone of both\nrelation extraction and true label discovery, we adopt embedding techniques to\nlearn the distributed representations of context, which bridges all components\nwith mutual enhancement in an iterative fashion. Extensive experimental results\ndemonstrate the superiority of REHession over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 15:23:23 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 18:25:15 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Liu", "Liyuan", ""], ["Ren", "Xiang", ""], ["Zhu", "Qi", ""], ["Zhi", "Shi", ""], ["Gui", "Huan", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "1707.00189", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Andrew Yates, Kai Hui, Ophir Frieder", "title": "Content-Based Weak Supervision for Ad-Hoc Re-Ranking", "comments": "SIGIR 2019 (short paper)", "journal-ref": null, "doi": "10.1145/3331184.3331316", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge with neural ranking is the need for a large amount of\nmanually-labeled relevance judgments for training. In contrast with prior work,\nwe examine the use of weak supervision sources for training that yield pseudo\nquery-document pairs that already exhibit relevance (e.g., newswire\nheadline-content pairs and encyclopedic heading-paragraph pairs). We also\npropose filtering techniques to eliminate training samples that are too far out\nof domain using two techniques: a heuristic-based approach and novel supervised\nfilter that re-purposes a neural ranker. Using several leading neural ranking\narchitectures and multiple weak supervision datasets, we show that these\nsources of training pairs are effective on their own (outperforming prior weak\nsupervision techniques), and that filtering can further improve performance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 18:42:29 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 12:05:43 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 12:00:09 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["MacAvaney", "Sean", ""], ["Yates", "Andrew", ""], ["Hui", "Kai", ""], ["Frieder", "Ophir", ""]]}, {"id": "1707.00201", "submitter": "Ziteng Wang", "authors": "Ziteng Wang, Emmanuel Vincent, Romain Serizel, Yonghong Yan", "title": "Rank-1 Constrained Multichannel Wiener Filter for Speech Recognition in\n  Noisy Environments", "comments": "for Computer Speech and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multichannel linear filters, such as the Multichannel Wiener Filter (MWF) and\nthe Generalized Eigenvalue (GEV) beamformer are popular signal processing\ntechniques which can improve speech recognition performance. In this paper, we\npresent an experimental study on these linear filters in a specific speech\nrecognition task, namely the CHiME-4 challenge, which features real recordings\nin multiple noisy environments. Specifically, the rank-1 MWF is employed for\nnoise reduction and a new constant residual noise power constraint is derived\nwhich enhances the recognition performance. To fulfill the underlying rank-1\nassumption, the speech covariance matrix is reconstructed based on eigenvectors\nor generalized eigenvectors. Then the rank-1 constrained MWF is evaluated with\nalternative multichannel linear filters under the same framework, which\ninvolves a Bidirectional Long Short-Term Memory (BLSTM) network for mask\nestimation. The proposed filter outperforms alternative ones, leading to a 40%\nrelative Word Error Rate (WER) reduction compared with the baseline Weighted\nDelay and Sum (WDAS) beamformer on the real test set, and a 15% relative WER\nreduction compared with the GEV-BAN method. The results also suggest that the\nspeech recognition accuracy correlates more with the Mel-frequency cepstral\ncoefficients (MFCC) feature variance than with the noise reduction or the\nspeech distortion level.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 20:50:33 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 03:26:31 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Wang", "Ziteng", ""], ["Vincent", "Emmanuel", ""], ["Serizel", "Romain", ""], ["Yan", "Yonghong", ""]]}, {"id": "1707.00206", "submitter": "Zhiting Hu", "authors": "Junxian He, Zhiting Hu, Taylor Berg-Kirkpatrick, Ying Huang, Eric P.\n  Xing", "title": "Efficient Correlated Topic Modeling with Topic Embedding", "comments": "KDD 2017 oral. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated topic modeling has been limited to small model and problem sizes\ndue to their high computational cost and poor scaling. In this paper, we\npropose a new model which learns compact topic embeddings and captures topic\ncorrelations through the closeness between the topic vectors. Our method\nenables efficient inference in the low-dimensional embedding space, reducing\nprevious cubic or quadratic time complexity to linear w.r.t the topic size. We\nfurther speedup variational inference with a fast sampler to exploit sparsity\nof topic occurrence. Extensive experiments show that our approach is capable of\nhandling model and data scales which are several orders of magnitude larger\nthan existing correlation results, without sacrificing modeling quality by\nproviding competitive or superior performance in document classification and\nretrieval.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 21:10:15 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["He", "Junxian", ""], ["Hu", "Zhiting", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Huang", "Ying", ""], ["Xing", "Eric P.", ""]]}, {"id": "1707.00248", "submitter": "Xinchi Chen", "authors": "Xinchi Chen, Zhan Shi, Xipeng Qiu, Xuanjing Huang", "title": "DAG-based Long Short-Term Memory for Neural Word Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural word segmentation has attracted more and more research interests for\nits ability to alleviate the effort of feature engineering and utilize the\nexternal resource by the pre-trained character or word embeddings. In this\npaper, we propose a new neural model to incorporate the word-level information\nfor Chinese word segmentation. Unlike the previous word-based models, our model\nstill adopts the framework of character-based sequence labeling, which has\nadvantages on both effectiveness and efficiency at the inference stage. To\nutilize the word-level information, we also propose a new long short-term\nmemory (LSTM) architecture over directed acyclic graph (DAG). Experimental\nresults demonstrate that our model leads to better performances than the\nbaseline models.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 07:41:10 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Chen", "Xinchi", ""], ["Shi", "Zhan", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1707.00299", "submitter": "Keisuke Sakaguchi", "authors": "Keisuke Sakaguchi, Matt Post, Benjamin Van Durme", "title": "Grammatical Error Correction with Neural Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural encoder-decoder model with reinforcement learning (NRL)\nfor grammatical error correction (GEC). Unlike conventional maximum likelihood\nestimation (MLE), the model directly optimizes towards an objective that\nconsiders a sentence-level, task-specific evaluation metric, avoiding the\nexposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in\nhuman and automated evaluation metrics, achieving the state-of-the-art on a\nfluency-oriented GEC corpus.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 14:39:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sakaguchi", "Keisuke", ""], ["Post", "Matt", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1707.00621", "submitter": "Marcos Zampieri", "authors": "Alina Maria Ciobanu, Marcos Zampieri, Shervin Malmasi, Liviu P. Dinu", "title": "Including Dialects and Language Varieties in Author Profiling", "comments": "Proceedings of PAN at CLEF 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computational approach to author profiling taking\ngender and language variety into account. We apply an ensemble system with the\noutput of multiple linear SVM classifiers trained on character and word\n$n$-grams. We evaluate the system using the dataset provided by the organizers\nof the 2017 PAN lab on author profiling. Our approach achieved 75% average\naccuracy on gender identification on tweets written in four languages and 97%\naccuracy on language variety identification for Portuguese.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 16:06:16 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Ciobanu", "Alina Maria", ""], ["Zampieri", "Marcos", ""], ["Malmasi", "Shervin", ""], ["Dinu", "Liviu P.", ""]]}, {"id": "1707.00683", "submitter": "Florian Strub", "authors": "Harm de Vries, Florian Strub, J\\'er\\'emie Mary, Hugo Larochelle,\n  Olivier Pietquin, Aaron Courville", "title": "Modulating early visual processing by language", "comments": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 04:06:01 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 02:58:44 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 20:04:53 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Larochelle", "Hugo", ""], ["Pietquin", "Olivier", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.00722", "submitter": "Jayadev Billa", "authors": "Jayadev Billa", "title": "Improving LSTM-CTC based ASR performance in domains with limited\n  training data", "comments": "13 pages Revised Figure 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the observed performance gap between automatic speech\nrecognition (ASR) systems based on Long Short Term Memory (LSTM) neural\nnetworks trained with the connectionist temporal classification (CTC) loss\nfunction and systems based on hybrid Deep Neural Networks (DNNs) trained with\nthe cross entropy (CE) loss function on domains with limited data. We step\nthrough a number of experiments that show incremental improvements on a\nbaseline EESEN toolkit based LSTM-CTC ASR system trained on the Librispeech\n100hr (train-clean-100) corpus. Our results show that with effective\ncombination of data augmentation and regularization, a LSTM-CTC based system\ncan exceed the performance of a strong Kaldi based baseline trained on the same\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:25:51 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 15:26:14 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Billa", "Jayadev", ""]]}, {"id": "1707.00781", "submitter": "David Sanchez", "authors": "Bruno Gon\\c{c}alves, Luc\\'ia Loureiro-Porto, Jos\\'e J. Ramasco, David\n  S\\'anchez", "title": "Mapping the Americanization of English in Space and Time", "comments": "16 pages, 6 figures, 2 tables. Published version", "journal-ref": "PLoS ONE 13: e0197741 (2018)", "doi": "10.1371/journal.pone.0197741", "report-no": null, "categories": "cs.CL cond-mat.stat-mech cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As global political preeminence gradually shifted from the United Kingdom to\nthe United States, so did the capacity to culturally influence the rest of the\nworld. In this work, we analyze how the world-wide varieties of written English\nare evolving. We study both the spatial and temporal variations of vocabulary\nand spelling of English using a large corpus of geolocated tweets and the\nGoogle Books datasets corresponding to books published in the US and the UK.\nThe advantage of our approach is that we can address both standard written\nlanguage (Google Books) and the more colloquial forms of microblogging messages\n(Twitter). We find that American English is the dominant form of English\noutside the UK and that its influence is felt even within the UK borders.\nFinally, we analyze how this trend has evolved over time and the impact that\nsome cultural events have had in shaping it.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:32:55 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 12:27:21 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["Loureiro-Porto", "Luc\u00eda", ""], ["Ramasco", "Jos\u00e9 J.", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1707.00836", "submitter": "Kyungmin Kim", "authors": "Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang", "title": "DeepStory: Video Story QA by Deep Embedded Memory Networks", "comments": "7 pages, accepted for IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question-answering (QA) on video contents is a significant challenge for\nachieving human-level intelligence as it involves both vision and language in\nreal-world settings. Here we demonstrate the possibility of an AI agent\nperforming video story QA by learning from a large amount of cartoon videos. We\ndevelop a video-story learning model, i.e. Deep Embedded Memory Networks\n(DEMN), to reconstruct stories from a joint scene-dialogue video stream using a\nlatent embedding space of observed data. The video stories are stored in a\nlong-term memory component. For a given question, an LSTM-based attention model\nuses the long-term memory to recall the best question-story-answer triplet by\nfocusing on specific words containing key information. We trained the DEMN on a\nnovel QA dataset of children's cartoon video series, Pororo. The dataset\ncontains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained\nsentences for scene description, and 8,913 story-related QA pairs. Our\nexperimental results show that the DEMN outperforms other QA models. This is\nmainly due to 1) the reconstruction of video stories in a scene-dialogue\ncombined form that utilize the latent embedding and 2) attention. DEMN also\nachieved state-of-the-art results on the MovieQA benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 07:42:05 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Kim", "Kyung-Min", ""], ["Heo", "Min-Oh", ""], ["Choi", "Seong-Ho", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1707.00896", "submitter": "Nikolaos Pappas", "authors": "Nikolaos Pappas and Andrei Popescu-Belis", "title": "Multilingual Hierarchical Attention Networks for Document Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical attention networks have recently achieved remarkable performance\nfor document classification in a given language. However, when multilingual\ndocument collections are considered, training such models separately for each\nlanguage entails linear parameter growth and lack of cross-language transfer.\nLearning a single multilingual model with fewer parameters is therefore a\nchallenging but potentially beneficial objective. To this end, we propose\nmultilingual hierarchical attention networks for learning document structures,\nwith shared encoders and/or shared attention mechanisms across languages, using\nmulti-task learning and an aligned semantic space as input. We evaluate the\nproposed models on multilingual document classification with disjoint label\nsets, on a large dataset which we provide, with 600k news documents in 8\nlanguages, and 5k labels. The multilingual models outperform monolingual ones\nin low-resource as well as full-resource settings, and use fewer parameters,\nthus confirming their computational efficiency and the utility of\ncross-language transfer.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 10:28:04 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 10:37:52 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 15:06:16 GMT"}, {"version": "v4", "created": "Fri, 15 Sep 2017 10:47:26 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Pappas", "Nikolaos", ""], ["Popescu-Belis", "Andrei", ""]]}, {"id": "1707.00995", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck, St\\'ephane Dupont", "title": "An empirical study on the effectiveness of images in Multimodal Neural\n  Machine Translation", "comments": "Accepted to EMNLP 2017", "journal-ref": "Proceedings of the 2017 Conference on Empirical Methods in Natural\n  Language Processing", "doi": "10.18653/v1/D17-1095", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In state-of-the-art Neural Machine Translation (NMT), an attention mechanism\nis used during decoding to enhance the translation. At every step, the decoder\nuses this mechanism to focus on different parts of the source sentence to\ngather the most useful information before outputting its target word. Recently,\nthe effectiveness of the attention mechanism has also been explored for\nmultimodal tasks, where it becomes possible to focus both on sentence parts and\nimage regions that they describe. In this paper, we compare several attention\nmechanism on the multimodal translation task (English, image to German) and\nevaluate the ability of the model to make use of images to improve translation.\nWe surpass state-of-the-art scores on the Multi30k data set, we nevertheless\nidentify and report different misbehavior of the machine while translating.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 13:57:04 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "1707.01009", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck, St\\'ephane Dupont, Omar Seddati", "title": "Visually Grounded Word Embeddings and Richer Visual Features for\n  Improving Multimodal Neural Machine Translation", "comments": "Accepted to GLU 2017. arXiv admin note: text overlap with\n  arXiv:1707.00995", "journal-ref": "Proc. GLU 2017 International Workshop on Grounding Language\n  Understanding", "doi": "10.21437/GLU.2017-13", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Multimodal Neural Machine Translation (MNMT), a neural model generates a\ntranslated sentence that describes an image, given the image itself and one\nsource descriptions in English. This is considered as the multimodal image\ncaption translation task. The images are processed with Convolutional Neural\nNetwork (CNN) to extract visual features exploitable by the translation model.\nSo far, the CNNs used are pre-trained on object detection and localization\ntask. We hypothesize that richer architecture, such as dense captioning models,\nmay be more suitable for MNMT and could lead to improved translations. We\nextend this intuition to the word-embeddings, where we compute both linguistic\nand visual representation for our corpus vocabulary. We combine and compare\ndifferent confi\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 14:26:56 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 15:21:47 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 13:23:54 GMT"}, {"version": "v4", "created": "Wed, 26 Jul 2017 13:42:46 GMT"}, {"version": "v5", "created": "Sat, 16 Dec 2017 13:12:05 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Dupont", "St\u00e9phane", ""], ["Seddati", "Omar", ""]]}, {"id": "1707.01066", "submitter": "Lifu Huang", "authors": "Lifu Huang, Heng Ji, Kyunghyun Cho, Clare R. Voss", "title": "Zero-Shot Transfer Learning for Event Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most previous event extraction studies have relied heavily on features\nderived from annotated event mentions, thus cannot be applied to new event\ntypes without annotation effort. In this work, we take a fresh look at event\nextraction and model it as a grounding problem. We design a transferable neural\narchitecture, mapping event mentions and types jointly into a shared semantic\nspace using structural and compositional neural networks, where the type of\neach event mention can be determined by the closest of all candidate types . By\nleveraging (1)~available manual annotations for a small set of existing event\ntypes and (2)~existing event ontologies, our framework applies to new event\ntypes without requiring additional annotation. Experiments on both existing\nevent types (e.g., ACE, ERE) and new event types (e.g., FrameNet) demonstrate\nthe effectiveness of our approach. \\textit{Without any manual annotations} for\n23 new event types, our zero-shot framework achieved performance comparable to\na state-of-the-art supervised model which is trained from the annotations of\n500 event mentions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 16:48:28 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Huang", "Lifu", ""], ["Ji", "Heng", ""], ["Cho", "Kyunghyun", ""], ["Voss", "Clare R.", ""]]}, {"id": "1707.01075", "submitter": "Lifu Huang", "authors": "Lifu Huang, Avirup Sil, Heng Ji, Radu Florian", "title": "Improving Slot Filling Performance with Attentive Neural Networks on\n  Dependency Structures", "comments": "EMNLP'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Slot Filling (SF) aims to extract the values of certain types of attributes\n(or slots, such as person:cities\\_of\\_residence) for a given entity from a\nlarge collection of source documents. In this paper we propose an effective DNN\narchitecture for SF with the following new strategies: (1). Take a regularized\ndependency graph instead of a raw sentence as input to DNN, to compress the\nwide contexts between query and candidate filler; (2). Incorporate two\nattention mechanisms: local attention learned from query and candidate filler,\nand global attention learned from external knowledge bases, to guide the model\nto better select indicative contexts to determine slot type. Experiments show\nthat this framework outperforms state-of-the-art on both relation extraction\n(16\\% absolute F-score gain) and slot filling validation for each individual\nsystem (up to 8.5\\% absolute F-score gain).\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 17:18:50 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Huang", "Lifu", ""], ["Sil", "Avirup", ""], ["Ji", "Heng", ""], ["Florian", "Radu", ""]]}, {"id": "1707.01090", "submitter": "Daniel Dzibela", "authors": "Daniel Dzibela and Armin Sehr", "title": "Hidden-Markov-Model Based Speech Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of this contribution is to use a parametric speech synthesis system\nfor reducing background noise and other interferences from recorded speech\nsignals. In a first step, Hidden Markov Models of the synthesis system are\ntrained.\n  Two adequate training corpora consisting of text and corresponding speech\nfiles have been set up and cleared of various faults, including inaudible\nutterances or incorrect assignments between audio and text data. Those are\ntested and compared against each other regarding e.g. flaws in the synthesized\nspeech, it's naturalness and intelligibility. Thus different voices have been\nsynthesized, whose quality depends less on the number of training samples used,\nbut much more on the cleanliness and signal-to-noise ratio of those.\nGeneralized voice models have been used for synthesis and the results greatly\ndiffer between the two speech corpora.\n  Tests regarding the adaptation to different speakers show that a resemblance\nto the original speaker is audible throughout all recordings, yet the\nsynthesized voices sound robotic and unnatural in smaller parts. The spoken\ntext, however, is usually intelligible, which shows that the models are working\nwell.\n  In a novel approach, speech is synthesized using side information of the\noriginal audio signal, particularly the pitch frequency. Results show an\nincrease of speech quality and intelligibility in comparison to speech\nsynthesized solely from text, up to the point of being nearly indistinguishable\nfrom the original.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 09:58:40 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Dzibela", "Daniel", ""], ["Sehr", "Armin", ""]]}, {"id": "1707.01161", "submitter": "Harsh Jhamtani", "authors": "Harsh Jhamtani, Varun Gangal, Eduard Hovy, Eric Nyberg", "title": "Shakespearizing Modern Language Using Copy-Enriched Sequence-to-Sequence\n  Models", "comments": "Accepted at EMNLP 2017 Workshop on Stylistic Variation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variations in writing styles are commonly used to adapt the content to a\nspecific context, audience, or purpose. However, applying stylistic variations\nis still by and large a manual process, and there have been little efforts\ntowards automating it. In this paper we explore automated methods to transform\ntext from modern English to Shakespearean English using an end to end trainable\nneural model with pointers to enable copy action. To tackle limited amount of\nparallel data, we pre-train embeddings of words by leveraging external\ndictionaries mapping Shakespearean words to modern English words as well as\nadditional text. Our methods are able to get a BLEU score of 31+, an\nimprovement of ~6 points above the strongest baseline. We publicly release our\ncode to foster further research in this area.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 21:42:55 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 21:09:25 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Jhamtani", "Harsh", ""], ["Gangal", "Varun", ""], ["Hovy", "Eduard", ""], ["Nyberg", "Eric", ""]]}, {"id": "1707.01176", "submitter": "Varun Gangal", "authors": "Varun Gangal, Harsh Jhamtani, Graham Neubig, Eduard Hovy, Eric Nyberg", "title": "CharManteau: Character Embedding Models For Portmanteau Creation", "comments": "Accepted for publication in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portmanteaus are a word formation phenomenon where two words are combined to\nform a new word. We propose character-level neural sequence-to-sequence (S2S)\nmethods for the task of portmanteau generation that are end-to-end-trainable,\nlanguage independent, and do not explicitly use additional phonetic\ninformation. We propose a noisy-channel-style model, which allows for the\nincorporation of unsupervised word lists, improving performance over a standard\nsource-to-target model. This model is made possible by an exhaustive candidate\ngeneration strategy specifically enabled by the features of the portmanteau\ntask. Experiments find our approach superior to a state-of-the-art FST-based\nbaseline with respect to ground truth accuracy and human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 23:11:52 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 16:27:55 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Gangal", "Varun", ""], ["Jhamtani", "Harsh", ""], ["Neubig", "Graham", ""], ["Hovy", "Eduard", ""], ["Nyberg", "Eric", ""]]}, {"id": "1707.01183", "submitter": "Souvick Ghosh", "authors": "Souvick Ghosh, Satanu Ghosh, and Dipankar Das", "title": "Complexity Metric for Code-Mixed Social Media Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An evaluation metric is an absolute necessity for measuring the performance\nof any system and complexity of any data. In this paper, we have discussed how\nto determine the level of complexity of code-mixed social media texts that are\ngrowing rapidly due to multilingual interference. In general, texts written in\nmultiple languages are often hard to comprehend and analyze. At the same time,\nin order to meet the demands of analysis, it is also necessary to determine the\ncomplexity of a particular document or a text segment. Thus, in the present\npaper, we have discussed the existing metrics for determining the code-mixing\ncomplexity of a corpus, their advantages, and shortcomings as well as proposed\nseveral improvements on the existing metrics. The new index better reflects the\nvariety and complexity of a multilingual document. Also, the index can be\napplied to a sentence and seamlessly extended to a paragraph or an entire\ndocument. We have employed two existing code-mixed corpora to suit the\nrequirements of our study.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 23:29:31 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ghosh", "Souvick", ""], ["Ghosh", "Satanu", ""], ["Das", "Dipankar", ""]]}, {"id": "1707.01184", "submitter": "Souvick Ghosh", "authors": "Souvick Ghosh, Satanu Ghosh, and Dipankar Das", "title": "Sentiment Identification in Code-Mixed Social Media Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is the Natural Language Processing (NLP) task dealing with\nthe detection and classification of sentiments in texts. While some tasks deal\nwith identifying the presence of sentiment in the text (Subjectivity analysis),\nother tasks aim at determining the polarity of the text categorizing them as\npositive, negative and neutral. Whenever there is a presence of sentiment in\nthe text, it has a source (people, group of people or any entity) and the\nsentiment is directed towards some entity, object, event or person. Sentiment\nanalysis tasks aim to determine the subject, the target and the polarity or\nvalence of the sentiment. In our work, we try to automatically extract\nsentiment (positive or negative) from Facebook posts using a machine learning\napproach.While some works have been done in code-mixed social media data and in\nsentiment analysis separately, our work is the first attempt (as of now) which\naims at performing sentiment analysis of code-mixed social media text. We have\nused extensive pre-processing to remove noise from raw text. Multilayer\nPerceptron model has been used to determine the polarity of the sentiment. We\nhave also developed the corpus for this task by manually labeling Facebook\nposts with their associated sentiments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 23:29:44 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ghosh", "Souvick", ""], ["Ghosh", "Satanu", ""], ["Das", "Dipankar", ""]]}, {"id": "1707.01265", "submitter": "Jonggu Kim", "authors": "Jonggu Kim and Jong-Hyeok Lee", "title": "Multiple Range-Restricted Bidirectional Gated Recurrent Units with\n  Attention for Relation Classification", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of neural approaches to relation classification have focused on finding\nshort patterns that represent the semantic relation using Convolutional Neural\nNetworks (CNNs) and those approaches have generally achieved better\nperformances than using Recurrent Neural Networks (RNNs). In a similar\nintuition to the CNN models, we propose a novel RNN-based model that strongly\nfocuses on only important parts of a sentence using multiple range-restricted\nbidirectional layers and attention for relation classification. Experimental\nresults on the SemEval-2010 relation classification task show that our model is\ncomparable to the state-of-the-art CNN-based and RNN-based models that use\nadditional linguistic information.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 08:55:28 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 03:37:23 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Kim", "Jonggu", ""], ["Lee", "Jong-Hyeok", ""]]}, {"id": "1707.01321", "submitter": "Sanda Martincic-Ipsic", "authors": "Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c, Tanja Mili\\v{c}i\\'c, Ljup\\v{c}o\n  Todorovski", "title": "The Influence of Feature Representation of Text on the Performance of\n  Document Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we perform a comparative analysis of three models for feature\nrepresentation of text documents in the context of document classification. In\nparticular, we consider the most often used family of models bag-of-words,\nrecently proposed continuous space models word2vec and doc2vec, and the model\nbased on the representation of text documents as language networks. While the\nbag-of-word models have been extensively used for the document classification\ntask, the performance of the other two models for the same task have not been\nwell understood. This is especially true for the network-based model that have\nbeen rarely considered for representation of text documents for classification.\nIn this study, we measure the performance of the document classifiers trained\nusing the method of random forests for features generated the three models and\ntheir variants. The results of the empirical comparison show that the commonly\nused bag-of-words model has performance comparable to the one obtained by the\nemerging continuous-space model of doc2vec. In particular, the low-dimensional\nvariants of doc2vec generating up to 75 features are among the top-performing\ndocument representation models. The results finally point out that doc2vec\nshows a superior performance in the tasks of classifying large documents.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 11:09:31 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""], ["Mili\u010di\u0107", "Tanja", ""], ["Todorovski", "Ljup\u010do", ""]]}, {"id": "1707.01355", "submitter": "Peter Makarov", "authors": "Peter Makarov, Tatiana Ruzsics, Simon Clematide", "title": "Align and Copy: UZH at SIGMORPHON 2017 Shared Task for Morphological\n  Reinflection", "comments": "To appear in Proceedings of the 15th Annual SIGMORPHON Workshop on\n  Computational Research in Phonetics, Phonology, and Morphology at CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the submissions by the University of Zurich to the\nSIGMORPHON 2017 shared task on morphological reinflection. The task is to\npredict the inflected form given a lemma and a set of morpho-syntactic\nfeatures. We focus on neural network approaches that can tackle the task in a\nlimited-resource setting. As the transduction of the lemma into the inflected\nform is dominated by copying over lemma characters, we propose two recurrent\nneural network architectures with hard monotonic attention that are strong at\ncopying and, yet, substantially different in how they achieve this. The first\napproach is an encoder-decoder model with a copy mechanism. The second approach\nis a neural state-transition system over a set of explicit edit actions,\nincluding a designated COPY action. We experiment with character alignment and\nfind that naive, greedy alignment consistently produces strong results for some\nlanguages. Our best system combination is the overall winner of the SIGMORPHON\n2017 Shared Task 1 without external resources. At a setting with 100 training\nsamples, both our approaches, as ensembles of models, outperform the next best\ncompetitor.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:24:55 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 10:09:44 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Makarov", "Peter", ""], ["Ruzsics", "Tatiana", ""], ["Clematide", "Simon", ""]]}, {"id": "1707.01378", "submitter": "Andrej Zukov Gregoric", "authors": "Yoram Bachrach, Andrej Zukov-Gregoric, Sam Coope, Ed Tovell, Bogdan\n  Maksak, Jose Rodriguez, Conan McMurtie", "title": "An Attention Mechanism for Answer Selection Using a Combined Global and\n  Local View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new attention mechanism for neural based question answering,\nwhich depends on varying granularities of the input. Previous work focused on\naugmenting recurrent neural networks with simple attention mechanisms which are\na function of the similarity between a question embedding and an answer\nembeddings across time. We extend this by making the attention mechanism\ndependent on a global embedding of the answer attained using a separate\nnetwork.\n  We evaluate our system on InsuranceQA, a large question answering dataset.\nOur model outperforms current state-of-the-art results on InsuranceQA. Further,\nwe visualize which sections of text our attention mechanism focuses on, and\nexplore its performance across different parameter settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:08:03 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 22:01:56 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 14:08:03 GMT"}, {"version": "v4", "created": "Wed, 20 Sep 2017 13:18:58 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Bachrach", "Yoram", ""], ["Zukov-Gregoric", "Andrej", ""], ["Coope", "Sam", ""], ["Tovell", "Ed", ""], ["Maksak", "Bogdan", ""], ["Rodriguez", "Jose", ""], ["McMurtie", "Conan", ""]]}, {"id": "1707.01425", "submitter": "Souvick Ghosh", "authors": "Souvick Ghosh, Dipankar Das and Tanmoy Chakraborty", "title": "Determining sentiment in citation text and analyzing its impact on the\n  proposed ranking index", "comments": "Sentiment Analysis, Citation, Citation Sentiment Analysis, Citation\n  Polarity, Ranking, Bibliometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whenever human beings interact with each other, they exchange or express\nopinions, emotions, and sentiments. These opinions can be expressed in text,\nspeech or images. Analysis of these sentiments is one of the popular research\nareas of present day researchers. Sentiment analysis, also known as opinion\nmining tries to identify or classify these sentiments or opinions into two\nbroad categories - positive and negative. In recent years, the scientific\ncommunity has taken a lot of interest in analyzing sentiment in textual data\navailable in various social media platforms. Much work has been done on social\nmedia conversations, blog posts, newspaper articles and various narrative\ntexts. However, when it comes to identifying emotions from scientific papers,\nresearchers have faced some difficulties due to the implicit and hidden nature\nof opinion. By default, citation instances are considered inherently positive\nin emotion. Popular ranking and indexing paradigms often neglect the opinion\npresent while citing. In this paper, we have tried to achieve three objectives.\nFirst, we try to identify the major sentiment in the citation text and assign a\nscore to the instance. We have used a statistical classifier for this purpose.\nSecondly, we have proposed a new index (we shall refer to it hereafter as\nM-index) which takes into account both the quantitative and qualitative factors\nwhile scoring a paper. Thirdly, we developed a ranking of research papers based\non the M-index. We also try to explain how the M-index impacts the ranking of\nscientific papers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 15:03:30 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ghosh", "Souvick", ""], ["Das", "Dipankar", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "1707.01450", "submitter": "Romain Laroche", "authors": "Romain Laroche", "title": "The Complex Negotiation Dialogue Game", "comments": "Position paper for Sigdial/Semdial 2017 special session on\n  negotiation dialogue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper formalises an abstract model for complex negotiation\ndialogue. This model is to be used for the benchmark of optimisation algorithms\nranging from Reinforcement Learning to Stochastic Games, through Transfer\nLearning, One-Shot Learning or others.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 16:11:31 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Laroche", "Romain", ""]]}, {"id": "1707.01477", "submitter": "Michael Veale", "authors": "Reuben Binns, Michael Veale, Max Van Kleek, Nigel Shadbolt", "title": "Like trainer, like bot? Inheritance of bias in algorithmic content\n  moderation", "comments": "12 pages, 3 figures, 9th International Conference on Social\n  Informatics (SocInfo 2017), Oxford, UK, 13--15 September 2017 (forthcoming in\n  Springer Lecture Notes in Computer Science)", "journal-ref": null, "doi": "10.1007/978-3-319-67256-4_32", "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet has become a central medium through which `networked publics'\nexpress their opinions and engage in debate. Offensive comments and personal\nattacks can inhibit participation in these spaces. Automated content moderation\naims to overcome this problem using machine learning classifiers trained on\nlarge corpora of texts manually annotated for offence. While such systems could\nhelp encourage more civil debate, they must navigate inherently normatively\ncontestable boundaries, and are subject to the idiosyncratic norms of the human\nraters who provide the training data. An important objective for platforms\nimplementing such measures might be to ensure that they are not unduly biased\ntowards or against particular norms of offence. This paper provides some\nexploratory methods by which the normative biases of algorithmic content\nmoderation systems can be measured, by way of a case study using an existing\ndataset of comments labelled for offence. We train classifiers on comments\nlabelled by different demographic subsets (men and women) to understand how\ndifferences in conceptions of offence between these groups might affect the\nperformance of the resulting models on various test sets. We conclude by\ndiscussing some of the ethical choices facing the implementers of algorithmic\nmoderation systems, given various desired levels of diversity of viewpoints\namongst discussion participants.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:19:45 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Binns", "Reuben", ""], ["Veale", "Michael", ""], ["Van Kleek", "Max", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1707.01521", "submitter": "Zhaocheng Zhu", "authors": "Zhaocheng Zhu, Junfeng Hu", "title": "Context Aware Document Embedding", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, doc2vec has achieved excellent results in different tasks. In this\npaper, we present a context aware variant of doc2vec. We introduce a novel\nweight estimating mechanism that generates weights for each word occurrence\naccording to its contribution in the context, using deep neural networks. Our\ncontext aware model can achieve similar results compared to doc2vec initialized\nbyWikipedia trained vectors, while being much more efficient and free from\nheavy external corpus. Analysis of context aware weights shows they are a kind\nof enhanced IDF weights that capture sub-topic level keywords in documents.\nThey might result from deep neural networks that learn hidden representations\nwith the least entropy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 18:18:37 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Zhu", "Zhaocheng", ""], ["Hu", "Junfeng", ""]]}, {"id": "1707.01555", "submitter": "Hongyu Guo", "authors": "Hongyu Guo", "title": "A Deep Network with Visual Text Composition Behavior", "comments": "accepted to ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While natural languages are compositional, how state-of-the-art neural models\nachieve compositionality is still unclear. We propose a deep network, which not\nonly achieves competitive accuracy for text classification, but also exhibits\ncompositional behavior. That is, while creating hierarchical representations of\na piece of text, such as a sentence, the lower layers of the network distribute\ntheir layer-specific attention weights to individual words. In contrast, the\nhigher layers compose meaningful phrases and clauses, whose lengths increase as\nthe networks get deeper until fully composing the sentence.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:37:23 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Guo", "Hongyu", ""]]}, {"id": "1707.01561", "submitter": "Sixun Ouyang", "authors": "Felipe Costa, Sixun Ouyang, Peter Dolog, Aonghus Lawlor", "title": "Automatic Generation of Natural Language Explanations", "comments": "7 pages, 5 figures, 2nd workshop on Deep Learning for Recommender\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task for recommender system is to generate explanations\naccording to a user's preferences. Most of the current methods for explainable\nrecommendations use structured sentences to provide descriptions along with the\nrecommendations they produce. However, those methods have neglected the\nreview-oriented way of writing a text, even though it is known that these\nreviews have a strong influence over user's decision.\n  In this paper, we propose a method for the automatic generation of natural\nlanguage explanations, for predicting how a user would write about an item,\nbased on user ratings from different items' features. We design a\ncharacter-level recurrent neural network (RNN) model, which generates an item's\nreview explanations using long-short term memories (LSTM). The model generates\ntext reviews given a combination of the review and ratings score that express\nopinions about different factors or aspects of an item. Our network is trained\non a sub-sample from the large real-world dataset BeerAdvocate. Our empirical\nevaluation using natural language processing metrics shows the generated text's\nquality is close to a real user written review, identifying negation,\nmisspellings, and domain specific vocabulary.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 14:52:41 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Costa", "Felipe", ""], ["Ouyang", "Sixun", ""], ["Dolog", "Peter", ""], ["Lawlor", "Aonghus", ""]]}, {"id": "1707.01626", "submitter": "Mohamed Abdalla", "authors": "Mohamed Abdalla, Graeme Hirst", "title": "Cross-Lingual Sentiment Analysis Without (Good) Translation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to cross-lingual sentiment analysis try to leverage the\nwealth of labeled English data using bilingual lexicons, bilingual vector space\nembeddings, or machine translation systems. Here we show that it is possible to\nuse a single linear transformation, with as few as 2000 word pairs, to capture\nfine-grained sentiment relationships between words in a cross-lingual setting.\nWe apply these cross-lingual sentiment models to a diverse set of tasks to\ndemonstrate their functionality in a non-English context. By effectively\nleveraging English sentiment knowledge without the need for accurate\ntranslation, we can analyze and extract features from other languages with\nscarce data at a very low cost, thus making sentiment and related analyses for\nmany languages inexpensive.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 03:50:38 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 04:26:26 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Abdalla", "Mohamed", ""], ["Hirst", "Graeme", ""]]}, {"id": "1707.01662", "submitter": "Seunghak Yu", "authors": "Seunghak Yu, Nilesh Kulkarni, Haejun Lee, Jihie Kim", "title": "An Embedded Deep Learning based Word Prediction", "comments": "5 pages, 3 figures, EMNLP 2017 submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in deep learning with application to language modeling\nhave led to success in tasks of text processing, summarizing and machine\ntranslation. However, deploying huge language models for mobile device such as\non-device keyboards poses computation as a bottle-neck due to their puny\ncomputation capacities. In this work we propose an embedded deep learning based\nword prediction method that optimizes run-time memory and also provides a real\ntime prediction environment. Our model size is 7.40MB and has average\nprediction time of 6.47 ms. We improve over the existing methods for word\nprediction in terms of key stroke savings and word prediction rate.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 07:39:06 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Yu", "Seunghak", ""], ["Kulkarni", "Nilesh", ""], ["Lee", "Haejun", ""], ["Kim", "Jihie", ""]]}, {"id": "1707.01736", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg, Desmond Elliott, Piek Vossen", "title": "Cross-linguistic differences and similarities in image descriptions", "comments": "Accepted for INLG 2017, Santiago de Compostela, Spain, 4-7 September,\n  2017. Camera-ready version. See the ACL anthology for full bibliographic\n  information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image description systems are commonly trained and evaluated on\nlarge image description datasets. Recently, researchers have started to collect\nsuch datasets for languages other than English. An unexplored question is how\ndifferent these datasets are from English and, if there are any differences,\nwhat causes them to differ. This paper provides a cross-linguistic comparison\nof Dutch, English, and German image descriptions. We find that these\ndescriptions are similar in many respects, but the familiarity of crowd workers\nwith the subjects of the images has a noticeable influence on description\nspecificity.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 11:53:41 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 10:18:44 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["van Miltenburg", "Emiel", ""], ["Elliott", "Desmond", ""], ["Vossen", "Piek", ""]]}, {"id": "1707.01780", "submitter": "Jose Camacho-Collados", "authors": "Jose Camacho-Collados and Mohammad Taher Pilehvar", "title": "On the Role of Text Preprocessing in Neural Network Architectures: An\n  Evaluation Study on Text Categorization and Sentiment Analysis", "comments": "Blackbox EMNLP 2018. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text preprocessing is often the first step in the pipeline of a Natural\nLanguage Processing (NLP) system, with potential impact in its final\nperformance. Despite its importance, text preprocessing has not received much\nattention in the deep learning literature. In this paper we investigate the\nimpact of simple text preprocessing decisions (particularly tokenizing,\nlemmatizing, lowercasing and multiword grouping) on the performance of a\nstandard neural text classifier. We perform an extensive evaluation on standard\nbenchmarks from text categorization and sentiment analysis. While our\nexperiments show that a simple tokenization of input text is generally\nadequate, they also highlight significant degrees of variability across\npreprocessing techniques. This reveals the importance of paying attention to\nthis usually-overlooked step in the pipeline, particularly when comparing\ndifferent models. Finally, our evaluation provides insights into the best\npreprocessing practices for training word embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 13:31:13 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 09:41:25 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 10:05:33 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Camacho-Collados", "Jose", ""], ["Pilehvar", "Mohammad Taher", ""]]}, {"id": "1707.01793", "submitter": "Weicong Ding", "authors": "Yifan Sun, Nikhil Rao, Weicong Ding", "title": "A Simple Approach to Learn Polysemous Word Embeddings", "comments": "Updated link to our data, test datasets, and scripts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many NLP applications require disambiguating polysemous words. Existing\nmethods that learn polysemous word vector representations involve first\ndetecting various senses and optimizing the sense-specific embeddings\nseparately, which are invariably more involved than single sense learning\nmethods such as word2vec. Evaluating these methods is also problematic, as\nrigorous quantitative evaluations in this space is limited, especially when\ncompared with single-sense embeddings. In this paper, we propose a simple\nmethod to learn a word representation, given any context. Our method only\nrequires learning the usual single sense representation, and coefficients that\ncan be learnt via a single pass over the data. We propose several new test sets\nfor evaluating word sense induction, relevance detection, and contextual word\nsimilarity, significantly supplementing the currently available tests. Results\non these and other tests show that while our method is embarrassingly simple,\nit achieves excellent results when compared to the state of the art models for\nunsupervised polysemous word representation learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 13:54:01 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 18:44:20 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Sun", "Yifan", ""], ["Rao", "Nikhil", ""], ["Ding", "Weicong", ""]]}, {"id": "1707.01830", "submitter": "Raphael Shu", "authors": "Raphael Shu and Hideki Nakayama", "title": "Single-Queue Decoding for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation models rely on the beam search algorithm for\ndecoding. In practice, we found that the quality of hypotheses in the search\nspace is negatively affected owing to the fixed beam size. To mitigate this\nproblem, we store all hypotheses in a single priority queue and use a universal\nscore function for hypothesis selection. The proposed algorithm is more\nflexible as the discarded hypotheses can be revisited in a later step. We\nfurther design a penalty function to punish the hypotheses that tend to produce\na final translation that is much longer or shorter than expected. Despite its\nsimplicity, we show that the proposed decoding algorithm is able to select\nhypotheses with better qualities and improve the translation performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 15:12:55 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 06:13:11 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Shu", "Raphael", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1707.01890", "submitter": "Gaurav Trivedi", "authors": "Gaurav Trivedi, Phuong Pham, Wendy Chapman, Rebecca Hwa, Janyce Wiebe,\n  Harry Hochheiser", "title": "An Interactive Tool for Natural Language Processing on Clinical Text", "comments": "8 pages, 2 figures, 2 tables, Presented at IUI TextVis 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Processing (NLP) systems often make use of machine learning\ntechniques that are unfamiliar to end-users who are interested in analyzing\nclinical records. Although NLP has been widely used in extracting information\nfrom clinical text, current systems generally do not support model revision\nbased on feedback from domain experts.\n  We present a prototype tool that allows end users to visualize and review the\noutputs of an NLP system that extracts binary variables from clinical text. Our\ntool combines multiple visualizations to help the users understand these\nresults and make any necessary corrections, thus forming a feedback loop and\nhelping improve the accuracy of the NLP models. We have tested our prototype in\na formative think-aloud user study with clinicians and researchers involved in\ncolonoscopy research. Results from semi-structured interviews and a System\nUsability Scale (SUS) analysis show that the users are able to quickly start\nrefining NLP models, despite having very little or no experience with machine\nlearning. Observations from these sessions suggest revisions to the interface\nto better support review workflow and interpretation of results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:44:15 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 14:04:13 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Trivedi", "Gaurav", ""], ["Pham", "Phuong", ""], ["Chapman", "Wendy", ""], ["Hwa", "Rebecca", ""], ["Wiebe", "Janyce", ""], ["Hochheiser", "Harry", ""]]}, {"id": "1707.01917", "submitter": "Madhav Nimishakavi Mr", "authors": "Madhav Nimishakavi and Partha Talukdar", "title": "Higher-order Relation Schema Induction using Tensor Factorization with\n  Back-off and Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation Schema Induction (RSI) is the problem of identifying type signatures\nof arguments of relations from unlabeled text. Most of the previous work in\nthis area have focused only on binary RSI, i.e., inducing only the subject and\nobject type signatures per relation. However, in practice, many relations are\nhigh-order, i.e., they have more than two arguments and inducing type\nsignatures of all arguments is necessary. For example, in the sports domain,\ninducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is\nmore informative than inducing just win(WinningPlayer, OpponentPlayer). We\nrefer to this problem as Higher-order Relation Schema Induction (HRSI). In this\npaper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a\nnovel framework for the HRSI problem. To the best of our knowledge, this is the\nfirst attempt at inducing higher-order relation schemata from unlabeled text.\nUsing the experimental analysis on three real world datasets, we show how TFBA\nhelps in dealing with sparsity and induce higher order schemata.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 18:02:12 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 10:45:46 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Nimishakavi", "Madhav", ""], ["Talukdar", "Partha", ""]]}, {"id": "1707.01961", "submitter": "Fenglong Ma", "authors": "Fenglong Ma, Radha Chitta, Saurabh Kataria, Jing Zhou, Palghat Ramesh,\n  Tong Sun, Jing Gao", "title": "Long-Term Memory Networks for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering is an important and difficult task in the natural language\nprocessing domain, because many basic natural language processing tasks can be\ncast into a question answering task. Several deep neural network architectures\nhave been developed recently, which employ memory and inference components to\nmemorize and reason over text information, and generate answers to questions.\nHowever, a major drawback of many such models is that they are capable of only\ngenerating single-word answers. In addition, they require large amount of\ntraining data to generate accurate answers. In this paper, we introduce the\nLong-Term Memory Network (LTMN), which incorporates both an external memory\nmodule and a Long Short-Term Memory (LSTM) module to comprehend the input data\nand generate multi-word answers. The LTMN model can be trained end-to-end using\nback-propagation and requires minimal supervision. We test our model on two\nsynthetic data sets (based on Facebook's bAbI data set) and the real-world\nStanford question answering data set, and show that it can achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 20:48:42 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Ma", "Fenglong", ""], ["Chitta", "Radha", ""], ["Kataria", "Saurabh", ""], ["Zhou", "Jing", ""], ["Ramesh", "Palghat", ""], ["Sun", "Tong", ""], ["Gao", "Jing", ""]]}, {"id": "1707.02026", "submitter": "Jianshu Ji", "authors": "Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong, Steven\n  Truong, Jianfeng Gao", "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical error correction (GEC) systems strive to correct both global\nerrors in word order and usage, and local errors in spelling and inflection.\nFurther developing upon recent work on neural machine translation, we propose a\nnew hybrid neural model with nested attention layers for GEC. Experiments show\nthat the new model can effectively correct errors of both types by\nincorporating word and character-level information,and that the model\nsignificantly outperforms previous neural models for GEC as measured on the\nstandard CoNLL-14 benchmark dataset. Further analysis also shows that the\nsuperiority of the proposed model can be largely attributed to the use of the\nnested attention mechanism, which has proven particularly effective in\ncorrecting local errors that involve small edits in orthography.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 03:10:32 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 02:56:49 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Ji", "Jianshu", ""], ["Wang", "Qinlong", ""], ["Toutanova", "Kristina", ""], ["Gong", "Yongen", ""], ["Truong", "Steven", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1707.02063", "submitter": "Michael Spranger", "authors": "Wojciech Kusa and Michael Spranger", "title": "External Evaluation of Event Extraction Classifiers for Automatic\n  Pathway Curation: An extended study of the mTOR pathway", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the impact of various event extraction systems on\nautomatic pathway curation using the popular mTOR pathway. We quantify the\nimpact of training data sets as well as different machine learning classifiers\nand show that some improve the quality of automatically extracted pathways.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 07:46:54 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Kusa", "Wojciech", ""], ["Spranger", "Michael", ""]]}, {"id": "1707.02230", "submitter": "Jens Nevens", "authors": "Jens Nevens and Michael Spranger", "title": "Computational Models of Tutor Feedback in Language Acquisition", "comments": "6 pages, 8 figures, Seventh Joint IEEE International Conference on\n  Development and Learning and on Epigenetic Robotics", "journal-ref": null, "doi": "10.1109/DEVLRN.2017.8329811", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the role of tutor feedback in language learning using\ncomputational models. We compare two dominant paradigms in language learning:\ninteractive learning and cross-situational learning - which differ primarily in\nthe role of social feedback such as gaze or pointing. We analyze the\nrelationship between these two paradigms and propose a new mixed paradigm that\ncombines the two paradigms and allows to test algorithms in experiments that\ncombine no feedback and social feedback. To deal with mixed feedback\nexperiments, we develop new algorithms and show how they perform with respect\nto traditional knn and prototype approaches.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 15:34:08 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Nevens", "Jens", ""], ["Spranger", "Michael", ""]]}, {"id": "1707.02268", "submitter": "Mehdi Allahyari", "authors": "Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei,\n  Elizabeth D. Trippe, Juan B. Gutierrez, Krys Kochut", "title": "Text Summarization Techniques: A Brief Survey", "comments": "Some of references format have updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a explosion in the amount of text data from a\nvariety of sources. This volume of text is an invaluable source of information\nand knowledge which needs to be effectively summarized to be useful. In this\nreview, the main approaches to automatic text summarization are described. We\nreview the different processes for summarization and describe the effectiveness\nand shortcomings of the different methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 16:55:56 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 15:37:02 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 16:37:16 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Allahyari", "Mehdi", ""], ["Pouriyeh", "Seyedamin", ""], ["Assefi", "Mehdi", ""], ["Safaei", "Saeid", ""], ["Trippe", "Elizabeth D.", ""], ["Gutierrez", "Juan B.", ""], ["Kochut", "Krys", ""]]}, {"id": "1707.02275", "submitter": "Antonio Valerio Miceli Barone", "authors": "Antonio Valerio Miceli Barone and Rico Sennrich", "title": "A parallel corpus of Python functions and documentation strings for\n  automated code documentation and code generation", "comments": "5 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated documentation of programming source code and automated code\ngeneration from natural language are challenging tasks of both practical and\nscientific interest. Progress in these areas has been limited by the low\navailability of parallel corpora of code and natural language descriptions,\nwhich tend to be small and constrained to specific domains.\n  In this work we introduce a large and diverse parallel corpus of a hundred\nthousands Python functions with their documentation strings (\"docstrings\")\ngenerated by scraping open source repositories on GitHub. We describe baseline\nresults for the code documentation and code generation tasks obtained by neural\nmachine translation. We also experiment with data augmentation techniques to\nfurther increase the amount of training data.\n  We release our datasets and processing scripts in order to stimulate research\nin these areas.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 17:15:27 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""], ["Sennrich", "Rico", ""]]}, {"id": "1707.02363", "submitter": "Ankur Bapna", "authors": "Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck", "title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling", "comments": "4 pages + 1 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art slot filling models for goal-oriented human/machine\nconversational language understanding systems rely on deep learning methods.\nWhile multi-task training of such models alleviates the need for large\nin-domain annotated datasets, bootstrapping a semantic parsing model for a new\ndomain using only the semantic frame, such as the back-end API or knowledge\ngraph schema, is still one of the holy grail tasks of language understanding\nfor dialogue systems. This paper proposes a deep learning based approach that\ncan utilize only the slot description in context without the need for any\nlabeled or unlabeled in-domain examples, to quickly bootstrap a new domain. The\nmain idea of this paper is to leverage the encoding of the slot names and\ndescriptions within a multi-task deep learned slot filling model, to implicitly\nalign slots across domains. The proposed approach is promising for solving the\ndomain scaling problem and eliminating the need for any manually annotated data\nor explicit schema alignment. Furthermore, our experiments on multiple domains\nshow that this approach results in significantly better slot-filling\nperformance when compared to using only in-domain data, especially in the low\ndata regime.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 21:21:33 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bapna", "Ankur", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""], ["Heck", "Larry", ""]]}, {"id": "1707.02377", "submitter": "Minmin Chen", "authors": "Minmin Chen", "title": "Efficient Vector Representation for Documents through Corruption", "comments": "5th International Conference on Learning Representations, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient document representation learning framework, Document\nVector through Corruption (Doc2VecC). Doc2VecC represents each document as a\nsimple average of word embeddings. It ensures a representation generated as\nsuch captures the semantic meanings of the document during learning. A\ncorruption model is included, which introduces a data-dependent regularization\nthat favors informative or rare words while forcing the embeddings of common\nand non-discriminative ones to be close to zero. Doc2VecC produces\nsignificantly better word embeddings than Word2Vec. We compare Doc2VecC with\nseveral state-of-the-art document representation learning algorithms. The\nsimple model architecture introduced by Doc2VecC matches or out-performs the\nstate-of-the-art in generating high-quality document representations for\nsentiment analysis, document classification as well as semantic relatedness\ntasks. The simplicity of the model enables training on billions of words per\nhour on a single machine. At the same time, the model is very efficient in\ngenerating representations of unseen documents at test time.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 00:57:01 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Minmin", ""]]}, {"id": "1707.02459", "submitter": "Jian Ni", "authors": "Jian Ni and Radu Florian", "title": "Improving Multilingual Named Entity Recognition with Wikipedia Entity\n  Type Mapping", "comments": "11 pages, Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), 2016", "journal-ref": null, "doi": "10.18653/v1/D16-1135", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art named entity recognition (NER) systems are statistical\nmachine learning models that have strong generalization capability (i.e., can\nrecognize unseen entities that do not appear in training data) based on lexical\nand contextual information. However, such a model could still make mistakes if\nits features favor a wrong entity type. In this paper, we utilize Wikipedia as\nan open knowledge base to improve multilingual NER systems. Central to our\napproach is the construction of high-accuracy, high-coverage multilingual\nWikipedia entity type mappings. These mappings are built from weakly annotated\ndata and can be extended to new languages with no human annotation or\nlanguage-dependent knowledge involved. Based on these mappings, we develop\nseveral approaches to improve an NER system. We evaluate the performance of the\napproaches via experiments on NER systems trained for 6 languages. Experimental\nresults show that the proposed approaches are effective in improving the\naccuracy of such systems on unseen entities, especially when a system is\napplied to a new domain or it is trained with little training data (up to 18.3\nF1 score improvement).\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 16:17:04 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Ni", "Jian", ""], ["Florian", "Radu", ""]]}, {"id": "1707.02483", "submitter": "Jian Ni", "authors": "Jian Ni and Georgiana Dinu and Radu Florian", "title": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective\n  Annotation and Representation Projection", "comments": "11 pages, The 55th Annual Meeting of the Association for\n  Computational Linguistics (ACL), 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1135", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art named entity recognition (NER) systems are supervised\nmachine learning models that require large amounts of manually annotated data\nto achieve high accuracy. However, annotating NER data by human is expensive\nand time-consuming, and can be quite difficult for a new language. In this\npaper, we present two weakly supervised approaches for cross-lingual NER with\nno human annotation in a target language. The first approach is to create\nautomatically labeled NER data for a target language via annotation projection\non comparable corpora, where we develop a heuristic scheme that effectively\nselects good-quality projection-labeled data from noisy data. The second\napproach is to project distributed representations of words (word embeddings)\nfrom a target language to a source language, so that the source-language NER\nsystem can be applied to the target language without re-training. We also\ndesign two co-decoding schemes that effectively combine the outputs of the two\nprojection-based approaches. We evaluate the performance of the proposed\napproaches on both in-house and open NER data for several target languages. The\nresults show that the combined systems outperform three other weakly supervised\napproaches on the CoNLL data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 19:45:47 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Ni", "Jian", ""], ["Dinu", "Georgiana", ""], ["Florian", "Radu", ""]]}, {"id": "1707.02499", "submitter": "Boyang Li", "authors": "Tong Wang, Ping Chen, Boyang Li", "title": "Predicting the Quality of Short Narratives from Social Media", "comments": "7 pages, 2 figures. Accepted at the 2017 IJCAI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important and difficult challenge in building computational models for\nnarratives is the automatic evaluation of narrative quality. Quality evaluation\nconnects narrative understanding and generation as generation systems need to\nevaluate their own products. To circumvent difficulties in acquiring\nannotations, we employ upvotes in social media as an approximate measure for\nstory quality. We collected 54,484 answers from a crowd-powered\nquestion-and-answer website, Quora, and then used active learning to build a\nclassifier that labeled 28,320 answers as stories. To predict the number of\nupvotes without the use of social network features, we create neural networks\nthat model textual regions and the interdependence among regions, which serve\nas strong benchmarks for future research. To our best knowledge, this is the\nfirst large-scale study for automatic evaluation of narrative quality.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 21:30:40 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wang", "Tong", ""], ["Chen", "Ping", ""], ["Li", "Boyang", ""]]}, {"id": "1707.02575", "submitter": "Sun Chong Wang", "authors": "Sun-Chong Wang", "title": "Neural Machine Translation between Herbal Prescriptions and Diseases", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study applies deep learning to herbalism. Toward the goal, we\nacquired the de-identified health insurance reimbursements that were claimed in\na 10-year period from 2004 to 2013 in the National Health Insurance Database of\nTaiwan, the total number of reimbursement records equaling 340 millions. Two\nartificial intelligence techniques were applied to the dataset: residual\nconvolutional neural network multitask classifier and attention-based recurrent\nneural network. The former works to translate from herbal prescriptions to\ndiseases; and the latter from diseases to herbal prescriptions. Analysis of the\nclassification results indicates that herbal prescriptions are specific to:\nanatomy, pathophysiology, sex and age of the patient, and season and year of\nthe prescription. Further analysis identifies temperature and gross domestic\nproduct as the meteorological and socioeconomic factors that are associated\nwith herbal prescriptions. Analysis of the neural machine transitional result\nindicates that the recurrent neural network learnt not only syntax but also\nsemantics of diseases and herbal prescriptions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 12:51:47 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wang", "Sun-Chong", ""]]}, {"id": "1707.02633", "submitter": "Jessica Ficler", "authors": "Jessica Ficler and Yoav Goldberg", "title": "Controlling Linguistic Style Aspects in Neural Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on neural natural language generation (NNLG) focus on controlling\nthe content of the generated text. We experiment with controlling several\nstylistic aspects of the generated text, in addition to its content. The method\nis based on conditioned RNN language model, where the desired content as well\nas the stylistic parameters serve as conditioning contexts. We demonstrate the\napproach on the movie reviews domain and show that it is successful in\ngenerating coherent sentences corresponding to the required linguistic style\nand content.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 20:21:36 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Ficler", "Jessica", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1707.02657", "submitter": "Edilson Anselmo Corr\\^ea J\\'unior", "authors": "Edilson A. Corr\\^ea Jr, Vanessa Q. Marinho, Leandro B. dos Santos,\n  Thales F. C. Bertaglia, Marcos V. Treviso, Henrico B. Brum", "title": "PELESent: Cross-domain polarity classification using distant supervision", "comments": "Accepted for publication in BRACIS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enormous amount of texts published daily by Internet users has fostered\nthe development of methods to analyze this content in several natural language\nprocessing areas, such as sentiment analysis. The main goal of this task is to\nclassify the polarity of a message. Even though many approaches have been\nproposed for sentiment analysis, some of the most successful ones rely on the\navailability of large annotated corpus, which is an expensive and\ntime-consuming process. In recent years, distant supervision has been used to\nobtain larger datasets. So, inspired by these techniques, in this paper we\nextend such approaches to incorporate popular graphic symbols used in\nelectronic messages, the emojis, in order to create a large sentiment corpus\nfor Portuguese. Trained on almost one million tweets, several models were\ntested in both same domain and cross-domain corpora. Our methods obtained very\ncompetitive results in five annotated corpora from mixed domains (Twitter and\nproduct reviews), which proves the domain-independent property of such\napproach. In addition, our results suggest that the combination of emoticons\nand emojis is able to properly capture the sentiment of a message.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 23:13:58 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Corr\u00eaa", "Edilson A.", "Jr"], ["Marinho", "Vanessa Q.", ""], ["Santos", "Leandro B. dos", ""], ["Bertaglia", "Thales F. C.", ""], ["Treviso", "Marcos V.", ""], ["Brum", "Henrico B.", ""]]}, {"id": "1707.02774", "submitter": "Slava Mikhaylov", "authors": "Alexander Baturo, Niheer Dasandi, Slava J. Mikhaylov", "title": "Understanding State Preferences With Text As Data: Introducing the UN\n  General Debate Corpus", "comments": null, "journal-ref": "Research & Politics, Volume 4, Issue 2, 2017", "doi": "10.1177/2053168017712821", "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year at the United Nations, member states deliver statements during the\nGeneral Debate discussing major issues in world politics. These speeches\nprovide invaluable information on governments' perspectives and preferences on\na wide range of issues, but have largely been overlooked in the study of\ninternational politics. This paper introduces a new dataset consisting of over\n7,701 English-language country statements from 1970-2016. We demonstrate how\nthe UN General Debate Corpus (UNGDC) can be used to derive country positions on\ndifferent policy dimensions using text analytic methods. The paper provides\napplications of these estimates, demonstrating the contribution the UNGDC can\nmake to the study of international politics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:40:12 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Baturo", "Alexander", ""], ["Dasandi", "Niheer", ""], ["Mikhaylov", "Slava J.", ""]]}, {"id": "1707.02786", "submitter": "Jihun Choi", "authors": "Jihun Choi, Kang Min Yoo, Sang-goo Lee", "title": "Learning to Compose Task-Specific Tree Structures", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For years, recursive neural networks (RvNNs) have been shown to be suitable\nfor representing text into fixed-length vectors and achieved good performance\non several natural language processing tasks. However, the main drawback of\nRvNNs is that they require structured input, which makes data preparation and\nmodel implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel\ntree-structured long short-term memory architecture that learns how to compose\ntask-specific tree structures only from plain text data efficiently. Our model\nuses Straight-Through Gumbel-Softmax estimator to decide the parent node among\ncandidates dynamically and to calculate gradients of the discrete decision. We\nevaluate the proposed model on natural language inference and sentiment\nanalysis, and show that our model outperforms or is at least comparable to\nprevious models. We also find that our model converges significantly faster\nthan other models.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 10:18:08 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 15:44:48 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 05:23:21 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 12:00:19 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Choi", "Jihun", ""], ["Yoo", "Kang Min", ""], ["Lee", "Sang-goo", ""]]}, {"id": "1707.02812", "submitter": "Suranjana Samanta", "authors": "Suranjana Samanta, Sameep Mehta", "title": "Towards Crafting Text Adversarial Samples", "comments": "11 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial samples are strategically modified samples, which are crafted\nwith the purpose of fooling a classifier at hand. An attacker introduces\nspecially crafted adversarial samples to a deployed classifier, which are being\nmis-classified by the classifier. However, the samples are perceived to be\ndrawn from entirely different classes and thus it becomes hard to detect the\nadversarial samples. Most of the prior works have been focused on synthesizing\nadversarial samples in the image domain. In this paper, we propose a new method\nof crafting adversarial text samples by modification of the original samples.\nModifications of the original text samples are done by deleting or replacing\nthe important or salient words in the text or by introducing new words in the\ntext sample. Our algorithm works best for the datasets which have\nsub-categories within each of the classes of examples. While crafting\nadversarial samples, one of the key constraint is to generate meaningful\nsentences which can at pass off as legitimate from language (English)\nviewpoint. Experimental results on IMDB movie review dataset for sentiment\nanalysis and Twitter dataset for gender detection show the efficiency of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 11:58:08 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Samanta", "Suranjana", ""], ["Mehta", "Sameep", ""]]}, {"id": "1707.02892", "submitter": "Honglun Zhang", "authors": "Honglun Zhang, Liqiang Xiao, Yongkun Wang, Yaohui Jin", "title": "A Generalized Recurrent Neural Architecture for Text Classification with\n  Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning leverages potential correlations among related tasks to\nextract common features and yield performance gains. However, most previous\nworks only consider simple or weak interactions, thereby failing to model\ncomplex correlations among three or more tasks. In this paper, we propose a\nmulti-task learning architecture with four types of recurrent neural layers to\nfuse information across multiple related tasks. The architecture is\nstructurally flexible and considers various interactions among tasks, which can\nbe regarded as a generalized case of many previous works. Extensive experiments\non five benchmark datasets for text classification show that our model can\nsignificantly improve performances of related tasks with additional information\nfrom others.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 14:58:53 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhang", "Honglun", ""], ["Xiao", "Liqiang", ""], ["Wang", "Yongkun", ""], ["Jin", "Yaohui", ""]]}, {"id": "1707.02919", "submitter": "Mehdi Allahyari", "authors": "Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saied Safaei,\n  Elizabeth D. Trippe, Juan B. Gutierrez, Krys Kochut", "title": "A Brief Survey of Text Mining: Classification, Clustering and Extraction\n  Techniques", "comments": "some of References format have updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of text that is generated every day is increasing dramatically.\nThis tremendous volume of mostly unstructured text cannot be simply processed\nand perceived by computers. Therefore, efficient and effective techniques and\nalgorithms are required to discover useful patterns. Text mining is the task of\nextracting meaningful information from text, which has gained significant\nattentions in recent years. In this paper, we describe several of the most\nfundamental text mining tasks and techniques including text pre-processing,\nclassification and clustering. Additionally, we briefly explain text mining in\nbiomedical and health care domains.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 16:02:44 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 16:32:25 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Allahyari", "Mehdi", ""], ["Pouriyeh", "Seyedamin", ""], ["Assefi", "Mehdi", ""], ["Safaei", "Saied", ""], ["Trippe", "Elizabeth D.", ""], ["Gutierrez", "Juan B.", ""], ["Kochut", "Krys", ""]]}, {"id": "1707.03017", "submitter": "Ethan Perez", "authors": "Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron\n  Courville", "title": "Learning Visual Reasoning Without Strong Priors", "comments": "Full AAAI 2018 paper is at arXiv:1709.07871. Presented at ICML 2017's\n  Machine Learning in Speech and Language Processing Workshop. Code is at\n  http://github.com/ethanjperez/film", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:49:28 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 15:02:50 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 14:56:51 GMT"}, {"version": "v4", "created": "Wed, 4 Oct 2017 20:01:27 GMT"}, {"version": "v5", "created": "Mon, 18 Dec 2017 21:37:16 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Perez", "Ethan", ""], ["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.03058", "submitter": "Daniel Fried", "authors": "Daniel Fried, Mitchell Stern, Dan Klein", "title": "Improving Neural Parsing by Disentangling Model Combination and\n  Reranking Effects", "comments": "ACL 2017. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has proposed several generative neural models for constituency\nparsing that achieve state-of-the-art results. Since direct search in these\ngenerative models is difficult, they have primarily been used to rescore\ncandidate outputs from base parsers in which decoding is more straightforward.\nWe first present an algorithm for direct search in these generative models. We\nthen demonstrate that the rescoring results are at least partly due to implicit\nmodel combination rather than reranking effects. Finally, we show that explicit\nmodel combination can improve performance even further, resulting in new\nstate-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data\nand 94.66 F1 when using external data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 20:47:33 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Fried", "Daniel", ""], ["Stern", "Mitchell", ""], ["Klein", "Dan", ""]]}, {"id": "1707.03095", "submitter": "Demival Vasques Filho", "authors": "Ben Curran, Kyle Higham, Elisenda Ortiz, Demival Vasques Filho", "title": "Look Who's Talking: Bipartite Networks as Representations of a Topic\n  Model of New Zealand Parliamentary Speeches", "comments": "28 pages, 12 figures, 3 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0199072", "report-no": null, "categories": "cs.CL cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative methods to measure the participation to parliamentary debate and\ndiscourse of elected Members of Parliament (MPs) and the parties they belong to\nare lacking. This is an exploratory study in which we propose the development\nof a new approach for a quantitative analysis of such participation. We utilize\nthe New Zealand government's digital Hansard database to construct a topic\nmodel of parliamentary speeches consisting of nearly 40 million words in the\nperiod 2003-2016. A Latent Dirichlet Allocation topic model is implemented in\norder to reveal the thematic structure of our set of documents. This generative\nstatistical model enables the detection of major themes or topics that are\npublicly discussed in the New Zealand parliament, as well as permitting their\nclassification by MP. Information on topic proportions is subsequently analyzed\nusing a combination of statistical methods. We observe patterns arising from\ntime-series analysis of topic frequencies which can be related to specific\nsocial, economic and legislative events. We then construct a bipartite network\nrepresentation, linking MPs to topics, for each of four parliamentary terms in\nthis time frame. We build projected networks (onto the set of nodes represented\nby MPs) and proceed to the study of the dynamical changes of their topology,\nincluding community structure. By performing this longitudinal network\nanalysis, we can observe the evolution of the New Zealand parliamentary topic\nnetwork and its main parties in the period studied.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 01:25:31 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 01:40:56 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 03:12:32 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Curran", "Ben", ""], ["Higham", "Kyle", ""], ["Ortiz", "Elisenda", ""], ["Filho", "Demival Vasques", ""]]}, {"id": "1707.03103", "submitter": "Jorge Balazs", "authors": "Jorge A. Balazs, Edison Marrese-Taylor, Pablo Loyola, Yutaka Matsuo", "title": "Refining Raw Sentence Representations for Textual Entailment Recognition\n  via Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the model used by the team Rivercorners for the 2017\nRepEval shared task. First, our model separately encodes a pair of sentences\ninto variable-length representations by using a bidirectional LSTM. Later, it\ncreates fixed-length raw representations by means of simple aggregation\nfunctions, which are then refined using an attention mechanism. Finally it\ncombines the refined representations of both sentences into a single vector to\nbe used for classification. With this model we obtained test accuracies of\n72.057% and 72.055% in the matched and mismatched evaluation tracks\nrespectively, outperforming the LSTM baseline, and obtaining performances\nsimilar to a model that relies on shared information between sentences (ESIM).\nWhen using an ensemble both accuracies increased to 72.247% and 72.827%\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 02:02:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 07:23:58 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Balazs", "Jorge A.", ""], ["Marrese-Taylor", "Edison", ""], ["Loyola", "Pablo", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1707.03172", "submitter": "Florin Brad", "authors": "Florin Brad, Radu Iacob, Ionel Hosu, Traian Rebedea", "title": "Dataset for a Neural Natural Language Interface for Databases (NNLIDB)", "comments": "13 pages, 2 figures", "journal-ref": "Proceedings of the Eighth International Joint Conference on\n  Natural Language Processing (Volume 1: Long Papers), 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in natural language interfaces to databases (NLIDB) has been slow\nmainly due to linguistic issues (such as language ambiguity) and domain\nportability. Moreover, the lack of a large corpus to be used as a standard\nbenchmark has made data-driven approaches difficult to develop and compare. In\nthis paper, we revisit the problem of NLIDBs and recast it as a sequence\ntranslation problem. To this end, we introduce a large dataset extracted from\nthe Stack Exchange Data Explorer website, which can be used for training neural\nnatural language interfaces for databases. We also report encouraging baseline\nresults on a smaller manually annotated test corpus, obtained using an\nattention-based sequence-to-sequence neural network.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:33:55 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Brad", "Florin", ""], ["Iacob", "Radu", ""], ["Hosu", "Ionel", ""], ["Rebedea", "Traian", ""]]}, {"id": "1707.03228", "submitter": "David Vilares", "authors": "David Vilares and Carlos G\\'omez-Rodr\\'iguez", "title": "A non-projective greedy dependency parser with bidirectional LSTMs", "comments": "12 pages, 2 figures, 5 tables", "journal-ref": "In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing\n  from Raw Text to Universal Dependencies, pages 152-162, Vancouver, Canada,\n  2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation of\nthe Covington (2001) algorithm for non-projective dependency parsing. The\nbidirectional LSTM approach by Kipperwasser and Goldberg (2016) is used to\ntrain a greedy parser with a dynamic oracle to mitigate error propagation. The\nmodel participated in the CoNLL 2017 UD Shared Task. In spite of not using any\nensemble methods and using the baseline segmentation and PoS tagging, the\nparser obtained good results on both macro-average LAS and UAS in the big\ntreebanks category (55 languages), ranking 7th out of 33 teams. In the all\ntreebanks category (LAS and UAS) we ranked 16th and 12th. The gap between the\nall and big categories is mainly due to the poor performance on four parallel\nPUD treebanks, suggesting that some `suffixed' treebanks (e.g. Spanish-AnCora)\nperform poorly on cross-treebank settings, which does not occur with the\ncorresponding `unsuffixed' treebank (e.g. Spanish). By changing that, we obtain\nthe 11th best LAS among all runs (official and unofficial). The code is made\navailable at https://github.com/CoNLL-UD-2017/LyS-FASTPARSE\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 11:44:31 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Vilares", "David", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1707.03253", "submitter": "Gregor Wiedemann", "authors": "Andreas Niekler, Gregor Wiedemann, Gerhard Heyer", "title": "Leipzig Corpus Miner - A Text Mining Infrastructure for Qualitative Data\n  Analysis", "comments": "https://hal.archives-ouvertes.fr/hal-01005878; Proceedings of\n  Terminology and Knowledge Engineering 2014 (TKE'14), Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the \"Leipzig Corpus Miner\", a technical infrastructure\nfor supporting qualitative and quantitative content analysis. The\ninfrastructure aims at the integration of 'close reading' procedures on\nindividual documents with procedures of 'distant reading', e.g. lexical\ncharacteristics of large document collections. Therefore information retrieval\nsystems, lexicometric statistics and machine learning procedures are combined\nin a coherent framework which enables qualitative data analysts to make use of\nstate-of-the-art Natural Language Processing techniques on very large document\ncollections. Applicability of the framework ranges from social sciences to\nmedia studies and market research. As an example we introduce the usage of the\nframework in a political science study on post-democracy and neoliberalism.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 13:04:15 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Niekler", "Andreas", ""], ["Wiedemann", "Gregor", ""], ["Heyer", "Gerhard", ""]]}, {"id": "1707.03255", "submitter": "Gregor Wiedemann", "authors": "Gerhard Heyer, Cathleen Kantner, Andreas Niekler, Max Overbeck, Gregor\n  Wiedemann", "title": "Modeling the dynamics of domain specific terminology in diachronic\n  corpora", "comments": "http://openarchive.cbs.dk/handle/10398/9323; Proceedings of the 12th\n  International conference on Terminology and Knowledge Engineering (TKE 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In terminology work, natural language processing, and digital humanities,\nseveral studies address the analysis of variations in context and meaning of\nterms in order to detect semantic change and the evolution of terms. We\ndistinguish three different approaches to describe contextual variations:\nmethods based on the analysis of patterns and linguistic clues, methods\nexploring the latent semantic space of single words, and methods for the\nanalysis of topic membership. The paper presents the notion of context\nvolatility as a new measure for detecting semantic change and applies it to key\nterm extraction in a political science case study. The measure quantifies the\ndynamics of a term's contextual variation within a diachronic corpus to\nidentify periods of time that are characterised by intense controversial\ndebates or substantial semantic transformations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 13:11:45 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Heyer", "Gerhard", ""], ["Kantner", "Cathleen", ""], ["Niekler", "Andreas", ""], ["Overbeck", "Max", ""], ["Wiedemann", "Gregor", ""]]}, {"id": "1707.03264", "submitter": "Benjamin Riedel", "authors": "Benjamin Riedel, Isabelle Augenstein, Georgios P. Spithourakis,\n  Sebastian Riedel", "title": "A simple but tough-to-beat baseline for the Fake News Challenge stance\n  detection task", "comments": "6 pages, 1 figure, 3 tables; additional reference and details added,\n  typos and wording corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying public misinformation is a complicated and challenging task. An\nimportant part of checking the veracity of a specific claim is to evaluate the\nstance different news sources take towards the assertion. Automatic stance\nevaluation, i.e. stance detection, would arguably facilitate the process of\nfact checking. In this paper, we present our stance detection system which\nclaimed third place in Stage 1 of the Fake News Challenge. Despite our\nstraightforward approach, our system performs at a competitive level with the\ncomplex ensembles of the top two winning teams. We therefore propose our system\nas the 'simple but tough-to-beat baseline' for the Fake News Challenge stance\ndetection task.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 13:44:51 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 10:31:40 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Riedel", "Benjamin", ""], ["Augenstein", "Isabelle", ""], ["Spithourakis", "Georgios P.", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1707.03457", "submitter": "Andreas Maletti", "authors": "Joost Engelfriet, Andreas Maletti, Sebastian Maneth", "title": "Multiple Context-Free Tree Grammars: Lexicalization and Characterization", "comments": "78 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple (simple) context-free tree grammars are investigated, where \"simple\"\nmeans \"linear and nondeleting\". Every multiple context-free tree grammar that\nis finitely ambiguous can be lexicalized; i.e., it can be transformed into an\nequivalent one (generating the same tree language) in which each rule of the\ngrammar contains a lexical symbol. Due to this transformation, the rank of the\nnonterminals increases at most by 1, and the multiplicity (or fan-out) of the\ngrammar increases at most by the maximal rank of the lexical symbols; in\nparticular, the multiplicity does not increase when all lexical symbols have\nrank 0. Multiple context-free tree grammars have the same tree generating power\nas multi-component tree adjoining grammars (provided the latter can use a\nroot-marker). Moreover, every multi-component tree adjoining grammar that is\nfinitely ambiguous can be lexicalized. Multiple context-free tree grammars have\nthe same string generating power as multiple context-free (string) grammars and\npolynomial time parsing algorithms. A tree language can be generated by a\nmultiple context-free tree grammar if and only if it is the image of a regular\ntree language under a deterministic finite-copying macro tree transducer.\nMultiple context-free tree grammars can be used as a synchronous translation\ndevice.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 20:48:57 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Engelfriet", "Joost", ""], ["Maletti", "Andreas", ""], ["Maneth", "Sebastian", ""]]}, {"id": "1707.03490", "submitter": "Slava Mikhaylov", "authors": "Stefano Gurciullo and Slava Mikhaylov", "title": "Detecting Policy Preferences and Dynamics in the UN General Debate with\n  Neural Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreign policy analysis has been struggling to find ways to measure policy\npreferences and paradigm shifts in international political systems. This paper\npresents a novel, potential solution to this challenge, through the application\nof a neural word embedding (Word2vec) model on a dataset featuring speeches by\nheads of state or government in the United Nations General Debate. The paper\nprovides three key contributions based on the output of the Word2vec model.\nFirst, it presents a set of policy attention indices, synthesizing the semantic\nproximity of political speeches to specific policy themes. Second, it\nintroduces country-specific semantic centrality indices, based on topological\nanalyses of countries' semantic positions with respect to each other. Third, it\ntests the hypothesis that there exists a statistical relation between the\nsemantic content of political speeches and UN voting behavior, falsifying it\nand suggesting that political speeches contain information of different nature\nthen the one behind voting outcomes. The paper concludes with a discussion of\nthe practical use of its results and consequences for foreign policy analysis,\npublic accountability, and transparency.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:16:20 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Gurciullo", "Stefano", ""], ["Mikhaylov", "Slava", ""]]}, {"id": "1707.03550", "submitter": "Yingjie Hu", "authors": "Yingjie Hu", "title": "Geospatial Semantics", "comments": "Yingjie Hu (2017). Geospatial Semantics. In Bo Huang, Thomas J. Cova,\n  and Ming-Hsiang Tsou et al. (Eds): Comprehensive Geographic Information\n  Systems, Elsevier. Oxford, UK", "journal-ref": null, "doi": "10.1016/B978-0-12-409548-9.09597-X", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial semantics is a broad field that involves a variety of research\nareas. The term semantics refers to the meaning of things, and is in contrast\nwith the term syntactics. Accordingly, studies on geospatial semantics usually\nfocus on understanding the meaning of geographic entities as well as their\ncounterparts in the cognitive and digital world, such as cognitive geographic\nconcepts and digital gazetteers. Geospatial semantics can also facilitate the\ndesign of geographic information systems (GIS) by enhancing the\ninteroperability of distributed systems and developing more intelligent\ninterfaces for user interactions. During the past years, a lot of research has\nbeen conducted, approaching geospatial semantics from different perspectives,\nusing a variety of methods, and targeting different problems. Meanwhile, the\narrival of big geo data, especially the large amount of unstructured text data\non the Web, and the fast development of natural language processing methods\nenable new research directions in geospatial semantics. This chapter,\ntherefore, provides a systematic review on the existing geospatial semantic\nresearch. Six major research areas are identified and discussed, including\nsemantic interoperability, digital gazetteers, geographic information\nretrieval, geospatial Semantic Web, place semantics, and cognitive geographic\nconcepts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 05:41:06 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 05:40:49 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Hu", "Yingjie", ""]]}, {"id": "1707.03569", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Simon Moura, Massih-Reza Amini", "title": "Multitask Learning for Fine-Grained Twitter Sentiment Analysis", "comments": "International ACM SIGIR Conference on Research and Development in\n  Information Retrieval 2017", "journal-ref": null, "doi": "10.1145/3077136.3080702", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional sentiment analysis approaches tackle problems like ternary\n(3-category) and fine-grained (5-category) classification by learning the tasks\nseparately. We argue that such classification tasks are correlated and we\npropose a multitask approach based on a recurrent neural network that benefits\nby jointly learning them. Our study demonstrates the potential of multitask\nmodels on this type of problems and improves the state-of-the-art results in\nthe fine-grained sentiment classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 07:17:50 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Balikas", "Georgios", ""], ["Moura", "Simon", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1707.03736", "submitter": "Preslav Nakov", "authors": "Georgi Karadjov, Tsvetomila Mihaylova, Yasen Kiprov, Georgi Georgiev,\n  Ivan Koychev, and Preslav Nakov", "title": "The Case for Being Average: A Mediocrity Approach to Style Masking and\n  Author Obfuscation", "comments": "Best of the Labs Track at CLEF-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users posting online expect to remain anonymous unless they have logged in,\nwhich is often needed for them to be able to discuss freely on various topics.\nPreserving the anonymity of a text's writer can be also important in some other\ncontexts, e.g., in the case of witness protection or anonymity programs.\nHowever, each person has his/her own style of writing, which can be analyzed\nusing stylometry, and as a result, the true identity of the author of a piece\nof text can be revealed even if s/he has tried to hide it. Thus, it could be\nhelpful to design automatic tools that can help a person obfuscate his/her\nidentity when writing text. In particular, here we propose an approach that\nchanges the text, so that it is pushed towards average values for some general\nstylometric characteristics, thus making the use of these characteristics less\ndiscriminative. The approach consists of three main steps: first, we calculate\nthe values for some popular stylometric metrics that can indicate authorship;\nthen we apply various transformations to the text, so that these metrics are\nadjusted towards the average level, while preserving the semantics and the\nsoundness of the text; and finally, we add random noise. This approach turned\nout to be very efficient, and yielded the best performance on the Author\nObfuscation task at the PAN-2016 competition.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 14:27:00 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 16:11:04 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Karadjov", "Georgi", ""], ["Mihaylova", "Tsvetomila", ""], ["Kiprov", "Yasen", ""], ["Georgiev", "Georgi", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "1707.03764", "submitter": "Gareth Dwyer", "authors": "Angelo Basile, Gareth Dwyer, Maria Medvedeva, Josine Rawee, Hessel\n  Haagsma and Malvina Nissim", "title": "N-GrAM: New Groningen Author-profiling Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our participation in the PAN 2017 shared task on Author\nProfiling, identifying authors' gender and language variety for English,\nSpanish, Arabic and Portuguese. We describe both the final, submitted system,\nand a series of negative results. Our aim was to create a single model for both\ngender and language, and for all language varieties. Our best-performing system\n(on cross-validated results) is a linear support vector machine (SVM) with word\nunigrams and character 3- to 5-grams as features. A set of additional features,\nincluding POS tags, additional datasets, geographic entities, and Twitter\nhandles, hurt, rather than improve, performance. Results from cross-validation\nindicated high performance overall and results on the test set confirmed them,\nat 0.86 averaged accuracy, with performance on sub-tasks ranging from 0.68 to\n0.98.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 15:34:21 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Basile", "Angelo", ""], ["Dwyer", "Gareth", ""], ["Medvedeva", "Maria", ""], ["Rawee", "Josine", ""], ["Haagsma", "Hessel", ""], ["Nissim", "Malvina", ""]]}, {"id": "1707.03804", "submitter": "Hao Tan", "authors": "Hao Tan, Mohit Bansal", "title": "Source-Target Inference Models for Spatial Instruction Understanding", "comments": "Accepted to AAAI 2018 (8 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that can execute natural language instructions for situated robotic\ntasks such as assembly and navigation have several useful applications in\nhomes, offices, and remote scenarios. We study the semantics of\nspatially-referred configuration and arrangement instructions, based on the\nchallenging Bisk-2016 blank-labeled block dataset. This task involves finding a\nsource block and moving it to the target position (mentioned via a reference\nblock and offset), where the blocks have no names or colors and are just\nreferred to via spatial location features. We present novel models for the\nsubtasks of source block classification and target position regression, based\non joint-loss language and spatial-world representation learning, as well as\nCNN-based and dual attention models to compute the alignment between the world\nblocks and the instruction phrases. For target position prediction, we compare\ntwo inference approaches: annealed sampling via policy gradient versus\nexpectation inference via supervised regression. Our models achieve the new\nstate-of-the-art on this task, with an improvement of 47% on source block\naccuracy and 22% on target position distance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 17:15:57 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 16:57:02 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "1707.03819", "submitter": "Minh Le", "authors": "Minh Le", "title": "A Critique of a Critique of Word Similarity Datasets: Sanity Check or\n  Unnecessary Confusion?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Critical evaluation of word similarity datasets is very important for\ncomputational lexical semantics. This short report concerns the sanity check\nproposed in Batchkarov et al. (2016) to evaluate several popular datasets such\nas MC, RG and MEN -- the first two reportedly failed. I argue that this test is\nunstable, offers no added insight, and needs major revision in order to fulfill\nits purported goal.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 10:09:32 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Le", "Minh", ""]]}, {"id": "1707.03903", "submitter": "Alexander Panchenko", "authors": "Dmitry Ustalov, Nikolay Arefyev, Chris Biemann, Alexander Panchenko", "title": "Negative Sampling Improves Hypernymy Extraction Based on Projection\n  Learning", "comments": "In Proceedings of the 15th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL'2017). Valencia, Spain.\n  Association for Computational Linguistics", "journal-ref": null, "doi": "10.18653/v1/E17-2087", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a new approach to extraction of hypernyms based on projection\nlearning and word embeddings. In contrast to classification-based approaches,\nprojection-based methods require no candidate hyponym-hypernym pairs. While it\nis natural to use both positive and negative training examples in supervised\nrelation extraction, the impact of negative examples on hypernym prediction was\nnot studied so far. In this paper, we show that explicit negative examples used\nfor regularization of the model significantly improve performance compared to\nthe state-of-the-art approach of Fu et al. (2014) on three datasets from\ndifferent languages.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:47:47 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 10:49:55 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Ustalov", "Dmitry", ""], ["Arefyev", "Nikolay", ""], ["Biemann", "Chris", ""], ["Panchenko", "Alexander", ""]]}, {"id": "1707.03904", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Kathryn Mazaitis and William W. Cohen", "title": "Quasar: Datasets for Question Answering by Search and Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new large-scale datasets aimed at evaluating systems designed\nto comprehend a natural language query and extract its answer from a large\ncorpus of text. The Quasar-S dataset consists of 37000 cloze-style\n(fill-in-the-gap) queries constructed from definitions of software entity tags\non the popular website Stack Overflow. The posts and comments on the website\nserve as the background corpus for answering the cloze questions. The Quasar-T\ndataset consists of 43000 open-domain trivia questions and their answers\nobtained from various internet sources. ClueWeb09 serves as the background\ncorpus for extracting these answers. We pose these datasets as a challenge for\ntwo related subtasks of factoid Question Answering: (1) searching for relevant\npieces of text that include the correct answer to a query, and (2) reading the\nretrieved text to answer the query. We also describe a retrieval system for\nextracting relevant sentences and documents from the corpus given a query, and\ninclude these in the release for researchers wishing to only focus on (2). We\nevaluate several baselines on both datasets, ranging from simple heuristics to\npowerful neural models, and show that these lag behind human performance by\n16.4% and 32.1% for Quasar-S and -T respectively. The datasets are available at\nhttps://github.com/bdhingra/quasar .\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:53:26 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 01:48:08 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Mazaitis", "Kathryn", ""], ["Cohen", "William W.", ""]]}, {"id": "1707.03938", "submitter": "Michael Janner", "authors": "Michael Janner, Karthik Narasimhan, Regina Barzilay", "title": "Representation Learning for Grounded Spatial Reasoning", "comments": "Accepted to TACL 2017, code:\n  https://github.com/jannerm/spatial-reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of spatial references is highly contextual, requiring\njoint inference over both language and the environment. We consider the task of\nspatial reasoning in a simulated environment, where an agent can act and\nreceive rewards. The proposed model learns a representation of the world\nsteered by instruction text. This design allows for precise alignment of local\nneighborhoods with corresponding verbalizations, while also handling global\nreferences in the instructions. We train our model with reinforcement learning\nusing a variant of generalized value iteration. The model outperforms\nstate-of-the-art approaches on several metrics, yielding a 45% reduction in\ngoal localization error.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 00:17:45 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 02:20:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Janner", "Michael", ""], ["Narasimhan", "Karthik", ""], ["Barzilay", "Regina", ""]]}, {"id": "1707.03968", "submitter": "Shumpei Sano", "authors": "Shumpei Sano, Nobuhiro Kaji, and Manabu Sassano", "title": "Predicting Causes of Reformulation in Intelligent Assistants", "comments": "11 pages, 2 figures, accepted as a long paper for SIGDIAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent assistants (IAs) such as Siri and Cortana conversationally\ninteract with users and execute a wide range of actions (e.g., searching the\nWeb, setting alarms, and chatting). IAs can support these actions through the\ncombination of various components such as automatic speech recognition, natural\nlanguage understanding, and language generation. However, the complexity of\nthese components hinders developers from determining which component causes an\nerror. To remove this hindrance, we focus on reformulation, which is a useful\nsignal of user dissatisfaction, and propose a method to predict the\nreformulation causes. We evaluate the method using the user logs of a\ncommercial IA. The experimental results have demonstrated that features\ndesigned to detect the error of a specific component improve the performance of\nreformulation cause detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 03:47:32 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Sano", "Shumpei", ""], ["Kaji", "Nobuhiro", ""], ["Sassano", "Manabu", ""]]}, {"id": "1707.03997", "submitter": "John J. Camilleri", "authors": "John J. Camilleri and Mohammad Reza Haghshenas and Gerardo Schneider", "title": "A Web-Based Tool for Analysing Normative Documents in English", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to use formal methods to analyse normative documents written in\nEnglish, such as privacy policies and service-level agreements. This requires\nthe combination of a number of different elements, including information\nextraction from natural language, formal languages for model representation,\nand an interface for property specification and verification. We have worked on\na collection of components for this task: a natural language extraction tool, a\nsuitable formalism for representing such documents, an interface for building\nmodels in this formalism, and methods for answering queries asked of a given\nmodel. In this work, each of these concerns is brought together in a web-based\ntool, providing a single interface for analysing normative texts in English.\nThrough the use of a running example, we describe each component and\ndemonstrate the workflow established by our tool.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 07:22:18 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Camilleri", "John J.", ""], ["Haghshenas", "Mohammad Reza", ""], ["Schneider", "Gerardo", ""]]}, {"id": "1707.04095", "submitter": "Chlo\\'e Braud", "authors": "Chlo\\'e Braud and Anders S{\\o}gaard", "title": "Is writing style predictive of scientific fraud?", "comments": "To appear in the Proceedings of the Workshop on Stylistic Variation\n  2017 (EMNLP), 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting scientific fraud using machine learning was recently\nintroduced, with initial, positive results from a model taking into account\nvarious general indicators. The results seem to suggest that writing style is\npredictive of scientific fraud. We revisit these initial experiments, and show\nthat the leave-one-out testing procedure they used likely leads to a slight\nover-estimate of the predictability, but also that simple models can outperform\ntheir proposed model by some margin. We go on to explore more abstract\nlinguistic features, such as linguistic complexity and discourse structure,\nonly to obtain negative results. Upon analyzing our models, we do see some\ninteresting patterns, though: Scientific fraud, for examples, contains less\ncomparison, as well as different types of hedging and ways of presenting\nlogical reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 12:51:39 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Braud", "Chlo\u00e9", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1707.04108", "submitter": "Le-Thien Hoa", "authors": "Hoa T. Le, Christophe Cerisara, Alexandre Denis", "title": "Do Convolutional Networks need to be Deep for Text Classification ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this work the importance of depth in convolutional models for\ntext classification, either when character or word inputs are considered. We\nshow on 5 standard text classification and sentiment analysis tasks that deep\nmodels indeed give better performances than shallow networks when the text\ninput is represented as a sequence of characters. However, a simple\nshallow-and-wide network outperforms deep models such as DenseNet with word\ninputs. Our shallow word model further establishes new state-of-the-art\nperformances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%).\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:18:52 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Le", "Hoa T.", ""], ["Cerisara", "Christophe", ""], ["Denis", "Alexandre", ""]]}, {"id": "1707.04218", "submitter": "Yanpeng Li", "authors": "Yanpeng Li", "title": "Learning Features from Co-occurrences: A Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a word by its co-occurrences with other words in context is an\neffective way to capture the meaning of the word. However, the theory behind\nremains a challenge. In this work, taking the example of a word classification\ntask, we give a theoretical analysis of the approaches that represent a word X\nby a function f(P(C|X)), where C is a context feature, P(C|X) is the\nconditional probability estimated from a text corpus, and the function f maps\nthe co-occurrence measure to a prediction score. We investigate the impact of\ncontext feature C and the function f. We also explain the reasons why using the\nco-occurrences with multiple context features may be better than just using a\nsingle one. In addition, some of the results shed light on the theory of\nfeature learning and machine learning in general.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:46:50 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Li", "Yanpeng", ""]]}, {"id": "1707.04221", "submitter": "Jonathan K Kummerfeld", "authors": "Jonathan K. Kummerfeld and Dan Klein", "title": "Parsing with Traces: An $O(n^4)$ Algorithm and a Structural\n  Representation", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics", "journal-ref": "TACL 5 (2017) 441-454", "doi": "10.1162/tacl_a_00072", "report-no": null, "categories": "cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General treebank analyses are graph structured, but parsers are typically\nrestricted to tree structures for efficiency and modeling reasons. We propose a\nnew representation and algorithm for a class of graph structures that is\nflexible enough to cover almost all treebank structures, while still admitting\nefficient learning and inference. In particular, we consider directed, acyclic,\none-endpoint-crossing graph structures, which cover most long-distance\ndislocation, shared argumentation, and similar tree-violating linguistic\nphenomena. We describe how to convert phrase structure parses, including\ntraces, to our new representation in a reversible manner. Our dynamic program\nuniquely decomposes structures, is sound and complete, and covers 97.3% of the\nPenn English Treebank. We also implement a proof-of-concept parser that\nrecovers a range of null elements and trace types.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:52:08 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kummerfeld", "Jonathan K.", ""], ["Klein", "Dan", ""]]}, {"id": "1707.04227", "submitter": "Seppo Enarvi", "authors": "Seppo Enarvi, Peter Smit, Sami Virpioja, Mikko Kurimo", "title": "Automatic Speech Recognition with Very Large Conversational Finnish and\n  Estonian Vocabularies", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 25, no. 11, pp. 2085-2097, November 2017", "doi": "10.1109/TASLP.2017.2743344", "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, the vocabulary size for language models in large vocabulary speech\nrecognition is typically several hundreds of thousands of words. While this is\nalready sufficient in some applications, the out-of-vocabulary words are still\nlimiting the usability in others. In agglutinative languages the vocabulary for\nconversational speech should include millions of word forms to cover the\nspelling variations due to colloquial pronunciations, in addition to the word\ncompounding and inflections. Very large vocabularies are also needed, for\nexample, when the recognition of rare proper names is important.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:16:16 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 14:55:18 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 10:09:31 GMT"}, {"version": "v4", "created": "Tue, 19 Sep 2017 18:27:31 GMT"}, {"version": "v5", "created": "Fri, 29 Sep 2017 18:29:57 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Enarvi", "Seppo", ""], ["Smit", "Peter", ""], ["Virpioja", "Sami", ""], ["Kurimo", "Mikko", ""]]}, {"id": "1707.04242", "submitter": "Christophe Van Gysel", "authors": "Tom Kenter, Alexey Borisov, Christophe Van Gysel, Mostafa Dehghani,\n  Maarten de Rijke, Bhaskar Mitra", "title": "Neural Networks for Information Retrieval", "comments": "Overview of full-day tutorial at SIGIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning plays a role in many aspects of modern IR systems, and deep\nlearning is applied in all of them. The fast pace of modern-day research has\ngiven rise to many different approaches for many different IR problems. The\namount of information available can be overwhelming both for junior students\nand for experienced researchers looking for new research topics and directions.\nAdditionally, it is interesting to see what key insights into IR problems the\nnew technologies are able to give us. The aim of this full-day tutorial is to\ngive a clear overview of current tried-and-trusted neural methods in IR and how\nthey benefit IR research. It covers key architectures, as well as the most\npromising future directions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:46:59 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Kenter", "Tom", ""], ["Borisov", "Alexey", ""], ["Van Gysel", "Christophe", ""], ["Dehghani", "Mostafa", ""], ["de Rijke", "Maarten", ""], ["Mitra", "Bhaskar", ""]]}, {"id": "1707.04244", "submitter": "Preeti Bhargava", "authors": "Preeti Bhargava and Nemanja Spasojevic and Guoning Hu", "title": "Lithium NLP: A System for Rich Information Extraction from Noisy User\n  Generated Text on Social Media", "comments": "9 pages, 6 figures, 2 tables, EMNLP 2017 Workshop on Noisy User\n  Generated Text WNUT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the Lithium Natural Language Processing (NLP)\nsystem - a resource-constrained, high- throughput and language-agnostic system\nfor information extraction from noisy user generated text on social media.\nLithium NLP extracts a rich set of information including entities, topics,\nhashtags and sentiment from text. We discuss several real world applications of\nthe system currently incorporated in Lithium products. We also compare our\nsystem with existing commercial and academic NLP systems in terms of\nperformance, information extracted and languages supported. We show that\nLithium NLP is at par with and in some cases, outperforms state- of-the-art\ncommercial NLP systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:52:51 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Bhargava", "Preeti", ""], ["Spasojevic", "Nemanja", ""], ["Hu", "Guoning", ""]]}, {"id": "1707.04408", "submitter": "Soujanya Poria", "authors": "Rajiv Bajpai, Soujanya Poria, Danyun Ho, and Erik Cambria", "title": "Developing a concept-level knowledge base for sentiment analysis in\n  Singlish", "comments": "CICLing 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we present Singlish sentiment lexicon, a concept-level\nknowledge base for sentiment analysis that associates multiword expressions to\na set of emotion labels and a polarity value. Unlike many other sentiment\nanalysis resources, this lexicon is not built by manually labeling pieces of\nknowledge coming from general NLP resources such as WordNet or DBPedia.\nInstead, it is automatically constructed by applying graph-mining and\nmulti-dimensional scaling techniques on the affective common-sense knowledge\ncollected from three different sources. This knowledge is represented\nredundantly at three levels: semantic network, matrix, and vector space.\nSubsequently, the concepts are labeled by emotions and polarity through the\nensemble application of spreading activation, neural networks and an emotion\ncategorization model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 08:00:07 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Bajpai", "Rajiv", ""], ["Poria", "Soujanya", ""], ["Ho", "Danyun", ""], ["Cambria", "Erik", ""]]}, {"id": "1707.04412", "submitter": "Alon Talmor", "authors": "Alon Talmor, Mor Geva, Jonathan Berant", "title": "Evaluating Semantic Parsing against a Simple Web-based Question\n  Answering Model", "comments": "*sem 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing shines at analyzing complex natural language that involves\ncomposition and computation over multiple pieces of evidence. However, datasets\nfor semantic parsing contain many factoid questions that can be answered from a\nsingle web document. In this paper, we propose to evaluate semantic\nparsing-based question answering models by comparing them to a question\nanswering baseline that queries the web and extracts the answer only from web\nsnippets, without access to the target knowledge-base. We investigate this\napproach on COMPLEXQUESTIONS, a dataset designed to focus on compositional\nlanguage, and find that our model obtains reasonable performance (35 F1\ncompared to 41 F1 of state-of-the-art). We find in our analysis that our model\nperforms well on complex questions involving conjunctions, but struggles on\nquestions that involve relation composition and superlatives.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 08:25:36 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Talmor", "Alon", ""], ["Geva", "Mor", ""], ["Berant", "Jonathan", ""]]}, {"id": "1707.04481", "submitter": "Ozan Caglayan", "authors": "Ozan Caglayan, Walid Aransa, Adrien Bardet, Mercedes\n  Garc\\'ia-Mart\\'inez, Fethi Bougares, Lo\\\"ic Barrault, Marc Masana, Luis\n  Herranz and Joost van de Weijer", "title": "LIUM-CVC Submissions for WMT17 Multimodal Translation Task", "comments": "MMT System Description Paper for WMT17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the monomodal and multimodal Neural Machine Translation\nsystems developed by LIUM and CVC for WMT17 Shared Task on Multimodal\nTranslation. We mainly explored two multimodal architectures where either\nglobal visual features or convolutional feature maps are integrated in order to\nbenefit from visual context. Our final systems ranked first for both En-De and\nEn-Fr language pairs according to the automatic evaluation metrics METEOR and\nBLEU.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:33:25 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Caglayan", "Ozan", ""], ["Aransa", "Walid", ""], ["Bardet", "Adrien", ""], ["Garc\u00eda-Mart\u00ednez", "Mercedes", ""], ["Bougares", "Fethi", ""], ["Barrault", "Lo\u00efc", ""], ["Masana", "Marc", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1707.04499", "submitter": "Ozan Caglayan", "authors": "Mercedes Garc\\'ia-Mart\\'inez, Ozan Caglayan, Walid Aransa, Adrien\n  Bardet, Fethi Bougares, Lo\\\"ic Barrault", "title": "LIUM Machine Translation Systems for WMT17 News Translation Task", "comments": "News Translation Task System Description paper for WMT17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes LIUM submissions to WMT17 News Translation Task for\nEnglish-German, English-Turkish, English-Czech and English-Latvian language\npairs. We train BPE-based attentive Neural Machine Translation systems with and\nwithout factored outputs using the open source nmtpy framework. Competitive\nscores were obtained by ensembling various systems and exploiting the\navailability of target monolingual corpora for back-translation. The impact of\nback-translation quantity and quality is also analyzed for English-Turkish\nwhere our post-deadline submission surpassed the best entry by +1.6 BLEU.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 13:10:22 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Garc\u00eda-Mart\u00ednez", "Mercedes", ""], ["Caglayan", "Ozan", ""], ["Aransa", "Walid", ""], ["Bardet", "Adrien", ""], ["Bougares", "Fethi", ""], ["Barrault", "Lo\u00efc", ""]]}, {"id": "1707.04538", "submitter": "Tomasz Jurczyk", "authors": "Tomasz Jurczyk, Jinho D. Choi", "title": "Cross-genre Document Retrieval: Matching between Conversational and\n  Formal Writings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper challenges a cross-genre document retrieval task, where the\nqueries are in formal writing and the target documents are in conversational\nwriting. In this task, a query, is a sentence extracted from either a summary\nor a plot of an episode in a TV show, and the target document consists of\ntranscripts from the corresponding episode. To establish a strong baseline, we\nemploy the current state-of-the-art search engine to perform document retrieval\non the dataset collected for this work. We then introduce a structure reranking\napproach to improve the initial ranking by utilizing syntactic and semantic\nstructures generated by NLP tools. Our evaluation shows an improvement of more\nthan 4% when the structure reranking is applied, which is very promising.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 15:21:25 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Jurczyk", "Tomasz", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1707.04546", "submitter": "Samridhi Shree Choudhary", "authors": "Shrimai Prabhumoye and Samridhi Choudhary, Evangelia Spiliopoulou,\n  Christopher Bogart, Carolyn Penstein Rose, Alan W Black", "title": "Linguistic Markers of Influence in Informal Interactions", "comments": "10 pages, Accepted in NLP+CSS workshop for ACL (Association for\n  Computational Linguistics) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a long standing interest in understanding `Social Influence'\nboth in Social Sciences and in Computational Linguistics. In this paper, we\npresent a novel approach to study and measure interpersonal influence in daily\ninteractions. Motivated by the basic principles of influence, we attempt to\nidentify indicative linguistic features of the posts in an online knitting\ncommunity. We present the scheme used to operationalize and label the posts\nwith indicator features. Experiments with the identified features show an\nimprovement in the classification accuracy of influence by 3.15%. Our results\nillustrate the important correlation between the characteristics of the\nlanguage and its potential to influence others.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 15:43:45 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Prabhumoye", "Shrimai", ""], ["Choudhary", "Samridhi", ""], ["Spiliopoulou", "Evangelia", ""], ["Bogart", "Christopher", ""], ["Rose", "Carolyn Penstein", ""], ["Black", "Alan W", ""]]}, {"id": "1707.04550", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Helcl and Jind\\v{r}ich Libovick\\'y", "title": "CUNI System for the WMT17 Multimodal Translation Task", "comments": "8 pages; Camera-ready submission to WMT17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our submissions to the WMT17 Multimodal\nTranslation Task. For Task 1 (multimodal translation), our best scoring system\nis a purely textual neural translation of the source image caption to the\ntarget language. The main feature of the system is the use of additional data\nthat was acquired by selecting similar sentences from parallel corpora and by\ndata synthesis with back-translation. For Task 2 (cross-lingual image\ncaptioning), our best submitted system generates an English caption which is\nthen translated by the best system used in Task 1. We also present negative\nresults, which are based on ideas that we believe have potential of making\nimprovements, but did not prove to be useful in our particular setup.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 15:58:47 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Helcl", "Jind\u0159ich", ""], ["Libovick\u00fd", "Jind\u0159ich", ""]]}, {"id": "1707.04596", "submitter": "Akshay Soni", "authors": "Sheng Chen, Akshay Soni, Aasish Pappu, Yashar Mehdad", "title": "DocTag2Vec: An Embedding Based Multi-label Learning Approach for\n  Document Tagging", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tagging news articles or blog posts with relevant tags from a collection of\npredefined ones is coined as document tagging in this work. Accurate tagging of\narticles can benefit several downstream applications such as recommendation and\nsearch. In this work, we propose a novel yet simple approach called DocTag2Vec\nto accomplish this task. We substantially extend Word2Vec and Doc2Vec---two\npopular models for learning distributed representation of words and documents.\nIn DocTag2Vec, we simultaneously learn the representation of words, documents,\nand tags in a joint vector space during training, and employ the simple\n$k$-nearest neighbor search to predict tags for unseen documents. In contrast\nto previous multi-label learning methods, DocTag2Vec directly deals with raw\ntext instead of provided feature vector, and in addition, enjoys advantages\nlike the learning of tag representation, and the ability of handling newly\ncreated tags. To demonstrate the effectiveness of our approach, we conduct\nexperiments on several datasets and show promising results against\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 18:05:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Chen", "Sheng", ""], ["Soni", "Akshay", ""], ["Pappu", "Aasish", ""], ["Mehdad", "Yashar", ""]]}, {"id": "1707.04652", "submitter": "Sanjaya Wijeratne", "authors": "Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran", "title": "EmojiNet: An Open Service and API for Emoji Sense Discovery", "comments": "This paper was published at ICWSM 2017 as a full paper, Proc. of the\n  11th International AAAI Conference on Web and Social Media (ICWSM 2017).\n  Montreal, Canada. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the release of EmojiNet, the largest machine-readable\nemoji sense inventory that links Unicode emoji representations to their English\nmeanings extracted from the Web. EmojiNet is a dataset consisting of: (i)\n12,904 sense labels over 2,389 emoji, which were extracted from the web and\nlinked to machine-readable sense definitions seen in BabelNet, (ii) context\nwords associated with each emoji sense, which are inferred through word\nembedding models trained over Google News corpus and a Twitter message corpus\nfor each emoji sense definition, and (iii) recognizing discrepancies in the\npresentation of emoji on different platforms, specification of the most likely\nplatform-based emoji sense for a selected set of emoji. The dataset is hosted\nas an open service with a REST API and is available at\nhttp://emojinet.knoesis.org/. The development of this dataset, evaluation of\nits quality, and its applications including emoji sense disambiguation and\nemoji sense similarity are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 22:02:05 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wijeratne", "Sanjaya", ""], ["Balasuriya", "Lakshika", ""], ["Sheth", "Amit", ""], ["Doran", "Derek", ""]]}, {"id": "1707.04653", "submitter": "Sanjaya Wijeratne", "authors": "Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran", "title": "A Semantics-Based Measure of Emoji Similarity", "comments": "This paper is accepted at Web Intelligence 2017 as a full paper, In\n  2017 IEEE/WIC/ACM International Conference on Web Intelligence (WI). Leipzig,\n  Germany: ACM, 2017", "journal-ref": null, "doi": "10.1145/3106426.3106490", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emoji have grown to become one of the most important forms of communication\non the web. With its widespread use, measuring the similarity of emoji has\nbecome an important problem for contemporary text processing since it lies at\nthe heart of sentiment analysis, search, and interface design tasks. This paper\npresents a comprehensive analysis of the semantic similarity of emoji through\nembedding models that are learned over machine-readable emoji meanings in the\nEmojiNet knowledge base. Using emoji descriptions, emoji sense labels and emoji\nsense definitions, and with different training corpora obtained from Twitter\nand Google News, we develop and test multiple embedding models to measure emoji\nsimilarity. To evaluate our work, we create a new dataset called EmoSim508,\nwhich assigns human-annotated semantic similarity scores to a set of 508\ncarefully selected emoji pairs. After validation with EmoSim508, we present a\nreal-world use-case of our emoji embedding models using a sentiment analysis\ntask and show that our models outperform the previous best-performing emoji\nembedding model on this task. The EmoSim508 dataset and our emoji embedding\nmodels are publicly released with this paper and can be downloaded from\nhttp://emojinet.knoesis.org/.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 22:08:15 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wijeratne", "Sanjaya", ""], ["Balasuriya", "Lakshika", ""], ["Sheth", "Amit", ""], ["Doran", "Derek", ""]]}, {"id": "1707.04662", "submitter": "Alexey Zobnin", "authors": "Alexey Zobnin", "title": "Rotations and Interpretability of Word Embeddings: the Case of the\n  Russian Language", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-73013-4_11", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a continuous word embedding model. Usually, the cosines between word\nvectors are used as a measure of similarity of words. These cosines do not\nchange under orthogonal transformations of the embedding space. We demonstrate\nthat, using some canonical orthogonal transformations from SVD, it is possible\nboth to increase the meaning of some components and to make the components more\nstable under re-learning. We study the interpretability of components for\npublicly available models for the Russian language (RusVectores, fastText,\nRDT).\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 23:26:35 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zobnin", "Alexey", ""]]}, {"id": "1707.04678", "submitter": "Alexandros Tsaptsinos", "authors": "Alexandros Tsaptsinos", "title": "Lyrics-Based Music Genre Classification Using a Hierarchical Attention\n  Network", "comments": "8 pages, 4 figures, ISMIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music genre classification, especially using lyrics alone, remains a\nchallenging topic in Music Information Retrieval. In this study we apply\nrecurrent neural network models to classify a large dataset of intact song\nlyrics. As lyrics exhibit a hierarchical layer structure - in which words\ncombine to form lines, lines form segments, and segments form a complete song -\nwe adapt a hierarchical attention network (HAN) to exploit these layers and in\naddition learn the importance of the words, lines, and segments. We test the\nmodel over a 117-genre dataset and a reduced 20-genre dataset. Experimental\nresults show that the HAN outperforms both non-neural models and simpler neural\nmodels, whilst also classifying over a higher number of genres than previous\nresearch. Through the learning process we can also visualise which words or\nlines in a song the model believes are important to classifying the genre. As a\nresult the HAN provides insights, from a computational perspective, into\nlyrical structure and language features that differentiate musical genres.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 02:22:41 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tsaptsinos", "Alexandros", ""]]}, {"id": "1707.04724", "submitter": "Samer Abdallah", "authors": "Samer Abdallah", "title": "Memoisation: Purely, Left-recursively, and with (Continuation Passing)\n  Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memoisation, or tabling, is a well-known technique that yields large\nimprovements in the performance of some recursive computations. Tabled\nresolution in Prologs such as XSB and B-Prolog can transform so called\nleft-recursive predicates from non-terminating computations into finite and\nwell-behaved ones. In the functional programming literature, memoisation has\nusually been implemented in a way that does not handle left-recursion,\nrequiring supplementary mechanisms to prevent non-termination. A notable\nexception is Johnson's (1995) continuation passing approach in Scheme. This,\nhowever, relies on mutation of a memo table data structure and coding in\nexplicit continuation passing style. We show how Johnson's approach can be\nimplemented purely functionally in a modern, strongly typed functional language\n(OCaml), presented via a monadic interface that hides the implementation\ndetails, yet providing a way to return a compact represention of the memo\ntables at the end of the computation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 11:03:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Abdallah", "Samer", ""]]}, {"id": "1707.04817", "submitter": "Shervin Malmasi", "authors": "Shervin Malmasi", "title": "Open-Set Language Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first open-set language identification experiments using\none-class classification. We first highlight the shortcomings of traditional\nfeature extraction methods and propose a hashing-based feature vectorization\napproach as a solution. Using a dataset of 10 languages from different writing\nsystems, we train a One- Class Support Vector Machine using only a monolingual\ncorpus for each language. Each model is evaluated against a test set of data\nfrom all 10 languages and we achieve an average F-score of 0.99, highlighting\nthe effectiveness of this approach for open-set language identification.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 04:17:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Malmasi", "Shervin", ""]]}, {"id": "1707.04848", "submitter": "Kumiko Tanaka-Ishii", "authors": "Shuntaro Takahashi and Kumiko Tanaka-Ishii", "title": "Do Neural Nets Learn Statistical Laws behind Natural Language?", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0189326", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep learning in natural language processing has been\nspectacular, but the reasons for this success remain unclear because of the\ninherent complexity of deep learning. This paper provides empirical evidence of\nits effectiveness and of a limitation of neural networks for language\nengineering. Precisely, we demonstrate that a neural language model based on\nlong short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law,\ntwo representative statistical properties underlying natural language. We\ndiscuss the quality of reproducibility and the emergence of Zipf's law and\nHeaps' law as training progresses. We also point out that the neural language\nmodel has a limitation in reproducing long-range correlation, another\nstatistical property of natural language. This understanding could provide a\ndirection for improving the architectures of neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 09:08:42 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 07:36:25 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Takahashi", "Shuntaro", ""], ["Tanaka-Ishii", "Kumiko", ""]]}, {"id": "1707.04860", "submitter": "Amir Bakarov", "authors": "Amir Bakarov, Olga Gureenkova", "title": "Automated Detection of Non-Relevant Posts on the Russian Imageboard\n  \"2ch\": Importance of the Choice of Word Representations", "comments": "6 pages, 1 figure, 1 table, main proceedings of AIST-2017 (Analysis\n  of Images, Social Networks, and Texts)", "journal-ref": null, "doi": "10.1007/978-3-319-73013-4_2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the problem of automated detection of non-relevant posts\non Web forums and discusses the approach of resolving this problem by\napproximation it with the task of detection of semantic relatedness between the\ngiven post and the opening post of the forum discussion thread. The\napproximated task could be resolved through learning the supervised classifier\nwith a composed word embeddings of two posts. Considering that the success in\nthis task could be quite sensitive to the choice of word representations, we\npropose a comparison of the performance of different word embedding models. We\ntrain 7 models (Word2Vec, Glove, Word2Vec-f, Wang2Vec, AdaGram, FastText,\nSwivel), evaluate embeddings produced by them on dataset of human judgements\nand compare their performance on the task of non-relevant posts detection. To\nmake the comparison, we propose a dataset of semantic relatedness with posts\nfrom one of the most popular Russian Web forums, imageboard \"2ch\", which has\nchallenging lexical and grammatical features.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 11:08:08 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Bakarov", "Amir", ""], ["Gureenkova", "Olga", ""]]}, {"id": "1707.04879", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura", "title": "Listening while Speaking: Speech Chain by Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the close relationship between speech perception and production,\nresearch in automatic speech recognition (ASR) and text-to-speech synthesis\n(TTS) has progressed more or less independently without exerting much mutual\ninfluence on each other. In human communication, on the other hand, a\nclosed-loop speech chain mechanism with auditory feedback from the speaker's\nmouth to her ear is crucial. In this paper, we take a step further and develop\na closed-loop speech chain model based on deep learning. The\nsequence-to-sequence model in close-loop architecture allows us to train our\nmodel on the concatenation of both labeled and unlabeled data. While ASR\ntranscribes the unlabeled speech features, TTS attempts to reconstruct the\noriginal speech waveform based on the text from ASR. In the opposite direction,\nASR also attempts to reconstruct the original text transcription given the\nsynthesized speech. To the best of our knowledge, this is the first deep\nlearning model that integrates human speech perception and production\nbehaviors. Our experimental results show that the proposed approach\nsignificantly improved the performance more than separate systems that were\nonly trained with labeled data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 13:27:56 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1707.04913", "submitter": "Rasmus Berg Palm", "authors": "Rasmus Berg Palm, Dirk Hovy, Florian Laws, Ole Winther", "title": "End-to-End Information Extraction without Token-Level Supervision", "comments": "http://speechnlp.github.io/2017 @ EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art information extraction approaches rely on token-level\nlabels to find the areas of interest in text. Unfortunately, these labels are\ntime-consuming and costly to create, and consequently, not available for many\nreal-life IE tasks. To make matters worse, token-level labels are usually not\nthe desired output, but just an intermediary step. End-to-end (E2E) models,\nwhich take raw text as input and produce the desired output directly, need not\ndepend on token-level labels. We propose an E2E model based on pointer\nnetworks, which can be trained directly on pairs of raw input and output text.\nWe evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT\nmovie corpus and compare to neural baselines that do use token-level labels. We\nachieve competitive results, within a few percentage points of the baselines,\nshowing the feasibility of E2E information extraction without the need for\ntoken-level labels. This opens up new possibilities, as for many tasks\ncurrently addressed by human extractors, raw input and output data are\navailable, but not token-level labels.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 16:57:36 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Palm", "Rasmus Berg", ""], ["Hovy", "Dirk", ""], ["Laws", "Florian", ""], ["Winther", "Ole", ""]]}, {"id": "1707.04935", "submitter": "Yuri G. Gordienko", "authors": "Serhii Hamotskyi, Anis Rojbi, Sergii Stirenko, and Yuri Gordienko", "title": "Automatized Generation of Alphabets of Symbols", "comments": "4 pages, 3 figures; Federated Conference on Computer Science and\n  Information Systems, Prague (FedCSIS-2017) (Prague, Czech Republic)", "journal-ref": "Proceedings of the 2017 Federated Conference on Computer Science\n  and Information Systems (FedCSIS-2017), p.639-642, Prague, Czech Republic,\n  September 3-6, 2017", "doi": "10.15439/2017F413", "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the generation of symbols (and alphabets) based on\nspecific user requirements (medium, priorities, type of information that needs\nto be conveyed). A framework for the generation of alphabets is proposed, and\nits use for the generation of a shorthand writing system is explored. We\ndiscuss the possible use of machine learning and genetic algorithms to gather\ninputs for generation of such alphabets and for optimization of already\ngenerated ones. The alphabets generated using such methods may be used in very\ndifferent fields, from the creation of synthetic languages and constructed\nscripts to the creation of sensible commands for multimodal interaction through\nHuman-Computer Interfaces, such as mouse gestures, touchpads, body gestures,\neye-tracking cameras, and brain-computing Interfaces, especially in\napplications for elderly care and people with disabilities.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 19:40:26 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Hamotskyi", "Serhii", ""], ["Rojbi", "Anis", ""], ["Stirenko", "Sergii", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1707.04968", "submitter": "Chao Ma", "authors": "Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den\n  Hengel, Ian Reid", "title": "Visual Question Answering with Memory-Augmented Networks", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit a memory-augmented neural network to predict\naccurate answers to visual questions, even when those answers occur rarely in\nthe training set. The memory network incorporates both internal and external\nmemory blocks and selectively pays attention to each training exemplar. We show\nthat memory-augmented neural networks are able to maintain a relatively\nlong-term memory of scarce training exemplars, which is important for visual\nquestion answering due to the heavy-tailed distribution of answers in a general\nVQA setting. Experimental results on two large-scale benchmark datasets show\nthe favorable performance of the proposed algorithm with a comparison to state\nof the art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 00:42:56 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 09:46:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ma", "Chao", ""], ["Shen", "Chunhua", ""], ["Dick", "Anthony", ""], ["Wu", "Qi", ""], ["Wang", "Peng", ""], ["Hengel", "Anton van den", ""], ["Reid", "Ian", ""]]}, {"id": "1707.05000", "submitter": "Jiangming Liu", "authors": "Jiangming Liu and Yue Zhang", "title": "In-Order Transition-based Constituent Parsing", "comments": "Accepted by TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both bottom-up and top-down strategies have been used for neural\ntransition-based constituent parsing. The parsing strategies differ in terms of\nthe order in which they recognize productions in the derivation tree, where\nbottom-up strategies and top-down strategies take post-order and pre-order\ntraversal over trees, respectively. Bottom-up parsers benefit from rich\nfeatures from readily built partial parses, but lack lookahead guidance in the\nparsing process; top-down parsers benefit from non-local guidance for local\ndecisions, but rely on a strong encoder over the input to predict a constituent\nhierarchy before its construction.To mitigate both issues, we propose a novel\nparsing system based on in-order traversal over syntactic trees, designing a\nset of transition actions to find a compromise between bottom-up constituent\ninformation and top-down lookahead information. Based on stack-LSTM, our\npsycholinguistically motivated constituent parsing system achieves 91.8 F1 on\nWSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised\nreranking and 94.2 F1 with semi-supervised reranking, which are the best\nresults on the WSJ benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 04:27:11 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Liu", "Jiangming", ""], ["Zhang", "Yue", ""]]}, {"id": "1707.05005", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan,\n  Lihui Chen, Yang Liu and Shantanu Jaiswal", "title": "graph2vec: Learning Distributed Representations of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CR cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on representation learning for graph structured data\npredominantly focus on learning distributed representations of graph\nsubstructures such as nodes and subgraphs. However, many graph analytics tasks\nsuch as graph classification and clustering require representing entire graphs\nas fixed length feature vectors. While the aforementioned approaches are\nnaturally unequipped to learn such representations, graph kernels remain as the\nmost effective way of obtaining them. However, these graph kernels use\nhandcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\nhampered by problems such as poor generalization. To address this limitation,\nin this work, we propose a neural embedding framework named graph2vec to learn\ndata-driven distributed representations of arbitrary sized graphs. graph2vec's\nembeddings are learnt in an unsupervised manner and are task agnostic. Hence,\nthey could be used for any downstream task such as graph classification,\nclustering and even seeding supervised representation learning approaches. Our\nexperiments on several benchmark and large real-world datasets show that\ngraph2vec achieves significant improvements in classification and clustering\naccuracies over substructure representation learning approaches and are\ncompetitive with state-of-the-art graph kernels.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 05:09:03 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Chandramohan", "Mahinthan", ""], ["Venkatesan", "Rajasekar", ""], ["Chen", "Lihui", ""], ["Liu", "Yang", ""], ["Jaiswal", "Shantanu", ""]]}, {"id": "1707.05015", "submitter": "Ethan Fast", "authors": "Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, Michael\n  Bernstein", "title": "Iris: A Conversational Agent for Complex Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's conversational agents are restricted to simple standalone commands.\nIn this paper, we present Iris, an agent that draws on human conversational\nstrategies to combine commands, allowing it to perform more complex tasks that\nit has not been explicitly designed to support: for example, composing one\ncommand to \"plot a histogram\" with another to first \"log-transform the data\".\nTo enable this complexity, we introduce a domain specific language that\ntransforms commands into automata that Iris can compose, sequence, and execute\ndynamically by interacting with a user through natural language, as well as a\nconversational type system that manages what kinds of commands can be combined.\nWe have designed Iris to help users with data science tasks, a domain that\nrequires support for command combination. In evaluation, we find that data\nscientists complete a predictive modeling task significantly faster (2.6 times\nspeedup) with Iris than a modern non-conversational programming environment.\nIris supports the same kinds of commands as today's agents, but empowers users\nto weave together these commands to accomplish complex goals.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 06:55:43 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Fast", "Ethan", ""], ["Chen", "Binbin", ""], ["Mendelsohn", "Julia", ""], ["Bassen", "Jonathan", ""], ["Bernstein", "Michael", ""]]}, {"id": "1707.05114", "submitter": "Derek Wong F.", "authors": "Baosong Yang, Derek F. Wong, Tong Xiao, Lidia S. Chao, Jingbo Zhu", "title": "Towards Bidirectional Hierarchical Representations for Attention-Based\n  Neural Machine Translation", "comments": "Accepted for publication at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hierarchical attentional neural translation model which\nfocuses on enhancing source-side hierarchical representations by covering both\nlocal and global semantic information using a bidirectional tree-based encoder.\nTo maximize the predictive likelihood of target words, a weighted variant of an\nattention mechanism is used to balance the attentive information between\nlexical and phrase vectors. Using a tree-based rare word encoding, the proposed\nmodel is extended to sub-word level to alleviate the out-of-vocabulary (OOV)\nproblem. Empirical results reveal that the proposed model significantly\noutperforms sequence-to-sequence attention-based and tree-based neural\ntranslation models in English-Chinese translation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 12:09:08 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Yang", "Baosong", ""], ["Wong", "Derek F.", ""], ["Xiao", "Tong", ""], ["Chao", "Lidia S.", ""], ["Zhu", "Jingbo", ""]]}, {"id": "1707.05115", "submitter": "Anssi Yli-Jyr\\\"a", "authors": "Anssi Yli-Jyr\\\"a", "title": "The Power of Constraint Grammars Revisited", "comments": "9 pages, 4 figures", "journal-ref": "Proceedings of the NoDaLiDa 2017 Workshop on Constraint Grammar -\n  Methods, Tools and Applications, 22 May 2017, Gothenburg, Sweden. Pages 23-31", "doi": null, "report-no": null, "categories": "cs.FL cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Constraint Grammar (SCG) (Karlsson, 1990) and its extensions have\nlacked clear connections to formal language theory. The purpose of this article\nis to lay a foundation for these connections by simplifying the definition of\nstrings processed by the grammar and by showing that Nonmonotonic SCG is\nundecidable and that derivations similar to the Generative Phonology exist. The\ncurrent investigations propose resource bounds that restrict the generative\npower of SCG to a subset of context sensitive languages and present a strong\nfinite-state condition for grammars as wholes. We show that a grammar is\nequivalent to a finite-state transducer if it is implemented with a Turing\nmachine that runs in o(n log n) time. This condition opens new finite-state\nhypotheses and avenues for deeper analysis of SCG instances in the way inspired\nby Finite-State Phonology.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 12:20:15 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Yli-Jyr\u00e4", "Anssi", ""]]}, {"id": "1707.05116", "submitter": "Barbara Plank", "authors": "Rob van der Goot and Barbara Plank and Malvina Nissim", "title": "To Normalize, or Not to Normalize: The Impact of Normalization on\n  Part-of-Speech Tagging", "comments": "In WNUT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does normalization help Part-of-Speech (POS) tagging accuracy on noisy,\nnon-canonical data? To the best of our knowledge, little is known on the actual\nimpact of normalization in a real-world scenario, where gold error detection is\nnot available. We investigate the effect of automatic normalization on POS\ntagging of tweets. We also compare normalization to strategies that leverage\nlarge amounts of unlabeled data kept in its raw form. Our results show that\nnormalization helps, but does not add consistently beyond just word embedding\nlayer initialization. The latter approach yields a tagging model that is\ncompetitive with a Twitter state-of-the-art tagger.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 12:20:53 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["van der Goot", "Rob", ""], ["Plank", "Barbara", ""], ["Nissim", "Malvina", ""]]}, {"id": "1707.05118", "submitter": "Laurent Besacier", "authors": "Alexandre Berard and Olivier Pietquin and Laurent Besacier", "title": "LIG-CRIStAL System for the WMT17 Automatic Post-Editing Task", "comments": "keywords: neural post-edition, attention models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the LIG-CRIStAL submission to the shared Automatic Post-\nEditing task of WMT 2017. We propose two neural post-editing models: a\nmonosource model with a task-specific attention mechanism, which performs\nparticularly well in a low-resource scenario; and a chained architecture which\nmakes use of the source sentence to provide extra context. This latter\narchitecture manages to slightly improve our results when more training data is\navailable. We present and discuss our results on two datasets (en-de and de-en)\nthat are made available for the task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 12:25:52 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Berard", "Alexandre", ""], ["Pietquin", "Olivier", ""], ["Besacier", "Laurent", ""]]}, {"id": "1707.05127", "submitter": "Jie Yang", "authors": "Jie Yang and Yue Zhang and Fei Dong", "title": "Neural Reranking for Named Entity Recognition", "comments": "Accepted as regular paper by RANLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural reranking system for named entity recognition (NER). The\nbasic idea is to leverage recurrent neural network models to learn\nsentence-level patterns that involve named entity mentions. In particular,\ngiven an output sentence produced by a baseline NER model, we replace all\nentity mentions, such as \\textit{Barack Obama}, into their entity types, such\nas \\textit{PER}. The resulting sentence patterns contain direct output\ninformation, yet is less sparse without specific named entities. For example,\n\"PER was born in LOC\" can be such a pattern. LSTM and CNN structures are\nutilised for learning deep representations of such sentences for reranking.\nResults show that our system can significantly improve the NER accuracies over\ntwo different baselines, giving the best reported results on a standard\nbenchmark.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 12:57:53 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Yang", "Jie", ""], ["Zhang", "Yue", ""], ["Dong", "Fei", ""]]}, {"id": "1707.05227", "submitter": "Marek Rei", "authors": "Marek Rei, Helen Yannakoudakis", "title": "Auxiliary Objectives for Neural Error Detection Models", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the utility of different auxiliary objectives and training\nstrategies within a neural sequence labeling approach to error detection in\nlearner writing. Auxiliary costs provide the model with additional linguistic\ninformation, allowing it to learn general-purpose compositional features that\ncan then be exploited for other objectives. Our experiments show that a joint\nlearning approach trained with parallel labels on in-domain data improves\nperformance over the previous best error detection system. While the resulting\nmodel has the same number of parameters, the additional objectives allow it to\nbe optimised more efficiently and achieve better performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:24:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Yannakoudakis", "Helen", ""]]}, {"id": "1707.05233", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Detecting Off-topic Responses to Visual Prompts", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for essay scoring have made great progress in recent years,\nachieving accuracies very close to human annotators. However, a known weakness\nof such automated scorers is not taking into account the semantic relevance of\nthe submitted text. While there is existing work on detecting answer relevance\ngiven a textual prompt, very little previous research has been done to\nincorporate visual writing prompts. We propose a neural architecture and\nseveral extensions for detecting off-topic responses to visual prompts and\nevaluate it on a dataset of texts written by language learners.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:31:20 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1707.05236", "submitter": "Marek Rei", "authors": "Marek Rei, Mariano Felice, Zheng Yuan, Ted Briscoe", "title": "Artificial Error Generation with Machine Translation and Syntactic\n  Patterns", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortage of available training data is holding back progress in the area of\nautomated error detection. This paper investigates two alternative methods for\nartificially generating writing errors, in order to create additional\nresources. We propose treating error generation as a machine translation task,\nwhere grammatically correct text is translated to contain errors. In addition,\nwe explore a system for extracting textual patterns from an annotated corpus,\nwhich can then be used to insert errors into grammatically correct sentences.\nOur experiments show that the inclusion of artificially generated errors\nsignificantly improves error detection accuracy on both FCE and CoNLL 2014\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:38:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Felice", "Mariano", ""], ["Yuan", "Zheng", ""], ["Briscoe", "Ted", ""]]}, {"id": "1707.05246", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Barbara Plank", "title": "Learning to select data for transfer learning with Bayesian Optimization", "comments": "EMNLP 2017. Code available at:\n  https://github.com/sebastianruder/learn-to-select-data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain similarity measures can be used to gauge adaptability and select\nsuitable data for transfer learning, but existing approaches define ad hoc\nmeasures that are deemed suitable for respective tasks. Inspired by work on\ncurriculum learning, we propose to \\emph{learn} data selection measures using\nBayesian Optimization and evaluate them across models, domains and tasks. Our\nlearned measures outperform existing domain similarity measures significantly\non three tasks: sentiment analysis, part-of-speech tagging, and parsing. We\nshow the importance of complementing similarity with diversity, and that\nlearned measures are -- to some degree -- transferable across models, domains,\nand even tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:53:18 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Ruder", "Sebastian", ""], ["Plank", "Barbara", ""]]}, {"id": "1707.05254", "submitter": "Rose Catherine", "authors": "Rose Catherine, Kathryn Mazaitis, Maxine Eskenazi and William Cohen", "title": "Explainable Entity-based Recommendations with Knowledge Graphs", "comments": "Accepted for publication in the 11th ACM Conference on Recommender\n  Systems (RecSys 2017) - Posters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable recommendation is an important task. Many methods have been\nproposed which generate explanations from the content and reviews written for\nitems. When review text is unavailable, generating explanations is still a hard\nproblem. In this paper, we illustrate how explanations can be generated in such\na scenario by leveraging external knowledge in the form of knowledge graphs.\nOur method jointly ranks items and knowledge graph entities using a\nPersonalized PageRank procedure to produce recommendations together with their\nexplanations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 23:18:58 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Catherine", "Rose", ""], ["Mazaitis", "Kathryn", ""], ["Eskenazi", "Maxine", ""], ["Cohen", "William", ""]]}, {"id": "1707.05261", "submitter": "Franziska Horn", "authors": "Franziska Horn, Leila Arras, Gr\\'egoire Montavon, Klaus-Robert\n  M\\\"uller, and Wojciech Samek", "title": "Exploring text datasets by visualizing relevant words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with a new dataset, it is important to first explore and\nfamiliarize oneself with it, before applying any advanced machine learning\nalgorithms. However, to the best of our knowledge, no tools exist that quickly\nand reliably give insight into the contents of a selection of documents with\nrespect to what distinguishes them from other documents belonging to different\ncategories. In this paper we propose to extract `relevant words' from a\ncollection of texts, which summarize the contents of documents belonging to a\ncertain class (or discovered cluster in the case of unlabeled datasets), and\nvisualize them in word clouds to allow for a survey of salient features at a\nglance. We compare three methods for extracting relevant words and demonstrate\nthe usefulness of the resulting word clouds by providing an overview of the\nclasses contained in a dataset of scientific publications as well as by\ndiscovering trending topics from recent New York Times article snippets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 16:12:34 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Horn", "Franziska", ""], ["Arras", "Leila", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1707.05266", "submitter": "Oren Melamud", "authors": "Oren Melamud, Ido Dagan, Jacob Goldberger", "title": "A Simple Language Model based on PMI Matrix Approximations", "comments": "Accepted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a new approach for learning language models by\ntraining them to estimate word-context pointwise mutual information (PMI), and\nthen deriving the desired conditional probabilities from PMI at test time.\nSpecifically, we show that with minor modifications to word2vec's algorithm, we\nget principled language models that are closely related to the well-established\nNoise Contrastive Estimation (NCE) based language models. A compelling aspect\nof our approach is that our models are trained with the same simple negative\nsampling objective function that is commonly used in word2vec to learn word\nembeddings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 16:21:46 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Melamud", "Oren", ""], ["Dagan", "Ido", ""], ["Goldberger", "Jacob", ""]]}, {"id": "1707.05288", "submitter": "Diego Moussallem", "authors": "Diego Moussallem, Ricardo Usbeck, Michael R\\\"oder and Axel-Cyrille\n  Ngonga Ngomo", "title": "MAG: A Multilingual, Knowledge-base Agnostic and Deterministic Entity\n  Linking Approach", "comments": "Accepted in K-CAP 2017: Knowledge Capture Conference", "journal-ref": null, "doi": "10.1145/3148011.3148024", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking has recently been the subject of a significant body of\nresearch. Currently, the best performing approaches rely on trained\nmono-lingual models. Porting these approaches to other languages is\nconsequently a difficult endeavor as it requires corresponding training data\nand retraining of the models. We address this drawback by presenting a novel\nmultilingual, knowledge-based agnostic and deterministic approach to entity\nlinking, dubbed MAG. MAG is based on a combination of context-based retrieval\non structured knowledge bases and graph algorithms. We evaluate MAG on 23 data\nsets and in 7 languages. Our results show that the best approach trained on\nEnglish datasets (PBOH) achieves a micro F-measure that is up to 4 times worse\non datasets in other languages. MAG, on the other hand, achieves\nstate-of-the-art performance on English datasets and reaches a micro F-measure\nthat is up to 0.6 higher than that of PBOH on non-English languages.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 17:14:52 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 16:50:17 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 09:27:39 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Moussallem", "Diego", ""], ["Usbeck", "Ricardo", ""], ["R\u00f6der", "Michael", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1707.05315", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Cheng-Yu Tsai, Chia-Hsiang Liu and Lin-Shan Lee", "title": "Unsupervised Iterative Deep Learning of Speech Features and Acoustic\n  Tokens with Applications to Spoken Term Detection", "comments": "Accepted by IEEE/ACM Transactions on Audio Speech and Language\n  Processing. arXiv admin note: text overlap with arXiv:1602.00426,\n  arXiv:1506.02327", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim to automatically discover high quality frame-level\nspeech features and acoustic tokens directly from unlabeled speech data. A\nMulti-granular Acoustic Tokenizer (MAT) was proposed for automatic discovery of\nmultiple sets of acoustic tokens from the given corpus. Each acoustic token set\nis specified by a set of hyperparameters describing the model configuration.\nThese different sets of acoustic tokens carry different characteristics for the\ngiven corpus and the language behind, thus can be mutually reinforced. The\nmultiple sets of token labels are then used as the targets of a Multi-target\nDeep Neural Network (MDNN) trained on frame-level acoustic features. Bottleneck\nfeatures extracted from the MDNN are then used as the feedback input to the MAT\nand the MDNN itself in the next iteration. The multi-granular acoustic token\nsets and the frame-level speech features can be iteratively optimized in the\niterative deep learning framework. We call this framework the Multi-granular\nAcoustic Tokenizing Deep Neural Network (MATDNN). The results were evaluated\nusing the metrics and corpora defined in the Zero Resource Speech Challenge\norganized at Interspeech 2015, and improved performance was obtained with a set\nof experiments of query-by-example spoken term detection on the same corpora.\nVisualization for the discovered tokens against the English phonemes was also\nshown.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 10:20:15 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Tsai", "Cheng-Yu", ""], ["Liu", "Chia-Hsiang", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1707.05436", "submitter": "Huadong Chen", "authors": "Huadong Chen, Shujian Huang, David Chiang, Jiajun Chen", "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and\n  Decoder", "comments": "Accepted as a long paper by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most neural machine translation (NMT) models are based on the sequential\nencoder-decoder framework, which makes no use of syntactic information. In this\npaper, we improve this model by explicitly incorporating source-side syntactic\ntrees. More specifically, we propose (1) a bidirectional tree encoder which\nlearns both sequential and tree structured representations; (2) a tree-coverage\nmodel that lets the attention depend on the source-side syntax. Experiments on\nChinese-English translation demonstrate that our proposed models outperform the\nsequential attentional model as well as a stronger baseline with a bottom-up\ntree encoder and word coverage.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 01:53:58 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Chen", "Huadong", ""], ["Huang", "Shujian", ""], ["Chiang", "David", ""], ["Chen", "Jiajun", ""]]}, {"id": "1707.05438", "submitter": "Huadong Chen", "authors": "Huadong Chen, Shujian Huang, David Chiang, Xinyu Dai, Jiajun Chen", "title": "Top-Rank Enhanced Listwise Optimization for Statistical Machine\n  Translation", "comments": "Accepted to CONLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise ranking methods are the basis of many widely used discriminative\ntraining approaches for structure prediction problems in natural language\nprocessing(NLP). Decomposing the problem of ranking hypotheses into pairwise\ncomparisons enables simple and efficient solutions. However, neglecting the\nglobal ordering of the hypothesis list may hinder learning. We propose a\nlistwise learning framework for structure prediction problems such as machine\ntranslation. Our framework directly models the entire translation list's\nordering to learn parameters which may better fit the given listwise samples.\nFurthermore, we propose top-rank enhanced loss functions, which are more\nsensitive to ranking errors at higher positions. Experiments on a large-scale\nChinese-English translation task show that both our listwise learning framework\nand top-rank enhanced listwise losses lead to significant improvements in\ntranslation quality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 02:04:37 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Chen", "Huadong", ""], ["Huang", "Shujian", ""], ["Chiang", "David", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1707.05468", "submitter": "Elena Mikhalkova", "authors": "Elena Mikhalkova and Yuri Karyakin", "title": "Detecting Intentional Lexical Ambiguity in English Puns", "comments": "In Proceedings of the International Conference \"Dialogue 2017\"\n  Moscow, May 31-June 3, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes a model of automatic analysis of puns, where a word is\nintentionally used in two meanings at the same time (the target word). We\nemploy Roget's Thesaurus to discover two groups of words which, in a pun, form\naround two abstract bits of meaning (semes). They become a semantic vector,\nbased on which an SVM classifier learns to recognize puns, reaching a score\n0.73 for F-measure. We apply several rule-based methods to locate intentionally\nambiguous (target) words, based on structural and semantic criteria. It appears\nthat the structural criterion is more effective, although it possibly\ncharacterizes only the tested dataset. The results we get correlate with the\nresults of other teams at SemEval-2017 competition (Task 7 Detection and\nInterpretation of English Puns) considering effects of using supervised\nlearning models and word statistics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:04:03 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Mikhalkova", "Elena", ""], ["Karyakin", "Yuri", ""]]}, {"id": "1707.05479", "submitter": "Elena Mikhalkova", "authors": "Elena Mikhalkova and Yuri Karyakin", "title": "PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in\n  Automatic Pun Recognition and Interpretation", "comments": "Proceedings of the 11th International Workshop on Semantic Evaluation\n  (SemEval-2017) Task 7: Detection and Interpretation of English Puns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes a model of automatic interpretation of English puns,\nbased on Roget's Thesaurus, and its implementation, PunFields. In a pun, the\nalgorithm discovers two groups of words that belong to two main semantic\nfields. The fields become a semantic vector based on which an SVM classifier\nlearns to recognize puns. A rule-based model is then applied for recognition of\nintentionally ambiguous (target) words and their definitions. In SemEval Task 7\nPunFields shows a considerably good result in pun classification, but requires\nimprovement in searching for the target word and its definition.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:40:18 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Mikhalkova", "Elena", ""], ["Karyakin", "Yuri", ""]]}, {"id": "1707.05481", "submitter": "Elena Mikhalkova", "authors": "Elena Mikhalkova and Nadezhda Ganzherli and Yuri Karyakin", "title": "A Comparative Analysis of Social Network Pages by Interests of Their\n  Followers", "comments": "11 pages, submitted for reviewing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being a matter of cognition, user interests should be apt to classification\nindependent of the language of users, social network and content of interest\nitself. To prove it, we analyze a collection of English and Russian Twitter and\nVkontakte community pages by interests of their followers. First, we create a\nmodel of Major Interests (MaIs) with the help of expert analysis and then\nclassify a set of pages using machine learning algorithms (SVM, Neural Network,\nNaive Bayes, and some other). We take three interest domains that are typical\nof both English and Russian-speaking communities: football, rock music,\nvegetarianism. The results of classification show a greater correlation between\nRussian-Vkontakte and Russian-Twitter pages while English-Twitterpages appear\nto provide the highest score.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:49:30 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 13:57:24 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mikhalkova", "Elena", ""], ["Ganzherli", "Nadezhda", ""], ["Karyakin", "Yuri", ""]]}, {"id": "1707.05501", "submitter": "Anirban Laha", "authors": "Parag Jain, Priyanka Agrawal, Abhijit Mishra, Mohak Sukhwani, Anirban\n  Laha, Karthik Sankaranarayanan", "title": "Story Generation from Sequence of Independent Short Descriptions", "comments": "Accepted in SIGKDD Workshop on Machine Learning for Creativity\n  (ML4Creativity), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Natural Language Generation (NLG) systems are weak AI systems and\nexhibit limited capabilities when language generation tasks demand higher\nlevels of creativity, originality and brevity. Effective solutions or, at least\nevaluations of modern NLG paradigms for such creative tasks have been elusive,\nunfortunately. This paper introduces and addresses the task of coherent story\ngeneration from independent descriptions, describing a scene or an event.\nTowards this, we explore along two popular text-generation paradigms -- (1)\nStatistical Machine Translation (SMT), posing story generation as a translation\nproblem and (2) Deep Learning, posing story generation as a sequence to\nsequence learning problem. In SMT, we chose two popular methods such as phrase\nbased SMT (PB-SMT) and syntax based SMT (SYNTAX-SMT) to `translate' the\nincoherent input text into stories. We then implement a deep recurrent neural\nnetwork (RNN) architecture that encodes sequence of variable length input\ndescriptions to corresponding latent representations and decodes them to\nproduce well formed comprehensive story like summaries. The efficacy of the\nsuggested approaches is demonstrated on a publicly available dataset with the\nhelp of popular machine translation and summarization evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 07:08:31 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 13:25:21 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Jain", "Parag", ""], ["Agrawal", "Priyanka", ""], ["Mishra", "Abhijit", ""], ["Sukhwani", "Mohak", ""], ["Laha", "Anirban", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "1707.05589", "submitter": "G\\'abor Melis", "authors": "G\\'abor Melis, Chris Dyer, Phil Blunsom", "title": "On the State of the Art of Evaluation in Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 12:35:53 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 17:57:58 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Melis", "G\u00e1bor", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1707.05612", "submitter": "Fartash Faghri", "authors": "Fartash Faghri, David J. Fleet, Jamie Ryan Kiros and Sanja Fidler", "title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives", "comments": "Accepted as spotlight presentation at British Machine Vision\n  Conference (BMVC) 2018. Code: https://github.com/fartashf/vsepp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by hard negative mining, the use of hard\nnegatives in structured prediction, and ranking loss functions, we introduce a\nsimple change to common loss functions used for multi-modal embeddings. That,\ncombined with fine-tuning and use of augmented data, yields significant gains\nin retrieval performance. We showcase our approach, VSE++, on MS-COCO and\nFlickr30K datasets, using ablation studies and comparisons with existing\nmethods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%\nin caption retrieval and 11.3% in image retrieval (at R@1).\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 13:51:32 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:55:21 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 20:42:43 GMT"}, {"version": "v4", "created": "Sun, 29 Jul 2018 19:11:57 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Faghri", "Fartash", ""], ["Fleet", "David J.", ""], ["Kiros", "Jamie Ryan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1707.05635", "submitter": "Ruqing Zhang", "authors": "Ruqing Zhang, Jiafeng Guo, Yanyan Lan, Jun Xu, Xueqi Cheng", "title": "Spherical Paragraph Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing texts as fixed-length vectors is central to many language\nprocessing tasks. Most traditional methods build text representations based on\nthe simple Bag-of-Words (BoW) representation, which loses the rich semantic\nrelations between words. Recent advances in natural language processing have\nshown that semantically meaningful representations of words can be efficiently\nacquired by distributed models, making it possible to build text\nrepresentations based on a better foundation called the Bag-of-Word-Embedding\n(BoWE) representation. However, existing text representation methods using BoWE\noften lack sound probabilistic foundations or cannot well capture the semantic\nrelatedness encoded in word vectors. To address these problems, we introduce\nthe Spherical Paragraph Model (SPM), a probabilistic generative model based on\nBoWE, for text representation. SPM has good probabilistic interpretability and\ncan fully leverage the rich semantics of words, the word co-occurrence\ninformation as well as the corpus-wide information to help the representation\nlearning of texts. Experimental results on topical classification and sentiment\nanalysis demonstrate that SPM can achieve new state-of-the-art performances on\nseveral benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 14:19:50 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Zhang", "Ruqing", ""], ["Guo", "Jiafeng", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1707.05720", "submitter": "Mohit Shridhar", "authors": "Mohit Shridhar, David Hsu", "title": "Grounding Spatio-Semantic Referring Expressions for Human-Robot\n  Interaction", "comments": "8 pages, 4 figures, Accepted at RSS 2017 Workshop on Spatial-Semantic\n  Representations in Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human language is one of the most natural interfaces for humans to\ninteract with robots. This paper presents a robot system that retrieves\neveryday objects with unconstrained natural language descriptions. A core issue\nfor the system is semantic and spatial grounding, which is to infer objects and\ntheir spatial relationships from images and natural language expressions. We\nintroduce a two-stage neural-network grounding pipeline that maps natural\nlanguage referring expressions directly to objects in the images. The first\nstage uses visual descriptions in the referring expressions to generate a\ncandidate set of relevant objects. The second stage examines all pairwise\nrelationships between the candidates and predicts the most likely referred\nobject according to the spatial descriptions in the referring expressions. A\nkey feature of our system is that by leveraging a large dataset of images\nlabeled with text descriptions, it allows unrestricted object types and natural\nlanguage referring expressions. Preliminary results indicate that our system\noutperforms a near state-of-the-art object comprehension system on standard\nbenchmark datasets. We also present a robot system that follows voice commands\nto pick and place previously unseen objects.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:02:05 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Shridhar", "Mohit", ""], ["Hsu", "David", ""]]}, {"id": "1707.05850", "submitter": "Elham Shahab", "authors": "Elham Shahab", "title": "A Short Survey of Biomedical Relation Extraction Techniques", "comments": "updated keywords and reference format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical information is growing rapidly in the recent years and retrieving\nuseful data through information extraction system is getting more attention. In\nthe current research, we focus on different aspects of relation extraction\ntechniques in biomedical domain and briefly describe the state-of-the-art for\nrelation extraction between a variety of biological elements.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 20:38:42 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 16:16:09 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 21:00:14 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Shahab", "Elham", ""]]}, {"id": "1707.05853", "submitter": "Glorianna Jagfeld", "authors": "Glorianna Jagfeld and Ngoc Thang Vu", "title": "Encoding Word Confusion Networks with Recurrent Neural Networks for\n  Dialog State Tracking", "comments": "Speech-Centric Natural Language Processing Workshop @EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our novel method to encode word confusion networks, which\ncan represent a rich hypothesis space of automatic speech recognition systems,\nvia recurrent neural networks. We demonstrate the utility of our approach for\nthe task of dialog state tracking in spoken dialog systems that relies on\nautomatic speech recognition output. Encoding confusion networks outperforms\nencoding the best hypothesis of the automatic speech recognition in a neural\nsystem for dialog state tracking on the well-known second Dialog State Tracking\nChallenge dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 20:47:06 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 09:58:43 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Jagfeld", "Glorianna", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1707.05928", "submitter": "Yanyao Shen", "authors": "Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, Animashree\n  Anandkumar", "title": "Deep Active Learning for Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has yielded state-of-the-art performance on many natural\nlanguage processing tasks including named entity recognition (NER). However,\nthis typically requires large amounts of labeled data. In this work, we\ndemonstrate that the amount of labeled training data can be drastically reduced\nwhen deep learning is combined with active learning. While active learning is\nsample-efficient, it can be computationally expensive since it requires\niterative retraining. To speed this up, we introduce a lightweight architecture\nfor NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and\nword encoders and a long short term memory (LSTM) tag decoder. The model\nachieves nearly state-of-the-art performance on standard datasets for the task\nwhile being computationally much more efficient than best performing models. We\ncarry out incremental active learning, during the training process, and are\nable to nearly match state-of-the-art performance with just 25\\% of the\noriginal training data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:18:40 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 08:43:46 GMT"}, {"version": "v3", "created": "Sun, 4 Feb 2018 03:04:57 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Shen", "Yanyao", ""], ["Yun", "Hyokun", ""], ["Lipton", "Zachary C.", ""], ["Kronrod", "Yakov", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1707.05967", "submitter": "Enrico Santus", "authors": "Enrico Santus, Emmanuele Chersoni, Alessandro Lenci, Philippe Blache", "title": "Measuring Thematic Fit with Distributional Feature Overlap", "comments": "9 pages, 2 figures, 5 tables, EMNLP, 2017, thematic fit, selectional\n  preference, semantic role, DSMs, Distributional Semantic Models, Vector Space\n  Models, VSMs, cosine, APSyn, similarity, prototype", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new distributional method for modeling\npredicate-argument thematic fit judgments. We use a syntax-based DSM to build a\nprototypical representation of verb-specific roles: for every verb, we extract\nthe most salient second order contexts for each of its roles (i.e. the most\nsalient dimensions of typical role fillers), and then we compute thematic fit\nas a weighted overlap between the top features of candidate fillers and role\nprototypes. Our experiments show that our method consistently outperforms a\nbaseline re-implementing a state-of-the-art system, and achieves better or\ncomparable results to those reported in the literature for the other\nunsupervised systems. Moreover, it provides an explicit representation of the\nfeatures characterizing verb-specific semantic roles.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 07:51:05 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 17:22:54 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Santus", "Enrico", ""], ["Chersoni", "Emmanuele", ""], ["Lenci", "Alessandro", ""], ["Blache", "Philippe", ""]]}, {"id": "1707.06002", "submitter": "Ivan Habernal", "authors": "Ivan Habernal, Raffael Hannemann, Christian Pollak, Christopher Klamm,\n  Patrick Pauli and Iryna Gurevych", "title": "Argotario: Computational Argumentation Meets Serious Games", "comments": "EMNLP 2017 demo paper. Source codes:\n  https://github.com/UKPLab/argotario", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important skill in critical thinking and argumentation is the ability to\nspot and recognize fallacies. Fallacious arguments, omnipresent in\nargumentative discourse, can be deceptive, manipulative, or simply leading to\n`wrong moves' in a discussion. Despite their importance, argumentation scholars\nand NLP researchers with focus on argumentation quality have not yet\ninvestigated fallacies empirically. The nonexistence of resources dealing with\nfallacious argumentation calls for scalable approaches to data acquisition and\nannotation, for which the serious games methodology offers an appealing, yet\nunexplored, alternative. We present Argotario, a serious game that deals with\nfallacies in everyday argumentation. Argotario is a multilingual, open-source,\nplatform-independent application with strong educational aspects, accessible at\nwww.argotario.net.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:08:51 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Habernal", "Ivan", ""], ["Hannemann", "Raffael", ""], ["Pollak", "Christian", ""], ["Klamm", "Christopher", ""], ["Pauli", "Patrick", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1707.06012", "submitter": "Ale\\v{s} Tamchyna", "authors": "Ale\\v{s} Tamchyna, Marion Weller-Di Marco, Alexander Fraser", "title": "Modeling Target-Side Inflection in Neural Machine Translation", "comments": "Accepted as a research paper at WMT17. (Updated version with\n  corrected references.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NMT systems have problems with large vocabulary sizes. Byte-pair encoding\n(BPE) is a popular approach to solving this problem, but while BPE allows the\nsystem to generate any target-side word, it does not enable effective\ngeneralization over the rich vocabulary in morphologically rich languages with\nstrong inflectional phenomena. We introduce a simple approach to overcome this\nproblem by training a system to produce the lemma of a word and its\nmorphologically rich POS tag, which is then followed by a deterministic\ngeneration step. We apply this strategy for English-Czech and English-German\ntranslation scenarios, obtaining improvements in both settings. We furthermore\nshow that the improvement is not due to only adding explicit morphological\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:47:22 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 09:29:10 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Tamchyna", "Ale\u0161", ""], ["Marco", "Marion Weller-Di", ""], ["Fraser", "Alexander", ""]]}, {"id": "1707.06065", "submitter": "Taesup Kim", "authors": "Taesup Kim, Inchul Song, Yoshua Bengio", "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition", "comments": "INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:04:09 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Kim", "Taesup", ""], ["Song", "Inchul", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1707.06100", "submitter": "Franziska Horn", "authors": "Franziska Horn, Leila Arras, Gr\\'egoire Montavon, Klaus-Robert\n  M\\\"uller, and Wojciech Samek", "title": "Discovering topics in text datasets by visualizing relevant words", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.05261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with large collections of documents, it is imperative to quickly\nget an overview of the texts' contents. In this paper we show how this can be\nachieved by using a clustering algorithm to identify topics in the dataset and\nthen selecting and visualizing relevant words, which distinguish a group of\ndocuments from the rest of the texts, to summarize the contents of the\ndocuments belonging to each topic. We demonstrate our approach by discovering\ntrending topics in a collection of New York Times article snippets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:46:47 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Horn", "Franziska", ""], ["Arras", "Leila", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1707.06130", "submitter": "Fr\\'ederic Godin", "authors": "Fr\\'ederic Godin, Joni Dambre and Wesley De Neve", "title": "Improving Language Modeling using Densely Connected Recurrent Neural\n  Networks", "comments": "Accepted at Workshop on Representation Learning, ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the novel concept of densely connected layers\ninto recurrent neural networks. We evaluate our proposed architecture on the\nPenn Treebank language modeling task. We show that we can obtain similar\nperplexity scores with six times fewer parameters compared to a standard\nstacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In\ncontrast with the current usage of skip connections, we show that densely\nconnecting only a few stacked layers with skip connections already yields\nsignificant perplexity reductions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 14:49:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Godin", "Fr\u00e9deric", ""], ["Dambre", "Joni", ""], ["De Neve", "Wesley", ""]]}, {"id": "1707.06151", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Samarth Agrawal, Pushpak Bhattacharyya, Mark Carman", "title": "Expect the unexpected: Harnessing Sentence Completion for Sarcasm\n  Detection", "comments": "6 pages, The paper will be presented at PACLING 2017 as an oral\n  presentation. A note saying so has been added to the beginning of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trigram `I love being' is expected to be followed by positive words such\nas `happy'. In a sarcastic sentence, however, the word `ignored' may be\nobserved. The expected and the observed words are, thus, incongruous. We model\nsarcasm detection as the task of detecting incongruity between an observed and\nan expected word. In order to obtain the expected word, we use Context2Vec, a\nsentence completion library based on Bidirectional LSTM. However, since the\nexact word where such an incongruity occurs may not be known in advance, we\npresent two approaches: an All-words approach (which consults sentence\ncompletion for every content word) and an Incongruous words-only approach\n(which consults sentence completion for the 50% most incongruous content\nwords). The approaches outperform reported values for tweets but not for\ndiscussion forum posts. This is likely to be because of redundant consultation\nof sentence completion for discussion forum posts. Therefore, we consider an\noracle case where the exact incongruous word is manually labeled in a corpus\nreported in past work. In this case, the performance is higher than the\nall-words approach. This sets up the promise for using sentence completion for\nsarcasm detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:24:20 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Joshi", "Aditya", ""], ["Agrawal", "Samarth", ""], ["Bhattacharyya", "Pushpak", ""], ["Carman", "Mark", ""]]}, {"id": "1707.06163", "submitter": "Georgi Dzhambazov", "authors": "Georgi Dzhambazov, Andre Holzapfel, Ajay Srinivasamurthy, Xavier Serra", "title": "Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio", "comments": "International Society for Music Information Retrieval Conferece\n  (ISMIR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of this study is the automatic detection of onsets of the singing\nvoice in polyphonic audio recordings. Starting with a hypothesis that the\nknowledge of the current position in a metrical cycle (i.e. metrical accent)\ncan improve the accuracy of vocal note onset detection, we propose a novel\nprobabilistic model to jointly track beats and vocal note onsets. The proposed\nmodel extends a state of the art model for beat and meter tracking, in which\na-priori probability of a note at a specific metrical accent interacts with the\nprobability of observing a vocal note onset. We carry out an evaluation on a\nvaried collection of multi-instrument datasets from two music traditions\n(English popular music and Turkish makam) with different types of metrical\ncycles and singing styles. Results confirm that the proposed model reasonably\nimproves vocal note onset detection accuracy compared to a baseline model that\ndoes not take metrical position into account.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:39:09 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Dzhambazov", "Georgi", ""], ["Holzapfel", "Andre", ""], ["Srinivasamurthy", "Ajay", ""], ["Serra", "Xavier", ""]]}, {"id": "1707.06167", "submitter": "Eleftherios Avramidis", "authors": "Eleftherios Avramidis", "title": "Sentence-level quality estimation by predicting HTER as a\n  multi-component metric", "comments": "Preview for the Quality Estimation Shared Task Description Paper for\n  the 2nd Conference of Machine Translation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This submission investigates alternative machine learning models for\npredicting the HTER score on the sentence level. Instead of directly predicting\nthe HTER score, we suggest a model that jointly predicts the amount of the 4\ndistinct post-editing operations, which are then used to calculate the HTER\nscore. This also gives the possibility to correct invalid (e.g. negative)\npredicted values prior to the calculation of the HTER score. Without any\nfeature exploration, a multi-layer perceptron with 4 outputs yields small but\nsignificant improvements over the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:48:27 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Avramidis", "Eleftherios", ""]]}, {"id": "1707.06195", "submitter": "Natalia Tomashenko", "authors": "Yuri Khokhlov, Natalia Tomashenko, Ivan Medennikov, Alexei Romanenko", "title": "Fast and Accurate OOV Decoder on High-Level Features", "comments": "Interspeech 2017, August 2017, Stockholm, Sweden. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel approach to out-of-vocabulary (OOV) keyword search\n(KWS) task. The proposed approach is based on using high-level features from an\nautomatic speech recognition (ASR) system, so called phoneme posterior based\n(PPB) features, for decoding. These features are obtained by calculating\ntime-dependent phoneme posterior probabilities from word lattices, followed by\ntheir smoothing. For the PPB features we developed a special novel very fast,\nsimple and efficient OOV decoder. Experimental results are presented on the\nGeorgian language from the IARPA Babel Program, which was the test language in\nthe OpenKWS 2016 evaluation campaign. The results show that in terms of maximum\nterm weighted value (MTWV) metric and computational speed, for single ASR\nsystems, the proposed approach significantly outperforms the state-of-the-art\napproach based on using in-vocabulary proxies for OOV keywords in the indexed\ndatabase. The comparison of the two OOV KWS approaches on the fusion results of\nthe nine different ASR systems demonstrates that the proposed OOV decoder\noutperforms the proxy-based approach in terms of MTWV metric given the\ncomparable processing speed. Other important advantages of the OOV decoder\ninclude extremely low memory consumption and simplicity of its implementation\nand parameter optimization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:03:34 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Khokhlov", "Yuri", ""], ["Tomashenko", "Natalia", ""], ["Medennikov", "Ivan", ""], ["Romanenko", "Alexei", ""]]}, {"id": "1707.06209", "submitter": "Johannes Welbl", "authors": "Johannes Welbl, Nelson F. Liu, Matt Gardner", "title": "Crowdsourcing Multiple Choice Science Questions", "comments": "accepted for the Workshop on Noisy User-generated Text (W-NUT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for obtaining high-quality, domain-targeted\nmultiple choice questions from crowd workers. Generating these questions can be\ndifficult without trading away originality, relevance or diversity in the\nanswer options. Our method addresses these problems by leveraging a large\ncorpus of domain-specific text and a small set of existing questions. It\nproduces model suggestions for document selection and answer distractor choice\nwhich aid the human question generation process. With this method we have\nassembled SciQ, a dataset of 13.7K multiple choice science exam questions\n(Dataset available at http://allenai.org/data.html). We demonstrate that the\nmethod produces in-domain questions by providing an analysis of this new\ndataset and by showing that humans cannot distinguish the crowdsourced\nquestions from original questions. When using SciQ as additional training data\nto existing questions, we observe accuracy improvements on real science exams.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:28:46 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Welbl", "Johannes", ""], ["Liu", "Nelson F.", ""], ["Gardner", "Matt", ""]]}, {"id": "1707.06226", "submitter": "Debanjan Ghosh", "authors": "Debanjan Ghosh, Alexander Richard Fabbri, Smaranda Muresan", "title": "The Role of Conversation Context for Sarcasm Detection in Online\n  Interactions", "comments": "SIGDial 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models for sarcasm detection have often relied on the content\nof utterances in isolation. However, speaker's sarcastic intent is not always\nobvious without additional context. Focusing on social media discussions, we\ninvestigate two issues: (1) does modeling of conversation context help in\nsarcasm detection and (2) can we understand what part of conversation context\ntriggered the sarcastic reply. To address the first issue, we investigate\nseveral types of Long Short-Term Memory (LSTM) networks that can model both the\nconversation context and the sarcastic response. We show that the conditional\nLSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level\nattention on context and response outperform the LSTM model that reads only the\nresponse. To address the second issue, we present a qualitative analysis of\nattention weights produced by the LSTM models with attention and discuss the\nresults compared with human performance on the task.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 01:21:26 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ghosh", "Debanjan", ""], ["Fabbri", "Alexander Richard", ""], ["Muresan", "Smaranda", ""]]}, {"id": "1707.06265", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, James Glass", "title": "Unsupervised Domain Adaptation for Robust Speech Recognition via\n  Variational Autoencoder-Based Data Augmentation", "comments": "Accepted to IEEE ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain mismatch between training and testing can lead to significant\ndegradation in performance in many machine learning scenarios. Unfortunately,\nthis is not a rare situation for automatic speech recognition deployments in\nreal-world applications. Research on robust speech recognition can be regarded\nas trying to overcome this domain mismatch issue. In this paper, we address the\nunsupervised domain adaptation problem for robust speech recognition, where\nboth source and target domain speech are presented, but word transcripts are\nonly available for the source domain speech. We present novel\naugmentation-based methods that transform speech in a way that does not change\nthe transcripts. Specifically, we first train a variational autoencoder on both\nsource and target domain data (without supervision) to learn a latent\nrepresentation of speech. We then transform nuisance attributes of speech that\nare irrelevant to recognition by modifying the latent representations, in order\nto augment labeled training data with additional data whose distribution is\nmore similar to the target domain. The proposed method is evaluated on the\nCHiME-4 dataset and reduces the absolute word error rate (WER) by as much as\n35% compared to the non-adapted baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 19:10:44 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 16:31:05 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1707.06299", "submitter": "Stefan Ultes", "authors": "Stefan Ultes, Pawe{\\l} Budzianowski, I\\~nigo Casanueva, Nikola\n  Mrk\\v{s}i\\'c, Lina Rojas-Barahona, Pei-Hao Su, Tsung-Hsien Wen, Milica\n  Ga\\v{s}i\\'c and Steve Young", "title": "Reward-Balancing for Statistical Spoken Dialogue Systems using\n  Multi-objective Reinforcement Learning", "comments": "Accepted at SIGDial 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is widely used for dialogue policy optimization where\nthe reward function often consists of more than one component, e.g., the\ndialogue success and the dialogue length. In this work, we propose a structured\nmethod for finding a good balance between these components by searching for the\noptimal reward component weighting. To render this search feasible, we use\nmulti-objective reinforcement learning to significantly reduce the number of\ntraining dialogues required. We apply our proposed method to find optimized\ncomponent weights for six domains and compare them to a default baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 21:21:03 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ultes", "Stefan", ""], ["Budzianowski", "Pawe\u0142", ""], ["Casanueva", "I\u00f1igo", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Rojas-Barahona", "Lina", ""], ["Su", "Pei-Hao", ""], ["Wen", "Tsung-Hsien", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Young", "Steve", ""]]}, {"id": "1707.06320", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Alexis Conneau, Allan Jabri and Maximilian Nickel", "title": "Learning Visually Grounded Sentence Representations", "comments": "Published at NAACL-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variety of models, trained on a supervised image captioning\ncorpus to predict the image features for a given caption, to perform sentence\nrepresentation grounding. We train a grounded sentence encoder that achieves\ngood performance on COCO caption and image retrieval and subsequently show that\nthis encoder can successfully be transferred to various NLP tasks, with\nimproved performance over text-only models. Lastly, we analyze the contribution\nof grounding, and show that word embeddings learned by this system outperform\nnon-grounded ones.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 23:12:57 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 20:19:28 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kiela", "Douwe", ""], ["Conneau", "Alexis", ""], ["Jabri", "Allan", ""], ["Nickel", "Maximilian", ""]]}, {"id": "1707.06341", "submitter": "Karl Stratos", "authors": "Karl Stratos", "title": "A Sub-Character Architecture for Korean Language Processing", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel sub-character architecture that exploits a unique\ncompositional structure of the Korean language. Our method decomposes each\ncharacter into a small set of primitive phonetic units called jamo letters from\nwhich character- and word-level representations are induced. The jamo letters\ndivulge syntactic and semantic information that is difficult to access with\nconventional character-level units. They greatly alleviate the data sparsity\nproblem, reducing the observation space to 1.6% of the original while\nincreasing accuracy in our experiments. We apply our architecture to dependency\nparsing and achieve dramatic improvement over strong lexical baselines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 02:09:23 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 16:00:50 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Stratos", "Karl", ""]]}, {"id": "1707.06355", "submitter": "Yunan Ye", "authors": "Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao and Yueting Zhuang", "title": "Video Question Answering via Attribute-Augmented Attention Network\n  Learning", "comments": "Accepted for SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080655", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering is a challenging problem in visual information\nretrieval, which provides the answer to the referenced video content according\nto the question. However, the existing visual question answering approaches\nmainly tackle the problem of static image question, which may be ineffectively\nfor video question answering due to the insufficiency of modeling the temporal\ndynamics of video contents. In this paper, we study the problem of video\nquestion answering by modeling its temporal dynamics with frame-level attention\nmechanism. We propose the attribute-augmented attention network learning\nframework that enables the joint frame-level attribute detection and unified\nvideo representation learning for video question answering. We then incorporate\nthe multi-step reasoning process for our proposed attention network to further\nimprove the performance. We construct a large-scale video question answering\ndataset. We conduct the experiments on both multiple-choice and open-ended\nvideo question answering tasks to show the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:12:29 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ye", "Yunan", ""], ["Zhao", "Zhou", ""], ["Li", "Yimeng", ""], ["Chen", "Long", ""], ["Xiao", "Jun", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1707.06357", "submitter": "Majid Laali", "authors": "Majid Laali and Leila Kosseim", "title": "Improving Discourse Relation Projection to Build Discourse Annotated\n  Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The naive approach to annotation projection is not effective to project\ndiscourse annotations from one language to another because implicit discourse\nrelations are often changed to explicit ones and vice-versa in the translation.\nIn this paper, we propose a novel approach based on the intersection between\nstatistical word-alignment models to identify unsupported discourse\nannotations. This approach identified 65% of the unsupported annotations in the\nEnglish-French parallel sentences from Europarl. By filtering out these\nunsupported annotations, we induced the first PDTB-style discourse annotated\ncorpus for French from Europarl. We then used this corpus to train a classifier\nto identify the discourse-usage of French discourse connectives and show a 15%\nimprovement of F1-score compared to the classifier trained on the non-filtered\nannotations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:17:19 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Laali", "Majid", ""], ["Kosseim", "Leila", ""]]}, {"id": "1707.06378", "submitter": "Todor Mihaylov", "authors": "Todor Mihaylov, Daniel Belchev, Yasen Kiprov, Ivan Koychev, Preslav\n  Nakov", "title": "Large-Scale Goodness Polarity Lexicons for Community Question Answering", "comments": "SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan; Community\n  Question Answering; Goodness polarity lexicons; Sentiment Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We transfer a key idea from the field of sentiment analysis to a new domain:\ncommunity question answering (cQA). The cQA task we are interested in is the\nfollowing: given a question and a thread of comments, we want to re-rank the\ncomments so that the ones that are good answers to the question would be ranked\nhigher than the bad ones. We notice that good vs. bad comments use specific\nvocabulary and that one can often predict the goodness/badness of a comment\neven ignoring the question, based on the comment contents only. This leads us\nto the idea to build a good/bad polarity lexicon as an analogy to the\npositive/negative sentiment polarity lexicons, commonly used in sentiment\nanalysis. In particular, we use pointwise mutual information in order to build\nlarge-scale goodness polarity lexicons in a semi-supervised manner starting\nwith a small number of initial seeds. The evaluation results show an\nimprovement of 0.7 MAP points absolute over a very strong baseline and\nstate-of-the art performance on SemEval-2016 Task 3.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 05:40:37 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Mihaylov", "Todor", ""], ["Belchev", "Daniel", ""], ["Kiprov", "Yasen", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "1707.06456", "submitter": "Nafise Sadat Moosavi", "authors": "Benjamin Heinzerling, Nafise Sadat Moosavi and Michael Strube", "title": "Revisiting Selectional Preferences for Coreference Resolution", "comments": "EMNLP 2017 - short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectional preferences have long been claimed to be essential for\ncoreference resolution. However, they are mainly modeled only implicitly by\ncurrent coreference resolvers. We propose a dependency-based embedding model of\nselectional preferences which allows fine-grained compatibility judgments with\nhigh coverage. We show that the incorporation of our model improves coreference\nresolution performance on the CoNLL dataset, matching the state-of-the-art\nresults of a more complex system. However, it comes with a cost that makes it\ndebatable how worthwhile such improvements are.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 11:54:37 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Heinzerling", "Benjamin", ""], ["Moosavi", "Nafise Sadat", ""], ["Strube", "Michael", ""]]}, {"id": "1707.06480", "submitter": "Zhenisbek Assylbekov", "authors": "Zhenisbek Assylbekov, Rustem Takhanov, Bagdat Myrzakhmetov and\n  Jonathan N. Washington", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware\n  Ones", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Syllabification does not seem to improve word-level RNN language modeling\nquality when compared to character-based segmentation. However, our best\nsyllable-aware language model, achieving performance comparable to the\ncompetitive character-aware model, has 18%-33% fewer parameters and is trained\n1.2-2.2 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:46:09 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Assylbekov", "Zhenisbek", ""], ["Takhanov", "Rustem", ""], ["Myrzakhmetov", "Bagdat", ""], ["Washington", "Jonathan N.", ""]]}, {"id": "1707.06519", "submitter": "Chia-Hao Shen", "authors": "Chia-Hao Shen, Janet Y. Sung, Hung-Yi Lee", "title": "Language Transfer of Audio Word2Vec: Learning Audio Segment\n  Representations without Target Language Data", "comments": "arXiv admin note: text overlap with arXiv:1603.00982", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Word2Vec offers vector representations of fixed dimensionality for\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with real world applications\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\ncapability of language transfer of Audio Word2Vec. We train SA from one\nlanguage (source language) and use it to extract the vector representation of\nthe audio segments of another language (target language). We found that SA can\nstill catch phonetic structure from the audio segments of the target language\nif the source and target languages are similar. In query-by-example STD, we\nobtain the vector representations from the SA learned from a large amount of\nsource language data, and found them surpass the representations from naive\nencoder and SA directly learned from a small amount of target language data.\nThe result shows that it is possible to learn Audio Word2Vec model from\nhigh-resource languages and use it on low-resource languages. This further\nexpands the usability of Audio Word2Vec.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:54:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shen", "Chia-Hao", ""], ["Sung", "Janet Y.", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1707.06527", "submitter": "Yanmin Qian", "authors": "Yanmin Qian, Xuankai Chang and Dong Yu", "title": "Single-Channel Multi-talker Speech Recognition with Permutation\n  Invariant Training", "comments": "11 pages, 6 figures, Submitted to IEEE/ACM Transactions on Audio,\n  Speech and Language Processing. arXiv admin note: text overlap with\n  arXiv:1704.01985", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although great progresses have been made in automatic speech recognition\n(ASR), significant performance degradation is still observed when recognizing\nmulti-talker mixed speech. In this paper, we propose and evaluate several\narchitectures to address this problem under the assumption that only a single\nchannel of mixed signal is available. Our technique extends permutation\ninvariant training (PIT) by introducing the front-end feature separation module\nwith the minimum mean square error (MSE) criterion and the back-end recognition\nmodule with the minimum cross entropy (CE) criterion. More specifically, during\ntraining we compute the average MSE or CE over the whole utterance for each\npossible utterance-level output-target assignment, pick the one with the\nminimum MSE or CE, and optimize for that assignment. This strategy elegantly\nsolves the label permutation problem observed in the deep learning based\nmulti-talker mixed speech separation and recognition systems. The proposed\narchitectures are evaluated and compared on an artificially mixed AMI dataset\nwith both two- and three-talker mixed speech. The experimental results indicate\nthat our proposed architectures can cut the word error rate (WER) by 45.0% and\n25.0% relatively against the state-of-the-art single-talker speech recognition\nsystem across all speakers when their energies are comparable, for two- and\nthree-talker mixed speech, respectively. To our knowledge, this is the first\nwork on the multi-talker mixed speech recognition on the challenging\nspeaker-independent spontaneous large vocabulary continuous speech task.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:48:54 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Qian", "Yanmin", ""], ["Chang", "Xuankai", ""], ["Yu", "Dong", ""]]}, {"id": "1707.06556", "submitter": "Aurelie Herbelot", "authors": "Aurelie Herbelot and Marco Baroni", "title": "High-risk learning: acquiring new word vectors from tiny data", "comments": "Accepted as short paper at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantics models are known to struggle with small data. It is\ngenerally accepted that in order to learn 'a good vector' for a word, a model\nmust have sufficient examples of its usage. This contradicts the fact that\nhumans can guess the meaning of a word from a few occurrences only. In this\npaper, we show that a neural language model such as Word2Vec only necessitates\nminor modifications to its standard architecture to learn new terms from tiny\ndata, using background knowledge from a previously learnt semantic space. We\ntest our model on word definitions and on a nonce task involving 2-6 sentences'\nworth of context, showing a large increase in performance over state-of-the-art\nmodels on the definitional task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:02:14 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Herbelot", "Aurelie", ""], ["Baroni", "Marco", ""]]}, {"id": "1707.06562", "submitter": "Steffen Schnitzer", "authors": "Steffen Schnitzer and Svenja Neitzel and Christoph Rensing", "title": "From Task Classification Towards Similarity Measures for Recommendation\n  in Crowdsourcing Systems", "comments": "Work in Progress Paper at HCOMP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task selection in micro-task markets can be supported by recommender systems\nto help individuals to find appropriate tasks. Previous work showed that for\nthe selection process of a micro-task the semantic aspects, such as the\nrequired action and the comprehensibility, are rated more important than\nfactual aspects, such as the payment or the required completion time. This work\ngives a foundation to create such similarity measures. Therefore, we show that\nan automatic classification based on task descriptions is possible.\nAdditionally, we propose similarity measures to cluster micro-tasks according\nto semantic aspects.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:06:43 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Schnitzer", "Steffen", ""], ["Neitzel", "Svenja", ""], ["Rensing", "Christoph", ""]]}, {"id": "1707.06588", "submitter": "Yaniv Taigman", "authors": "Yaniv Taigman, Lior Wolf, Adam Polyak, Eliya Nachmani", "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new neural text to speech (TTS) method that is able to transform\ntext to speech in voices that are sampled in the wild. Unlike other systems,\nour solution is able to deal with unconstrained voice samples and without\nrequiring aligned phonemes or linguistic features. The network architecture is\nsimpler than those in the existing literature and is based on a novel shifting\nbuffer working memory. The same buffer is used for estimating the attention,\ncomputing the output audio, and for updating the buffer itself. The input\nsentence is encoded using a context-free lookup table that contains one entry\nper character or phoneme. The speakers are similarly represented by a short\nvector that can also be fitted to new identities, even with only a few samples.\nVariability in the generated speech is achieved by priming the buffer prior to\ngenerating the audio. Experimental results on several datasets demonstrate\nconvincing capabilities, making TTS accessible to a wider range of\napplications. In order to promote reproducibility, we release our source code\nand models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 16:18:00 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 15:29:44 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 14:48:11 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Taigman", "Yaniv", ""], ["Wolf", "Lior", ""], ["Polyak", "Adam", ""], ["Nachmani", "Eliya", ""]]}, {"id": "1707.06598", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz and Bhaskar Mitra and Mihai Lupu and Allan Hanbury", "title": "Toward Incorporation of Relevant Documents in word2vec", "comments": "Neu-IR Workshop at the ACM Conference on Research and Development in\n  Information Retrieval (NeuIR-SIGIR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural word embedding provide significant benefit to\nvarious information retrieval tasks. However as shown by recent studies,\nadapting the embedding models for the needs of IR tasks can bring considerable\nfurther improvements. The embedding models in general define the term\nrelatedness by exploiting the terms' co-occurrences in short-window contexts.\nAn alternative (and well-studied) approach in IR for related terms to a query\nis using local information i.e. a set of top-retrieved documents. In view of\nthese two methods of term relatedness, in this work, we report our study on\nincorporating the local information of the query in the word embeddings. One\nmain challenge in this direction is that the dense vectors of word embeddings\nand their estimation of term-to-term relatedness remain difficult to interpret\nand hard to analyze. As an alternative, explicit word representations propose\nvectors whose dimensions are easily interpretable, and recent methods show\ncompetitive performance to the dense vectors. We introduce a neural-based\nexplicit representation, rooted in the conceptual ideas of the word2vec\nSkip-Gram model. The method provides interpretable explicit vectors while\nkeeping the effectiveness of the Skip-Gram model. The evaluation of various\nexplicit representations on word association collections shows that the newly\nproposed method out- performs the state-of-the-art explicit representations\nwhen tasked with ranking highly similar terms. Based on the introduced ex-\nplicit representation, we discuss our approaches on integrating local documents\nin globally-trained embedding models and discuss the preliminary results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 16:33:48 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 07:36:05 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Mitra", "Bhaskar", ""], ["Lupu", "Mihai", ""], ["Hanbury", "Allan", ""]]}, {"id": "1707.06690", "submitter": "Wenhan Xiong", "authors": "Wenhan Xiong and Thien Hoang and William Yang Wang", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "comments": "EMNLP 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning to reason in large scale knowledge graphs\n(KGs). More specifically, we describe a novel reinforcement learning framework\nfor learning multi-hop relational paths: we use a policy-based agent with\ncontinuous states based on knowledge graph embeddings, which reasons in a KG\nvector space by sampling the most promising relation to extend its path. In\ncontrast to prior work, our approach includes a reward function that takes the\naccuracy, diversity, and efficiency into consideration. Experimentally, we show\nthat our proposed method outperforms a path-ranking based algorithm and\nknowledge graph embedding methods on Freebase and Never-Ending Language\nLearning datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:39:23 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 23:11:17 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 06:42:02 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Xiong", "Wenhan", ""], ["Hoang", "Thien", ""], ["Wang", "William Yang", ""]]}, {"id": "1707.06799", "submitter": "Nils Reimers", "authors": "Nils Reimers and Iryna Gurevych", "title": "Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling\n  Tasks", "comments": "34 pages. 9 page version of this paper published at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Selecting optimal parameters for a neural network architecture can often make\nthe difference between mediocre and state-of-the-art performance. However,\nlittle is published which parameters and design choices should be evaluated or\nselected making the correct hyperparameter optimization often a \"black art that\nrequires expert experiences\" (Snoek et al., 2012). In this paper, we evaluate\nthe importance of different network design choices and hyperparameters for five\ncommon linguistic sequence tagging tasks (POS, Chunking, NER, Entity\nRecognition, and Event Detection). We evaluated over 50.000 different setups\nand found, that some parameters, like the pre-trained word embeddings or the\nlast layer of the network, have a large impact on the performance, while other\nparameters, for example the number of LSTM layers or the number of recurrent\nunits, are of minor importance. We give a recommendation on a configuration\nthat performs well among different tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 08:36:31 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 14:06:34 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1707.06806", "submitter": "Tomasz Trzcinski", "authors": "Wociech Stokowiec, Tomasz Trzcinski, Krzysztof Wolk, Krzysztof\n  Marasek, Przemyslaw Rokita", "title": "Shallow reading with Deep Learning: Predicting popularity of online\n  content using only its title", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-60438-1_14", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever decreasing attention span of contemporary Internet users, the\ntitle of online content (such as a news article or video) can be a major factor\nin determining its popularity. To take advantage of this phenomenon, we propose\na new method based on a bidirectional Long Short-Term Memory (LSTM) neural\nnetwork designed to predict the popularity of online content using only its\ntitle. We evaluate the proposed architecture on two distinct datasets of news\narticles and news videos distributed in social media that contain over 40,000\nsamples in total. On those datasets, our approach improves the performance over\ntraditional shallow approaches by a margin of 15%. Additionally, we show that\nusing pre-trained word vectors in the embedding layer improves the results of\nLSTM models, especially when the training set is small. To our knowledge, this\nis the first attempt of applying popularity prediction using only textual\ninformation from the title.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 09:02:55 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Stokowiec", "Wociech", ""], ["Trzcinski", "Tomasz", ""], ["Wolk", "Krzysztof", ""], ["Marasek", "Krzysztof", ""], ["Rokita", "Przemyslaw", ""]]}, {"id": "1707.06841", "submitter": "Youmna Farag", "authors": "Youmna Farag, Marek Rei, Ted Briscoe", "title": "An Error-Oriented Approach to Word Embedding Pre-Training", "comments": "10 pages, 2 figures, 4 tables, BEA 2017", "journal-ref": "The 12th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel word embedding pre-training approach that exploits writing\nerrors in learners' scripts. We compare our method to previous models that tune\nthe embeddings based on script scores and the discrimination between correct\nand corrupt word contexts in addition to the generic commonly-used embeddings\npre-trained on large corpora. The comparison is achieved by using the\naforementioned models to bootstrap a neural network that learns to predict a\nholistic score for scripts. Furthermore, we investigate augmenting our model\nwith error corrections and monitor the impact on performance. Our results show\nthat our error-oriented approach outperforms other comparable ones which is\nfurther demonstrated when training on more data. Additionally, extending the\nmodel with corrections provides further performance gains when data sparsity is\nan issue.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:06:12 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Farag", "Youmna", ""], ["Rei", "Marek", ""], ["Briscoe", "Ted", ""]]}, {"id": "1707.06875", "submitter": "Jekaterina Novikova Dr.", "authors": "Jekaterina Novikova, Ond\\v{r}ej Du\\v{s}ek, Amanda Cercas Curry and\n  Verena Rieser", "title": "Why We Need New Evaluation Metrics for NLG", "comments": "accepted to EMNLP 2017", "journal-ref": "Proceedings of the 2017 Conference on Empirical Methods in Natural\n  Language Processing, pages 2231-2242, Copenhagen, Denmark, September 7-11,\n  2017", "doi": "10.18653/v1/D17-1237", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of NLG evaluation relies on automatic metrics, such as BLEU . In\nthis paper, we motivate the need for novel, system- and data-independent\nautomatic evaluation methods: We investigate a wide range of metrics, including\nstate-of-the-art word-based and novel grammar-based ones, and demonstrate that\nthey only weakly reflect human judgements of system outputs as generated by\ndata-driven, end-to-end NLG. We also show that metric performance is data- and\nsystem-specific. Nevertheless, our results also suggest that automatic metrics\nperform reliably at system-level and can support system development by finding\ncases where a system performs poorly.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 12:47:03 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Novikova", "Jekaterina", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Curry", "Amanda Cercas", ""], ["Rieser", "Verena", ""]]}, {"id": "1707.06878", "submitter": "Alexander Panchenko", "authors": "Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli,\n  Dmitry Ustalov, Simone Paolo Ponzetto, Chris Biemann", "title": "Unsupervised, Knowledge-Free, and Interpretable Word Sense\n  Disambiguation", "comments": "In Proceedings of the the Conference on Empirical Methods on Natural\n  Language Processing (EMNLP 2017). 2017. Copenhagen, Denmark. Association for\n  Computational Linguistics", "journal-ref": null, "doi": "10.18653/v1/D17-2016", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretability of a predictive model is a powerful feature that gains the\ntrust of users in the correctness of the predictions. In word sense\ndisambiguation (WSD), knowledge-based systems tend to be much more\ninterpretable than knowledge-free counterparts as they rely on the wealth of\nmanually-encoded elements representing word senses, such as hypernyms, usage\nexamples, and images. We present a WSD system that bridges the gap between\nthese two so far disconnected groups of methods. Namely, our system, providing\naccess to several state-of-the-art WSD models, aims to be interpretable as a\nknowledge-based system while it remains completely unsupervised and\nknowledge-free. The presented tool features a Web interface for all-word\ndisambiguation of texts that makes the sense predictions human readable by\nproviding interpretable word sense inventories, sense representations, and\ndisambiguation results. We provide a public API, enabling seamless integration.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 12:56:06 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Panchenko", "Alexander", ""], ["Marten", "Fide", ""], ["Ruppert", "Eugen", ""], ["Faralli", "Stefano", ""], ["Ustalov", "Dmitry", ""], ["Ponzetto", "Simone Paolo", ""], ["Biemann", "Chris", ""]]}, {"id": "1707.06885", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg, Eva Hasler, Danielle Saunders and Bill Byrne", "title": "SGNMT -- A Flexible NMT Decoding Platform for Quick Prototyping of New\n  Models and Search Strategies", "comments": "Accepted as EMNLP 2017 demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SGNMT, our experimental platform for machine\ntranslation research. SGNMT provides a generic interface to neural and symbolic\nscoring modules (predictors) with left-to-right semantic such as translation\nmodels like NMT, language models, translation lattices, $n$-best lists or other\nkinds of scores and constraints. Predictors can be combined with other\npredictors to form complex decoding tasks. SGNMT implements a number of search\nstrategies for traversing the space spanned by the predictors which are\nappropriate for different predictor constellations. Adding new predictors or\ndecoding strategies is particularly easy, making it a very efficient tool for\nprototyping new research ideas. SGNMT is actively being used by students in the\nMPhil program in Machine Learning, Speech and Language Technology at the\nUniversity of Cambridge for course work and theses, as well as for most of the\nresearch work in our group.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 13:14:25 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Stahlberg", "Felix", ""], ["Hasler", "Eva", ""], ["Saunders", "Danielle", ""], ["Byrne", "Bill", ""]]}, {"id": "1707.06932", "submitter": "Marinella Petrocchi", "authors": "Michela Fazzolari and Vittoria Cozza and Marinella Petrocchi and\n  Angelo Spognardi", "title": "A study on text-score disagreement in online reviews", "comments": "This is the accepted version of the paper. The final version will be\n  published in the Journal of Cognitive Computation, available at Springer via\n  http://dx.doi.org/10.1007/s12559-017-9496-y", "journal-ref": null, "doi": "10.1007/s12559-017-9496-y", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on online reviews and employ artificial intelligence\ntools, taken from the cognitive computing field, to help understanding the\nrelationships between the textual part of the review and the assigned numerical\nscore. We move from the intuitions that 1) a set of textual reviews expressing\ndifferent sentiments may feature the same score (and vice-versa); and 2)\ndetecting and analyzing the mismatches between the review content and the\nactual score may benefit both service providers and consumers, by highlighting\nspecific factors of satisfaction (and dissatisfaction) in texts.\n  To prove the intuitions, we adopt sentiment analysis techniques and we\nconcentrate on hotel reviews, to find polarity mismatches therein. In\nparticular, we first train a text classifier with a set of annotated hotel\nreviews, taken from the Booking website. Then, we analyze a large dataset, with\naround 160k hotel reviews collected from Tripadvisor, with the aim of detecting\na polarity mismatch, indicating if the textual content of the review is in\nline, or not, with the associated score.\n  Using well established artificial intelligence techniques and analyzing in\ndepth the reviews featuring a mismatch between the text polarity and the score,\nwe find that -on a scale of five stars- those reviews ranked with middle scores\ninclude a mixture of positive and negative aspects.\n  The approach proposed here, beside acting as a polarity detector, provides an\neffective selection of reviews -on an initial very large dataset- that may\nallow both consumers and providers to focus directly on the review subset\nfeaturing a text/score disagreement, which conveniently convey to the user a\nsummary of positive and negative features of the review target.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 15:19:09 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Fazzolari", "Michela", ""], ["Cozza", "Vittoria", ""], ["Petrocchi", "Marinella", ""], ["Spognardi", "Angelo", ""]]}, {"id": "1707.06939", "submitter": "James Bagrow", "authors": "Xipei Liu and James P. Bagrow", "title": "Autocompletion interfaces make crowd workers slower, but their use\n  promotes response diversity", "comments": "12 pages, 6 figures", "journal-ref": "Human Computation 6:1:42-55 (2019)", "doi": "10.15346/hc.v6i1.3", "report-no": null, "categories": "cs.HC cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creative tasks such as ideation or question proposal are powerful\napplications of crowdsourcing, yet the quantity of workers available for\naddressing practical problems is often insufficient. To enable scalable\ncrowdsourcing thus requires gaining all possible efficiency and information\nfrom available workers. One option for text-focused tasks is to allow assistive\ntechnology, such as an autocompletion user interface (AUI), to help workers\ninput text responses. But support for the efficacy of AUIs is mixed. Here we\ndesigned and conducted a randomized experiment where workers were asked to\nprovide short text responses to given questions. Our experimental goal was to\ndetermine if an AUI helps workers respond more quickly and with improved\nconsistency by mitigating typos and misspellings. Surprisingly, we found that\nneither occurred: workers assigned to the AUI treatment were slower than those\nassigned to the non-AUI control and their responses were more diverse, not\nless, than those of the control. Both the lexical and semantic diversities of\nresponses were higher, with the latter measured using word2vec. A crowdsourcer\ninterested in worker speed may want to avoid using an AUI, but using an AUI to\nboost response diversity may be valuable to crowdsourcers interested in\nreceiving as much novel information from workers as possible.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 15:41:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Liu", "Xipei", ""], ["Bagrow", "James P.", ""]]}, {"id": "1707.06945", "submitter": "Ivan Vuli\\'c", "authors": "Ivan Vuli\\'c, Nikola Mrk\\v{s}i\\'c, and Anna Korhonen", "title": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word\n  Vector Space Specialisation", "comments": "EMNLP 2017 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing approaches to automatic VerbNet-style verb classification are\nheavily dependent on feature engineering and therefore limited to languages\nwith mature NLP pipelines. In this work, we propose a novel cross-lingual\ntransfer method for inducing VerbNets for multiple languages. To the best of\nour knowledge, this is the first study which demonstrates how the architectures\nfor learning word embeddings can be applied to this challenging\nsyntactic-semantic task. Our method uses cross-lingual translation pairs to tie\neach of the six target languages into a bilingual vector space with English,\njointly specialising the representations to encode the relational information\nfrom English VerbNet. A standard clustering algorithm is then run on top of the\nVerbNet-specialised representations, using vector dimensions as features for\nlearning verb classes. Our results show that the proposed cross-lingual\ntransfer approach sets new state-of-the-art verb classification performance\nacross all six target languages explored in this work.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 15:52:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Vuli\u0107", "Ivan", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Korhonen", "Anna", ""]]}, {"id": "1707.06957", "submitter": "Karl Stratos", "authors": "Karl Stratos", "title": "Reconstruction of Word Embeddings from Sub-Word Parameters", "comments": "EMNLP 2017, Workshop on Subword and Character Level Models in NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained word embeddings improve the performance of a neural model at the\ncost of increasing the model size. We propose to benefit from this resource\nwithout paying the cost by operating strictly at the sub-lexical level. Our\napproach is quite simple: before task-specific training, we first optimize\nsub-word parameters to reconstruct pre-trained word embeddings using various\ndistance measures. We report interesting results on a variety of tasks: word\nsimilarity, word analogy, and part-of-speech tagging.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 16:10:51 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Stratos", "Karl", ""]]}, {"id": "1707.06961", "submitter": "Yuval Pinter", "authors": "Yuval Pinter, Robert Guthrie, Jacob Eisenstein", "title": "Mimicking Word Embeddings using Subword RNNs", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings improve generalization over lexical features by placing each\nword in a lower-dimensional space, using distributional information obtained\nfrom unlabeled data. However, the effectiveness of word embeddings for\ndownstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which\nembeddings do not exist. In this paper, we present MIMICK, an approach to\ngenerating OOV word embeddings compositionally, by learning a function from\nspellings to distributional embeddings. Unlike prior work, MIMICK does not\nrequire re-training on the original word embedding corpus; instead, learning is\nperformed at the type level. Intrinsic and extrinsic evaluations demonstrate\nthe power of this simple approach. On 23 languages, MIMICK improves performance\nover a word-based baseline for tagging part-of-speech and morphosyntactic\nattributes. It is competitive with (and complementary to) a supervised\ncharacter-based model in low-resource settings.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 16:18:10 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Pinter", "Yuval", ""], ["Guthrie", "Robert", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1707.06971", "submitter": "Shashi Narayan", "authors": "Shashi Narayan and Claire Gardent and Shay B. Cohen and Anastasia\n  Shimorina", "title": "Split and Rephrase", "comments": "11 pages, EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sentence simplification task (Split-and-Rephrase) where the\naim is to split a complex sentence into a meaning preserving sequence of\nshorter sentences. Like sentence simplification, splitting-and-rephrasing has\nthe potential of benefiting both natural language processing and societal\napplications. Because shorter sentences are generally better processed by NLP\nsystems, it could be used as a preprocessing step which facilitates and\nimproves the performance of parsers, semantic role labellers and machine\ntranslation systems. It should also be of use for people with reading\ndisabilities because it allows the conversion of longer sentences into shorter\nones. This paper makes two contributions towards this new task. First, we\ncreate and make available a benchmark consisting of 1,066,115 tuples mapping a\nsingle complex sentence to a sequence of sentences expressing the same meaning.\nSecond, we propose five models (vanilla sequence-to-sequence to\nsemantically-motivated models) to understand the difficulty of the proposed\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 16:47:56 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Narayan", "Shashi", ""], ["Gardent", "Claire", ""], ["Cohen", "Shay B.", ""], ["Shimorina", "Anastasia", ""]]}, {"id": "1707.06996", "submitter": "Ankush Chatterjee", "authors": "Umang Gupta, Ankush Chatterjee, Radhakrishnan Srikanth, Puneet Agrawal", "title": "A Sentiment-and-Semantics-Based Approach for Emotion Detection in\n  Textual Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are physiological states generated in humans in reaction to internal\nor external events. They are complex and studied across numerous fields\nincluding computer science. As humans, on reading \"Why don't you ever text me!\"\nwe can either interpret it as a sad or angry emotion and the same ambiguity\nexists for machines. Lack of facial expressions and voice modulations make\ndetecting emotions from text a challenging problem. However, as humans\nincreasingly communicate using text messaging applications, and digital agents\ngain popularity in our society, it is essential that these digital agents are\nemotion aware, and respond accordingly.\n  In this paper, we propose a novel approach to detect emotions like happy, sad\nor angry in textual conversations using an LSTM based Deep Learning model. Our\napproach consists of semi-automated techniques to gather training data for our\nmodel. We exploit advantages of semantic and sentiment based embeddings and\npropose a solution combining both. Our work is evaluated on real-world\nconversations and significantly outperforms traditional Machine Learning\nbaselines as well as other off-the-shelf Deep Learning models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:52:45 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 10:22:54 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 07:47:32 GMT"}, {"version": "v4", "created": "Fri, 30 Mar 2018 15:41:50 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Gupta", "Umang", ""], ["Chatterjee", "Ankush", ""], ["Srikanth", "Radhakrishnan", ""], ["Agrawal", "Puneet", ""]]}, {"id": "1707.07045", "submitter": "Kenton Lee", "authors": "Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer", "title": "End-to-end Neural Coreference Resolution", "comments": "Accepted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first end-to-end coreference resolution model and show that\nit significantly outperforms all previous work without using a syntactic parser\nor hand-engineered mention detector. The key idea is to directly consider all\nspans in a document as potential mentions and learn distributions over possible\nantecedents for each. The model computes span embeddings that combine\ncontext-dependent boundary representations with a head-finding attention\nmechanism. It is trained to maximize the marginal likelihood of gold antecedent\nspans from coreference clusters and is factored to enable aggressive pruning of\npotential mentions. Experiments demonstrate state-of-the-art performance, with\na gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model\nensemble, despite the fact that this is the first approach to be successfully\ntrained with no external resources.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 21:05:04 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 21:45:56 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Lee", "Kenton", ""], ["He", "Luheng", ""], ["Lewis", "Mike", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1707.07048", "submitter": "Zhehuai Chen", "authors": "Zhehuai Chen, Jasha Droppo, Jinyu Li, Wayne Xiong", "title": "Progressive Joint Modeling in Unsupervised Single-channel Overlapped\n  Speech Recognition", "comments": "submitted to TASLP, 07/20/2017; accepted by TASLP, 10/13/2017", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  26 (2018) 184-196", "doi": "10.1109/TASLP.2017.2765834", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised single-channel overlapped speech recognition is one of the\nhardest problems in automatic speech recognition (ASR). Permutation invariant\ntraining (PIT) is a state of the art model-based approach, which applies a\nsingle neural network to solve this single-input, multiple-output modeling\nproblem. We propose to advance the current state of the art by imposing a\nmodular structure on the neural network, applying a progressive pretraining\nregimen, and improving the objective function with transfer learning and a\ndiscriminative training criterion. The modular structure splits the problem\ninto three sub-tasks: frame-wise interpreting, utterance-level speaker tracing,\nand speech recognition. The pretraining regimen uses these modules to solve\nprogressively harder tasks. Transfer learning leverages parallel clean speech\nto improve the training targets for the network. Our discriminative training\nformulation is a modification of standard formulations, that also penalizes\ncompeting outputs of the system. Experiments are conducted on the artificial\noverlapped Switchboard and hub5e-swb dataset. The proposed framework achieves\nover 30% relative improvement of WER over both a strong jointly trained system,\nPIT for ASR, and a separately optimized system, PIT for speech separation with\nclean speech ASR model. The improvement comes from better model generalization,\ntraining efficiency and the sequence level linguistic knowledge integration.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 21:21:09 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 03:06:23 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Chen", "Zhehuai", ""], ["Droppo", "Jasha", ""], ["Li", "Jinyu", ""], ["Xiong", "Wayne", ""]]}, {"id": "1707.07062", "submitter": "Xinyu Hua", "authors": "Xinyu Hua, Lu Wang", "title": "A Pilot Study of Domain Adaptation Effect for Neural Abstractive\n  Summarization", "comments": "This paper is accepted by EMNLP 2017 Workshop on New Frontiers in\n  Summarization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of domain adaptation for neural abstractive\nsummarization. We make initial efforts in investigating what information can be\ntransferred to a new domain. Experimental results on news stories and opinion\narticles indicate that neural summarization model benefits from pre-training\nbased on extractive summaries. We also find that the combination of in-domain\nand out-of-domain setup yields better summaries when in-domain data is\ninsufficient. Further analysis shows that, the model is capable to select\nsalient content even trained on out-of-domain data, but requires in-domain data\nto capture the style for a target domain.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 22:42:52 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hua", "Xinyu", ""], ["Wang", "Lu", ""]]}, {"id": "1707.07066", "submitter": "Hayafumi Watanabe", "authors": "Hayafumi Watanabe", "title": "Ultraslow diffusion in language: Dynamics of appearance of already\n  popular adjectives on Japanese blogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What dynamics govern a time series representing the appearance of words in\nsocial media data? In this paper, we investigate an elementary dynamics, from\nwhich word-dependent special effects are segregated, such as breaking news,\nincreasing (or decreasing) concerns, or seasonality. To elucidate this problem,\nwe investigated approximately three billion Japanese blog articles over a\nperiod of six years, and analysed some corresponding solvable mathematical\nmodels. From the analysis, we found that a word appearance can be explained by\nthe random diffusion model based on the power-law forgetting process, which is\na type of long memory point process related to ARFIMA(0,0.5,0). In particular,\nwe confirmed that ultraslow diffusion (where the mean squared displacement\ngrows logarithmically), which the model predicts in an approximate manner,\nreproduces the actual data. In addition, we also show that the model can\nreproduce other statistical properties of a time series: (i) the fluctuation\nscaling, (ii) spectrum density, and (iii) shapes of the probability density\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 23:13:50 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 05:05:33 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 05:39:01 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Watanabe", "Hayafumi", ""]]}, {"id": "1707.07086", "submitter": "Katherine Keith", "authors": "Katherine A. Keith, Abram Handler, Michael Pinkham, Cara Magliozzi,\n  Joshua McDuffie, and Brendan O'Connor", "title": "Identifying civilians killed by police with distantly supervised\n  entity-event extraction", "comments": null, "journal-ref": "Proceedings of the 2017 Conference on Empirical Methods in Natural\n  Language Processing", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new, socially-impactful task for natural language processing:\nfrom a news corpus, extract names of persons who have been killed by police. We\npresent a newly collected police fatality corpus, which we release publicly,\nand present a model to solve this problem that uses EM-based distant\nsupervision with logistic regression and convolutional neural network\nclassifiers. Our model outperforms two off-the-shelf event extractor systems,\nand it can suggest candidate victim names in some cases faster than one of the\nmajor manually-collected police fatality databases.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 01:47:36 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Keith", "Katherine A.", ""], ["Handler", "Abram", ""], ["Pinkham", "Michael", ""], ["Magliozzi", "Cara", ""], ["McDuffie", "Joshua", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1707.07102", "submitter": "Xuwang Yin", "authors": "Xuwang Yin, Vicente Ordonez", "title": "OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating captions for images is a task that has recently received\nconsiderable attention. In this work we focus on caption generation for\nabstract scenes, or object layouts where the only information provided is a set\nof objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence\nmodel that encodes a set of objects and their locations as an input sequence\nusing an LSTM network, and decodes this representation using an LSTM language\nmodel. We show that our model, despite encoding object layouts as a sequence,\ncan represent spatial relationships between objects, and generate descriptions\nthat are globally coherent and semantically relevant. We test our approach in a\ntask of object-layout captioning by using only object annotations as inputs. We\nadditionally show that our model, combined with a state-of-the-art object\ndetector, improves an image captioning model from 0.863 to 0.950 (CIDEr score)\nin the test benchmark of the standard MS-COCO Captioning task.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 04:17:42 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Yin", "Xuwang", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1707.07129", "submitter": "Ali Septiandri", "authors": "Ali Akbar Septiandri", "title": "Predicting the Gender of Indonesian Names", "comments": "Submitted to ICoDIS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigated a way to predict the gender of a name using character-level\nLong-Short Term Memory (char-LSTM). We compared our method with some\nconventional machine learning methods, namely Naive Bayes, logistic regression,\nand XGBoost with n-grams as the features. We evaluated the models on a dataset\nconsisting of the names of Indonesian people. It is not common to use a family\nname as the surname in Indonesian culture, except in some ethnicities.\nTherefore, we inferred the gender from both full names and first names. The\nresults show that we can achieve 92.25% accuracy from full names, while using\nfirst names only yields 90.65% accuracy. These results are better than the ones\nfrom applying the classical machine learning algorithms to n-grams.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 09:35:10 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 07:05:12 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Septiandri", "Ali Akbar", ""]]}, {"id": "1707.07167", "submitter": "Lei Xie", "authors": "Changhao Shan, Junbo Zhang, Yujun Wang, Lei Xie", "title": "Attention-Based End-to-End Speech Recognition on Voice Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in end-to-end speech recognition\nthat directly transcribes speech to text without any predefined alignments. In\nthis paper, we explore the use of attention-based encoder-decoder model for\nMandarin speech recognition on a voice search task. Previous attempts have\nshown that applying attention-based encoder-decoder to Mandarin speech\nrecognition was quite difficult due to the logographic orthography of Mandarin,\nthe large vocabulary and the conditional dependency of the attention model. In\nthis paper, we use character embedding to deal with the large vocabulary.\nSeveral tricks are used for effective model training, including L2\nregularization, Gaussian weight noise and frame skipping. We compare two\nattention mechanisms and use attention smoothing to cover long context in the\nattention model. Taken together, these tricks allow us to finally achieve a\ncharacter error rate (CER) of 3.58% and a sentence error rate (SER) of 7.43% on\nthe MiTV voice search dataset. While together with a trigram language model,\nCER and SER reach 2.81% and 5.77%, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 13:53:28 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 03:29:40 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 08:20:30 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Shan", "Changhao", ""], ["Zhang", "Junbo", ""], ["Wang", "Yujun", ""], ["Xie", "Lei", ""]]}, {"id": "1707.07182", "submitter": "Marcos Zampieri", "authors": "Marcos Zampieri, Alina Maria Ciobanu, Liviu P. Dinu", "title": "Native Language Identification on Text and Speech", "comments": "Proceedings of the Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an ensemble system combining the output of multiple SVM\nclassifiers to native language identification (NLI). The system was submitted\nto the NLI Shared Task 2017 fusion track which featured students essays and\nspoken responses in form of audio transcriptions and iVectors by non-native\nEnglish speakers of eleven native languages. Our system competed in the\nchallenge under the team name ZCD and was based on an ensemble of SVM\nclassifiers trained on character n-grams achieving 83.58% accuracy and ranking\n3rd in the shared task.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 16:04:49 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zampieri", "Marcos", ""], ["Ciobanu", "Alina Maria", ""], ["Dinu", "Liviu P.", ""]]}, {"id": "1707.07191", "submitter": "Ting-Hao Huang", "authors": "Chieh-Yang Huang, Tristan Labetoulle, Ting-Hao Kenneth Huang, Yi-Pei\n  Chen, Hung-Chen Chen, Vallari Srivastava, Lun-Wei Ku", "title": "MoodSwipe: A Soft Keyboard that Suggests Messages Based on\n  User-Specified Emotions", "comments": "6 pages (including references), EMNLP 2017 Demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MoodSwipe, a soft keyboard that suggests text messages given the\nuser-specified emotions utilizing the real dialog data. The aim of MoodSwipe is\nto create a convenient user interface to enjoy the technology of emotion\nclassification and text suggestion, and at the same time to collect labeled\ndata automatically for developing more advanced technologies. While users\nselect the MoodSwipe keyboard, they can type as usual but sense the emotion\nconveyed by their text and receive suggestions for their message as a benefit.\nIn MoodSwipe, the detected emotions serve as the medium for suggested texts,\nwhere viewing the latter is the incentive to correcting the former. We conduct\nseveral experiments to show the superiority of the emotion classification\nmodels trained on the dialog data, and further to verify good emotion cues are\nimportant context for text suggestion.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 16:32:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Huang", "Chieh-Yang", ""], ["Labetoulle", "Tristan", ""], ["Huang", "Ting-Hao Kenneth", ""], ["Chen", "Yi-Pei", ""], ["Chen", "Hung-Chen", ""], ["Srivastava", "Vallari", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1707.07212", "submitter": "Sandesh Swamy", "authors": "Sandesh Swamy, Alan Ritter and Marie-Catherine de Marneffe", "title": "\"i have a feeling trump will win..................\": Forecasting Winners\n  and Losers from User Predictions on Twitter", "comments": "Accepted at EMNLP 2017 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media users often make explicit predictions about upcoming events.\nSuch statements vary in the degree of certainty the author expresses toward the\noutcome:\"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\"\nor \"No way Leonardo wins!\". Can popular beliefs on social media predict who\nwill win? To answer this question, we build a corpus of tweets annotated for\nveridicality on which we train a log-linear classifier that detects positive\nveridicality with high precision. We then forecast uncertain outcomes using the\nwisdom of crowds, by aggregating users' explicit predictions. Our method for\nforecasting winners is fully automated, relying only on a set of contenders as\ninput. It requires no training data of past outcomes and outperforms sentiment\nand tweet volume baselines on a broad range of contest prediction tasks. We\nfurther demonstrate how our approach can be used to measure the reliability of\nindividual accounts' predictions and retrospectively identify surprise\noutcomes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 20:34:53 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 05:06:17 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 01:15:44 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Swamy", "Sandesh", ""], ["Ritter", "Alan", ""], ["de Marneffe", "Marie-Catherine", ""]]}, {"id": "1707.07240", "submitter": "Bin Wang", "authors": "Bin Wang and Zhijian Ou", "title": "Language modeling with Neural trans-dimensional random fields", "comments": "6 pages, 2 figures and 3 tables, accepted to ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trans-dimensional random field language models (TRF LMs) have recently been\nintroduced, where sentences are modeled as a collection of random fields. The\nTRF approach has been shown to have the advantages of being computationally\nmore efficient in inference than LSTM LMs with close performance and being able\nto flexibly integrating rich features. In this paper we propose neural TRFs,\nbeyond of the previous discrete TRFs that only use linear potentials with\ndiscrete features. The idea is to use nonlinear potentials with continuous\nfeatures, implemented by neural networks (NNs), in the TRF framework. Neural\nTRFs combine the advantages of both NNs and TRFs. The benefits of word\nembedding, nonlinear feature learning and larger context modeling are inherited\nfrom the use of NNs. At the same time, the strength of efficient inference by\navoiding expensive softmax is preserved. A number of technical contributions,\nincluding employing deep convolutional neural networks (CNNs) to define the\npotentials and incorporating the joint stochastic approximation (JSA) strategy\nin the training algorithm, are developed in this work, which enable us to\nsuccessfully train neural TRF LMs. Various LMs are evaluated in terms of speech\nrecognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The\nresults show that neural TRF LMs not only improve over discrete TRF LMs, but\nalso perform slightly better than LSTM LMs with only one fifth of parameters\nand 16x faster inference efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 03:06:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 01:25:18 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 08:28:42 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Wang", "Bin", ""], ["Ou", "Zhijian", ""]]}, {"id": "1707.07250", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, Louis-Philippe\n  Morency", "title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "comments": "Accepted as full paper in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis is an increasingly popular research area, which\nextends the conventional language-based definition of sentiment analysis to a\nmultimodal setup where other relevant modalities accompany language. In this\npaper, we pose the problem of multimodal sentiment analysis as modeling\nintra-modality and inter-modality dynamics. We introduce a novel model, termed\nTensor Fusion Network, which learns both such dynamics end-to-end. The proposed\napproach is tailored for the volatile nature of spoken language in online\nvideos as well as accompanying gestures and voice. In the experiments, our\nmodel outperforms state-of-the-art approaches for both multimodal and unimodal\nsentiment analysis.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 05:54:20 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zadeh", "Amir", ""], ["Chen", "Minghai", ""], ["Poria", "Soujanya", ""], ["Cambria", "Erik", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1707.07265", "submitter": "Sho Takase", "authors": "Sho Takase, Naoaki Okazaki, Kentaro Inui", "title": "Composing Distributed Representations of Relational Patterns", "comments": "Published as a conference paper at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning distributed representations for relation instances is a central\ntechnique in downstream NLP applications. In order to address semantic modeling\nof relational patterns, this paper constructs a new dataset that provides\nmultiple similarity ratings for every pair of relational patterns on the\nexisting dataset. In addition, we conduct a comparative study of different\nencoders including additive composition, RNN, LSTM, and GRU for composing\ndistributed representations of relational patterns. We also present Gated\nAdditive Composition, which is an enhancement of additive composition with the\ngating mechanism. Experiments show that the new dataset does not only enable\ndetailed analyses of the different encoders, but also provides a gauge to\npredict successes of distributed representations of relational patterns in the\nrelation classification task.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 08:12:59 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Takase", "Sho", ""], ["Okazaki", "Naoaki", ""], ["Inui", "Kentaro", ""]]}, {"id": "1707.07270", "submitter": "Yixing Fan", "authors": "Yixing Fan, Liang Pang, JianPeng Hou, Jiafeng Guo, Yanyan Lan, Xueqi\n  Cheng", "title": "MatchZoo: A Toolkit for Deep Text Matching", "comments": "2 pages, 1 figures, Neu-IR: The SIGIR 2017 Workshop on Neural\n  Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, deep neural models have been widely adopted for text\nmatching tasks, such as question answering and information retrieval, showing\nimproved performance as compared with previous methods. In this paper, we\nintroduce the MatchZoo toolkit that aims to facilitate the designing, comparing\nand sharing of deep text matching models. Specifically, the toolkit provides a\nunified data preparation module for different text matching problems, a\nflexible layer-based model construction process, and a variety of training\nobjectives and evaluation metrics. In addition, the toolkit has implemented two\nschools of representative deep text matching models, namely\nrepresentation-focused models and interaction-focused models. Finally, users\ncan easily modify existing models, create and share their own models for text\nmatching in MatchZoo.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 09:36:50 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Fan", "Yixing", ""], ["Pang", "Liang", ""], ["Hou", "JianPeng", ""], ["Guo", "Jiafeng", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1707.07273", "submitter": "Kim Anh Nguyen", "authors": "Kim Anh Nguyen, Maximilian K\\\"oper, Sabine Schulte im Walde, Ngoc\n  Thang Vu", "title": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "comments": "11 pages, accepted as long paper at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural model HyperVec to learn hierarchical embeddings for\nhypernymy detection and directionality. While previous embeddings have shown\nlimitations on prototypical hypernyms, HyperVec represents an unsupervised\nmeasure where embeddings are learned in a specific order and capture the\nhypernym$-$hyponym distributional hierarchy. Moreover, our model is able to\ngeneralize over unseen hypernymy pairs, when using only small sets of training\ndata, and by mapping to other languages. Results on benchmark datasets show\nthat HyperVec outperforms both state$-$of$-$the$-$art unsupervised measures and\nembedding models on hypernymy detection and directionality, and on predicting\ngraded lexical entailment.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 09:55:48 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Nguyen", "Kim Anh", ""], ["K\u00f6per", "Maximilian", ""], ["Walde", "Sabine Schulte im", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1707.07278", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu and Katja Markert and Avishek Anand", "title": "Fine Grained Citation Span for References in Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\emph{Verifiability} is one of the core editing principles in Wikipedia,\neditors being encouraged to provide citations for the added content. For a\nWikipedia article, determining the \\emph{citation span} of a citation, i.e.\nwhat content is covered by a citation, is important as it helps decide for\nwhich content citations are still missing.\n  We are the first to address the problem of determining the \\emph{citation\nspan} in Wikipedia articles. We approach this problem by classifying which\ntextual fragments in an article are covered by a citation. We propose a\nsequence classification approach where for a paragraph and a citation, we\ndetermine the citation span at a fine-grained level.\n  We provide a thorough experimental evaluation and compare our approach\nagainst baselines adopted from the scientific domain, where we show improvement\nfor all evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 10:43:26 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Fetahu", "Besnik", ""], ["Markert", "Katja", ""], ["Anand", "Avishek", ""]]}, {"id": "1707.07279", "submitter": "Haijing Liu", "authors": "Haijing Liu (1 and 2), Yang Gao (1), Pin Lv (1), Mengxue Li (1 and 2),\n  Shiqiang Geng (3), Minglan Li (1 and 2), Hao Wang (4) ((1) Institute of\n  Software, Chinese Academy of Sciences, (2) University of Chinese Academy of\n  Sciences, (3) School of Automation, Beijing Information Science and\n  Technology University, (4) Qihoo 360 Search Lab)", "title": "Using Argument-based Features to Predict and Analyse Review Helpfulness", "comments": "6 pages, EMNLP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the helpful product reviews identification problem in this paper. We\nobserve that the evidence-conclusion discourse relations, also known as\narguments, often appear in product reviews, and we hypothesise that some\nargument-based features, e.g. the percentage of argumentative sentences, the\nevidences-conclusions ratios, are good indicators of helpful reviews. To\nvalidate this hypothesis, we manually annotate arguments in 110 hotel reviews,\nand investigate the effectiveness of several combinations of argument-based\nfeatures. Experiments suggest that, when being used together with the\nargument-based features, the state-of-the-art baseline features can enjoy a\nperformance boost (in terms of F1) of 11.01\\% in average.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 10:51:30 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Liu", "Haijing", "", "1 and 2"], ["Gao", "Yang", "", "1 and 2"], ["Lv", "Pin", "", "1 and 2"], ["Li", "Mengxue", "", "1 and 2"], ["Geng", "Shiqiang", "", "1 and 2"], ["Li", "Minglan", "", "1 and 2"], ["Wang", "Hao", ""]]}, {"id": "1707.07328", "submitter": "Robin Jia", "authors": "Robin Jia and Percy Liang", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to $7\\%$. We hope our insights will\nmotivate the development of new models that understand language more precisely.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 18:26:29 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Jia", "Robin", ""], ["Liang", "Percy", ""]]}, {"id": "1707.07331", "submitter": "Natalie Ahn", "authors": "Natalie Ahn", "title": "Rule-Based Spanish Morphological Analyzer Built From Spell Checking\n  Lexicon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preprocessing tools for automated text analysis have become more widely\navailable in major languages, but non-English tools are often still limited in\ntheir functionality. When working with Spanish-language text, researchers can\neasily find tools for tokenization and stemming, but may not have the means to\nextract more complex word features like verb tense or mood. Yet Spanish is a\nmorphologically rich language in which such features are often identifiable\nfrom word form. Conjugation rules are consistent, but many special verbs and\nnouns take on different rules. While building a complete dictionary of known\nwords and their morphological rules would be labor intensive, resources to do\nso already exist, in spell checkers designed to generate valid forms of known\nwords. This paper introduces a set of tools for Spanish-language morphological\nanalysis, built using the COES spell checking tools, to label person, mood,\ntense, gender and number, derive a word's root noun or verb infinitive, and\nconvert verbs to their nominal form.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 18:42:24 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ahn", "Natalie", ""]]}, {"id": "1707.07343", "submitter": "Prafulla Kumar Choubey", "authors": "Prafulla Kumar Choubey and Ruihong Huang", "title": "A Sequential Model for Classifying Temporal Relations between\n  Intra-Sentence Events", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequential model for temporal relation classification between\nintra-sentence events. The key observation is that the overall syntactic\nstructure and compositional meanings of the multi-word context between events\nare important for distinguishing among fine-grained temporal relations.\nSpecifically, our approach first extracts a sequence of context words that\nindicates the temporal relation between two events, which well align with the\ndependency path between two event mentions. The context word sequence, together\nwith a parts-of-speech tag sequence and a dependency relation sequence that are\ngenerated corresponding to the word sequence, are then provided as input to\nbidirectional recurrent neural network (LSTM) models. The neural nets learn\ncompositional syntactic and semantic representations of contexts surrounding\nthe two events and predict the temporal relation between them. Evaluation of\nthe proposed approach on TimeBank corpus shows that sequential modeling is\ncapable of accurately recognizing temporal relations between events, which\noutperforms a neural net model using various discrete features as input that\nimitates previous feature based models.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 20:47:02 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Choubey", "Prafulla Kumar", ""], ["Huang", "Ruihong", ""]]}, {"id": "1707.07344", "submitter": "Prafulla Kumar Choubey", "authors": "Prafulla Kumar Choubey and Ruihong Huang", "title": "Event Coreference Resolution by Iteratively Unfolding Inter-dependencies\n  among Events", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel iterative approach for event coreference resolution that\ngradually builds event clusters by exploiting inter-dependencies among event\nmentions within the same chain as well as across event chains. Among event\nmentions in the same chain, we distinguish within- and cross-document event\ncoreference links by using two distinct pairwise classifiers, trained\nseparately to capture differences in feature distributions of within- and\ncross-document event clusters. Our event coreference approach alternates\nbetween WD and CD clustering and combines arguments from both event clusters\nafter every merge, continuing till no more merge can be made. And then it\nperforms further merging between event chains that are both closely related to\na set of other chains of events. Experiments on the ECB+ corpus show that our\nmodel outperforms state-of-the-art methods in joint task of WD and CD event\ncoreference resolution.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 20:49:18 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Choubey", "Prafulla Kumar", ""], ["Huang", "Ruihong", ""]]}, {"id": "1707.07402", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Hal Daum\\'e III and Jordan Boyd-Graber", "title": "Reinforcement Learning for Bandit Neural Machine Translation with\n  Simulated Human Feedback", "comments": "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural\n  Language Processing (EMNLP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is a natural candidate problem for reinforcement learning\nfrom human feedback: users provide quick, dirty ratings on candidate\ntranslations to guide a system to improve. Yet, current neural machine\ntranslation training focuses on expensive human-generated reference\ntranslations. We describe a reinforcement learning algorithm that improves\nneural machine translation systems from simulated human feedback. Our algorithm\ncombines the advantage actor-critic algorithm (Mnih et al., 2016) with the\nattention-based neural encoder-decoder architecture (Luong et al., 2015). This\nalgorithm (a) is well-designed for problems with a large action space and\ndelayed rewards, (b) effectively optimizes traditional corpus-level machine\ntranslation metrics, and (c) is robust to skewed, high-variance, granular\nfeedback modeled after actual human behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 04:35:19 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 17:19:01 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 06:10:55 GMT"}, {"version": "v4", "created": "Sat, 11 Nov 2017 05:01:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1707.07413", "submitter": "Sanjeev Satheesh", "authors": "Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates, Yashesh Gaur,\n  Yi Li, Hairong Liu, Sanjeev Satheesh, David Seetapun, Anuroop Sriram, Zhenyao\n  Zhu", "title": "Exploring Neural Transducers for End-to-End Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform an empirical comparison among the CTC,\nRNN-Transducer, and attention-based Seq2Seq models for end-to-end speech\nrecognition. We show that, without any language model, Seq2Seq and\nRNN-Transducer models both outperform the best reported CTC models with a\nlanguage model, on the popular Hub5'00 benchmark. On our internal diverse\ndataset, these trends continue - RNNTransducer models rescored with a language\nmodel after beam search outperform our best CTC models. These results simplify\nthe speech recognition pipeline so that decoding can now be expressed purely as\nneural network operations. We also study how the choice of encoder architecture\naffects the performance of the three models - when all encoder layers are\nforward only, and when encoders downsample the input representation\naggressively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 06:05:21 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Battenberg", "Eric", ""], ["Chen", "Jitong", ""], ["Child", "Rewon", ""], ["Coates", "Adam", ""], ["Gaur", "Yashesh", ""], ["Li", "Yi", ""], ["Liu", "Hairong", ""], ["Satheesh", "Sanjeev", ""], ["Seetapun", "David", ""], ["Sriram", "Anuroop", ""], ["Zhu", "Zhenyao", ""]]}, {"id": "1707.07469", "submitter": "Marta R. Costa-juss\\`a", "authors": "Han Yang, Marta R. Costa-juss\\`a and Jos\\'e A. R. Fonollosa", "title": "Character-level Intra Attention Network for Natural Language Inference", "comments": "EMNLP Workshop RepEval 2017: The Second Workshop on Evaluating Vector\n  Space Representations for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is a central problem in language\nunderstanding. End-to-end artificial neural networks have reached\nstate-of-the-art performance in NLI field recently.\n  In this paper, we propose Character-level Intra Attention Network (CIAN) for\nthe NLI task. In our model, we use the character-level convolutional network to\nreplace the standard word embedding layer, and we use the intra attention to\ncapture the intra-sentence semantics. The proposed CIAN model provides improved\nresults based on a newly published MNLI corpus.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 10:35:46 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Yang", "Han", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1707.07499", "submitter": "Rudolf Schneider", "authors": "Rudolf Schneider, Tom Oberhauser, Tobias Klatt, Felix A. Gers,\n  Alexander L\\\"oser", "title": "Analysing Errors of Open Information Extraction Systems", "comments": "Accepted at Building Linguistically Generalizable NLP Systems at\n  EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report results on benchmarking Open Information Extraction (OIE) systems\nusing RelVis, a toolkit for benchmarking Open Information Extraction systems.\nOur comprehensive benchmark contains three data sets from the news domain and\none data set from Wikipedia with overall 4522 labeled sentences and 11243\nbinary or n-ary OIE relations. In our analysis on these data sets we compared\nthe performance of four popular OIE systems, ClausIE, OpenIE 4.2, Stanford\nOpenIE and PredPatt. In addition, we evaluated the impact of five common error\nclasses on a subset of 749 n-ary tuples. From our deep analysis we unreveal\nimportant research directions for a next generation of OIE systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 11:49:00 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Schneider", "Rudolf", ""], ["Oberhauser", "Tom", ""], ["Klatt", "Tobias", ""], ["Gers", "Felix A.", ""], ["L\u00f6ser", "Alexander", ""]]}, {"id": "1707.07554", "submitter": "Victor Prokhorov", "authors": "Victor Prokhorov, Mohammad Taher Pilehvar, Dimitri Kartsaklis, Pietro\n  Li\\'o and Nigel Collier", "title": "Learning Rare Word Representations using Semantic Bridging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology that adapts graph embedding techniques (DeepWalk\n(Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016)) as well as\ncross-lingual vector space mapping approaches (Least Squares and Canonical\nCorrelation Analysis) in order to merge the corpus and ontological sources of\nlexical knowledge. We also perform comparative analysis of the used algorithms\nin order to identify the best combination for the proposed system. We then\napply this to the task of enhancing the coverage of an existing word\nembedding's vocabulary with rare and unseen words. We show that our technique\ncan provide considerable extra coverage (over 99%), leading to consistent\nperformance gain (around 10% absolute gain is achieved with w2v-gn-500K cf.\\S\n3.3) on the Rare Word Similarity dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 13:38:00 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Prokhorov", "Victor", ""], ["Pilehvar", "Mohammad Taher", ""], ["Kartsaklis", "Dimitri", ""], ["Li\u00f3", "Pietro", ""], ["Collier", "Nigel", ""]]}, {"id": "1707.07568", "submitter": "Georgios Balikas", "authors": "C\\'edric Lopez, Ioannis Partalas, Georgios Balikas, Nadia Derbas,\n  Am\\'elie Martin, Coralie Reutenauer, Fr\\'ed\\'erique Segond, Massih-Reza Amini", "title": "CAp 2017 challenge: Twitter Named Entity Recognition", "comments": "Presented at CAp 2017 (French Conference on Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the CAp 2017 challenge. The challenge concerns the\nproblem of Named Entity Recognition (NER) for tweets written in French. We\nfirst present the data preparation steps we followed for constructing the\ndataset released in the framework of the challenge. We begin by demonstrating\nwhy NER for tweets is a challenging problem especially when the number of\nentities increases. We detail the annotation process and the necessary\ndecisions we made. We provide statistics on the inter-annotator agreement, and\nwe conclude the data description part with examples and statistics for the\ndata. We, then, describe the participation in the challenge, where 8 teams\nparticipated, with a focus on the methods employed by the challenge\nparticipants and the scores achieved in terms of F$_1$ measure. Importantly,\nthe constructed dataset comprising $\\sim$6,000 tweets annotated for 13 types of\nentities, which to the best of our knowledge is the first such dataset in\nFrench, is publicly available at \\url{http://cap2017.imag.fr/competition.html} .\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:20:07 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Lopez", "C\u00e9dric", ""], ["Partalas", "Ioannis", ""], ["Balikas", "Georgios", ""], ["Derbas", "Nadia", ""], ["Martin", "Am\u00e9lie", ""], ["Reutenauer", "Coralie", ""], ["Segond", "Fr\u00e9d\u00e9rique", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1707.07585", "submitter": "Weizheng Chen", "authors": "Zeya Zhang, Weizheng Chen and Hongfei Yan", "title": "Stock Prediction: a method based on extraction of news features and\n  recurrent neural networks", "comments": "in Chinese, The 22nd China Conference on Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a method for stock prediction. In terms of feature\nextraction, we extract the features of stock-related news besides stock prices.\nWe first select some seed words based on experience which are the symbols of\ngood news and bad news. Then we propose an optimization method and calculate\nthe positive polar of all words. After that, we construct the features of news\nbased on the positive polar of their words. In consideration of sequential\nstock prices and continuous news effects, we propose a recurrent neural network\nmodel to help predict stock prices. Compared to SVM classifier with price\nfeatures, we find our proposed method has an over 5% improvement on stock\nprediction accuracy in experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:40:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhang", "Zeya", ""], ["Chen", "Weizheng", ""], ["Yan", "Hongfei", ""]]}, {"id": "1707.07591", "submitter": "Timo Schick", "authors": "Timo Schick", "title": "Transition-Based Generation from Abstract Meaning Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the task of generating English sentences from Abstract\nMeaning Representation (AMR) graphs. To cope with this task, we transform each\ninput AMR graph into a structure similar to a dependency tree and annotate it\nwith syntactic information by applying various predefined actions to it.\nSubsequently, a sentence is obtained from this tree structure by visiting its\nnodes in a specific order. We train maximum entropy models to estimate the\nprobability of each individual action and devise an algorithm that efficiently\napproximates the best sequence of actions to be applied. Using a substandard\nlanguage model, our generator achieves a Bleu score of 27.4 on the LDC2014T12\ntest set, the best result reported so far without using silver standard\nannotations from another corpus as additional training data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:52:32 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Schick", "Timo", ""]]}, {"id": "1707.07601", "submitter": "Spandana Gella", "authors": "Spandana Gella, Rico Sennrich, Frank Keller, Mirella Lapata", "title": "Image Pivoting for Learning Multilingual Multimodal Representations", "comments": "7 pages, EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a model to learn multimodal multilingual\nrepresentations for matching images and sentences in different languages, with\nthe aim of advancing multilingual versions of image search and image\nunderstanding. Our model learns a common representation for images and their\ndescriptions in two different languages (which need not be parallel) by\nconsidering the image as a pivot between two languages. We introduce a new\npairwise ranking loss function which can handle both symmetric and asymmetric\nsimilarity between the two modalities. We evaluate our models on\nimage-description ranking for German and English, and on semantic textual\nsimilarity of image descriptions in English. In both cases we achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:08:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Gella", "Spandana", ""], ["Sennrich", "Rico", ""], ["Keller", "Frank", ""], ["Lapata", "Mirella", ""]]}, {"id": "1707.07605", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hosein Azarbonyad, Jaap Kamps, Maarten de Rijke", "title": "Share your Model instead of your Data: Privacy Preserving Mimic Learning\n  for Ranking", "comments": "SIGIR 2017 Workshop on Neural Information Retrieval\n  (Neu-IR'17)}{}{August 7--11, 2017, Shinjuku, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become a primary tool for solving problems in many\nfields. They are also used for addressing information retrieval problems and\nshow strong performance in several tasks. Training these models requires large,\nrepresentative datasets and for most IR tasks, such data contains sensitive\ninformation from users. Privacy and confidentiality concerns prevent many data\nowners from sharing the data, thus today the research community can only\nbenefit from research on large-scale datasets in a limited manner. In this\npaper, we discuss privacy preserving mimic learning, i.e., using predictions\nfrom a privacy preserving trained model instead of labels from the original\nsensitive training data as a supervision signal. We present the results of\npreliminary experiments in which we apply the idea of mimic learning and\nprivacy preserving mimic learning for the task of document re-ranking as one of\nthe core IR tasks. This research is a step toward laying the ground for\nenabling researchers from data-rich environments to share knowledge learned\nfrom actual users' data, which should facilitate research collaborations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:23:41 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Azarbonyad", "Hosein", ""], ["Kamps", "Jaap", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1707.07628", "submitter": "Yuanzhi Ke", "authors": "Yuanzhi Ke and Masafumi Hagiwara", "title": "Improve Lexicon-based Word Embeddings By Word Sense Disambiguation", "comments": "We submitted a highly overlapped one to Transactions of the Japanese\n  Society for Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been some works that learn a lexicon together with the corpus to\nimprove the word embeddings. However, they either model the lexicon separately\nbut update the neural networks for both the corpus and the lexicon by the same\nlikelihood, or minimize the distance between all of the synonym pairs in the\nlexicon. Such methods do not consider the relatedness and difference of the\ncorpus and the lexicon, and may not be the best optimized. In this paper, we\npropose a novel method that considers the relatedness and difference of the\ncorpus and the lexicon. It trains word embeddings by learning the corpus to\npredicate a word and its corresponding synonym under the context at the same\ntime. For polysemous words, we use a word sense disambiguation filter to\neliminate the synonyms that have different meanings for the context. To\nevaluate the proposed method, we compare the performance of the word embeddings\ntrained by our proposed model, the control groups without the filter or the\nlexicon, and the prior works in the word similarity tasks and text\nclassification task. The experimental results show that the proposed model\nprovides better embeddings for polysemous words and improves the performance\nfor text classification.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 16:07:01 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ke", "Yuanzhi", ""], ["Hagiwara", "Masafumi", ""]]}, {"id": "1707.07631", "submitter": "Rico Sennrich", "authors": "Antonio Valerio Miceli Barone and Jind\\v{r}ich Helcl and Rico Sennrich\n  and Barry Haddow and Alexandra Birch", "title": "Deep Architectures for Neural Machine Translation", "comments": "WMT 2017 research track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been shown that increasing model depth improves the quality of neural\nmachine translation. However, different architectural variants to increase\nmodel depth have been proposed, and so far, there has been no thorough\ncomparative study.\n  In this work, we describe and evaluate several existing approaches to\nintroduce depth in neural machine translation. Additionally, we explore novel\narchitectural variants, including deep transition RNNs, and we vary how\nattention is used in the deep decoder. We introduce a novel \"BiDeep\" RNN\narchitecture that combines deep transition RNNs and stacked RNNs.\n  Our evaluation is carried out on the English to German WMT news translation\ndataset, using a single-GPU machine for both training and inference. We find\nthat several of our proposed architectures improve upon existing approaches in\nterms of speed and translation quality. We obtain best improvements with a\nBiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU\nover a strong shallow baseline.\n  We release our code for ease of adoption.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 16:19:59 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""], ["Helcl", "Jind\u0159ich", ""], ["Sennrich", "Rico", ""], ["Haddow", "Barry", ""], ["Birch", "Alexandra", ""]]}, {"id": "1707.07660", "submitter": "Maarten De Rijke", "authors": "Dat Tien Nguyen, Shafiq Joty, Basma El Amel Boussaha, Maarten de Rijke", "title": "Thread Reconstruction in Conversational Data using Neural Coherence\n  Models", "comments": "Neu-IR: Workshop on Neural Information Retrieval 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion forums are an important source of information. They are often used\nto answer specific questions a user might have and to discover more about a\ntopic of interest. Discussions in these forums may evolve in intricate ways,\nmaking it difficult for users to follow the flow of ideas. We propose a novel\napproach for automatically identifying the underlying thread structure of a\nforum discussion. Our approach is based on a neural model that computes\ncoherence scores of possible reconstructions and then selects the highest\nscoring, i.e., the most coherent one. Preliminary experiments demonstrate\npromising results outperforming a number of strong baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 17:42:32 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 08:48:20 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Joty", "Shafiq", ""], ["Boussaha", "Basma El Amel", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1707.07678", "submitter": "Tobias Kuhn", "authors": "Tom Jansen and Tobias Kuhn", "title": "Extracting Core Claims from Scientific Articles", "comments": "In Post-proceedings of the 28th Benelux Conference on Artificial\n  Intelligence (BNAIC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of scientific articles has grown rapidly over the years and there\nare no signs that this growth will slow down in the near future. Because of\nthis, it becomes increasingly difficult to keep up with the latest developments\nin a scientific field. To address this problem, we present here an approach to\nhelp researchers learn about the latest developments and findings by extracting\nin a normalized form core claims from scientific articles. This normalized\nrepresentation is a controlled natural language of English sentences called\nAIDA, which has been proposed in previous work as a method to formally\nstructure and organize scientific findings and discourse. We show how such AIDA\nsentences can be automatically extracted by detecting the core claim of an\narticle, checking for AIDA compliance, and - if necessary - transforming it\ninto a compliant sentence. While our algorithm is still far from perfect, our\nresults indicate that the different steps are feasible and they support the\nclaim that AIDA sentences might be a promising approach to improve scientific\ncommunication in the future.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:10:40 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Jansen", "Tom", ""], ["Kuhn", "Tobias", ""]]}, {"id": "1707.07719", "submitter": "Heike Adel", "authors": "Heike Adel and Hinrich Sch\\\"utze", "title": "Global Normalization of Convolutional Neural Networks for Joint Entity\n  and Relation Classification", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce globally normalized convolutional neural networks for joint\nentity classification and relation extraction. In particular, we propose a way\nto utilize a linear-chain conditional random field output layer for predicting\nentity types and relations between entities at the same time. Our experiments\nshow that global normalization outperforms a locally normalized softmax layer\non a benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 19:39:22 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 06:37:55 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 11:15:29 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Adel", "Heike", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1707.07755", "submitter": "Miguel Ballesteros", "authors": "Miguel Ballesteros and Yaser Al-Onaizan", "title": "AMR Parsing using Stack-LSTMs", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transition-based AMR parser that directly generates AMR parses\nfrom plain text. We use Stack-LSTMs to represent our parser state and make\ndecisions greedily. In our experiments, we show that our parser achieves very\ncompetitive scores on English using only AMR training data. Adding additional\ninformation, such as POS tags and dependency trees, improves the results\nfurther.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 21:33:21 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 15:47:32 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ballesteros", "Miguel", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "1707.07792", "submitter": "Jimmy Lin", "authors": "Jinfeng Rao, Hua He, Haotian Zhang, Ferhan Ture, Royal Sequiera,\n  Salman Mohammed, and Jimmy Lin", "title": "Integrating Lexical and Temporal Signals in Neural Ranking Models for\n  Searching Social Media Streams", "comments": "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17),\n  August 7-11, 2017, Shinjuku, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time is an important relevance signal when searching streams of social media\nposts. The distribution of document timestamps from the results of an initial\nquery can be leveraged to infer the distribution of relevant documents, which\ncan then be used to rerank the initial results. Previous experiments have shown\nthat kernel density estimation is a simple yet effective implementation of this\nidea. This paper explores an alternative approach to mining temporal signals\nwith recurrent neural networks. Our intuition is that neural networks provide a\nmore expressive framework to capture the temporal coherence of neighboring\ndocuments in time. To our knowledge, we are the first to integrate lexical and\ntemporal signals in an end-to-end neural network architecture, in which\nexisting neural ranking models are used to generate query-document similarity\nvectors that feed into a bidirectional LSTM layer for temporal modeling. Our\nresults are mixed: existing neural models for document ranking alone yield\nlimited improvements over simple baselines, but the integration of lexical and\ntemporal signals yield significant improvements over competitive temporal\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 02:29:31 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Rao", "Jinfeng", ""], ["He", "Hua", ""], ["Zhang", "Haotian", ""], ["Ture", "Ferhan", ""], ["Sequiera", "Royal", ""], ["Mohammed", "Salman", ""], ["Lin", "Jimmy", ""]]}, {"id": "1707.07804", "submitter": "Jimmy Lin", "authors": "Royal Sequiera, Gaurav Baruah, Zhucheng Tu, Salman Mohammed, Jinfeng\n  Rao, Haotian Zhang, and Jimmy Lin", "title": "Exploring the Effectiveness of Convolutional Neural Networks for Answer\n  Selection in End-to-End Question Answering", "comments": "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17),\n  August 7-11, 2017, Shinjuku, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on natural language question answering today focuses on answer\nselection: given a candidate list of sentences, determine which contains the\nanswer. Although important, answer selection is only one stage in a standard\nend-to-end question answering pipeline. This paper explores the effectiveness\nof convolutional neural networks (CNNs) for answer selection in an end-to-end\ncontext using the standard TrecQA dataset. We observe that a simple\nidf-weighted word overlap algorithm forms a very strong baseline, and that\ndespite substantial efforts by the community in applying deep learning to\ntackle answer selection, the gains are modest at best on this dataset.\nFurthermore, it is unclear if a CNN is more effective than the baseline in an\nend-to-end context based on standard retrieval metrics. To further explore this\nfinding, we conducted a manual user evaluation, which confirms that answers\nfrom the CNN are detectably better than those from idf-weighted word overlap.\nThis result suggests that users are sensitive to relatively small differences\nin answer selection quality.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 03:38:57 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Sequiera", "Royal", ""], ["Baruah", "Gaurav", ""], ["Tu", "Zhucheng", ""], ["Mohammed", "Salman", ""], ["Rao", "Jinfeng", ""], ["Zhang", "Haotian", ""], ["Lin", "Jimmy", ""]]}, {"id": "1707.07806", "submitter": "Panupong Pasupat", "authors": "Yuchen Zhang, Panupong Pasupat and Percy Liang", "title": "Macro Grammars and Holistic Triggering for Efficient Semantic Parsing", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn a semantic parser from denotations, a learning algorithm must search\nover a combinatorially large space of logical forms for ones consistent with\nthe annotated denotations. We propose a new online learning algorithm that\nsearches faster as training progresses. The two key ideas are using macro\ngrammars to cache the abstract patterns of useful logical forms found thus far,\nand holistic triggering to efficiently retrieve the most relevant patterns\nbased on sentence similarity. On the WikiTableQuestions dataset, we first\nexpand the search space of an existing model to improve the state-of-the-art\naccuracy from 38.7% to 42.7%, and then use macro grammars and holistic\ntriggering to achieve an 11x speedup and an accuracy of 43.7%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 03:42:40 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 23:35:23 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zhang", "Yuchen", ""], ["Pasupat", "Panupong", ""], ["Liang", "Percy", ""]]}, {"id": "1707.07835", "submitter": "Ajinkya Kale", "authors": "Ajinkya Kale, Thrivikrama Taula, Sanjika Hewavitharana, Amit\n  Srivastava", "title": "Towards Semantic Query Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Segmentation is one of the critical components for understanding users'\nsearch intent in Information Retrieval tasks. It involves grouping tokens in\nthe search query into meaningful phrases which help downstream tasks like\nsearch relevance and query understanding. In this paper, we propose a novel\napproach to segment user queries using distributed query embeddings. Our key\ncontribution is a supervised approach to the segmentation task using\nlow-dimensional feature vectors for queries, getting rid of traditional hand\ntuned and heuristic NLP features which are quite expensive.\n  We benchmark on a 50,000 human-annotated web search engine query corpus\nachieving comparable accuracy to state-of-the-art techniques. The advantage of\nour technique is its fast and does not use external knowledge-base like\nWikipedia for score boosting. This helps us generalize our approach to other\ndomains like eCommerce without any fine-tuning. We demonstrate the\neffectiveness of this method on another 50,000 human-annotated eCommerce query\ncorpus from eBay search logs. Our approach is easy to implement and generalizes\nwell across different search domains proving the power of low-dimensional\nembeddings in query segmentation task, opening up a new direction of research\nfor this problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:57:39 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Kale", "Ajinkya", ""], ["Taula", "Thrivikrama", ""], ["Hewavitharana", "Sanjika", ""], ["Srivastava", "Amit", ""]]}, {"id": "1707.07847", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Hyperbolic Representation Learning for Fast and Efficient Neural\n  Question Answering", "comments": "Accepted at WSDM 2018", "journal-ref": null, "doi": "10.1145/3159652.3159664", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant neural architectures in question answer retrieval are based on\nrecurrent or convolutional encoders configured with complex word matching\nlayers. Given that recent architectural innovations are mostly new word\ninteraction layers or attention-based matching mechanisms, it seems to be a\nwell-established fact that these components are mandatory for good performance.\nUnfortunately, the memory and computation cost incurred by these complex\nmechanisms are undesirable for practical applications. As such, this paper\ntackles the question of whether it is possible to achieve competitive\nperformance with simple neural architectures. We propose a simple but novel\ndeep learning architecture for fast and efficient question-answer ranking and\nretrieval. More specifically, our proposed model, \\textsc{HyperQA}, is a\nparameter efficient neural network that outperforms other parameter intensive\nmodels such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple\nQA benchmarks. The novelty behind \\textsc{HyperQA} is a pairwise ranking\nobjective that models the relationship between question and answer embeddings\nin Hyperbolic space instead of Euclidean space. This empowers our model with a\nself-organizing ability and enables automatic discovery of latent hierarchies\nwhile learning embeddings of questions and answers. Our model requires no\nfeature engineering, no similarity matrix matching, no complicated attention\nmechanisms nor over-parameterized layers and yet outperforms and remains\ncompetitive to many models that have these functionalities on multiple\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 08:21:30 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 01:21:20 GMT"}, {"version": "v3", "created": "Thu, 23 Nov 2017 05:54:17 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1707.07911", "submitter": "Maxim Khalilov", "authors": "Pavel Levin, Nishikant Dhanuka, Maxim Khalilov", "title": "Machine Translation at Booking.com: Journey and Lessons Learned", "comments": "6 pages, 8 figures, 2 tables. In proceedings of EAMT 2017. Prague,\n  Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our recently developed neural machine translation (NMT) system\nand benchmark it against our own statistical machine translation (SMT) system\nas well as two other general purpose online engines (statistical and neural).\nWe present automatic and human evaluation results of the translation output\nprovided by each system. We also analyze the effect of sentence length on the\nquality of output for SMT and NMT systems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 10:51:19 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Levin", "Pavel", ""], ["Dhanuka", "Nishikant", ""], ["Khalilov", "Maxim", ""]]}, {"id": "1707.07922", "submitter": "Giuseppe Attardi", "authors": "Andrea Madotto and Giuseppe Attardi", "title": "Question Dependent Recurrent Entity Network for Question Answering", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering is a task which requires building models capable of\nproviding answers to questions expressed in human language. Full question\nanswering involves some form of reasoning ability. We introduce a neural\nnetwork architecture for this task, which is a form of $Memory\\ Network$, that\nrecognizes entities and their relations to answers through a focus attention\nmechanism. Our model is named $Question\\ Dependent\\ Recurrent\\ Entity\\ Network$\nand extends $Recurrent\\ Entity\\ Network$ by exploiting aspects of the question\nduring the memorization process. We validate the model on both synthetic and\nreal datasets: the $bAbI$ question answering dataset and the $CNN\\ \\&\\ Daily\\\nNews$ $reading\\ comprehension$ dataset. In our experiments, the models achieved\na State-of-The-Art in the former and competitive results in the latter.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 11:34:14 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 00:25:10 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Madotto", "Andrea", ""], ["Attardi", "Giuseppe", ""]]}, {"id": "1707.07930", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke and Evangelos Kanoulas", "title": "Structural Regularities in Text-based Entity Vector Spaces", "comments": "ICTIR2017. Proceedings of the 3rd ACM International Conference on the\n  Theory of Information Retrieval. 2017", "journal-ref": null, "doi": "10.1145/3121050.3121066", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity retrieval is the task of finding entities such as people or products\nin response to a query, based solely on the textual documents they are\nassociated with. Recent semantic entity retrieval algorithms represent queries\nand experts in finite-dimensional vector spaces, where both are constructed\nfrom text sequences.\n  We investigate entity vector spaces and the degree to which they capture\nstructural regularities. Such vector spaces are constructed in an unsupervised\nmanner without explicit information about structural aspects. For concreteness,\nwe address these questions for a specific type of entity: experts in the\ncontext of expert finding. We discover how clusterings of experts correspond to\ncommittees in organizations, the ability of expert representations to encode\nthe co-author graph, and the degree to which they encode academic rank. We\ncompare latent, continuous representations created using methods based on\ndistributional semantics (LSI), topic models (LDA) and neural networks\n(word2vec, doc2vec, SERT). Vector spaces created using neural methods, such as\ndoc2vec and SERT, systematically perform better at clustering than LSI, LDA and\nword2vec. When it comes to encoding entity relations, SERT performs best.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 11:54:19 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1707.08041", "submitter": "Michael Filhol", "authors": "Michael Filhol, Gilles Falquet", "title": "Synthesising Sign Language from semantics, approaching \"from the target\n  and back\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Sign Language modelling approach allowing to build grammars and\ncreate linguistic input for Sign synthesis through avatars. We comment on the\ntype of grammar it allows to build, and observe a resemblance between the\nresulting expressions and traditional semantic representations. Comparing the\nways in which the paradigms are designed, we name and contrast two essentially\ndifferent strategies for building higher-level linguistic input:\n\"source-and-forward\" vs. \"target-and-back\". We conclude by favouring the\nlatter, acknowledging the power of being able to automatically generate output\nfrom semantically relevant input straight into articulations of the target\nlanguage.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:28:59 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Filhol", "Michael", ""], ["Falquet", "Gilles", ""]]}, {"id": "1707.08052", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Stuart M. Shieber, Alexander M. Rush", "title": "Challenges in Data-to-Document Generation", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:42:25 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wiseman", "Sam", ""], ["Shieber", "Stuart M.", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1707.08081", "submitter": "Guy Rosin", "authors": "Guy D. Rosin, Eytan Adar, Kira Radinsky", "title": "Learning Word Relatedness over Time", "comments": "11 pages, EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search systems are often focused on providing relevant results for the \"now\",\nassuming both corpora and user needs that focus on the present. However, many\ncorpora today reflect significant longitudinal collections ranging from 20\nyears of the Web to hundreds of years of digitized newspapers and books.\nUnderstanding the temporal intent of the user and retrieving the most relevant\nhistorical content has become a significant challenge. Common search features,\nsuch as query expansion, leverage the relationship between terms but cannot\nfunction well across all times when relationships vary temporally. In this\nwork, we introduce a temporal relationship model that is extracted from\nlongitudinal data collections. The model supports the task of identifying,\ngiven two words, when they relate to each other. We present an algorithmic\nframework for this task and show its application for the task of query\nexpansion, achieving high gain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:41:49 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 17:03:35 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Rosin", "Guy D.", ""], ["Adar", "Eytan", ""], ["Radinsky", "Kira", ""]]}, {"id": "1707.08084", "submitter": "Radu Tudor Ionescu", "authors": "Andrei M. Butnaru, Radu Tudor Ionescu, Florentina Hristea", "title": "ShotgunWSD: An unsupervised algorithm for global word sense\n  disambiguation inspired by DNA sequencing", "comments": "In Proceedings of EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel unsupervised algorithm for word sense\ndisambiguation (WSD) at the document level. Our algorithm is inspired by a\nwidely-used approach in the field of genetics for whole genome sequencing,\nknown as the Shotgun sequencing technique. The proposed WSD algorithm is based\non three main steps. First, a brute-force WSD algorithm is applied to short\ncontext windows (up to 10 words) selected from the document in order to\ngenerate a short list of likely sense configurations for each window. In the\nsecond step, these local sense configurations are assembled into longer\ncomposite configurations based on suffix and prefix matching. The resulted\nconfigurations are ranked by their length, and the sense of each word is chosen\nbased on a voting scheme that considers only the top k configurations in which\nthe word appears. We compare our algorithm with other state-of-the-art\nunsupervised WSD algorithms and demonstrate better performance, sometimes by a\nvery large margin. We also show that our algorithm can yield better performance\nthan the Most Common Sense (MCS) baseline on one data set. Moreover, our\nalgorithm has a very small number of parameters, is robust to parameter tuning,\nand, unlike other bio-inspired methods, it gives a deterministic solution (it\ndoes not involve random choices).\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:56:53 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Butnaru", "Andrei M.", ""], ["Ionescu", "Radu Tudor", ""], ["Hristea", "Florentina", ""]]}, {"id": "1707.08098", "submitter": "Radu Tudor Ionescu", "authors": "Andrei M. Butnaru and Radu Tudor Ionescu", "title": "From Image to Text Classification: A Novel Approach based on Clustering\n  Word Embeddings", "comments": "Accepted at KES 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for text classification based on\nclustering word embeddings, inspired by the bag of visual words model, which is\nwidely used in computer vision. After each word in a collection of documents is\nrepresented as word vector using a pre-trained word embeddings model, a k-means\nalgorithm is applied on the word vectors in order to obtain a fixed-size set of\nclusters. The centroid of each cluster is interpreted as a super word embedding\nthat embodies all the semantically related word vectors in a certain region of\nthe embedding space. Every embedded word in the collection of documents is then\nassigned to the nearest cluster centroid. In the end, each document is\nrepresented as a bag of super word embeddings by computing the frequency of\neach super word embedding in the respective document. We also diverge from the\nidea of building a single vocabulary for the entire collection of documents,\nand propose to build class-specific vocabularies for better performance. Using\nthis kind of representation, we report results on two text mining tasks, namely\ntext categorization by topic and polarity classification. On both tasks, our\nmodel yields better performance than the standard bag of words.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:29:18 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Butnaru", "Andrei M.", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1707.08139", "submitter": "Jacob Andreas", "authors": "Jacob Andreas and Dan Klein", "title": "Analogs of Linguistic Structure in Deep Representations", "comments": "In EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the compositional structure of message vectors computed by a\ndeep network trained on a communication game. By comparing truth-conditional\nrepresentations of encoder-produced message vectors to human-produced referring\nexpressions, we are able to identify aligned (vector, utterance) pairs with the\nsame meaning. We then search for structured relationships among these aligned\npairs to discover simple vector space transformations corresponding to\nnegation, conjunction, and disjunction. Our results suggest that neural\nrepresentations are capable of spontaneously developing a \"syntax\" with\nfunctional analogues to qualitative properties of natural language.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:10:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Andreas", "Jacob", ""], ["Klein", "Dan", ""]]}, {"id": "1707.08172", "submitter": "Nikita Nangia", "authors": "Nikita Nangia, Adina Williams, Angeliki Lazaridou, Samuel R. Bowman", "title": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference\n  with Sentence Representations", "comments": "10 pages, 1 figure, 6 tables, in Proceedings of The Second Workshop\n  on Evaluating Vector Space Representations for NLP (RepEval 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results of the RepEval 2017 Shared Task, which\nevaluated neural network sentence representation learning models on the\nMulti-Genre Natural Language Inference corpus (MultiNLI) recently introduced by\nWilliams et al. (2017). All of the five participating teams beat the\nbidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in\nWilliams et al.. The best single model used stacked BiLSTMs with residual\nconnections to extract sentence features and reached 74.5% accuracy on the\ngenre-matched test set. Surprisingly, the results of the competition were\nfairly consistent across the genre-matched and genre-mismatched test sets, and\nacross subsets of the test data representing a variety of linguistic phenomena,\nsuggesting that all of the submitted systems learned reasonably\ndomain-independent representations for sentence meaning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 19:27:05 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Nangia", "Nikita", ""], ["Williams", "Adina", ""], ["Lazaridou", "Angeliki", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1707.08209", "submitter": "Jaydeep Chipalkatti", "authors": "Jaydeep Chipalkatti, Mihir Kulkarni", "title": "On the letter frequencies and entropy of written Marathi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carry out a comprehensive analysis of letter frequencies in contemporary\nwritten Marathi. We determine sets of letters which statistically predominate\nany large generic Marathi text, and use these sets to estimate the entropy of\nMarathi.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 19:52:56 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Chipalkatti", "Jaydeep", ""], ["Kulkarni", "Mihir", ""]]}, {"id": "1707.08214", "submitter": "Fr\\'ederic Godin", "authors": "Fr\\'ederic Godin, Jonas Degrave, Joni Dambre, Wesley De Neve", "title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\n  Functions in Quasi-Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2018.09.006", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\nunbounded positive and negative image, can be used as a drop-in replacement for\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\nprone to the vanishing gradient problem, they are noise robust, and they induce\nsparse activations.\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\nlanguage modeling. Additionally, we evaluate on character-level language\nmodeling, showing that we are able to stack up to eight QRNN layers with\nDReLUs, thus making it possible to improve the current state-of-the-art in\ncharacter-level language modeling over shallow architectures based on LSTMs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:52:32 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 15:50:57 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Godin", "Fr\u00e9deric", ""], ["Degrave", "Jonas", ""], ["Dambre", "Joni", ""], ["De Neve", "Wesley", ""]]}, {"id": "1707.08290", "submitter": "Ramon Ferrer i Cancho", "authors": "Antoni Lozano, Bernardino Casas, Chris Bentz and Ramon Ferrer-i-Cancho", "title": "Fast calculation of entropy with Zhang's estimator", "comments": null, "journal-ref": "Issues in Quantitative Linguistics 4. E. Kelih, R. Knight, J.\n  Macutek and A. Wilson (eds.). No. 23 of the series \"Studies in Quantitative\n  Linguistics\". L\\\"udenscheid: RAM-Verlag. pp. 273-285", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy is a fundamental property of a repertoire. Here, we present an\nefficient algorithm to estimate the entropy of types with the help of Zhang's\nestimator. The algorithm takes advantage of the fact that the number of\ndifferent frequencies in a text is in general much smaller than the number of\ntypes. We justify the convenience of the algorithm by means of an analysis of\nthe statistical properties of texts from more than 1000 languages. Our work\nopens up various possibilities for future research.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 05:11:58 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Lozano", "Antoni", ""], ["Casas", "Bernardino", ""], ["Bentz", "Chris", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1707.08309", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee", "title": "Probabilistic Graphical Models for Credibility Analysis in Evolving\n  Online Communities", "comments": "PhD thesis, Mar 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major hurdles preventing the full exploitation of information from\nonline communities is the widespread concern regarding the quality and\ncredibility of user-contributed content. Prior works in this domain operate on\na static snapshot of the community, making strong assumptions about the\nstructure of the data (e.g., relational tables), or consider only shallow\nfeatures for text classification.\n  To address the above limitations, we propose probabilistic graphical models\nthat can leverage the joint interplay between multiple factors in online\ncommunities --- like user interactions, community dynamics, and textual content\n--- to automatically assess the credibility of user-contributed online content,\nand the expertise of users and their evolution with user-interpretable\nexplanation. To this end, we devise new models based on Conditional Random\nFields for different settings like incorporating partial expert knowledge for\nsemi-supervised learning, and handling discrete labels as well as numeric\nratings for fine-grained analysis. This enables applications such as extracting\nreliable side-effects of drugs from user-contributed posts in healthforums, and\nidentifying credible content in news communities.\n  Online communities are dynamic, as users join and leave, adapt to evolving\ntrends, and mature over time. To capture this dynamics, we propose generative\nmodels based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian\nMotion to trace the continuous evolution of user expertise and their language\nmodel over time. This allows us to identify expert users and credible content\njointly over time, improving state-of-the-art recommender systems by explicitly\nconsidering the maturity of users. This also enables applications such as\nidentifying helpful product reviews, and detecting fake and anomalous reviews\nwith limited information.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 07:41:27 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Mukherjee", "Subhabrata", ""]]}, {"id": "1707.08349", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu and Marius Popescu", "title": "Can string kernels pass the test of time in Native Language\n  Identification?", "comments": "In Proceedings of the 12th Workshop on Building Educational\n  Applications Using NLP, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a machine learning approach for the 2017 shared task on Native\nLanguage Identification (NLI). The proposed approach combines several kernels\nusing multiple kernel learning. While most of our kernels are based on\ncharacter p-grams (also known as n-grams) extracted from essays or speech\ntranscripts, we also use a kernel based on i-vectors, a low-dimensional\nrepresentation of audio recordings, provided by the shared task organizers. For\nthe learning stage, we choose Kernel Discriminant Analysis (KDA) over Kernel\nRidge Regression (KRR), because the former classifier obtains better results\nthan the latter one on the development set. In our previous work, we have used\na similar machine learning approach to achieve state-of-the-art NLI results.\nThe goal of this paper is to demonstrate that our shallow and simple approach\nbased on string kernels (with minor improvements) can pass the test of time and\nreach state-of-the-art performance in the 2017 NLI shared task, despite the\nrecent advances in natural language processing. We participated in all three\ntracks, in which the competitors were allowed to use only the essays (essay\ntrack), only the speech transcripts (speech track), or both (fusion track).\nUsing only the data provided by the organizers for training our models, we have\nreached a macro F1 score of 86.95% in the closed essay track, a macro F1 score\nof 87.55% in the closed speech track, and a macro F1 score of 93.19% in the\nclosed fusion track. With these scores, our team (UnibucKernel) ranked in the\nfirst group of teams in all three tracks, while attaining the best scores in\nthe speech and the fusion tracks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:03:40 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 14:52:49 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Popescu", "Marius", ""]]}, {"id": "1707.08435", "submitter": "William Havard", "authors": "William Havard, Laurent Besacier, Olivier Rosec", "title": "SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO\n  Data Set", "comments": "Data set available on https://zenodo.org/record/4282267. Presented at\n  GLU (Grounded Language Understanding) Satellite Workshop of Interspeech 2017", "journal-ref": null, "doi": "10.21437/GLU.2017-9", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an augmentation of MSCOCO dataset where speech is added\nto image and text. Speech captions are generated using text-to-speech (TTS)\nsynthesis resulting in 616,767 spoken captions (more than 600h) paired with\nimages. Disfluencies and speed perturbation are added to the signal in order to\nsound more natural. Each speech signal (WAV) is paired with a JSON file\ncontaining exact timecode for each word/syllable/phoneme in the spoken caption.\nSuch a corpus could be used for Language and Vision (LaVi) tasks including\nspeech input or output instead of text. Investigating multimodal learning\nschemes for unsupervised speech pattern discovery is also possible with this\ncorpus, as demonstrated by a preliminary study conducted on a subset of the\ncorpus (10h, 10k spoken captions). The dataset is available on Zenodo:\nhttps://zenodo.org/record/4282267\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 13:40:21 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 09:07:03 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 19:14:27 GMT"}, {"version": "v4", "created": "Tue, 26 Feb 2019 16:32:37 GMT"}, {"version": "v5", "created": "Mon, 23 Nov 2020 16:16:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Havard", "William", ""], ["Besacier", "Laurent", ""], ["Rosec", "Olivier", ""]]}, {"id": "1707.08446", "submitter": "Jasabanta Patro", "authors": "Jasabanta Patro, Bidisha Samanta, Saurabh Singh, Abhipsa Basu,\n  Prithwish Mukherjee, Monojit Choudhury, Animesh Mukherjee", "title": "All that is English may be Hindi: Enhancing language identification\n  through automatic ranking of likeliness of word borrowing in social media", "comments": "11 pages, accepted in the 2017 conference on Empirical Methods on\n  Natural Language Processing(EMNLP 2017) arXiv admin note: substantial text\n  overlap with arXiv:1703.05122", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a set of computational methods to identify the\nlikeliness of a word being borrowed, based on the signals from social media. In\nterms of Spearman correlation coefficient values, our methods perform more than\ntwo times better (nearly 0.62) in predicting the borrowing likeliness compared\nto the best performing baseline (nearly 0.26) reported in literature. Based on\nthis likeliness estimate we asked annotators to re-annotate the language tags\nof foreign words in predominantly native contexts. In 88 percent of cases the\nannotators felt that the foreign language tag should be replaced by native\nlanguage tag, thus indicating a huge scope for improvement of automatic\nlanguage identification systems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 04:17:42 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 04:47:16 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Patro", "Jasabanta", ""], ["Samanta", "Bidisha", ""], ["Singh", "Saurabh", ""], ["Basu", "Abhipsa", ""], ["Mukherjee", "Prithwish", ""], ["Choudhury", "Monojit", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1707.08458", "submitter": "Ekaterina Vylomova", "authors": "Ekaterina Vylomova, Andrei Shcherbakov, Yuriy Philippovich, Galina\n  Cherkasova", "title": "Men Are from Mars, Women Are from Venus: Evaluation and Modelling of\n  Verbal Associations", "comments": "AIST 2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a quantitative analysis of human word association pairs and study\nthe types of relations presented in the associations. We put our main focus on\nthe correlation between response types and respondent characteristics such as\noccupation and gender by contrasting syntagmatic and paradigmatic associations.\nFinally, we propose a personalised distributed word association model and show\nthe importance of incorporating demographic factors into the models commonly\nused in natural language processing.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 14:20:12 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Vylomova", "Ekaterina", ""], ["Shcherbakov", "Andrei", ""], ["Philippovich", "Yuriy", ""], ["Cherkasova", "Galina", ""]]}, {"id": "1707.08470", "submitter": "Amit Sheth", "authors": "Sujan Perera, Pablo N. Mendes, Adarsh Alex, Amit Sheth, Krishnaprasad\n  Thirunarayan", "title": "Implicit Entity Linking in Tweets", "comments": "This paper was accepted at the Extended Semantic Web Conference 2016\n  as a full research track paper", "journal-ref": null, "doi": "10.1007/978-3-319-34129-3_8", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, Twitter has become one of the largest communication platforms\nproviding key data to various applications such as brand monitoring, trend\ndetection, among others. Entity linking is one of the major tasks in natural\nlanguage understanding from tweets and it associates entity mentions in text to\ncorresponding entries in knowledge bases in order to provide unambiguous\ninterpretation and additional con- text. State-of-the-art techniques have\nfocused on linking explicitly mentioned entities in tweets with reasonable\nsuccess. However, we argue that in addition to explicit mentions i.e. The movie\nGravity was more ex- pensive than the mars orbiter mission entities (movie\nGravity) can also be mentioned implicitly i.e. This new space movie is crazy.\nyou must watch it!. This paper introduces the problem of implicit entity\nlinking in tweets. We propose an approach that models the entities by\nexploiting their factual and contextual knowledge. We demonstrate how to use\nthese models to perform implicit entity linking on a ground truth dataset with\n397 tweets from two domains, namely, Movie and Book. Specifically, we show: 1)\nthe importance of linking implicit entities and its value addition to the\nstandard entity linking task, and 2) the importance of exploiting contextual\nknowledge associated with an entity for linking their implicit mentions. We\nalso make the ground truth dataset publicly available to foster the research in\nthis new research area.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 14:36:58 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Perera", "Sujan", ""], ["Mendes", "Pablo N.", ""], ["Alex", "Adarsh", ""], ["Sheth", "Amit", ""], ["Thirunarayan", "Krishnaprasad", ""]]}, {"id": "1707.08559", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Joon Lee, Mohit Bansal, Alexander C. Berg", "title": "Video Highlight Prediction Using Audience Chat Reactions", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:44:38 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Lee", "Joon", ""], ["Bansal", "Mohit", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1707.08588", "submitter": "Yikang Shen", "authors": "Yikang Shen, Shawn Tan, Chrisopher Pal and Aaron Courville", "title": "Self-organized Hierarchical Softmax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 18:01:32 GMT"}], "update_date": "2017-07-29", "authors_parsed": [["Shen", "Yikang", ""], ["Tan", "Shawn", ""], ["Pal", "Chrisopher", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.08608", "submitter": "Jay Yoon Lee", "authors": "Jay Yoon Lee, Sanket Vaibhav Mehta, Michael Wick, Jean-Baptiste\n  Tristan, Jaime Carbonell", "title": "Gradient-based Inference for Networks with Output Constraints", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners apply neural networks to increasingly complex problems in\nnatural language processing, such as syntactic parsing and semantic role\nlabeling that have rich output structures. Many such structured-prediction\nproblems require deterministic constraints on the output values; for example,\nin sequence-to-sequence syntactic parsing, we require that the sequential\noutputs encode valid trees. While hidden units might capture such properties,\nthe network is not always able to learn such constraints from the training data\nalone, and practitioners must then resort to post-processing. In this paper, we\npresent an inference method for neural networks that enforces deterministic\nconstraints on outputs without performing rule-based post-processing or\nexpensive discrete search. Instead, in the spirit of gradient-based training,\nwe enforce constraints with gradient-based inference (GBI): for each input at\ntest-time, we nudge continuous model weights until the network's unconstrained\ninference procedure generates an output that satisfies the constraints. We\nstudy the efficacy of GBI on three tasks with hard constraints: semantic role\nlabeling, syntactic parsing, and sequence transduction. In each case, the\nalgorithm not only satisfies constraints but improves accuracy, even when the\nunderlying network is state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 19:00:10 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 17:14:27 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 15:24:11 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Lee", "Jay Yoon", ""], ["Mehta", "Sanket Vaibhav", ""], ["Wick", "Michael", ""], ["Tristan", "Jean-Baptiste", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1707.08616", "submitter": "Mark Riedl", "authors": "Brent Harrison, Upol Ehsan, Mark O. Riedl", "title": "Guiding Reinforcement Learning Exploration Using Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a technique to use natural language to help\nreinforcement learning generalize to unseen environments. This technique uses\nneural machine translation, specifically the use of encoder-decoder networks,\nto learn associations between natural language behavior descriptions and\nstate-action information. We then use this learned model to guide agent\nexploration using a modified version of policy shaping to make it more\neffective at learning in unseen environments. We evaluate this technique using\nthe popular arcade game, Frogger, under ideal and non-ideal conditions. This\nevaluation shows that our modified policy shaping algorithm improves over a\nQ-learning agent as well as a baseline version of policy shaping.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 19:23:54 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 02:06:26 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Harrison", "Brent", ""], ["Ehsan", "Upol", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1707.08660", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov, Erik Velldal, Lilja {\\O}vrelid", "title": "Temporal dynamics of semantic relations in word embeddings: an\n  application to predicting armed conflict participants", "comments": "to appear in EMNLP 2017 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with using word embedding models to trace the temporal\ndynamics of semantic relations between pairs of words. The set-up is similar to\nthe well-known analogies task, but expanded with a time dimension. To this end,\nwe apply incremental updating of the models with new training texts, including\nincremental vocabulary expansion, coupled with learned transformation matrices\nthat let us map between members of the relation. The proposed approach is\nevaluated on the task of predicting insurgent armed groups based on\ngeographical locations. The gold standard data for the time span 1994--2010 is\nextracted from the UCDP Armed Conflicts dataset. The results show that the\nmethod is feasible and outperforms the baselines, but also that important work\nstill remains to be done.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 23:02:11 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Velldal", "Erik", ""], ["\u00d8vrelid", "Lilja", ""]]}, {"id": "1707.08668", "submitter": "Siddharth Karamcheti", "authors": "Siddharth Karamcheti, Edward C. Williams, Dilip Arumugam, Mina Rhee,\n  Nakul Gopalan, Lawson L. S. Wong, and Stefanie Tellex", "title": "A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting\n  Action-Oriented and Goal-Oriented Instructions", "comments": "Accepted at the 1st Workshop on Language Grounding for Robotics at\n  ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots operating alongside humans in diverse, stochastic environments must be\nable to accurately interpret natural language commands. These instructions\noften fall into one of two categories: those that specify a goal condition or\ntarget state, and those that specify explicit actions, or how to perform a\ngiven task. Recent approaches have used reward functions as a semantic\nrepresentation of goal-based commands, which allows for the use of a\nstate-of-the-art planner to find a policy for the given task. However, these\nreward functions cannot be directly used to represent action-oriented commands.\nWe introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding\nNetwork (DRAGGN), for task grounding and execution that handles natural\nlanguage from either category as input, and generalizes to unseen environments.\nOur robot-simulation results demonstrate that a system successfully\ninterpreting both goal-oriented and action-oriented task specifications brings\nus closer to robust natural language understanding for human-robot interaction.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 23:57:29 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Karamcheti", "Siddharth", ""], ["Williams", "Edward C.", ""], ["Arumugam", "Dilip", ""], ["Rhee", "Mina", ""], ["Gopalan", "Nakul", ""], ["Wong", "Lawson L. S.", ""], ["Tellex", "Stefanie", ""]]}, {"id": "1707.08713", "submitter": "Hitomi Yanaka", "authors": "Hitomi Yanaka, Koji Mineshima, Pascual Martinez-Gomez, Daisuke Bekki", "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "comments": "11 pages, 5 figures, accepted as long paper of EMNLP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining semantic textual similarity is a core research subject in natural\nlanguage processing. Since vector-based models for sentence representation\noften use shallow information, capturing accurate semantics is difficult. By\ncontrast, logical semantic representations capture deeper levels of sentence\nsemantics, but their symbolic nature does not offer graded notions of textual\nsimilarity. We propose a method for determining semantic textual similarity by\ncombining shallow features with features extracted from natural deduction\nproofs of bidirectional entailment relations between sentence pairs. For the\nnatural deduction proofs, we use ccg2lambda, a higher-order automatic inference\nsystem, which converts Combinatory Categorial Grammar (CCG) derivation trees\ninto semantic representations and conducts natural deduction proofs.\nExperiments show that our system was able to outperform other logic-based\nsystems and that features derived from the proofs are effective for learning\ntextual similarity.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 05:49:51 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Yanaka", "Hitomi", ""], ["Mineshima", "Koji", ""], ["Martinez-Gomez", "Pascual", ""], ["Bekki", "Daisuke", ""]]}, {"id": "1707.08783", "submitter": "Rocco Tripodi", "authors": "Rocco Tripodi, Stefano Li Pira", "title": "Analysis of Italian Word Embeddings", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we analyze the performances of two of the most used word\nembeddings algorithms, skip-gram and continuous bag of words on Italian\nlanguage. These algorithms have many hyper-parameter that have to be carefully\ntuned in order to obtain accurate word representation in vectorial space. We\nprovide an accurate analysis and an evaluation, showing what are the best\nconfiguration of parameters for specific tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 08:56:29 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 12:37:43 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 09:56:35 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 08:57:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tripodi", "Rocco", ""], ["Pira", "Stefano Li", ""]]}, {"id": "1707.08852", "submitter": "Varun Gangal", "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, Eduard Hovy", "title": "Detecting and Explaining Causes From Text For a Time Series Event", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining underlying causes or effects about events is a challenging but\nvaluable task. We define a novel problem of generating explanations of a time\nseries event by (1) searching cause and effect relationships of the time series\nwith textual data and (2) constructing a connecting chain between them to\ngenerate an explanation. To detect causal features from text, we propose a\nnovel method based on the Granger causality of time series between features\nextracted from text such as N-grams, topics, sentiments, and their composition.\nThe generation of the sequence of causal entities requires a commonsense\ncausative knowledge base with efficient reasoning. To ensure good\ninterpretability and appropriate lexical usage we combine symbolic and neural\nrepresentations, using a neural reasoning algorithm trained on commonsense\ncausal tuples to predict the next cause step. Our quantitative and human\nanalysis show empirical evidence that our method successfully extracts\nmeaningful causality relationships between time series with textual features\nand generates appropriate explanation between them.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 13:14:57 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Kang", "Dongyeop", ""], ["Gangal", "Varun", ""], ["Lu", "Ang", ""], ["Chen", "Zheng", ""], ["Hovy", "Eduard", ""]]}, {"id": "1707.08866", "submitter": "YiYao Huang", "authors": "Yi Yao Huang, William Yang Wang", "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "comments": "Accepted by EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual learning (ResNet) is a new method for training very deep neural\nnetworks using identity map-ping for shortcut connections. ResNet has won the\nImageNet ILSVRC 2015 classification task, and achieved state-of-the-art\nperformances in many computer vision tasks. However, the effect of residual\nlearning on noisy natural language processing tasks is still not well\nunderstood. In this paper, we design a novel convolutional neural network (CNN)\nwith residual learning, and investigate its impacts on the task of distantly\nsupervised noisy relation extraction. In contradictory to popular beliefs that\nResNet only works well for very deep networks, we found that even with 9 layers\nof CNNs, using identity mapping could significantly improve the performance for\ndistantly-supervised relation extraction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 13:56:36 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Huang", "Yi Yao", ""], ["Wang", "William Yang", ""]]}, {"id": "1707.08939", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Strawman: an Ensemble of Deep Bag-of-Ngrams for Sentiment Analysis", "comments": "A builder entry to the sentence-level sentiment analysis task of the\n  \"Build It, Break It\" shared task of the First Workshop on Building\n  Linguistically Generalizable NLP Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a builder entry, named \"strawman\", to the sentence-level\nsentiment analysis task of the \"Build It, Break It\" shared task of the First\nWorkshop on Building Linguistically Generalizable NLP Systems. The goal of a\nbuilder is to provide an automated sentiment analyzer that would serve as a\ntarget for breakers whose goal is to find pairs of minimally-differing\nsentences that break the analyzer.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:30:57 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1707.08976", "submitter": "Mitchell Stern", "authors": "Mitchell Stern, Daniel Fried, Dan Klein", "title": "Effective Inference for Generative Neural Parsing", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural models have recently achieved state-of-the-art results for\nconstituency parsing. However, without a feasible search procedure, their use\nhas so far been limited to reranking the output of external parsers in which\ndecoding is more tractable. We describe an alternative to the conventional\naction-level beam search used for discriminative neural models that enables us\nto decode directly in these generative models. We then show that by improving\nour basic candidate selection strategy and using a coarse pruning function, we\ncan improve accuracy while exploring significantly less of the search space.\nApplied to the model of Choe and Charniak (2016), our inference procedure\nobtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior\nstate-of-the-art results for single-model systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 18:01:18 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Stern", "Mitchell", ""], ["Fried", "Daniel", ""], ["Klein", "Dan", ""]]}, {"id": "1707.08998", "submitter": "Imane Guellil", "authors": "Im\\`ene Guellil (ESI), Fai\\c{c}al Azouaou (ESI)", "title": "ASDA : Analyseur Syntaxique du Dialecte Alg{\\'e}rien dans un but\n  d'analyse s{\\'e}mantique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining and sentiment analysis in social media is a research issue\nhaving a great interest in the scientific community. However, before begin this\nanalysis, we are faced with a set of problems. In particular, the problem of\nthe richness of languages and dialects within these media. To address this\nproblem, we propose in this paper an approach of construction and\nimplementation of Syntactic analyzer named ASDA. This tool represents a parser\nfor the Algerian dialect that label the terms of a given corpus. Thus, we\nconstruct a labeling table containing for each term its stem, different\nprefixes and suffixes, allowing us to determine the different grammatical parts\na sort of POS tagging. This labeling will serve us later in the semantic\nprocessing of the Algerian dialect, like the automatic translation of this\ndialect or sentiment analysis\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 07:48:46 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Guellil", "Im\u00e8ne", "", "ESI"], ["Azouaou", "Fai\u00e7al", "", "ESI"]]}, {"id": "1707.09050", "submitter": "Julia Kreutzer", "authors": "Artem Sokolov, Julia Kreutzer, Kellen Sunderland, Pavel Danchenko,\n  Witold Szymaniak, Hagen F\\\"urstenau, Stefan Riezler", "title": "A Shared Task on Bandit Learning for Machine Translation", "comments": "Conference on Machine Translation (WMT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and describe the results of a novel shared task on bandit\nlearning for machine translation. The task was organized jointly by Amazon and\nHeidelberg University for the first time at the Second Conference on Machine\nTranslation (WMT 2017). The goal of the task is to encourage research on\nlearning machine translation from weak user feedback instead of human\nreferences or post-edits. On each of a sequence of rounds, a machine\ntranslation system is required to propose a translation for an input, and\nreceives a real-valued estimate of the quality of the proposed translation for\nlearning. This paper describes the shared task's learning and evaluation setup,\nusing services hosted on Amazon Web Services (AWS), the data and evaluation\nmetrics, and the results of various machine translation architectures and\nlearning protocols.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 21:16:41 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Sokolov", "Artem", ""], ["Kreutzer", "Julia", ""], ["Sunderland", "Kellen", ""], ["Danchenko", "Pavel", ""], ["Szymaniak", "Witold", ""], ["F\u00fcrstenau", "Hagen", ""], ["Riezler", "Stefan", ""]]}, {"id": "1707.09067", "submitter": "Allen Schmaltz", "authors": "Allen Schmaltz, Yoon Kim, Alexander M. Rush, Stuart M. Shieber", "title": "Adapting Sequence Models for Sentence Correction", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a controlled experiment of sequence-to-sequence approaches for the task of\nsentence correction, we find that character-based models are generally more\neffective than word-based models and models that encode subword information via\nconvolutions, and that modeling the output data as a series of diffs improves\neffectiveness over standard approaches. Our strongest sequence-to-sequence\nmodel improves over our strongest phrase-based statistical machine translation\nmodel, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally,\nin the data environment of the standard CoNLL-2014 setup, we demonstrate that\nmodeling (and tuning against) diffs yields similar or better M2 scores with\nsimpler models and/or significantly less data than previous\nsequence-to-sequence approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 22:50:55 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Schmaltz", "Allen", ""], ["Kim", "Yoon", ""], ["Rush", "Alexander M.", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1707.09098", "submitter": "Boyuan Pan", "authors": "Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai, Xiaofei He", "title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine\n  Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine comprehension(MC) style question answering is a representative\nproblem in natural language processing. Previous methods rarely spend time on\nthe improvement of encoding layer, especially the embedding of syntactic\ninformation and name entity of the words, which are very crucial to the quality\nof encoding. Moreover, existing attention methods represent each query word as\na vector or use a single vector to represent the whole query sentence, neither\nof them can handle the proper weight of the key words in query sentence. In\nthis paper, we introduce a novel neural network architecture called Multi-layer\nEmbedding with Memory Network(MEMEN) for machine reading task. In the encoding\nlayer, we employ classic skip-gram model to the syntactic and semantic\ninformation of the words to train a new kind of embedding layer. We also\npropose a memory network of full-orientation matching of the query and passage\nto catch more pivotal information. Experiments show that our model has\ncompetitive results both from the perspectives of precision and efficiency in\nStanford Question Answering Dataset(SQuAD) among all published results and\nachieves the state-of-the-art results on TriviaQA dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 03:41:18 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Pan", "Boyuan", ""], ["Li", "Hao", ""], ["Zhao", "Zhou", ""], ["Cao", "Bin", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "1707.09118", "submitter": "Carolin Lawrence", "authors": "Carolin Lawrence, Artem Sokolov, Stefan Riezler", "title": "Counterfactual Learning from Bandit Feedback under Deterministic\n  Logging: A Case Study in Statistical Machine Translation", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2017, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of counterfactual learning for statistical machine translation (SMT)\nis to optimize a target SMT system from logged data that consist of user\nfeedback to translations that were predicted by another, historic SMT system. A\nchallenge arises by the fact that risk-averse commercial SMT systems\ndeterministically log the most probable translation. The lack of sufficient\nexploration of the SMT output space seemingly contradicts the theoretical\nrequirements for counterfactual learning. We show that counterfactual learning\nfrom deterministic bandit logs is possible nevertheless by smoothing out\ndeterministic components in learning. This can be achieved by additive and\nmultiplicative control variates that avoid degenerate behavior in empirical\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\npoints by counterfactual learning from deterministic bandit feedback.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:32:47 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 13:22:23 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 13:44:31 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Lawrence", "Carolin", ""], ["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""]]}, {"id": "1707.09168", "submitter": "Bingfeng Luo", "authors": "Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang and Dongyan Zhao", "title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "comments": "10 pages, accepted by EMNLP 2017", "journal-ref": null, "doi": "10.18653/v1/D17-1289", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The charge prediction task is to determine appropriate charges for a given\ncase, which is helpful for legal assistant systems where the user input is fact\ndescription. We argue that relevant law articles play an important role in this\ntask, and therefore propose an attention-based neural network method to jointly\nmodel the charge prediction task and the relevant article extraction task in a\nunified framework. The experimental results show that, besides providing legal\nbasis, the relevant articles can also clearly improve the charge prediction\nresults, and our full model can effectively predict appropriate charges for\ncases with different expression styles.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 09:46:29 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Luo", "Bingfeng", ""], ["Feng", "Yansong", ""], ["Xu", "Jianbo", ""], ["Zhang", "Xiang", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1707.09231", "submitter": "Sabrina Stehwien", "authors": "Ina R\\\"osiger, Sabrina Stehwien, Arndt Riester, Ngoc Thang Vu", "title": "Improving coreference resolution with automatically predicted prosodic\n  information", "comments": "1st Workshop on Speech-Centric Natural Language Processing (SCNLP) at\n  EMNLP 2017; 6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adding manually annotated prosodic information, specifically pitch accents\nand phrasing, to the typical text-based feature set for coreference resolution\nhas previously been shown to have a positive effect on German data. Practical\napplications on spoken language, however, would rely on automatically predicted\nprosodic information. In this paper we predict pitch accents (and phrase\nboundaries) using a convolutional neural network (CNN) model from acoustic\nfeatures extracted from the speech signal. After an assessment of the quality\nof these automatic prosodic annotations, we show that they also significantly\nimprove coreference resolution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 13:49:46 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["R\u00f6siger", "Ina", ""], ["Stehwien", "Sabrina", ""], ["Riester", "Arndt", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1707.09406", "submitter": "Wenlin Yao", "authors": "Wenlin Yao, Zeyu Dai, Ruihong Huang, James Caverlee", "title": "Online Deception Detection Refueled by Real World Data Collection", "comments": "10 pages, Accepted to Recent Advances in Natural Language Processing\n  (RANLP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of large realistic datasets presents a bottleneck in online\ndeception detection studies. In this paper, we apply a data collection method\nbased on social network analysis to quickly identify high-quality deceptive and\ntruthful online reviews from Amazon. The dataset contains more than 10,000\ndeceptive reviews and is diverse in product domains and reviewers. Using this\ndataset, we explore effective general features for online deception detection\nthat perform well across domains. We demonstrate that with generalized features\n- advertising speak and writing complexity scores - deception detection\nperformance can be further improved by adding additional deceptive reviews from\nassorted domains in training. Finally, reviewer level evaluation gives an\ninteresting insight into different deceptive reviewers' writing styles.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 20:27:35 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Yao", "Wenlin", ""], ["Dai", "Zeyu", ""], ["Huang", "Ruihong", ""], ["Caverlee", "James", ""]]}, {"id": "1707.09410", "submitter": "Wenlin Yao", "authors": "Wenlin Yao, Saipravallika Nettyam, Ruihong Huang", "title": "A Weakly Supervised Approach to Train Temporal Relation Classifiers and\n  Acquire Regular Event Pairs Simultaneously", "comments": "10 pages, Recent Advances in Natural Language Processing (RANLP),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capabilities of detecting temporal relations between two events can benefit\nmany applications. Most of existing temporal relation classifiers were trained\nin a supervised manner. Instead, we explore the observation that regular event\npairs show a consistent temporal relation despite of their various contexts,\nand these rich contexts can be used to train a contextual temporal relation\nclassifier, which can further recognize new temporal relation contexts and\nidentify new regular event pairs. We focus on detecting after and before\ntemporal relations and design a weakly supervised learning approach that\nextracts thousands of regular event pairs and learns a contextual temporal\nrelation classifier simultaneously. Evaluation shows that the acquired regular\nevent pairs are of high quality and contain rich commonsense knowledge and\ndomain specific knowledge. In addition, the weakly supervised trained temporal\nrelation classifier achieves comparable performance with the state-of-the-art\nsupervised systems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 20:38:15 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Yao", "Wenlin", ""], ["Nettyam", "Saipravallika", ""], ["Huang", "Ruihong", ""]]}, {"id": "1707.09443", "submitter": "Ulrich Germann", "authors": "Ulrich Germann", "title": "Bilingual Document Alignment with Latent Semantic Indexing", "comments": "Proceedings of the First Conference on Machine Translation (2016),\n  Volume 2: Shared Task Papers", "journal-ref": null, "doi": "10.5281/zenodo.834343", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We apply cross-lingual Latent Semantic Indexing to the Bilingual Document\nAlignment Task at WMT16. Reduced-rank singular value decomposition of a\nbilingual term-document matrix derived from known English/French page pairs in\nthe training data allows us to map monolingual documents into a joint semantic\nspace. Two variants of cosine similarity between the vectors that place each\ndocument into the joint semantic space are combined with a measure of string\nsimilarity between corresponding URLs to produce 1:1 alignments of\nEnglish/French web pages in a variety of domains. The system achieves a recall\nof ca. 88% if no in-domain data is used for building the latent semantic model,\nand 93% if such data is included.\n  Analysing the system's errors on the training data, we argue that evaluating\naligner performance based on exact URL matches under-estimates their true\nperformance and propose an alternative that is able to account for duplicates\nand near-duplicates in the underlying data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 00:46:23 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Germann", "Ulrich", ""]]}, {"id": "1707.09448", "submitter": "Vineet John", "authors": "Vineet John, Olga Vechtomova", "title": "Sentiment Analysis on Financial News Headlines using Training Dataset\n  Augmentation", "comments": "5 pages", "journal-ref": "Association for Computational Linguistics, Proceedings of the 11th\n  International Workshop on Semantic Evaluation (SemEval-2017), 869-873", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the approach taken by the UWaterloo team to arrive at a\nsolution for the Fine-Grained Sentiment Analysis problem posed by Task 5 of\nSemEval 2017. The paper describes the document vectorization and sentiment\nscore prediction techniques used, as well as the design and implementation\ndecisions taken while building the system for this task. The system uses text\nvectorization models, such as N-gram, TF-IDF and paragraph embeddings, coupled\nwith regression model variants to predict the sentiment scores. Amongst the\nmethods examined, unigrams and bigrams coupled with simple linear regression\nobtained the best baseline accuracy. The paper also explores data augmentation\nmethods to supplement the training dataset. This system was designed for\nSubtask 2 (News Statements and Headlines).\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 01:47:53 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["John", "Vineet", ""], ["Vechtomova", "Olga", ""]]}, {"id": "1707.09457", "submitter": "Kai-Wei Chang", "authors": "Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and\n  Kai-Wei Chang", "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints", "comments": "11 pages, published in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 03:38:32 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Zhao", "Jieyu", ""], ["Wang", "Tianlu", ""], ["Yatskar", "Mark", ""], ["Ordonez", "Vicente", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1707.09468", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Yejin Choi", "title": "Zero-Shot Activity Recognition with Verb Attribute Induction", "comments": "accepted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate large-scale zero-shot activity recognition by\nmodeling the visual and linguistic attributes of action verbs. For example, the\nverb \"salute\" has several properties, such as being a light movement, a social\nact, and short in duration. We use these attributes as the internal mapping\nbetween visual and textual representations to reason about a previously unseen\naction. In contrast to much prior work that assumes access to gold standard\nattributes for zero-shot classes and focuses primarily on object attributes,\nour model uniquely learns to infer action attributes from dictionary\ndefinitions and distributed word representations. Experimental results confirm\nthat action attributes inferred from language can provide a predictive signal\nfor zero-shot prediction of previously unseen activities.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 06:05:52 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 19:53:20 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zellers", "Rowan", ""], ["Choi", "Yejin", ""]]}, {"id": "1707.09491", "submitter": "Slava Mikhaylov", "authors": "Stefano Gurciullo and Slava Mikhaylov", "title": "Topology Analysis of International Networks Based on Debates in the\n  United Nations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CG math.AT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex, high dimensional and unstructured data it is often difficult to\nextract meaningful patterns. This is especially the case when dealing with\ntextual data. Recent studies in machine learning, information theory and\nnetwork science have developed several novel instruments to extract the\nsemantics of unstructured data, and harness it to build a network of relations.\nSuch approaches serve as an efficient tool for dimensionality reduction and\npattern detection. This paper applies semantic network science to extract\nideological proximity in the international arena, by focusing on the data from\nGeneral Debates in the UN General Assembly on the topics of high salience to\ninternational community. UN General Debate corpus (UNGDC) covers all high-level\ndebates in the UN General Assembly from 1970 to 2014, covering all UN member\nstates. The research proceeds in three main steps. First, Latent Dirichlet\nAllocation (LDA) is used to extract the topics of the UN speeches, and\ntherefore semantic information. Each country is then assigned a vector\nspecifying the exposure to each of the topics identified. This intermediate\noutput is then used in to construct a network of countries based on information\ntheoretical metrics where the links capture similar vectorial patterns in the\ntopic distributions. Topology of the networks is then analyzed through network\nproperties like density, path length and clustering. Finally, we identify\nspecific topological features of our networks using the map equation framework\nto detect communities in our networks of countries.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 10:09:04 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Gurciullo", "Stefano", ""], ["Mikhaylov", "Slava", ""]]}, {"id": "1707.09533", "submitter": "Tom Kocmi", "authors": "Tom Kocmi and Ondrej Bojar", "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine\n  Translation", "comments": "Accepted to RANLP 2017", "journal-ref": "Proceedings of the International Conference Recent Advances in\n  Natural Language Processing, RANLP 2017", "doi": "10.26615/978-954-452-049-6_050", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effects of particular orderings of sentence pairs on the\non-line training of neural machine translation (NMT). We focus on two types of\nsuch orderings: (1) ensuring that each minibatch contains sentences similar in\nsome aspect and (2) gradual inclusion of some sentence types as the training\nprogresses (so called \"curriculum learning\"). In our English-to-Czech\nexperiments, the internal homogeneity of minibatches has no effect on the\ntraining but some of our \"curricula\" achieve a small improvement over the\nbaseline.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 15:58:55 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kocmi", "Tom", ""], ["Bojar", "Ondrej", ""]]}, {"id": "1707.09538", "submitter": "Soujanya Poria", "authors": "Erik Cambria, Devamanyu Hazarika, Soujanya Poria, Amir Hussain, R.B.V.\n  Subramaanyam", "title": "Benchmarking Multimodal Sentiment Analysis", "comments": "Accepted in CICLing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework for multimodal sentiment analysis and emotion\nrecognition using convolutional neural network-based feature extraction from\ntext and visual modalities. We obtain a performance improvement of 10% over the\nstate of the art by combining visual, text and audio features. We also discuss\nsome major issues frequently ignored in multimodal sentiment analysis research:\nthe role of speaker-independent models, importance of the modalities and\ngeneralizability. The paper thus serve as a new benchmark for further research\nin multimodal sentiment analysis and also demonstrates the different facets of\nanalysis to be considered while performing such tasks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 16:40:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cambria", "Erik", ""], ["Hazarika", "Devamanyu", ""], ["Poria", "Soujanya", ""], ["Hussain", "Amir", ""], ["Subramaanyam", "R. B. V.", ""]]}, {"id": "1707.09569", "submitter": "Chaitanya Malaviya", "authors": "Chaitanya Malaviya, Graham Neubig, Patrick Littell", "title": "Learning Language Representations for Typology Prediction", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One central mystery of neural NLP is what neural models \"know\" about their\nsubject matter. When a neural machine translation system learns to translate\nfrom one language to another, does it learn the syntax or semantics of the\nlanguages? Can this knowledge be extracted from the system to fill holes in\nhuman scientific knowledge? Existing typological databases contain relatively\nfull feature specifications for only a few hundred languages. Exploiting the\nexistence of parallel texts in more than a thousand languages, we build a\nmassive many-to-one neural machine translation (NMT) system from 1017 languages\ninto English, and use this to predict information missing from typological\ndatabases. Experiments show that the proposed method is able to infer not only\nsyntactic, but also phonological and phonetic inventory features, and improves\nover a baseline that has access to information about the languages' geographic\nand phylogenetic neighbors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 23:38:25 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Malaviya", "Chaitanya", ""], ["Neubig", "Graham", ""], ["Littell", "Patrick", ""]]}, {"id": "1707.09611", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk", "title": "Joint Named Entity Recognition and Stance Detection in Tweets", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a well-established task of information\nextraction which has been studied for decades. More recently, studies reporting\nNER experiments on social media texts have emerged. On the other hand, stance\ndetection is a considerably new research topic usually considered within the\nscope of sentiment analysis. Stance detection studies are mostly applied to\ntexts of online debates where the stance of the text owner for a particular\ntarget, either explicitly or implicitly mentioned in text, is explored. In this\nstudy, we investigate the possible contribution of named entities to the stance\ndetection task in tweets. We report the evaluation results of NER experiments\nas well as that of the subsequent stance detection experiments using named\nentities, on a publicly-available stance-annotated data set of tweets. Our\nresults indicate that named entities obtained with a high-performance NER\nsystem can contribute to stance detection performance on tweets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 11:34:40 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""]]}, {"id": "1707.09751", "submitter": "Van-Duyet Le", "authors": "Le Van-Duyet, Vo Minh Quan, Dang Quang An", "title": "Skill2vec: Machine Learning Approach for Determining the Relevant Skills\n  from Job Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervise learned word embeddings have seen tremendous success in numerous\nNatural Language Processing (NLP) tasks in recent years. The main contribution\nof this paper is to develop a technique called Skill2vec, which applies machine\nlearning techniques in recruitment to enhance the search strategy to find\ncandidates possessing the appropriate skills. Skill2vec is a neural network\narchitecture inspired by Word2vec, developed by Mikolov et al. in 2013. It\ntransforms skills to new vector space, which has the characteristics of\ncalculation and presents skills relationships. We conducted an experiment\nevaluation manually by a recruitment company's domain experts to demonstrate\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 08:10:49 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 10:47:24 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 16:39:33 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Van-Duyet", "Le", ""], ["Quan", "Vo Minh", ""], ["An", "Dang Quang", ""]]}, {"id": "1707.09769", "submitter": "Ottokar Tilk", "authors": "Ottokar Tilk and Tanel Alum\\\"ae", "title": "Low-Resource Neural Headline Generation", "comments": "Accepted to EMNLP 2017 Workshop on New Frontiers in Summarization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural headline generation models have shown great results, but are\ngenerally trained on very large datasets. We focus our efforts on improving\nheadline quality on smaller datasets by the means of pretraining. We propose\nnew methods that enable pre-training all the parameters of the model and\nutilize all available text, resulting in improvements by up to 32.4% relative\nin perplexity and 2.84 points in ROUGE.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 08:56:32 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Tilk", "Ottokar", ""], ["Alum\u00e4e", "Tanel", ""]]}, {"id": "1707.09816", "submitter": "Natalia Loukachevitch", "authors": "Natalia Loukachevitch and Michael Nokel and Kirill Ivanov", "title": "Combining Thesaurus Knowledge and Probabilistic Topic Models", "comments": "Accepted to AIST-2017 conference (http://aistconf.ru/). The final\n  publication will be available at link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the approach of introducing thesaurus knowledge into\nprobabilistic topic models. The main idea of the approach is based on the\nassumption that the frequencies of semantically related words and phrases,\nwhich are met in the same texts, should be enhanced: this action leads to their\nlarger contribution into topics found in these texts. We have conducted\nexperiments with several thesauri and found that for improving topic models, it\nis useful to utilize domain-specific knowledge. If a general thesaurus, such as\nWordNet, is used, the thesaurus-based improvement of topic models can be\nachieved with excluding hyponymy relations in combined topic models.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 12:32:16 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Loukachevitch", "Natalia", ""], ["Nokel", "Michael", ""], ["Ivanov", "Kirill", ""]]}, {"id": "1707.09823", "submitter": "Chen Li", "authors": "Di Jiang, Zeyu Chen, Rongzhong Lian, Siqi Bao, Chen Li", "title": "Familia: An Open-Source Toolkit for Industrial Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Familia is an open-source toolkit for pragmatic topic modeling in industry.\nFamilia abstracts the utilities of topic modeling in industry as two paradigms:\nsemantic representation and semantic matching. Efficient implementations of the\ntwo paradigms are made publicly available for the first time. Furthermore, we\nprovide off-the-shelf topic models trained on large-scale industrial corpora,\nincluding Latent Dirichlet Allocation (LDA), SentenceLDA and Topical Word\nEmbedding (TWE). We further describe typical applications which are\nsuccessfully powered by topic modeling, in order to ease the confusions and\ndifficulties of software engineers during topic model selection and\nutilization.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 12:48:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Jiang", "Di", ""], ["Chen", "Zeyu", ""], ["Lian", "Rongzhong", ""], ["Bao", "Siqi", ""], ["Li", "Chen", ""]]}, {"id": "1707.09861", "submitter": "Nils Reimers", "authors": "Nils Reimers and Iryna Gurevych", "title": "Reporting Score Distributions Makes a Difference: Performance Study of\n  LSTM-networks for Sequence Tagging", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we show that reporting a single performance score is\ninsufficient to compare non-deterministic approaches. We demonstrate for common\nsequence tagging tasks that the seed value for the random number generator can\nresult in statistically significant (p < 10^-4) differences for\nstate-of-the-art systems. For two recent systems for NER, we observe an\nabsolute difference of one percentage point F1-score depending on the selected\nseed value, making these systems perceived either as state-of-the-art or\nmediocre. Instead of publishing and reporting single performance scores, we\npropose to compare score distributions based on multiple executions. Based on\nthe evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we\npresent network architectures that produce both superior performance as well as\nare more stable with respect to the remaining hyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:25:24 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1707.09872", "submitter": "Armand Vilalta", "authors": "Armand Vilalta, Dario Garcia-Gasulla, Ferran Par\\'es, Eduard\n  Ayguad\\'e, Jesus Labarta, Ulises Cort\\'es, Toyotaro Suzumura", "title": "Full-Network Embedding in a Multimodal Embedding Pipeline", "comments": "In 2nd Workshop on Semantic Deep Learning (SemDeep-2) at the 12th\n  International Conference on Computational Semantics (IWCS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art for image annotation and image retrieval tasks\nis obtained through deep neural networks, which combine an image representation\nand a text representation into a shared embedding space. In this paper we\nevaluate the impact of using the Full-Network embedding in this setting,\nreplacing the original image representation in a competitive multimodal\nembedding generation scheme. Unlike the one-layer image embeddings typically\nused by most approaches, the Full-Network embedding provides a multi-scale\nrepresentation of images, which results in richer characterizations. To measure\nthe influence of the Full-Network embedding, we evaluate its performance on\nthree different datasets, and compare the results with the original multimodal\nembedding generation scheme when using a one-layer image embedding, and with\nthe rest of the state-of-the-art. Results for image annotation and image\nretrieval tasks indicate that the Full-Network embedding is consistently\nsuperior to the one-layer embedding. These results motivate the integration of\nthe Full-Network embedding on any multimodal embedding generation scheme,\nsomething feasible thanks to the flexibility of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 10:27:33 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:11:42 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Vilalta", "Armand", ""], ["Garcia-Gasulla", "Dario", ""], ["Par\u00e9s", "Ferran", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jesus", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1707.09879", "submitter": "Duygu Ataman", "authors": "Duygu Ataman, Matteo Negri, Marco Turchi, Marcello Federico", "title": "Linguistically Motivated Vocabulary Reduction for Neural Machine\n  Translation from Turkish to English", "comments": "The 20th Annual Conference of the European Association for Machine\n  Translation (EAMT), Research Paper, 12 pages", "journal-ref": "The Prague Bulletin of Mathematical Linguistics. No. 108, 2017,\n  pp. 331-342", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The necessity of using a fixed-size word vocabulary in order to control the\nmodel complexity in state-of-the-art neural machine translation (NMT) systems\nis an important bottleneck on performance, especially for morphologically rich\nlanguages. Conventional methods that aim to overcome this problem by using\nsub-word or character-level representations solely rely on statistics and\ndisregard the linguistic properties of words, which leads to interruptions in\nthe word structure and causes semantic and syntactic losses. In this paper, we\npropose a new vocabulary reduction method for NMT, which can reduce the\nvocabulary of a given input corpus at any rate while also considering the\nmorphological properties of the language. Our method is based on unsupervised\nmorphology learning and can be, in principle, used for pre-processing any\nlanguage pair. We also present an alternative word segmentation method based on\nsupervised morphological analysis, which aids us in measuring the accuracy of\nour model. We evaluate our method in Turkish-to-English NMT task where the\ninput language is morphologically rich and agglutinative. We analyze different\nrepresentation methods in terms of translation accuracy as well as the semantic\nand syntactic properties of the generated output. Our method obtains a\nsignificant improvement of 2.3 BLEU points over the conventional vocabulary\nreduction technique, showing that it can provide better accuracy in open\nvocabulary translation of morphologically rich languages.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:31:01 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Ataman", "Duygu", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""], ["Federico", "Marcello", ""]]}, {"id": "1707.09920", "submitter": "Rico Sennrich", "authors": "Antonio Valerio Miceli Barone and Barry Haddow and Ulrich Germann and\n  Rico Sennrich", "title": "Regularization techniques for fine-tuning in neural machine translation", "comments": "EMNLP 2017 short paper; for bibtex, see\n  http://homepages.inf.ed.ac.uk/rsennric/bib.html#micelibarone2017b", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate techniques for supervised domain adaptation for neural machine\ntranslation where an existing model trained on a large out-of-domain dataset is\nadapted to a small in-domain dataset. In this scenario, overfitting is a major\nchallenge. We investigate a number of techniques to reduce overfitting and\nimprove transfer learning, including regularization techniques such as dropout\nand L2-regularization towards an out-of-domain prior. In addition, we introduce\ntuneout, a novel regularization technique inspired by dropout. We apply these\ntechniques, alone and in combination, to neural machine translation, obtaining\nimprovements on IWSLT datasets for English->German and English->Russian. We\nalso investigate the amounts of in-domain training data needed for domain\nadaptation in NMT, and find a logarithmic relationship between the amount of\ntraining data and gain in BLEU score.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 15:31:12 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""], ["Haddow", "Barry", ""], ["Germann", "Ulrich", ""], ["Sennrich", "Rico", ""]]}]