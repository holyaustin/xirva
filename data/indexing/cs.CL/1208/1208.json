[{"id": "1208.0200", "submitter": "Asma Boudhief", "authors": "Asma Boudhief, Mohsen Maraoui and Mounir Zrigui", "title": "Adaptation of pedagogical resources description standard (LOM) with the\n  specificity of Arabic language", "comments": "8 pages,10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1206.2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we focus firstly on the principle of pedagogical indexing and\ncharacteristics of Arabic language and secondly on the possibility of adapting\nthe standard for describing learning resources used (the LOM and its\nApplication Profiles) with learning conditions such as the educational levels\nof students and their levels of understanding,... the educational context with\ntaking into account the representative elements of text, text length, ... in\nparticular, we put in relief the specificity of the Arabic language which is a\ncomplex language, characterized by its flexion, its voyellation and\nagglutination.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 13:06:54 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Boudhief", "Asma", ""], ["Maraoui", "Mohsen", ""], ["Zrigui", "Mounir", ""]]}, {"id": "1208.2777", "submitter": "Hyonil Kim", "authors": "Hyonil Kim and Changil Choe", "title": "A Method for Selecting Noun Sense using Co-occurrence Relation in\n  English-Korean Translation", "comments": null, "journal-ref": "Serdica Journal of Computing, Vol.'6 No.4, 2012, pp. 401-408", "doi": null, "report-no": "KISU-MATH-2012-E-R-004", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sense analysis is still critical problem in machine translation system,\nespecially such as English-Korean translation which the syntactical different\nbetween source and target languages is very great. We suggest a method for\nselecting the noun sense using contextual feature in English-Korean\nTranslation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 03:25:33 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Kim", "Hyonil", ""], ["Choe", "Changil", ""]]}, {"id": "1208.2873", "submitter": "Vasileios Lampos", "authors": "Vasileios Lampos", "title": "Detecting Events and Patterns in Large-Scale User Generated Textual\n  Streams with Statistical Learning Methods", "comments": "PhD thesis, 238 pages, 9 chapters, 2 appendices, 58 figures, 49\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A vast amount of textual web streams is influenced by events or phenomena\nemerging in the real world. The social web forms an excellent modern paradigm,\nwhere unstructured user generated content is published on a regular basis and\nin most occasions is freely distributed. The present Ph.D. Thesis deals with\nthe problem of inferring information - or patterns in general - about events\nemerging in real life based on the contents of this textual stream. We show\nthat it is possible to extract valuable information about social phenomena,\nsuch as an epidemic or even rainfall rates, by automatic analysis of the\ncontent published in Social Media, and in particular Twitter, using Statistical\nMachine Learning methods. An important intermediate task regards the formation\nand identification of features which characterise a target event; we select and\nuse those textual features in several linear, non-linear and hybrid inference\napproaches achieving a significantly good performance in terms of the applied\nloss function. By examining further this rich data set, we also propose methods\nfor extracting various types of mood signals revealing how affective norms - at\nleast within the social web's population - evolve during the day and how\nsignificant events emerging in the real world are influencing them. Lastly, we\npresent some preliminary findings showing several spatiotemporal\ncharacteristics of this textual information as well as the potential of using\nit to tackle tasks such as the prediction of voting intentions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:59:54 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Lampos", "Vasileios", ""]]}, {"id": "1208.3001", "submitter": "Zhili Chen Dr.", "authors": "Zhili Chen, Liusheng Huang, Wei Yang, Peng Meng, and Haibo Miao", "title": "More than Word Frequencies: Authorship Attribution via Natural Frequency\n  Zoned Word Distribution Analysis", "comments": "27pages, 7figures, submited to Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With such increasing popularity and availability of digital text data,\nauthorships of digital texts can not be taken for granted due to the ease of\ncopying and parsing. This paper presents a new text style analysis called\nnatural frequency zoned word distribution analysis (NFZ-WDA), and then a basic\nauthorship attribution scheme and an open authorship attribution scheme for\ndigital texts based on the analysis. NFZ-WDA is based on the observation that\nall authors leave distinct intrinsic word usage traces on texts written by them\nand these intrinsic styles can be identified and employed to analyze the\nauthorship. The intrinsic word usage styles can be estimated through the\nanalysis of word distribution within a text, which is more than normal word\nfrequency analysis and can be expressed as: which groups of words are used in\nthe text; how frequently does each group of words occur; how are the\noccurrences of each group of words distributed in the text. Next, the basic\nauthorship attribution scheme and the open authorship attribution scheme\nprovide solutions for both closed and open authorship attribution problems.\nThrough analysis and extensive experimental studies, this paper demonstrates\nthe efficiency of the proposed method for authorship attribution.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 00:53:39 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Chen", "Zhili", ""], ["Huang", "Liusheng", ""], ["Yang", "Wei", ""], ["Meng", "Peng", ""], ["Miao", "Haibo", ""]]}, {"id": "1208.3047", "submitter": "Edi Winarko", "authors": "Arif Nurwidyantoro and Edi Winarko", "title": "Parallelization of Maximum Entropy POS Tagging for Bahasa Indonesia with\n  MapReduce", "comments": "International Journal of Computer Science Issues (IJCSI), Vol. 9,\n  Issue 4, No 2, July 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, MapReduce programming model is used to parallelize training\nand tagging proceess in Maximum Entropy part of speech tagging for Bahasa\nIndonesia. In training process, MapReduce model is implemented dictionary,\ntagtoken, and feature creation. In tagging process, MapReduce is implemented to\ntag lines of document in parallel. The training experiments showed that total\ntraining time using MapReduce is faster, but its result reading time inside the\nprocess slow down the total training time. The tagging experiments using\ndifferent number of map and reduce process showed that MapReduce implementation\ncould speedup the tagging process. The fastest tagging result is showed by\ntagging process using 1,000,000 word corpus and 30 map process.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 07:16:14 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Nurwidyantoro", "Arif", ""], ["Winarko", "Edi", ""]]}, {"id": "1208.3530", "submitter": "Haimonti Dutta", "authors": "Haimonti Dutta, William Chan, Deepak Shankargouda, Manoj Pooleery,\n  Axinia Radeva, Kyle Rego, Boyi Xie, Rebecca Passonneau, Austin Lee and\n  Barbara Taranto", "title": "Leveraging Subjective Human Annotation for Clustering Historic Newspaper\n  Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The New York Public Library is participating in the Chronicling America\ninitiative to develop an online searchable database of historically significant\nnewspaper articles. Microfilm copies of the newspapers are scanned and high\nresolution Optical Character Recognition (OCR) software is run on them. The\ntext from the OCR provides a wealth of data and opinion for researchers and\nhistorians. However, categorization of articles provided by the OCR engine is\nrudimentary and a large number of the articles are labeled editorial without\nfurther grouping. Manually sorting articles into fine-grained categories is\ntime consuming if not impossible given the size of the corpus. This paper\nstudies techniques for automatic categorization of newspaper articles so as to\nenhance search and retrieval on the archive. We explore unsupervised (e.g.\nKMeans) and semi-supervised (e.g. constrained clustering) learning algorithms\nto develop article categorization schemes geared towards the needs of\nend-users. A pilot study was designed to understand whether there was unanimous\nagreement amongst patrons regarding how articles can be categorized. It was\nfound that the task was very subjective and consequently automated algorithms\nthat could deal with subjective labels were used. While the small scale pilot\nstudy was extremely helpful in designing machine learning algorithms, a much\nlarger system needs to be developed to collect annotations from users of the\narchive. The \"BODHI\" system currently being developed is a step in that\ndirection, allowing users to correct wrongly scanned OCR and providing keywords\nand tags for newspaper articles used frequently. On successful implementation\nof the beta version of this system, we hope that it can be integrated with\nexisting software being developed for the Chronicling America project.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 04:48:58 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Dutta", "Haimonti", ""], ["Chan", "William", ""], ["Shankargouda", "Deepak", ""], ["Pooleery", "Manoj", ""], ["Radeva", "Axinia", ""], ["Rego", "Kyle", ""], ["Xie", "Boyi", ""], ["Passonneau", "Rebecca", ""], ["Lee", "Austin", ""], ["Taranto", "Barbara", ""]]}, {"id": "1208.4079", "submitter": "Nishal Shah Mr.", "authors": "Nishal Pradeepkumar Shah", "title": "Recent Technological Advances in Natural Language Processing and\n  Artificial Intelligence", "comments": "6 pages,0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent advance in computer technology has permitted scientists to implement\nand test algorithms that were known from quite some time (or not) but which\nwere computationally expensive. Two such projects are IBM's Jeopardy as a part\nof its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods\nimplement natural language processing (another goal of AI scientists) and try\nto answer questions as asked by the user. Though the goal of the two projects\nis similar, both of them have a different procedure at it's core. In the\nfollowing sections, the mechanism and history of IBM's Jeopardy and Wolfram\nalpha has been explained followed by the implications of these projects in\nrealizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe\nof taking the above projects to a new level is also explained.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 18:34:27 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Shah", "Nishal Pradeepkumar", ""]]}, {"id": "1208.4503", "submitter": "Gueddah Hicham", "authors": "Gueddah Hicham", "title": "Introduction of the weight edition errors in the Levenshtein distance", "comments": "3 pages, 5 figures; International Journal of Advanced Research in\n  Artificial Intelligence (IJARAI)2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach dedicated to correcting the spelling\nerrors of the Arabic language. This approach corrects typographical errors like\ninserting, deleting, and permutation. Our method is inspired from the\nLevenshtein algorithm, and allows a finer and better scheduling than\nLevenshtein. The results obtained are very satisfactory and encouraging, which\nshows the interest of our new approach.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 14:11:07 GMT"}], "update_date": "2012-08-23", "authors_parsed": [["Hicham", "Gueddah", ""]]}, {"id": "1208.6109", "submitter": "Vladimir Bochkarev", "authors": "Vladimir V. Bochkarev, Anna V. Shevlyakova, Valery D. Solovyev", "title": "Average word length dynamics as indicator of cultural changes in society", "comments": "16 pages, 9 figures", "journal-ref": "Social Evolution & History. Volume 14, number 2, p. 153-175 (2015)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamics of average length of words in Russian and English is analysed in the\narticle. Words belonging to the diachronic text corpus Google Books Ngram and\ndated back to the last two centuries are studied. It was found out that average\nword length slightly increased in the 19th century, and then it was growing\nrapidly most of the 20th century and started decreasing over the period from\nthe end of the 20th - to the beginning of the 21th century. Words which\ncontributed mostly to increase or decrease of word average length were\nidentified. At that, content words and functional words are analysed\nseparately. Long content words contribute mostly to word average length of\nword. As it was shown, these words reflect the main tendencies of social\ndevelopment and thus, are used frequently. Change of frequency of personal\npronouns also contributes significantly to change of average word length. The\nother parameters connected with average length of word were also analysed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 08:30:32 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Bochkarev", "Vladimir V.", ""], ["Shevlyakova", "Anna V.", ""], ["Solovyev", "Valery D.", ""]]}, {"id": "1208.6268", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty", "title": "Authorship Identification in Bengali Literature: a Comparative Analysis", "comments": "9 pages, 5 tables, 4 pictures", "journal-ref": "Chakraborty, T., Authorship Identification in Bengali Literature:\n  a Comparative Analysis, Proceedings of COLING 2012: Demonstration Papers,\n  December, 2012, pp. 41-50", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stylometry is the study of the unique linguistic styles and writing behaviors\nof individuals. It belongs to the core task of text categorization like\nauthorship identification, plagiarism detection etc. Though reasonable number\nof studies have been conducted in English language, no major work has been done\nso far in Bengali. In this work, We will present a demonstration of authorship\nidentification of the documents written in Bengali. We adopt a set of\nfine-grained stylistic features for the analysis of the text and use them to\ndevelop two different models: statistical similarity model consisting of three\nmeasures and their combination, and machine learning model with Decision Tree,\nNeural Network and SVM. Experimental results show that SVM outperforms other\nstate-of-the-art methods after 10-fold cross validations. We also validate the\nrelative importance of each stylistic feature to show that some of them remain\nconsistently significant in every model used in this experiment.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 19:09:24 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2012 19:34:18 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2012 17:08:33 GMT"}, {"version": "v4", "created": "Sun, 24 Feb 2013 09:27:56 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Chakraborty", "Tanmoy", ""]]}]