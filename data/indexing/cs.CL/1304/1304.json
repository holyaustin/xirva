[{"id": "1304.0104", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Jan Broekaert, Sandro Sozzo and Tomas Veloz", "title": "Meaning-focused and Quantum-inspired Information Retrieval", "comments": "11 pages", "journal-ref": "Quantum Interaction. Lecture Notes in Computer Science, 8369, pp.\n  71-83, 2014", "doi": "10.1007/978-3-642-54943-4_7", "report-no": null, "categories": "cs.IR cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, quantum-based methods have promisingly integrated the\ntraditional procedures in information retrieval (IR) and natural language\nprocessing (NLP). Inspired by our research on the identification and\napplication of quantum structures in cognition, more specifically our work on\nthe representation of concepts and their combinations, we put forward a\n'quantum meaning based' framework for structured query retrieval in text\ncorpora and standardized testing corpora. This scheme for IR rests on\nconsidering as basic notions, (i) 'entities of meaning', e.g., concepts and\ntheir combinations and (ii) traces of such entities of meaning, which is how\ndocuments are considered in this approach. The meaning content of these\n'entities of meaning' is reconstructed by solving an 'inverse problem' in the\nquantum formalism, consisting of reconstructing the full states of the entities\nof meaning from their collapsed states identified as traces in relevant\ndocuments. The advantages with respect to traditional approaches, such as\nLatent Semantic Analysis (LSA), are discussed by means of concrete examples.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2013 13:08:58 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Broekaert", "Jan", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1304.0715", "submitter": "Ladislau B\\\"ol\\\"oni", "authors": "Ladislau B\\\"ol\\\"oni", "title": "A cookbook of translating English to Xapi", "comments": null, "journal-ref": null, "doi": null, "report-no": "XTR-001", "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Xapagy cognitive architecture had been designed to perform narrative\nreasoning: to model and mimic the activities performed by humans when\nwitnessing, reading, recalling, narrating and talking about stories. Xapagy\ncommunicates with the outside world using Xapi, a simplified, \"pidgin\" language\nwhich is strongly tied to the internal representation model (instances, scenes\nand verb instances) and reasoning techniques (shadows and headless shadows).\nWhile not fully a semantic equivalent of natural language, Xapi can represent a\nwide range of complex stories. We illustrate the representation technique used\nin Xapi through examples taken from folk physics, folk psychology as well as\nsome more unusual literary examples. We argue that while the Xapi model\nrepresents a conceptual shift from the English representation, the mapping is\nlogical and consistent, and a trained knowledge engineer can translate between\nEnglish and Xapi at near-native speed.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 22:17:19 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["B\u00f6l\u00f6ni", "Ladislau", ""]]}, {"id": "1304.1018", "submitter": "Ronan Collobert", "authors": "Dimitri Palaz, Ronan Collobert, Mathew Magimai.-Doss", "title": "Estimating Phoneme Class Conditional Probabilities from Raw Speech\n  Signal using Convolutional Neural Networks", "comments": "In Interspeech 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic\nspeech recognition (ASR) system, the phoneme class conditional probabilities\nare estimated by first extracting acoustic features from the speech signal\nbased on prior knowledge such as, speech perception or/and speech production\nknowledge, and, then modeling the acoustic features with an ANN. Recent\nadvances in machine learning techniques, more specifically in the field of\nimage processing and text processing, have shown that such divide and conquer\nstrategy (i.e., separating feature extraction and modeling steps) may not be\nnecessary. Motivated from these studies, in the framework of convolutional\nneural networks (CNNs), this paper investigates a novel approach, where the\ninput to the ANN is raw speech signal and the output is phoneme class\nconditional probability estimates. On TIMIT phoneme recognition task, we study\ndifferent ANN architectures to show the benefit of CNNs and compare the\nproposed approach against conventional approach where, spectral-based feature\nMFCC is extracted and modeled by a multilayer perceptron. Our studies show that\nthe proposed approach can yield comparable or better phoneme recognition\nperformance when compared to the conventional approach. It indicates that CNNs\ncan learn features relevant for phoneme classification automatically from the\nraw speech signal.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 17:20:41 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2013 11:23:34 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Palaz", "Dimitri", ""], ["Collobert", "Ronan", ""], ["-Doss", "Mathew Magimai.", ""]]}, {"id": "1304.2476", "submitter": "M.M.A. Hashem", "authors": "Rushdi Shams, M.M.A. Hashem, Afrina Hossain, Suraiya Rumana Akter, and\n  Monika Gope", "title": "Corpus-based Web Document Summarization using Statistical and Linguistic\n  Approach", "comments": null, "journal-ref": "Procs. of the IEEE International Conference on Computer and\n  Communication Engineering (ICCCE10), pp. 115-120, Kuala Lumpur, Malaysia, May\n  11-13, (2010)", "doi": "10.1109/ICCCE.2010.5556854", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single document summarization generates summary by extracting the\nrepresentative sentences from the document. In this paper, we presented a novel\ntechnique for summarization of domain-specific text from a single web document\nthat uses statistical and linguistic analysis on the text in a reference corpus\nand the web document. The proposed summarizer uses the combinational function\nof Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of a\nsentence, where SW is the function of number of terms (t_n) and number of words\n(w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is the\nfunction of t_n and w_n in a subject, and t_f in the corpus. 30 percent of the\nranked sentences are considered to be the summary of the web document. We\ngenerated three web document summaries using our technique and compared each of\nthem with the summaries developed manually from 16 different human subjects.\nResults showed that 68 percent of the summaries produced by our approach\nsatisfy the manual summaries.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 07:30:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shams", "Rushdi", ""], ["Hashem", "M. M. A.", ""], ["Hossain", "Afrina", ""], ["Akter", "Suraiya Rumana", ""], ["Gope", "Monika", ""]]}, {"id": "1304.3092", "submitter": "Steven J. Henkind", "authors": "Steven J. Henkind", "title": "Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based\n  Systems", "comments": "Appears in Proceedings of the Second Conference on Uncertainty in\n  Artificial Intelligence (UAI1986)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1986-PG-129-134", "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a considerable amount of work on uncertainty in\nknowledge-based systems. This work has generally been concerned with\nuncertainty arising from the strength of inferences and the weight of evidence.\nIn this paper we discuss another type of uncertainty: that which is due to\nimprecision in the underlying primitives used to represent the knowledge of the\nsystem. In particular, a given word may denote many similar but not identical\nentities. Such words are said to be lexically imprecise. Lexical imprecision\nhas caused widespread problems in many areas. Unless this phenomenon is\nrecognized and appropriately handled, it can degrade the performance of\nknowledge-based systems. In particular, it can lead to difficulties with the\nuser interface, and with the inferencing processes of these systems. Some\ntechniques are suggested for coping with this phenomenon.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:52:35 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Henkind", "Steven J.", ""]]}, {"id": "1304.3265", "submitter": "Maher Jebali", "authors": "Maher Jebali, Patrice Dalle, Mohamed Jemni", "title": "Extension of hidden markov model for recognizing large vocabulary of\n  sign language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 11:56:39 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Jebali", "Maher", ""], ["Dalle", "Patrice", ""], ["Jemni", "Mohamed", ""]]}, {"id": "1304.3432", "submitter": "Stephen Jose Hanson", "authors": "Stephen Jose Hanson, Malcolm Bauer", "title": "Machine Learning, Clustering, and Polymorphy", "comments": "Appears in Proceedings of the First Conference on Uncertainty in\n  Artificial Intelligence (UAI1985)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1985-PG-117-128", "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a machine induction program (WITT) that attempts to\nmodel human categorization. Properties of categories to which human subjects\nare sensitive includes best or prototypical members, relative contrasts between\nputative categories, and polymorphy (neither necessary or sufficient features).\nThis approach represents an alternative to usual Artificial Intelligence\napproaches to generalization and conceptual clustering which tend to focus on\nnecessary and sufficient feature rules, equivalence classes, and simple search\nand match schemes. WITT is shown to be more consistent with human\ncategorization while potentially including results produced by more traditional\nclustering schemes. Applications of this approach in the domains of expert\nsystems and information retrieval are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:56:55 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Hanson", "Stephen Jose", ""], ["Bauer", "Malcolm", ""]]}, {"id": "1304.3841", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho and Haitao Liu", "title": "The risks of mixing dependency lengths from sequences of different\n  length", "comments": "Laguage and referencing has been improved; Eqs. 7, 11, B7 and B8 have\n  been corrected", "journal-ref": "Glottotheory 5 (2), 143-155 (2014)", "doi": "10.1515/glot-2014-0014", "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixing dependency lengths from sequences of different length is a common\npractice in language research. However, the empirical distribution of\ndependency lengths of sentences of the same length differs from that of\nsentences of varying length and the distribution of dependency lengths depends\non sentence length for real sentences and also under the null hypothesis that\ndependencies connect vertices located in random positions of the sequence. This\nsuggests that certain results, such as the distribution of syntactic dependency\nlengths mixing dependencies from sentences of varying length, could be a mere\nconsequence of that mixing. Furthermore, differences in the global averages of\ndependency length (mixing lengths from sentences of varying length) for two\ndifferent languages do not simply imply a priori that one language optimizes\ndependency lengths better than the other because those differences could be due\nto differences in the distribution of sentence lengths and other factors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 20:19:50 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 10:24:00 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""], ["Liu", "Haitao", ""]]}, {"id": "1304.3879", "submitter": "Valmi Dufour-Lussier", "authors": "Valmi Dufour-Lussier (INRIA Nancy - Grand Est / LORIA), Florence Le\n  Ber (ICube), Jean Lieber (INRIA Nancy - Grand Est / LORIA), Emmanuel Nauer\n  (INRIA Nancy - Grand Est / LORIA)", "title": "Automatic case acquisition from texts for process-oriented case-based\n  reasoning", "comments": "Sous presse, publication pr\\'evue en 2013", "journal-ref": "Information Systems (2012)", "doi": "10.1016/j.is.2012.11.014", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method for the automatic acquisition of a rich case\nrepresentation from free text for process-oriented case-based reasoning. Case\nengineering is among the most complicated and costly tasks in implementing a\ncase-based reasoning system. This is especially so for process-oriented\ncase-based reasoning, where more expressive case representations are generally\nused and, in our opinion, actually required for satisfactory case adaptation.\nIn this context, the ability to acquire cases automatically from procedural\ntexts is a major step forward in order to reason on processes. We therefore\ndetail a methodology that makes case acquisition from processes described as\nfree text possible, with special attention given to assembly instruction texts.\nThis methodology extends the techniques we used to extract actions from cooking\nrecipes. We argue that techniques taken from natural language processing are\nrequired for this task, and that they give satisfactory results. An evaluation\nbased on our implemented prototype extracting workflows from recipe texts is\nprovided.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2013 05:52:11 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Dufour-Lussier", "Valmi", "", "INRIA Nancy - Grand Est / LORIA"], ["Ber", "Florence Le", "", "ICube"], ["Lieber", "Jean", "", "INRIA Nancy - Grand Est / LORIA"], ["Nauer", "Emmanuel", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1304.4086", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Hubiness, length, crossings and their relationships in dependency trees", "comments": "The upper bound for for the number of crossings has been improved", "journal-ref": "Ferrer-i-Cancho, R. (2013). Hubiness, length, crossings and their\n  relationships in dependency trees. Glottometrics 25, 1-21", "doi": null, "report-no": null, "categories": "cs.CL cs.DM cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here tree dependency structures are studied from three different\nperspectives: their degree variance (hubiness), the mean dependency length and\nthe number of dependency crossings. Bounds that reveal pairwise dependencies\namong these three metrics are derived. Hubiness (the variance of degrees) plays\na central role: the mean dependency length is bounded below by hubiness while\nthe number of crossings is bounded above by hubiness. Our findings suggest that\nthe online memory cost of a sentence might be determined not just by the\nordering of words but also by the hubiness of the underlying structure. The 2nd\nmoment of degree plays a crucial role that is reminiscent of its role in large\ncomplex networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 13:10:22 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2013 14:01:35 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2013 10:30:09 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2013 10:08:50 GMT"}, {"version": "v5", "created": "Wed, 15 May 2013 13:45:21 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1304.4520", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee and Pushpak Bhattacharyya", "title": "Sentiment Analysis : A Literature Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our day-to-day life has always been influenced by what people think. Ideas\nand opinions of others have always affected our own opinions. The explosion of\nWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,\nContributing to RSS, Social Bookmarking, and Social Networking. As a result\nthere has been an eruption of interest in people to mine these vast resources\nof data for opinions. Sentiment Analysis or Opinion Mining is the computational\ntreatment of opinions, sentiments and subjectivity of text. In this report, we\ntake a look at the various challenges and applications of Sentiment Analysis.\nWe will discuss in details various approaches to perform a computational\ntreatment of sentiments and opinions. Various supervised or data-driven\ntechniques to SA like Na\\\"ive Byes, Maximum Entropy, SVM, and Voted Perceptrons\nwill be discussed and their strengths and drawbacks will be touched upon. We\nwill also see a new dimension of analyzing sentiments by Cognitive Psychology\nmainly through the work of Janyce Wiebe, where we will see ways to detect\nsubjectivity, perspective in narrative and understanding the discourse\nstructure. We will also study some specific topics in Sentiment Analysis and\nthe contemporary works in those areas.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 17:06:24 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1304.5823", "submitter": "Edward Grefenstette", "authors": "Edward Grefenstette", "title": "Towards a Formal Distributional Semantics: Simulating Logical Calculi\n  with Tensors", "comments": "10 pages, to appear in Proceedings of the Second Joint Conference on\n  Lexical and Computational Semantics. June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of compositional distributional models of semantics\nreconciling the empirical aspects of distributional semantics with the\ncompositional aspects of formal semantics is a popular topic in the\ncontemporary literature. This paper seeks to bring this reconciliation one step\nfurther by showing how the mathematical constructs commonly used in\ncompositional distributional models, such as tensors and matrices, can be used\nto simulate different aspects of predicate logic.\n  This paper discusses how the canonical isomorphism between tensors and\nmultilinear maps can be exploited to simulate a full-blown quantifier-free\npredicate calculus using tensors. It provides tensor interpretations of the set\nof logical connectives required to model propositional calculi. It suggests a\nvariant of these tensor calculi capable of modelling quantifiers, using few\nnon-linear operations. It finally discusses the relation between these\nvariants, and how this relation should constitute the subject of future work.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 01:39:42 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2013 20:09:17 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Grefenstette", "Edward", ""]]}, {"id": "1304.5880", "submitter": "Isis Truck", "authors": "M.-A. Abchir (CHART), Isis Truck (CHART), Anna Pappa (LIASD)", "title": "Dealing with natural language interfaces in a geolocation context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the geolocation field where high-level programs and low-level devices\ncoexist, it is often difficult to find a friendly user inter- face to configure\nall the parameters. The challenge addressed in this paper is to propose\nintuitive and simple, thus natural lan- guage interfaces to interact with\nlow-level devices. Such inter- faces contain natural language processing and\nfuzzy represen- tations of words that facilitate the elicitation of\nbusiness-level objectives in our context.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 09:06:36 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Abchir", "M. -A.", "", "CHART"], ["Truck", "Isis", "", "CHART"], ["Pappa", "Anna", "", "LIASD"]]}, {"id": "1304.7157", "submitter": "Leon Derczynski", "authors": "Leon Derczynski, Richard Shaw, Ben Solway, Jun Wang", "title": "Question Answering Against Very-Large Text Collections", "comments": null, "journal-ref": "Master's theses, 2008, University of Sheffield", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Question answering involves developing methods to extract useful information\nfrom large collections of documents. This is done with specialised search\nengines such as Answer Finder. The aim of Answer Finder is to provide an answer\nto a question rather than a page listing related documents that may contain the\ncorrect answer. So, a question such as \"How tall is the Eiffel Tower\" would\nsimply return \"325m\" or \"1,063ft\". Our task was to build on the current version\nof Answer Finder by improving information retrieval, and also improving the\npre-processing involved in question series analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 13:27:19 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Derczynski", "Leon", ""], ["Shaw", "Richard", ""], ["Solway", "Ben", ""], ["Wang", "Jun", ""]]}, {"id": "1304.7282", "submitter": "Urmila Shrawankar Ms", "authors": "Priti Saktel, Urmila Shrawankar", "title": "An Improved Approach for Word Ambiguity Removal", "comments": "Pages:12 Tables: 07 Figures: 14, International Journal of Human\n  Computer Interaction (IJHCI), Volume (3): Issue (3): 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word ambiguity removal is a task of removing ambiguity from a word, i.e.\ncorrect sense of word is identified from ambiguous sentences. This paper\ndescribes a model that uses Part of Speech tagger and three categories for word\nsense disambiguation (WSD). Human Computer Interaction is very needful to\nimprove interactions between users and computers. For this, the Supervised and\nUnsupervised methods are combined. The WSD algorithm is used to find the\nefficient and accurate sense of a word based on domain information. The\naccuracy of this work is evaluated with the aim of finding best suitable domain\nof word.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 10:25:41 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Saktel", "Priti", ""], ["Shrawankar", "Urmila", ""]]}, {"id": "1304.7289", "submitter": "Leon Derczynski", "authors": "Leon Derczynski, Hector Llorens, Naushad UzZaman", "title": "TimeML-strict: clarifying temporal annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  TimeML is an XML-based schema for annotating temporal information over\ndiscourse. The standard has been used to annotate a variety of resources and is\nfollowed by a number of tools, the creation of which constitute hundreds of\nthousands of man-hours of research work. However, the current state of\nresources is such that many are not valid, or do not produce valid output, or\ncontain ambiguous or custom additions and removals. Difficulties arising from\nthese variances were highlighted in the TempEval-3 exercise, which included its\nown extra stipulations over conventional TimeML as a response.\n  To unify the state of current resources, and to make progress toward easy\nadoption of its current incarnation ISO-TimeML, this paper introduces\nTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We\nalso introduce three resources -- a schema for TimeML-strict; a validator tool\nfor TimeML-strict, so that one may ensure documents are in the correct form;\nand a repair tool that corrects common invalidating errors and adds\ndisambiguating markup in order to convert documents from the laxer TimeML\nstandard to TimeML-strict.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 21:31:08 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Derczynski", "Leon", ""], ["Llorens", "Hector", ""], ["UzZaman", "Naushad", ""]]}, {"id": "1304.7359", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho, {\\L}ukasz D\\k{e}bowski and Ferm\\'in Moscoso del\n  Prado Mart\\'in", "title": "Constant conditional entropy and related hypotheses", "comments": "introduction improved; typos corrected", "journal-ref": "Journal of Statistical Mechanics, L07001 (2013)", "doi": "10.1088/1742-5468/2013/07/L07001", "report-no": null, "categories": "cond-mat.stat-mech cs.CL cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constant entropy rate (conditional entropies must remain constant as the\nsequence length increases) and uniform information density (conditional\nprobabilities must remain constant as the sequence length increases) are two\ninformation theoretic principles that are argued to underlie a wide range of\nlinguistic phenomena. Here we revise the predictions of these principles to the\nlight of Hilberg's law on the scaling of conditional entropy in language and\nrelated laws. We show that constant entropy rate (CER) and two interpretations\nfor uniform information density (UID), full UID and strong UID, are\ninconsistent with these laws. Strong UID implies CER but the reverse is not\ntrue. Full UID, a particular case of UID, leads to costly uncorrelated\nsequences that are totally unrealistic. We conclude that CER and its particular\ncases are incomplete hypotheses about the scaling of conditional entropies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2013 11:47:40 GMT"}, {"version": "v2", "created": "Thu, 23 May 2013 10:12:24 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""], ["D\u0119bowski", "\u0141ukasz", ""], ["Mart\u00edn", "Ferm\u00edn Moscoso del Prado", ""]]}, {"id": "1304.7507", "submitter": "Eugene Yuta Bann", "authors": "Eugene Yuta Bann, Joanna J. Bryson", "title": "Measuring Cultural Relativity of Emotional Valence and Arousal using\n  Semantic Clustering and Twitter", "comments": "To be presented at the 35th Annual Meeting of the Cognitive Science\n  Society (CogSci 2013), Berlin, Germany, Wednesday, July 31 - Saturday, August\n  3, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers since at least Darwin have debated whether and to what extent\nemotions are universal or culture-dependent. However, previous studies have\nprimarily focused on facial expressions and on a limited set of emotions. Given\nthat emotions have a substantial impact on human lives, evidence for cultural\nemotional relativity might be derived by applying distributional semantics\ntechniques to a text corpus of self-reported behaviour. Here, we explore this\nidea by measuring the valence and arousal of the twelve most popular emotion\nkeywords expressed on the micro-blogging site Twitter. We do this in three\ngeographical regions: Europe, Asia and North America. We demonstrate that in\nour sample, the valence and arousal levels of the same emotion keywords differ\nsignificantly with respect to these geographical regions --- Europeans are, or\nat least present themselves as more positive and aroused, North Americans are\nmore negative and Asians appear to be more positive but less aroused when\ncompared to global valence and arousal levels of the same emotion keywords. Our\nwork is the first in kind to programatically map large text corpora to a\ndimensional model of affect.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 19:30:11 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Bann", "Eugene Yuta", ""], ["Bryson", "Joanna J.", ""]]}, {"id": "1304.7728", "submitter": "Sugata Sanyal", "authors": "Sugata Sanyal, Rajdeep Borgohain", "title": "Machine Translation Systems in India", "comments": "5 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation is the translation of one natural language into another\nusing automated and computerized means. For a multilingual country like India,\nwith the huge amount of information exchanged between various regions and in\ndifferent languages in digitized format, it has become necessary to find an\nautomated process from one language to another. In this paper, we take a look\nat the various Machine Translation System in India which is specifically built\nfor the purpose of translation between the Indian languages. We discuss the\nvarious approaches taken for building the machine translation system and then\ndiscuss some of the Machine Translation Systems in India along with their\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 18:04:26 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Sanyal", "Sugata", ""], ["Borgohain", "Rajdeep", ""]]}, {"id": "1304.7942", "submitter": "Michele Filannino", "authors": "Michele Filannino, Gavin Brown, and Goran Nenadic", "title": "ManTIME: Temporal expression identification and normalization in the\n  TempEval-3 challenge", "comments": "5 pages, 1 figure, 2 tables Second Joint Conference on Lexical and\n  Computational Semantics (*SEM), Volume 2: Seventh International Workshop on\n  Semantic Evaluation (SemEval 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes a temporal expression identification and normalization\nsystem, ManTIME, developed for the TempEval-3 challenge. The identification\nphase combines the use of conditional random fields along with a\npost-processing identification pipeline, whereas the normalization phase is\ncarried out using NorMA, an open-source rule-based temporal normalizer. We\ninvestigate the performance variation with respect to different feature types.\nSpecifically, we show that the use of WordNet-based features in the\nidentification task negatively affects the overall performance, and that there\nis no statistically significant difference in using gazetteers, shallow parsing\nand propositional noun phrases labels on top of the morphological features. On\nthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the\nidentification phase. Normalization accuracies are 0.84 (type attribute) and\n0.77 (value attribute). Surprisingly, the use of the silver data (alone or in\naddition to the gold annotated ones) does not improve the performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 10:12:54 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Filannino", "Michele", ""], ["Brown", "Gavin", ""], ["Nenadic", "Goran", ""]]}, {"id": "1304.8016", "submitter": "Lukas Barth", "authors": "Lukas Barth, Stephen Kobourov, Sergey Pupyrev, Torsten Ueckerdt", "title": "On Semantic Word Cloud Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing semantic-preserving word clouds in which\nsemantically related words are close to each other. While several heuristic\napproaches have been described in the literature, we formalize the underlying\ngeometric algorithm problem: Word Rectangle Adjacency Contact (WRAC). In this\nmodel each word is associated with rectangle with fixed dimensions, and the\ngoal is to represent semantically related words by ensuring that the two\ncorresponding rectangles touch. We design and analyze efficient polynomial-time\nalgorithms for some variants of the WRAC problem, show that several general\nvariants are NP-hard, and describe a number of approximation algorithms.\nFinally, we experimentally demonstrate that our theoretically-sound algorithms\noutperform the early heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 22:14:18 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Barth", "Lukas", ""], ["Kobourov", "Stephen", ""], ["Pupyrev", "Sergey", ""], ["Ueckerdt", "Torsten", ""]]}]